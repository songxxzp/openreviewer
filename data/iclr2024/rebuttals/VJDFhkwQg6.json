[
    {
        "title": "Federated contrastive GFlowNets"
    },
    {
        "review": {
            "id": "guNZTDxOqK",
            "forum": "VJDFhkwQg6",
            "replyto": "VJDFhkwQg6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6008/Reviewer_rsSg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6008/Reviewer_rsSg"
            ],
            "content": {
                "summary": {
                    "value": "This submission is mathematically exciting but, unfortunately, poorly explained and poorly motivated. The methodological section requires a much better explanation. It is currently mainly an enumeration of results. In addition, no natural problems that in any apparent way benefit from the approach are presented. Moreover, the plots in the experimental section need to be more convincing. Running time and other gains need to be made crystal clear. If this is an uncut diamond, please cut it."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This submission is mathematically exciting."
                },
                "weaknesses": {
                    "value": "This submission is poorly explained and poorly motivated."
                },
                "questions": {
                    "value": "\"The main purpose of our experiments is to verify the empirical performance of FC-GFlowNets, i.e., their capacity to accurately sample from the combination of local rewards \"\nThis is not sufficient. \n\n\" This is also reflected in the average reward over the top 800 samples \u2014 identical to the centralized version for FC-GFlowNet, but an order of magnitude smaller for PCVI. Again, these results corroborate our theoretical claims about the correctness of our scheme for combining GFlowNets. \"\nIs the centralized using a more standard GFN training approach?\n\n\" The five left- most plots indicate that the local GFlowNets were accurately trained \"\nWhy\n\n\" Bayesian phylogenetic inference: learned \u00d7 ground truth distributions. Following the pattern in Figures 2-4, the goodness-of-fit from local GFlowNets (Clients 1-5) is directly reflected in the distribution learned by FC-GFlowNet. \"\nWhy is this good?\n\n\" In particular, GFlowNets trained via CB demonstrated faster convergence compared to current art when intermediate states are not terminal, while being com- petitive in other scenarios \"\nWhen are intermediate states terminal. Where is this shown?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698570816266,
            "cdate": 1698570816266,
            "tmdate": 1699636644291,
            "mdate": 1699636644291,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uAUhAflr7G",
                "forum": "VJDFhkwQg6",
                "replyto": "guNZTDxOqK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review. If you need any further clarification, please let us know. If you are happy with the answers, please consider raising your score. \n\n**Weaknesses**\n\n> **This submission is poorly explained and poorly motivated.**\n\nWe are glad you found our work exciting and agree our motivation deserved improvement. In fact, this was also pointed out by other reviewers --- see answers to reviewers wWFS and Npeg for a thorough discussion wrt motivation. Based on that, we have expanded the 3rd paragraph in the introduction. We have also linked Appendix D in the main paper which, e.g., provides more details on federated Bayesian inference. We hope this helps elevate our work in your opinion.\n\n**Questions**\n\n> **\"The main purpose of our experiments is to verify the empirical performance of FC-GFlowNets, i.e., their capacity to accurately sample from the combination of local rewards \" This is not sufficient.**\n\nThe overall goal of most federated learning (FL) works is to get models that preserve the same performance as a model trained in centralized settings, even if under severe communication constraints --- see, e.g., the seminal work by McMahan et al. [1]. If the FL algorithm is after point estimates, it is natural to compare federated point estimates with the one obtained with centralized data. If we are sampling (e.g., from some posterior), the analogous is comparing how well federated sampling approximates the centralized posterior [2,3]. \n\n[1] Communication-Efficient Learning of Deep Networks from Decentralized Data, AISTATS 2016\n\n[2] Federated stochastic gradient Langevin Dynamics, UAI 2021\n\n[3] QLSD: Quantised Langevin Stochastic Dynamics for Bayesian Federated Learning, AISTATS 2022\n\n\n\n> **\" This is also reflected in the average reward over the top 800 samples \u2014 identical to the centralized version for FC-GFlowNet, but an order of magnitude smaller for PCVI. Again, these results corroborate our theoretical claims about the correctness of our scheme for combining GFlowNets. \" Is the centralized using a more standard GFN training approach?**\n\nThe centralized GFlowNet is a standard GFlowNet trained to sample from $\\prod_{n=1}^N R_n$. We train it using the CB loss.\n\n> **\"The five left- most plots indicate that the local GFlowNets were accurately trained\" why**\n\nEach plot compares the distribution of a locally-trained GFlowNet against the rewards they were trained to sample proportionally from. More specifically, we sample from the trained GFlowNet to build an explicit empirical probability mass function. Then, we compare it against the (target) probability defined by the local reward. The points in the plot show the empirical probabilities in the horizontal axis and the target one in the vertical one. Each point corresponds to some element $x \\in \\mathcal{X}$ in the support of the local reward.  If the empirical and target probabilities coincide, the plot should coincide on the highlighted diagonal, as is the case in these plots.\n\n> **\" Bayesian phylogenetic inference: learned \u00d7 ground truth distributions. Following the pattern in Figures 2-4, the goodness-of-fit from local GFlowNets (Clients 1-5) is directly reflected in the distribution learned by FC-GFlowNet. \" Why is this good?**\n\nThis is just a throwback to Theorem 2: good local approximations are necessary for a good global approximation. In other words, for FC-GflowNet to sample accurately from $\\prod_n R_n$, each client $n$ must be well trained to sample proportional to its local reward $R_n$.\n\n\n> **\" In particular, GFlowNets trained via CB demonstrated faster convergence compared to current art when intermediate states are not terminal, while being competitive in other scenarios \" When are intermediate states terminal. Where is this shown?**\n\nIntermediate states are terminal whenever they belong to the support of the reward, but can also be incremented to create another valid object. For instance, in the sequence experiments, any sequence of size 5 belongs to the set of terminal states $\\mathcal{X}$ but can be increased with an extra element to yield another sequence (of size 6) --- that also belongs to $\\mathcal{X}$. We have included more details in Appendix C."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699988756035,
                "cdate": 1699988756035,
                "tmdate": 1700005451587,
                "mdate": 1700005451587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uI6NOqhT25",
                "forum": "VJDFhkwQg6",
                "replyto": "uAUhAflr7G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_rsSg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_rsSg"
                ],
                "content": {
                    "title": {
                        "value": "Motivation"
                    },
                    "comment": {
                        "value": "Thanks for your work. I still find the paper very interesting but poorly motivated. I had hoped that you would provide an intersting real world application where the FC-GFlowNets approach is crucial."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573797274,
                "cdate": 1700573797274,
                "tmdate": 1700573797274,
                "mdate": 1700573797274,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r2CSgc1Lqe",
                "forum": "VJDFhkwQg6",
                "replyto": "guNZTDxOqK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "As mentioned by reviewer Xj4h, FC-GFlowNets may have a range of different applications. For instance, in drug discovery [e.g., 1-4], we can use FC-GFlowNets to scalarize different utilities/objectives or combine pre-trained models. In the context of causal discovery [e.g., 5-9], we can use FC-GFlowNets to learn cause/effect structures from federated data.  FC-GFlowNets can also scale up Bayesian inference when the likelihood is hard to evaluate, so we deliberately distribute data for computational efficiency, similar to what is done in [10-13].\n\nWould discussing these examples further in the main text alleviate your concerns? We'll be happy to incorporate suggestions.\n\n[1] Compositional Sculpting of Iterative Generative Processes. NeurIPS 2023\n\n[2] Multi-Objective GFlowNets. ArXiV 2022\n\n[3] MARS: Markov Molecular Sampling for Multi-objective Drug Discovery ICLR 2021.\n\n[4] Computer-aided multi-objective optimization in small molecule discovery. Patterns, 2023.\n\n[5] DynGFN: Bayesian Dynamic Causal Discovery using Generative Flow Networks. NeurIPS 2023.\n\n[6] Bayesian Structure Learning with Generative Flow Networks. UAI 2022\n\n[7] Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network. NeurIPS 2023\n\n[8] Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets. ArXiV 2023\n\n[9] Bayesian learning of Causal Structure and Mechanisms with GFlowNets and Variational Bayes. ArXiv 2022 \n\n[10] Asymptotically Exact, Embarrassingly Parallel MCMC. UAI 2014\n\n[11] Merging MCMC Subposteriors through Gaussian-Process Approximations. Bayesian Analysis, 2019.\n\n[12] Speeding Up MCMC by Efficient Data Subsampling. Journal of the American Statistical Association, 2019.\n\n[13] DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm via Langevin Monte Carlo within Gibbs. ICML 2021."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577616035,
                "cdate": 1700577616035,
                "tmdate": 1700577799403,
                "mdate": 1700577799403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Om8tHTwSEN",
            "forum": "VJDFhkwQg6",
            "replyto": "VJDFhkwQg6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
            ],
            "content": {
                "summary": {
                    "value": "An algorithm is proposed for training a generative flow network (GFlowNet) to match a product of distributions, each of which is sampled by a \"client\" GFlowNet. A training objective is stated, its correctness is proved, and bounds relating error of clients to that of the centralized model are derived. Experiments are done on federated versions of several existing tasks and on a new domain for GFlowNets (a toy case of Bayesian phylogenetic inference), where it performs well compared to a non-GFlowNet baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Excellent exposition. I have no complaints on the presentation or on the math. I easily grasped the main idea on the first reading.\n  - But see one of the weaknesses on why two of the proofs could be one-line reductions to existing results.\n- Beautiful and original idea. Sampling from a product of GFlowNets is natural, but the application to federated learning is creative. \n  - A reference to consider adding is \"Compositional sculpting of iterative generative processes\" [Garipov et al., NeurIPS 2023, to appear], which considers a different kind of modular combination of samplers and could probably be used in a federated setting as well. (I am well aware that its omission is not a weakness as this paper appeared on arXiv on 28 September!)\n- Application to phylogenetic inference is also a new contribution that could be expanded; I wonder how it would scale."
                },
                "weaknesses": {
                    "value": "- Critical missing details for phylogenetic inference. What are the states? What are the actions?\n- It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a *stochastic* reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023].\n- The experimental validation is a little thin (esp. given the next point). \n  - This is not a major weakness for me given that there may be no natural baselines, and it is made up for by the good idea and diversity of experiments.\n  - However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).\n- The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions. \n  - It is well known that the expected square difference between two independent samples from a distribution is the same as twice the variance. So the loss in section 3.3 is equivalently optimizing variance of $\\log P_F(\\tau)-\\log R(x)-\\log P_B(\\tau\\mid x)$.\n  - This variance loss is described in \"GFlowNets and VI\" [Malkin et al., ICLR 2023] (\"local baseline\"). It was independently discovered and tested in \"Robust scheduling\" [Zhang et al., ICLR 2023]. Such a gradient variance reduction method is originally proposed in \"VarGrad\" [Richter et al., NeurIPS 2020].\n  - This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms. \n    - Figure 7, which is hidden in the Appendix, shows that on other domains, CB performs similarly to TB/DB.\n    - It seems a little misleading not to point to Figure 7 in the main text. In my opinion, it would not make the paper weaker to show all four plots in the main text and to say that first steps are made towards understanding when CB is helpful and when it is not.\n  - Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.\n- The bound in Theorem 2 is nice but may not be very useful in practice. If one of the models is missing a mode, which is quite possible with reverse-KL objectives like those used here, $\\alpha_n$ will be very small, and the final bound is not useful. Therefore, I wonder if one can obtain similar bounds on other divergences that are less sensitive than the Jeffrey divergence to mode-dropping.\n\nI am happy to increase the score from 5 to 6 or even 8 if these are satisfactorily addressed."
                },
                "questions": {
                    "value": "I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?\n\nIn particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6008/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6008/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700151233,
            "cdate": 1698700151233,
            "tmdate": 1700328522910,
            "mdate": 1700328522910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mHjAHnRuGa",
                "forum": "VJDFhkwQg6",
                "replyto": "Om8tHTwSEN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer Xj4h, Part 1/2"
                    },
                    "comment": {
                        "value": "We appreciate your thoughtful feedback. Hopefully, the answers below sufficiently address your concerns. Please let us know if something else needs further clarification or if we missed something.\n\n**Weaknesses**\n\n> **Critical missing details for phylogenetic inference. What are the states? What are the actions?**\n\n**A:** In the phylogeny experiments, a state is represented as a forest. Initially, each leaf belongs to a different singleton tree. An action consists of picking two trees and joining their roots to a newly added node. The generative process is finished when all nodes are connected in a single tree. We understand that this description was missing in the document, and we included it in Appendix C.1, along with an illustrative picture.\n\n> **It should be noted that an alternative approach to the problem in the Bayesian posterior setting is to train a single GFlowNet with a stochastic reward, as done in [Deleu et al., UAI 2022] and [Deleu et al., NeurIPS 2023]**\n\n**A:** Thanks for pointing this out. We are unsure about the relevance of the two references to stochastic rewards \u2014 both focus on causal structure learning. If they were not a typo, could you elaborate on that?\n\nNonetheless, we could use Zhang et al.'s (ICML 2023) scheme for training GFlowNets in environments with stochastic rewards drawn from the distribution\n\n$$\nR(x) = R_{c}(x), \\, c \\sim q(\\cdot | \\{1, \\dots, K\\}), % \\{R_{1}(x), \\dots, R_{K}(x)\\}),  .  \n$$\n\nin which $q$ is a categorical distribution over the clients $\\{1, \\dots, K\\}$. By assigning a weight $q(c)^{-1}$ to the $c$th client's reward, $R_{c}(x)^{q(c)^{-1}}$, Proposition 1 of Zhang et al. ensures that the trained GFlowNet would sample an object $x$ with probability proportional to\n\n$\\exp \\underset{c \\sim q}{\\mathbb{E}} \\left[\\log R\\_{c}(x)^{q(c)^{-1}}\\right]  = \\underset{1 \\le k \\le K}{\\prod} {R}_{{k}}(x) $\n\nThe downside is that this requires many communication steps between clients and server. This is precisely the bottleneck we are trying to avoid with FC-GFlowNets, imposing one *single communication step* between the clients and the server. We wrote a short version of this discussion in related works (Appendix D).\n\n\n\n> **However, there are still comparisons to be made, such as using different training objectives for the client models (TB or CB).**\n\n**A:** Thanks for the suggestion. In principle, FC-GFlowNets are agnostic to how the local models were trained as long as they provide us with forward and backward policies. We have now included Table 2 (below) in Appendix C, comparing the performance of FC-GFlowNets when clients are trained with TB vs. CB. The metrics are all very similar.\n\n|                          \t\t | Grid World         \t\t |       \t\t | Multisets         \t\t |      \t\t | Sequence          \t\t |       \t\t |\n|-----------------------------------|-------------------------|-------------------------|-------------------------|-------------------------|-------------------------|--------------------|\n|                          \t\t | L\u2081 \u2193          \t\t | Top-800 \u2191      \t\t | L\u2081 \u2193          \t\t | Top-800 \u2191      \t\t | L\u2081 \u2193          \t\t | Top-800 \u2191      \t\t |\n| FC-GFlowNet (CB)         \t\t | 0.038          \t\t | -6.355         \t\t | 0.130          \t\t | 27.422         \t\t | 0.005          \t\t | -1.535         \t\t |\n|                          \t\t | (\u00b1 0.016)      \t\t | (\u00b1 0.000)      \t\t | (\u00b1 0.004)      \t\t | (\u00b1 0.000)      \t\t | (\u00b1 0.002)      \t\t | (\u00b1 0.000)      \t\t |\n| FC-GFlowNets (TB)        \t\t | 0.039          \t\t | -6.355         \t\t | 0.131          \t\t | 27.422         \t\t | 0.006          \t\t | -1.535         \t\t |\n|                          \t\t | (\u00b1 0.006)      \t\t | (\u00b1 0.000)      \t\t | (\u00b1 0.018)      \t\t | (\u00b1 0.000)      \t\t | (\u00b1 0.005)      \t\t | (\u00b1 0.000)      \t\t |\n\n\n\n> **The \"contrastive loss\" is claimed as original, but in fact it is not. I suggest that the authors revise the discussion on this in section 3.3 and in the claimed contributions**\n\n**A:**  We once again thank you for noticing the relationship between the contrastive loss (CL) and the log-partition variance loss (VL) of Zhang et al. (ICLR 2023) \u2014 which we did not notice.  In fact, CL equals twice VL *in expectation*. Nonetheless, as you pointed out, CL and VL use different estimators for the variance up to some positive multiplicative constant. Additionally, our balance condition (CB) is novel per se and essential to deriving the federated loss (Corollary 1), which is the basis of our method.  We have updated the introduction, section 3.2 (which introduces the CB), and related works (Appendix D) to acknowledge this work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700004756071,
                "cdate": 1700004756071,
                "tmdate": 1700008954680,
                "mdate": 1700008954680,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0D9X3GT3KF",
                "forum": "VJDFhkwQg6",
                "replyto": "Om8tHTwSEN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer Xj4h, Part 2/2"
                    },
                    "comment": {
                        "value": "> **This leads me to wonder whether differences between CL and TB (Figure 6) are only due to insufficiently high learning rate on logZ for TB, or differently tuned learning rates for the policies in both algorithms.**\n\n**A:** To verify whether our gains stemmed from a poor choice of learning rate, we have run additional experiments (in Figure 8), sliding it from $0.001$ to $10$. In all cases, we saw CL converging to better optima than TB. That being said, while there may be some learning rate for log Z leading to better convergence, finding it may be difficult. We have updated Figure 6 to include the results from (former) Figure 7.\n\n> **Related to this, the proofs of all the results in section 3.3 have one-line reductions to existing results (the TB training theorem and the TB gradient analysis theorem). The current proofs are quite obfuscated.**\n\n**A:** We tried to make the proofs as clear and as self-contained as possible. In particular, as our arguments in Section 3.2 consist primarily of standard and easy-to-follow calculations, we believe that articulating a comprehensive proof in the Appendix is beneficial for enhancing the results' clarity. It is unclear whether shortening the proofs would make them easier to follow.\n\n> **The bound in Theorem 2 is nice but may not be very useful in practice.**\n\n**A:** We did not intend Theorem 2 to be used for diagnosing pathological implementations of FC-GFlowNet. Theorem 2 only aims to illustrate that the global distribution may diverge considerably from the target distribution when a client's policies are inadequately trained. Particularly, if one of the clients fails to detect some modes, as you suggested, then the global distribution will also miss these modes. This effect, similarly observed in e.g. De Souza et al. (AISTATS 2022), is accurately represented by our upper bound on the Jeffrey's divergence.\n\nWe agree that developing good diagnostics for federated GFlownets is an important topic that we would like to pursue in future works. However, the design of tight divergence bounds and effective diagnostic methods is currently an unaddressed issue in the GFlowNets literature, even for centralized settings.\n\n\n**Questions**\n\n> **I like the motivation paragraph on the first page, but what do you see as the main future applications: Privacy-preserving distributed training? Large-scale Bayesian inference where the full reward is expensive to compute on a single client? Modular combination of pretrained GFlowNets?**\n\n\n**A:** Thank you for the question \u2014 those are precisely the three main applications we envision for federated GFlowNets. We realized this was hard to grasp for most reviewers and decided to elaborate further on this paragraph and the conclusion.\n\nWe believe that the distributed training of GFlowNets will be invaluable in addressing large-scale Bayesian inference problems, e.g., structure learning (Deleu et al., NeurIPS 2023) and phylogenetic inference (Altekar et al., Bioinformatics 2004), in which the reward function may be expensive to evaluate due to the size of the conditioning dataset.\n\n> **In particular, I am not sure to understand the private-rewards application. If each GFlowNet fits (close to) perfectly and can share its policies, then each client's reward density can be recovered as the ratio of its forward and backward policies.**\n\n**A:** Indeed, the rewards can be retrieved if the GFlowNets are perfectly trained.  However, having access to the reward does not entail, e.g., disclosing the data used to compute a likelihood (or a local posterior up to some multiplicative constant). In this case, the local GFlowNet can be seen as a proxy that provides some notion of privacy w.r.t. local data."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700004785101,
                "cdate": 1700004785101,
                "tmdate": 1700009021841,
                "mdate": 1700009021841,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mD3p1BrLFC",
                "forum": "VJDFhkwQg6",
                "replyto": "Om8tHTwSEN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thanks for the clarifications and new experiments, which have improved the paper. Just to clarify a couple more things:\n\n> We tried to make the proofs as clear and as self-contained as possible...\n\nI don't think repeating a difficult proof is worth it when there is such a simple reduction (zero CL implies TB satisfied for some value of $\\log Z$). I may be unable to convince you on this, but at least I think that **the existence of such a reduction should be mentioned** (could be just a sentence in the main text saying that CL implies TB for the obvious reason) -- it would only make the paper stronger!\n\n> We are unsure about the relevance of the two references to stochastic rewards \u2014 both focus on causal structure learning. If they were not a typo, could you elaborate on that?\n\nNo, not a typo. In those papers, the goal is to sample a posterior $p(G\\mid D)$ where $D=\\{x_i\\}_{i=1}^n$. The reward is the joint $p(G,D)=p(G)\\prod_ip(x_i\\mid G)$. To avoid scoring all samples $x_i$ using the latent structure graph $G$, a stochastic reward of $p(G)p(x_i)^n$ for a random $x_i$ is used. This gives an unbiased estimate of the log-reward and therefore of the loss. Such an approach is applicable to the general setting where the reward decomposes as prior times product of data-dependent terms, including the phylogenetic tree setting you consider.\n\n> Nonetheless, we could use Zhang et al.'s (ICML 2023) scheme for training GFlowNets in environments with stochastic rewards\n\nI am not sure which paper you are talking about here."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328509995,
                "cdate": 1700328509995,
                "tmdate": 1700328649280,
                "mdate": 1700328649280,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6KQp43At4I",
                "forum": "VJDFhkwQg6",
                "replyto": "hWUadoHIh6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Xj4h"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for clarifying. That paper is not published in ICML 2023, and the proposition there is not an original result (actually, the fact is already in \"GFlowNet foundations\", section 3.3.2)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700349741816,
                "cdate": 1700349741816,
                "tmdate": 1700349741816,
                "mdate": 1700349741816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MqkRJXKUrp",
            "forum": "VJDFhkwQg6",
            "replyto": "VJDFhkwQg6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of training a GFlowNet in federated manner. Each client has its own reward function and local data. And each client trains its local GFlowNet. The goal here for the server is learn a GFlowNet model that samples data proportional to the product of clients reward. The paper conducts experiments on various tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studies the unexplored problem of federated training of GFlowNet.\n2. The paper conducts extensive experiments on various tasks."
                },
                "weaknesses": {
                    "value": "1. I believe should give more concrete example on why learning a GFlowNet samples data proportional to product of reward is important in federated setting. From reading the paper and experiments, I am not clear about the significance of the problem studied by this paper. In fact, it seems the problem tackled by this paper is an special case while it is not evident that this special case is important in federated setting.\n2. It seems that the main contribution of this paper is its problem setting where learning a GFlowNet model that samples data proportional to product of rewards can be done in a federated fashion. However, given the problem, learning the global model aggregating local GFlowNet seems to be straightforward. Therefore, I think I believe the contribution of the paper in federated learning is not high."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749801114,
            "cdate": 1698749801114,
            "tmdate": 1699636644094,
            "mdate": 1699636644094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cVil7e2Dec",
                "forum": "VJDFhkwQg6",
                "replyto": "MqkRJXKUrp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for your feedback. We realize the motivation provided in the introduction was brief, which may have underplayed the impact of our contribution. We hope our answers clear your concerns, and we have incorporated their content in the revised manuscript. If your concerns are sufficiently solved, please consider raising your score. Otherwise, we will be happy to engage further and provide more clarifications. \n\n\n> **Q: I am not clear about the significance of the problem studied by this paper. In fact, it seems the problem tackled by this paper is an special case while it is not evident that this special case is important in federated setting.**\n\n**A:** The most common problem in federated learning is learning some point estimate $\\theta^{\\star}$ that minimizes the sum of loss functions  $\\ell_{1}, \\ldots, \\ell_K$ for each client $k=1\\ldots K$. Typically, the loss is equivalent to the negative log-likelihood of $\\theta$ on some local dataset $\\mathcal{D}_k$. In the Bayesian realm, the corresponding problem is sampling from a posterior distribution $p(\\theta | \\mathcal{D})$ proportional to the products of the local likelihoods and some prior over theta. Each of these terms in the product can be seen as a local reward. Then, the product of local rewards is proportional to the posterior. That being said, FC-GFlowNets are a natural solution for Bayesian federated learning cases in which $\\theta$ is a discrete-valued random variable.  More generally, FC-GFlowNets work for any reward that factorizes as products. Thus, it can be used for, e.g., multi-objective active learning. Importantly, there well-established interest of the ML community in Bayesian federated learning \u2014 see., e.g., Appendix D for more details. To clarify the significance of our problem, we have also extended paragraph 3 (Introduction) in the revised manuscript. Please see our first answer to reviewer wWFS for further discussion about the motivation.\n\n> **Q: It seems that the main contribution of this paper is its problem setting where learning a GFlowNet model that samples data proportional to product of rewards can be done in a federated fashion. However, given the problem, learning the global model aggregating local GFlowNet seems to be straightforward.**\n\n**A:** We are not aware of any simpler approach to sampling from the product of GFlownets. The naive strategy would be to train a GFlowNet to sample from the product of terminal distributions $\\hat{R} = \\prod_{k=1}^{K} p_k^\\intercal(x)$ of the clients $k=1\\ldots K$. Note, however, that $p_k^\\intercal(x)$ requires summing over all terminal paths leading to $x$ \u2014 which is infeasible in practice. Furthermore, using a Monte Carlo estimate of $p_k^\\intercal(x)$ plus some standard GFlowNet loss results in biased gradients, which are undesirable if we want to converge with stochastic gradient descent.\n\nFurthermore, sampling from products of arbitrary distributions over discrete objects is not a trivial task. This is the reason, for instance, why most of the literature on parallel MCMC focuses solely on continuous cases. Importantly, while there is a good body of literature on Bayesian federated learning, to the best of our knowledge, FC-GFlowNets is the first method capable of handling discrete cases."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699986858041,
                "cdate": 1699986858041,
                "tmdate": 1700650959630,
                "mdate": 1700650959630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IZO8CCdjOY",
                "forum": "VJDFhkwQg6",
                "replyto": "cVil7e2Dec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors response. However, the rebuttal does not address most of my concerns. I still believe that the motivation of this work need some improvements.\n\nFirst, it seems that the paper formalizes the objective function of training a GFlowNet model as the product of local objectives for training a GFlowNet locally. GFlowNet is a generative model and as a result there might be a user that wants to use GFlowNet to generate new samples with desirable characteristics with respect to their own objective. I do not understand the motivation behind learning a global GFlowNet while clients have different objectives. I understand that federated learning can be helpful in cases where clients do not have enough data to train a good model locally. However, when it comes to training a GFlowNet with limited data another alternative is to leverage active learning techniques where the model is trained iteratively and at each iteration generated samples are added to the initial dataset. My main point is federated training of a GFlowNet is not well-motivated when clients have different objectives and data distributions. The author response did not address this concern.\n\nAnother thing that prevents me to vote positively for this paper is that I think the problem studied by this paper can be solved by applying existing Bayesian federated learning methods. Then the paper only introduces an unexplored problem in federated learning and the paper does not overcome any considerable challenge in federated learning. Moreover, the paper does not show that the problem is important by giving concrete examples in real world applications. The response does not indicate the importance of the studied problem. This limits the contribution of the paper from federated learning point of view,\n\nTo sum up, reading the rebuttal I decide to stay with my initial score. I am sorry if I get back to authors late. However, authors are welcome to respond to my comments again and I will definitely consider the authors response carefully."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702167987,
                "cdate": 1700702167987,
                "tmdate": 1700702167987,
                "mdate": 1700702167987,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8AcmVQzy84",
                "forum": "VJDFhkwQg6",
                "replyto": "MqkRJXKUrp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for engaging"
                    },
                    "comment": {
                        "value": "Thanks for engaging in the discussion. To allow further interaction, we've crafted a very to-the-point answer --- touching key points of your reply. Please let us know if this clarifies your concerns. If not, we hope this message opens a channel for further clarification. \n\n> I do not understand the motivation behind learning a global GFlowNet while clients have different objectives. \n\n**Local posteriors are realistically never the same.** Note that the GFlowNet\u2019s training objective (reward function) is dependent on the clients\u2019 data. For example, in a Bayesian federated learning setting, each client possesses a different data set and, in particular, has a different training objective (posterior distribution) upon which to train a GFlowNet. Thus, except in the very unlikely scenario in which the posterior distribution is the same for all clients, the objective function will differ among the clients. \n\n**Our method also applies to multi-objective optimization.** We also note that the utility of our setup goes beyond federated Bayes. As we have discussed in our answer to reviewer wWFS, our proposal can be applied to multi-objective optimization, where the goal is to obtain samples with high values across a pool of utility functions. For instance, in drug discovery, a key challenge is finding molecules that satisfy multiple constraints (e.g., affinity, solubility, safety).  In a setting where different experts work independently to obtain property-specific models using GFlowNets, this challenge can be cast as sampling from the product of these local GFlowNets. Note also that our work allows for reusing pre-trained GFlownNets to sample from a weighted product, effectively reusing these models as pointed by reviewer X4jh.\n\n\n> However, when it comes to training a GFlowNet with limited data another alternative is to leverage active learning techniques where the model is trained iteratively and at each iteration generated samples are added to the initial dataset.\n\n**GFlowNets are not used for data augmentation.** We would like to emphasize that GFlowNet is not a generative model designed to mimic the (unknown) data distribution and subsequently generate more samples; instead, it is a method tailored to sampling from an unnormalized distribution over compositional objects like graphs. In this sense, GFlowNets are one-of-a-kind, different from VAEs, GANs, Normalizing Flows, etc. In fact, GFlowNets are closer to Markov Chain Monte Carlo (MCMC) than to other deep generative models.\n\n\n> I think the problem studied by this paper can be solved by applying existing Bayesian federated learning methods\u2026 \n\n**Existing federated sampling method cannot handle discrete-valued random variables.** We would like to clarify that this problem (federated Bayes on discrete posteriors) cannot be solved using previous techniques from the Bayesian federated learning literature.  By discrete posteriors, we mean that we are sampling from a discrete-valued random variable with a reward proportional to said posterior. The vast majority of the literature on Bayesian federated learning (as known as federated sampling) relies on either gradients wrt the parameter of interest (e.g., SGLD-based methods) . The works on embarrassingly parallel Bayes also do not apply, since they impose continuous approximations (e.g., Gaussians and KDEs) to the local contributions/subposteriors\n\n---\nWe do hope for, and would greatly appreciate, your stronger support for this work once you feel your concerns have been addressed. Otherwise, please do not hesitate to engage further."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707748078,
                "cdate": 1700707748078,
                "tmdate": 1700708017201,
                "mdate": 1700708017201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RmyWT0HGuL",
                "forum": "VJDFhkwQg6",
                "replyto": "8AcmVQzy84",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I agree with you that GFlowNet does not mimic the unknown data distribution to generate new samples. However, I do not agree with the argument that since GFlowNet does not mimic the unknown data distribution, it cannot be used for data augmentation. In contrast, I believe data augmentation can improve GFlowNet training. Data augmentation for generative methods such as diffusion models and VAE should be performed carefully to preserve the original data distribution since these methods work based on mapping the data distribution to a noise. However, GFlowNet is trained using labeled data and learns a policy that samples objects proportional to their reward regardless of labeled data distribution. As the number of labeled data increases the performance of GFlowNet may improve. Therefore, one can add new data samples to the available labeled data without being concerned about the data distribution. Also, the paper \"Biological Sequence Design with GFlowNets\" published by ICML 2022 trains the GFlowNet by adding generated data step by step to the initial data. The experiments in the mentioned work show that the resulting GFlowNet model works better than GFlowNet without data augmentation. Furthermore, reading the paper and rebuttal it is not obvious to me that what is the technical and theoretical challenges of handling discrete-valued random variables."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729381681,
                "cdate": 1700729381681,
                "tmdate": 1700729381681,
                "mdate": 1700729381681,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8FHw7lUOoP",
                "forum": "VJDFhkwQg6",
                "replyto": "MqkRJXKUrp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_Npeg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I believe there is not any misunderstanding here. I am confident that experiments in the mentioned paper show that GFlowNet-AL show better performance than GFlowNet in some biological sequence design tasks."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738547251,
                "cdate": 1700738547251,
                "tmdate": 1700738578487,
                "mdate": 1700738578487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "84BdtBO0we",
            "forum": "VJDFhkwQg6",
            "replyto": "VJDFhkwQg6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6008/Reviewer_wWFS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6008/Reviewer_wWFS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel framework for federated learning of GFlowNets, called FC-GFlowNet, which is based on a divide-and-conquer strategy that requires only a single communication step. The authors introduce a new concept, contrastive balance, which provides necessary and sufficient conditions for the correctness of general GFlowNets. The paper also presents experiments demonstrating the effectiveness of FC-GFlowNets in various settings, including grid-world, sequence, and multiset generation, and Bayesian phylogenetic inference. Overall, the paper provides a significant contribution to the community interested in generative modeling, enabling GFlowNets in federated settings, introducing a novel training scheme for centralized settings, and demonstrating the potential of GFlowNets on a new application domain."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides a contribution to the community interested in generative modeling, enabling GFlowNets in federated settings, introducing a novel training scheme for centralized settings, and demonstrating the potential of GFlowNets on a new application domain.\n  2. The authors introduce a new concept, contrastive balance, which provides necessary and sufficient conditions for the correctness of general GFlowNets, and demonstrate its effectiveness in training GFlowNets."
                },
                "weaknesses": {
                    "value": "1. However, the motivation for introducing GFlowNets into Federated Learning (FL) appears to be lacking. Despite the existing research gap, what specific challenges or benefits does FL encounter that require the integration of GFlowNets?\n\n2. Conversely, is there a scenario in which GFlowNets necessitate Federated Learning (FL)? In my view, the explanation provided in the third paragraph is somewhat lacking. It would be helpful to provide more practical examples to illustrate this.\n\n3. The experimental setup regarding the clients is not sufficiently clear. Are all clients identical, or does this setup involve non-iid (non-independent and identically distributed) problems?\n\n4. It would enhance clarity if the authors could include a figure or pseudocode illustrating the algorithm."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6008/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6008/Reviewer_wWFS",
                        "ICLR.cc/2024/Conference/Submission6008/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6008/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769905235,
            "cdate": 1698769905235,
            "tmdate": 1700645476052,
            "mdate": 1700645476052,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lO7gjblHyZ",
                "forum": "VJDFhkwQg6",
                "replyto": "84BdtBO0we",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We hope the answers below solve your issues. If your concerns were not cleared, please let us know and we'll try to clarify them further. Otherwise, please consider raising your score. \n\n\n>**Q:** However, **the motivation for introducing GFlowNets into Federated Learning (FL) appears to be lacking**. Despite the existing research gap, **what specific challenges or benefits does FL encounter that require the integration of GFlowNets? Conversely, is there a scenario in which GFlowNets necessitate Federated Learning (FL)?**\n\n\n**A:** Thanks for the opportunity to motivate Federated GFlowNets further. As mentioned in the Introduction, we foresee at least two impactful applications of FC-GFlowNets (our proposal): multi-objective active learning and Federated Bayesian Inference.\n\nIn multi-objective active learning, we want to obtain samples with high values across a pool of utility functions. For instance, in drug discovery, a key challenge is finding molecules that satisfy multiple constraints (e.g., affinity, solubility, safety). In a setting where different experts work independently to obtain property-specific models using GFlowNets, this challenge can be cast as sampling from the product of these local GFlowNets. This corresponds to the scalarization approach for multi-objective optimization, but we are interested in sampling to ensure diversity instead of maximizing --- which was also one of the primary motivations behind GFlowNets.\n\nFederated Bayesian Inference/Learning entails sampling from a posterior distribution $p(\\theta|\\mathcal{D})$ where $\\mathcal{D}$ is a dataset partitioned into a number of clients $k=1\\ldots K$, each with a subset $\\mathcal{D}_k$. The common setting is that the posterior factorizes as\n\n$p(\\theta|\\mathcal{D}) \\propto p(\\theta) \\prod_{k} p(\\mathcal{D}_k | \\theta)$, where $p(\\theta)$ is a prior and $p(\\mathcal{D}_k | \\theta)$ is the likelihood contribution of client $k$. Note that the likelihood is typically different for each $k$; more generally, they don't even need to share a functional form. There are a series of methods to solve this problem in a single communication round when $\\theta$ is a real-valued random variable. FC-GFlowNets (our proposal) solves this problem for the discrete case \u2014 and is the first work to do so.\n\nThanks to your comment, we have extended paragraph 3 (Introduction) in the revised manuscript. We also highlighted that our related work section discusses the Federated Bayes setting in detail (Appendix D).\n\n> **Q: The experimental setup regarding the clients is not sufficiently clear. Are all clients identical, or does this setup involve non-iid (non-independent and identically distributed) problems?**\n\n**A:** In our experiments, clients are never identical, i.e., there is no pair of local rewards $(R_i, R_j)$ that are equal. For instance, in the phylogeny experiments, each client's reward depends on different parts of the nucleotide sequences. In this case, no two clients have access to the same part of the sequence. For the grid world, each client has different reward beacons. Finally, the multiset and design-of-sequences experiment, clients value items differently. We have clarified that no clients have identical rewards in the experiment section.\n\n\n> **Q: It would enhance clarity if the authors could include a figure or pseudocode illustrating the algorithm.**\n\n**A:** Thanks for the suggestion. We have included a pseudo-code for FC-GFlowNets in the Appendix."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699985932298,
                "cdate": 1699985932298,
                "tmdate": 1699986120106,
                "mdate": 1699986120106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QpnpBP8goz",
                "forum": "VJDFhkwQg6",
                "replyto": "lO7gjblHyZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_wWFS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6008/Reviewer_wWFS"
                ],
                "content": {
                    "comment": {
                        "value": "My concerns have been well addressed by the response of the authors, I would raise my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6008/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645446847,
                "cdate": 1700645446847,
                "tmdate": 1700645446847,
                "mdate": 1700645446847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]