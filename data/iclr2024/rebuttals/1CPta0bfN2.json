[
    {
        "title": "Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders"
    },
    {
        "review": {
            "id": "Xz27KPogcN",
            "forum": "1CPta0bfN2",
            "replyto": "1CPta0bfN2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4117/Reviewer_hzee"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4117/Reviewer_hzee"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes AXN, a test-time multi-run query embedding adaptation approach that leverages KNN search to approximate cross-encoder scores. This method successfully reduces the expensive computational costs associated with cross-encoder calculations, surpassing the performance of DE-based and CUR-based alternatives."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents a novel adaptive retrieval technique that utilizes a limited number of cross-encoder (CE) calls to approximate the quality of cross-encoder results. This approach is scalable to handle a large volume of items."
                },
                "weaknesses": {
                    "value": "1)\tThis paper uses K nearest neighbor search to iteratively update query embedding and approximate cross-encoder results. However, many references of nearest neighbor search are missing.\n2)\tAXN utilizes sparse matrix to reduce index costs. The paper lacks detailed analysis regarding this technique's impact on results concerning varying degrees of sparsity.\n3)\tBoth AXN and CUR-based methods need to compute low-dimensional embeddings for queries and items. AXN uses sparse matrix to reduce the cost. This can also be applied to CUR-based methods to reduce the index time. It is unclear if other techniques of AXN generate substantial improvements over CUR-based methods. It would be great if the authors can add more ablation study experiments.\n4)\tThe paper only covers the total index time as a benchmark, and future exploration could include query latency measurements since various steps are executed many times, including Solve-Linear-Regression and topk search.\n5)\tIt introduces lambda to ensemble the generated query embedding with a query embedding from DE or inductive matrix factorization. However, it fails to conduct an analysis of lambda impact on evaluation. It is the same for all experiments, or should be tuned in each experiment?\n6)\tIt runs R times Solve-Linear-Regression and topk search. How to choose R? It is fixed in all experiments or should be tuned in each experiment? Should it be large in large dataset?\n7)\tThe same problem for hyper-parameter Ks."
                },
                "questions": {
                    "value": "1)\t\u201cCUR\u201d appears without any definition.\n2)\tWhat is the topk search method? Is it brute-force search?\n3)\tPlease address the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Reviewer_hzee",
                        "ICLR.cc/2024/Conference/Submission4117/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749802151,
            "cdate": 1698749802151,
            "tmdate": 1700709030863,
            "mdate": 1700709030863,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mXLMmntcjn",
                "forum": "1CPta0bfN2",
                "replyto": "Xz27KPogcN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hzee (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your time and valuable feedback. We hope that our response helps to address the points you\u2019ve raised. We would be happy to further discuss and clarify any remaining questions/concerns. \n\n\n> (1) This paper uses K nearest neighbor search to iteratively update query embedding and approximate cross-encoder results. However, many references of nearest neighbor search are missing\n\n   - In the related work section, we have over 25 citations outlining the relationship between our work and kNN search literature, especially in the **Nearest Neighbor Search** paragraph of related work. This paragraph in related work covers data structures for efficient indexing, speeding up similarity computation (e.g. using quantization), and the combination of graph-based/tree-based indexes and neural models.  \n   - Our work focuses on approximating an expensive black-box similarity function (eg. cross-encoders) using matrix factorization approaches to perform kNN search, and we discuss related work in the **Approximating Similarity Function** paragraph in the Related Work section. If you have further suggestions for citations for this paragraph, we greatly appreciate them.\n   - Compared to work on index structures for efficient kNN search (e.g.,CoverTree, HNSW, etc), we view our work as orthogonal (e.g., the vectors produced by our method could be indexed by any such vector-based kNN index). See second paragraph Sec 2.1 of paper for more discussion. \n   - If, given the above comments, the reviewer feels we have inadequately placed our work in the landscape of kNN search, please do let us know. Clearly presenting our work amidst that landscape is important to us. We welcome any further content or citation suggestions that the reviewer might have. Thank you for raising this point to us.\n\n\n> (2) AXN utilizes sparse matrix to reduce index costs. The paper lacks detailed analysis regarding this technique's impact on results concerning varying degrees of sparsity.\n    \n   - The ablation you reference is indeed already contained in our paper; it is present both in Table 3 and in Appendix B.4. These experiments vary the number of queries in the sparse matrix as well as number of items scored per query (i.e. sparsity) and measure the change in test-time Top-k-Recall and indexing latency. \n   - In Appendix B.4, we observe that increasing the number of items per query in the matrix (i.e. reducing sparsity), and increasing number of train queries generally leads to improvement in test-time Top-k-Recall although at increased indexing cost, as shown in Figure 10 (in updated pdf). This is an expected consequence; e.g., more observed data improves recall performance.\n   -  In Table 3, we show a detailed breakdown of indexing cost for different configurations of the sparse matrix. Here are some key observations:\n      -  Computing the cross-encoder (CE) scores for query-item pairs in the sparse matrix takes a significant fraction of overall indexing time.\n      -  The time taken to factorize the sparse matrix is proportional to the number of query-item pairs observed in the sparse matrix. However, it remains a small fraction of the overall indexing cost. \n\n> (3) \u2026  AXN and CUR-based methods need to compute low-dimensional embeddings for queries and items. AXN uses sparse matrix to reduce the cost. This can also be applied to CUR-based methods to reduce the index time. \u2026\n\n   - The most significant component of indexing cost is computation of cross-encoder scores between train/anchor queries and items.\n   - To the best of our understanding, the item indexing step for CUR-based methods _must_ compute a _dense matrix_ by scoring each item against _all_ of the anchor queries while we propose use of sparse matrices to significantly bring down the number of query-item scores computed using the cross-encoder. \n   - The indexing cost for AdaCUR can be controlled by the varying number of anchor/train queries, and in Fig 1, we report AdaCUR results for varying number of anchor/train queries.\n   - As shown in Fig 1, at a given indexing cost, our sparse matrix factorization based approaches yield better test-time Top-k-Recall values than AdaCUR.\n   -  Were there other ablations that the reviewer had in mind that might help improve our understanding of where our proposed sparse matrix based approaches outperforms CUR-based methods such as AdaCUR?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069749680,
                "cdate": 1700069749680,
                "tmdate": 1700069749680,
                "mdate": 1700069749680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KXvVpCSOZj",
            "forum": "1CPta0bfN2",
            "replyto": "1CPta0bfN2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4117/Reviewer_ujET"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4117/Reviewer_ujET"
            ],
            "content": {
                "summary": {
                    "value": "Cross-encoder (CE) models outperform Dual-encoder (DE) models (especially at zero-shot problems) in the ranking task but are very expensive to use during inference. To alleviate this usually a retrieve then re-rank approach is used where a set of items are first retrieved using a DE model and then further ranked by CE. This paper proposes an alternate approach where the CE model is first distilled into a lightweight factorized model and at test time query representation is iteratively fine-tuned such that the dot product between test query embedding and indexed item embeddings gets closer and closer to the CE assigned relevance. This approach helps in reducing the CE calls required to accurately rank items for the test query."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The approach is simple to plug into existing retrieval and ranking frameworks\n- The paper is in general well-written and easy to follow\n- The proposed approach is compared against relevant baselines and the evaluation is thorough"
                },
                "weaknesses": {
                    "value": "- The proposed approach is evaluated only on zero-shot tasks, does this approach also benefits standard retrieval tasks\n- Gains are primarily under fixed index time scenario which is usually a one-time cost"
                },
                "questions": {
                    "value": "- It is a bit surprising that RnR DE models are performing worse than TF-IDF on Hotpot, is it because this CE model was trained on triplets mined using TF-IDF?\n- CE model is trained conditioned on some specific negativing mining distribution so maybe for RnR baselines we should also compare with a retrieval model which is the same as the retrieval model used for the negative mining so that the train and test-time behaviours are same\n- For a given budget $X$ of CE calls, how should one distribute $X$ calls in the number of rounds and $K$ CE calls inside each round in AXN inference"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Reviewer_ujET"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801671279,
            "cdate": 1698801671279,
            "tmdate": 1699636376647,
            "mdate": 1699636376647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l99a8VYqTP",
                "forum": "1CPta0bfN2",
                "replyto": "KXvVpCSOZj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ujET"
                    },
                    "comment": {
                        "value": "Thank you for your time and valuable feedback. We have attempted to address the concerns raised by the reviewer and we would be happy to further discuss and clarify any remaining questions/concerns. \n\n> Gains are primarily under fixed index time scenario which is usually a one-time cost\n\n- While indexing is a one-time cost, it is a significant one. In particular, it is crucial to scaling to large numbers of items in terms of time and compute resources. \n   - Existing approaches such as  CUR-based methods do not scale well to millions of items and can require 1000s of GPU hours, \nand training parametric dual-encoder models via distillation also requires significant amount of time and multiple GPUs with \nlarge amounts of memory. \n   - In contrast, our proposed matrix factorization based approaches incur up to 100x and 5x less indexing time as compared to AdaCUR or dual-encoder distillation-based approaches respectively. \n\n- In addition to proposing time- and resource-efficient indexing methods, we propose an adaptive retrieval method (AXN) \nthat yields significant improvement over existing dual-encoder (DE) based retrieve-and-rerank pipelines, \n- Moreover, AXN can be used in combination with _any_ existing DE model to improve test-time performance _without incurring any \nadditional indexing cost_. \n\n> The proposed approach is evaluated only on zero-shot tasks, does this approach also benefits standard retrieval tasks\n\n- We focus on zero-shot tasks as poor performance of dense-embedding models in zero-shot setting motivates the \nuse expensive cross-encoder models. \n- As shown in Table 2 in Thakur et al. (2021), there is a substantial gap of about 15 points between zero-shot performance of dual-encoder and cross-encoder models. \n- As reported in Table 2 in Thakur et al. (2021), using cross-encoder models yield marginal (about 1%) improvements over dual-encoder/dense\nembedding models on _in-domain_ datasets such as MS-MARCO, and the marginal gains\nmay not justify using cross-encoders for retrieval in such _in-domain_ settings.\n\n\n### Response to Questions\n> It is a bit surprising that RnR DE models are performing worse than TF-IDF on Hotpot, is it because this CE model was trained on triplets mined using TF-IDF?\n   - As shown and reported in prior work (Thakur et al., 2021), TF-IDF is a strong baseline for these _zero-shot_ information retrieval tasks \n   and dual-encoders typically perform worse than TF-IDF by 3-4% on average (see Table 2 in Thakur et al. (2021)). \n   - While TF-IDF performing better than dual-encoders on downstream task metric is a result of the nature of the task and data, we  hypothesize that TF-IDF also performing better at kNN search with cross-encoder indicates that the cross-encoder is aligned with the TF-IDF similarity measure to some extent. \n       - We hypothesize that this alignment could be a result of  both the training strategy used for training the cross-encoder model as well as the nature of the task and data.\n  \n> CE model is trained conditioned on some specific negativing mining distribution so maybe for RnR baselines we should also compare with a retrieval model which is the same as the retrieval model used for the negative mining so that the train and test-time behaviours are same\n\n   - We agree that the training strategy used for training the cross-encoder can play a critical role in the performance of various baselines. \n   - A detailed study of how the cross-encoder training strategy affects performance of various retrieval methods is an interesting research question but beyond the scope of this paper. \n   - For ZeShEL datasets, the CE model is trained using negatives retrieved using a separately trained dual-encoder (DE), and this is the same DE model that is used in the DE-based retrieve-and-rerank baseline reported in the paper. We used the CE model released by Yadav et al. (2022) in our experiments.\n   - For BeIR,  the baseline DE model is trained on the same set of labeled triplets as the cross-encoder model.\n\n\n> For a given budget of X CE calls, how should one distribute X calls in the number of rounds and K CE calls inside each round in AXN inference\n\n**Distributing CE call budget over multiple rounds**\n   - In our experiments, we distributed the CE call budget uniformly over each round. \n   - We observed that for a given CE call budget, increasing the number of rounds leads to improvements in Top-k-Recall \n   metrics (as reported in Fig 6 in updated pdf) at the expense of increased overhead (as reported in Figure 5) \n   with the Top-k-Recall saturating around 5-10 rounds. \n   - We use 10 rounds for BeIR domains and 5 rounds for ZeShEL domains.\n\n### References:\n- Thakur, Nandan, et al. \"BeIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models.\" NeurIPS 2021 Dataset and Benchmark Track. \n- Yadav, Nishant, et al. \"Efficient nearest neighbor search for cross-encoder models using matrix factorization.\" EMNLP 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069618516,
                "cdate": 1700069618516,
                "tmdate": 1700069618516,
                "mdate": 1700069618516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KGvobWrAIU",
                "forum": "1CPta0bfN2",
                "replyto": "l99a8VYqTP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4117/Reviewer_ujET"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4117/Reviewer_ujET"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response, it added to my understanding of the paper, I would like to maintain my original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719728844,
                "cdate": 1700719728844,
                "tmdate": 1700719728844,
                "mdate": 1700719728844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PZAGEhJkhX",
            "forum": "1CPta0bfN2",
            "replyto": "1CPta0bfN2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4117/Reviewer_ja98"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4117/Reviewer_ja98"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a sparse-matrix factorization-based approach to improve the efficiency of fitting an embedding space to approximate the cross-encoder for k-NN search. Unlike DE-based and CUR-based methods, which lack good generalizations and computation efficiency, the new AXN method constructs a sparse matrix containing a cross-encoder score of training queries and all items.  The item embeddings are learned from matrix factorization. During test time, AXN alternates between updating the query embedding and retrieving more items for k-NN indexing."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors proposed AXN, a novel cross-encoder-based k-NN search algorithm. By learning item embeddings from sparse matrix factorization and fixing them during query time, the algorithm is more computationally efficient than other methods.\n2. The authors explained their method very clearly in section 2.\n3. The extensive experiments and ablation studies supported their claims."
                },
                "weaknesses": {
                    "value": "1. Figure 1's legends and corresponding subplots are hard to read. The subplots are too small, and hard to map points to the legends."
                },
                "questions": {
                    "value": "1. In the experiment section it was not clear how many rounds of updates are performed in all AXN experiments. \n2. Consider fixing max of CE calls B_{CE}, but varying the number of iterative search rounds and the number of items to retrieve in each round. Will it affect AXN's performance and total indexing time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Reviewer_ja98"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816437519,
            "cdate": 1698816437519,
            "tmdate": 1699636376558,
            "mdate": 1699636376558,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3v8UBw1q4f",
                "forum": "1CPta0bfN2",
                "replyto": "PZAGEhJkhX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ja98"
                    },
                    "comment": {
                        "value": "Thank you for your time, thoughtful review, and support of our paper. \n \n> Figure 1's legends and corresponding subplots are hard to read. The subplots are too small, and hard to map points to the legends.\n\nWe have included a larger version of the figures in the Appendix (Figures 9 through 12 in the original submission and Figures 11 through 16 in the updated pdf) as we were able to enlarge the figure 1 in the main paper marginally given the current page limit constraints.\n\n### Response to Questions:\n>In the experiment section it was not clear how many rounds of updates are performed in all AXN experiments.\n - As mentioned in Appendix A.4, we use 10 rounds for BeIR datasets and 5 rounds for ZeShEL datasets, unless specified otherwise.\n\n> Consider fixing max of CE calls B_{CE}, but varying the number of iterative search rounds and the number of items to retrieve in each round. Will it affect AXN's performance and total indexing time?\n   - We added additional results in Figure 6 which show that, for a fixed CE call budget, performance of AXN improves as we increase the number of iterative search rounds, with the performance saturating around 5-10 rounds. We divide the CE call budget uniformly over each round.\n  - Figure 5 shows the trend for how the overhead for AXN varies under different test-time cost budgets ( $B_{CE}$ ), and we observe that AXN incur 2 to 5% overhead when using 5-10 rounds for inference.\n  -  The offline indexing time is independent of maximum test-time CE call budget, and depends on the number of query-item pairs scored using the cross-encoder in the sparse matrix as well as time taken to factorize the matrix during the offline indexing stage."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069516471,
                "cdate": 1700069516471,
                "tmdate": 1700069516471,
                "mdate": 1700069516471,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6NDRBrMXrt",
            "forum": "1CPta0bfN2",
            "replyto": "1CPta0bfN2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4117/Reviewer_fa41"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4117/Reviewer_fa41"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a new approach to retrieval and indexing that attempts to match the accuracy of cross encoders with lesser training time. Cross-encoders based retrieval allows for computing a relevance function over query and each point in the retrieval corpus and finding the point(s) most relevant. As this are expensive for inference, in practice, dual encoders with separate encoding stacks for query and corpus are used with k-NN used to quickly find the most relevant documents. A compromise on cross-encoder is a CUR decomposition of the relevance signal. This paper proposed a method (AXN) that improves upon recall of dual encoder methods and is much faster than CUR based approaches.\n\nAXN starts with a dual encoder (treated as black box) and iteratively mines for items near a query using the query's representation. Then it uses a cross encoder to score the limited set of items retrieved. It uses this to refine the query representation, search for another limited set of items and so on. The method limits the cross attention to far fewer items than the corpus size. Experiments show that AXN improves the recall of dual encoder and can match the recall of CUR methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. AXN yields better recall than dual encoders with faster training times compared to CUR methods\n2. They can build on any dual encoder methods (although experiments don't say if they improve on DE encoders)"
                },
                "weaknesses": {
                    "value": "1. The inference time and its comparison to simple DE+kNN approaches is not made clear. Would a large dual encoder with same inference time as AXN match its recall? Some of this is tucked away in the appendix in Fig 5 and 6 which seems to suggest the margins between DE and AXN are low normalized for inference cost.\n2. The choice of DE is not discussed. Is it completely irrelevant? Is it being compared to state-of-the-art encoders?"
                },
                "questions": {
                    "value": "1. would a large dual encoder with same inference time as AXN match it's recall? Or would DE saturate well before AXN.\n2. What is the unit on x-axis in Figure 6?\n3. Could you talk about the scaling properties of your algorithm as the corpus grows larger? \n4. Why are not all methods not represented on each sub-figure in Figure 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4117/Reviewer_fa41"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832442422,
            "cdate": 1698832442422,
            "tmdate": 1699636376499,
            "mdate": 1699636376499,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4ttscVgyAV",
                "forum": "1CPta0bfN2",
                "replyto": "6NDRBrMXrt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fa41 (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your time and valuable feedback. We have attempted to address the concerns raised by the reviewer and we would be happy to further discuss and clarify any remaining questions/concerns. \n\n### Inference Time for AXN vs DE-based retrieve-and-rerank\n\n> The inference time and its comparison to simple DE+kNN approaches is not made clear. Would a large dual encoder with the same inference time as AXN match its recall? Some of this is tucked away in the appendix in Fig 5 and 6 which seems to suggest the margins between DE and AXN are low normalized for inference cost.\n\n- We present a direct comparison between AXN and DE-based retrieve-and-rerank at varying inference cost budgets in Figure 6 (Fig 7 in updated pdf) where AXN consistently outperforms DE-based retrieve-and-rerank baseline.\n- We measure inference cost by the number of cross-encoder (CE) calls made at test-time in Figure 6 (Fig 7 in updated pdf) as inference latency for AXN (with 5 to 10 rounds) and DE-based retrieve-and-rerank approaches is similar at a given CE call budget.\n    -   The reason behind similar inference latency for AXN (with 5 to 10 rounds) and DE-based retrieve-and-rerank is that the inference latency overhead incurred by AXN is less than 2 to 5% of overall time when using 5-10 rounds for inference and the majority of time is spent in computing cross-encoder scores for the retrieved items, as shown in Figure 5.\n- Using a larger dual-encoder would not increase the inference time significantly as the main test-time bottleneck is computing CE scores for retrieved items. \n    - We add additional results for a larger dual-encoder (Fig 27 and 28 in updated pdf) where we observe AXN continues to outperform DE-based retrieve-and-rerank baseline. See second bullet in **Choice of Dual-Encoder** paragraph for details.   \n \n\n### Choice of Dual-Encoder\n\n> The choice of DE is not discussed. Is it completely irrelevant? Is it being compared to state-of-the-art encoders?\n> would a large dual encoder with same inference time as AXN match it's recall? Or would DE saturate well before AXN.\n\nWe run experiments with different dual-encoder baselines and justify the choices below\n- For ZeShEL dataset, the dual-encoder model used is representative of the state-of-the-art model. \n- For BeIR benchmark, we used a [dual-encoder model](https://huggingface.co/sentence-transformers/msmarco-distilbert-base-v2) released publicly that is representative of the state-of-the-art model under the setting which _does not_ involve any distillation-based training on MS-MARCO dataset. \n  - We refer the reviewer to Appendix A.2.1 for more details of the dual-encoder models used in this paper. \n\n- We report additional results in Figures 27 and 28 (in updated pdf) for another larger [dual-encoder model](https://huggingface.co/sentence-transformers/msmarco-bert-base-dot-v5) which is representative of the state-of-the-art on MS-MARCO \ndataset (see [here](https://www.sbert.net/docs/pretrained_models.html#msmarco-passage-models)). \n  - This is bert-base model finetuned on MS-MARCO using ground-truth data as well as further trained through several rounds of distillation using a cross-encoder (different from the ones used in this paper). \n  - We observe similar trends such as \n    - Adaptive retrieval (AXN) outperforms retrieve-and-rerank style inference, and \n    - Sparse matrix factorization based approaches can offer improved performance as compared to the dual-encoder.\n\n- Note that we additionally also compare with $DE_{Dstl}$, dual-encoder models further finetuned on the _target_ domain via distillation using the _given cross-encoder model_. \n  - We find that such distillation based training is compute- and resource-intensive. \n  - In contrast, our proposed sparse matrix factorization based approaches can be up to 5x faster while matching or improving over the performance of distilled dual-encoder models.\n  - Also, as shown in Figure 1, using AXN with $DE_{Dstl}$, outperforms retrieve-and-rerank approach (RnR) with the same distilled dual-encoder  $DE_{Dstl}$ for k=100 and matches performance for k=1."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069392627,
                "cdate": 1700069392627,
                "tmdate": 1700069392627,
                "mdate": 1700069392627,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]