[
    {
        "title": "Provable Repair of Vision Transformers: Last Layer is All You Need"
    },
    {
        "review": {
            "id": "MRWu2ceUDp",
            "forum": "aEGUT3OGCW",
            "replyto": "aEGUT3OGCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8836/Reviewer_QPLU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8836/Reviewer_QPLU"
            ],
            "content": {
                "summary": {
                    "value": "This paper talks about on how to add robustness in VIT models, something which was guaranteed in VGG and ResNet. \nThey achieved this using a set patter of modifying last layers of VIT model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Great theoretical backing for their proof.\n* Mentions initial hypothesis what they are trying to achieve using this task. This is a great way to start any research problem."
                },
                "weaknesses": {
                    "value": "* I fail to see lot of practical sense from this paper.\n* Adding some pictorial references to the idea might be able to help readers to grasp the idea completely."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8836/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698419409426,
            "cdate": 1698419409426,
            "tmdate": 1699637111015,
            "mdate": 1699637111015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b5OpUTaXJ9",
                "forum": "aEGUT3OGCW",
                "replyto": "MRWu2ceUDp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8836/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8836/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks so much for taking the time to review our paper!\n\nWe have added some figures representing the types of ImageNet images we repair in our experimental evaluation.\n\nIn terms of clarifying the practical sense of the paper, we believe that provable repair of Vision Transformers is incredibly important for scenarios in which Vision Transformers are used in safety-critical domains such as self-driving automobiles and medical diagnostics. We have cited additional references in the introduction that justify the necessity for provable repair of Vision Transformers.\n\nThank you for the suggestions!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8836/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268167424,
                "cdate": 1700268167424,
                "tmdate": 1700268167424,
                "mdate": 1700268167424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w2TBJ6xpho",
            "forum": "aEGUT3OGCW",
            "replyto": "aEGUT3OGCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8836/Reviewer_ub3F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8836/Reviewer_ub3F"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for provable repair the correction of classification outputs using Vision Transformers on a specified set of images with certain guarantees and with limited degradation of performance (drawdown) on the initial training set. The proposed provable repair method claims that suitable modification of the final fully-connected layer of a vision transformer is sufficient to achieve these goals. Specifically, the paper builds on provable repair methods proposed for DNNs, like MMDNN and APRNN, that, while they cannot be fully implemented in the case of vision transformers (ViTs) due to the presence of self-similarity modules, they can be applied to the last fully-connected layer of ViTs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written and easy to read. The main ideas behind the proposed method are clearly exposed and their relation to previous work is presented. Based on concepts introduced in the MMDNN and ARPNN works, a modification of the last layers weights is performed by defining a linear programming problem for satisfying the required constrains. To improve the efficiency of the method, the problem is limited only to the class labels contained in the given repair set. Additionally, fine-tuning of the final layer is also applied in an alternating way, to further improve generalization and limit drawdown."
                },
                "weaknesses": {
                    "value": "The contribution is somewhat limited, as the main idea is based on similar approaches applied for DNNs, such as MMDNN (Goldberger et al., 2020) and APRNN (Tao et al., 2023). Nevertheless, these methods have been suitably modified for their application in the context of ViTs. \n\nSome aspects of the evaluation could be improved. For example, it is mentioned that standard LP-based repair of the last layer could not be considered due to scalability issues. While this is a major limitation of this baseline, it would be interesting to provide some results on a dataset with a reduced number of categories that would make this comparison possible. Also, another aspect that would be interesting to consider in the ablative study is the performance of the proposed method for large values of K. Is there a maximum K value in practice and, if yes, how can this limit be estimated in practice?"
                },
                "questions": {
                    "value": "As stated above, is there a maximum K value inpractice? If yes, is it easy to estimate it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8836/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699274282142,
            "cdate": 1699274282142,
            "tmdate": 1699637110898,
            "mdate": 1699637110898,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sQeQROh2vz",
                "forum": "aEGUT3OGCW",
                "replyto": "w2TBJ6xpho",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8836/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8836/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks so much for taking the time to review our paper!\n\nWe are running an experiment with a reduced number of ImageNet categories so that we can compare PRoViT against APRNN (Tao et al., 2023). We will include the results of this experiment in the Appendix.\n\nWe will also include an experiment that tests the limits of the maximum K value and determine whether this value is easy to estimate in practice. Thank you for the suggestions on additional experiments to include!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8836/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268105225,
                "cdate": 1700268105225,
                "tmdate": 1700268105225,
                "mdate": 1700268105225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mZENuGSKCZ",
                "forum": "aEGUT3OGCW",
                "replyto": "sQeQROh2vz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8836/Reviewer_ub3F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8836/Reviewer_ub3F"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their feedback. I think covering these topics will make their work stronger."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8836/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662805797,
                "cdate": 1700662805797,
                "tmdate": 1700662805797,
                "mdate": 1700662805797,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WS4tTkq9M9",
            "forum": "aEGUT3OGCW",
            "replyto": "aEGUT3OGCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8836/Reviewer_CCeY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8836/Reviewer_CCeY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for provable repair of neural networks by modifying only the weights of the last linear layers so as to maximise the classification accuracy on the repair set. This problem can be cast as a linear program (LP), as previously done in the literature. The novelty of this work that sparsity is taken into account when solving this LP."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method can exploit the sparsity in the LP (given that the number of labels in the repair set remains small). The results on ViT are quite good."
                },
                "weaknesses": {
                    "value": "1. The authors claim that their method has \"provable correctness guarantees\" (all images in the repair set will be classified correctly post-repair), which is of course not true because the final accuracy deepens on the repair set and the capacity of the last layer. (Take the ImageNet training set as the repair set for example, can the method guarantee a 100% accuracy? Obviously not.) This incorrect claim seems to come from a misconception regarding the algorithm, as discussed below in the next point.\n\n2. There's a fundamental algorithmic issue. \"Since FTall only terminates once all inputs are classified correctly, it is a provable repair approach.\" (at the end of section 2), and \"If there is no solution to the LP, the loop continues. Otherwise, the repaired Vision Transformer is returned.\" (at the end of page 5): the authors claim that their algorithm terminates when all the images are correctly classified. The question is: Is it guarantee to terminate?\n\n3. Some other claims are also questionable. For example, \"PRoViT scales to thousands of images\": this depends on the number of classes present in the repair set. For example, if there's only a single class, then this claim becomes uninteresting. \n\n4. Are you sure that the optimization problems in Equations (2) and (3) are linear programs? In addition, please avoid using \"Theorem\" for everything. For example, Theorems 2 and 4 should be stated as remarks, in my opinion. (And perhaps the other ones should be labeled with Proposition instead of Theorem.)\n\n5. Most importantly, the proposed method has nothing to do with vision transformers. It is largely based on existing works, the optimization problem is the same, only the resolution has been improved by taking into account sparsity. And it also works for any models other than ViT (as long as they has a linear layer at the end, which is the case for all common architectures). It happens to work well for ViTs, but there was absolutely no explanation for this phenomenon, and ViTs were also not the motivation for the design of the method, so the title and the framing of this paper seem to use ViTs only to attract attention."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8836/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8836/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8836/Reviewer_CCeY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8836/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699662423662,
            "cdate": 1699662423662,
            "tmdate": 1699662423662,
            "mdate": 1699662423662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5oICH1wl7D",
                "forum": "aEGUT3OGCW",
                "replyto": "WS4tTkq9M9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8836/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8836/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for taking the time to review our paper! We will address your comments in the following ways:\n\n1. In our presentation of the LPs in Sections 2 and 3, we originally noted that an LP solver can find a solution \u201cif it exists.\u201d However, we also included such language in the introduction to be clear about the soundness vs. completeness, in line with prior provable repair approaches. Our approach is sound; that is, if the approach says that it has repaired the network, then the resulting network is guaranteed to satisfy the repair specification. Similar to all prior provable repair approaches, our approach is not complete; that is, it may not be able to find a network that satisfies the given repair specification. The reason for such incompleteness could either be because such a network does not exist due to, for instance, a contradictory repair specification or because the approach is restricted to modify only certain parameters of the network. (Note that Section 5 of APRNN (Tao et al., 2023) shows that the algorithm of REASSURE (Fu and Li, 2022) is not sound or complete.)\n2. The issue of termination is merely related to how one expresses an approach that is sound but not complete. One alternative to describing such an approach is for the algorithm to always terminate and return either a solution or $\\bot$ (representing \u201cI could not find a solution\u201d). In this case, one would prove that if the return value is not $\\bot$, then the answer is correct (sound). A second alternative is to only terminate when a solution has been found. In this second case, one would prove that any value returned by the algorithm is correct. However, the algorithm might not terminate.  We just happened to pick this latter approach for describing $FT_{all}$ in Section 2 and $PRoViT_{LP+FT}$ in Algorithm 1.  \n&nbsp;&nbsp;&nbsp;&nbsp;In practice, we use a timeout to terminate the algorithm (both $FT_{all}$ and $PRoViT_{LP+FT}$), so in practice we have implemented the first alternative described above. Furthermore, in our experiments $FT_{all}$ timed out while PRoViT always terminated within the timeout limit.  \n&nbsp;&nbsp;&nbsp;&nbsp;We have also clarified the presentation of $FT_{all}$ at the end of Section 2. $FT_{all}$ is a baseline, not part of PRoViT. $FT_{all}$ is just regular fine tuning across the entire Vision Transformer with the termination condition of achieving 100% accuracy on the repair set.\n\n3. None of the prior provable repair approaches are able to repair thousands of images for ImageNet Vision Transformers with low drawdown and high generalization, even when restricted to just one class. APRNN runs out of memory, fine tuning times out or causes high drawdown, etc. Hence, we believe that the claim that PRoViT can repair thousands of images is interesting. For example, it would be incredibly useful to repair thousands of instances of a single class such as a \u201cstop sign\u201d for an autonomous vehicle.\n4. Yes, we are certain that Equations 2 and 3 are LPs. Is there notation that we have used that is confusing in our LP formulation? Please let us know so we can rectify that. We have changed the Theorems to Remarks and Propositions.\n5. This work was originally inspired by trying to find a provable repair method for Vision Transformers, because one does not already exist. We were pleasantly surprised to find that by only focusing on repairing the last layer, we were able to find a simple, efficient, and effective way to repair Vision Transformers. The novelty of this work is not only taking sparsity into account in the LP formulation of the repair, but also that the combination of fine tuning and LP-based repair of the last layer strikes a balance between drawdown and generalization. Our experiments on ResNet and VGG (Section 4) show that focusing on the last layer does not work as well as it does on Vision Transformers, so we still believe that this approach is ViT-specific. Our hypothesis for this phenomenon is that because the last layer of the Vision Transformer is an output vector containing the \u201cclass token\u201d from the final encoder layer, the class information encoded in that vector is more robust than the information that is just the result of convolutions. However, further theoretical justification for this hypothesis is left to future work. We have added this note regarding future work in Section 4.\n\nThank you for the suggestions!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8836/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268005064,
                "cdate": 1700268005064,
                "tmdate": 1700268005064,
                "mdate": 1700268005064,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]