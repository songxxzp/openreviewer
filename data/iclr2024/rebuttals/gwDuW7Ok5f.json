[
    {
        "title": "Dual Associated Encoder for Face Restoration"
    },
    {
        "review": {
            "id": "i3mh9rqDnw",
            "forum": "gwDuW7Ok5f",
            "replyto": "gwDuW7Ok5f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3133/Reviewer_X7AW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3133/Reviewer_X7AW"
            ],
            "content": {
                "summary": {
                    "value": "To address the domain gap between low-quality and high-quality images and improve the performance of face restoration, the paper introduces a novel framework called DAEFR. This framework incorporates LQ (low-quality) image domain information by introducing an auxiliary branch that extracts unique LQ domain-specific features to complement the HQ (high-quality) domain information. To further align the features between the HQ and LQ domains, the paper employs a CLIP-like constraint to enhance the correlation between the two domains. Additionally, to facilitate better feature fusion between these two domains, the framework introduces a multihead cross-attention module. Evaluation results demonstrate the effectiveness of DAEFR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper proposes a framework designed to incorporate distinctive features from low-quality (LQ) images, thereby enhancing the face restoration task.\n2.\tTo mitigate the domain gap between HQ and LQ images, the paper proposes an association strategy during training, and incorporates a multihead cross-attention module for better feature fusion between these two domains.\n3.\tThe experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed framework."
                },
                "weaknesses": {
                    "value": "1.\tThe full name of the proposed framework, DAEFR, is missing. It should be mentioned on its first occurrence in the paper.\n2.\tThe proposed method requires training two sets of encoder and decoder for both HQ and LQ images. This will double the training resource requirements.\n3.\tI think it will be better if there is more elaboration on the domain gap issue that the current works exist, i.e., the motivation of the paper. Currently, it is not intuitive from figure 1 and from current discussion.\n4.\tCheck the spellings. For example, \u201crecently\u201d on the beginning of second paragraph in the \u201cVector Quantized Codebook Prior\u201d of the related work.\n5.\tThere are some confusions about the training process of the network. In the first stage (section 3.1), you firstly train the two autoencoders of LQ and HQ using the codebook loss. After the first-stage training is complete, you train the two encoders using both the codebook loss and the association loss. Why not combine the two stages into one, or just apply the association loss in stage 2? Besides, in stage 3, you state in the Training Objectives that the MHCA and transformer module are trained in this stage. However, from figure 2(c), the two encoders seem not to be frozen during stage 3.\n6.\tThe results in Table 1 indicate that the proposed method does not significantly outperform other methods, especially for the synthetic CelebA-Test dataset."
                },
                "questions": {
                    "value": "Refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3133/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719560760,
            "cdate": 1698719560760,
            "tmdate": 1699636260210,
            "mdate": 1699636260210,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8DPDceYGNR",
                "forum": "gwDuW7Ok5f",
                "replyto": "i3mh9rqDnw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3133/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3133/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We extend our gratitude for the positive and constructive feedback.\n\nWe would like to address the raised concerns as follows:\n\n---\n**[Q1] The full name of the proposed framework, DAEFR, is missing. It should be mentioned on its first occurrence in the paper.**\n\nThe acronym 'DAEFR' stands for the title of our paper, which is an initialism of \"**D**ual **A**ssociated **E**ncoder for **F**ace **R**estoration.\"\nWe will revise these parts in our updated manuscript.\n\n---\n**[Q2] The proposed method requires training two sets of encoder and decoder for both HQ and LQ images. This will double the training resource requirements.**\n\nIn our implementation, obtaining the codebooks, which demand the most training resources, involves training the HQ path for 300 epochs and the LQ path for 200 epochs. In comparison, CodeFormer requires 700 epochs to create its HQ codebook. Despite this, our use of training resources remains less intensive than that of CodeFormer.\n\n---\n**[Q3] I think it will be better if there is more elaboration on the domain gap issue that the current works exist, i.e., the motivation of the paper. Currently, it is not intuitive from Figure 1 and the current discussion.**\n\n* We thoroughly analyze the domain gap in three distinct pathways: HQ reconstruction path, LQ reconstruction path, and Hybrid path. All three ways involve the utilization of LQ images as input. As Figure 1 shows in our supplementary material, both the HQ and Hybrid paths exhibit limitations in effectively reconstructing or restoring the LQ image due to the domain gap. Notably, the Hybrid path fails to generate any facial features.\n* In these experiments, we observe that the LQ path better reconstructs the identity information from the LQ images, and the domain gap actually exists. Based on these observations, we design the additional LQ path to better encode the domain-specific information and introduction association stage and feature fusion to solve the domain gap issue.\n\n---\n**[Q4] Check the spelling. For example, \"recently\" at the beginning of the second paragraph in the \"Vector Quantized Codebook Prior\" of the related work.**\n\nWe will revise these parts in our updated manuscript.\n\n---\n**[Q5] Why not combine the two stages or apply the association loss in stage 2? Besides, in stage 3, you state in the Training Objectives that the MHCA and transformer module are trained in this stage. However, from Figure 2(c), the two encoders seem not to be frozen during stage 3.**\n\n* We would like to clarify the role of each stage. In the codebook learning stage, we aim to obtain a pure HQ decoder from the HQ path for use in the final stage. If we are to simply combine the codebook learning stage with the association stage, there is a risk that the HQ decoder might interfere with the LQ path, leading to a decrease in image quality.\n* We also conduct experiments where we apply only the association loss in stage 2. This resulted in less constraint on both paths, potentially causing a misalignment between the HQ and LQ paths. The detailed results are presented in the supplementary material.\n* To address your question, we do not freeze the two encoders in stage 3. We believe that fine-tuning these encoders can accelerate the convergence and enhance feature fusion and code prediction in our restoration process.\n\n---\n**[Q6] The results in Table 1 indicate that the proposed method does not significantly outperform other methods, especially for the synthetic CelebA-Test dataset.**\n\nIn Table 1 of the manuscript, our approach outperforms the state-of-the-art methods, especially when dealing with **real-world datasets**. When it comes to the synthetic CelebA-test dataset, we achieve the **best** performance when we combine the final feature with the LQ features before passing them to the HQ decoder, as shown in the following table. These results highlight the strength and reliability of our approach.\n\n| CelebA-Test       | LPIPS &darr; | PSNR   &uarr; | SSIM &uarr; |\n| ----------------- | ------------ | ------------- | ----------- |\n| RestoreFormer [1] | 0.467        | 20.146        | 0.494       |\n| CodeFormer [2]    | 0.365        | 21.449        | 0.575       |\n| VQFR [3]          | 0.456        | 19.484        | 0.472       |\n| DR2 [4]           | 0.409        | 20.327        | 0.595       |\n| DAEFR (Ours)      | **0.363**    | **21.488**    | **0.616**   |\n\n[1] RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs. CVPR 2022.\n\n[2] Towards Robust Blind Face Restoration with Codebook Lookup Transformer. NeurIPS 2022.\n\n[3] VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder. ECCV 2022.\n\n[4] DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration. CVPR 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3133/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512505938,
                "cdate": 1700512505938,
                "tmdate": 1700617223906,
                "mdate": 1700617223906,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2RDZ0toPqb",
            "forum": "gwDuW7Ok5f",
            "replyto": "gwDuW7Ok5f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3133/Reviewer_cKwn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3133/Reviewer_cKwn"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a framework, named dual associated encoder for face restoration (DAEFR), for face restoration. Specifically, different from the existing codebook based methods using only one autoencoder for high-resolution images, the authors propose to add another stream for low-resolution images. To fuse and align the features from both low and high resolution images, an association stage is designed. The associated features then will be extracted and utilized for face restoration.\n\nExperimental results have demonstrated the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written.\n2. The idea is well presented, explained, and demonstrated.\n3. The proposed method may inspire the researchers in this area."
                },
                "weaknesses": {
                    "value": "1. The contribution looks marginal to me since all the methods used in different stage are well designed and demonstrated. Adding another stream for low-resolution might not be a major contribution for a top-tier venue like ICLR.\n2. I got some questions for the experimental results which can be seen in the questions part."
                },
                "questions": {
                    "value": "In Table 2, it seems like all the alter methods outperform the proposed method in terms of LPIPS. Please give discussions or visualizations to explain why this happens."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3133/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877241147,
            "cdate": 1698877241147,
            "tmdate": 1699636260118,
            "mdate": 1699636260118,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "30wRsn01Bi",
                "forum": "gwDuW7Ok5f",
                "replyto": "2RDZ0toPqb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3133/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3133/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We extend our gratitude for the positive and constructive feedback.\n\nWe would like to address the raised concerns as follows:\n\n---\n**[Q1] The contribution looks marginal to me since all the methods used in the different stages are well-designed and demonstrated. Adding another stream for low-resolution might not be a major contribution for a top-tier venue like ICLR.**\n\nWe would like to further clarify the novelty and effectiveness of this work by putting this work in proper context and emphasizing the empirical results over state-of-the-art approaches. \n\n* We would like to clarify that although some proposed model components are similar to prior arts, it requires **meticulous algorithmic design** and **new modules** to integrate them and achieve state-of-the-art results. As shown in Table 2 of the manuscript, the **straightforward combination** of HQ and LQ features **does not improve performance**. Thus, we introduce the **association stage** and the **feature fusion** modules to exploit the information from both domains effectively. The performance of the proposed model **significantly outperforms** state-of-the-art approaches, as shown in the following table.\n\n\n\n| Ablation Study           | LMD &darr; | NIQE   &darr; |\n| ------------------------ | ---------- | ------------- |\n| Naively combine features | 4.258      | 4.297         |\n| Association & Fusion     | **4.019**  | **3.815**     |\n\n\n\n* We propose an **additional LQ reconstruction path** to specifically encode the LQ domain information, which **does not exist in prior works**. Furthermore, we provide a **new perspective** to handle the LQ domain information better. We show the **effectiveness of our additional LQ path** with extensive experiments. How to better deal with the LQ input, the only source of information in this task, to assist the restoration process is crucial. Our contributions in **providing new encoding strategies and viewpoints to handle LQ information** are essential to advance the field.\n\n* We demonstrate the novel components of our work by comparing with RestoreFormer [1] and CodeFormer [2], whose network building blocks mostly come from VQGAN [3] and Transformer [4]. Although these models are mostly built upon prior works, they show great empirical results can be achieved by **novel and essential integration**. Such works are important and make significant contributions. We use these examples to emphasize that **sufficient novel components** are introduced in proposing an additional LQ encoder to specifically deal with the LQ information and demonstrate its **effectiveness** with extensive experiments. We hope reviewers can appreciate the contribution of this work from this perspective.\n\n* We show the ability of our method in real-world datasets with **significant improvements** over SOTA methods in both quantitative and qualitative evaluation. For the qualitative results, please refer to our manuscript and supplementary material. For the quantitative results, please refer to the following tables.\n\n| LFW-Test          | FID &darr; | NIQE   &darr; |\n| ----------------- | ---------- | ------------- |\n| RestoreFormer [1] | 48.412     | 4.168         |\n| CodeFormer [2]    | 52.350     | 4.482         |\n| VQFR [5]          | 50.712     | 3.589         |\n| DAEFR (Ours)      | **47.532** | **3.552**     |\n\n| WIDER-Test        | FID &darr; | NIQE   &darr; |\n| ----------------- | ---------- | ------------- |\n| RestoreFormer [1] | 49.839     | 3.894         |\n| CodeFormer [2]    | 38.798     | 4.164         |\n| VQFR [5]          | 44.158     | **3.054**     |\n| DAEFR (Ours)      | **36.720** | 3.655         |\n\n[1] RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs. CVPR 2022.\n\n[2] Towards Robust Blind Face Restoration with Codebook Lookup Transformer. NeurIPS 2022.\n\n[3] Taming Transformers for High-Resolution Image Synthesis. CVPR 2021.\n\n[4] Attention Is All You Need. NeurIPS 2017.\n\n[5] VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder. ECCV 2022.\n\n---\n**[Q2] In Table 2, it seems like all the altered methods outperform the proposed method in terms of LPIPS. Please give discussions or visualizations to explain why this happens.**\n\n\nThe LPIPS calculation involves using deep neural networks to analyze image features and then calculating a distance or similarity score based on these features. In our case, we input entire images, including background elements that are not our primary restoration focus. These background elements can affect the LPIPS score.\n\nIn this ablation study, our main goal is to demonstrate significant improvements in identity metrics. Additionally, we present qualitative results in Fig. 5 of the manuscript, showcasing the best visual outcomes achieved with our final setting."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3133/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512247404,
                "cdate": 1700512247404,
                "tmdate": 1700617300988,
                "mdate": 1700617300988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HeqKDCFvhT",
            "forum": "gwDuW7Ok5f",
            "replyto": "gwDuW7Ok5f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3133/Reviewer_qW23"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3133/Reviewer_qW23"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a dual-branch framework, named DAEFR, designed for the restoration of high-quality (HQ) facial details from low-quality (LQ) images. Within this framework, an auxiliary LQ encoder and an HQ encoder are employed in conjunction with feature association techniques to capture visual characteristics from LQ images. Subsequently, the features extracted from both encoders are combined to enhance their quality. Finally, the HQ decoder is utilized for the reconstruction of high-quality images. The effectiveness of DAEFR is evaluated using both real-world and synthetic datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The notion of incorporating an additional encoder with weight sharing is intriguing.\n\n2. The authors have extensively verified the significance of each component via thorough ablation studies.\n\n3. This approach adeptly addresses various common and severe degradations and maintains a high standard of writing quality."
                },
                "weaknesses": {
                    "value": "1. Can you provide a detailed explanation of the primary differentiation between DAEFR and CodeFormer?\n\n2. The paper does not delve into its limitations or potential factors for analysis, which would greatly enrich its discussion.\n\n3. The paper outperforms baseline methods in the downstream face recognition task. Could you provide a comprehensive explanation of these results?\n\n4. The paper does not provide any suggestions or insights into potential avenues for future research or improvements to the proposed method."
                },
                "questions": {
                    "value": "Please discuss the concerns in the Weaknesses Section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3133/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698958333067,
            "cdate": 1698958333067,
            "tmdate": 1699636260019,
            "mdate": 1699636260019,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EzF4Wo62XD",
                "forum": "gwDuW7Ok5f",
                "replyto": "HeqKDCFvhT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3133/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3133/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We extend our gratitude for the positive and constructive feedback.\n\nWe would like to address the raised concerns as follows:\n\n---\n**[Q1] Can you provide a detailed explanation of the primary differentiation between DAEFR and CodeFormer?**\n\n* Our method, DAEFR, introduces a crucial distinction by incorporating an additional encoder specifically designed for improved feature encoding in the LQ domain. This allows us to leverage essential information from the LQ images effectively. Moreover, DAEFR uses an association stage that effectively bridges the domain gap between the HQ and LQ domains.\n* In contrast, CodeFormer adopts a different approach by utilizing a pretrained HQ encoder, which is then fine-tuned on LQ images. However, the domain gap still exists despite this adaptation, which may present certain challenges.\n* By integrating the LQ encoder in DAEFR, we aim to encode LQ features better, thereby facilitating more accurate and refined results. This emphasis on enhancing the LQ encoding process demonstrates the critical distinction between DAEFR and CodeFormer, ultimately leading to improved performance and overcoming potential domain gap issues.\n\n---\n**[Q2] The paper does not delve into its limitations or potential factors for analysis, which would greatly enrich its discussion.**\n\nWe have provided visual results with large poses in Figure 20 of the supplementary material. While our method demonstrates robustness in most severe degradation scenarios, we also observe instances where it may fail, particularly in cases with large face poses. This can be expected as the FFHQ dataset contains few samples with large face poses, leading to a scarcity of relevant codebook features to effectively address such situations, resulting in less satisfactory restoration and reconstruction outcomes.\n\nWe include failure cases in the supplementary material to ensure a more comprehensive understanding of our method's limitations.\n\n---\n**[Q3] The paper outperforms baseline methods in the downstream face recognition task. Could you provide a comprehensive explanation of these results?**\n\nOur quantitative experiments demonstrate the strength of our method. The issue with identity evaluation on the synthetic CelebA-test dataset lies in the impracticality of achieving identical images in real-world scenarios. It only measures the likeness between the original image and the restored one. Real face recognition datasets comprise a vast number of images belonging to the same individual, presenting more significant challenges and closely mimicking real-world scenarios. These results confirm the effectiveness of our added LQ encoder, which extracts valuable details from the LQ domain, thereby significantly enhancing the restoration process.\n\n---\n**[Q4] The paper does not provide any suggestions or insights into potential avenues for future research or improvements to the proposed method.**\n\nWe would like to provide detailed suggestions and directions for future research:\n\n* Codebook of Local Facial Parts: As part of future work, we plan to integrate a codebook of local facial parts. This addition could enhance the accuracy of facial restoration, particularly for detailed regions such as the eyes, nose, and mouth, which are crucial for achieving realistic results.\n* Extension to Severe Degraded Video Input: To broaden the applicability of our method, extending it to handle video input is an intriguing direction for future exploration. However, this line of work should consider the consistency of restored results across different time frames. Degradation in videos may vary, and the model should generate coherent and consistent outputs throughout the entire video sequence."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3133/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512097778,
                "cdate": 1700512097778,
                "tmdate": 1700616736998,
                "mdate": 1700616736998,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yCTBYYuYoK",
            "forum": "gwDuW7Ok5f",
            "replyto": "gwDuW7Ok5f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3133/Reviewer_huYM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3133/Reviewer_huYM"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new approach called the Dual Associated Encoder for facial restoration. In this method, an auxiliary Low-Quality (LQ) branch is introduced to extract vital information from LQ inputs. Subsequently, it employs a structure similar to CLIP to establish connections between the LQ and High-Quality (HQ) encoders. This connection aims to reduce the domain gap and information loss when restoring HQ images from LQ inputs. The experimental outcomes illustrate the highly promising performance of this novel approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe paper offers a coherent and well-founded justification for the research, with a method design that closely aligns with the research objectives.\n2.\tThe paper effectively communicates the method, ensuring readers can easily comprehend the underlying concepts and techniques.\n3.\tThe experimental results showcase remarkable performance, affirming the method's efficacy in tackling the face restoration challenge."
                },
                "weaknesses": {
                    "value": "1.\tAbsence of Future Research Guidance: The paper does not offer any recommendations or insights into potential future research directions or enhancements for the proposed method.\n2.\tOmission of Limitation Discourse: The paper lacks a discussion regarding its limitations and possible factors for analysis."
                },
                "questions": {
                    "value": "1.\tWhile the paper predominantly highlights the advantages of the proposed method, could you offer instances where the method encountered shortcomings or limitations?\n2.\tCould you elaborate on the key distinction between DAEFR and CodeFormer?\n3.\tCan you provide further experimental details into the \"Effectiveness of Low-Quality Feature from Auxiliary Branch\" as examined in your ablation studies?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Please see my comments above."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3133/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698968448122,
            "cdate": 1698968448122,
            "tmdate": 1699636259947,
            "mdate": 1699636259947,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "96VqnRmB7W",
                "forum": "gwDuW7Ok5f",
                "replyto": "yCTBYYuYoK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3133/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3133/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We extend our gratitude for the positive and constructive feedback.\n\nWe would like to address the raised concerns as follows:\n\n---\n**[Q1] Absence of Future Research Guidance: The paper does not offer any recommendations or insights into potential future research directions or enhancements for the proposed method.**\n\nWe would like to provide detailed suggestions and directions for future research:\n\n* Codebook of Local Facial Parts: As part of future work, we plan to integrate a codebook of local facial parts. This addition could enhance the accuracy of facial restoration, particularly for detailed regions such as the eyes, nose, and mouth, which are crucial for achieving realistic results.\n* Extension to Severe Degraded Video Input: To broaden the applicability of our method, extending it to handle video input is an intriguing direction for future exploration. However, this line of work should consider the consistency of restored results across different time frames. Degradation in videos may vary, and the model should generate coherent and consistent outputs throughout the entire video sequence.\n\n---\n**[Q2] Omission of Limitation Discourse: The paper lacks a discussion regarding its limitations and possible factors for analysis. Could you offer instances where the method encountered shortcomings or limitations?**\n\nWe have provided visual results with large poses in Figure 20 of the supplementary material. While our method demonstrates robustness in most severe degradation scenarios, we also observe instances where it may fail, particularly in cases with large face poses. This can be expected as the FFHQ dataset contains few samples with large face poses, leading to a scarcity of relevant codebook features to effectively address such situations, resulting in less satisfactory restoration and reconstruction outcomes.\n\nWe include failure cases in the supplementary material to ensure a more comprehensive understanding of our method's limitations.\n\n---\n**[Q3] Could you elaborate on the key distinction between DAEFR and CodeFormer?**\n\n* Our method, DAEFR, introduces a crucial distinction by incorporating an additional encoder specifically designed for improved feature encoding in the LQ domain. This allows us to leverage essential information from the LQ images effectively. Moreover, DAEFR uses an association stage that effectively bridges the domain gap between the HQ and LQ domains.\n* In contrast, CodeFormer adopts a different approach by utilizing a pretrained HQ encoder, which is then fine-tuned on LQ images. However, the domain gap still exists despite this adaptation, which may present certain challenges.\n* By integrating the LQ encoder in DAEFR, we aim to encode LQ features better, thereby facilitating more accurate and refined results. This emphasis on enhancing the LQ encoding process demonstrates the critical distinction between DAEFR and CodeFormer, ultimately leading to improved performance and overcoming potential domain gap issues.\n\n---\n**[Q4] Can you provide further experimental details into the \"Effectiveness of Low-Quality Feature from Auxiliary Branch\" as examined in your ablation studies?**\n\nTo demonstrate the effectiveness of our auxiliary LQ branch, we conduct validated experiments of fusing LQ features with feature $Z^{c}\\_{f}$ in the feature fusion and code prediction stage. These experiments involve extracting LQ features $Z^{c}\\_{l}$ from the LQ codebook and adding a control module [1] [2]. Given a feature control module and feature scalar $s\\_{lq}$, we can control the scale of the LQ feature $Z^{c}\\_{l}$ to fuse with the feature $Z^{c}\\_{f}$ before feeding to the HQ decoder. The network architecture is shown in Figure 10 in the supplementary material.\n\nWe conduct an additional training stage to enable the control module to fuse different features effectively. In our implementation, we fine-tune the control module for ten epochs and maintain a feature scalar value of $s\\_{lq} =1$ during training.\n\n[1] Recovering realistic texture in image super-resolution by deep spatial feature transform, CVPR 2018.\n\n[2] Towards Robust Blind Face Restoration with Codebook Lookup Transformer. NeurIPS 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3133/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511995232,
                "cdate": 1700511995232,
                "tmdate": 1700616556001,
                "mdate": 1700616556001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]