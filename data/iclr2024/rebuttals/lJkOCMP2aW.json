[
    {
        "title": "Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting"
    },
    {
        "review": {
            "id": "eogpsp59VJ",
            "forum": "lJkOCMP2aW",
            "replyto": "lJkOCMP2aW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2153/Reviewer_LNPN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2153/Reviewer_LNPN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes multi-scale transformers with adaptive pathways. The proposed method integrates both temporal resolution and temporal distance for multi-scale modeling. It further enriches the multi-scale transformer with adaptive pathways. Experimental results showed the efficacy of proposed method and state-of-the-art performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1)  It's novel to propose multi-scale transformers with adaptive pathways. \n\n2) It's novel to integrate both temporal resolution and temporal distance for multi-scale modeling.\n\n3) The experiments showed state-of-the-art performance."
                },
                "weaknesses": {
                    "value": "1.The current time series forecasting datasets are pretty small, and performance may be satuated or over-fitting. Could the method be used for larger datasets?\n\n2. Scalformer [1] also uses the multi-scale nature of time series data, this paper didn't mention and compare the similarities and differences with Scalformer.\n\n[1] Shabani, Amin, et al. \"Scaleformer: iterative multi-scale refining transformers for time series forecasting.\" ICLR (2023)."
                },
                "questions": {
                    "value": "1. The current time series forecasting datasets are pretty small, and performance may be satuated or over-fitting. Could the method be used for larger datasets?\n\n2. Scalformer [1] also uses hierarchical design and the scales of time series data, Could this paper compare the similarities and differences with Scalformer.\n\n[1] Shabani, Amin, et al. \"Scaleformer: iterative multi-scale refining transformers for time series forecasting.\" ICLR (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2153/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2153/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2153/Reviewer_LNPN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699060260877,
            "cdate": 1699060260877,
            "tmdate": 1700775081291,
            "mdate": 1700775081291,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jrnBv2JZ8r",
                "forum": "lJkOCMP2aW",
                "replyto": "eogpsp59VJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LNPN"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank Reviewer LNPN for acknowledging our technical novelty and empirical contributions, as well as the comments regarding larger datasets and the related multi-scale baseline method. We have revised our paper accordingly.\n\n**Q1:** Results on larger datasets.\n\n**A1:** We seek larger datasets from two perspectives: data volume and the number of variables. We add two datasets, the Wind Power dataset and the PEMS07 dataset, to evaluate the performance of Pathformer on larger datasets. The Wind Power dataset comprises 7397147 timestamps, reaching a sample size in the millions, and the PEMS07 dataset includes 883 variables. If the reviewer may mention other larger datasets, we are also willing to test on them. Pathformer also demonstrates superior predictive performance on these larger datasets compared with some state-of-the-art methods such as PatchTST, DLinear, and Scaleformer. We add experiments on these larger datasets in $\\underline{\\text{Section A.6 of the revised supplementary}}$.\n\n| Methods |     | Pathformer        | PatchTST |     DLinear |     Scaleformer |     \n|:---------:|:----------------:|:-------------:|:-------------------:|:-------------:|:-----:|\n| Metrics |     | MSE    / MAE   | MSE   /  MAE   | MSE   / MAE   | MSE   / MAE |\n| PEMS07    | 96  | **0.135**    / **0.243** | 0.146  / 0.259 | 0.564  / 0.536 |       0.152      /  0.268  |\n|         | 192 | **0.177**    / **0.271** | 0.185   / 0.286 | 0.596  / 0.555 |        0.195     /   0.302  |\n|         | 336 | **0.188**     / **0.278** | 0.205   / 0.289 | 0.475  / 0.482 |      0.276       /  0.394  |\n|         | 720 | **0.208**     / **0.296** | 0.235    / 0.325 | 0.543  / 0.523 |        0.305     /   0.410  |\n| Wind Power   | 96  | **0.062**     / **0.146** | 0.070    / 0.158 | 0.078   / 0.184 |      0.089       /  0.167   |\n|         | 192 | **0.123**     / **0.214** | 0.131    / 0.237 | 0.133  / 0.252 |      0.163       /  0.246   |\n|         | 336 | **0.200**       / **0.283** | 0.215   / 0.307 | 0.205  / 0.325 |      0.225       /    0.352 |\n|         | 720 | **0.388**     / **0.414** | 0.404   / 0.429 | 0.407  / 0.457 | 0.414 / 0.426 |\n\n\n\n**Q2:** Compare with Scaleformer.\n\n**A2:** We mentioned the method of Scaleformer in $\\underline{\\text{the related work of the original submission}}$. In the revision, we add Scaleformer results in $\\underline{\\text{Tabel 1 of the revised paper}}$ as an important baseline of multi-scale models. We also provide a more detailed comparison between Scaleformer and our model in $\\underline{\\text{the related work of the revised paper and Section A.5.3 of the revised supplementary}}$, as follows:\n\nScaleformer also utilizes the modeling of multi-scale features for time series forecasting. It differs from our proposed Pathformer in the following aspects:\n\n- Scaleformer employs fixed sampling rates, while Pathformer has the capability for adaptive multi-scale modeling based on the differences in time series samples.\n- Scaleformer obtains multi-scale features with different temporal resolutions through downsampling. In contrast, Pathformer not only considers time series features of different resolutions but also models from the perspective of temporal distance, taking into account global correlations and local details. This provides a more comprehensive approach to multi-scale modeling through both temporal resolutions and temporal distances.\n- Scaleformer requires the allocation of a predictive model at different temporal resolutions, resulting in higher model complexity than Pathformer."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298899330,
                "cdate": 1700298899330,
                "tmdate": 1700311949284,
                "mdate": 1700311949284,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jo2OqrcxNU",
                "forum": "lJkOCMP2aW",
                "replyto": "eogpsp59VJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer LNPN,\n\nWe would like to sincerely thank you for your time and efforts in reviewing our paper.\n\nWe have made an extensive effort to try to successfully address your concerns, by conducting experiments on larger datasets with more timestamps and variables, comparing our proposed Pathformer with Scaleformer, and making revisions to the paper and appendix accordingly.\n\nWe hope our response can effectively address your concerns, If you have any further concerns or questions, please do not hesitate to let us know, and we will respond timely.\n\nAll the best, \n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493775448,
                "cdate": 1700493775448,
                "tmdate": 1700493775448,
                "mdate": 1700493775448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1SaChqAPQw",
            "forum": "lJkOCMP2aW",
            "replyto": "lJkOCMP2aW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2153/Reviewer_Sqch"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2153/Reviewer_Sqch"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new multi-scale Transformer architecture for long-range time series modeling.The Discriminant Fourier Transform (DFT) is utilized to determine the patch sizes so as to divide the input time series into patches of different sizes, thus enabling cross-scale information fusion. In the multiscale transformer block, intra-patch attention and inter-patch attention mechanisms are utilized to perform attentional operations, thus enhancing the processing of temporal information.Experiments show the proposed method achieves state-of-the-art performance among existing models and exhibits superior generalization capabilities across different transfer scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The paper is well written and well-motivated.\n2.The Multi-Scale Router combines the advantages of patch division and seasonality decomposition.\n3.Dual attention helps to harmonize operations between intra-patch and inter-patch components, allowing the transformer to efficiently process time series data."
                },
                "weaknesses": {
                    "value": "Time-series Dense Encoder (TiDE) is also a popular long-term time-series forecasting benchmark.But the paper does not include experiments comparing the proposed method to TiDE."
                },
                "questions": {
                    "value": "Why is there a gap between the benchmark data in the paper and the original paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699084496326,
            "cdate": 1699084496326,
            "tmdate": 1699636148315,
            "mdate": 1699636148315,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GapMkhEdn3",
                "forum": "lJkOCMP2aW",
                "replyto": "1SaChqAPQw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sqch"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank Reviewer Sqch for acknowledging our technical contributions, providing the recent advanced method to compare with, and the suggestion for a more complete benchmark. We have revised our paper accordingly.\n\n**Q1:** Compare with Time-series Dense Encoder (TiDE).\n\n**A1:** The results of TiDE presented in the original paper are the best ones selected from different input sequence lengths of (48,96,192,336,720). Considering the urgency and fairness, we conduct a comparison under the condition of a fixed input sequence length of 96. We show results of some datasets here, and the complete results of other datasets and methods are available in $\\underline{\\text{Table 1 of the revised paper}}$.\n\n\n| Methods |  | Pathformer | TIDE |\n| :-------:|:-------:|:----------:|:---------:|\n|Metrics|  |MSE / MAE| MSE / MAE | \n| ETTm2      | 96|  0.170 / 0.248 | 0.182 / 0.264 |\n|            | 192|    0.238 / 0.295 |0.256 / 0.323 |\n|            | 336|  0.293 / 0.331 | 0.313 / 0.354 |\n|            | 720|  0.390 / 0.389 | 0.419 / 0.410|\n| Electricity| 96|  0.145 / 0.236   |    0.194/ 0.277  |\n|            | 192|  0.167 / 0.258  |   0.193 / 0.280   |\n|            | 336|  0.186 / 0.275  |    0.206 / 0.296  |\n|            | 720|   0.231 / 0.309 |    0.242 / 0.328  |\n| Weather    | 96 |  0.156 / 0.192  |      0.202 / 0.261             |\n|            | 192|  0.206 / 0.240  |      0.242 / 0.298 |\n|            | 336|  0.254 / 0.282  |      0.287 / 0.335 |\n|            | 720|  0.340 / 0.336  |      0.351 / 0.386 |\n\n\n\n**Q2:** More datasets in the benchmark.\n\n**A2:** We add the ILI and Traffic datasets in the benchmark commonly used by previous papers such as PatchTST. The complete results of other compared methods can be found in $\\underline{\\text{Table 1 of the revised paper}}$.\n\n\n| Methods | |Pathformer      | PatchTST |      \n|:------------:|:---------------:|:-------:|:---------:|\n| Metrics     |     | MSE      / MAE   | MSE     / MAE   |\n| ILI        | 24  | 1.587   / 0.758 | 1.724   / 0.843 | \n|            | 36  | 1.429    / 0.711 | 1.536   / 0.752 | \n|            | 48  | 1.505    / 0.742 | 1.821   / 0.832 | \n|            | 60  | 1.731    / 0.799 | 1.923   / 0.842 | \n| Traffic    | 96  | 0.479    / 0.283 | 0.492   / 0.324 | \n|            | 192 | 0.484    / 0.292 | 0.487   / 0.303 | \n|            | 336 | 0.503    / 0.299 | 0.505   / 0.317 | \n|            | 720 | 0.537    / 0.322 | 0.542   / 0.337 |\n\nWe also evaluate the Exchange_rate dataset, which is proposed in the benchmark of the Autoformer paper. Our proposed Pathformer also outperforms Autoformer on this dataset.\n\n| Methods | |Pathformer      | Autoformer|      \n|:------------:|:---------------:|:-------:|:------:|\n| Metrics     |     | MSE      / MAE   | MSE     / MAE   |\n| Exchange    | 96  | 0.084    / 0.203 | 0.197   / 0.323 | \n|            | 192 | 0.178    / 0.300 | 0.300   / 0.369 | \n|            | 336 | 0.346    / 0.425 | 0.509   / 0.524 |\n|            | 720 | 0.889    / 0.704 | 1.447   / 0.941 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298804705,
                "cdate": 1700298804705,
                "tmdate": 1700298804705,
                "mdate": 1700298804705,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d6JHxBS7nZ",
                "forum": "lJkOCMP2aW",
                "replyto": "1SaChqAPQw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer Sqch,\n\nWe would like to sincerely thank you for your time and efforts in reviewing our paper.\n\nWe have exerted significant effort to effectively address your concerns, by including experiments comparing the proposed Pathformer to TiDE, adding ILI, Traffic and Exchange datasets in the benchmark, and making revisions to the paper and appendix accordingly.\n\nWe hope our response can address your concerns. If you have any further concerns or questions, please do not hesitate to let us know, and we will respond timely.\n\nAll the best, \n\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493666233,
                "cdate": 1700493666233,
                "tmdate": 1700493666233,
                "mdate": 1700493666233,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vN19rBewUx",
            "forum": "lJkOCMP2aW",
            "replyto": "lJkOCMP2aW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2153/Reviewer_5cD6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2153/Reviewer_5cD6"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a variation of PatchTST architecture in the context long-horizon time series forecating."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- An interesting study that makes an incremental step towards making transformers effective at long-horizon forecasting task\n- Paper is clearly written and topic is important for the ICLR audience"
                },
                "weaknesses": {
                    "value": "- \"Recent advances for time series forecasting are mainly based on Transformer architectures\". I would say this statement is not aligned with the most recent empirical results and contradicts existing facts. Having read the following papers one could argue that the transformer based models in time series forecasting have been basically a disaster in the recent years, mainly because authors of papers based on transformer-driven models disregarded including some basic baselines in their studies. Please rewrite intro and related work accordingly.\n    - Challu et al. N-HiTS: Neural hierarchical interpolation for time series forecasting. AAAI'23\n    - Zeng et al. Are transformers effective for time series forecasting? AAAI'23\n    - Li et al. Do Simpler Statistical Methods Perform Better in Multivariate Long Sequence Time-Series Forecasting? CIKM'23\n- Not all datasets are present in the study. Please include additional results on ILI and Traffic from PatchTST\n- Please include results from Zeng et al. Are transformers effective for time series forecasting? AAAI'23 in your table and you will see that your results are not state of the art. This makes the results unconvincing, because basically a very complex model is not able to use the same inputs as very simple models in an effective way. Transformer based papers have consistently failed to include appropriate baselines in the studies creating a large gap in methodology and undermining the ultimate reliability of these studies. The work can be interesting if authors show that with the proposed modifications a transformer based model can be more effective than much simpler and faster models presented in Zeng et al. and Li et al.\n- The model seems to borrow conceptually very heavily from the PatchTST model without explicitly recognizing the source of inspiration. Without a detailed explanation of the actual difference between the two architectures the proposed architecture appears to be a minor perturbation of the original PatchTST."
                },
                "questions": {
                    "value": "- When talking about multi-scale processing in related work please discuss relation to Challu et al. N-HiTS: Neural hierarchical interpolation for time series forecasting AAAI'23, which seems to be relevant work on multi-scale modelling"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2153/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2153/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2153/Reviewer_5cD6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699391502019,
            "cdate": 1699391502019,
            "tmdate": 1699640280538,
            "mdate": 1699640280538,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y5AY2Pi8pT",
                "forum": "lJkOCMP2aW",
                "replyto": "vN19rBewUx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5cD6 (Part I)"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank Reviewer 5cD6 for providing a detailed review and insightful comments regarding important basic baselines, more datasets, and more detailed comparisons with existing methods. We have revised our paper accordingly.\n\n**Q1:** More discussion on some basic baselines.\n\n**A1:** Thanks a lot for raising this valuable comment. We agree with the reviewer that we should also include some basic baselines as proposed in the comment, which may put our paper in a more correct context, give the readers a clearer view of the potentials and challenges of current Transformer methods, and make our contributions more convincing. We have made the following revisions according to your comment:\n\n- We add these important basic baselines in $\\underline{\\text{Introduction or Related Work in the revised paper}}$, and rewrite these two parts accordingly.\n\n- We compare the performance of our method with these baselines, such as DLinear, NLinear and NHITS, to make our empirical improvements more convincing. The detailed experimental results are in the following parts.\n\n\n**Q2:** Include additional results on ILI and Traffic from PatchTST\n\n**A2:** We add the ILI and Traffic datasets in $\\underline{\\text{Table 1 in the revised paper}}$. Here, we show some models with superior performance: PatchTST, DLinear, FEDformer, and Autoformer (bold indicates the best). Results of other compared methods are also included in the revised paper.\n\n\n\n| Methods      |     | PathFormer |     PatchTST  | DLinear |      Fedformer   |   Autoformer |\n|:-------------:|:----------:|:-------------------:|:----------------:|:-----------------:|:------------:|:-------------:|\n| Metrics     |     | MSE / MAE   | MSE / MAE   | MSE  / MAE   | MSE / MAE|   MSE / MAE |\n| ILI       | 96  | **1.587** / **0.758** | 1.724 / 0.843 | 2.573 / 1.073 | 2.624 / 1.095 | 2.906 / 1.182 |\n|             | 192 | **1.429** / **0.711** | 1.536 / 0.752 | 2.673 / 1.085 | 2.516 / 1.021 | 2.585 / 1.038 |\n|             | 336 | **1.505** / **0.742** | 1.821 / 0.832 | 2.773 / 1.126 | 2.505 / 1.041 | 3.024 / 1.145 |\n|             | 720 | **1.731** / **0.799** | 1.923 /  0.842 | 2.827 / 1.152 | 2.742 / 1.122 | 2.761 / 1.114 |\n| Traffic       | 96  | **0.479** / **0.283** | 0.492  / 0.324 | 0.648 / 0.396 | 0.576 / 0.359 | 0.597 / 0.371 |\n|             | 192 | **0.484** / **0.292** | 0.487  / 0.303 | 0.613 / 0.614 | 0.610 / 0.380 | 0.607 / 0.382 |\n|             | 336 | **0.503** / **0.299** | 0.505 / 0.317 | 0.614 / 0.383 | 0.608 / 0.375 | 0.623 / 0.387 |\n|             | 720 | **0.537** / **0.322** | 0.542  / 0.337 | 0.655 / 0.405 | 0.621 / 0.375 | 0.639 / 0.395 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298292467,
                "cdate": 1700298292467,
                "tmdate": 1700298292467,
                "mdate": 1700298292467,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TKwNatlip1",
                "forum": "lJkOCMP2aW",
                "replyto": "vN19rBewUx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5cD6 (Part II)"
                    },
                    "comment": {
                        "value": "**Q3:** Include and compare with the results from Zeng et al.\n\n**A3:** Some results presented in the paper of Zeng et al. (DLinear) are better than ours in Table 1, which is because that DLinear uses a **larger input length** (H=336) than ours (H=96). To ensure a fair comparison, we conduct separate experiments for input sequence lengths (H) of 96 and 336. Here, we show results of some models and datasets, with the complete results of other models and on other datasets available in the $\\underline{\\text{Table 1 in the revised paper and Table 9 in supplementary}}$.\n\n**The results for the input sequence length H=96:**\n\n| Methods |     | Pathformer |    DLinear     | NLinear | \n|:---------:|:-----:|:------------------:|:---------------:|:----:|\n| Metrics |     | MSE       /MAE   | MSE     / MAE   |MSE / MAE| \n| ETTm1   | 96  | **0.316**      / **0.346** | 0.342   / 0.370 |0.339 / 0.369| \n|         | 192 | **0.366**      / **0.370** | 0.383   / 0.394 |0.379 / 0.386|\n|         | 336 | **0.386**     / **0.394** | 0.413   / 0.414 |0.411 / 0.407| \n|         | 720 | **0.460**    / **0.432** | 0.472   / 0.452 |0.478 / 0.442| \n| Weather | 96  | **0.156**      / **0.192** | 0.195   / 0.253 |0.168 / 0.208|\n|         | 192 | **0.206**     / **0.240** | 0.239   / 0.299 |0.217 / 0.255| \n|         | 336 | **0.254**     / **0.282**| 0.282   / 0.333 |0.267 / 0.292| \n|         | 720 | **0.340**    / **0.336** | 0.352   / 0.390 |0.351 / 0.346| \n| ILI     | 24  | **1.587**     / **0.758** | 2.573   / 1.073 |2.725 / 1.069|\n|         | 36  | **1.429**     / **0.711** | 2.673   / 1.085 |2.530 / 1.032|\n|         | 48  | **1.505**     / **0.742**| 2.773   / 1.126 |2.510 / 1.031| \n|         | 60  | **1.731**     / **0.799** | 2.827   / 1.152 |2.492 / 1.026|\n\n**The results for the input sequence length H=336:**\n\n| Methods  |     | Pathformer        | DLinear     | NLinear    |\n|:---------:|:-----:|:------------------:|:----------------:|:----------------:|\n| Metrics  |     | MSE       / MAE    | MSE     / MAE    | MSE     / MAE    | MSE   / MAE    |\n| ETTm1   | 96  | **0.285**    / **0.336**  | 0.299   / 0.353  | 0.306   / 0.348  | \n|         | 192 | **0.331**    / **0.361**  | 0.335   / 0.365  | 0.349   / 0.375  | \n|         | 336 | **0.362**     / **0.382**  | 0.369  / 0.386  | 0.375   / 0.388  | \n|         | 720 | **0.412**    / **0.414**  | 0.425   / 0.421  | 0.433   / 0.422  | \n| Weather | 96  | **0.144**   / **0.184**  | 0.176  / 0.237  | 0.182   / 0.232  | \n|         | 192 | **0.191**   / **0.229**  | 0.220 / 0.282  | 0.225  / 0.269  |\n|         | 336 | **0.234**   / **0.268**  | 0.265  / 0.319  | 0.271  / 0.301  | \n|         | 720 | **0.316**    / **0.323**  | 0.323   / 0.362  | 0.338    / 0.348  | \n| ILI     | 24  | **1.411**      / **0.705**  | 2.215   / 1.081  | 1.683    / 0.868  | \n|         | 36  | **1.365**      / **0.727**  | 1.963   / 0.963  | 1.703    / 0.859  | \n|         | 48  | **1.537**      / **0.764**  | 2.130   / 1.024  | 1.719    / 0.884  | \n|         | 60  | **1.418**      / **0.772**  | 2.368   / 1.096  | 1.819    / 0.917  | \n\nThe results above reveal that our proposed model Pathformer outperforms DLinear with both input sequence lengths of 96 and 336. Zeng et al. point out that previous Transformer cannot extract temporal relations well from longer input sequences, but our proposed Pathformer performs better with a longer input length, indicating that considering adaptive multi-scale modeling can be an effective way to enhance such a relation extraction ability of Transformers."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298429306,
                "cdate": 1700298429306,
                "tmdate": 1700298429306,
                "mdate": 1700298429306,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AtdHDpZ4x3",
                "forum": "lJkOCMP2aW",
                "replyto": "vN19rBewUx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5cD6 (Part III)"
                    },
                    "comment": {
                        "value": "We also conduct experiments to compare the **generalization capabilities** of Pathformer and DLinear, where we train the model on one dataset and test the performance on other datasets. The specific experimental setting is the same with Section 4.2 of the paper.\n\n**The results of the generalization on other datasets:**\n\n| Methods  |     | Pathformer  | DLinear     |\n|:----------:|:-----:|:-----------------:|:---------------:|\n| Metrics  |     | MSE        / MAE    | MSE      / MAE    | MSE     / MAE    |\n| ETTh2    | 96  | **0.340**      / **0.369**  |  0.37    / 0.398  |\n|          | 192 | **0.411**      / **0.406**  |  0.502   / 0.498  |\n|          | 336 | **0.384**      / **0.401**  |  0.563   / 0.531  |\n|          | 720 | **0.450**      / **0.448**  |  0.723   / 0.605  |\n| ETTm2    | 96  | **0.220**      / **0.294**  |  0.272   / 0.325  |\n|          | 192 | **0.258**      / **0.306**  |  0.352   / 0.398  |\n|          | 336 | **0.325**      / **0.350**  |  0.425   / 0.478  |\n|          | 720 | **0.422**      / **0.408**  |  0.553   / 0.517  |\n| Cluster-A | 24  | **0.121**     / **0.223**  |  0.342   / 0.418  |\n|          | 48  | **0.186**      / **0.281**  |  0.389   / 0.468  |\n|          | 96  | **0.249**      / **0.334**  |  0.392   / 0.473  |\n|          | 192 | **0.372**      / **0.416**  |  0.523   / 0.616  |\n| Cluster-B | 24  | **0.140**     / **0.243**  |  0.201   / 0.342  |\n|          | 48  | **0.202**      / **0.298**  |  0.256   / 0.387  |\n|          | 96  | **0.296**      / **0.357**  |  0.389   / 0.476  |\n|          | 192 | **0.464**      / **0.468**  |  0.628   / 0.635  |\n| Cluster-C | 24  | **0.069**      / **0.173**  |  0.145   / 0.242  |\n|          | 48  | **0.144**      / **0.254**  |  0.267   / 0.387  |\n|          | 96  | **0.174**     / **0.284**  |  0.411   / 0.512  |\n|          | 192 | **0.327**      / **0.386**  |  0.522   / 0.532  |\n\nIn the generalization experiments, Pathformer still outperforms DLinear. This indicates that a relatively complex Transformer architecture may have better generalization capabilities than a simple linear model.\n\n**Q4:** Compare with PatchTST\n\n**A4:** We mention the source of inspiration of patching in $\\underline{\\text{Section 3 of our revised paper}}$. We also want to clarify that Pathformer extends from patch division to realize adaptive multi-scale modeling, which is a novel design. It is not a perturbation of PatchTST, as how the series are divided, and how the correlations are modeled are both designed differently. The main differences with PatchTST are as follows:\n\n- **Adaptive Multi-scale Modeling:** \nPatchTST employs a fixed patch size for all data, hindering the grasp of critical patterns in different time series. We are the first to propose adaptive pathways that dynamically select varying patch sizes tailored to the dynamic features of individual samples, enabling adaptive multi-scale modeling.\n\n- **Partitioning with Multiple Patch Sizes:**\nPatchTST employs a single patch size to partition time series, obtaining features with a singular resolution. Pathformer utilizes multiple different patch sizes for partitioning, which captures multi-scale features from various temporal resolutions.\n\n- **Global correlations between patches and local details in each patch:**\nPatchTST performs attention between divided patches, overlooking the internal details in each patch. Pathformer not only considers the correlations between patches but also the detailed information within each patch. It introduces dual attention (inter-patch attention and intra-patch attention) to integrate global correlations and local details, capturing multi-scale features from various temporal distances.\n\nWe also add the above discussion in $\\underline{\\text{Section A.5.1 of the revised supplementary}}$.\n\n**Q5:** Compare with NHITS\n\n**A5:** NHITS also models multi-scale features for time series forecasting, and Pathformer differs from it in the following aspects:\n\n- NHITS models time series features of different resolutions through multi-rate data sampling and hierarchical interpolation. Pathformer not only takes into account time series features of different resolutions but also approaches multi-scale modeling from the perspective of temporal distance. Simultaneously considering temporal resolutions and temporal distances enables a more comprehensive approach to multi-scale modeling.\n- NHITS employs fixed sampling rates for multi-rate data sampling, lacking the ability to adaptively perform multi-scale modeling based on differences in time series samples. In contrast, Pathformer has the capability for adaptive multi-scale modeling.\n- NHITS adopts a linear structure to build its model framework, whereas Pathformer enables multi-scale modeling in a Transformer architecture.\n\nWe add the above discussion in $\\underline{\\text{Section A.5.2 of the revised supplementary}}$. We also compare our performance with NHITS in $\\underline{\\text{Table 9 of the revised supplementary}}$"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298633209,
                "cdate": 1700298633209,
                "tmdate": 1700324593544,
                "mdate": 1700324593544,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PTLLEp4Rmo",
                "forum": "lJkOCMP2aW",
                "replyto": "vN19rBewUx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5cD6,\n\nWe would like to express our sincere gratitude for your time and efforts in reviewing our paper.\n\nWe have made an extensive effort to try to successfully address your concerns. In our response:\n\n- We provide more discussions with basic models, such as DLinear, NLinear, and rewrite the introduction and related work according to your suggestions. \n\n- We add ILI and Traffic datasets from PatchTST.\n\n- We compare our performance with these basic baselines on diverse datasets to make the results of the proposed pathformer more convincing.\n \n- We also compare Pathformer with PatchTST and NHITS to show the novelty and effectiveness of Pathformer and make revisions to the paper and appendix accordingly. \n\nWe hope our response can address your concerns. If you have any further concerns or questions, please do not hesitate to inform us, and we will be more than happy to address them promptly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493560134,
                "cdate": 1700493560134,
                "tmdate": 1700493560134,
                "mdate": 1700493560134,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]