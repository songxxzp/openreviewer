[
    {
        "title": "Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data"
    },
    {
        "review": {
            "id": "ayw3Ytu5VA",
            "forum": "4VIgNuQ1pY",
            "replyto": "4VIgNuQ1pY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2125/Reviewer_gG9F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2125/Reviewer_gG9F"
            ],
            "content": {
                "summary": {
                    "value": "The problem of learning neural stochastic differential equations (NSDE) to solve classification/interpolation tasks in the context of (irregularly sampled) time series data is considered. The authors focus on the analysis of theoretically well-defined SDE classes that exhibit desirable properties in terms of parameterization of drift and diffusion coefficients by neural networks, e.g., in so-called NSDEs. In contrast to naive NSDEs, which can (theoretically) learn almost arbitrary classes of functions, the authors restrict themselves to classes of SDEs for which (i) a (uniquely) strong solution exists, (ii) which can be approximated in a numerically stable manner, and (iii) which remain robust to input perturbations. In addition, they build on concepts from the field of controlled differential equations, which are known to improve model performance on irregularly sampled time series data.  Extensive experiments with established benchmark data sets are included. These provide empirical evidence for the proposed improvements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I appreciate the idea of taking a step back from the state of unlimited expressiveness in NSDE and instead concentrating on sub-classes of SDEs that have favorable properties that combine well with the functional class properties of neural networks. The authors reveal shortcomings associated with the use of unrestricted parameterization of drift and diffusion coefficients by standard neural networks. In turn, an ablation study empirically supports the assumption that careful design of drift and diffusion coefficients is indeed reflected in improved model performance.\nThe content of the paper is well organized, original to the best of my knowledge and shows no obvious spelling or grammatical flaws.\nLast but not least, I enjoyed the theoretically analysis of robustness under distribution shift."
                },
                "weaknesses": {
                    "value": "1. I don't see the necessity to include the details on Neural ODE and CDE into the main manuscript. However, thats more ore less a \nmatter of taste.\n2. (Section 4.2) Comparing such a rich variety of models is challenging. The main difficulty arises due to major differences in model structure. E.g., vanilla RNN based methods are by nature not capable to process irregularly sampled time series. As reported, data imputation strategies must be applied additionally. Another challenge arises from comparing methods that include a control mechanism (e.g., Neural CDE) with methods that do not (e.g., NSDE). The former are able to continuously correct the sampled trajectories over time during learning, while the latter can largely only adjust the initial state. However, the authors elaborate on the latter problem in Table 11, where the proposed methods nevertheless showed their advantage. However, I can't escape the impression that the built-in control mechanism is a big part of the success; because Neural CDE often takes the closely followed second place.\n\nNevertheless, I am on the positive side of this work."
                },
                "questions": {
                    "value": "1. For example, to evaluate robustness empirically, *explicit Euler* is used for all experiments (see page 25). What are the reasons for this choice? I am very curious about the impact of different numerical solution methods on these and other results. Can you explain this in more detail?\n2. Are you planning to release your Code which would unlock reproducibility of the results?\n\nMinor:\n\n3. Aren't the initial state in the Eq. (2) and (3) supposed to be vectors and therefore should be bold?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "--"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2125/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2125/Reviewer_gG9F",
                        "ICLR.cc/2024/Conference/Submission2125/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776952802,
            "cdate": 1698776952802,
            "tmdate": 1700567470783,
            "mdate": 1700567470783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cOy2JbfTYd",
                "forum": "4VIgNuQ1pY",
                "replyto": "ayw3Ytu5VA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses and Questions"
                    },
                    "comment": {
                        "value": ">**Response to weakness 1**\n\nThank you for your valuable suggestion. After careful consideration, we have decided to keep them as they are.\n\n>**Response to weakness 2**\n\nThank you for the insightful comment. Vanilla RNN-based methods face challenges in handling irregularly sampled time series, necessitating additional efforts like data imputation. Other traditional methods also struggle with irregularly sampled time series, requiring extra inputs such as observation intensity, masks for missing data, or specialized architectural features. NDEs without controlled paths heavily rely on initial states, which limits their applicability and performance. \n\nThese limitations have motivated us to incorporate the controlled path, which significantly enhances the performance of models in handling irregularly sampled time series. However, as shown in **Table 11** of the *original manuscript* (**Table 16** of the *revised manuscript*), the naive Neural SDE combined with the controlled path does not outperform the Neural CDE. Therefore, we would like to highlight the importance of well-designed drift and diffusion functions, along with the controlled path, in improving model performance.\n\n>**Response to question 1**\n\nThank you for your insightful comments. In our numerical experiments, we utilized three numerical solvers: the Euler-Maruyama, Milstein, and Stochastic Runge-Kutta (SRK) methods, provided by the $\\texttt{torchsde}$ Python library. The Euler-Maruyama method, an explicit forward method, is highly efficient for solving high-dimensional SDEs, making it a preferable choice over other implicit and high-order solvers like the Milstein and SRK methods. The **Table 8** in the *revised manuscript* presents a comparison of training accuracy and runtime across these three solvers on the `BasicMotion' dataset with a 50% missing rate. This comparison highlights the exceptional efficiency of the Euler-Maruyama method. Consequently, we chose the Euler-Maruyama method for computing the numerical solutions of the Neural SDEs in all our experiments.\n\n>**Response to question 2**\n\nHere is the link for the code: https://bit.ly/3XCKiN5. The link for the implementation code can be found in Section 4.1 - Experimental Protocols of the original manuscript. This section provides detailed information about the experimental setup and the corresponding implementation for each experiment. Also, full details are explained in Appendix D.\n\n>**Response to question 3**\n\nYes, they should be written in bold. We have now updated the manuscript accordingly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464290917,
                "cdate": 1700464290917,
                "tmdate": 1700465681550,
                "mdate": 1700465681550,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ao6SGrIZNX",
                "forum": "4VIgNuQ1pY",
                "replyto": "cOy2JbfTYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2125/Reviewer_gG9F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2125/Reviewer_gG9F"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors very much for their response and for the discussion\nof my questions and concerns.  \n\nYou have clarified things, and I will increase my score correspondingly, under the assumption that these clarifications will make it into the updated version of the paper.\n\nThank you again."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567453044,
                "cdate": 1700567453044,
                "tmdate": 1700567453044,
                "mdate": 1700567453044,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EwwXgJy0K2",
            "forum": "4VIgNuQ1pY",
            "replyto": "4VIgNuQ1pY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2125/Reviewer_Q7tH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2125/Reviewer_Q7tH"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces three stable classes of Neural SDEs (Langevin-type SDE, Linear Noise SDE, and Geometric SDE) to capture complex dynamics and improve robustness under distribution shifts in time series data. Theoretically, this paper shows the existence and uniqueness of the solutions of these SDEs, and presents their performance guarantee under distribution shifts. Extensive experiments are conducted to validate the good performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Topic-wise, modeling time series data with irregular sampling intervals and missing values is an essential research topic and of great importance in practice.\n\nTheory-wise, this paper proves the existence and uniqueness of the solutions of the proposed three SDEs, and shows their robustness to input data. \n\nAdditionally, extensive numerical results are presented to compare the proposed method with existing algorithms for time series modeling."
                },
                "weaknesses": {
                    "value": "The computational complexity of the proposed method is certainly high. \n\nThis paper lacks sufficient details on the implementation of the method, especially when there are irregular time steps and missing data in the series.\n\nFor time series data, in addition to interpolation and classification tasks, it would also be meaningful to consider forecasting tasks as well, which seems to be absent in this paper."
                },
                "questions": {
                    "value": "Detailed discussions on the training procedure are needed, especially for dealing with irregular time steps and missing data. For instance, with different irregular time steps across different sample time series, is it still possible to train the algorithm using mini-batch optimization; and is there a way to improve the computational efficiency in practice? When there is missing data in the sequence, how do we deal with missing values in the training phase -- are these missing values being imputed or ignored during the pre-processing step?\n\nMore explanations are needed for Figure 1 \u2014 line (i) exhibits a constant loss, and in fact, most of the loss trajectories are not satisfactory, with unstable trends and not decaying with the increase of epochs.\n \nFrom Table 5, it is a bit confusing why there is no result for LSDE, LNSDE, and GSDE-\u2018+Z\u2019.\n\nThe downstream tasks considered in this paper are interpolation and classification; it could also be meaningful and worthwhile to consider the prediction task for time series data as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787087815,
            "cdate": 1698787087815,
            "tmdate": 1699636145093,
            "mdate": 1699636145093,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i6Bz5xP5w7",
                "forum": "4VIgNuQ1pY",
                "replyto": "EwwXgJy0K2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses and Questions"
                    },
                    "comment": {
                        "value": "> **W1: The computational complexity of the proposed method is certainly high.**\n\nWe have conducted additional experiments on the `BasicMotion' dataset with 50\\% dataset to compare the training time of the proposed Neural SDEs with other methods. The results, as shown in **Table 9** of the *revised manuscript*, indicate that it takes slightly longer for the proposed Neural SDEs to be trained for 100 epochs than the naive Neural SDEs. However, it is crucial to observe that they train faster and achieve the best accuracy compared to other neural differential equations such as Latent SDE, LEAP, EXIT, ANCDE, Neural CDE, ODE, RNN, GRU ODE, and Neural RDE. Therefore, we emphasize the benefits of the proposed Neural SDEs, highlighting their powerful empirical performance along with theoretical advantages like robustness and well-posedness.\n\n> **W2: This paper lacks sufficient details on the implementation of the method~**\n\nThis part will be addressed in the response to **Q1**.\n\n> **W3: For time series data, in addition to interpolation and classification tasks~**\n\nThis part will be addressed in the response to **Q4**.\n\n> **Q1: Detailed discussions on the training procedure**\n\nWe thank the referee for the comments. \n- **Is it still possible to train the algorithm using mini-batch optimization?** \nYES, we employ the `torchsde` library, which leverages the adjoint-sensitivity algorithm [1] to train Neural SDEs. The adjoint sensitivity algorithm is compatible with the mini-batch scheme, allowing us to efficiently compute gradients of the Neural SDEs even in the presence of irregular time steps.\n\n- **Is there a way to improve the computational efficiency in practice?**\nTo improve the computational efficiency of Neural SDEs in practice, we recommend some simple and effective methods: 1) use the Euler-Maruyama solver for solving SDEs, see **Table 8** of the revised manuscript for the computational efficiency of the Euler-Maruyama solver, 2) employ the adjoint sensitivity algorithm for computing gradients, and 3) use tanh activation functions at the last layer.\n\n- **how do we deal with missing values in the training phase?**\nInterpolation is applied for the given data with irregularly-sampled or missingness. Then, the interpolated value is used to map into the initial value of the Neural SDEs. This approach aligns with the methods used by Kidger et al. [2] and subsequent works. Specifically, we used hermite cubic splines with backward differences [3]. \n\n> **Q2: More explanations are needed for Figure 1**\n\nThank you very much for your valuable comment. The purpose of this experiment was to investigate the impact of the diffusion term on the performance of Neural SDEs. Therefore, we only implemented naive SDEs with various diffusion functions, but without a controlled path and a properly defined drift function. Consequently, the overall loss may not be as optimal as in our final model. This highlights the importance of well-designed drift and diffusion functions, along with the incorporation of a controlled path, which are key contributions of our work.\n\n> **Q3: From Table 5, it is a bit confusing why there is no result for LSDE, LNSDE, and GSDE-\u2018+Z\u2019.**\n\nThank you for your comment. In **Table 5** of the *revised manuscript*, we have reorganized the table based on whether to incorporate the control path and whether to use the diffusion function as an affine function or a nonlinear neural network, to reduce confusion. The results clearly show that, compared to naive Neural SDEs, our proposed neural SDEs with well-designed diffusion and drift functions, along with the controlled path, significantly enhance performance.\n\n> **Q4: The downstream tasks considered in this paper~**\n\nWe conducted additional experiments to evaluate the performance of the proposed methods in forecasting tasks. For these experiments, we utilized the MuJoCo dataset [4], and a detailed description of the data is provided in Appendix C of the revised manuscript. **Table 12** in Appendix E of the *revised manuscript* presents the forecasting performance across varying data drop ratios. We confirmed that our methods consistently demonstrate lower MSE scores, indicating their superior forecasting capabilities.\n\n\n**References**\n- [1] Li, X., Wong, T. K. L., Chen, R. T., & Duvenaud, D. (2020, June). Scalable gradients for stochastic differential equations. In\u00a0International Conference on Artificial Intelligence and Statistics\u00a0(pp. 3870-3882). PMLR.\n- [2] Kidger, P., Morrill, J., Foster, J., & Lyons, T. (2020). Neural controlled differential equations for irregular time series.\u00a0Advances in Neural Information Processing Systems,\u00a033, 6696-6707.\n- [3] Morrill, J., Kidger, P., Yang, L., & Lyons, T. (2021). Neural controlled differential equations for online prediction tasks.\u00a0*arXiv preprint arXiv:2106.11028*.\n- [4] Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. D. L., ... & Riedmiller, M. (2018). Deepmind control suite. arXiv preprint arXiv:1801.00690."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467728262,
                "cdate": 1700467728262,
                "tmdate": 1700468387003,
                "mdate": 1700468387003,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5QtsuD8nLI",
                "forum": "4VIgNuQ1pY",
                "replyto": "i6Bz5xP5w7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2125/Reviewer_Q7tH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2125/Reviewer_Q7tH"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their kind response. My overall rating remains."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698239110,
                "cdate": 1700698239110,
                "tmdate": 1700698239110,
                "mdate": 1700698239110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dRzq95ht0o",
            "forum": "4VIgNuQ1pY",
            "replyto": "4VIgNuQ1pY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2125/Reviewer_8YRs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2125/Reviewer_8YRs"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenges posed by irregular sampling intervals and missing values in real-world time series data. The authors propose three classes of Neural Stochastic Differential Equations (Neural SDEs) to improve robustness under distribution shifts in time series data. The proposed Neural SDEs include Langevin-type SDE, Linear Noise SDE, and Geometric SDE. The study demonstrates the robustness of these Neural SDEs theoretically and through extensive experiments, showing their effectiveness in handling real-world irregular time series data and maintaining excellent performance under distribution shifts due to missing data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors provide a solid theoretical foundation for the proposed Neural SDEs, including the existence and uniqueness of solutions. \n\nThe robustness section in the paper provides valuable insights into the proposed Neural SDEs' resilience against distribution shifts and input perturbations. \n\n The paper conducts extensive experiments to validate the effectiveness of the proposed Neural SDEs. The models are tested on various datasets, and their robustness is analyzed under different missing rates, providing a comprehensive evaluation."
                },
                "weaknesses": {
                    "value": "I didn\u2019t identify major weakness in this paper; however, there are a few minor concerns that I would like to address:\n\n1. Section 3.4 could be more explicit in detailing how the controlled path is incorporated into the Neural SDEs. More comprehensive explanations or illustrations could help in understanding the model\u2019s architecture and functionality better.\n\n2.  While section 3.3 discusses the robustness of the proposed Neural SDEs under distribution shifts, it might benefit from a more thorough exploration or comparison with other neural SDEs' robustness aspects in related works.\n\n3. What is the $\\| \\sigma_\\theta  \\|$ in Theorem 3.6 ?\n\n4. In the experiments of missing data (Table 4), the proposed Neural SDEs show only marginal improvements when compared to the Neural CDE model. How do the theoretical bounds in Theorems 3.5 and 3.6 relate to the robustness of the Neural SDEs in practical implementations? Are these bounds tight or rather loose in actual application scenarios?\n\n5. The implementation code is not provided."
                },
                "questions": {
                    "value": "The details of the proposed models seem unclear.  Can you clarify which neural networks are used in the diffusion and drift functions of each Neural SDE?  \n\nAdditionally,  how are the proposed neural SDEs solved, especially the neural GSDE?   Do you employ numerical solvers for the Neural SDEs? If so, which specific solver was utilized, and was there any ablation study conducted to evaluate its effectiveness?\n\nHow much computational time is required for training the proposed Neural SDEs, and what is the complexity of these models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842526482,
            "cdate": 1698842526482,
            "tmdate": 1699636145012,
            "mdate": 1699636145012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "teJRvk4xCc",
                "forum": "4VIgNuQ1pY",
                "replyto": "dRzq95ht0o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "> **Response to weakness 1**\n\nThank you very much for your comment. Due to the page limit, we deferred the detailed explanation of how the controlled path is integrated into our Neural SDEs to Appendix A.1 of the original manuscript. It contains the associated SDEs combined with the controlled path, e.g., see Eqs (12), (13), (14), and discusses that the drift function with the controlled path also satisfies the Lipschitz continuity condition. \n\nAt the implementation stage, we first consider a neural network $\\zeta: \\mathbb R_+ \\times \\mathbb{R}^{d_z} \\times \\mathbb{R}^{d_x} \\rightarrow \\mathbb{R}^{d_z}$ by concatenating the latent variable $\\mathbf{z}(t)$ and the controlled path $X(t)$ to produce $\\overline{\\mathbf{z}}(t)$. Then, the drift functions of the proposed Neural SDEs take $\\overline{\\mathbf{z}}(t)$ instead of $\\mathbf{z}(t)$, effectively incorporating the controlled path.\n\n> **Response to weakness 2**\n\nThank you for your valuable suggestion. As injected noise in SDEs can destabilize a stable system, the performance of na\u00efve neural SDEs can be extremely vulnerable. For example, we found that the na\u00efve Neural SDE was not properly trained and performed poorly in the interpolation and classification experiments of Section 4.1. Furthermore, in GSDE, it is crucial to satisfy the uniformly bounded condition as stated in Eq (7). This is achieved by assuming that neural networks for the drift and diffusion functions use sigmoid or tanh activation functions at their last layers. Without such careful consideration, GSDE also fails to achieve the robustness under distribution shifts described in Theorem 3.6.\n\nIn addition, we would like to highlight that, to the best of our knowledge, our work is the first attempt to rigorously study the robustness of Neural SDEs under distribution shifts. The most closely related work is [1], which reiterates the almost sure exponential stability result found in the textbook [2]. However, [1] focuses only on the LNSDE type and does not clearly discuss the relationship between the stability of SDEs and the robustness of trained Neural SDEs.\n\n\n> **Response to weakness 3**\n\nWe have now provided a clear definition of $\\sigma_\\theta$ on page 5 in the revised manuscript as follows:\n\n''For our stability analysis, we assume $\\sigma(t;\\theta_\\sigma)$ to be either a constant $\\sigma_{\\theta}$ or to have a limit such that\n$\\lim_{t\\rightarrow \\infty}\\sigma(t;\\theta_\\sigma) =: \\sigma_{\\theta}.$\"\n\n> **Response to weakness 4**\n\nWe thank the referee for the comments. Although our numerical results in Table 4 indicate that the Neural CDE model seems quite robust with respect to missing data, potentially due to the good properties of controlled paths such as smoothness and boundedness discussed in [3], a rigorous analysis of the robustness of the Neural CDE under distribution shifts remains unaddressed in the literature.\n\nTheorems 3.5 and 3.6 provide non-asymptotic upper bounds for the differences between the output distributions of the original input data and its perturbed version in terms of the degree of distribution shift $\\rho$ and the depth of the Neural SDEs $T$. In practical implementation, the key implication of Theorems 3.5 and 3.6 is that smaller perturbations and larger depths yield smaller differences. Therefore, it is expected that a larger $T$ will yield a model more robust to missingness and irregularity in data. To confirm this, we have conducted an additional experiment to investigate the performance changes in Neural SDEs with respect to $T$. Please also refer to **Table 7** in the *revised manuscript* in detail. \n\nRegarding the tightness of the bounds, they also depend on the Lipscthiz constant $L_F$ for the classifier $F$, the initial condition $L_h$, and the contraction parameters $c_1$ and $c_2$. Notably, $c_1$ and $c_2$ are closely linked to the dimension of the given SDE. Thus, the tightness of the bounds can vary significantly depending on the problems. In particular, our assumptions do not impose convexity on the drift and diffusion functions, meaning our theoretical results encompass pathological cases. Therefore, the upper bounds may be loose in some cases. \n\n> **Response to weakness 5**\n\nHere is the link to the code: https://bit.ly/3XCKiN5. The implementation code can be found in Section 4.1 of the original manuscript. For comprehensive details of the experimental setup, please refer to Appendix D.\n\n**References**\n- [1] X. Liu et al. Neural SDE: Stabilizing Neural ODE networks with stochastic noise. arXiv, 2019.\n- [2] X. Mao. Stochastic differential equations and applications. Elsevier, 2007.\n- [3] J. Morrill et al. Neural Controlled Differential Equations for Online Prediction Tasks, 2021, arxiv."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462753035,
                "cdate": 1700462753035,
                "tmdate": 1700466628419,
                "mdate": 1700466628419,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C1kOt5hJxB",
                "forum": "4VIgNuQ1pY",
                "replyto": "dRzq95ht0o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions"
                    },
                    "comment": {
                        "value": "> **Response to question about the clarification of neural networks**\n\nWe employ standard feedforward neural networks, commonly known as multilayer perceptron models, for both the drift and diffusion functions in our Neural SDEs. These networks vary in the number of layers $n_l$ and the number of neurons per layer $n_h$. To determine the optimal hyperparameters, we tested combinations of $n_l=[1,2,3,4]$ and $ n_h=[16,32,64,128]$ across all datasets. For the diffusion term, we also evaluated the effectiveness of linear affine functions. Detailed results can be found in the updated **Table 5**. Additionally, we apply tanh functions at the final layer as recommended by [4]. For a comprehensive overview of our training strategy and model architectures, please refer to Appendix D.1.\n\n> **Response to question about the numerical solvers for the Neural SDEs**\n\nThank you for your insightful comments. In our numerical experiments, we utilized three numerical solvers: the Euler-Maruyama, Milstein, and Stochastic Runge-Kutta (SRK) methods, provided by the torchsde Python library. Each solver has its own strengths and weaknesses in terms of computation speed and convergence rate. The Euler-Maruyama method, an explicit forward method, is notably efficient for solving high-dimensional SDEs. While other implicit and high-order solvers like the Milstein and SRK methods offer faster convergence, they demand more computational time. \n\nIn particular, as our proposed Neural SDEs are well-posed and achieve the best convergence rate (0.5), the Euler-Maruyama method offers significantly faster training times compared to other methods without losing noticeable accuracy. The **Table 8** in the *revised manuscript* shows a comparison of training accuracy and runtime across these three solvers on the `BassicMotion' dataset with 50\\% missing rate, highlighting the exceptional efficiency of the Euler-Maruyama method. Therefore, we chose the Euler-Maruyama method for computing the numerical solutions of the Neural SDEs in all our experiments.\n\n> **Response to question about the computational time**\n\nWe thank the referee for the comment. We have conducted additional experiments on the `BasicMotion' dataset with 50\\% dataset to compare the training time of the proposed Neural SDEs with other methods. The results, as shown in the **Table 9** of the *revised manuscript*, indicate that it takes slightly longer for the proposed Neural SDEs to be trained for 100 epochs than the naive Neural SDEs. However, it is crucial to observe that they train faster and achieve the best accuracy compared to other neural differential equations such as Latent SDE, LEAP, EXIT, ANCDE, Neural CDE, ODE, RNN, GRU ODE, and Neural RDE. Therefore, we emphasize the benefits of the proposed Neural SDEs, highlighting their powerful empirical performance along with theoretical advantages like robustness and well-posedness.\n\n**References**\n- [4] Kidger, P., Morrill, J., Foster, J., & Lyons, T. (2020). Neural controlled differential equations for irregular time series.\u00a0Advances in Neural Information Processing Systems,\u00a033, 6696-6707."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463004406,
                "cdate": 1700463004406,
                "tmdate": 1700466681913,
                "mdate": 1700466681913,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WDFK5HgfPa",
                "forum": "4VIgNuQ1pY",
                "replyto": "C1kOt5hJxB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2125/Reviewer_8YRs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2125/Reviewer_8YRs"
                ],
                "content": {
                    "comment": {
                        "value": "Appreciating the authors' response, I have decided to maintain my current score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710057724,
                "cdate": 1700710057724,
                "tmdate": 1700710057724,
                "mdate": 1700710057724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]