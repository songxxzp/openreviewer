[
    {
        "title": "Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information"
    },
    {
        "review": {
            "id": "fLM5kciq3f",
            "forum": "yV6wwEbtkR",
            "replyto": "yV6wwEbtkR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6436/Reviewer_AG5J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6436/Reviewer_AG5J"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new distillation technique which is based on training teacher models so that they are well-suited for conveying information to the student models. Towards that end, the authors introduce a\"conditional mutual information\"(CMI) objective into the training process of the teacher model, whose goal is to improve the teacher's Bayes conditional probability estimates (via its soft-labels) \u2014\u00a0according to recent knowledge-distillation literature, more accurate Bayes conditional probability estimates result in better student's performance.\n\nOverall:\n\n(i) The authors argue that the so-called dark knowledge passed by the teacher to the student is the contextual information of the images which can be quantified via the conditional mutual information.\n(ii) They provide evidence that temperature-scaling in KD increases the teacher's CMI value\n(iii) They provide evidence that show that models with lower CMI values are not good teacher's, even if they're more accurate.\n(iv) They provide experiments on CIFAR-100 and Imagenet datasets showing evidence that their method helps in improving the student's performance, compared to other standard distillation techniques.\n(v) They show that their technique is especially effective in few-shot and zero-shot settings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a well-written paper that presents a novel approach to knowledge distillation. They authors have provided extensive experimental evidence."
                },
                "weaknesses": {
                    "value": "\u2014\u00a0The role of the teacher as a \"provider of estimates for the unknown Bayes conditional probability distribution\" is a theory for why distillation works that applies well mainly in the context of multi-class classification, and especially in the case where the input is images. (Indeed, there are other explanations for why knowledge distillation works, as it can be seen as a curriculum learning mechanism, a regularization mechanism etc see e.g. [1])\n\nIn that sense, I feel that the author should either make the above more explicit in the text, i.e., explicitly restrict the scope of their claims to multi-classifcation and images, or provide evidence that their technique gives substantial improvements on binary classification tasks in NLP datasets (but even in vision datasets).\n\n\u2014\u00a0One of the main reasons why knowledge distillation is such a popular technique, is because the teacher can generate pseudo-labels for new, unlabeled examples, increasing the size of the student's dataset. (This is known as semi-supervised distillation, or distillation with unlabeled examples, see e.g. [2, 3]. )  It seems that, in order to apply the current approach, one requires the ground-truth labels and, thus,  one has to give up a big part of the power of knowledge distillation as a technique.)\n\nTo be clear, I still like the paper and I am leaning towards acceptance even if the scope of the paper is more limited, but I think it would be beneficial to the research community if the above comments were addressed.\n\n[1] Understanding and Improving Knowledge Distillation [Tang\u2217, Shivanna, Zhao, Lin, Singh, H.Chi, Jain]\n[2] Big self-supervised models are strong semi-supervised learners [Chen, Kornblith, Swersky, Norouzi, Hinton]\n[3] Weighted Distillation with Unlabeled Examples [Iliopoulos, Kontonis, Baykal, Trinh, Menghani, Vee]"
                },
                "questions": {
                    "value": "\u2014\u00a0Does the proposed method and theory works well/applies in NLP datasets/binary classification contexts? \n\u2014\u00a0Is there a way to apply this technique in the context of semi-supervised distillation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6436/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6436/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6436/Reviewer_AG5J"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698618359604,
            "cdate": 1698618359604,
            "tmdate": 1700420692877,
            "mdate": 1700420692877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "npHzu5cdDS",
                "forum": "yV6wwEbtkR",
                "replyto": "fLM5kciq3f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AG5J"
                    },
                    "comment": {
                        "value": "Thank you very much for your positive and valuable comments, which have helped us to improve the manuscript. We have implemented semi-supervised distillation, and demonstrated once again that the MCMI teacher outperforms the MLL teacher in the semi-supervised distillation as well. The respective results are included in appendix A.9 and shown in **\u201cBLUE\u201d** color in the revised version of the paper. Below, please find our point-by-point response to your comments.\n\n\n\n> Comment 1. The role of the teacher as a \"provider of estimates for the unknown Bayes conditional probability distribution\" is a theory for why distillation works that applies well mainly in the context of multi-class classification, and especially in the case where the input is images. (Indeed, there are other explanations for why knowledge distillation works, as it can be seen as a curriculum learning mechanism, a regularization mechanism etc see e.g. [1])\nIn that sense, I feel that the author should either make the above more explicit in the text, i.e., explicitly restrict the scope of their claims to multi-classifcation and images, or provide evidence that their technique gives substantial improvements on binary classification tasks in NLP datasets (but even in vision datasets).\n\n\n**Response**: Yes, we agree that the experiments we have implemented so far are for image multi-classification problem. Having said this, our method, i.e., the MCMI estimator, is applicable to any DNN which outputs a probability distribution in scenarios where knowledge distillation (KD) is applicable, no matter how large or small the dimension of the probability distribution is. Note that the dimension of the probability distribution is 2 in binary classification and greater 2 in multi-classification. We have tested the MCMI estimator in the cases of 10 class classification (CIFAR 10), 100 class classification (CIFAR 100), and 1000 class classification (ImageNet). From our results (Table1 vs Table 2, and Table 9 vs Figures 14 to 19), it follows that in general, the accuracy performance gain offered by the MCMI estimator over the MLL estimator is even more when the number of classes is smaller. This can be explained by a larger percentage increase in the value of CMI when the number of classes is smaller. For example, the percentage increase in the value of CMI offered by the MCMI estimator in Table 1 is several magnitude larger than that in Table 2. Since CMI is well defined in the case of binary classification, there is no reason to believe that the MCMI estimator won\u2019t work for binary classification problems to which KD is applicable. Of course, for those problems to which KD is applicable, how much gain the MCMI estimator would offer in comparison with the MLL estimator, especially for NLP datasets,  needs to be found out in future work. \n\n> Comment 2: One of the main reasons why knowledge distillation is such a popular technique, is because the teacher can generate pseudo-labels for new, unlabeled examples, increasing the size of the student's dataset. (This is known as semi-supervised distillation, or distillation with unlabeled examples, see e.g. [2, 3]. ) It seems that, in order to apply the current approach, one requires the ground-truth labels and, thus, one has to give up a big part of the power of knowledge distillation as a technique.)\n\n**Response:** No, that is not the case. The requirements for our approach are nothing more and nothing less than those for KD since we do not change the KD framework. The knowledge in terms of training samples and their labels required to train the teacher using MCMI is identical to that required to train the teacher using MLL. To further demonstrate this, We have implemented semi-supervised distillation, and demonstrated once again that the MCMI teacher outperforms the MLL teacher in the semi-supervised distillation as well. Please see appendix A.9 for details. \n\n> Comment 3: Does the proposed method and theory works well/applies in NLP datasets/binary classification contexts?\n\n**Response**: Please refer to our response to Comment 1.\n\n> Comment 4: Is there a way to apply this technique in the context of semi-supervised distillation?\n\n**Response**: Yes, the same as in the case of KD. Please refer to our response to Comment 2."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185311260,
                "cdate": 1700185311260,
                "tmdate": 1700185448653,
                "mdate": 1700185448653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EHpcUkezCM",
                "forum": "yV6wwEbtkR",
                "replyto": "npHzu5cdDS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6436/Reviewer_AG5J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6436/Reviewer_AG5J"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors."
                    },
                    "comment": {
                        "value": "*\"Since CMI is well defined in the case of binary classification, there is no reason to believe that the MCMI estimator won\u2019t work for binary classification problems to which KD is applicable. Of course, for those problems to which KD is applicable, how much gain the MCMI estimator would offer in comparison with the MLL estimator, especially for NLP datasets, needs to be found out in future work.\"*\n\nI fully understand that the author's method is in principle applicable to binary classification and NLP datasets. However, in my experience, in the case of binary classification (and especially for NLP datasets) the improvements over vanilla distillation of almost all the distillation techniques I am aware of are particularly limited, if any. (Indeed, the limitations of distillation methods in binary classification settings is discussed in [4], where a method is provided that gives improvement in the cases where the input distribution has \"subclasses\"- structure.)\n\n \nMy larger point here is this: There are various reasons why distillation works depending on the setting. For multiclass-classification (and especially on vision tasks) it seems fairly plausible that  the \"approximating the bayesian prior\"-approach is the main source of the improvements. On the other hand, for binary classification tasks, it seems to me that the \"curriculum learning / regularization\"-nature of distillation is the main source of the improvement (that's why the improvements tend to be marginal). In that sense, I think it is very likely that your technique that aims to improve the Bayes conditional probability estimates might give no significant improvements in the latter setting \u2014 exactly because it is not optimizing for the \"most valuable objective\" for that setting.\n\nRegardless of whether my intuition above is correct or incorrect though, I think you should be a bit more careful with the claims you are making unless you are providing experimental evidence for it. In particular, unless you do experimentally show that you can get significant benefits over vanilla distillation in binary classification tasks (and especially NLP datasets), you should not assume your approach works in these settings.\n\n[4] Subclass Distillation [Rafael Muller, Simon Kornblith, Geoffrey Hinton]\n\n\n*No, that is not the case. The requirements for our approach are nothing more and nothing less than those for KD since we do not change the KD framework. The knowledge in terms of training samples and their labels required to train the teacher using MCMI is identical to that required to train the teacher using MLL. To further demonstrate this, We have implemented semi-supervised distillation, and demonstrated once again that the MCMI teacher outperforms the MLL teacher in the semi-supervised distillation as well. Please see appendix A.9 for details.*\n\nOh I see, thank you for the explanation and the extra experiments!\n\n\nI raised my score to 8."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420713285,
                "cdate": 1700420713285,
                "tmdate": 1700420713285,
                "mdate": 1700420713285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LyvRsYo0ch",
                "forum": "yV6wwEbtkR",
                "replyto": "fLM5kciq3f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really thank the reviewer for reading our responses and raising our score. \n\nWe have done some further experiments to address your concern about binary classification task as explained in the sequel. \nWe acknowledge the fact raised by the reviewer in that \u201cdistillation works depending on the setting\u201d; hence, the answer to the question that whether our method works for binary classification task might be elusive. As such, we have conducted some experiments on binary classification task to show the effectiveness of the MCMI teacher compared to the MLL teacher. Please kindly refer to **Appendix A.13** for the details. Briefly speaking, the MCMI teacher is also effective in binary classification task as well. \n\nWe hope these extra experiments resolves all the concerns you have. Again, thank you for all your valuable comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615123712,
                "cdate": 1700615123712,
                "tmdate": 1700615513950,
                "mdate": 1700615513950,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J4o2wR26FP",
            "forum": "yV6wwEbtkR",
            "replyto": "yV6wwEbtkR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6436/Reviewer_Nadg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6436/Reviewer_Nadg"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method which aims to train the teacher to optimise the student. This is achieved through maximising the conditional mutual information between input and predicted label, conditioned on the true label. The approach demonstrates improved knowledge distillation on CIFAR100 and Imagenet using varies CNN architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is very simple to understand and implement, which only a simple regulariser added to the training of the teacher model, which minimises the KL between the predicted probability and the average probability. \n* The results are conclusive and well presented on ImageNet using plenty of architectures. \n* The extension to few and single-shot experiments are nice."
                },
                "weaknesses": {
                    "value": "In terms of weaknesses:\n* I'm interested to read more about what the role of the CMI regulariser actually does, is it just decreasing the variance of the predictions? Or leading to a distribution with higher entropy? Does this method work just as well if you add an entropy regulariser?\n* As far as I can tell, the value $T$ is not defined, is this for the softmax?"
                },
                "questions": {
                    "value": "* What is the value of $T$? \n* Does the CMI loss just reduce the entropy?\n* If so, is it possible that the same effect can be achieved by simply running this method with temperature scaling? I.e. drop the CMI term?\n* With regards to 6.2. my understanding is that this is using the negative scores during training, so is this really zero-shot classification? Why do you expect this?\n* Did you try varying different classes to drop? \n* In Figure 3, why is the heat map on the terrier not on the body of the animal? Bottom, third from left."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6436/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6436/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6436/Reviewer_Nadg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742739163,
            "cdate": 1698742739163,
            "tmdate": 1700654464213,
            "mdate": 1700654464213,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IHLJvpX1vK",
                "forum": "yV6wwEbtkR",
                "replyto": "J4o2wR26FP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nadg (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your time reading our paper, and your valuable comments. Please find your responses to your comments in the following. \n\n## Weaknesses\n> Comment 1: I'm interested to read more about what the role of the CMI regulariser actually does, is it just decreasing the variance of the predictions? Or leading to a distribution with higher entropy? Does this method work just as well if you add an entropy regulariser?\n\n**Response:** Please refer to our reply to the comments of Reviewer oUDj for the role of the CMI regularizer and the reason why the MCMI estimator outperforms the MLL estimator. CMI is a completely different concept from the variance of the predictions and the entropy of the output distribution. Adding the entropy as a regularizer to the teacher\u2019s loss does not improve the student\u2019s accuracy, which was already demonstrated in the literature where it was shown that applying label smoothing and entropy penalty to the teacher\u2019s training actually decreases the accuracy performance of the student in KD [1]. \n\nHere, we further elucidate what is the **physical meaning** of CMI for DNNs. First, we note that the probability vectors $P_X$ for a specific class $y$ form a cluster in the output probability space of a DNN; we denote the centroid of this cluster by $Q^y$ (see equation (11) in the body of the paper). \n\nNow, referring to equation (11), by fixing the label $Y=y$, the conditional mutual information $I(X;\\hat{Y}|Y=y)$ measures the average KL divergence between the centroid $Q^y$ and the output probability vectors $P_X$ for class $y$. This means that $I(X;\\hat{Y}|Y=y)$ tells how all output probability vectors $P_X$ given $y$ are concentrated around its centroid $Q^y$. In this sense, a lower (higher) value for $I(X;\\hat{Y}|Y=y)$ implies that the DNN\u2019s predictions for class $y$ are more (less) concentrated around its centroid. On the other hand, as mentioned in equation (10), the CMI value for a classifier f denoted by CMI$(f)=I(X;\\hat{Y}|Y)$ is related to $I(X;\\hat{Y}|Y=y)$ as follows: $I(X;\\hat{Y}|Y)=\\sum_{y \\in [C]} P_{Y}(y)I(X;\\hat{Y}|Y=y)$. Hence,  $I(X;\\hat{Y}|Y)$  tells how, on average, the DNN\u2019s predictions are concentrated around the centroids of different classes. To better visualize this concept, please kindly see figure 6 (in the appendix).\n\nBut what is our objective in this paper? As seen in equation (14), the proposed BCPD in the paper relies on **maximizing** the CMI value. Therefore, we are promoting the DNN\u2019s predictions to be less concentrated (more dispersed) in each output probability\u2019s cluster of one class, which can capture more contextual information from the input image compared to their MLL counterparts. \n\nOn the other hand, variance and entropy are two different values with different meanings. Therefore, the value of CMI is unrelated to these quantities, and using another regularization method such as entropy regularization does not have the same effect as using CMI regularization. \n\nTo further demonstrate this lack of relevance, **in appendix A.4.2**, we have compared teachers trained by three different regularization terms, namely CMI, entropy and label smoothing. The results further suggest that the CMI value is not related to entropy, and that for an effective KD, a high CMI value for the teacher matters, and not its entropy. \n\n> Comment 2. As far as I can tell, the value T is not defined, is this for the softmax? \n\n**Response:** \n\nYes, $T$ is for softmax. Please note that we did not make any changes over KD framework; we only replace the conventional MLL teacher by the MCMI teacher. Therefore, all the notations employed in this paper, including $T$, are adopted from the knowledge distillation (KD) framework.\n\n[1]M\u00fcller, Rafael, Simon Kornblith, and Geoffrey E. Hinton. \"When does label smoothing help?.\" Advances in neural information processing systems 32 (2019)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328364418,
                "cdate": 1700328364418,
                "tmdate": 1700334368473,
                "mdate": 1700334368473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3pdea4wcPz",
                "forum": "yV6wwEbtkR",
                "replyto": "J4o2wR26FP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nadg (2/3)"
                    },
                    "comment": {
                        "value": "## Response to Questions\n\n> Q1. What is the value of T?\n\nAs discussed in the response to the second comment above, $T$ is indeed the temperature of the softmax function in KD. Additionally, the value of $T$ is the same as that used in the benchmark methods in their original paper. Hence, $T=1$ for all the KD methods tested over ImageNet dataset, and $T=4$ for the majority of the KD methods tested over CIFAR-100 methods. \n\nAs also emphasized in the main body of the paper, one of the prominent advantage of the proposed method is \u201cPlug-and-play\u201d nature of MCMI teacher in that we do not change any hyper-parameters in the underlying knowledge transfer methods, all of which are the same as in the corresponding benchmark methods. \n\n> Q2. Does the CMI loss just reduce the entropy?\n\nPlease refer to our response to comment 1. \n\n> Q3. If so, is it possible that the same effect can be achieved by simply running this method with temperature scaling? I.e. drop the CMI term?\n\nThank you for your insightful comment. The temperature scaling (which is referred to as post-hoc regularization) has different effect compared to CMI regularization. \n\nIn fact, the gain obtained via maximizing CMI is orthogonal to that obtained via tuning the $T$ values. More precisely, although by scaling $T$ for teacher, while fixing the $T$ for student, one can get some gain, this enhancement is independent of the gains achieved through maximizing the CMI value. To clarify, we have included **Appendix A.7**, where we conducted experiments with different $T$ values for the teacher and student, observing that the MCMI teacher consistently provides gains for any such combinations.\n\n> Q4. With regards to 6.2. my understanding is that this is using the negative scores during training, so is this really zero-shot classification? Why do you expect this?\n\nNo, we are indeed aligned with the zero-shot setting for knowledge distillation as discussed in Section 3 of Hinton\u2019s paper [1], where the authors specifically performed zero-shot distillation for the MNIST dataset. It's important to note that, unlike our method, the conventional knowledge distillation used in [1] does not work for other challenging datasets such as CIFAR-100 and CIFAR-10.\n\nWe further note that our method is different from negative scoring method used in zero-shot learning which needs predefined semantic vectors for both seen and unseen classes and assigns positive scores to related semantics and negative scores to unrelated semantics during the evaluation stage [2]. We argue that our method is more generalized, as during the training process, teacher don\u2019t know which classes will be omitted when distill knowledge to the student model.\n\nLastly, we assume that you are asking why our teacher can lead to a student with better zero-shot performance. This is due to the fact that the information regarding the labels for the missing classes is still contained within $P_X ^*$ of the samples presented in the dataset. To elucidate, consider the following example in which the objective is to train a DNN to classify three classes: dog, cat, and car. Assume that the training samples for dog is entirely missing from the training dataset. Suppose that for a particular training sample from the cat class, $P_X ^*=[0.30,0.68,0.02]$, with the probability values corresponding to the dog, cat, and car classes, respectively. The value of 0.30 suggests that this sample exhibits some features resembling the dog class, such as a similarity in the shape of legs. Such information provided from the BCPD vectors for the samples of existing classes can collectively help the DNN to classify the samples for the missing class. We note that the one-hot vector for this sample is $[0,1,0]$, and therefore, such information does not exist when one-hot vectors are used as unbiased estimate of $P_X ^*$.\n\n[1] Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff \u201dDistilling the Knowledge in a Neural Network\u201d stat. 2015\n\n[2] Huynh, Dat, and Ehsan Elhamifar. \"Fine-grained generalized zero-shot learning via dense attribute-based attention.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328732835,
                "cdate": 1700328732835,
                "tmdate": 1700334033534,
                "mdate": 1700334033534,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gTrDPlb7jr",
                "forum": "yV6wwEbtkR",
                "replyto": "J4o2wR26FP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nadg (3/3)"
                    },
                    "comment": {
                        "value": "> Q5. Did you try varying different classes to drop?\n\nYes, please refer to Figure 14 to 19 in Appendix A.11, where you will find experiments on CIFAR-100 and CIFAR-10 datasets. Specifically for CIFAR-10, we conducted experiments with {1,2,\u20266} classes dropped from the dataset, each with two different combinations of the dropped classes. As observed, the gain obtained is independent of the classes dropped, and this gain is consistent for different numbers of dropped classes.\n\n> Q6. In Figure 3, why is the heat map on the terrier not on the body of the animal? Bottom, third from left.\n\nAs discussed in our response to comment 1, the CMI is a class-based (cluster-based) concept, and cannot be evaluated by looking at a specific image. In other words, to evaluate the CMI value for a DNN, all the probability vectors (or the corresponding feature maps) within one class should be considered **as a whole**.\n\nIf the CMI value for a given class is high, then the output probability vectors and their corresponding feature maps contain more information. These information show up in two formats: (i) the prototype, and (i) the contextual information which can be the background or the unique characters of the object in the images (noise of the cluster). \n\nOnce looking at the Eigen-CAM for MLL teacher, for all the images the MLL teacher only captures the dog\u2019s head (prototype); however, for the MCMI teacher this focus is more dispersed ranging from dog\u2019s head, doge\u2019s legs, and the background of the image (contextual information). As such, the MCMI teacher contains more information when we consider **all the images from one class**. \n\nNevertheless, depicting only four images is not a good representation of the images in one class. To this end, in appendix A9 we sample and depict more images from four specific classes of the ImageNet training set to better illustrate this fact."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328782043,
                "cdate": 1700328782043,
                "tmdate": 1700333163711,
                "mdate": 1700333163711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QeT4oMilyH",
            "forum": "yV6wwEbtkR",
            "replyto": "yV6wwEbtkR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6436/Reviewer_oUDj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6436/Reviewer_oUDj"
            ],
            "content": {
                "summary": {
                    "value": "This work  builds upon the insights from the previous study on knowledge distillation [1], which implies that producing a good teacher model \nsimilar to the optimal Bayes class probability $P^{*}_{X}$, is crucial for enhancing the performance of the student model. To convey this message, the authors propose a new training objective for the \"teacher model\" by introducing the empirical estimate of conditional mutual information as a regularizing term (MCMI). \n\nThe authors provide empirical evidence between MCMI and the accuracy of the student model; as the MCMI attains higher values, the the corresponding teach model obtains the highest accuracy. Furthermore, when using the teacher model trained with the MCMI regularizer, the corresponding teacher exhibits improved accuracy in most existing knowledge distillation algorithms. The proposed regularizer leads to improved performance of the student model in zero-shot and few-shot classification tasks  as well.\n\n[1] A Statistical Perspective on Distillation - ICML 21"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Simple idea:\n\n> In implementation sense, the idea looks simple and easy to implement this idea; introducing a estimate of the MCMI in Eq (2) is additionally necessary.\n\n### Empirical improvement:\n> It seems that the proposed objective for the teacher model can be integrated with existing knowledge distillation algorithms which mainly focus on the distillation objective in view of \"student\" model. The proposed regularizer for the 'teacher' model seems to be effective in enhancing the performance of the 'student' model trained with existing knowledge distillation algorithms."
                },
                "weaknesses": {
                    "value": "### Less elaboration on relationship between conditional mutual information $I(X , \\hat{Y} | Y)$ and optimal bayes classifier $P^{*}_{X}$\n\n> While it is intuitively clear that using the conditional mutual information as the regularizer term can capture the contextual information of $X$ (Image) and provide additional information to a student model, the direct connection between conditional mutual information and the optimal Bayes classifier is less explained. I believe explaining this connection is important because this approach is motivated from the importance of optimal classifier $P^{*}_{X}$."
                },
                "questions": {
                    "value": "* Q1.  Could you elaborately explain why minimizing $I(X , \\hat{Y} | Y)$ can make the teacher model $f$ to be more similar to the optimal bayes classifier ? \n\n\n\n* Q2. It seems that the proposed regularizer requires the pre-trained model as the teacher model and apply the further training to the teacher model with the proposed objective of Eq. (14). How do we set the number of iterations further training? Based on my understanding, since we expect this regularizer to make the teacher model contain additional information as well as to be properly certain (not overconfident), setting the number of iterations is important hyperparameters and might significantly affect the performance of student model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6436/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6436/Reviewer_oUDj",
                        "ICLR.cc/2024/Conference/Submission6436/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840782690,
            "cdate": 1698840782690,
            "tmdate": 1700681646671,
            "mdate": 1700681646671,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yOdFsov0vz",
                "forum": "yV6wwEbtkR",
                "replyto": "QeT4oMilyH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oUDj (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking time to review our paper and provide valuable feedbacks. Below please find our responses to your comments.\n\n## 1. Clarification\n> Comment 1:  This work builds upon the insights from the previous study on knowledge distillation [1], which implies that producing a good teacher model similar to the optimal Bayes class probability $P_X^*$, is crucial for enhancing the performance of the student. The authors propose a new training objective for the teacher by introducing the empirical estimate of conditional mutual information as a regularizing term (MCMI).\n\n**Response**: We appreciate the above comment and the work [1]. Indeed, the work [1] above along with other recent work in the literature cited in our paper helped us realize that training the teacher for the benefit of the student in the KD framework is a different task from training the teacher for its own accuracy performance. The former can be regarded as a process of estimating the unknown Bayes class probability $P_X^*$, and the teacher trained with the cross entropy objective function is actually a maximum log-likelyhood (MLL) estimate of $P_X^*$. The work [1] and others cited in the paper, however, do not suggest any new specific means to estimate $P_X^*$. Our paper introduces the concept of conditional mutual information (CMI) into the estimation of $P_X^*$ and propose, for the first time, an estimator called MCMI estimator which is different from and better than the MLL estimator. \n\n> Comment 2: The authors provide empirical evidence between MCMI and the accuracy of the student model; as the MCMI attains higher values, the the corresponding teach model obtains the highest accuracy. When using the teacher model trained with the MCMI regularizer, the corresponding teacher exhibits improved accuracy in most existing knowledge distillation algorithms. The proposed regularizer leads to improved performance of the student in zero-shot and few-shot tasks.\n\n**Response**: If we understand the above comment correctly, there is a slight misunderstanding here. Please refer to our response above for the difference between (1) estimating $P_X^*$ via training the teacher for the benefit of the student in the KD framework and (2) training the teacher for its own accuracy performance. The teacher trained with MCMI, acting as the MCMI estimator of $P_X^*$, increases the student\u2019s accuracy across the board in all tested KD settings, with dramatic performance improvement in zero-shot and few-shot classification tasks, demonstrating that the MCMI estimator is better than the MLL estimator. However, as the CMI value increases, the teacher\u2019s own accuracy would be degraded slightly. This, nonetheless, is not a problem since as mentioned above, estimating $P_X^*$ is a different task from training the teacher for its own accuracy performance, which is consistent with observations made in literature that teachers with higher accuracy on their own are not necessarily good teachers for the student in KD. \n\n> Comment 3: Less elaboration on relationship between conditional mutual information $I(X;\\hat{Y}|Y)$ and optimal bayes classifier $P_X^*$\n\n**Response**: We truly appreciate this theoretic question in general. However, at this point, the formulation of this theoretic question is not clear at all for a couple of reasons: (1) the underlying joint distribution of (X,Y) and hence P_X^* are unknown, and any amenable assumption would go against the current philosophy of deep learning---if the joint distribution is known, there is no need for learning; and (2) CMI $I(X;\\hat{Y}|Y)$ highly depends on the underlying deep neural network (DNN) which varies from one setting to another and for which deep understanding still lacks. Without certain assumptions on the underlying DNN, there might be no theoretical relationship between  $I(X;\\hat{Y}|Y)$ and $P_X^*$. For example, when the underlying DNN is poorly designed---all output probability vectors huddle together,  $I(X;\\hat{Y}|Y)$ could be always near 0 no matter what the joint distribution of $(X,Y)$ is. This setting is very different from the work [1] where the law of total probability for variance can be nicely applied in general. To overcome this difficulty, in this paper we instead choose the following approaches to justify and evaluate the proposed MCMI estimator:\n\n1. Since the purpose of estimating $P_X^*$ is for the benefit of the student in KD, a good litmus test for an estimator is to see how the estimator can help the student improve its accuracy. In this regard, our proposed MCMI estimator is thoroughly compared to the MLL estimator and shown  superior to the MLL estimator across the board for all tested KD frameworks. \n\n2. A synthetic Gaussian dataset is created and used to evaluate how close our MCMI estimate is to P_X^* . Please see appendix A.6 for details. Again, the results therein demonstrate the superiority of  the MCMI estimator to the MLL estimator in this synthetic setting as well."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183799644,
                "cdate": 1700183799644,
                "tmdate": 1700183799644,
                "mdate": 1700183799644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SAfhHiCBUj",
                "forum": "yV6wwEbtkR",
                "replyto": "QeT4oMilyH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oUDj (2/2)"
                    },
                    "comment": {
                        "value": "## 2. Response to Questions\n\n> Response to Q1.\n\nWe guess you mean maximizing $I(X;\\hat{Y}|Y)$. We won\u2019t refer to the unknown Bayes class probability $P_X^*$ as the optimal Bayes classifier since the context for defining the optimality is not clear. In our view, the unknown Bayes class probability $P_X^*$ is defined by nature, or by human beings as a whole; it should be the average of all predictions made by a very large group of people. Intuitively, $P_X^*$ is influenced by different contextual information of the object in $X$ with respect to the corresponding image cluster, which would cause different people to make different predictions, and hence should stay a little away from the probability vector corresponding to the pure prototype (i.e., the object) which is the label one-hot vector in conventional deep learning, and the \u201ccentroid\u201d vector $Q^Y$ defined in Eq. (11) in our case. It is the difference between $P_X^*$ and the prototype probability vector that reflects the variety of contextual information of the object. Maximizing $I(X;\\hat{Y}|Y)$ to some degree would enable the teacher model f to extract more contextual information of the object into its output probability vector, thereby making it to be closer to $P_X^*$. This is indeed further confirmed by our results on synthetic Gaussian dataset in appendix A.6.\n\n> Response to Q2.\n\nFor all the experiments we conducted in the paper, we fixed the number of fine-tuning epochs equal 20 for CIFAR-100 and 10 for ImageNet. This choice was made because we did not observe any gains in student\u2019s accuracy beyond this number of epochs. However, it's important to note that the optimal number of epochs for fine-tuning may vary for different teacher-student pairs, and tuning this hyper-parameter could potentially yield additional improvements."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184006368,
                "cdate": 1700184006368,
                "tmdate": 1700184006368,
                "mdate": 1700184006368,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]