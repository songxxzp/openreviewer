[
    {
        "title": "The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric"
    },
    {
        "review": {
            "id": "topUyGblnL",
            "forum": "e4FG5PJ9uC",
            "replyto": "e4FG5PJ9uC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2943/Reviewer_QSZk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2943/Reviewer_QSZk"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on solving the weighted least squares problem for image quality distance in full-reference image quality assessment. Inspired by lossless compression, this work proposes the Autoregressive Similarity Index (LASI), which obtains perceptual embeddings by calculating a weighted sum of the causal neighborhood subset of pixel values to predict the current pixel value. The performance of the data-free LASI is comparable to that of supervised methods such as LPIPS and unsupervised methods like PIM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe LASI metric is designed in a simple yet solid manner. Full-reference Image Quality Assessment (FR IQA) can be approached from the perspective of lossless compression and the semantic information within the neighborhood of pixels.\n2.\tThe experiments of JND\uff0c2AFC and MAD are very detailed and the explanations are very clear."
                },
                "weaknesses": {
                    "value": "1. The experimental dataset consists only BAPPS. \n2. An ablation study is missing for the causal neighborhood and non-causal neighborhood, as semantic understanding relies on contextual relationships around pixels or regions."
                },
                "questions": {
                    "value": "1. The sentence in the section 4.1 \u2018\u2019Our method relies on self-supervision (at inference-time) to learn a representation for each pixel that captures global perceptual semantics of the image. The underlying assumption is that, given a representation vector for some pixel, it must successfully predict the value of other pixels in the image in order to capture useful semantics of the image\u2019s structure.\u201d But the relationship of the perceptual embedding in FR-IQA and the semantic extraction is confused in this work. Because many previous methods also use semantic features extracted by pre-trained models on high-level classification tasks to calculate perceptual distance. Is there any difference between this semantic feature and the embedding derived by Eq 1 in this work?\n2. Since the LASI is designed on the semantic extraction in the images\u2019 structure, so the correlation of the prediction task and perceptual task is good, which is a little obvious.  \n4. Perhaps LASI can only measure perceptual distance at patch level, and will it be useful for high-resolution images? \n5. An ablation study is missing for the causal neighborhood and non-causal neighborhood, as semantic understanding relies on contextual relationships around pixels or regions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2943/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2943/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2943/Reviewer_QSZk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2943/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758882807,
            "cdate": 1698758882807,
            "tmdate": 1700720685257,
            "mdate": 1700720685257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BkW2aCweuY",
                "forum": "e4FG5PJ9uC",
                "replyto": "topUyGblnL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2943/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2943/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the questions regarding ablations and interest in the correlation between perceptual and downstream tasks, of which answers will be incorporated into the main text.\n\nWe hope our rebuttal addresses your concerns and you will consider increasing your score.\n\n---\n\n> The experimental dataset consists only BAPPS.\n\nWe are aware of the existence of other full-reference (i.e. pairwise) IQA datasets, however BAPPS is the only one we know of that contains a rich set of distortions. Other established datasets such as TID2013 or LIVE do not contain CNN-based distortions such as warping, and as such are prone to overfitting to (which we believe has already happened with traditional metrics such as MS-SSIM). A topic for future work would be to collect our own data to validate on, but this is unfortunately outside of the scope of this paper.\n\n---\n\n> Perhaps LASI can only measure perceptual distance at patch level, and will it be useful for high-resolution images?\n\nDistortions are usually applied to the image as a whole and therefore different patches are distorted in a similar way.\nIt is common in the FR-IQA literature to scale up to larger images by computing the metric on smaller patches for this reason.\nLASI can be run in the same way by parallelizing over patches and averaging the distance, a common technique in the literature.\nWe expect this to perform similarly, the same way other methods do, and will include an experiment on LIVE IQA [1] in the follow-up.\n\n[1] Sheikh H. LIVE image quality assessment database release 2. http://live.ece.utexas.edu/research/quality. 2005.\n\n---\n\n> An ablation study is missing for the causal neighborhood and non-causal neighborhood, as semantic understanding relies on contextual relationships around pixels or regions.\n\nWe thank the reviewer for this suggestion of what is effectively a new version of our LASI metric.\nThis exact ablation has been run and is shown below.\nThe performance of the new suggestion is slightly worse, however, the implementation is simplified as computing the non-causal mask consumes less memory.\n\n[Please click for ablation plot](https://postimg.cc/V5ZPm8jk)\n\n---\n\n> The sentence in the section 4.1 \u2018\u2019Our method relies on self-supervision (at inference-time) to learn a representation for each pixel that captures global perceptual semantics of the image. The underlying assumption is that, given a representation vector for some pixel, it must successfully predict the value of other pixels in the image in order to capture useful semantics of the image\u2019s structure.\u201d But the relationship of the perceptual embedding in FR-IQA and the semantic extraction is confused in this work. Because many previous methods also use semantic features extracted by pre-trained models on high-level classification tasks to calculate perceptual distance. Is there any difference between this semantic feature and the embedding derived by Eq 1 in this work?\n\nWe are not sure if we understand your question correctly, but we do not think that we are confusing two different concepts in this work. Computing a perceptual embedding of an image in many prior works is done via a function $f(x; \\theta)$, where $x$ is the image and $\\theta$ are, for example, neural network parameters that are determined by a separate training stage.\nIn LASI, computing the weights is done via a function $f(x)$ that does not have trainable parameters, hence no training stage is necessary. Computing the embedding is performed via solving a linear least squares problem, and hence there is no analytic formula for $f$.\nHowever, that does not matter, since $f$ is still a deterministic function of the image: every time we solve for $w$ (the weights of the linear least squares problem), we get the same answer.\n\n---\n\n> Since the LASI is designed on the semantic extraction in the images\u2019 structure, so the correlation of the prediction task and perceptual task is good, which is a little obvious.\n\nThere is no evidence in the psychophysical literature that points towards the human visual system performing low-level reconstruction tasks.\nIt is not immediately obvious to us that good perceptual embeddings can be learned from linear prediction of pixels.\n\nHowever, we agree that it is intuitive to expect the correlation to be better for LASI (prediction) than for LPIPS (classification).\nThe reason being that a useful feature for classification is not necessarily useful for visual tasks regarding the image\u2019s structure."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2943/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281094426,
                "cdate": 1700281094426,
                "tmdate": 1700578771243,
                "mdate": 1700578771243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DLHQc4MzDN",
                "forum": "e4FG5PJ9uC",
                "replyto": "BkW2aCweuY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_QSZk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_QSZk"
                ],
                "content": {
                    "comment": {
                        "value": "thanks for the clarification which partly address my concerns, I will keep the score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2943/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720729918,
                "cdate": 1700720729918,
                "tmdate": 1700720729918,
                "mdate": 1700720729918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2yPbZ9RNYT",
            "forum": "e4FG5PJ9uC",
            "replyto": "e4FG5PJ9uC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2943/Reviewer_8dYU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2943/Reviewer_8dYU"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new perceptual metric that requires no training data nor DNN features. In particular, taking inspiration from psychology finding that visual working memory compresses visual stimuli, the proposed method, named Linear Autoregressive Similarity Index (LASI), compresses a visual stimuli during inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a new perceptual metric that requires no training data or DNN features.\n\n- The paper is clearly written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The paper claims to have competitive performance, compared with LPIPS method. But, 11% difference in SR in Table 1 seems rather high.\n\n- The paper claims that the advantage of the proposed method is that it requires no training data or DNN features. But, the method requires computations at inference time. Considering that once training is done, DNN-feature based methods do not require much of extra computations (hence the cost is amortized) while the proposed method requires extra computations, is requiring training data or DNN features really a bad thing?\n\n- All experiments are performed with 64x64 resolution, which seem rather small. Is the proposed method effective for larger images, compared to other works? Is the choice of $N$ the size of neighborhood robust against the image size? It seems $N$ needs to be tuned for each image resolution, which can be critical, since images can come in at various sizes.\n\n- The paper claims that the method can be combined with LPIPS but I cannot find the experimental results on this. Does the combination actually bring improvements?"
                },
                "questions": {
                    "value": "Written in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2943/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2943/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2943/Reviewer_8dYU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2943/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839973792,
            "cdate": 1698839973792,
            "tmdate": 1700735930580,
            "mdate": 1700735930580,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PX7BYh4jpZ",
                "forum": "e4FG5PJ9uC",
                "replyto": "2yPbZ9RNYT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2943/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2943/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their questions regarding compute time which we will incorporate the response into the main text. LASI (ours) requires less compute than DNN methods due to their expensive forward-pass.\n\nWe hope our rebuttal addresses your concerns and you will consider increasing your score.\n\n---\n\n> The paper claims that the advantage of the proposed method is that it requires no training data or DNN features. But, the method requires computations at inference time. Considering that once training is done, DNN-feature based methods do not require much of extra computations (hence the cost is amortized) while the proposed method requires extra computations, is requiring training data or DNN features really a bad thing?\n\nEven with amortization, computing DNN features requires more computation at inference time than LASI (ours).\nDNN features require a forward-pass in the neural network to compute embeddings.\nThe plot below shows the wall-time for computing PIM (a DNN) and LASI (ours) distances, compiled with `tf.function` and `jax.jit`, respectively.\nNote how LASI is faster for the neighborhood sizes considered in the experiments (i.e., $10$ to $12$).\n\n[Please click for LASI and PIM comparison plot](https://postimg.cc/ygnnh7Vg)\n\n---\n\n> The paper claims to have competitive performance, compared with LPIPS method. But, 11% difference in SR in Table 1 seems rather high.\n\nThe difference mentioned by the reviewer is with respect to supervised LPIPS, i.e., after fine-tuning LPIPS on human annotated data.\nWhen comparing to unsupervised LPIPS (i.e., neural network embeddings before fine-tuning), which we hold to be the fairer comparison, the difference is less than $2\\\\%$.\n\nWe will call attention to this fact in the caption of Table 1.\n\n---\n \n> All experiments are performed with 64x64 resolution, which seem rather small. Is the proposed method effective for larger images, compared to other works?\n\nDistortions are usually applied to the image as a whole and therefore different patches are distorted in a similar way.\nIt is common in the FR-IQA literature to scale up to larger images by computing the metric on smaller patches for this reason.\nLASI can be run in the same way by parallelizing over patches and averaging the distance, a common technique in the literature.\nWe expect this to perform similarly, the same way other methods do, and will include an experiment on LIVE IQA [1] in the follow-up.\n\n[1] Sheikh H. LIVE image quality assessment database release 2. http://live.ece.utexas.edu/research/quality. 2005.\n\n---\n\n> Is the choice of $N$ the size of neighborhood robust against the image size? It seems $N$ needs to be tuned for each image resolution, which can be critical, since images can come in at various sizes.\n\n$N$ does not need to be tuned for each image resolution.\nOur experiments show increasing $N$ gives better downstream performance (see Figure 3).\n$N$ should be set to as large as possible while respecting the compute and memory budget of the application.\n\n---\n\n> The paper claims that the method can be combined with LPIPS but I cannot find the experimental results on this. Does the combination actually bring improvements?\n\nThe MAD experiment highlights that LPIPS and LASI fail in different ways.\nWe experimented with merging these models by taking geometric averages of their distances.\nThis resulted in a slightly better perceptual performance but not enough to justify merging, as the compute required is now that of LPIPS + LASI.\n\nMerging these models is a non-trivial manner as the embedding dimensions differ in size making it difficult to perform simple aggregation techniques at the embedding level. Note this is true in general for any pair of models.\n\nWe will update the writing of the paper to make this clear in the main text."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2943/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281141580,
                "cdate": 1700281141580,
                "tmdate": 1700449796592,
                "mdate": 1700449796592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CotgfurZaV",
                "forum": "e4FG5PJ9uC",
                "replyto": "PX7BYh4jpZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_8dYU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_8dYU"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response.\nIn regards to performance comparison regarding super resolution, I do not completely agree with the authors' arguments. The paper seems to point the proposed method as an alternative of LPIPS. Regardless of finetuning or not, if fine-tuned LPIPS is commonly used, shouldn't the comparison be done against fine-tuned LPIPS. There is no difference in inference computational complexity, regardless of whether the fine-tuning is done or not. And also, why do authors think that there is such difference observed only in super-resolution task?\n\nAlso, the paper can be revised during the discussion phase, and I suggest sharing the revision to show the reviewers how the paper would be updated."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2943/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551527208,
                "cdate": 1700551527208,
                "tmdate": 1700551527208,
                "mdate": 1700551527208,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jDFu22WlzQ",
                "forum": "e4FG5PJ9uC",
                "replyto": "2yPbZ9RNYT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_8dYU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_8dYU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the efforts and clarifications that have addressed my concerns. Thus, I have updated the score accordingly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2943/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735917507,
                "cdate": 1700735917507,
                "tmdate": 1700735945704,
                "mdate": 1700735945704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a66I2On7yD",
            "forum": "e4FG5PJ9uC",
            "replyto": "e4FG5PJ9uC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2943/Reviewer_WqhQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2943/Reviewer_WqhQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on generating perceptual embedding of an image, without using any training data. The paper also introduces and proposes a distance metric called LASI. The author goes on to compare the proposed method's performance with existing methods such as LPIPS and others."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper looks to solve an important problem in the domain of computer vision which is to qualify the quality of embedding generated which matches with its perceptual quality, without using any training dataset. \n- Conducts evaluation on the BAPPS dataset with other metrics such as LPIPS, PIM, and MS-SSIM.\n- Achieves comparative and better results in some cases compared to current state-of-art methods.\n- A good amount of side experiment details are shown to better verify the claims presented in the paper.\n-The paper for the most part of it well organized without any obvious typos and a writing structure that is easy to follow."
                },
                "weaknesses": {
                    "value": "- There is minimal discussion about the failure cases using the proposed method. Would be great to have some qualitative results and the probable reason we are seeing the results as we see it.\n- Authors fails to discuss adequately why in some cases other metrics (such as PIM) excel compared to the LASI metric."
                },
                "questions": {
                    "value": "1. In the results presented in Table:1, why does MS-SSIM outdo the author's proposed method for the BAPPS-JND task.. whereas it outperforms it for the BAPPS-2AFC task?\n2. As mentioned in the weakness section, it is imperative that authors present more details qualitative and quantitative about the failure cases seen using the LASI method proposed in the paper.\n3. In Section 5.3: \"....Results indicate LASI can find failure points for LPIPS and vice-versa......\" Can authors elaborate on this point ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2943/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2943/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2943/Reviewer_WqhQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2943/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699473843339,
            "cdate": 1699473843339,
            "tmdate": 1699636237891,
            "mdate": 1699636237891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PtIxYqnJjC",
                "forum": "e4FG5PJ9uC",
                "replyto": "a66I2On7yD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2943/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2943/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the questions regarding the surprising performance of LASI (ours) over neural network baselines. Indeed the performance of LASI (ours) is extremely close to  that of LPIPS and PIM, which is the central point of this paper.\n\nWe hope our rebuttal addresses your concerns and you will consider increasing your score.\n\n---\n\n> There is minimal discussion about the failure cases using the proposed method. Would be great to have some qualitative results and the probable reason we are seeing the results as we see it.\n\n> Authors fails to discuss adequately why in some cases other metrics (such as PIM) excel compared to the LASI metric.\n\n> As mentioned in the weakness section, it is imperative that authors present more details qualitative and quantitative about the failure cases seen using the LASI method proposed in the paper.\n\nThe difference in performance between LASI (ours) and PIM is insignificant (less than $2.3\\\\%$ averaged across categories) leaving very little signal to perform a detailed analysis, which is the main point of the paper.\n\nA qualitative analysis on the examples of the dataset yields no immediately obvious pattern that would explain this small difference in performance.\n\nThe comparison in Table 1 favors alternative metrics by design.\nLASI (ours) is held fixed while the best performing models for other metrics are chosen for each category. \nFor example, for LPIPS we picked the best performing embedding between VGG, SqueezeNet, and AlexNet, for each category, while for PIM we chose between PIM-1 and PIM-5 (whichever scored higher).\nThis cherry picking explains the gap: if we tune LASI (ours) by varying $\\omega$, then the gap can be removed.\n\nThis is exactly the main message of the paper: we can achieve perceptual performance virtually indistinguishable from that of neural network methods while using no training data or deep features, even when neural network methods are allowed to be fine-tuned for each category.\n\n---\n\n> In the results presented in Table:1, why does MS-SSIM outdo the author's proposed method for the BAPPS-JND task.. whereas it outperforms it for the BAPPS-2AFC task?\n\nMS-SSIM does not outdo our method on the JND task.\nThis is only true in one category of the JND task and by less than $1\\\\%$, while in the other category our method is superior by $54.4\\\\%$.\n\nIn all other categories, for both 2AFC and JND, LASI (ours) outperforms MS-SSIM.\n\nThis is highlighted in Table 1, in the last row labeled \"Improv. over MS-SSIM\", where only the last column is negative ($-0.9\\\\%$) and all others are positive.\n\n---\n\n> In Section 5.3: \"....Results indicate LASI can find failure points for LPIPS and vice-versa......\" Can authors elaborate on this point ?\n\nImages in the box labeled \u201cLPIPS loss fixed\u201d have the same LPIPS distance to the reference.\nThis means LPIPS says all images have equal perceptual quality relative to the reference.\nHowever, clearly the bottom row has better perceptual quality than the top row.\nThis is known as a \u201cfailure mode\u201d in the MAD literature.\n\nConceptually, this is analogous to finding adversarial examples for a classifier \u2013 all known perceptual metrics have failure modes. In conducting this experiment, we were hoping to find qualitative explanations for the unexpected performance of LASI, but the results have so far been inconclusive."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2943/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281174782,
                "cdate": 1700281174782,
                "tmdate": 1700449778907,
                "mdate": 1700449778907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M0pugxxbRs",
                "forum": "e4FG5PJ9uC",
                "replyto": "PtIxYqnJjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_WqhQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_WqhQ"
                ],
                "content": {
                    "comment": {
                        "value": ">>>> Authors fails to discuss adequately why in some cases other metrics (such as PIM) excel compared to the LASI metric.\n\n'''.....This cherry-picking explains the gap: if we tune LASI (ours) by varying \u03c9, then the gap can be removed...''' \n1. Are we sure about this, If yes can authors present results for the same and show their method outperforming PIM-based evaluation? \n2. What are the probable disadvantages of choosing (a much refined ) \u03c9, since the authors have refrained from choosing one till this point?\n3. Do authors use the same \u03c9 for a given task while comparing with other counterparts?\n\n\n>>> In the results presented in Table:1, why does MS-SSIM outdo the author's proposed method for the BAPPS-JND task.. whereas it outperforms it for the BAPPS-2AFC task? \nThe results presented and as explained by the author more-or-so require an explanation of why is trend reversed across the same task, across Traditional and CNN, where you see an improvement of about 54% but a degradation of 1% ? How would the author  explain this ?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2943/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591052828,
                "cdate": 1700591052828,
                "tmdate": 1700591052828,
                "mdate": 1700591052828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PcX5aGjgGq",
                "forum": "e4FG5PJ9uC",
                "replyto": "nawgn1Afzh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_WqhQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2943/Reviewer_WqhQ"
                ],
                "content": {
                    "comment": {
                        "value": "I am fairly satisfied with the author's response. I would like to maintain my rating of marginally above accept."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2943/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689609370,
                "cdate": 1700689609370,
                "tmdate": 1700689609370,
                "mdate": 1700689609370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]