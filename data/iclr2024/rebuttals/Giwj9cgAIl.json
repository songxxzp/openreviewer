[
    {
        "title": "Mechanistic Neural Networks"
    },
    {
        "review": {
            "id": "LAJ0MlWXFO",
            "forum": "Giwj9cgAIl",
            "replyto": "Giwj9cgAIl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6069/Reviewer_L7km"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6069/Reviewer_L7km"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes mechanistic neural networks, a framework to learn differential equations. Given an input, these networks output (the parameters of) a differential equation. To solve the differential equations and backprogate through the solution, the authors first leverage old work from the 1960s, which frames numerically solving ODEs as a quadratic optimization problem. Then, they leverage more recent work allowing to solve and backpropagate through the solution of quadratic optimization problems given by neural networks. This way of solving and backpropagating through ODEs is more amenable to parallelization in a GPU than other methods used in neural ODEs. Finally, the authors perform a large amount of qualitative comparisons to show the effectiveness of their method.\n\nBefore starting my review, I do want to highlight that I am not an expert in differential equations nor in neural ODEs. While I found the high-level idea of the paper to be understandable, the details were very confusing. Notation is poorly chosen and the paper reads like a rushed submission. This opinion might nonetheless be a consequence of my own lack of expertise in the topic, so I will happily change my opinion on this if other reviewers who are more knowledgeable in the area disagree."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. I believe the authors are tackling a highly relevant problem, which can be of interest not only to the machine learning community itself, but to physicists and scientists in general who are interested in using machine learning.\n\n2. Improving the computational efficiency of ODE solvers with neural networks in mind is also a relevant problem, and the proposed method seems to perform well."
                },
                "weaknesses": {
                    "value": "3. The notation is quite confusing, and I think the authors use the same symbols to denote different objects in different sections. For example, in section 2, $x$ is the input to the neural network, and $c$, $g$, and $b$ define the differential equation (eq 2). In section 3, $x$ is the variable being optimized over, $c$ is part of the objective, $b$ provides a constraint, and the matrix $G$ is a multiple of the identity that has nothing to do with the $g$ scalars defined in the previous section. The notation used by the authors is highly suggestive of a connection between $x$, $b$, $c$, $g$, and $G$ in these two sections, but unless I hugely misunderstood, there is no connection other than the quantities in section 2 characterize the optimization problem in eq 5.\n\n4. I also found eqs 8 and 9 to not be clear, could you please further explain their role and how they are obtained?\n\n5. The paper emphasizes throughout how it is fundamentally different than neural ODEs, making it sound like neural ODEs cannot be used to solve tasks that mechanistic neural networks can be used for. Yet, most of the empirical comparisons are carried out against neural ODEs.\n\n5. Other than table 1, which compares exclusively solvers, the paper presents no quantitative comparisons against its baselines, only plots qualitatively comparing trajectories.\n\nFinally, I have many minor comments:\n\n- The line after eq 2 has $u^{(k)}$, should this be $u^{(i)}$?\n\n- The line after eq 3 says $c_{d, t}$ and $c_d$, should these be $c_{i, t}$ and $c_i$, respectively?\n\n- Using the same symbol for transpose than for time indexing is also confusing, I'd recommend using ^\\top.\n\n- \\citep and \\citet are used exchangeably throughout the manuscript, please use each one when appropriate only.\n\n- In the abstract, lists are numbered as (i), (ii); whereas throughout the manuscript as (1), (2).\n\n- Missing parenthesis above eq 4.\n\n- Bottom of page 6: \"not enough represent\" -> \"not enough to represent\".\n\n- \"Planar and Lorenz System\" paragraph: \"represent\" -> \"represented\"\n\n- The phrasing \"where we must classify per particles which class out of two it belongs too\" is awkward.\n\n- \"a wing on gives rise\" -> \"a wing gives rise\"\n\n- \"only and to predicting\" -> \"only and to predict\"\n\n- Period missing after the first sentence in sec 5.4."
                },
                "questions": {
                    "value": "What is $\\hat{r}$ in 5.4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698694338507,
            "cdate": 1698694338507,
            "tmdate": 1699636653675,
            "mdate": 1699636653675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NeoIinkzyI",
                "forum": "Giwj9cgAIl",
                "replyto": "LAJ0MlWXFO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6069/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments.\n\n>> I also found eqs 8 and 9 to not be clear, could you please further explain their role and how they are obtained?\n\n**Smoothness Constraint Elaboration**. The purpose of the constraints in eqs 8 and 9 is to enforce the solution of the ODE to be smooth.\n\nThe QP defines ODE solution as variables $u\\_t, u'\\_t, u''\\_t$ over the time grid.\nHowever, these would not be smooth without additional constraints.\n\nWe use Taylor approximations to enforce smoothness between solution at time $t$ and $t+1$.\n\nGiven $u\\_t, u'\\_t, u''\\_t$ we build the second order Taylor polynomial to approximate the function at time $t+1$ with step size $h$ as\n$$\\tilde{u}\\_{t+1} = u\\_t + hu'\\_t + \\frac{1}{2}h^2 u''\\_t. $$\nNext we say that the difference between the approximation and the solution at $t+1$ should be $\\epsilon$.\n$$\\tilde{u}\\_{t+1} - u\\_{t+1} = \\epsilon,$$\nwhich is the first equation in 8 which enforces the function to be smooth by minimizing $\\epsilon^2$ which is the optimization objective.\n\nSimilarly, the next equation in 8 enforces the first derivative to be smooth using the Taylor approximation.\n\nThe constraints in 8 say that we should be able to approximate the next time step $t+1$ using values at $t$.\nThe constraints in 9 go backward using values at $t+1$ to approximate values at $t$.\nWe find both constraints to be necessary to get proper solutions.\n\n\n>>The paper emphasizes throughout how it is fundamentally different than neural ODEs, making it sound like neural ODEs cannot be used to solve tasks that mechanistic neural networks can be used for. Yet, most of the empirical comparisons are carried out against neural ODEs.\n\n**Neural ODE Comparison**. We emphasize the difference of our _approach_ in combining Neural networks and ODEs which is very different from that of Neural ODEs.\nNevertheless, there is an overlap in the types of tasks for which Neural ODEs and MNN can be used, such as modeling dynamical systems and processes.\nWe compare against Neural ODE for such tasks (Classification, Vibration prediction, N-body prediction) in Sections 5.2,5.3,5.5.\nMNNs have better performance on these tasks because the underlying solver performs better at learning dynamical processes than the solver type used in Neural ODE (see Section C.7).\n\nOn the other hand, MNNs are also designed for discovery. In discovery experiments we compare against a SINDy (Section 5.1) which is a popular sparse discovery method for ODEs.\n\nFor our PDE solving architecture we compare against FNO-based and similar methods.\n\n>> Other than table 1, which compares exclusively solvers, the paper presents no quantitative comparisons against its baselines, only plots qualitatively comparing trajectories.\n\n**Quantitative Results**. We point out that there are several other quantitative comparisons in the paper. \n\nIn Figure 4, we plot loss curves (left) and prediction error (second from left) which are quantitative and compare against Augmented Neural ODE and Second Order Neural ODEs.\n\nIn Table 2 in the appendix we compare the training and evaluation results on the 2-body and 10-body problem against Neural ODE.\n\nIn Table 3 in the appendix we compare the QP solver against the RK4 solver from torchdiffeq, where we look at the ability of the two solvers to fit long sinusoidal waves (300 and 1000 steps) and their respective training times.\n\nFinally in Table 1 (Section 5.5) we give quantitative comparisons for a PDE solving architecture against other well-known deep learning based PDE solving architectures.\nThese methods are not solvers in the traditional sense of differential equations solving but are learning-based approaches.\n\nThe above quantitative results are in addition to the host of results that we have shown for discovering governing equations (Sections 5.1, 5.4, C.1.2) and prediction (Sections 5.2,5.3,5.5).\n\n**Notation**.\n>>The notation is quite confusing, and I think the authors use the same symbols to denote different objects in different sections ...\n\nUnfortunately, describing our approach has to combine methods from differential equations, optimization and deep learning where each has standard notation and some overlap has been difficult to avoid.\nWe have made many of the changes suggested and appreciate the reviewer's suggestions for improvement.\n\n>> What is $\\hat{r}$ in 5.4?\n\n$\\hat{r}$ is the radius unit vector that gives the direction of force in Newton's law of gravitation.\nWe have added a clarification.\n\nFinally we appreciate the typographical corrections pointed out by the reviewer and are grateful for the review in general."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973824858,
                "cdate": 1699973824858,
                "tmdate": 1699973824858,
                "mdate": 1699973824858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7PJGafDIAl",
                "forum": "Giwj9cgAIl",
                "replyto": "NeoIinkzyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6069/Reviewer_L7km"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6069/Reviewer_L7km"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their reply and the updated draft. While some of the misunderstandings I had have been clarified, I still find that the exposition can be made clearer, a point that is also echoed by reviewer 5iBu. I will thus keep my score as is."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578356919,
                "cdate": 1700578356919,
                "tmdate": 1700578356919,
                "mdate": 1700578356919,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "72hqUPRvew",
            "forum": "Giwj9cgAIl",
            "replyto": "Giwj9cgAIl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6069/Reviewer_5iBu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6069/Reviewer_5iBu"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Mechanistic Neural Networks (MNNs), a neural module that represents the evolution of its input data in the form of differential explicit equations. Unlike some traditional neural networks that return vector-valued outputs, MNNs output the parameters of a mechanism in the form of an explicit symbolic ordinary differential equation. MNNs employ a new parallel and differentiable ODE solver design that can solve large batches of independent ODEs in parallel on GPU."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The introduction of Mechanistic Neural Networks provides a new approach to learning differential equations from the evolution of data."
                },
                "weaknesses": {
                    "value": "1. The paper's clarity is wanting. For example, around eq(2), there's an inconsistency in notation with both $c_i(t;x)$ and $c_i(t)$ being used. Which of these is the intended notation? Additionally, there's no explicit description or definition of $b(t;x)$.\n2. On page 2, the statement \"In general, one ODE is generated for a single data example $x$ and a different example $x'$ would lead to a different ODE\" is made. Could the authors elucidate why this is a characteristic of the modeling approach presented in eq(2)?\n3. If a new instance $x'$ necessitates retraining the model, wouldn't it be more streamlined to directly learn a neural ODE through parameter optimization, bypassing the need for coefficients as functions of $x$?\n4. The paper's approach to solving any nonlinear ODEs using equality-constrained quadratic programming must have gaps. These gaps aren't clearly addressed. Relying on such an algorithm to solve any ODE without theoretical guarantees is precarious. A more transparent discussion on potential limitations is needed.\n5. The methodology for handling nonlinear ODEs, as presented on page 4, lacks clarity and could benefit from a more detailed exposition.\n6. The literature review in section 4 seems outdated, with the most recent references dating back to 2020. A comprehensive literature survey, including more recent and relevant baselines, would strengthen the paper's context. For example for the ODE modelling, [1][2] may be included.\n7. The term $\\Theta \\xi$ on page 6 is introduced without clear definition or context. Could the authors provide clarity on this?\n8. The paper delineates two primary components: the learning of the ODE and its subsequent solving. However, the experimental section seems to lack comprehensive ablation studies that convincingly demonstrate the efficacy of each individual component.\n\n[1] Kidger, Patrick, et al. \"Neural controlled differential equations for irregular time series.\" Advances in Neural Information Processing Systems 33 (2020): 6696-6707.\n\n[2] Morrill, James, et al. \"Neural rough differential equations for long time series.\" International Conference on Machine Learning. PMLR, 2021."
                },
                "questions": {
                    "value": "Please clarify the issues raised in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741472304,
            "cdate": 1698741472304,
            "tmdate": 1699636653566,
            "mdate": 1699636653566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S1Fq131Euc",
                "forum": "Giwj9cgAIl",
                "replyto": "72hqUPRvew",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "We thank and appreciate the reviewer's comments and suggestions.\n\n>>The paper's clarity is wanting. For example, around eq(2), there's an inconsistency in notation with both $c\\_i(t;x)$ and  $c\\_i(t)$ being used. Which of these is the intended notation? Additionally, there's no explicit description or definition of $b(t;x)$ .\n\nThe notation $c\\_i(t;x)$ was intended to make dependence of the ODE coefficients on neural network input $x$ explicit.\nFor a fixed datum $x$, we use $c\\_i(t)$.\nTo avoid confusion, we have made the notation consistent in the updated draft and have given a definition of $b(x;t)$, defined similarly to the other coefficients.\n\n>> On page 2, the statement \"In general, one ODE is generated for a single data example and a different example would lead to a different ODE\" is made. Could the authors elucidate why this is a characteristic of the modeling approach presented in eq(2)?\n\n**Per-instance ODEs**. The ODE in equation 2 depending on the input datum $x$ means that we have **_one ODE per input datum_** in the dataset in the most general case. For instance, if wanted the MNN can derive a different governing ODE for each new input sequence trajectory, initial condition, etc. Of course, if wanted the MNN can also be defined to derive a *single governing ODE* from all the data in the dataset, e.g when there is a single governing equation in the discovery experiments in Sections 5.1 and 5.4.\n\nThis draws a contrast with Neural ODEs, which are only able to model a global ODE for an entire dataset.\n\n>> If a new instance $x'$ necessitates retraining the model, wouldn't it be more streamlined to directly learn a neural ODE through parameter optimization, bypassing the need for coefficients as functions of $x$ ?\n\n**Retraining**. It is **_not correct that a new instance $x'$ requires retraining the model_**.\nIn our model a neural network produces the parameters $c\\_i, b, \\phi\\_k$ (equation 2) of an ODE for a given input $x$.\nA dataset can then be modeled by a family of ODEs by training over the dataset.\nAs we mention above, MNN is more general than neural ODE and is able to model per-instance ODEs without retraining, simply returning the datum-specific coefficients by conditioning on the datum.\nThis is indicated by the dependence of the produced ODE on the input datum $x$.\n\n>> The paper's approach to solving any nonlinear ODEs using equality-constrained quadratic programming must have gaps. These gaps aren't clearly addressed. Relying on such an algorithm to solve any ODE without theoretical guarantees is precarious. A more transparent discussion on potential limitations is needed.\n\n**Theoretical Analysis**. We have added an error analysis section in the updated draft in appendix A.1 for linear second order ODEs with the QP solver\n$$c\\_2 u'' + c\\_1 u' + c\\_0 u = b.$$\n\n**_Sketch_**. The analysis proceeds by estimating total error introduced by the error in the second order Taylor approximation $O(h^3)$ and the error from the constraint $\\epsilon$. \nIn the first step the function and derivative values are known from the initial value constraints leading to an error of $O(\\epsilon + h^3).$\nFor the next step we use the approximate function and derivative values, plugged into the ODE, and show that the total error over $N$ steps remains bounded by $O(N(\\epsilon + h^3))$, under the assumption that $\\frac{c\\_1}{c\\_2}$ and $\\frac{c\\_0}{c\\_2}$ are bounded by $O(\\frac{1}{h^2})$ and $O(\\frac{1}{h}) $, respectively.\n\nThe analysis shows that the method has a local error of $O(\\epsilon + h^3)$, where $\\epsilon$ is the QP error variable and $h$ is a fixed step size.\nFor an $N$ step grid, the analysis shows that this error becomes\n$O(N(\\epsilon + h^3))$. \n\nIn particular for $\\epsilon = h^3$, which is reasonable in practice, and  $N=1/h$ the global error simplifies to $O(h^2)$.\nThis error depends on the number of terms of the Taylor expansion and could be further reduced by using central difference approximations for all derivatives, investigation of which we leave to future work.\n\nThe analysis implies that the cumulative error becomes potentially large only when $\\frac{c0}{c2}$, $\\frac{c1}{c2}$ are very large:\nIn little omega notation, when $\\frac{c0}{c2}$ is $\\omega(\\frac{1}{h^2})$ and $\\frac{c1}{c2}$ is $\\omega(\\frac{1}{h})$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972879043,
                "cdate": 1699972879043,
                "tmdate": 1699973620679,
                "mdate": 1699973620679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c69yKAhss6",
                "forum": "Giwj9cgAIl",
                "replyto": "72hqUPRvew",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2/2"
                    },
                    "comment": {
                        "value": ">>The methodology for handling nonlinear ODEs, as presented on page 4, lacks clarity and could benefit from a more detailed exposition.\n\n**Non-linear ODEs**. We have added further clarification in the appendix A.2 with an example non-linear ODE.\n\nBriefly, to solve an equation like \n\\begin{equation}\nc\\_2(t) u'' + c\\_1(t) u' + d(t) u^3 = b,\n\\end{equation}\nwhere $u^3$ is non-linear functions of $u$, we solve the following two equations.\n\\begin{align}\nc\\_2(t) u'' + c\\_1(t) u' + d(t) \\nu= b\\\\\\\\\nu^3 = \\nu\n\\end{align}\n\nWe solve the first equation by the QP solver by creating another set of variables $\\nu\\_t$ in addition to $u\\_t$.\nWe add equation constraints that include both types of variables and include smoothness constraints for $\\nu\\_t$.\n\nSolving the program gives a solution for $u\\_t$ and $\\nu\\_t$.\nWe enforce the non-linearity in the neural network loss function by add an MSE term $\\sum_t (u\\_t^3 - \\nu\\_t)^2$ which imposes that constraint that $u^3 = \\nu$.\nThis shows how non-linear relationships in the ODE terms are learned.\n\n\n>> The literature review in section 4 seems outdated, with the most recent references dating back to 2020. A comprehensive literature survey, including more recent and relevant baselines, would strengthen the paper's context. For example for the ODE modelling, [1][2] may be included.\n\n**References**. Thank you for the suggestion about more recent references; we will consider this for the final draft. \nThis is partly due to the fact that there hasn't been much significant development in Neural ODE style modeling over the past few years.\nThe reviewer's suggested references are also from 2020 and 2021.\n\nWe demonstrate that our method overcomes significant limitations of the Neural ODE framework and adds directions such as explicit equation discovery which were not possible in the Neural ODE method, in addition to faster parallel solving and better learning.\n\n>> The term $\\Theta \\xi$ on page 6 is introduced without clear definition or context. Could the authors provide clarity on this?\n\n**SINDy Notation**. In Section 5.1, we assume and follow the notation used by Brunton et al (2016) that introduces the SINDy method.\nFor clarity and completeness we have included further details in the appendix A.3 with a reference in the main text.\n\nBriefly $\\Theta$ is a data matrix that applies polynomial basis functions to time-series input $x\\_0, x\\_1 \\ldots x\\_t$.\n\\begin{equation}\n\\Theta = \n\\begin{bmatrix}\n1 & x\\_0 & x\\_0^2 & \\ldots\\\\\\\\\n1 & x\\_1 & x\\_1^2 &\\ldots\\\\\\\\\n\\vdots & \\vdots & \\vdots \\\\\\\\\n1 & x\\_t & x\\_t^2 &\\ldots\n\\end{bmatrix}\n\\end{equation}\n\n$\\xi$ is a sparse vector that selects a small number of basis functions. So ODE like \n$$ x'  = x + x^2$$\nmay be represented with the appropriate choice of $\\xi$ as $ x'  = \\Theta\\xi$.\n\n>>The paper delineates two primary components: the learning of the ODE and its subsequent solving. However, the experimental section seems to lack comprehensive ablation studies that convincingly demonstrate the efficacy of each individual component.\n\nOur original submission included the following ablations in the appendix (referenced in Section 5.7) due to lack of space.\n\n**RK4 Solver Comparison**. In Section B.2 we validate the QP solver and compare the solution with the RK4 solver solution with the SciPy Python package (Figure 7).\n\n**Non-linear ODEs**. We show learning a non-linear ODE also in B.2 and Figure 9.\n\n**Step Learning**. In Section B.2 and Figure 8 we show that the method learns step sizes during training.\n\n**Learning with Noise**. In Section B.3 we show that the method is able to learn ODEs from noisy data both with time-dependent and time-independent coefficients.\n\n**RK4 Learning and Timing Comparison**. In Section C.7 we show a comparison with the RK4 solver from torchdiffeq for learning longer noisy sine waves with 300 and 1000 steps and compare the times.\nThe results (Table 3. Figures 14,15) show that the QP solver is able to learn significantly better fits and is up to 200x times faster in this experiment.\n\nHowever, if there are suggestions for any additional ablation experiments, please let us know."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973023895,
                "cdate": 1699973023895,
                "tmdate": 1699973023895,
                "mdate": 1699973023895,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3ou92j7qhm",
                "forum": "Giwj9cgAIl",
                "replyto": "c69yKAhss6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6069/Reviewer_5iBu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6069/Reviewer_5iBu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses which mitigate my concerns. I acknowledge the paper's novelty and appreciate the value of your insights to the community. To meet the rigorous standards of ICLR, I however agree that a thorough review of notations, descriptions, and theoretical guarantees is essential. Acknowledging limitations is a key aspect of scholarly work, and I suggest the authors clearly articulate these in their revision. Further refinement is indeed necessary. I recommend submitting an enhanced version to upcoming conferences or journals for consideration.\n\nI keep my current score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584323782,
                "cdate": 1700584323782,
                "tmdate": 1700584323782,
                "mdate": 1700584323782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5jKJIo2t3z",
            "forum": "Giwj9cgAIl",
            "replyto": "Giwj9cgAIl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6069/Reviewer_Uvxo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6069/Reviewer_Uvxo"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors introduce a methodology wherein they learn a neural network to output the coefficients of an ODE instead of the solution in itself, hence making the output of the neural network more interpretable. \n\nThe key idea is that for the ODE written in the form of equation 2 in the paper (which is a general form that should comprise of both linear, nonlinear and time-varying PDEs), the authors propose to learn a neural network that outputs the coefficients of the ODE. The authors also point to the fact that their method can also be used to infer the (temporal) discretization of the ODE from observation data as well. \n\nThe learning of the neural networks approximating the ODE coefficients is done by solving a system of equations subject to quadratic constraints. The number of parameter that the network needs to approximate are determined by the number of time-grids and the order of the ODE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The work provides a very interesting methodology a single governing equation of a linear/nonlinear system in an interpretable manner. \n\nThe authors show through their experiments that they are able to learn nonlinear ODEs (unlike previous work like SINDy). \n\nThe methodology also enables the authors to infer quantities like the temporal discretization and also the initial conditions gives a set of trajectories which is quite useful/interesting."
                },
                "weaknesses": {
                    "value": "While the methodolgy is pretty interesting, I wonder how it scales with the number of dimensions in the input data, and for more complex systems like Navier-Stokes. Some discussion related to it would be useful! \n\nThe results on PDEs like Darcy Flow show that the network is not as much better (at least in performance) as compared to FNO baseline."
                },
                "questions": {
                    "value": "- I am a bit unclear about how the authors are able to parallelize in terms of the sequences, I understand the parallel batchwise training aspect of the training. Is is that since we have ground truth training data, and the we can write down the forward and backward Taylor approximations, we get different sets of equations that are solved in parallel?\n- It seems that the authors are solving a relatively complex system of equations stemming from the discretization of the ODEs. Are there any set of equations that may not be solvable due to the given methodology."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791963632,
            "cdate": 1698791963632,
            "tmdate": 1699636653450,
            "mdate": 1699636653450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eCpj6k5FaF",
                "forum": "Giwj9cgAIl",
                "replyto": "5jKJIo2t3z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6069/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for reviewing and appreciating our work. We address some of the points and questions raised below.\n\n\n>>I wonder how it scales with the number of dimensions in the input data, and for more complex systems like Navier-Stokes.\n>>\n>>The results on PDEs like Darcy Flow show that the network is not as much better (at least in performance) as compared to FNO baseline.\n\n**PDE**. In this paper we present a *general* framework for learning the governing equation from data and not a *specific* solution to a specialized class of PDEs. Naturally, when narrowing down the focus to PDE forecasting, a specialized solver like FNO works very well.\n\nMNN are a general and encompassing framework compared with the overarching state-of-the-art of ODE-related ML methods where\n\n1. Neural ODEs model sequences but cannot discover governing equations,\n2. SINDy discovers governing equations but is not with neural networks so it cannot scale and it cannot discover non-linearly entangled governing equations,\n3. FNO can forecast future trajectories but is purely data driven, has no notion of governing equations.\n\nMNNs cut across these types of methods and in many cases perform better.\n\n**Scaling**. Scaling to large dimensions especially for PDEs depends on how effectively sparse matrix methods can be used for solving linear equations, since QP constraint matrices are highly sparse.\nCurrently we use dense matrix factorizations which are fast and have manageable memory usage for the problems we have considered. \nWe plan to investigate sparse matrix methods for PDEs in follow-up work.\n\n >> I am a bit unclear about how the authors are able to parallelize in terms of the sequences, I understand the parallel batchwise training aspect of the training. Is is that since we have ground truth training data, and the we can write down the forward and backward Taylor approximations, we get different sets of equations that are solved in parallel?\n\n**Parallelization**. Yes, since we can write the discretized ODE over an entire time sequence in terms of the constraints, we only have to solve the constrained optimization to get the entire solution over the grid without stepping through time.\n\nOne minor point is that the parallel solution does not depend on the ground-truth data and is a feature of the forward pass.\n\n>>It seems that the authors are solving a relatively complex system of equations stemming from the discretization of the ODEs. Are there any set of equations that may not be solvable due to the given methodology.\n\nSolving the equality-constrained quadratic program only requires solving linear equations so as long as the constraints are not inconsistent (such as contradictory initial conditions) we do not encounter any problems.\n\nHowever, as we mention in Section 3.1 regarding non-linear ODEs, the QP method on its own does not solve non-linear ODEs (since the constraints are linear).\nFor this we introduce new variables representing non-linear terms (say a squared term) in the QP and solve the QP.\nThe the solution corresponding to the non-linear variables is related to the original in the MSE loss function using the required non-linear function (say the square function).\n\n\nWe thank the reviewer again for the review and appreciation and hope that we were able to answer the reviewer's questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972266102,
                "cdate": 1699972266102,
                "tmdate": 1699972266102,
                "mdate": 1699972266102,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]