[
    {
        "title": "A trainable manifold for accurate approximation with ReLU Networks"
    },
    {
        "review": {
            "id": "xSGqpMCV9u",
            "forum": "S4wo3MnlTr",
            "replyto": "S4wo3MnlTr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7610/Reviewer_FQgq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7610/Reviewer_FQgq"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the learnability of a simple neural network architecture. Specifically the problem of fitting a quadratic function is of interest. This paper proposes a compositional network architecture where each base component is composed by four simple linear or nonlinear activation functions, and that the neural network architecture is replication of the base component. The paper demonstrates its learnability, in other words the power of this architecture that approximates a quadratic function."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think this paper is readable and intuitive. Although it discusses a simple model, it is astonishing that the application of a single model can fit a different function with fast approximation rate. The content is precise and self-consistent, and there are many figures and discussions that help readers go through the approximation process. The proof is mathematically correct."
                },
                "weaknesses": {
                    "value": "I think it would be great to discuss this paper in a bigger picture, for example, how does the power of approximation of this model architecture compare with other kinds of neural networks, especially how does it present a tradeoff between learnability and simpleness, and why is this architecture of interest. Since today's neural networks are complicated and they perform well with a lot of reasons, while this unusual neural network is seldomly used, how is the proof in this paper share the light on the analysis of other types of neural network models, and how does it guide the selection of neural network architectures, training algorithms, and the simple complexities required to train a model without overfitting etc.? With the answers to the above questions, I think the importance of this paper is better presented."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698016665306,
            "cdate": 1698016665306,
            "tmdate": 1699636923394,
            "mdate": 1699636923394,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "TkwlyYdBFb",
            "forum": "S4wo3MnlTr",
            "replyto": "S4wo3MnlTr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7610/Reviewer_xLCZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7610/Reviewer_xLCZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a trainable manifold with ReLU networks for function approximations, by reparameterizing the ReLU networks. This work is built upon the previous work on constructing the weights of the ReLU networks such that the network can generate triangle waves (Telgarsky 2015) which can be utilized to show an exponential separation between deep and shallow ReLU networks. The trainable parameters of the networks are $a_i$ which control the center of the triangle and $s_i$ which is the coefficient of the depth-$i$ composition of the triangle waves. Hanin and Rolnick, 2019 shows that the number of expected number of linear regions in a randomly initialized ReLU network does not scale exponentially with depth. Thus, the benefit of using the reparameterization (the authors proposed in this work) is that the output will have an exponential number of line segments. The authors show by their experiments that their initialization is able to produce smaller MSE error."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I personally find this idea of initializing the network to be on a manifold with exponential number of linear region novel and interesting, which connects theory and practice."
                },
                "weaknesses": {
                    "value": "As for the current manuscript, the optimization and generalization property of such initialization is not sufficiently explored. Right now, only synthetic experiments are provided on some simple setting. Based on those simple setting, it seems that we can optimize the manifold, however, how hard it is to optimize such manifold in the real world setting is worth further studying. Further, in practice, the goal of people training a network is to hope the network can generalize. It is worth further investigation on the generalization property of the network. Since the network output has exponentially many linear region (and thus more capacity to fit), one may suspect that the network is highly vulnerable to input noise such that network overfits the data and is not able to generalize."
                },
                "questions": {
                    "value": "I find the second paragraph of section 4.1 confusing. In table 1, what is the difference between default network and stage 3 (GD only)? The default network is Kaiming initialized but the stage 3 network is initialized with exponential number of linear region?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7610/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7610/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7610/Reviewer_xLCZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698522197326,
            "cdate": 1698522197326,
            "tmdate": 1699636923257,
            "mdate": 1699636923257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x9A3QnqWXw",
                "forum": "S4wo3MnlTr",
                "replyto": "TkwlyYdBFb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7610/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "You're definitely correct in your point about the overfitting and generalization. The reason most of the experiments here are focused on interpolating perfect data is that I originally approached this paper in light of some of the theoretical literature where their main concern is whether certain representations exist in the set of neural networks. But given that this paper is concerned with whether efficient representations are learnable, I should probably place a greater emphasis on this.\n\nIn the experiments in this paper, some of the functions become very jagged if the scaling parameters are not controlled. This is related to the problem of overfitting, the difference is that they have settled into a local minimum that is ineffective on the training set. The differentiability constraint in training eliminates these instances (the starting locations are all the same). It seems to provide an amount of protection from this issue by producing a starting point for standard gradient descent that is biased towards smoothness. It could be interesting to see if that behavior carries over to other settings"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719185703,
                "cdate": 1700719185703,
                "tmdate": 1700719185703,
                "mdate": 1700719185703,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eZ3JDDGwS0",
            "forum": "S4wo3MnlTr",
            "replyto": "S4wo3MnlTr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7610/Reviewer_jsTK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7610/Reviewer_jsTK"
            ],
            "content": {
                "summary": {
                    "value": "A ReLU network trained with gradient descent in the parameter space does not efficiently leverage the usage of the linear segments given by ReLUs. To address this issue, the paper proposes a new reparameterization such that the output of a ReLU network is guaranteed to output a sawtooth-like function with exponentially many more linear segments as depth is increased. Since this procedure also creates many more discontinuities on the optimization landscape, a differentiability constraint is added to the reparameterization to steer the solution away from bad local minima. Such a constraint is derived from analyzing the derivative of an infinite-layer ReLU network. A theorem about this differentiability constraint is proved. Computer simulations are also provided to evaluate the proposed reparameterization. Several nonlinear functions are tested, and the results are promising."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Originality: Most existing approximation results rely on the base function $x^2$, however, the structure of this base function can be destroyed by gradient descent and then hinder the approximation performance. This paper provides a novel optimization approach that constrains the optimization on a low-dimensional manifold such that superior approximation performance can be achieved. This new method is new in the sense that the exponentially many linear segments of a sawtooth-like function are preserved during training. Furthermore, along with a novel differentiability constraint, bad local minima can be avoided.\n\n- Quality and clarity: This paper gives a comprehensive presentation on the approximation results of ReLU networks based on $x^2$. Using that argument, the paper seamlessly leads the read to understand how exponentially many linear segments can be preserved by designing a low-dimensional manifold. The part where the authors limit the manifold further with a differentiability constraint is also well presented. The whole paper is fairly well-written and easy to follow. I very much enjoy reading the paper.\n\n- Significance: This new low-dimensional manifold idea greatly improves the optimization accuracy of ReLU networks for nonlinear function approximation. The improvements are significant, and the results are well supported by the theoretical justification in the paper."
                },
                "weaknesses": {
                    "value": "1. The proposed optimization approach can be more useful if the reparameterization argument can be extended to high dimensional problems. It would be clearer for the reader if the authors can describe the main difficulties of such extension.\n\n2. The nonlinear functions picked in Section 4.2 seem to be simple. It would be more convincing if the authors could also show superior performance for complicated nonlinear functions. Is preserving the linear segments truly beneficial for learning general nonlinear functions? I think a discussion of the function family that is friendly to the proposed method is important."
                },
                "questions": {
                    "value": "1. Is the proposed method sensitive to the selection of optimizer? Can SGD yield the same performance?\n\n2. How many bits are used to represent the weights in the ReLU network?\n\n3. The performance is reported in mean and min. How is the worst-case scenario? Perhaps adding the max metric in Appendix.\n\n4. Please add a subtitle to Figure 5 to indicate which of them is using the differentiable manifold.\n\n5. Section 2.1, there is a typo in \u201cSince each layer converts \u2026\u201d\n\n6. Section 3.1 page 5, where is the definition of W(x)?\n\n7. Lemma 3.3 For all x\u2026\n\n8. Given the fractal nature of the sawtooth-like function, would the proposed method demonstrate superior performance on some fractal functions? For example, the Cantor function."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7610/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7610/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7610/Reviewer_jsTK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833604996,
            "cdate": 1698833604996,
            "tmdate": 1699636923136,
            "mdate": 1699636923136,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c55NFL6Iaq",
                "forum": "S4wo3MnlTr",
                "replyto": "eZ3JDDGwS0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7610/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "I'm so glad you liked the paper! this was actually my first time submitting to a conference. It got buried in the appendix, but there's a proof that the differentiability constraint forces the function represented by the network to be convex. While this constraint is important for ensuring consistent performance, it also takes away a great deal of flexibility. This is part of why I had difficulty extending this method to more complex problems. \n\nFully connected ReLU networks are great at representing fractals. If I remember correctly, the Perekrestenko et al. citation gives a novel approximation to the Weierstrass function that exponentially improves upon previously existing methods. I think it may be the case that the ability of neural networks to represent fractals could be leveraged to provide a more flexible replacement for the differentiability constraint in this paper. Perhaps non-convex functions require nowhere-differentiable representations, but somehow this can be controlled so as to prevent jaggedness in the infinite-depth limit.\n\nThe iterated composition of triangle waves is also a textbook example (I just saw it in a textbook the other day in fact) of a chaotic system. The \"spikes\" in the limit are dense in the input domain, creating a sensitivity to small perturbations in the input. This is important because it allows the slopes of two nearby input points to be distinguished from each other. In a deep enough network, one will trigger a different neuron activation pattern. Two inputs producing the same activation pattern lie in a linear region of the represented function. And it would likely be undesirable to have linear regions show up in your infinitely deep representation.\n\nThis is part of why leveraging depth exponentially in a neural network is a challenge. It requires a composition of chaotic features to be summed (possibly in a non-differentiable manner) that approaches nicely behaved functions in the limit."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717713583,
                "cdate": 1700717713583,
                "tmdate": 1700718084275,
                "mdate": 1700718084275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qvsflAZ92P",
            "forum": "S4wo3MnlTr",
            "replyto": "S4wo3MnlTr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7610/Reviewer_PUUB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7610/Reviewer_PUUB"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the approximation ability of ReLU networks by encoding complex operations into ReLU networks using smaller base components. The derivation in this paper can produce networks with exponentially many piecewise-linear segments. The author claims that Their construction can enable the training process to overcome drawbacks associated with random initialization. The authors conduct the experiments on some synthetic datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well organized and has clear illustration figures.  \n\n- This paper only requires four neurons per layer to approximate.\n\n- According to Tables, the constructed neuron networks can achieve better performance when approximating the simple function such as $y=x^3$, $y = x^{11}$, $y = \\sin(x)$, $y = \\tanh(x)$."
                },
                "weaknesses": {
                    "value": "- The presentation of this paper is not clear. \n\n- It is unclear how to apply the technique to real applications. \n\n- How to understand Theorem 3.4?"
                },
                "questions": {
                    "value": "- The author claims that their results are minimally probabilistic and thus can prevent weight collapse. Why? How to understand this claim, and which theorem supports it? The authors may better add some comments or remarks about that. \n\n- The authors propose a new architecture, but how to initialize it, can it be potentially extended to other structures like CNN or transformer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699207454963,
            "cdate": 1699207454963,
            "tmdate": 1699636923005,
            "mdate": 1699636923005,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]