[
    {
        "title": "Cascading Reinforcement Learning"
    },
    {
        "review": {
            "id": "IWhak2vAW3",
            "forum": "KjOAHlKMF5",
            "replyto": "KjOAHlKMF5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2902/Reviewer_6goX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2902/Reviewer_6goX"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the Cascading Bandits problem over a Markov Decision Process, where time is divided into episode of fixed length $H$, and the attractiveness of items (which determined the expected state transitions and rewards) is dependent on a state at the current step in the episode. At each step in the episode, the system gets to observe the current state and proposed a ranked list of items and the environment provides a click on one of the items according to the cascading click model. For this problem, the paper provides a method for computing the optimal offline strategy when all distributions are known and an introduces the CascadingVI algorithm along with a regret bound of $O(H\\sqrt{HSNK})$, where $H$ is the episode length, $S$ is the number of states, $K$ the number of episodes and $N$ is the number of individual items available to rank. The performance of the proposed algorithm is compared numerically with a baseline algorithm produced by adapting an existing algorithm to the combinatorial space, referred to here as AdaptRM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- generalizes the cascading bandit problem to MDPs\n- provides numerical experiments, even though, in my opinion the baseline is too weak\n- provides theoretical guarantees and proofs of correctness\n- generally well written, despite the convoluted algorithms I could navigate my way around the paper."
                },
                "weaknesses": {
                    "value": "- The significance of the contribution is not substantial enough to justify acceptance into the venue in my opinion. One of the main contributions is providing an algorithm that does not scale in complexity (both sample or computational) with the space of all possible rankings. I believe we have well-known recipes for how such algorithms can be formulated since the papers on Cascading Bandits from 2015 (Kveton et. al, Combes et al.). Providing a solution scaling with the number of items is expected in my opinion and not a surprising contribution (we know that estimating individual CTRs is enough to construct optimal rankings, we don't need to keep statistics on each individual ranking). This also leads me to believe the baseline used in the numerical experiments is weak. \n- The contribution related to the generalization to MDPs is not opening a significant amount of new doors. In my opinion, this generalization does not provide further theoretical insights and has limited additional practical relevance.\n- The experimental section use a setting that is too simplistic (very few items and states) and a weak baseline, in my opinion, thus not being representative enough of what we can expect from this algorithm in practice."
                },
                "questions": {
                    "value": "On line 8 in Algorithm 2, what is the complexity of computing $E_{s'\\sim p^k}[ .... ]$? \n\nRegarding BestPerm, to me it feels like the biggest hurdle is the computational complexity of computing the value functions $w$, which BestPerm assumes is an input. How does your proposed algorithm navigate this difficulty? \n\nI would also like to see more intuition regarding the exploration bonus $b^{k, pV}(s, a)$, in particular: how is the optimism of $\\overline{V}^k_h(s)$ connected to the uncertainty in the estimates of state transition probabilities $p$? The algorithm is already fairly convoluted and it is hard to see how we can construct optimistic estimates of the value functions in light of estimation noise in the state transition probabilities. I believe it is very valuable to clearly articulate such insights in the main body of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2902/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2902/Reviewer_6goX",
                        "ICLR.cc/2024/Conference/Submission2902/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698059628881,
            "cdate": 1698059628881,
            "tmdate": 1700563556164,
            "mdate": 1700563556164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rc6MDLxCzV",
                "forum": "KjOAHlKMF5",
                "replyto": "IWhak2vAW3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6goX (Part 1/3)"
                    },
                    "comment": {
                        "value": "Thank you very much for your time and effort in reviewing our paper! We have incorporated your comments in the revised version, and highlighted our revision in *red* font.\n\n\n**1. Contribution of our Generalization to Cascading MDPs**\n\nWe emphasize that our generalization to cascading MDPs is very different from prior cascading bandit works [Kveton et al. 2015; Combes et al. 2015], since we face a unique challenge on **how to efficiently compute the optimal permutation**.\nTo tackle this challenge, we make use of special properties of the objective function for cascading MDPs, and develop a novel and efficient oracle BestPerm using a dynamic programming approach for combinatorial optimization. \n\nSpecifically, prior cascading bandit works [Kveton et al. 2015; Combes et al. 2015] consider finding the optimal subset of items as follows:\n$$\n\\max_{A \\in \\mathcal{A}} \\quad f(A) = \\sum_{i=1}^{|A|} \\prod_{j=1}^{i-1} \\Big( 1-q(A(j)) \\Big) q(A(i)) = 1- \\prod_{i=1}^{|A|} \\Big( 1-q(A(i)) \\Big) . \\quad Eq. (i)\n$$\nThis objective function only depends on the attraction probabilities of items $q(A(i))$. Thus, prior works **only need to select $m$ items with the maximum attraction probabilities**, which do not involve computational difficulty.\n\nIn contrast, our cascading MDP problem considers  finding the optimal ordered subset of items as follows (for step $h=H,H-1,\\dots,1$):\n$$\n\\max_{A \\in \\mathcal{A}} \\quad Q^{*}_h(s,A) \n$$\n\n$$\n\\ \\qquad = \\sum_{i=1}^{|A|} \\prod_{j=1}^{i-1} \\Big( 1-q(s,A(j)) \\Big) q(s,A(i)) \\Big( r(s,A(i))+p(\\cdot|s,A(i))^\\top V^{*}_{h+1} \\Big) .\n$$ \n\nOur objective function depends on both the attraction probabilities $q(s,A(i))$ and the expected future rewards $r(s,A(i))+p(\\cdot|s,A(i))^\\top V^{*}_{h+1}$. \n\nThis requires us to **solve a combinatorial optimization problem** for each $s \\in \\mathcal{S}$ as follows:\n$$\n\\max_{A \\in \\mathcal{A}} \\quad f(A,q,w) =  \\sum_{i=1}^{|A|} \\prod_{j=1}^{i-1} \\Big( 1-q(A(j)) \\Big) q(A(i)) w(A(i)) , \\quad Eq. (ii)\n$$\nwhere $q(a):=q(s,a)$ and $w(a):=r(s,a)+p(\\cdot|s,a)^\\top V^{*}_{h+1}$ \n\n(in the online RL process, we can have the estimates of $r(s,a)$, $p(s'|s,a)$ and $V^*_{h+1}(s')$). \nThis optimization involves two attributes of items $q(a)$ and $w(a)$, instead of just depending on $q(a)$, which is far more challenging to efficiently compute than Eq. (i).\n\nTo overcome this computational difficulty, we first analyze the special properties of $f(A,q,w)$, including the fact that sorting items in descending order of $w(a)$ gives the optimal permutation (Section 4.1). Then, based on these properties, we design an efficient oracle BestPerm to solve Eq. (ii), building upon **a novel dynamic programming approach** for finding the best subset from sorted items (Section 4.2):\n$$\nF[i][k] = \\max \\\\{ F[i+1][k],\\  q(a_i)  w(a_i) +(1-u(a_i)) F[i+1][k-1] \\\\} ,\\quad 1\\leq i \\leq J-1,\\ 1 \\leq k \\leq \\min\\\\{m,J-i+1\\\\} .\n$$\nHere $F[i][k]:=\\max_{(a'_1,\\dots,a'_k) \\subseteq (a_i,\\dots,a_J)} f((a'_1,\\dots,a'_k),q,w)$ denotes the optimal objective value that can be achieved by selecting $k$ items from items $a_i,\\dots,a_J$ which are sorted in descending order of $w(a)$. Furthermore, we also provide rigorous analyses for the properties of $f(A,q,w)$ and the correctness of oracle BestPerm (Lemmas 1 and 2).\n\nTo sum up, our generalization to cascading MDPs has the following novelties: (1) This generalization introduces a significant computational challenge on how to efficiently find the optimal permutation. (2) To resolve this challenge, we analyze the special properties of the objective function for cascading MDPs, and design an efficient oracle BestPerm based on a novel dynamic programming. \nRigorous proofs are provided for the properties of the objective function and the correctness of oracle BestPerm. \n(3) Building upon oracle BestPerm and the carefully-designed exploration bonuses for estimates, we develop provably computationally-efficient and sample-efficient algorithms for cascading RL."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258192633,
                "cdate": 1700258192633,
                "tmdate": 1700258192633,
                "mdate": 1700258192633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P7nebFX4EY",
                "forum": "KjOAHlKMF5",
                "replyto": "IWhak2vAW3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6goX (Part 2/3)"
                    },
                    "comment": {
                        "value": "**2. Experiment**\n\nFollowing the reviewer's suggestion, we conduct experiments on a larger-scale **real-world** dataset MovieLens [Harper \\& Konstan 2015] (also used in prior works [Zong et al. 2016; Vial et al. 2022]) with more baselines. We present preliminary experimental results here (due to time limit), and will certainly include more results for larger numbers of states and items in our revision.\n\nThe MovieLens dataset contains 25 million ratings on a 5-star scale for 62000 movies by 162000 users. \nWe regard each user as a state, and each movie as an item. For each user-movie pair, we scale the rating to [0,1] and regard it as the attraction probability.\nThe reward of each user-movie pair is set to 1. For each user-movie pair which has a rating no lower than 4.5 stars, we set the transition probability to this state (user) itself as 0.9, and that to other states (users) as $\\frac{0.9}{S-1}$. For each user-movie pair which has a rating lower than 4.5 stars, we set the transition probability to all states (users) as $\\frac{1}{S}$. \nWe use a subset of data from MovieLens, and set $\\delta=0.005$, $K=100000$, $H=3$, $m=3$, $S=20$, $N \\in \\\\{10,15,20,25,30,40\\\\}$ (the number of items) and $|\\mathcal{A}|=\\sum_{\\tilde{m}=1}^{m} \\binom{N}{\\tilde{m}} \\tilde{m}! \\in \\\\{820,2955,7240,14425,25260,60880\\\\}$ (the number of item lists). \n\nWe compare our algorithm CascadingVI with **three baselines**, i.e., CascadingVI-Oracle, CascadingVI-Bonus and AdaptVI. Specifically, CascadingVI-Oracle is an ablated version of CascadingVI which replaces the efficient oracle BestPerm by a naive exhaustive search. CascadingVI-Bonus is an ablated variant of CascadingVI which replaces the variance-aware exploration bonus $b^{k,q}$ by a variance-unaware bonus. \nAdaptVI adapts the classic RL algorithm [Zanette\n\\& Brunskill 2019] to the combinatorial action space, which maintains the estimates for all $(s,A)$ rather than $(s,a)$.\nThe following table shows the cumulative regrets and running times of these algorithms. (CascadingVI-Oracle, which does not use our efficient computational oracle, is slow and still running in some instances with large $N$. We will update its results once it finishes.)\n\n| Regret; Running time  | CascadingVI (ours)  | CascadingVI-Oracle  | CascadingVI-Bonus  |  AdaptVI |\n| ------------ | ------------ | ------------ | ------------ | ------------ |\n| $N=10, \\mid \\mathcal{A} \\mid =820$  | 2781.48; 1194.29s  | 4097.56; 21809.87s  | 8503.45; 1099.32s  | 29874.84; 28066.90s  |\n| $N=15, \\mid \\mathcal{A} \\mid =2955$  | 4630.11; 2291.00s  | 6485.36; 71190.59s  | 13436.38; 2144.96s  | 30694.40; 53214.95s  |\n|  $N=20, \\mid \\mathcal{A} \\mid =7240$ | 6446.51; 2770.94s  | 9950.22; 145794.64s  | 17472.21; 2731.21s  | 34761.91; 81135.42s |\n|  $N=25, \\mid \\mathcal{A} \\mid =14425$ | 8283.60; 5216.00s   | 13170.39; 196125.72s  |  21868.54; 5066.27s | 42805.06; 95235.55s  |\n|  $N=30, \\mid \\mathcal{A} \\mid =25260$ | 11412.85; 7334.13s  |  Still running | 27738.31; 7256.94s  | 52505.85; 188589.60s  |\n|  $N=40, \\mid \\mathcal{A} \\mid =60880$ | 16825.65; 18628.83s  |  Still running | 37606.95; 17829.40s  |  56045.01; 319837.83s |\n\nAs shown in the above table, our algorithm CascadingVI achieves the lowest regret and a fast running time. CascadingVI-Oracle has a comparative regret performance to CascadingVI, but suffers a much higher running time, which demonstrates the power of our oracle BestPerm in computation.\nCascadingVI-Bonus attains a similar running time as CascadingVI, but has a worse regret (CascadingVI-Bonus looks slightly faster because computing a variance-unaware exploration bonus is simpler). This corroborates the effectiveness of our variance-aware exploration bonus in enhancing sample efficiency.\nAdaptVI suffers a very high regret and running time, since it learns the information of all permutations independently and its statistical and computational complexities depend on $|\\mathcal{A}|$.\n\nWe have also presented the figures of these experimental results in our revised version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258546675,
                "cdate": 1700258546675,
                "tmdate": 1700426080398,
                "mdate": 1700426080398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wGaK5wdava",
                "forum": "KjOAHlKMF5",
                "replyto": "IWhak2vAW3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**3. Replies to Questions**\n\n**3.1 Computational Complexity of $\\mathbb{E}_{s'\\sim\\tilde{p}^k}[\\dots]$**\n\nThe complexity of computing $\\mathbb{E} (s'\\sim\\hat{p}^k) [( \\bar{V}^k_{h+1}(s')-\\underline{V}^k_{h+1}(s') )^2 ]$ in Algorithm 2 is $O(\\mathcal{S})$. (In this response, we use $(s'\\sim\\hat{p}^k)$ to denote the subscript of $\\mathbb{E}$ due to the formula display issue of OpenReview.)\n\n**3.2 Computation of the Value Function $w$**\n\nAlgorithm CascadingVI performs optimistic value iteration by backward iteration for step $h=H,H-1,\\dots,1$.\nAt each step $h$, we already have the estimates $\\hat{p}^k(s'|s,a)$ and $\\bar{V}^k_{h+1}(s')$, and then we can compute the optimistic estimate for $w(s,a)$ as $\\bar{w}^k(s,a) \\leftarrow r(s,a)+\\hat{p}^k(\\cdot|s,a)^\\top \\bar{V}^k_{h+1}+ b^{k,pV}(s,a)$ (Line 10 of Algorithm 2). The complexity of computing $\\bar{w}^k(s,a)$ is $O(\\mathcal{S})$. After that, for each state $s \\in \\mathcal{S}$, we call oracle BestPerm with $\\bar{w}^k(s,a)$ to efficiently calculate the optimal value and the optimal permutation, i.e., $\\bar{V}^k_h(s),\\ \\pi^k_h(s) \\leftarrow BestPerm(A^{ground}, \\bar{q}^k(s,\\cdot), \\bar{w}^k(s,\\cdot))$ (Line 11 of Algorithm 2).\n\nActually, the biggest computational difficulty of BestPerm is how to efficiently solve the combinatorial optimization Eq. (ii) in our Reply 1. BestPerm takes advantage of the special properties of the objective function for cascading MDPs, and adopts a novel dynamic programming approach to solve it (see our Reply 1).\n\n**3.3 Intuition of the Exploration Bonus $b^{k,pV}(s,a)$**\n\nOur construction of $b^{k,pV}(s,a)$ follows that in prior RL work [Zanette \\& Brunskill 2019], which guarantees the optimistic estimation for $p(\\cdot|s,a)^{\\top}  V^*_{h+1}$. Below we describe the intuition behind $b^{k,pV}(s,a)$ (Lemmas 7-8 in Appendix C.2).\n\nAccording to Bernstern's inequality, we have that with high probability,\n$$\n| (\\hat{p}^k(\\cdot|s,a) - p(\\cdot|s,a))^{\\top}  V^*_{h+1} | \\leq 2 \\sqrt{ \\frac{Var_{s'\\sim p}(V^*_{h+1}(s')) \\log( \\frac{KHSA}{\\delta'} ) }{n^{k,p}(s,a)} } + \\frac{H \\log( \\frac{KHSA}{\\delta'} ) }{n^{k,p}(s,a)} . \\quad Eq. (iii)\n$$\nHence, the right-hand side of Eq. (iii) is the Bernstern-type exploration bonus of $p(\\cdot|s,a)^{\\top}  V^*_{h+1}$. However, the  algorithm does not know the exact $Var_{s'\\sim p}(V^*_{h+1}(s'))$, and thus we want to use the estimate of $Var_{s'\\sim p}(V^*_{h+1}(s'))$ instead of itself.\n\nFollowing the analysis in [Zanette \\& Brunskill 2019], we have that if $\\bar{V}^k_{h+1}(s') \\geq V^*_{h+1}(s') \\geq \\underline{V}^k_{h+1}(s')$ for any $s' \\in \\mathcal{S}$, then with high probability,\n$$\n\\sqrt{Var_{s'\\sim p }(V^*_{h+1}(s'))}  \\leq \\sqrt{Var_{s'\\sim \\hat{p}^k }(\\bar{V}^k_{h+1}(s'))} + \\sqrt{\\mathbb{E} (s'\\sim\\hat{p}^k) [( \\bar{V}^k_{h+1}(s')-\\underline{V}^k_{h+1}(s') )^2 ]} + 2H \\sqrt{\\frac{ \\log( \\frac{KHSA}{\\delta'}) }{n^{k,p}(s,a)}} . \n$$\n\nPlugging the above inequality into Eq. (iii), we have that with high probability,\n\n$$\n| (\\hat{p}^k(\\cdot|s,a) - p(\\cdot|s,a))^{\\top}  V^*_{h+1} | \\leq 2 \\sqrt{ \\frac{Var_{s'\\sim \\hat{p}^k} (\\bar{V}^k_{h+1}(s')) \\log( \\frac{KHSA}{\\delta'} ) }{n^{k,p}(s,a)} } + 2 \\sqrt{ \\frac{\\mathbb{E} (s'\\sim\\hat{p}^k) [( \\bar{V}^k_{h+1}(s')-\\underline{V}^k_{h+1}(s') )^2 ] \\log( \\frac{KHSA}{\\delta'} ) }{n^{k,p}(s,a)} } + \\frac{5H \\log( \\frac{KHSA}{\\delta'} ) }{n^{k,p}(s,a)} := b^{k,pV}(s,a) .\n$$\n\nThus, if $\\bar{V}^k_{h+1}(s') \\geq V^*_{h+1}(s') \\geq \\underline{V}^k_{h+1}(s')$ for any $s' \\in \\mathcal{S}$, then $\\hat{p}^k(\\cdot|s,a)^\\top \\bar{V}^k_{h+1}+ b^{k,pV}(s,a)$ is an optimistic estimate of $p(\\cdot|s,a)^{\\top}  V^*_{h+1}$. \n\nIn the following, we discuss how to guarantee the optimism of $\\bar{V}^k_{h+1}$, and the proof for pessimism is similar (Lemmas 14-15 in Appendix C.2). \n\nFirst, we prove that $f(A,q,w)$ satisfies the monotonicity property, i.e.,  if $\\bar{q} \\geq q$ element-wise and the items in $A$ are ranked in descending order of $w$, then $f(A,\\bar{q},w) \\geq f(A,q,w)$. \n\nThen, we prove the optimism of $\\bar{V}^k_{h+1}$ by induction. \nFor $h=H$, it trivially holds that $\\bar{V}^k_{H+1} \\geq V^*_{H+1}$ element-wise, and thus $\\hat{p}^k(\\cdot|s,a)^\\top \\bar{V}^k_{H+1}+ b^{k,pV}(s,a)$ is an optimistic estimate of $p(\\cdot|s,a)^{\\top}  V^*_{H+1}$.\nUsing the monotonicity property of $f(A,q,w)$, the fact that the items in the optimal permutation are ranked in descending order of weights, and the optimism of $\\bar{w}^k(s,a) = r(s,a)+\\hat{p}^k(\\cdot|s,a)^\\top \\bar{V}^k_{H+1}+ b^{k,pV}(s,a)$, we can prove that the output value $\\bar{V}^k_H$ of oracle BestPerm is an optimistic estimate of $V^*_{H}$. \nFor $h=H-1,H-2,\\dots,1$, by induction and similarly using the optimism of $\\bar{V}^k_{h+1}$ and $\\hat{p}^k(\\cdot|s,a)^\\top \\bar{V}^k_{h+1}+ b^{k,pV}(s,a)$, we can prove that $\\bar{V}^k_h$ is an optimistic estimate of $V^*_{h}$ for all $h \\in [H]$.\n\nWe have added more explanation in Section 5.1 in our revision."
                    },
                    "title": {
                        "value": "Response to Reviewer 6goX (Part 3/3)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259467333,
                "cdate": 1700259467333,
                "tmdate": 1700259616546,
                "mdate": 1700259616546,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iJECnC5a7e",
                "forum": "KjOAHlKMF5",
                "replyto": "IWhak2vAW3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_6goX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_6goX"
                ],
                "content": {
                    "title": {
                        "value": "Still not convinced on significance / novelty"
                    },
                    "comment": {
                        "value": "1. Contribution of our Generalization to Cascading MDPs\n\nThank you for the detailed response. Please allow to further articulate my point of view on why I believe this extension's significance is limited relative to what I believe is already known from the vanilla cascading bandits (and please correct me where I am missing something).\n\nDynamic programming is already an established method for computing optimal policies in an MDP. The proposed novelty here is the reduction of computation time for this algorithm by exploiting the cascading structure, i.e. the fact that \"basic\" actions are rankings (and hence the number of actual items in our ranking pool is much much smaller than that of actions). But in my opinion, it is already known that we don't need to evaluate every individual ranking, since it is sufficient to know the expected value of each individual item in this state (probability of click and expected future value) to construct the optimal ranking in a given state and avoid complexities in $O(|\\mathcal{A}|)$. Therefore I am not convinced this result (expanding the setting to the RL setting and providing an oracle algorithm avoiding $O(|\\mathcal{A}|)$ computational complexity) represents a sufficiently significant addition to the literature.\n\nI have one other standing curiosity for which I wonder if you can provide any insights: Would a naive algorithm that simply takes the top $m$ items as sorted by $q(s, i) \\times w(s, i)$ (the expected return of item $i$ weighted by its chance to be clicked when inspected) and further sorts these by $w(s, i)$ (items with the highest expected return at the top, regardless of their attractiveness) produces optimal rankings?\n\nAny insights you can offer are appreciated (I am not expecting a proof, just wondering if you have any counter-example at hand or whether you've seen this fail in your experiments)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516015490,
                "cdate": 1700516015490,
                "tmdate": 1700516069338,
                "mdate": 1700516069338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d3jPjRFBDZ",
                "forum": "KjOAHlKMF5",
                "replyto": "wGaK5wdava",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_6goX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_6goX"
                ],
                "content": {
                    "title": {
                        "value": "Other clarifications"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications. The intuition behind the exploration bonus is more clear to me now.\n\nThank you for introducing the real world experiments. These baselines are more meaningful in my opinion."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517633282,
                "cdate": 1700517633282,
                "tmdate": 1700517633282,
                "mdate": 1700517633282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4V8exlLcSq",
                "forum": "KjOAHlKMF5",
                "replyto": "IWhak2vAW3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Response to Reviewer 6goX"
                    },
                    "comment": {
                        "value": "Thank you for giving us an opportunity to interact with you!\n\n**1. Significance of Our Contribution**\n\nIndeed it is already known that one only needs to evaluate each item, instead of each ranking, in classic cascading (combinatorial) bandits [Kveton et al. 2015]. However, the objective function in cascading bandits\n$$\n\\max_{A \\in \\mathcal{A}} \\quad f(A,q) = \\sum_{i=1}^{|A|} \\prod_{j=1}^{i-1} \\Big( 1-q(A(j)) \\Big) q(A(i)) = 1- \\prod_{i=1}^{|A|} \\Big( 1-q(A(i)) \\Big) . \\quad Eq. (i)\n$$\nonly involves **one attribute** of items, i.e., the attraction probability $q(a)$. This objective function completely depends on $q(a)$, which is obvious to compute. They only need to select $m$ items with the maximum attraction probabilities $q(a)$.\n\nHowever, when generalizing to RL, the objective function at each step $h$ is\n$$\n\\max_{A \\in \\mathcal{A}} f(A,q,w) :=  \\sum_{i=1}^{|A|} \\prod_{j=1}^{i-1} \\Big( 1-q(A(j)) \\Big) q(A(i)) w(A(i)) , \\quad Eq. (ii) \n$$ \nwhich involves **two attributes** of items, i.e., the attraction probability $q(a)$ and the expected current and future reward $w(a)$ (here $w(a)=r(s,a)+p(\\cdot|s,a)^\\top V^*_{h+1}$ given the state $s$ and step $h$).\n**Eq. (ii) itself is a complex combinatorial optimization problem** even when one only evaluates the attributes of individual items and ignores the recursion over step $h$, and there is **no obvious monotone rule** with respect to $q(a)$, $w(a)$ or $q(a)*w(a)$.\n\nTo solve Eq. (ii), we design an efficient computational oracle BestPerm, using a novel dynamic programming rule based on the cascading feature of $q(a)$ and $w(a)$:\n$$\nF[i][k] = \\max \\\\{ F[i+1][k],\\  q(a_i)  w(a_i) +(1-q(a_i)) F[i+1][k-1] \\\\} ,\\quad 1\\leq i \\leq J-1,\\ 1 \\leq k \\leq \\min\\\\{m,J-i+1\\\\} , \\quad Eq. (iv)\n$$\nwhere \n\n$\nF[i][k]:=\\max_{(a'_1,\\dots,a'_k) \\subseteq (a_i,\\dots,a_J)} f((a'_1,\\dots,a'_k),q,w)\n$\n\ndenotes the optimal objective value achieved by selecting $k$ items from items $a_i,\\dots,a_J$ which are sorted in descending order of $w(a)$.\nThe intuition behind our dynamic programming rule is that: for a sorted item sequence $a_i,a_{i+1},\\dots,a_J$, if we do not select $a_{i}$, then the optimal objective value is equal to that achieved by selecting $k$ items from $a_{i+1},\\dots,a_J$, i.e., $F[i+1][k]$. Otherwise, if we select $a_{i}$, then the optimal objective value is equal to $(1-q(a_i))$ multiplying the optimal value achieved by selecting $k-1$ items from $a_{i+1},\\dots,a_J$, i.e., $(1-q(a_i)) F[i+1][k-1]$, due to the cascading feature of the objective function.\n\nWe note that the \"dynamic programming\" we mentioned in our contribution statements refers to the dynamic programming Eq. (iv) that we design to solve the combinatorial optimization Eq. (ii), which iterates over the indices of items $1\\leq i \\leq J-1$ and the number of selected items $1 \\leq k \\leq \\min\\\\{m,J-i+1\\\\}$. **We were not referring to the well-known MDP dynamic programming that iterates over step $h=H,H-1,\\dots,1$.**\n\nIn other words, our dynamic programming rule (Eq. (iv)) and oracle BestPerm provide an efficient computational solution to the combinatorial optimization Eq. (ii), which one needs to solve at each step $h$ in cascading RL even when only using the attributes of individual items. This is a significant contribution and novel to the combinatorial bandit/RL literature.\n\n**2. Counter-example for the Naive Algorithm**\n\n A naive algorithm that first sorts items by $q(a)*w(a)$ and further sorts them by $w(a)$ cannot find the optimal ranking.\n\n Consider a counter-example as follows:\n There are 10 items $a_1,a_2,\\dots,a_{10}$, and we want to find the optimal ranking with the maximum cardinality $m=3$. The attraction probabilities $q(a)$, weights $w(a)$ and their products $q(a)*w(a)$ of items are shown in the following table.\n\n|   |  $a_1$ |  $a_2$ | $a_3$  | $a_4$  | $a_5$  | $a_6$  | $a_7$  |  $a_8$ | $a_9$  | $a_{10}$  |\n| ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ |\n|  $q(a)$ | 0.1  |  0.2 |  0.3 |  0.4 | 0.5  | 0.6  | 0.7  | 0.8  | 0.9  | 1  |\n|  $w(a)$ |  1 |  0.9 |  0.8 | 0.7  | 0.6  |  0.5 |  0.4 | 0.3  |  0.2 | 0.1  |\n| $q(a)*w(a)$  |  0.1 | 0.18| 0.24| 0.28| 0.3 | 0.3 | 0.28| 0.24| 0.18| 0.1 |\n\nThe naive algorithm the reviewer mentioned will output $(a_4,a_5,a_6)$, while our computational oracle BestPerm will output $(a_3,a_4,a_5)$ according to our dynamic programming rule Eq. (iv).\n\nThe objective values $f(A,q,w) :=  \\sum_{i=1}^{|A|} \\prod_{j=1}^{i-1} ( 1-q(A(j)) ) q(A(i)) w(A(i))$ of $(a_4,a_5,a_6)$ and $(a_3,a_4,a_5)$ are\n\n$$\nf((a_4,a_5,a_6),q,w)=0.28+(1-0.4)*0.3\n$$\n\n$$\n\\qquad\\qquad\\qquad\\qquad\\qquad +(1-0.4)*(1-0.5)*0.3=0.55 ,\n$$\n\n$$\nf((a_3,a_4,a_5),q,w)=0.24+(1-0.3)*0.28\n$$\n\n$$\n\\qquad\\qquad\\qquad\\qquad\\qquad +(1-0.3)*(1-0.4)*0.3=0.562 > f((a_4,a_5,a_6),q,w) .\n$$\n\nThis demonstrates that the output of the naive algorithm $(a_4,a_5,a_6)$ is not the optimal solution."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532673917,
                "cdate": 1700532673917,
                "tmdate": 1700534018842,
                "mdate": 1700534018842,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b3r3gPrutO",
                "forum": "KjOAHlKMF5",
                "replyto": "4V8exlLcSq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_6goX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_6goX"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the clarifications"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications and especially for the insight on the difficulty of computing the best permutation. My concerns about the novelty are now somewhat eased. I recommend that in future revisions of the paper you put more emphasis on the difficulties solved by BestPerm and presenting this contrast between such a naive policy (picking the top $m$ items by expected value $q \\times w$) and the actual optimal ranking. The first few reads, I did not pick up on the nuances of this difficulty, my intuition being that it is fairly low hanging fruit."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563505869,
                "cdate": 1700563505869,
                "tmdate": 1700563505869,
                "mdate": 1700563505869,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w9HBg6gDzc",
            "forum": "KjOAHlKMF5",
            "replyto": "KjOAHlKMF5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2902/Reviewer_Rma4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2902/Reviewer_Rma4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an extension of the cascading bandits framework to a more general reinforcement learning framework. The authors thereby attempt to model user sessions or historical user behavior and their impact on the click behavior and payoff.  While the action space (i.e., combination of items) is combinatorial, the feedback is item-wise so that attraction and transition probabilities can be estimated efficiently. This allows the authors to design algorithms with non-trivial regret and sample complexity guarantees. In particular, the authors rely on monotonicity properties to design an efficient oracle. Finally, the authors support their theoretical findings through experiments which support the efficiency of the proposed algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Firstly, I think that the studied problem is interesting and the presentation of the paper clear. \n- The extension of the cascading bandit model to an RL formulation is highly non-trivial and the paper contains novel contributions including the RL formulation itself and an interesting algorithm design which relies on properties of the value function. \n- The authors do a good job explaining their algorithms and provide intuition for their results.  \n- The necessity of adopting standard RL algorithms to the proposed cascading model is highlighted in the paper several times and the authors thereby provide proper justification for the proposed algorithms."
                },
                "weaknesses": {
                    "value": "- I am not fully convinced of the practicality of this model. Firstly, historical user behavior can be modeled as part of the context in contextual bandit frameworks. Moreover, \"artificially\" creating states appears fairly cumbersome compared to the contextual bandit structure and it is also not entirely clear how such states would be defined in practice. However, I could be convinced otherwise."
                },
                "questions": {
                    "value": "- Could you further explain why contextual approaches are insufficient and what the merit of the RL formulation is in contrast to contextual bandits or recommendation approaches based on context trees? In the RL framework it is important what states the user transitions to, as some states may have the potential to yield higher rewards than others. Do you think that this is realistic in practice?  \n\n- It would be great if you could go into a bit more detail in your related work section and more clearly highlight the differences to prior models. For example, in contrast to the classical cascading bandits, the order of items also matters in your case as you usually would like the item with largest reward in state s to be clicked. \n\nMinor things: \n- In the 4th contribution, I found the statement about \"$\\varepsilon$ sufficiently large\" slightly confusing. It only becomes clearer later in Theorem 2 when the full bound is stated. Maybe there is a way to state this less conusingly in the introduction.  \n- Typo in the 3rd paragraph of Section 7: \"Regarding the best policy identification objective\" instead of \"Regarding the regret minimization objective\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2902/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2902/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2902/Reviewer_Rma4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698616578408,
            "cdate": 1698616578408,
            "tmdate": 1699636233825,
            "mdate": 1699636233825,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5mCFmZJWAp",
                "forum": "KjOAHlKMF5",
                "replyto": "w9HBg6gDzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Rma4"
                    },
                    "comment": {
                        "value": "Thank you very much for your time in reviewing our paper and your positive comments! We have revised our paper according to your suggestions. Our revision is highlighted in *orange* font.\n\n**1. Merit of the RL Formulation Compared to Contextual Bandits**\n\nWe agree with the reviewer that contextual bandits can also be used to formulate historical user behaviors as part of contexts.\nWe think that RL is more suitable for long-term reward maximization, since it considers state transition and potential future rewards in decision making.\n\nFor example, consider a video recommendation scenario. The recommendation system encounters a user who is interested in funny videos and TV series.\nGiven this context, contextual bandits mainly recommend videos which can maximize the reward in the current context. Hence, contextual bandits may recommend funny videos which usually give higher instantaneous utilities.\nIn contrast, RL recommends videos which maximize the expected cumulative (long-term) reward, which also considers the potential rewards generated by future states. Thus, in this case, RL may recommend TV series videos. Although TV series videos may have lower instantaneous utilities, once the user is attracted to some of them, the user can enter a high-reward successor state, i.e., he/she is obsessed with it and keeps watching subsequent videos of this TV series. Therefore, using the RL formulation can obtain higher rewards in the long term. \n\n**2. Related Works**\n\nWe detail the differences of our cascading RL model from prior works below.\n\n[Kveton et al. 2015; Cheung et al. 2019; Zhong et al. 2021; Vial et al. 2022] study the cascading bandit model. In their model, there is no state (context), and the reward generated by each item (if clicked) is one. Thus, the order of selected items does not matter, and they only need to select $m$ items with the maximum attraction probabilities.\n[Zong et al. 2016; Li \\& Zhang 2018] consider the  contextual cascading bandit problem. In their problem, the agent first observes the context (i.e., the feature vector of each item) at each timestep, and the attraction probability of each item is the inner-product of its feature vector and a global parameter. The order of selected items still does not matter, and they need to select $m$ items with the maximum attraction probabilities in the current context.\n[Li et al. 2016] investigate contextual cascading bandits with position discount factors and a general reward function. In their model, the order of selected items matters, and they assume to have access to an oracle that can output the optimal ordered subset of items for the general reward function.\n\nDifferent from the above works, our cascading RL formulation further considers state transition, and the attraction probabilities and rewards of items depend on states. Thus, we need to put the items with higher rewards in the current state in the front. In addition, we require to both maximize the attraction probabilities of selected items, and optimize the potential rewards from future states. Furthermore, instead of assuming access to an oracle, we design an efficient oracle to find the optimal ordered subset of items.\n\n**3. Replies to Minor Things**\n\nThank you for pointing out the unclear sentence and typo!\n\nWe have revised the statement in the fourth contribution to \"CascadingBPI is\noptimal up to a factor of $\\tilde{O}(H)$ when $\\varepsilon < \\frac{H}{S^2}$\" to avoid confusion, and fixed the typo.\n\n\n---\n\nReferences:  \n[1] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning\nto rank in the cascade model. ICML 2015.  \n[2] Wang Chi Cheung, Vincent Tan, and Zixin Zhong. A thompson sampling algorithm for cascading\nbandits. AISTATS\n2019.  \n[3] Zixin Zhong, Wang Chi Chueng, and Vincent YF Tan. Thompson sampling algorithms for cascading\nbandits. JMLR 2021.  \n[4] Daniel Vial, Sujay Sanghavi, Sanjay Shakkottai, and R Srikant. Minimax regret for cascading bandits.\nNeurIPS 2022.  \n[5] Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, and Branislav Kveton. Cascading\nbandits for large-scale recommendation problems. UAI 2016.  \n[6] Shuai Li and Shengyu Zhang. Online clustering of contextual cascading bandits. AAAI 2018.  \n[7] Shuai Li, Baoxiang Wang, Shengyu Zhang, and Wei Chen. Contextual combinatorial cascading\nbandits. ICML 2016."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257649055,
                "cdate": 1700257649055,
                "tmdate": 1700257649055,
                "mdate": 1700257649055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TUoQABSRqi",
                "forum": "KjOAHlKMF5",
                "replyto": "5mCFmZJWAp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_Rma4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_Rma4"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to my comments. \n\nRegarding your first answer. While it does make sense to me that you can model a \"customer journey\" like this and that this can be useful, defining sensible states seems quite hard and could simply boil down to something like a context tree (which would be simpler than the MDP framework). However, I still appreciate the advantages of \"forward\" planning and trying to transition to rewarding states, despite the challenges in designing such states in practice. I do not wish to change my original assessment and I am still in favour of accepting the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401591851,
                "cdate": 1700401591851,
                "tmdate": 1700401591851,
                "mdate": 1700401591851,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7Gq6bsuFV2",
            "forum": "KjOAHlKMF5",
            "replyto": "KjOAHlKMF5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2902/Reviewer_MkFu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2902/Reviewer_MkFu"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new framework for reinforcement learning (RL) called cascading RL, which builds upon the concept of cascading bandits but incorporates state information into the decision-making process. This framework is aimed at applications such as personalized recommendation systems and online advertising, where cascading items are displayed to users one by one.\n\nKey challenges addressed include computational difficulty due to the combinatorial nature of actions and ensuring sample efficiency without relying on the exponential number of potential actions. To overcome these, the authors developed an efficient oracle called BestPerm, which uses dynamic programming to optimize item selection. They also introduced two RL algorithms: CascadingVI for regret minimization and CascadingBPI for sample complexity, both of which utilize BestPerm to achieve polynomial regret and sample complexity.\n\nCascadingVI achieves near-optimal regret matching a known lower bound. CascadingBPI offers efficient computation and sample complexity, reaching near-optimal performance.\n\nIn summary, the paper contributes a new cascading RL framework that efficiently handles the computational and sample complexity challenges of stateful item selection, supported by theoretical guarantees and demonstrated effectiveness through experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- As far as I know, this would be the first theory RL work with cascading actions. The formulation is sound.\n- The paper proposes an efficient optimization oracle for cascading RL, providing its correctness (it would be better to show it in the main text rather than the appendix).\n- Although\u00a0it is not groundbreaking, the paper provides both regret analysis and sample complexity analysis."
                },
                "weaknesses": {
                    "value": "- It seems that the techniques used for regret analysis and sample complexity are largely borrowed from the previous literature. I wonder if there are sufficient technical challenges once you know how to solve Problem (2).\n- The readability of the algorithms can be improved. e.g., Update rule of $\\bar{q}^k$ and $\\underbar{q}^k$ in Line 6 of Algorithm 2, Line break in Line 8 of Algorithm 2, etc."
                },
                "questions": {
                    "value": "Where does the gap $\\sqrt{H}$ appear? Can you elaborate on this? Any possible direction to carve off this given that the tabular RL methods with a single action can achieve minimax optimality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699006454467,
            "cdate": 1699006454467,
            "tmdate": 1699636233744,
            "mdate": 1699636233744,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y0ewGTbjwh",
                "forum": "KjOAHlKMF5",
                "replyto": "7Gq6bsuFV2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MkFu (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for your time and effort in reviewing our paper! We have revised our paper according to your comments. Our revision is highlighted in *orange* font.\n\n**1. Presentation on the Correctness of Oracle BestPerm**\n\nThank you for your helpful suggestion! We have moved the correctness guarantee of oracle BestPerm to the main text (Lemma 2) in our revision.\n\n**2. Technical Challenges**\n\nBelow we elaborate the technical challenges and novelties in addition to the oracle for solving Problem Eq. (2). \n\n\n- Even if we know how to solve Problem Eq. (2), in the online RL process, we only have the estimates of attraction probabilities, transition distributions and value functions. Then, how to **guarantee the optimism of the output value by oracle BestPerm** with optimistic estimated inputs is a crucial challenge. To handle this challenge, we prove the *monotonicity* property of the objective function for cascading RL by induction, leveraging the fact that *the items in the optimal permutation are ranked in descending order of $w$*  (Lemma 14). This monotonicity property guarantees the optimism of our estimation under the use of oracle BestPerm with optimistic estimates (Lemma 15).\n\n- Since the objective function for cascading RL involves two attributes of items, i.e., attraction probabilities $q(s,a)$ and the expected future rewards $p(\\cdot|s,a)^\\top V^*_{h+1}$, how to construct exploration bonuses for them is an important step in algorithm design.\nWe build the exploration bonuses for $q(s,a)$ and $p(\\cdot|s,a)^\\top V^*_{h+1}$ separately for  $(s,a)$, rather than adding an overall exploration bonus for each $(s,A)$. This exploration bonus design enables us to achieve regret and sample complexity bounds that depend only on the number of items $N$, instead of the number of item lists $|\\mathcal{A}|$.\n\n- Another critical challenge is how to **achieve the optimal regret bound when our problem degenerates to cascading bandits** [Vial et al. 2022] given that cascading RL has a more complicated state-transition structure. To tackle this challenge, we use a variance-aware exploration bonus $b^{k,q}(s,a)=\\sqrt{\\frac{\\hat{q}^k(s,a) (1-\\hat{q}^k(s,a)) L}{n^{k,q}(s,a)}} + \\frac{5 L}{n^{k,q}(s,a)}$, which can adaptively shrink when the attraction probability $q(s,a)$ is small.\nIn regret analysis, when performing summation over $b^{k,q}(s,a)$, this variance-awareness saves a factor of $\\sqrt{m}$, and enables our regret bound to match the optimal result in cascading bandits [Vial et al. 2022].\n\n\n**3. Readability**\n\nThank you for your valuable suggestions! We have revised Algorithm 2 according to your comments to improve readability in our revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257174044,
                "cdate": 1700257174044,
                "tmdate": 1700257174044,
                "mdate": 1700257174044,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IdJVMIbb8B",
                "forum": "KjOAHlKMF5",
                "replyto": "7Gq6bsuFV2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MkFu (Part 2/2)"
                    },
                    "comment": {
                        "value": "**4. The Gap $\\sqrt{H}$**\n\nThe gap $\\sqrt{H}$ appears because we construct the exploration bonuses for $q(s,a)$ and $p(\\cdot|s,a)^\\top V^*_{h+1}$ separately, which leads to an additional $\\sqrt{H}$ factor when summing up $b^{k,q}(s,a)$ and $b^{k,pV}(s,a)$ in regret analysis.\n\nBelow we elaborate this in detail.  In regret analysis, using value difference lemma, the regret decomposition contains the following term\n$$ \n\\sum_{k=1}^{K} \\sum_{h=1}^{H} \\mathbb{E} \\Bigg[ \n\\sum_{i=1}^{|A_h|} \\prod_{j=1}^{i-1} (1-q(s_h,A_h(j))) \\bigg( \\bar{q}^k(s_h,A_h(i)) \\left( \\hat{p}^k(\\cdot|s_h,A_h(i))^\\top \\bar{V}^k_{h+1} + b^{k,pV}(s_h,A_h(i)) \\right)- q(s_h,A_h(i)) p(\\cdot|s_h,A_h(i))^\\top \\bar{V}^k_{h+1} \\bigg) \\Bigg]\n$$\n$$\n= \\sum_{k=1}^{K} \\sum_{h=1}^{H} \\mathbb{E} \\Bigg[ \\sum_{i=1}^{|A_h|} \\prod_{j=1}^{i-1} (1-q(s_h,A_h(j))) \\bigg( \\Big( q(s_h,A_h(i))+2b^{k,q}(s_h,A_h(i)) \\Big)  \\Big( \\hat{p}^k(\\cdot|s_h,A_h(i))^\\top \\bar{V}^k_{h+1} + b^{k,pV}(s_h,A_h(i)) \\Big) - q(s_h,A_h(i)) p(\\cdot|s_h,A_h(i))^\\top \\bar{V}^k_{h+1} \\bigg) \\Bigg] \\quad Eq. (i)\n$$\n$$\n\\leq \\sum_{k=1}^{K} \\sum_{h=1}^{H} \\mathbb{E} \\Bigg[ \\sum_{i=1}^{|A_h|} \\prod_{j=1}^{i-1} (1-q(s_h,A_h(j))) \\bigg( q(s_h,A_h(i)) \\Big( \\hat{p}^k(\\cdot|s_h,A_h(i))^\\top \\bar{V}^k_{h+1} + b^{k,pV}(s_h,A_h(i)) - p(\\cdot|s_h,A_h(i))^\\top \\bar{V}^k_{h+1} \\Big) + 4Hb^{k,q}(s_h,A_h(i)) \\bigg) \\quad Eq. (ii) \\Bigg]\n$$\nSince we construct exploration bonuses for $q(s,a)$ and $p(\\cdot|s,a)^\\top V^*_{h+1}$ separately, in Eq. (i) above, the regret contains a product $(q+b^{k,q})(\\hat{p}^\\top \\bar{V}+b^{k,pV}-p^\\top V)$, while the regret for classic RL only contains the term $\\hat{p}^\\top \\bar{V}+b^{k,pV}-p^\\top V$. To handle this product, we bound it by $q(\\hat{p}^\\top \\bar{V}+b^{k,pV}-p^\\top V)+b^{k,q}(\\hat{p}^\\top \\bar{V}+b^{k,pV}-p^\\top V)=q(\\hat{p}^\\top \\bar{V}+b^{k,pV}-p^\\top V)+O(H b^{k,q})$. Here the $O(H b^{k,q})$ term directly leads to a $\\tilde{O}(H\\sqrt{HK})$ regret.\n\nA straightforward idea to improve the $\\sqrt{H}$ gap is to combine the attraction probability $q$ and transition distribution $p$ as an integrated transition distribution $\\tilde{p}(s'|s,A)$ for each item list $A$, and construct an overall exploration bonus for $\\tilde{p}(s'|s,A)$. However, this strategy forces us to maintain the exploration bonus for each $A \\in \\mathcal{A}$, which will incur an additional dependency on $\\mathcal{A}$ in the regret bound and is computationally inefficient.\n\nWe think that a more fine-grained analysis for bounding $b^{k,q}(\\hat{p}^\\top \\bar{V}+b^{k,pV}-p^\\top V)$ or a more advancing bonus construction approach may be helpful to close the $\\sqrt{H}$ gap. We plan to further investigate it in our future work.\n\n---\n\nReference:  \nDaniel Vial, Sujay Sanghavi, Sanjay Shakkottai, and R Srikant. Minimax regret for cascading bandits.\nNeurIPS 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257497787,
                "cdate": 1700257497787,
                "tmdate": 1700261191275,
                "mdate": 1700261191275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zYTR6fkbe6",
                "forum": "KjOAHlKMF5",
                "replyto": "IdJVMIbb8B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_MkFu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_MkFu"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the responses. I will keep my positive score."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694968270,
                "cdate": 1700694968270,
                "tmdate": 1700694968270,
                "mdate": 1700694968270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aw3vggyMlB",
            "forum": "KjOAHlKMF5",
            "replyto": "KjOAHlKMF5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2902/Reviewer_z4b8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2902/Reviewer_z4b8"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies cascading reinforcement learning, which is a natural extension of cascading bandits to the episodic reinforcement learning setting. In particular, this paper has\n\n- proposed the cascading reinforcement learning framework;\n\n- developed a computationally efficient algorithm to solve the \"cascading MDPs\" (i.e. the cascading reinforcement learning problems with known models). The key ideas are summarized in the BestPerm algorithm (Algorithm 1). \n\n- developed a learning algorithm for the cumulative regret minimization setting, which is referred to as CascadingVI (Algorithm 2). A regret bound is also established (Theorem 1). This paper has also discussed the tightness of this regret bound.\n\n- also developed a learning algorithm for the best policy identification setting, which is referred to as CascadingBPI. A sample complexity bound is established (Theorem 2). This paper has also discussed the tightness of this regret bound.\n\n- demonstrated preliminary experiment results (Section 7)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In general, I think this paper is a strong theoretical paper, for the following reasons:\n\n- This paper studies a natural extension of a well-studied problem. Moreover, as summarized above, the contributions of this paper are clear.\n\n- The main results of this paper, summarized in Section 4 (efficient oracle), Section 5 (regret minimization), and Section 6 (best policy identification), are interesting and non-trivial. In particular, the regret bound in Theorem 1 and the sample complexity bound in Theorem 2 are non-trivial. Moreover, this paper has also discussed the tightness of these two bounds by comparing with existing lower bounds. Both bounds are near-optimal.\n\n- The paper is well-written in general, and is easy to read."
                },
                "weaknesses": {
                    "value": "- I am wondering if the developed algorithms are useful for practical recommendation problems. The reason is that, this paper considers a tabular setting, thus the developed regret bound and the sample complexity bound depend on the number of states $S$. However, my understanding is that for most practical recommendation problems, $S$ will be exponentially large. Might the authors identify a setting where $S$ is not too large and hence the proposed algorithms are practical?\n\n- Currently the experiment results are very limited. In particular, this paper has only demonstrated experiment results in small-scale synthetic problems. I think experiment results on large-scale practical problems will further strengthen this paper."
                },
                "questions": {
                    "value": "Please address the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699252479840,
            "cdate": 1699252479840,
            "tmdate": 1699636233650,
            "mdate": 1699636233650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TEO2hUl5yT",
                "forum": "KjOAHlKMF5",
                "replyto": "aw3vggyMlB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer z4b8 (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for reviewing our paper and your positive comments! We have incorporated your suggestions in the revised version, and highlighted our revision in *teal* font.\n\n**1. The Scenario Where $S$ Is Not Too Large**\n\nConsider a video recommendation scenario where the videos are categorized into multiple types, e.g., news, education, entertainment and movies.\nHere each item is a video, and each action (item list) is a list of videos. We can define each state as the types of the last one or two videos that the user just watched, which represents the recent preference and focus of the user. Then, the recommendation system suggests a list of videos according to the recent viewing record of the user. After the user chooses a new video to watch, the environment transitions to a next state which represents the user's latest interest.\nIn such scenarios, the number of states $S$ is polynomial in the number of types of videos, which is not too large. Therefore, our cascading RL formulation and algorithms are useful and practical for these applications."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259859721,
                "cdate": 1700259859721,
                "tmdate": 1700426288912,
                "mdate": 1700426288912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YXG4Jsqg6j",
                "forum": "KjOAHlKMF5",
                "replyto": "TEO2hUl5yT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_z4b8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_z4b8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "Thanks for the response! It has addressed one of my concerns."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637381490,
                "cdate": 1700637381490,
                "tmdate": 1700637381490,
                "mdate": 1700637381490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zkgCAP5ll2",
                "forum": "KjOAHlKMF5",
                "replyto": "Mvz4kKdB4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_z4b8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2902/Reviewer_z4b8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the experiment results!"
                    },
                    "comment": {
                        "value": "Thanks for the experiment results! They will definitely further strengthen the paper!"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637565805,
                "cdate": 1700637565805,
                "tmdate": 1700637565805,
                "mdate": 1700637565805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]