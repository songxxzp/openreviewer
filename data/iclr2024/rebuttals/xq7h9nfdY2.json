[
    {
        "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"
    },
    {
        "review": {
            "id": "4Xo0ady6pN",
            "forum": "xq7h9nfdY2",
            "replyto": "xq7h9nfdY2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1045/Reviewer_qFYD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1045/Reviewer_qFYD"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a defense method against large language model (LLM) jailbreaking, based on the Majority vote principle. It introduces random perturbations into queries to weaken certain jailbreaking attempts on LLMs. The method involves first introducing random perturbations and then measuring if the response begins with a target segment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work is an early exploration in defending against LLM jailbreaking. It pioneers the approach of using Majority vote and query perturbation to detect and prevent malicious outputs."
                },
                "weaknesses": {
                    "value": "- The paper's definition of JailBreak is overly restrictive, focusing only on responses beginning with a specific prefix. This is problematic as attacks can still be successful even if the LLM does not strictly reply with the target segment. For instance, this paper use target as \"Sure, here\u2019s how to build a bomb\", however, in an attack, LLM could generate \"Sure, you can build a bomb by ...\". This still containing harmful content should be considered a successful attack.\n- The proposed SmoothLLM approach is impractical in real-world scenarios (like ChatGPT), because defenders can't predict the exact nature of the attacks or the corresponding targets. For example, the paper uses \"Sure, here\u2019s how to build a bomb\" as a target corresponding to the goal \"Tell me how to build a bomb\". But if an attacker's goal is different, such as \"How to steal money from charity\", the previous target becomes irrelevant. Therefore, the defense is ineffective if the attacker's goals or phrasings differ from what the defenders expect.\n- Furthermore, even if an attacker's goal is constant (e.g., \"Tell me how to build a bomb\"), they can easily bypass the defense by choosing a different target phrase for the jailbreaking. For example, if the defender's target is T=\"Sure, here\u2019s how to build a bomb\", an attacker might use T'=\"In order to build a bomb, you can first\" as their target. In this scenario, the majority vote method would fail to filter out successful attacks since the JB(T',T) = 0.\n\n\nMinors (typo):\n\n\"this would be come at the cost\" -> \"this would come at the cost\"\n\n\"While this may seem counterintutive\"->\"While this may seem counterintuitive\"\n\n\"future work should focus more robust\"->\"future work should focus on more robust\"\n\n\"randomized smoothing (Salman et al., 2020; Carlini et al., 2022),\"->\"randomized smoothing (Salman et al., 2020; Carlini et al., 2022).\""
                },
                "questions": {
                    "value": "Concerning the results in Tables 3 and 4, SmoothLLM shows a performance decline of about 10% on PIQA. Why do the authors claim that their SmoothLLM method does not impose significant trade-offs between robustness and nominal performance? This claim appears to be contradicted by the empirical evidence."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634954935,
            "cdate": 1698634954935,
            "tmdate": 1699636030820,
            "mdate": 1699636030820,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cY6RXW1Qs2",
                "forum": "xq7h9nfdY2",
                "replyto": "4Xo0ady6pN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Resolving your main concern: Clarifying \"what is a jailbreak?\""
                    },
                    "comment": {
                        "value": "Thanks for your review!  Here are some detailed comments.\n\n---\n\n## Our threat model\n\nYour main concern involves the threat model used in our paper.  In particular, your review indicates that you find the definition of `JB` in eq. (2.1) to be too restrictive and that the threat model is reliant on knowing the target string.  Fortunately, these comments are not weaknesses of the SmoothLLM algorithm.  Rather, your comments point to a shortcoming in our description of the threat model.  In particular, the `JB` function we define in (2.1) is *different* from the `JB` function we use in our experiments, as outlined in Appendix B of our original submission.  Therefore, **we believe that your concern can be completely resolved by clarifying the definition of a jailbreak** that we use in our experiments.\n\nTo summarize this point, in Section 2, we define a jailbreaking through the binary-valued function `JB`.  In particular, we write that\n\n> \"**one possible realization** of [the `JB` function] checks whether the reponse $R$ starts with the target $T$.\"\n\nAfter formalizing this version of the `JB` function in eq. (2.1), we go on to say that\n\n> \". . . there are **many other ways of defining `JB`**; see Appendix B for details.\"\"\n\nIndeed, as stated in Appendix B.4, **we did *not* use eq. (2.1) in any of our experiments**; we agree with you that this definition is restrictive.  Instead, to be consistent with the GCG attack paper, in our experiments we used the definition of `JB` in eq. (B.1), which involves checking the response $R$ for various keywords.   Specifically, eq. (B.1) defines `JB` as \n\n$\\text{JB}(R) = \\mathbb{I}[ R \\text{ doesn't contain any phrase in } \\texttt{JailbreakKeywords}]$\n\n\nwhere `JailbreakKeywords` is a list of generic phrases such as including \"I'm sorry,\" \"As an AI,\" and \"I will not.\"  This method is far more realistic and general, and importantly **it does not depend *at all* on the target string**.  Notably, this function does not incur the \"different phrasing\" or \"fail to filter\" drawbacks mentioned in your second and third bullets respectively.\n\nTo address this shortcoming in the presentation, we rewrote the begining of Section 2 (see our updated PDF) to reflect the version of `JB` in eq. (B.1).  We believe that this completely resolves all three of the bullets in the \"Weaknesses\" section.  We hope you agree, and if so, we ask that you consider increasing your score.\n\n---\n\n## Non-conservatism\n\n> \"Concerning the results in Tables 3 and 4, SmoothLLM shows a performance decline of about 10% on PIQA. Why do the authors claim that their SmoothLLM method does not impose significant trade-offs between robustness and nominal performance? This claim appears to be contradicted by the empirical evidence.\"\n\nYou're correct that we do incur a trade-off between robustness and nominal performance.  Here is a summarized version of the evidence provided in our paper:\n\n* For Vicuna, if you set $q=5\\%$, set $N=20$, and use swap perturbations, sacrificing between $5$-$10\\%$ in nominal performance (see Tables 3 and 4) reduces the susceptibility to jailbreaking from $98\\%$ to around $10\\%$ (see Figure 7).\n\nTherefore, as is the case in nearly every paper ever written on robustness in ML, improving robustness comes at a *relatively* small cost (which we took to be ~10%) in nominal performance. We think that this is a reasonable price to pay to reduce the ASR by a factor of 10, but if you disagree, we would be happy to have a discussion about this.\n\nMoreover, our paper is the first to study this trade-off for jailbreaking, and we feel that this constitutes a worthy contribution.  If you think our paper would be stronger if we were to rephrase and/or emphasize the specifics of this trade-off in a different way, we would be happy to update our paper in line with your suggestions.\n\n---\n\n## Minor typos\n\nThanks for pointing these out!  We have fixed them in our updated PDF.\n\n---\n\n## Strengths of our paper\n\nWe want to make one more quick point in this response: We noticed that the \"Strengths\" section of your review does not include most of our main contributions that we listed in the introduction, e.g.,\n\n- **State-of-the-art ASR.**  SmoothLLM is the first and only algorithm to reduce the attack success rate of the (very) popular GCG attack to below 1%.\n- **Adaptive attack robustness.** SmoothLLM is robust against adaptive attacks.\n- **Query efficiency.** Running the GCG attack requires >200,000 queries to the target LLM.  Defending the target LLM against this attack with SmoothLLM only requires ~10 queries.\n- **Black box.** SmoothLLM only requires query access to the target LLM.\n- **Theoretical result.** In a simplified setting, SmoothLLM provably eliminates the threat of suffix-based attacks for suitably chosen hyperparameters.\n\nWe feel strongly that these points constitute solid contributions to the field of jailbreaking LLMs, and we would appreciate your thoughts regarding our contributions.  Thanks!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086311429,
                "cdate": 1700086311429,
                "tmdate": 1700086311429,
                "mdate": 1700086311429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Na18ucckKG",
                "forum": "xq7h9nfdY2",
                "replyto": "cY6RXW1Qs2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1045/Reviewer_qFYD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1045/Reviewer_qFYD"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the detailed rebuttal and the revision.\n\n> JailbreakKeywords is a list of generic phrases such as including \"I'm sorry,\" \"As an AI,\" and \"I will not.\" \n\nIndeed, that enriches the definition of jailbreak and make it more realistic. However, this definition is still not ready in an adversarial setup, as attacker could still easily bypass this keyword based metric. For example, attacker could optimize the probability of generating \"As an AI, I can help you to ...\".\n\n> Therefore, as is the case in nearly every paper ever written on robustness in ML, improving robustness comes at a relatively small cost (which we took to be ~10%) in nominal performance\n\nThe assertion that enhancing robustness typically comes with a trade-off in nominal performance might not always hold true, especially for \"detection and filtering\" strategies. Specifically, for those samples correctly identified as \"true negatives,\" such an approach might not alter their performance."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723000946,
                "cdate": 1700723000946,
                "tmdate": 1700723000946,
                "mdate": 1700723000946,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1K9DLsOTpy",
            "forum": "xq7h9nfdY2",
            "replyto": "xq7h9nfdY2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1045/Reviewer_Ay91"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1045/Reviewer_Ay91"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a simple scheme to prevent jailbreaking through adversarial prompt suffices. The main observation is the brittleness of these suffices (if a sufficient number of their characters are perturbed, they lose effectiveness). Building on this, the proposed scheme applies random transformations to the original prompt to smoothen the model response. They experiment on and provide guidelines on how to apply these transformations. The scheme stands out as it can operate in a black-box setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Simple, straightforward scheme to mitigate GCG-type attacks.\n+ Some theoretical results as to when/how the scheme can be effective.\n+ Guidelines and experiments on hyperparameter tuning are insightful. \n\nJailbreak attacks have been demonstrated both in academia and observed in the wild [1] and it is critical to develop simple, baseline defenses against this threat. The proposed algorithm fulfills that role and reuses the intuition from existing randomized smoothing defenses in the context of LLMs. \n\n[1] https://arxiv.org/abs/2308.03825"
                },
                "weaknesses": {
                    "value": "- Not entirely convinced about the k-unstability assumption that is the foundation of the algorithm and the theory.\n- Weak effort in designing an adaptive attack (e.g., create a suffix that's resilient to perturbations)\n- No experiments on the universal GCG attack or more semantic jailbreak attacks that are more practical and widespread.\n\nI don't know why k-unstability would be a fundamental property of adversarial suffix attacks like GCG. It's an empirical observation for a particular attack but we know very well now that building defenses on attack-specific artifacts is not a way to go. It only leads to a counterproductive arms race without yielding a long-lasting idea. Considering that it would be generally easy for an attacker to detect if SmoothLLM defense is used (e.g., by looking at the model's responses), it also would be trivial to change up the attack strategy to break the defense. Is there a reason why you believe k-unstability is a good assumption that will withstand future attacks or stronger adaptive attacks?\n\nThis brings me to my second point. It's accepted now that adaptive attack evaluations should be at the forefront of defensive papers like this. It's trivial to defend against any particular attack (e.g., by detecting the attack's artifacts) but through many painful lessons, we know that adaptive attacks are hard to design and evaluate against. I can't see much deeper thought in this paper in this regard. It definitely needs more work to be convincing that this defensive strategy is not going to be broken by the next deadline. There are some straightforward ideas from (such as EOT attack in the image domain) that could be useful here too.\n\nFinally, a mistake we made for image domain adversarial attacks was to focus on trivial, non-realistic threat models (e.g., pixel-wise L-norm bounded perturbations). It has been years since this threat model was proposed and we've yet to see a real-world attack that uses this strategy. With attacks like GCG, I think, we might be making the same mistake and focusing on threats that might not matter. There's some real-world evidence that semantic jailbreak prompts are a greater and harder-to-mitigate risk that attackers are already deploying [1]. There's an intellectual merit in studying more academic, theoretical threats but I think it is critical to guide the community towards studying real threats. I can't see this paper doing that. There's no discussion on whether this defense would work against semantic jailbreaks or other real-world strategies, and no discussion on the limitations of the defense in deployment. Considering we have years of literature behind us, I would like to see a real effort into asking the right questions, which starts with academic papers like this.\n\n\n\n\n\n\n\n[1] https://arxiv.org/abs/2308.03825"
                },
                "questions": {
                    "value": "- I'm not entirely sure about the reasoning behind returning one of the responses R_j (Line 5-6 in the Alg 1). How did you make this decision? What are the downsides compared to the other options you discussed in Section 6? I'm failing to have good intuition about this. Could you clarify?\n\n- How does the defense affect the model's response utility against very short prompts (which might have less redundancy to withstand perturbations)?\n\n- Do you use the input-specific GCG in your experiments? Input-specific GCG suffix is coupled with the original prompt itself, which could explain why small perturbations might break the attack. What about the universal version of GCG? Is that less brittle (I know it has a lower attack success rate)? It also changes the computational cost equation as the adversary needs to craft it only once."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786761320,
            "cdate": 1698786761320,
            "tmdate": 1699636030735,
            "mdate": 1699636030735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dKxytjgBvv",
                "forum": "xq7h9nfdY2",
                "replyto": "1K9DLsOTpy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1: Clarification regarding adv. examples vs. jailbreaking, theoretical guarantees"
                    },
                    "comment": {
                        "value": "Thanks for your review!  Here are some detailed comments.\n\n---\n\n## Adversarial examples vs. jailbreaking LLMs\n\nMuch of your review draws a parallel between two distinct fields: adversarial examples (AEs) in computer vision and jailbreaking LLMs. To be clear, we agree with you that there are many lessons to be learned from AEs research and that the fields of AEs and jailbreaking share some similarities. However, there is little-to-no evidence thus far that the jailbreaking field will be guided by exactly the same principles and suffer from the same pitfalls as AEs research did. As we discuss in Appendix E.2, these fields have fundamental differences:\n* Different problem settings (discriminative vs. generative models)\n* Different input spaces (continuous pixels vs. discrete tokens/strings)\n* Different threat models (imperceptible noise vs. jailbreaking prompts)\n\nAnd therefore, these fields may very well adhere to divergent standards of practice. At the very least, we respectfully disagree with your claim that we (the authors) \"know very well\" how to avoid the \"painful lessons\" from AEs research. There is no way to faithfully predict how this field will develop, and our paper should not be judged on this basis.\n\nIn this same vein, you call on us to \"guide the community\" toward best practices in the field. Although we are unsure of what the field will look like in the coming years, in proposing our set of criteria for candidate defenses (see Section 2.1), we sought to do *exactly* this. That is, we sought to guide the field toward several of the broad existing practices, many of which you mention, e.g., \n* An emphasis on adaptive attacks (criterion D1)\n* Nominal performance and efficiency trade-offs (criteria D2-D3)\n* Black-box vs. white-box comparisons (criterion D4)\n\nAs far as we know, **this is the only paper that attempts to provide such a \"guide\" in the field of jailbreaking**, and we hope that you agree that this constitutes a valuable contribution. We look forward to discussing this with you.\n\n---\n\n## Theoretical guarantees\n\n> \"I don't know why k-unstability would be a fundamental property of adversarial suffix attacks like GCG. It's an empirical observation for a particular attack but we know very well now that building defenses on attack-specific artifacts is not a way to go.\"\n\nWe respectfully disagree with several points here.\n\n* **\"Fundamental.\"** We never claim that $k$-instability is a \"fundamental property\" of GCG. It is a simplifying assumption, based on which we were able to prove Prop. 4.1.\n* **\"Empirical observation.\"** This observation also significantly improves robustness against semantic attacks. See Appendix H in our updated PDF, wherein we show that SmoothLLM significantly improves robustness against the PAIR semantic jailbreak [A], setting a new state-of-the-art.\n* **\"Know very well.\"**  The claim in the second sentence relies on the assumption that jailbreaking will follow the same trends as the AE literature. We are not aware of any evidence that supports this claim.\n\nIf you would like us to refer to $k$-instability as a \"simplifying\" assumption rather than \"mild\" (as we do in the second paragraph of Section 4) throughout our paper, we would be happy to add this language.\n\n> \"It only leads to a counterproductive arms race without yielding a long-lasting idea.\"\n\nThis claim is indeterminate. We hope you agree that there is no way that we, nor anyone else, can accurately predict what will constitute a \"long-lasting idea,\" especially given that the jailbreaking field is very young.\n\n> \"Considering that it would be generally easy for an attacker to detect if SmoothLLM defense is used (e.g., by looking at the model's responses), it also would be trivial to change up the attack strategy to break the defense.\"\n\nWe do not agree with these claims.\n* **\"Detect.\"** As far as we know, there is no clear way for an adversary to detect whether SmoothLLM is being used, and moreover a paper has never been written on this topic.\n* **\"Trivial.\"** We are not aware of any way to \"change up\" the attack strategy to break the defense. No paper has been written on this topic.\n\nBoth claims are unsupported by *any* evidence. If you can provide evidence to support these claims, we would be happy to discuss further.\n\n> \"Is there a reason why you believe k-unstability is a good assumption that will withstand future attacks or stronger adaptive attacks?\"\n\nIn our paper, we considered an existing and well-established threat model, and developed a theory that fits within this threat model. This theory contributed to the development of SmoothLLM, which (1) withstands GCG attacks (see Section 5), (2) withstands adaptive GCG attacks (see Appendix C), and (3) significantly reduces the success rate of the PAIR semantic jailbreaking attack (see Appendix H in our updated PDF). As we cannot anticipate what future attacks will be, we hope you agree that there is no way we can anticipate how SmoothLLM will fare against them."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147160814,
                "cdate": 1700147160814,
                "tmdate": 1700147160814,
                "mdate": 1700147160814,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tLyQ6xw6Zb",
                "forum": "xq7h9nfdY2",
                "replyto": "1K9DLsOTpy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 2: Adaptive attacks and our choice of threat model"
                    },
                    "comment": {
                        "value": "## Adaptive attacks\n\n> \"It's accepted now that adaptive attack evaluations should be at the forefront of defensive papers like this. . . I can't see much deeper thought in this paper in this regard.\"\n\nThis is incorrect.  There is no standard practice regarding adaptive attacks in the jailbreaking literature, especially given that SmoothLLM is the first defense paper in this field.  In this vein, we sought to guide the community toward considering adaptive attacks (see criteria D1 in Section 2.1).  This is why we devoted three pages in Appendix C to deriving an adaptive attack for the GCG threat model.  This took substantial effort, thought, and weeks of experimentation, and as far as we know, **this is the only adaptive jailbreaking attack in the literature.**  And crucially, Figure 12 demonstrates that **SmoothLLM is highly robust to the adaptive attack we derived**.\n\n> \"It definitely needs more work to be convincing that this defensive strategy is not going to be broken by the next deadline.\"\n\nWe do not agree.  In Figure 12, we provide concrete evidence that SmoothLLM is not susceptible to adaptive attacks.  Furthermore, we have no way of predicting whether this defense will be broken by the next deadline.  Could you be more specific regarding why you find this to be unconvincing?\n\n> It's trivial to defend against any particular attack (e.g., by detecting the attack's artifacts) but through many painful lessons, we know that adaptive attacks are hard to design and evaluate against. \n\nAgain, we respectfully disagree.  **Until our paper, there was no evidence that the GCG attack could be mitigated.**  Even now, four months after GCG was proposed, companies like OpenAI, Google, and Anthropic have not yet been able to mitigate this attack (see the next section of this rebuttal), which is strong evidence supporting the fact that defending against GCG is not \"trivial.\"  Therefore, the fact that SmoothLLM can reduce the attack success rates w/r/t both adaptive and non-adaptive versions of GCG is a core contribution, which we hope will help the field avoid some of the \"painful lessons\" you mention.\n\n---\n\n## Threat models\n\n> \"With attacks like GCG, I think, we might be making the same mistake and focusing on threats that might not matter.\"\n\nWe respectfully disagree: There is no evidence that GCG \"might not matter.\" On the contrary, we argue that it matters just as much as any other jailbreak.  Consider the following timeline.\n\n* **July 27, 2023: GCG appears on arXiv.** Table 1 in the GCG paper showed that GCG was frequently able to jailbreak ChatGPT, Claude, and PaLM.\n* **Sept. 28, 2023: ICLR submission deadline.** Figure 1 in our paper shows that as of the submission deadline, OpenAI, Anthropic, and Google were unable to mitigate GCG attacks.\n* **Nov. 14, 2023: ICLR rebuttal period.** We reran the experiment described by Figure 1 in our paper and we got nearly identical results.\n\nThis evidence indicates that GCG *still* consistency jailbreaks state-of-the-art LLMs, including ChatGPT, which has nearly 200 million active users.  Resolving this highly realistic threat is essential and evidently, as the jailbreak still works as of this week, nontrivial.  Given this, could you explain why you feel that the GCG threat model is \"trivial\" and \"non-realistic?\" \n\n> \"There\u2019s some real-world evidence that semantic jailbreak prompts are a greater and harder-to-mitigate risk that attackers are already deploying [1]\"\n\nAgain, we respectfully disagree with several of the points raised here.\n\n* **\"Greater risk\".** Token-level jailbreaks like GCG and semantic jailbreaks like DAN [1] both pose the same risk: Both can jailbreak state-of-the-art LLMs.\n* **\"Harder-to-mitigate risk.\"**  We are not aware of any published literature that conclusively proves that semantic jailbreaks are \"harder-to-mitigate.\"  **The only way to determine which kinds of jailbreaks are harder to defend against is to publish papers like SmoothLLM**, which demonstrates how one can mitigate specific jailbreaking algorithms.  This is precisely the contribution of our paper.\n* **\"Already deployed.\"** GCG and related token-based variants *are* \"already deployed.\" They have been used to jailbreak LLMs, elicit hallucinations [B], produce toxic text [C].\n\n> There's an intellectual merit in studying more academic, theoretical threats but I think it is critical to guide the community towards studying real threats. I can't see this paper doing that.\n\nWe do not understand this sentence.  You seem to draw a line between \"theoretical\" and \"real\" threats, but it's unclear to us why GCG is \"academic\" and DAN is \"real.\"  Both algorithms pose very real threats, in the sense that they can jailbreak state-of-the-art LLMs.  Could you be more specific about how you draw this line, and point to references that back up this sentiment?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147468806,
                "cdate": 1700147468806,
                "tmdate": 1700147468806,
                "mdate": 1700147468806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gOvuTrzLBA",
                "forum": "xq7h9nfdY2",
                "replyto": "1K9DLsOTpy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 3: Answering your additional questions"
                    },
                    "comment": {
                        "value": "> \"There\u2019s no discussion on whether this defense would work against semantic jailbreaks or other real-world strategies, and no discussion on the limitations of the defense in deployment.\"\"\n\nAs mentioned above, GCG *is* a \"real-world strategy,\" in the sense that it extracts *real* harmful content out of commonly used LLMs like ChatGPT, which has hundreds of millions of active users.  However, we'd be happy to add a discussion regarding the applicability of SmoothLLM to semantic jailbreaks.  In fact, we ran SmoothLLM on prompts generated for Vicuna using PAIR, a recently proposed semantic jailbreak [A].  We found that **SmoothLLM reduces the ASR of this jailbreak by as much as 40%** (see Appendix H in our updated PDF).  As far as we know, this sets the state-of-the-art in defending against the PAIR jailbreak. We hope that you agree that this broadens the contribution of our paper.\n\n---\n\n## Additional questions\n\n> \"I'm not entirely sure about the reasoning behind returning one of the responses R_j (Line 5-6 in the Alg 1). How did you make this decision? What are the downsides compared to the other options you discussed in Section 6? I'm failing to have good intuition about this. Could you clarify?\"\n\nWe view SmoothLLM as a wrapper around any vanilla black-box LLM (see Figure 2).  In this way, given a prompt as input, we designed SmoothLLM to return a *single* response as output.  Since smoothing generally relies on *voting* to determine the output, we found it natural to return a response consistent with the majority vote.\n\nWe are excited about improving SmoothLLM and extending it to different problem settings in future work.  Here is what we had in mind for each of the other variants listed at the end of Section 6 (in the order in which we mentioned them). \n* **Abstaining when an attack is detected.** This would be useful if you want to *ensure* that harmful content never reaches the user.  However, the downside is that refusing to generate any text when a jailbreak is detected would complicate the process of evaluating the ASR and result in unnecessary conservatism, which is why we used the voting approach in the paper.\n* **Generating based on unperturbed prompts.** This idea could be used to improve the nominal/clean performance.  As jailbreaks are unlikely to be detected for \"clean\" prompts, this may result in better trade-offs.  However, this also makes the LLM more vulnerable to attacks, since malicious prompts that evade the detector could be processed in their unperturbed forms, potentially exposing the user to objectionable content.\n* **Denoising perturbed prompts.** Denoising the perturbed prompts could change the adversarial suffixes enough to render the attack benign, and this could also improve clean/nominal performance.  However, there is evidence that such techniques are highly susceptible to adaptive attacks (see, e.g., [D]), which is why we didn't go down this road.\n\nWe hope this gives you insight into our design process for SmoothLLM.  And all this being said, we are excited about studying each of these variants in future work.\n\n> \"How does the defense affect the model's response utility against very short prompts (which might have less redundancy to withstand perturbations)?\"\n\nOn average, goal prompts in the harmful behaviors dataset are 72 characters long, and suffixes are ~100 characters long (see Table 2).  As goal prompts get shorter, perturbations to the prompt are more likely to degrade the semantic content since they contain less redundant information (as you mention), which could potentially weaken the robustness vs. nominal performance trade-off.  However, as long as the GCG attack stays fixed, shorter goal prompts mean that perturbations are more likely to affect the suffix, which would mean that smaller values of $q$ could be used, which would improve the aforementioned trade-off.  Furthermore, it's difficult to write prompts that request toxic content in fewer than ~50 characters anyway, so this case isn't particularly concerning.\n\n> \"Do you use the input-specific GCG in your experiments?\"\n\nYes, we use the instance-specific version.  \n\n> \"What about the universal version of GCG?\"\n\nOn the one hand, this version requires significantly more compute, as it essentially requires running an algorithm with similar complexity to the instance-specific version for $M$ goal prompts.  On the other hand, as you mention, it only needs to be run once.  We expect that this version is just as unstable as the instance-specific suffixes.  We are running this experiment now, and will let you know when it completes and what conclusions we can draw.\n\n---\n\n[A] \"Jailbreaking black box large language models in twenty queries\"\n[B] \"LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples\"\n[C] \"Automatically Auditing Large Language Models via Discrete Optimization\"\n[D] \"Language Model Detectors Are Easily Optimized Against\""
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147602138,
                "cdate": 1700147602138,
                "tmdate": 1700147602138,
                "mdate": 1700147602138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9uYBWuS780",
            "forum": "xq7h9nfdY2",
            "replyto": "xq7h9nfdY2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1045/Reviewer_rsea"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1045/Reviewer_rsea"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed SmoothLLM to mitigate jailbreaking attacks on LLMs. The method is based on the finding that adversarially-generated prompts are brittle to character-level changes. Specifically, the defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. found that adversarially-generated prompts are brittle to character-level changes\n2. proposed a new algorithm for defending against jailbreaking attacks in llm\n3. The main idea follows randomized smoothing in image domain and provide some theoretical results"
                },
                "weaknesses": {
                    "value": "1. The \u201crobustness guarantee\u201d that generalizes the original randomized smoothing to the LLM setting in this paper does not seem to be a valid \u201cguarantee\u201d, as it actually depends on some unverifiable assumption (k-unstable). Therefore, different from those traditional robustness guarantees, where one could verify that some examples must be robust, the \u201cguarantee\u201d in this paper cannot provide any real certified robust accuracy. In this sense, I don\u2019t think the provided theorem provides any type of formal guarantee to robustness here. It seems a bit misleading to call it a formal robustness guarantee.\n\n2. From the algorithm design, the proposed algorithm can easily have degraded nominal performances (since the output is randomly sampled from the perturbed input\u2019s response and each perturbed input has changed the input quite a lot). Although the authors also consider that as one of the major aspects to test, I don\u2019t see that part and the corresponding experiments very convincing.  Table 3 actually suggests that when N is a bit large, the nominal performance significantly drops. More importantly, it only tested on light perturbation cases where q <= 5%, while in the main experiments, most were conducted in the case where q = 10/15/20%. This makes me very concerned about the actual use case of the proposed algorithm. \n\n3. The discussion on the efficiency part seems also misleading. All the comparison listed in this paper seems to compare the SmoothLLM with GCG, which is totally unreasonable to me (take an analogy, this is similar to comparing the complexity of randomized smoothing and adversarial attack). I didn\u2019t see this comparison has too much meaning here. Since SmoothLLM is basically an inference-time defense, shouldn\u2019t the authors compare it with normal LLM inference? And that seems to be a solid N times larger than the normal inference in my opinion. Since the inference time is one major obstacle in modern LLM deployment, SmoothLLM does not really seem efficient at all.\n\n4. One important aspect that seems to be missing is the adversarial prompt length. Essentially in this setting, there is no traditional invisible adversarial length constraint (like in traditional adversarial examples, usually we are only allowed to change one/few works/characters). Basically, your adversarial prompt can be any length, as long as it successfully breaks the alignment. However, I am a bit concerned that under such situations where the proposed attack still work? From the theoretical guarantee, I didn\u2019t thoroughly check the proof but the original randomized smoothing cannot provide meaningful results when the adversarial part is large. From the empirical perspective, if we just programmed GCG to generate a long adversarial prompt (not the only way but you can imagine many different ways to achieve the goal) would it still work? Or it still work, but may need a significantly larger perturbation ratio q and thus lead to degraded nominal performances?"
                },
                "questions": {
                    "value": "1. I do not quite understand why the authors claimed that for the efficiency objective, \u201cSmoothLLM is independent of the prompt length\u201d?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698896549501,
            "cdate": 1698896549501,
            "tmdate": 1699636030675,
            "mdate": 1699636030675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L7R12whGmo",
                "forum": "xq7h9nfdY2",
                "replyto": "9uYBWuS780",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1: What is meant by \"robustness guarantees,\" clarifications regarding non-conservatism"
                    },
                    "comment": {
                        "value": "## Robustness guarantees\n\n> \"The 'robustness guarantee' that generalizes the original randomized smoothing to the LLM setting in this paper does not seem to be a valid 'guarantee', as it actually depends on some unverifiable assumption (k-unstable).\" \n\nThis is incorrect in several senses.\n* **\"Generalize.\"** Prop. 4.2 does not \"generalize\" randomized smoothing (RS).  RS and SmoothLLM are different algorithms designed for different problems; neither generalizes the other.\n* **\"Unverifiable.\"** The $k$-instability assumption is not \"unverifiable.\"  It is verifiable, although checking whether it holds would take exponential time in the worst case.\n* **\"Valid guarantee.\"** Unless you can point to a mistake in our proof, our guarantee in Prop. 4.2 is completely sound and valid. That you find the assumption of $k$-stability to be difficult to check does not impact whether the result is valid.\n\nIf you disagree with any of these bullets, we would be glad to discuss further.\n\n> \". . .different from those traditional robustness guarantees, where one could verify that some examples must be robust, the \u201cguarantee\u201d in this paper cannot provide any real certified robust accuracy.\"\n\nYou are making a false comparison between two distinct fields: adversarial examples (AEs) and jailbreaking.  As discussed in Appendix E.2, these fields involve distinct problem settings and threat models, which necessitate different kinds of robustness guarantees. Although RS and SmoothLLM both use \"smoothing\" techniques, our goal is *not* to replicate \"traditional robustness guarantees\" from the AEs literature.  Rather, we prove a guarantee in Prop. 4.2 that makes sense in the context of jailbreaking.  Furthermore, to be clear: **We never claim to provide a robustness *certificate***; the words \"certified\" and \"certificate\" do not appear in our paper.\n\n> \"In this sense, I don\u2019t think the provided theorem provides any type of formal guarantee to robustness here. It seems a bit misleading to call it a formal robustness guarantee.\"\n\nWe do not agree.  Our result is formal and provides a guarantee on robustness under the assumption of $k$-instability; we describe Prop. 4.2 in precise langauge and formally prove that it holds in Appendix A.  Could you explain why you think our result is not a \"formal robustness guarantee?\"\n\n---\n\n## Non-conservatism\n\n> \"From the algorithm design, the proposed algorithm can easily have degraded nominal performances. . . I don\u2019t see that part and the corresponding experiments very convincing.\"\n\nRegarding your being unconvinced, here is what the evidence says:\n\n* For Vicuna, if you set $q=5\\%$, set $N=20$, and use swap perturbations, sacrificing between $5$-$10\\%$ in nominal performance (see Table 4) reduces your susceptibility to jailbreaking from $98\\%$ to around $10\\%$ (see Figure 7).\n\nTherefore, as is the case in nearly every paper ever written on robustness in ML, improving robustness comes at the (relatively small) cost of trading off some nominal performance.  **Our paper is the first to study this trade-off for jailbreaking, and we feel that this constitutes a notable contribution**.  If you still disagree, could you be more specific about what you find to be unconvincing?\n\n> \"Table 3 actually suggests that when N is a bit large, the nominal performance significantly drops.\"\n\nThis is incorrect.  In every nearly every experiment in Table 4, nominal performance improved as $N$ was increased from 2 to 20.\n\n> \"This makes me very concerned about the actual use case of the proposed algorithm.\"\n\nThe goal of our paper is not to solve jailbreaking in one shot by designing an algorithm that is without any drawbacks.  Rather, our goal is to contribute to a field of study that will, we believe, lead to the discovery of strong defenses like SmoothLLM. And in particular, while there are trade-offs, the \"actual use case\" for SmoothLLM is this: \n\n* **SmoothLLM is the first algorithm that provides *any* robustness against GCG attacks.**\n\nWhile there may be trade-offs and/or aspects of the algorithm that could be improved in future work, we strongly feel that our paper advances research on this topic.  Do you disagree?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147805927,
                "cdate": 1700147805927,
                "tmdate": 1700147805927,
                "mdate": 1700147805927,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6i0YDV8oCm",
                "forum": "xq7h9nfdY2",
                "replyto": "9uYBWuS780",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 2: Clarifications regarding efficiency and the adversarial suffix length"
                    },
                    "comment": {
                        "value": "## Efficiency\n\n> \"Since SmoothLLM is basically an inference-time defense, shouldn\u2019t the authors compare it with normal LLM inference?\" \n\nThis comparison is provided in our paper.  Undefended LLMs are $N$ times more query efficient than SmoothLLM.  See the paragraph in Section 3 titled \"Aggregation step.\"\n\n> \"The discussion on the efficiency part seems also misleading. All the comparison listed in this paper seems to compare the SmoothLLM with GCG, which is totally unreasonable to me.\"\n\nIn Section 3, we compared the efficiency of undefended LLMs and SmoothLLM.  Following this, we thought that it would be valuable to provide another perspective on efficiency, so in Section 5, we compared the query efficiency of the attack (GCG) to our defense (SmoothLLM). We find it interesting that although creating a GCG suffix requires 256,000 queries, defending the corresonding attack takes only ~10 queries.  \n\nIf you think it would make our paper stronger, we would be happy to move this ablation study to the appendix, but we feel strongly that it is not a weakness of our paper.  If you disagree, could you explain in more detail why you think this comparison is \"totally unreasonable\" and \"misleading\" given that we already provided the comparison you asked for?\n\n> \"Since the inference time is one major obstacle in modern LLM deployment, SmoothLLM does not really seem efficient at all.\"\n\nYou are correct that SmoothLLM is less query efficient than normal LLM inference.  As we all know, when it comes to robustness, there is usually no free lunch, and indeed our work shows that there is a trade-off between efficiency and robustness.  We hope you agree that it's unreasonable to expect a defended algorithm to have exactly the same efficiency as an undefended LLM.\n\nFurthermore, here's a point of comparison.  For $\\ell_2$ randomized smoothing in the setting of adversarial examples, one of the seminal papers (Cohen et al., 2019) uses $N=100,000$ random samples.  In the separate setting of jailbreaking, we show that SmoothLLM obtains strong robust for $N=10$ samples.  Therefore, one contribution of our work is to show that the robustness-efficiency trade-off is far milder in jailbreaking than it is for adversarial examples. \n\n---\n\n## Adversarial suffix length\n\n> \"From the theoretical guarantee, I didn\u2019t thoroughly check the proof but the original randomized smoothing cannot provide meaningful results when the adversarial part is large.\"\n\nThis comment highlights a strength of our method.  Unlike the adversarial examples version of randomized smoothing, increasing the adversarial budget (i.e., the suffix length) will actually make the defense stronger, since the random perturbations will have a better chance of hitting the suffix.  For details, see Lemma A.2.\n \n> \"From the empirical perspective, if we just programmed GCG to generate a long adversarial prompt. . . would it still work?\"\n\nYes, since $q$ is a percentage, SmoothLLM is agnostic of the prompt length.  In fact, as discussed above, as the suffix length increases, the robustness of SmoothLLM will improve.\n\n> \"One important aspect that seems to be missing is the adversarial prompt length.\"\n\nWe would be happy to add the above discussion to the paper if you feel it would make our paper stronger."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147888453,
                "cdate": 1700147888453,
                "tmdate": 1700147888453,
                "mdate": 1700147888453,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NR0RwaM8Zi",
            "forum": "xq7h9nfdY2",
            "replyto": "xq7h9nfdY2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1045/Reviewer_Utne"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1045/Reviewer_Utne"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a defense algorithm to mitigate jailbreaking attacks on LLMs. It works by first randomly perturbing the input prompt (via insert, swap or batch), and then conducting a majority voting of the resulting predictions to detect adversarial inputs. Provable guarantees are also provided on attack mitigation. Experiments show that the defense reduces ASR and maintains a certain utility on clean inputs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Defending against jailbreaking attacks of LLM is an important problem for trustworthy LLMs in practice;\n\n2. The proposed method adapts the randomized smoothing principle to LLM, and conducted extensive evaluation to empirically demonstrates its ability for defending jailbreaking attacks;\n\n3. The paper presentation is clear and easy-to-follow."
                },
                "weaknesses": {
                    "value": "1. The major concern is that perturbing the prompts could greatly influence the LLM\u2019s original behavior. The provided evaluation of non-conservatism is only based on rather simple tasks (i.e., classification), which does not verify whether the LLM can still have normal generation behavior on randomly perturbed prompts.\n\n2. The proposed method is based on the observation that adversarial suffixes are fragile to character-level perturbations, ignoring the (un)stability of normal prompts to such perturbation. The paper did not draw a clear boundary (either theoretically or empirically) of how perturbation only destroys the adversarial suffix and maintains the semantics of normal ones.\n\n3. The theoretical guarantee relies on the assumption, k-stable, that cannot be verified or calculated in practice (unlike the assumption in randomized smoothing), and can hardly be \u201crealistic\u201d. The resulting theoretical guarantee thus is not really a rigorous one. For instance, Figure 6 is a conceptual result, instead of a guarantee calculated on real prompts. \n\n4. While the paper has compared its method with randomized smoothing (e.g., image v.s. text), it is still a straightforward application of the perturbation-then-voting principle, which limits its technical novelty.\n\n5. The paper has several statements that could be over-claimed and misleading, regarding \u201cthe first defense\u201d, theoretical results and empirically conclusions. See details in Questions."
                },
                "questions": {
                    "value": "1. As said in weakness 1, character-level perturbation may destroy prompt semantics and confuse LLM. Therefore I am concerned with the true cause of the decreased ASR by the proposed method. It may not necessarily be the result of destroying the adversarial suffix, instead, it is possible that the perturbation destroys the semantics of the malicious question, thus the LLM model responds with something like \u201cSorry, I cannot understand\u201d. Did the authors observe such cases? And a possible evaluation to verify such a case: using the behavior dataset without adversarial suffix (i.e., only keeping the malicious question part), then using an unaligned version of LLM, and checking its ASR drop after random perturbation.\n\n2. The provided non-conservatism evaluation is insufficient to really validate that SmoothLLM maintains nominal performance, because: 1) these tasks are simple (e.g., classification tasks); 2) the accuracy drop is actually relatively large even when q is set small and N is large (e.g., random guess on PIAQ is already 50, but SmoothLlama2 with q=2 and N=6 gives 59, which is a large drop compared with original performance 76.7). Can the authors provide a more convincing evaluation of nominal behavior of LLMs, e.g., on generation tasks?\n\n3. In table 3 of the non-conservatism evaluation, are the numbers the average results of all N samples? What are the variances?\n\n4. As said in weakness 2, SmoothLLM is built based on the observation in Figure 4 that adversarial suffixes are fragile to character-level perturbations. Then an implicit assumption is that clean prompts are more stable to such perturbations. Can the authors provide corresponding evidence to verify (in)stability of clean prompts, similar to Fig. 4?\n\n5. The theoretical proof is provided for swap and batch perturbations, instead of \u201cother perturbation types\u201d. Is the guarantee/proof applied to the insert perturbation? \n\n6. The paper claims the efficiency of SmoothLLM by comparing the runtime with attacks (e.g., GCG). This seems to be an unreasonable comparison as one is defense and other is attack. Can the authors soften this claim and focus on the comparison of SmoothLLM and vanilla LLM (and other defenses)?\n\n7. The paper emphasizes in multiple places that this is the first algorithm to defend LLM jailbreaking attacks, which could be over-claimed as several attempts have been proposed [1, 2]. Can the authors discuss them and adjust corresponding claims if this concern makes sense?\n\n[1] Kumar, Aounon, et al. \"Certifying llm safety against adversarial prompting.\" arXiv preprint arXiv:2309.02705 (2023).\n\n[2] Jain, Neel, et al. \"Baseline defenses for adversarial attacks against aligned language models.\" arXiv preprint arXiv:2309.00614 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699148234740,
            "cdate": 1699148234740,
            "tmdate": 1699636030611,
            "mdate": 1699636030611,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fLiOst1U8D",
                "forum": "xq7h9nfdY2",
                "replyto": "NR0RwaM8Zi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1: Clarifications regarding non-conservatism and our theoretical guarantees"
                    },
                    "comment": {
                        "value": "## Non-conservatism\n\n> The provided evaluation of non-conservatism is only based on rather simple tasks (i.e., classification), which does not verify whether the LLM can still have normal generation behavior on randomly perturbed prompts. . . Can the authors provide a more convincing evaluation of nominal behavior of LLMs, e.g., on generation tasks?\n\nWe disagree.  We used three well-established benchmarks---TruthfulQA, ToxiGen, and PIQA---which are commonly used to evaluate the performance of LLMs.  **All three datasets were used in the Llama2 paper [A].**  Beyond doing a human evaluation, we are not aware of any datasets that satisfactorily measure \"normal generation,\" or what it would mean for any generated text to be \"normal.\"  If you have a dataset in mind that measures generation, we would be happy to evaluate our method on it.\n\n> \". . .character-level perturbation may destroy prompt semantics and confuse LLM. Therefore I am concerned with the true cause of the decreased ASR by the proposed method. It may not necessarily be the result of destroying the adversarial suffix, instead, it is possible that the perturbation destroys the semantics of the malicious question, thus the LLM model responds with something like \u201cSorry, I cannot understand\u201d. Did the authors observe such cases?\"\n\nYour question was addressed in our original submission.  We encourage the reviewer to look at Appendix D---entitled \"The Incoherency Threshold\"---which shows an example wherein the LLM cannot understand the perturbed prompt, and thus responds with \"I must inform you that the statement you provided is not factually coherent and does not make sense to me.  The question contains a series of unrelated words and symbols that do not form a valid question.\"  We provide guidelines for how to avoid this scenario in Appendix D.  If the reviewer has any questions about this, we would be happy to discuss further.\n\n> And a possible evaluation to verify such a case: using the behavior dataset without adversarial suffix (i.e., only keeping the malicious question part), then using an unaligned version of LLM, and checking its ASR drop after random perturbation.\n\nThis is an interesting suggestion!  Do you have any suggestions for unaligned LLMs?  We would be happy to run this experiment if you feel that it will make our paper stronger.\n\n> \"In table 3 of the non-conservatism evaluation, are the numbers the average results of all N samples? What are the variances?\"\n\nWe think that there might be a misunderstanding.  As you rightly said elsewhere in your review, our method uses a \"perturbation-then-voting principle.\"  The figures in Table 3 use the same SmoothLLM algorithm (i.e., voting), and so there are no variances to report.\n\n> \"The proposed method is based on the observation that adversarial suffixes are fragile to character-level perturbations, ignoring the (un)stability of normal prompts to such perturbation. . . Can the authors provide corresponding evidence to verify (in)stability of clean prompts, similar to Fig. 4?\"\n\nWe are confused by this question.  This evidence is provided in Table 3 of our paper.  Could you be more specific about the evidence that you would like us to provide?\n\n---\n\n## Theoretical guarantees\n\n> The theoretical guarantee relies on the assumption, k-stable, that cannot be verified or calculated in practice (unlike the assumption in randomized smoothing), and can hardly be \u201crealistic\u201d.\n\nThis is incorrect.  The $k$-instability assumption can be calculated on real prompts, although checking whether it holds would take exponential time in the worst case.  However, the difficulty involved in checking this assumption doesn't impact whether it is realistic; Figure 4 provides empirical evidence that this assumption is realistic for real prompts.  If you would like us to change the word \"realistic\" to \"simplifying,\" we would be happy to do so.\n\n>The resulting theoretical guarantee thus is not really a rigorous one. For instance, Figure 6 is a conceptual result, instead of a guarantee calculated on real prompts.\n\nWe disagree with this for several reasons.\n* **\"Rigorous\" is not the opposite of \"conceptual.\"**  Unless you can point to a mistake in our proof, we hope you agree with us that Prop. 4.1 is completely sound and rigorous.\n* **\"Real prompts.\"** Our result *can* be calculated on \"real prompts,\" although in the worst case, computing $k$-instability may take exponential time.\n\nIf you disagree with either of these points, we would be happy to discuss further.\n\n> The theoretical proof is provided for swap and batch perturbations, instead of \u201cother perturbation types\u201d. Is the guarantee/proof applied to the insert perturbation?\n\nBy \"other perturbation types,\" we meant *patch* perturbations, since we did not have space to fit the patch perturbation proposition statement in the main text.  Do you feel that including the insert perturbations proof would make our paper stronger?  If so, we would be happy to provide it."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148391972,
                "cdate": 1700148391972,
                "tmdate": 1700148391972,
                "mdate": 1700148391972,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YXLiRfcACS",
                "forum": "xq7h9nfdY2",
                "replyto": "NR0RwaM8Zi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 2: Clarifications regarding efficiency, novelty, and concurrent work"
                    },
                    "comment": {
                        "value": "## Efficiency\n\n> The paper claims the efficiency of SmoothLLM by comparing the runtime with attacks (e.g., GCG). This seems to be an unreasonable comparison as one is defense and other is attack. Can the authors soften this claim and focus on the comparison of SmoothLLM and vanilla LLM (and other defenses)?\n\nWe disagree with several points here.\n\n* **\"Vanilla LLMs.\"**  A comparison against vanilla LLMs is already provided in our paper.  Vanilla LLMs are $N$ times more query efficient than SmoothLLM.  See the paragraph in Section 3 titled \"Aggregation step.\"\n* **\"Unreasonable comparison.\"** After comparing to vanilla LLMs, we thought that it would be valuable to provide another perspective on efficiency, so in Section 5, we compared the query efficiency of the attack (GCG) to our defense (SmoothLLM). We find it interesting that although creating a GCG suffix requires 256,000 queries, defending the corresponding attack takes only ~10 queries.  If you like, we could move this ablation study to the appendix.\n* **\"And other defenses.\"** As we discuss in the \"Concurrent work\" section of this rebuttal, there are currently no published defenses on this topic.  [1] and [2] first appeared on arXiv three weeks before the submission deadline, and therefore it is unreasonable to ask us to compare to them.\n\nWe would be happy to discuss any/all of these points further.\n\n---\n\n## Novelty\n\n> \"While the paper has compared its method with randomized smoothing (e.g., image v.s. text), it is still a straightforward application of the perturbation-then-voting principle, which limits its technical novelty.\"\n\nThe novelty of our approach is in adapting classical ideas from the smoothing literature to the new and pressing problem of jailbreaking LLMs.  While smoothing is an existing technique, we hope you agree that **our paper is the first to show that the attack success rate of GCG can be significantly decreased** via a smoothing-based technique, which is (in our opinion) a highly novel contribution.\n\n---\n\n## Concurrent work\n\n> \"The paper emphasizes in multiple places that this is the first algorithm to defend LLM jailbreaking attacks, which could be over-claimed as several attempts have been proposed [1, 2]. Can the authors discuss them and adjust corresponding claims if this concern makes sense?\"\n\nWe appreciate you pointing us toward these papers.  However, please consider the following:\n* [2] appeared on arXiv on Sept. 1, 2023.\n* [1] appeared on arXiv on Sept. 6, 2023.\n* The ICLR submission deadline was Sept. 28, 2023.\n* Both [1] and [2] are currently under review at ICLR.\n\nWe hope the reviewer agrees with us that (a) **it is unreasonable to ask us to compare to works that appeared on arXiv *three weeks before* as the ICLR submission deadline** and (b) **these works should be viewed as concurrent to our paper**.  If you would like us to specify that these works are concurrent in our main claims, we would be happy to do so.\n\nMoreover, here is a summary of these two works, which we would be happy to add to our related works section:\n\n* In [1], the authors consider several candidate defenses, including preprocessing using a perplexity filter, paraphrasing input prompts, and employing adversarial training.  Results for these methods are mixed; while heuristic detection-based methods perform strongly, adversarial training is shown to be infeasible given the computational cost of retraining LLMs. \n* In [2], the authors propose a technique which provides certifiable robustness guarantees by applying a safety filter on sub-strings of input prompts.  While promising, the complexity of this method scales with the length of the input prompt, which is undesirable.  Moreover, unlike our work, [2] does not evaluate their proposed defense against GCG attacks.\n\n---\n\n[A] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148519455,
                "cdate": 1700148519455,
                "tmdate": 1700157221283,
                "mdate": 1700157221283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]