[
    {
        "title": "Goodhart's Law in Reinforcement Learning"
    },
    {
        "review": {
            "id": "yfeAnO2dgn",
            "forum": "5o9G4XF1LI",
            "replyto": "5o9G4XF1LI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6832/Reviewer_MUg9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6832/Reviewer_MUg9"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the interplay between reward misspecification and optimisation in RL as an instance of Goodhart's law. The authors provice geometric explanations of how optimisation of a misspecified reward function can lead to worse performance beyond some threshold, and show in experiments that several environments and reward functions are prone to Goodhart's law and optimisation of a reward proxy eventually leads to worse performance. The authors also propose an early stopping algorithm to address this problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- First of all, the studied topic is, in my opinion, important and could be of interest to many in the ICLR community. \n- The paper is a good attempt at extending prior work on reward misspecification and reward gaming (e.g., Skalse et al. 2022) to the question of what role optimisation plays and whether we can characterize reward misspecification from a policy optimisation standpoint as well. I am not very well acquainted with the related work, but the contributions and many of the ideas in this paper seem novel to me.\n- The results are very interesting and provide some nice intuition about the interplay of reward distance, optimisation and MDP model. While I don't think that one should overinterpret the results as they are either based on empirical studies of a some specific set of environments or on theoretical insights with idealised assumptions, I think that the findings of this paper are overall very interesting."
                },
                "weaknesses": {
                    "value": "- The evidence on the \"Goodharting\" effect are only circumstantial. Experiments on some specific set of environments such as grid worlds do not necessarily allow us to extrapolate. After all, the Goodharting effect can only be \"explained\" but not characterised. Nevertheless, these experiments and the geometric explanations provide good intuition which I think is very interesting and could inspire future lines of work. \n- A minor weakness is that the proposed early stopping algorithm might not perform well due to large reward losses from stopping early, which is somewhat expected due to its pessimistic nature. The algorithm is also fairly impractical bcause it assumes prior knowledge of $\\theta$."
                },
                "questions": {
                    "value": "- Your work seems to be tailored to the specific choice of difference metric between two reward functions (their angle). I guess that the main reason for choosing this distance metric is that it is a STARC metric. \n\t- However, can you provide further justification for why the \"angle\" is a good choice or even the *right* choice? \n\t- What could another reasonable metric be?  \n\t- And, how would choosing a different metric impact your results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6832/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6832/Reviewer_MUg9",
                        "ICLR.cc/2024/Conference/Submission6832/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698619857521,
            "cdate": 1698619857521,
            "tmdate": 1700696782914,
            "mdate": 1700696782914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WpDQchANXW",
                "forum": "5o9G4XF1LI",
                "replyto": "yfeAnO2dgn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We kindly thank you for your review.\n\n - *A minor weakness is that the proposed early stopping algorithm might not perform well due to large reward losses from stopping early, which is somewhat expected due to its pessimistic nature.* It is true that the pessimistic nature of the algorithm leads to the loss of reward. However, as we point out in Section 5 (Figure 5b), if the proxy is not too far from the true reward, then one can probably expect the loss of reward to be fairly small. However, we think that In practice, early stopping algorithm will not be used on its own, but rather as part of the iterative improvement algorithm (described in Appendix F). This is how our work might lead to some sort of improved RLHF training regime that provably improves Goodharting. (We can then make calls to the true reward function through human feedback to estimate theta, and conservative stopping just translates to collecting human feedback throughout the training process, rather than all at once - which is also how RLHF training processes currently look).\n\n - *The algorithm is also fairly impractical because it assumes prior knowledge of \u03b8.* We argue that knowing the upper bound of the distance between the true and proxy reward functions is not an extremely unrealistic assumption. For example, for a given *reward learning algorithm,* we might have certain bounds or rates of convergence to the true reward. Inverse reinforcement learning algorithms often come with proofs of their convergence rates - see e.g. \"Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees\u201d, Zeng et al., 2022. Using those convergence rates, we could get an approximation of the distance to the true reward. Note that we can always make the bound less tight, and therefore trade off the accuracy of the input to the early stopping and the optimality of the stopping point, which means that even in the presence of a weak signal our approach might still be useful.\n\n - *Your work seems to be tailored to the specific choice of difference metric between two reward functions (their angle). I guess that the main reason for choosing this distance metric is that it is a STARC metric. However, can you provide further justification for why the \"angle\" is a good choice or even the right choice?* We have actually developed the angle metric independently, and only later realised that it is indeed a STARC metric. When working in the occupancy measure space, it is natural to consider the angle between the (projected) rewards - the dot product arises naturally when considering the distances between (normalised) vectors in the Euclidian space. We also note that Skalse et al. 2023, \u201cInvariance in Policy Optimisation and Partial Identifiability in Reward Learning\u201d (indirectly) show that many reward learning algorithms will converge to a reward with STARC-distance 0 to the true reward.\n\n   - *What could another reasonable metric be?* Our angle metric can be understood as an expectation $E_{S \\sim U, A \\sim U}[R(S, A) R(S, A)]$ , where both states and actions come from the uniform distribution. It would be interesting to develop a better understanding of the metric of the shape $ E_{S,A \\sim D}[R(S,A), R[S, A])$ for a larger class of distributions $D$ over states and actions. Particularly interesting are the distributions induced by a particular policy $D_\\pi$. Note that this is, in a way, no longer independent of where we are on the polytope $\\Omega$, which presents more conceptual difficulties.\n\n   - *And, how would choosing a different metric impact your results?* As long as we work with a STARC metric, we can always rescale the by a strictly monotonic function and get back to the proper angle. If we use a different distance, our geometric intuition might no longer hold."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482799580,
                "cdate": 1700482799580,
                "tmdate": 1700482799580,
                "mdate": 1700482799580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K44GYLlE3F",
                "forum": "5o9G4XF1LI",
                "replyto": "WpDQchANXW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6832/Reviewer_MUg9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6832/Reviewer_MUg9"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thank you for your response. Reading the other reviewers' comments and briefly going through the paper again, I've become a bit unsure whether the results of Section 5 are as significant as I originally deemed them to be. It seems that a statement such as in Corollary 1 could also be derived without much additional work from well-known results like the simulation lemma, which is anyways quite similar I guess. Nevertheless, I am still in favour of acceptance, though, I am reducing my confidence."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696769191,
                "cdate": 1700696769191,
                "tmdate": 1700696769191,
                "mdate": 1700696769191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CGXUwDNoh9",
            "forum": "5o9G4XF1LI",
            "replyto": "5o9G4XF1LI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6832/Reviewer_eHjA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6832/Reviewer_eHjA"
            ],
            "content": {
                "summary": {
                    "value": "Most reinforcement learning algorithms are designed for accurate reward feedback. However, in practice, accurate reward feedback may not be available. In the presence of inaccurate reward feedback, it is possible to observe a phenomenon that the performance of the training policy first increases and then decreases after passing a threshold point. This paper addresses this interesting phenomenon and names it \u201cGoodhart\u2019s Law in RL\u201d. To solve this problem, this paper quantifies the magnitude of this effect and how it exists in a wide range of environments and reward functions. It provides a geometric explanation and an optimal early stopping method with theoretical regret bounds. They then empirically showed the performance of their early stopping method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is quite novel because it raises an interesting and important observation \u2013 the performance of a policy increases first and then decreases. Such observation is caused by inaccurate reward feedback, which indeed exists in real RL applications.\n\n2. This paper quantifies the magnitude of such phenomena and provides a clear geometric explanation.\n\n3. With these insights, this paper proposes an optimal early stopping method with theoretical regret bound analysis.\n\n4. The experimental results supported the authors' claim.\n\n5. This paper is well-written. Concepts are conveyed efficiently. The analysis is detailed while keeping a clear line of high-level logic."
                },
                "weaknesses": {
                    "value": "1. The optimal early stopping rule relies on the knowledge of the occupancy measure and the upper bound $\\theta$ of the angle between the true reward and the proxy reward. Methods to approximate the occupancy measure are well-researched. My concern is on the approximation of $\\theta$, which is a relatively new concept and requires some knowledge of the true reward feedback or true reward samples. When such estimation is not accurate, the stopping method could exhibit negative performance. It would be better if the author could show empirical results with approximated $\\theta$.\n\n2. This paper is preliminary because it only considers finite state and action space. The empirical results are also only on small grid world environments. It is not clear whether such a phenomenon exists in more broad continuous settings and what would be the practical way to solve it in these settings."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6832/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6832/Reviewer_eHjA",
                        "ICLR.cc/2024/Conference/Submission6832/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801360116,
            "cdate": 1698801360116,
            "tmdate": 1700953335529,
            "mdate": 1700953335529,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n5ys9QP1b8",
                "forum": "5o9G4XF1LI",
                "replyto": "CGXUwDNoh9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you very much for your review. Regarding the weaknesses you have pointed out:\n\n- *My concern is on the approximation of\u00a0\u03b8, which is a relatively new concept and requires some knowledge of the true reward feedback or true reward samples. When such estimation is not accurate, the stopping method could exhibit negative performance. It would be better if the author could show empirical results with approximated\u00a0\u03b8.* The approximation of the distance between the true and proxy reward functions is not necessarily unprecedented. For example, for a given reward learning algorithm, we might have certain bounds or rates of convergence to the true reward. Inverse reinforcement learning algorithms often come with proofs of their convergence rates - see e.g. \"Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees\u201d, Zeng et al., 2022. Using those convergence rates, we could get an approximation of the distance to the true reward.  \nWe think that in practice, the early stopping algorithm will not be used on its own, but rather as a component of the iterative improvement algorithm (discussed in Appendix F). That algorithm provably converges to the optimal policy only if we keep the invariant of never falling into Goodharting. Therefore, using a sufficiently large number of real reward evaluations, in the end we do not lose any reward. In this way, we see our work potentially leading to some sort of improved RLHF training regime. (We can then make calls to the true reward function through human feedback to estimate theta, and conservative stopping just translates to collecting human feedback throughout the training process, rather than all at once - which is also how RLHF training processes currently look).  \nWe note that our work is meant to be the first step in the investigation of Goodhart\u2019s law in RL: we feel that a proper empirical test of the effects of approximated \u03b8 on the performance of the algorithm would benefit from deferring to future work. It would require implementing and testing different reward learning algorithms, which unfortunately lies outside of our scope here.\n- *This paper is preliminary because it only considers finite state and action space.* Extending our theory to the countable or continuous state spaces is an interesting problem. It seems that Theorem 1 holds for countably-infinite state and action spaces, the only change is in the proof of Proposition 1 and Lemma 1. In a continuous setting, the theory becomes more involved, and might require more complex tools from functional analysis. (Note most of our results just rely on separating the policy from the reward, which the above shows we can still do in the general setting.) \n- *The empirical results are also only on small grid world environments. It is only clear whether such a phenomenon exists in more broad continuous settings and what would be the practical way to solve it in these settings.*  We note that we use three fundamentally different types of MDPs (densely-connected, tree-shaped, and some variations of grid worlds).  Our key results being similar across the classes gives us some evidence that they also generalise to more complex environments. Grid worlds constitute a standard evaluation tool for RL algorithms, densely connected MDPs are used in place of \u201cuninformatively-random\u201d environment, while tree-like environments are good models for many game or search problems. We also note that our Goodharting curves match the ones found in Gao et al. 2022, who use much larger state spaces (a LLM model) and much more complex reward models. However, working out how to apply the early stopping algorithm in those large or infinite spaces is indeed an open problem.\n\nWe hope that this helps to clarify our paper, and that the reviewer might consider increasing their score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482088971,
                "cdate": 1700482088971,
                "tmdate": 1700482088971,
                "mdate": 1700482088971,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I4MT8FLLfE",
            "forum": "5o9G4XF1LI",
            "replyto": "5o9G4XF1LI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6832/Reviewer_YkV9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6832/Reviewer_YkV9"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of reward misspecification. The authors point out that over-optimizing an incorrect reward function can lead to the wrong behaviour for the true (\"test\") reward function, and dub this phenomenon Goodharting. The authors propose a quantitative way to evaluate this phenomenon (cf. Definition 5), and perform an experimental case study on some simple MDPs to establish that Goodharting is a common phenomenon in RL. The authors then provide an intuitive geometric explanation for this phenomenon and propose an early stopping method to avoid overfitting. Further experimental evaluations are performed on the early stopping method to"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper investigates an interesting, albeit not entirely surprising phenomenon, and investigates it thoroughly and carefully. The problem of reward misspecification is quite relevant for practical considerings of RL, so gaining some understanding of this problem is appreciated. The paper is well-written and the messages are conveyed clearly. The theoretical contributions, while not exactly practical, are a nice step towards preventing this problem from affecting performance."
                },
                "weaknesses": {
                    "value": "While I am overall positive about the paper, I have a few comments and suggestions for possible improvement. \n\n- The definition of optimization pressure is a bit strange. Why should we not define it as simply the distance from the optimal policy? For instance, we can say that the optimization pressure is epsilon if we obtain a policy $\\hat{\\pi}$ such that $J_R(\\pi^\\star) - J_R(\\hat{\\pi}) \\leq \\varepsilon$. I feel that tying the optimization pressure to a certain regularization scheme detracts from the fundamental aspect of the problem, and furthermore that regularization is only used here as a proxy for \"how close to optimal are we\", which can be defined more directly as above.\n- The environments that have been used to establish that Goodharting is pervasive (Section 3) are somewhat simple. I understand that it is difficult to measure the NDH metric in environments where we cannot solve for the optimal policy, but it would have been nice to understand how pervasive this is in \"real\" problems, or at least in popular RL benchmark environments. As a side note, the fact that the NDH metric is inherently difficult to measure can be considered as a drawback of the proposed methodology -- can the authors comment?\n- It would also have been interesting to more systematically study which properties of environments imply that Goodharting is more likely to take place, do the dynamics of the MDP (e.g. a bottleneck structure) have any role?\n- The proposed optimal stopping algorithm is very pessimistic since it tries to avoid overfitting to any possible reward function in a certain set (is this pessimism unavoidable?), and as the authors point out it is computationally infeasible. In addition, if I understand correctly, it requires knowing the transition dynamics and knowing the distance between the proxy reward and the true reward function, which is fairly unpractical.\n\n- Incorrect/unclear sentences: \n1. \"We observe that NDH is non-zero if and only if, over increasing optimisation pressure, the proxy and true rewards are initially correlated, and then become anti-correlated\". I believe the authors meant the NDH is non-negative, not non-zero.\n2. \"Note that this scheme is valid because for any environment, reward sampling scheme and fixed parameters, the sample space of rewards is convex. In high dimensions, two random vectors are approximately orthogonal with high probability, so the sequence R_t spans a range of distances.\". It is not clear what point the first sentence is attempting to communicate (what does \"valid\" mean?), and the second sentence is incorrect as stated (what distribution is one sampling from? I can imagine many distributions where this is untrue, say a deterministic one.)"
                },
                "questions": {
                    "value": "See weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6832/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6832/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6832/Reviewer_YkV9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698982859681,
            "cdate": 1698982859681,
            "tmdate": 1699636790337,
            "mdate": 1699636790337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N082b8BPG8",
                "forum": "5o9G4XF1LI",
                "replyto": "I4MT8FLLfE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We are grateful for your detailed comments and questions. We give a detailed answer point-by-point below:\n\n- *Why should we not define it [the optimisation pressure] as simply the distance from the optimal policy?* The optimisation pressure is intended to be an independent variable that we can vary freely, such that increasing the pressure leads to policies progressively closer to optimal (i.e. a parametrisation of some curve between the initial policy and the optimal policy). It is unclear how directly varying the distance $\\epsilon$ (such that  $J(\\pi) - J(\\pi') \\leq \\epsilon$) could work in practice. The contour lines (inverse images of $\\epsilon$) in this case would result in a sequence of n-dimensional spheres in the space of policies, not a linearly ordered sequence of policies.\n- *The environments that have been used to establish that Goodharting is pervasive (Section 3) are somewhat simple.* It is true that we mostly used small environments with at most a couple hundred states, which are far from real-life conditions. However, we do use three fundamentally different types of MDPs (densely-connected, tree-shaped, and some variations of grid worlds). Our key results being similar across the classes gives us some evidence that they also generalise to more complex environments. Grid worlds constitute a standard evaluation tool for RL algorithms, densely connected MDPs are used in place of \u201cuninformatively-random\u201d environment, while tree-like environments are good models for many game or search problems. We also note that our Goodharting curves match the ones found in Gao et al. 2022, who use much larger state spaces (a LLM model) and much more complex reward models.\n- *I understand that it is difficult to measure the NDH metric in environments where we cannot solve for the optimal policy, but it would have been nice to understand how pervasive this is in \"real\" problems, or at least in popular RL benchmark environments. As a side note, the fact that the NDH metric is inherently difficult to measure can be considered as a drawback of the proposed methodology -- can the authors comment?* We agree that the fact that the NDH metric requires knowledge of the true reward is a drawback. At the same time, we feel that every \u201ccomplete\u201d measure of Goodhart\u2019s law would require it (\u201dcomplete\u201d meaning a measure that is non-zero for any non-optimal policy, w.r.t. the true reward). Therefore, we still think it useful to have it defined - and supported by experiments, which we discuss in Appendix C. We expect that in cases where computing true NDH is computationally prohibitive, some approximate methods can be used. In our setup, we did not need to develop such approximations, so we leave this topic for future work. Another point of view might be that even though we used NDH, our reason for choosing it was simplicity (as we note in the paper), since all the measures we investigated were significantly correlated. Therefore, you could read the contribution here as the fact about the correlation of all \u201csensible\u201d measures, rather than about the primacy of NDH. That gives us a reason to expect that approximating NDH is achievable in practice.  \nAs a side note, we want to point out that even though the measure is predicated on knowing the true reward, the point of our work is developing a better understanding of how to optimise misspecified reward *without* knowing the true reward. Therefore, the NDH metric is primarily useful for analysis, and its weak applicability in the deployment setting would not be an issue.\n- *It would also have been interesting to more systematically study which properties of environments imply that Goodharting is more likely to take place, do the dynamics of the MDP (e.g. a bottleneck structure) have any role?* We agree that this is an interesting question! We have conducted the preliminary investigation, but due to a lack of space, we have delegated it to Appendix G. Specifically, we look at how different hyper-parameters of the experiment (the kind of the environment, the number of states |S|, number of actions |A|, temporal discount factor $\\gamma$, sparsity of the environment, determinism of the environment, and more) correlate with the severity of Goodharting. We agree that investigating more systematically the key properties of the environment that would potentially give a list of \u201csufficient and necessary\u201d conditions for significant Goodharting to occur (such as the *bottleneck structure on an MDP* you mentioned) would be very interesting, but we feel is outside of the scope of this work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481716403,
                "cdate": 1700481716403,
                "tmdate": 1700481716403,
                "mdate": 1700481716403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iwI3XrWEnS",
            "forum": "5o9G4XF1LI",
            "replyto": "5o9G4XF1LI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6832/Reviewer_Y2A1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6832/Reviewer_Y2A1"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an analysis of Goodhart\u2019s law in reinforcement learning. The paper starts with a formalization of the Goodhart problem in terms of misalignment of reward functions on finite MDPs: one proxy reward that the policy optimizes when we really wish to optimize the other. The paper justifies that the problem occurs in small scale experiments demonstrating that increasing the optimization pressure on the proxy eventually leads to a decrease in the true reward. A theoretical analysis is given with some examples about why this occurs in finite MDPs. Finally an early-stopping algorithm is proposed to mitigate this issue along with some preliminary experiments on the algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem is clearly very important and a better understanding of proxy rewards, overoptimization, and Goodhart\u2019s law are definitely needed in the community.\n\nThe paper is presented fairly clearly, except in some areas which I point out later.\n\nThe paper provides insights from multiple frontiers to help shape this understanding (empirical, theoretical, and conceptual).\n\nThe theoretical findings are useful, but not entirely surprising given what is known already in the literature (see below). However, I do believe it\u2019s useful to have this formalized and characterized when specifically talking about Goodhart\u2019s law."
                },
                "weaknesses": {
                    "value": "My primary complaint is that, although this is a solid analysis, I do not believe it strikes the heart of the Goodhart problem. The position of the paper is that misalignment can be characterized by the worst-case angle between reward functions. This is a fairly well-understood setting (e.g. see \u2018simulation lemma\u2019 by Kearns & Singh or any number of classical RL papers). However, it\u2019s unclear how this maps into problems that (1) are beyond the finite case, or (2) are classical examples of Goodhart\u2019s law like the snake bounty. While one could model (2) in the framework studied here, I am not sure this would be an informative model in those settings as the \u2018theta\u2019 is just so large.\n\nThe above is more of a conceptual disagreement about the premise. For the rest of the review, I give the benefit of the doubt and simply accept the premise is true.\n\nUnfortunately most of the important empirical results have been relegated to the appendix, leaving the main paper with vague / difficult-to-verify statement such as \u2018a Goodhart drop occurs for x% of all experiments. Without figures or tables, it\u2019s difficult to understand what this means, such as what the criteria of a \u2018Goodhart drop\u2019 is (any non-zero drop, some negligible drop, etc). It would be helpful to make room in the main paper for results that present a more comprehensive picture of the findings.\n\nThe early stopping proposal is natural, but also seems very conservative. This appears to be consistent with the empirical findings. Furthermore it requires knowledge of $\\theta$, which is just assumed to be known. While it\u2019s hard to imagine anything can be down without some knowledge of the true reward or structure, this seems quite coarse.\n\nFigure 5 is difficult to appreciate in absolute terms as one cannot tell if, for example, 0.4 is a large value relative to the reward achievable. I think this plot would be better replaced with a typical plot showing how the true and proxy rewards change as the policy is optimized and when the algorithm decides to stop, as well as the counterfactual of what would happen if it does not stop."
                },
                "questions": {
                    "value": "How do you think the theoretical results generalize to the setting where the reward function is considerably more sophisticated than simple finite MDPs? For example, high dimensional, continuous state-action, long-horizon problems?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699159377409,
            "cdate": 1699159377409,
            "tmdate": 1699636790214,
            "mdate": 1699636790214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o3XgUhF7fS",
                "forum": "5o9G4XF1LI",
                "replyto": "iwI3XrWEnS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your comments!\n\n- The simulation lemma is indeed somewhat related to our work - specifically regarding the regret bound for our early stopping algorithm. However, we disagree that it already captures our theoretical contribution. The lemma provides the bound on the maximal reward obtained, while we are more concerned about the dynamics of the training, which the lemma does not take into account.\n- It is a fair point that our work does not exhaust all relevant facets of Goodhart's law. Still, we note that the point of view we take already has precedence in the literature, and, for example, informs and explains some of the prior results on optimising LLMs with RLHF (e.g. Gao et al, 2022). Manheim and Gabarrant, 2019 provide a classification of different aspects of Goodhart\u2019s law in ML: our work is best understood as (mostly) addressing the *Regressional Goodharting,* and, in principle, potentially extending to *Extremal Goodharting*.\n- Regarding your specific point about the vagueness of the statement *\u2018a Goodhart drop occurs for x% of all experiments'. Without figures or tables, it\u2019s difficult to understand what this means, such as what the criteria of a \u2018Goodhart drop\u2019 is (any non-zero drop, some negligible drop, etc)\"*. We would like to point out that immediately after writing this we clarify: \"*meaning that the NDH > 0*\".  \nAs you noted, we are studying the problem from a conceptual, formal, and empirical point of view, which creates tradeoffs in what to include in the main text. We chose the high-level logic of the paper to be the four-step reasoning \u201cWhat is Goodharting? \u2192 Does it occur in practice? \u2192 Why does it occur? \u2192 How can we prevent it?\u201d.   \nSeparately, we did investigate many other questions - in particular, the relationship between kinds of environments/ hyperparameters and the severity of Goodharting, and provided the conclusions and all relevant tables and figures Appendix G. It was difficult to fit all this in the main text without sacrificing other parts, which we felt were more important to understand the \u201ccritical path\u201d of the argument outlined above.\n- *The early stopping proposal is natural, but also seems very conservative.* It is true that the conservative nature of the early algorithm might lead to a significant loss of reward, when used naively. But even then, as we point out in Section 5 (Figure 5b), if the proxy is not too far from the true reward, then one can probably expect the loss of reward to be fairly small. We have reasons to suspect this will be true in practice: we note that Skalse et al. 2023, \u201cInvariance in Policy Optimisation and Partial Identifiability in Reward Learning\u201d (indirectly) show that many reward learning algorithms will converge to a reward with STARC-distance 0 to the true reward.   \nHowever, potentially an even more important point is that we think in practice early stopping algorithm will not be used on its own, but rather as part of the iterative improvement algorithm (described in Appendix F). This is how our work might lead to some sort of improved RLHF training regime. We can then make calls to the true reward function through human feedback to estimate theta, and conservative stopping just translates to collecting human feedback throughout the training process, rather than all at once - which is also how RLHF training processes currently look.\n- *Furthermore it requires knowledge of \u03b8, which is just assumed to be known.* We argue that knowing the upper bound of the distance between the true and proxy reward functions is not an unrealistic assumption. For example, for a given reward learning algorithm, we might have certain bounds or rates of convergence to the true reward (e.g. \"Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees\u201d, Zeng et al., 2022). We also note that the angle distance is a STARC metric, and it is similar to the EPIC metric. There has been a considerable amount of literature on using EPIC in practice, demonstrating how to compute the angle alignment of true and proxy reward in concrete settings (e.g. \"Skill-Based Reinforcement Learning with Intrinsic Reward Matching\u201d, Adeniji et al. 2023, where they apply EPIC-based learning for robotic control, or \"Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning\u201d Rocamonde et al., 2023, who describe computing EPIC for CLIP-based rewards). The last thing to note is that we can always make the bound less tight, and therefore trade off the accuracy of the input to the early-stopping and the optimality of the stopping point, which means that even in the presence of a weak signal our approach might still be useful."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481421054,
                "cdate": 1700481421054,
                "tmdate": 1700481544611,
                "mdate": 1700481544611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]