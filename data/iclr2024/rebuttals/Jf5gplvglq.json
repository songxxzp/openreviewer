[
    {
        "title": "Skill-Mix: a Flexible and Expandable Family of Evaluations for AI Models"
    },
    {
        "review": {
            "id": "5MBDIMQC2q",
            "forum": "Jf5gplvglq",
            "replyto": "Jf5gplvglq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8809/Reviewer_VgUg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8809/Reviewer_VgUg"
            ],
            "content": {
                "summary": {
                    "value": "The authors present Skill-Mix, an evaluation for LLMs which utilizes skill combinations to evaluate LLM capability on likely unseen tasks. They demonstrate that some of their results contradict existing results on popular LLM evaluations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**********************Motivation:********************** One of the main motivations of the paper is good: LLM evaluations can be trained for so it makes sense to do some combinatorial benchmark which requires combining skills in a way that\u2019s likely unseen in any training corpus.\n\n**************Method:************** The idea of skill combination as an LLM evaluation is good, and to the best of my knowledge (i\u2019m not very familiar with LLM benchmarks), novel. It is definitely harder to expect knowledge of the benchmark to be contained in the training corpora of LLMs a priori.\n\n******************Dataset:****************** It\u2019s good idea to release only 10% of the skills and topics for evaluation to prevent overfitting/training on the testing criteria.\n\n****************Results:**************** I\u2019m glad the paper demonstrates that this evaluation is not so correlated with other existing benchmarks, as to prove that it is testing something different from the others with respect to LLM capability."
                },
                "weaknesses": {
                    "value": "****************************Skill picking:**************************** Section 3.1 (and the appendix) describe how the authors pick the skills by hand based on textbooks on logical reasoning, rhetoric, and theory of mind. But what is the justification for using these types of skills? For example, i\u2019m sure there are many skills not related to any of those topics which still require some sort of intelligent capability to combine meaningfully. The authors should explain this better.\n\n**********************Evaluation:**********************\n\n- One minor issue is that I don\u2019t think the models picked necessarily need to be instruction-finetuned as regular, non-instruction finetuned language models are still able to follow instructions in the prompt. In fact, given that some recent work has demonstrated that instruction-finetuning on smaller models reduces their ability to produce as diverse of a response distribution, it could artificially hurt the performance of the selected open-source models in evaluation due to the selected topics occurring with a Google n-gram probability of ~1e-6 and them likely being fine-tuned on a much smaller dataset that may not include the selected topics/skills.\n- What is the variance among the automatically graded scores? It is stated that human variance is higher than the automatic ones, but neither seem to be presented anywhere.\n- The analysis of Skill-Mix vs other benchmarks could be more comprehensive \u2014 it would be interesting to see ******************correlation scores****************** against other benchmarks to see how skill-mix differs overall wrt individual LLM rankings.\n\n********Writing + Clarity + Motivation:********\n\n- Intro first paragraph: A missing \u201cyet\u201d between the 2nd and third sentence, otherwise these two sentences next to each other are a bit confusing.\n- Intro: \u201c(including for some public models such as LLaMA-2)\u201d this could be put next to \u201csize of training corpora\u201d as right now it\u2019s directly after mentioning closed-source LLMs which makes the parenthesis seem confusing.\n- Intro: Why is a specific opinion from Hinton used as motivation for desiderata for the authors\u2019 benchmark? While Hinton is an important figure in the field, a good justification should come from (1) an intuitive explanation of the problem, (2) consensus in the field for the problem, or (3) direct citation of evidence of the problem (or some combination of all 3). I don\u2019t think citing a single person\u2019s opinion is good justification in general or intuitive to a reader presented with a new problem.\n- Section 1.1: How does SKILL-MIX directly address the desiderata presented in the prior paragraph? For reader clarity, this question should be answered immediately after presenting the desiderata or else we will forget.\n- Section 1.1: Arora & Goyal is cited as demonstrating that scaling up language models \u2192 improvements at applying k\u2019-tuples of basic skills, therefore motivating the authors\u2019 Skill-Mix benchmark. But this doesn\u2019t mean that applying k\u2019-tuples of basic skills \u2192 better general purpose understanding. This is a problem with many LLM benchmarks, but still presents an issue that the authors should discuss in this paragraph.\n- Prior work: This section isn\u2019t that well written. The authors should relate Skill-Mix to each of the types of prior work rather than describe them in isolation.\n- Section 4: This is a minor issue, but Section 4 can probably be made a subsection of Section 3 as it is part of the introduced SkillMix evaluation.\n- Table 1/2: Some sort of bolding on the best performing across each category for each k would be really helpful to parse all of these numbers\n- Section 6: Can the authors explain the derivation of the high probability statement for $O(T \\log T + N \\log N)$? Intuitively, it\u2019s quite surprising given \u201crandom prompts\u201d to expose these low-probability skills and their topics."
                },
                "questions": {
                    "value": "See weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8809/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698191345390,
            "cdate": 1698191345390,
            "tmdate": 1699637108036,
            "mdate": 1699637108036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rujLZ7AqDV",
                "forum": "Jf5gplvglq",
                "replyto": "5MBDIMQC2q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8809/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**We thank the reviewer for the helpful and insightful comments! Please see our general response above and our responses to the reviewer\u2019s main concern below:**\n\n\n> Skill picking: Section 3.1 (and the appendix) describe how the authors pick the skills by hand based on textbooks on logical reasoning, rhetoric, and theory of mind. But what is the justification for using these types of skills? For example, i\u2019m sure there are many skills not related to any of those topics which still require some sort of intelligent capability to combine meaningfully. The authors should explain this better.\n\nWe apologize for any confusion. The evaluation tests skills that are important in intelligence but is not meant to be a complete test of intelligence. Please see the response to all reviewers Q3.\n\n\nIn Appendix C.2, we discuss details of our skill selection process and how we remove skills from the original large set due to various reasons. Conceptually, the choice of topic should not affect the use of language skills. The capability to combine seemingly unrelated skills and topics is indeed what Skill-Mix aims to test."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506107543,
                "cdate": 1700506107543,
                "tmdate": 1700506107543,
                "mdate": 1700506107543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5wo8VwaXg4",
            "forum": "Jf5gplvglq",
            "replyto": "Jf5gplvglq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8809/Reviewer_NFpe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8809/Reviewer_NFpe"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new evaluation benchmark SKILL-MIX for general-purpose language capabilities, specifically, a particular sort of compositional generalization. It tests the model\u2019s ability to create text on-demand on a given topic and with a given combination of well-known skills. The paper provides an exemplary introduction and motivation for the task of appropriate evaluation of the newly emerging topics.  A key idea is that the skills and the topic are chosen randomly from a big list, which itself can be expanded in many ways to give new evaluations. Such evaluations may end up requiring the (Student) model to imagine and describe situations that does not exactly match anything seen during training. Grading this evaluation is shown to be possible with GPT-4 (and the authors spot-checked the grading). The authors also emphasized the important of human spot-checking, but since human evaluations were  found to have significant variance, they are not reported."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I found this paper one of the best papers I reviewed recently (including the Neurips reviewing).\nI found the Introduction (Section 1) and especially \"Desiderata for next-generation evaluations\" extremely valuable for any LLM development. I am currently working on a task that requires complex evaluation of LLMs and the guidance and observations summarized in the introduction are right to the point of what we are looking for. \n\nI also find the idea of mixing random skills with evolving N as a way to mitigate some of the issues in current evaluations simply, yet elegant.  The authors also validate its effectiveness with numerous experiments."
                },
                "weaknesses": {
                    "value": "1) I'd love to see some correlation of the presented results with human spot-checking and evaluation and if such correlation does not exist, discussion on what might be the reason\n\n2) I'd love to get some suggestion on how this framework can be extended beyond language to Vision - Language Models.\n\n3) I'm curious to see how the experiments with fine-tuning while releasing 10% of skills pan out."
                },
                "questions": {
                    "value": "1)Do you think that instead of have one strong evaluator, you can have a mixture of experts each capable to evaluate a single task exceptionally well?\n2) Can you please provide reference for \"While both models are creditable graders, they, like all current LLMs, were unreliable at simple arithmetic, which is important for calculating a total score.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8809/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8809/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8809/Reviewer_NFpe"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8809/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731893290,
            "cdate": 1698731893290,
            "tmdate": 1699637107903,
            "mdate": 1699637107903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uRN4jXQoyL",
                "forum": "Jf5gplvglq",
                "replyto": "5wo8VwaXg4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8809/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**We thank the reviewer for the helpful and insightful comments! Please see our general response above and our responses to the reviewer\u2019s main concerns below:**\n\n> I'd love to get some suggestion on how this framework can be extended beyond language to Vision - Language Models.\n\nWe sketch an example of how to extend Skill-Mix to evaluate vision-language models below:\n\n- The set of skills can include both vision skills (e.g., object recognition, object generation, 3D reconstruction, spatial relation between objects) and language/reasoning skills (e.g., description of the scene, object interaction, reasoning of past and future events). \n- The set of topics can be the environment or background of the image.\n- Task can either be answering questions based on given images, or generating images based on instructions (the instruction may include images as well). \n- The images fed to models (as part of the prompt) can be generated by graphics programs.\n\n> I'm curious to see how the experiments with fine-tuning while releasing 10% of skills pan out.\n\nWe include some preliminary results on finetuning LLaMA2-7B on 50% skills and topics, and observe improvements across the various Skill-Mix metrics. We are excited to release a more detailed exploration of this in future work! Please also see our response to all reviewers, Q2.\n\n> Can you please provide reference for \"While both models are creditable graders, they, like all current LLMs, were unreliable at simple arithmetic, which is important for calculating a total score\u201d\n\nWe provide some references below. We also include our experience in Appendix C.3.3.\n\n[1] Exposing Attention Glitches with Flip-Flop Language Modeling\nBingbin Li, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang\n\n[2] Table 10, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, Denny Zhou\n\n\n---\n\n**Below, we address the other concerns brought up.**\n\n\n> I'd love to see some correlation of the presented results with human spot-checking and evaluation and if such correlation does not exist, discussion on what might be the reason.\n\nWe thank the reviewer for bringing up this point. In Appendix B, we mentioned that the difference between GPT-4 grading and the average human grading is ~0.25, meaning that the human and GPT-4 agree on most of the points. We will include details in the final version. \n\n> Do you think that instead of having one strong evaluator, you can have a mixture of experts each capable to evaluate a single task exceptionally well? \n\n\nThis is a good point! If one decides to fine-tune the grader, it is unclear to us whether it is better to fine-tune one model or use a mixture of fine-tuned models. One concern of using a mixture of experts is that some experts may be harsher than others. However, in terms of human grading, we believe it would be more efficient to split skills and assign them to specific graders. We leave this a future work to explore."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505763775,
                "cdate": 1700505763775,
                "tmdate": 1700592397973,
                "mdate": 1700592397973,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n3zYtCsa5g",
            "forum": "Jf5gplvglq",
            "replyto": "Jf5gplvglq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8809/Reviewer_YRGK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8809/Reviewer_YRGK"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new general-purpose evaluation framework for LLMs called 'skill mix'. The approach is based on an N-choose-k sampling of a set of pre-defined 'skills', combined with a set of pre-defined topics. The authors present the prompting and evaluation configuration, an LLM-based automated evaluation methodology, empirical results of running this new benchmark against a collection of contemporary LLMs that score well on current leaderboards, and a limited set of human evaluations.\n\nThe language of the paper is reasonably clear, and lots of empirical evaluations have been performed, but I find the motivation and justification for the proposed framework lacking. The paper is a little under-prepared and would benefit from further work before publication."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* LLM evaluation (beyond statistical language modelling to general purpose planning systems) is an important and timely topic\n * In the empirical evaluation the authors have clearly made an effort to compare a wide range of contemporary LLMs that currently score well on leaderboards\n * The key concept behind the skill-mix framework is easy to understand"
                },
                "weaknesses": {
                    "value": "* The motivation for this evaluation framework needs further development. Sec 1. highlights accelerating rates of leaderboard saturation, training set contamination, and training corpora secrecy as pressing issues, and suggests 7 desiderata for new LLM evaluation frameworks; (a) relevant to general-purpose intelligence, (b) easy for humans to design and administer, (c) resistant to training-set contamination, (d) capable of revealing novelty in some sense, (e) easy to grade at scale, (f) easy to scale difficulty over time, and (g) comprehensible by the general public. This list seems promising, but the paper doesn't make it clear how skill-mix addresses each of these needs, or how these 7 desiderata are related to the 3 pressing issues with current evaluation methods. In particular, I see no reason to believe that, once released, skill-mix won't also suffer the problem of 'cramming for the leaderboard' (also see next point)\n * In the current form, skill-mix defends against 'cramming' by proposing a held-out private test set of skills and topics. However, any evaluation design that relies on private evaluation data is fundamentally flawed - the field of LLM evaluation design needs to learn from cryptography algorithm design on this count. This is also problematic from an open science perspective - making it harder for peer review to evaluate the efficacy of the evaluation framework for instance.\n * The theoretical justification for skill-mix is also unclear to me.\n    * For instance - does it make sense to combine /any/ $k$ of the N skills together, or are there some incompatible skill-tuples?\n    * The connections with pedagogy theory and the empirical observations from the Arora & Goyal 2023 paper are a good starting point, but it's not clear in what way combining skills relates to generalized intelligence. A clearer definition of 'skill' may help here.\n * Some 'late breaking' paper results are missing from the submitted version (e.g. footnote 3, Sec. 6) - the paper will be strengthened with the inclusion of these results in a future version.\n * (End of Sec. 5) \"Difference between model ranking on popular LLM leaderboards vs. SKILL-MIX provide evidence of \"cramming for the leaderboard\", further validating that SKILL-MIX is a good evaluation benchmark\" - the conclusion here doesn't follow from the premise."
                },
                "questions": {
                    "value": "# Questions and comments\n\n * It is great to see some human experiments done to compare against the automated grading scheme (however I note the lack of IRB or ethics approval in the paper related to human experiments). However, I note that variance in human grading responses (e.g. see end of Sec. 4) shouldn't automatically be interpreted as lower quality of responses - the variance can often be a signal, not noise. For instance, this could indicate combinations of skills and/or topics which are perhaps less compatible or sensible to query LLMs with.\n * I like the core idea of N-choose-k as an evaluation approach that scales quickly with k. The paper and results gave me the impression that you only consider linguistic skills, which seems like a big limitation. I note though that in the appendix the list of 10 published skills includes several non-linguistic skills (e.g. folks physics etc.). It would make review and evaluation of skill-mix much clearer if the full list of skills were published, along with (more) examples of select skill-topic-tuples. In the current form, the limited (linguistic) skill-tuple examples make it hard to imagine what the range and scope of questions looks like.\n * A key part of the skill-mix framework needs to be how to do evaluation at scale, including human oversight. To this end, the paper should include a proposed strategy or curriculum for how the human 'spot checking' can be performed. Currently there is not detail about this (very important) aspect of the evaluation procedure.\n * Lots of interesting and important open questions are raised in the paragraph 'difficulty of grading' at the end of Sec. 6. In particular, it seems worth investigating the duality between grading vs. answering questions more generally as part of a theory of LLM pedagogy. \n * Additional experiments exploring what skill-mix would look like in a multi-modal setting are welcome in future work - this seems like a great area to explore next, as the authors note (end of Sec 7).\n\n# Minor comments and grammatical points\n\n * Figure 1 caption - typo 'red hearing'."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This research included human subjects (PhD students and post-docs, see Sec. 4 and App. B), however is lacking any IRB or ethics approval details in the paper for this aspect of the experimental procedures. I am aware that research norms vary by geography, but from my perspective and in my country, this would count as a clear case of academic research misconduct requiring institutional review and/or disciplinary action from the host university. I defer to the ICLR ACs to determine if this constitutes a relevant breach of the conference code or not."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8809/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8809/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8809/Reviewer_YRGK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8809/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699265303440,
            "cdate": 1699265303440,
            "tmdate": 1700793172442,
            "mdate": 1700793172442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PYwz71ELgK",
                "forum": "Jf5gplvglq",
                "replyto": "n3zYtCsa5g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8809/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the helpful and insightful comments! Please see our general response above and our responses to the reviewer\u2019s main concerns below:\n\n> lack of IRB or ethics approval\n\nWe thank the reviewer for bringing up this point. The human graders were carrying out research too and we understand IRB is not required in such cases. The final version will fully explain this status. \n\n> the paper doesn't make it clear  \u2026 how these 7 desiderata are related to the 3 pressing issues with current evaluation methods.\n\nPlease see our response to all reviews, **Q1**. \n\n> no reason to believe that, once released, skill-mix won't also suffer the problem of 'cramming for the leaderboard' \n\nWe release 10 skills and 10 topics (randomly sampled)  to avoid \u201ccramming\u201d for Skill-Mix. For more detailed explanations, please see our response to all reviewers, **Q2**.\n\n> The theoretical justification for skill-mix is also unclear to me. For instance - does it make sense to combine any k of the N skills together, or are there some incompatible skill-tuples? The connections with pedagogy theory and the empirical observations from the Arora & Goyal 2023 paper are a good starting point, but it's not clear in what way combining skills relates to generalized intelligence. \n\nWe apologize for any confusion and would like to emphasize that as of now, the goal of our evaluation is to test general-purpose text generation capability, rather than generalized intelligence. Please also see our response to all reviewers, **Q3**.\n\nWith respect to incompatible skill tuples: We  eliminated  skills that in our judgement inherently compose poorly with other skills (see Appendix C.2). It is possible that this intuitive weeding still left some pair of skills that don\u2019t compose well. But this would equally affect the final performance number of all models, and not be expected to affect the ranking.\n\n> I like the core idea of N-choose-k as an evaluation approach that scales quickly with k. The paper and results gave me the impression that you only consider linguistic skills, which seems like a big limitation.\n\nFor our first version of Skill-Mix, we acknowledge that we only use language-related skills. However, we note that Skill-Mix can be expanded to other types of skills (e.g., domain-specific skills related to coding, science, economics, math, law, etc). \n\n--- \nBelow, we address the other concerns brought up.\n\n> Some 'late breaking' paper results are missing from the submitted version (e.g. footnote 3, Sec. 6) - the paper will be strengthened with the inclusion of these results in a future version.\n\nWe thank the reviewers for pointing this out, and have updated these results in our revised version of the paper. In particular,\n- For k = 5 and k = 6, we show that GPT-4 is generating correct answers a good fraction of time, and a reasonable portion (say more than 1/3) of these correct answers contain combinations of skills and topics that have never jointly appeared in the training. \n    - We verify this claim by providing upperbounds on the average frequency of skills, the average frequency of topics, and the number of sentences in the training corpus (see Sec. 6 and Appendix D). \n- We include preliminary finetuning results for LLaMA2 7b in Appendix E.\n\n>  (End of Sec. 5) \"Difference between model ranking on popular LLM leaderboards vs. SKILL-MIX provide evidence of \"cramming for the leaderboard\", further validating that SKILL-MIX is a good evaluation benchmark\" - the conclusion here doesn't follow from the premise.\n\n\nWe apologize for any confusion. We meant to convey that we believe Skill-Mix is a good evaluation benchmark because model rankings on Skill-Mix coincide with anecdotal human experience with these models (and that this is not the case with model rankings on e.g., Open LLM). Many derivatives of Llama2 models scored higher  than the Llama2 model on the Open LLM leaderboard but performed much worse on Skill-Mix, suggesting their general-purpose language skills were harmed along the way."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505514444,
                "cdate": 1700505514444,
                "tmdate": 1700505514444,
                "mdate": 1700505514444,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]