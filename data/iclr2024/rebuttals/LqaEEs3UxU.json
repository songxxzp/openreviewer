[
    {
        "title": "Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation"
    },
    {
        "review": {
            "id": "HAS3e11BNZ",
            "forum": "LqaEEs3UxU",
            "replyto": "LqaEEs3UxU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5333/Reviewer_hMSD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5333/Reviewer_hMSD"
            ],
            "content": {
                "summary": {
                    "value": "The paper propose Sign2GPT, which enjoys the benefit of large pretrained language model to promote gloss-free sign language translation. The authors also propose a CLIP-styple pseudo-gloss pretraining technique to better learn visual-lingusitic representations. The overall method achieves SOTA performance on two widely adopted benchmarks, Phoenix-2014T and CSL-Daily."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea is sound. It is good to see that current large language model can be helpful for sign language understanding.\n2. The proposed pseudo-gloss pretraining is novel, which can inspire future works.\n3. SOTA gloss-free sign language translation performance on two benchmarks."
                },
                "weaknesses": {
                    "value": "1. Using pretrained language model to boost sign language translation (SLT) is not surprisingly novel. Several works [1,2] have already verified that pretrained language model can boost (gloss-free) SLT.\n\n2. The name of pseudo-gloss pretraining is a bit confusing, although the method itself is sound. Because the pseudo-glosses are in spoken language order, it is not quite appropriate to call them \"glosses\".\n\n3. The notations of \"P\", \"F\", \"D\" in Table 2 and 3 are also confusing. For example, D denotes without pretraining and P denotes pretraining, then what does \"P+D\" mean? I suggest authors adding a \"check mark\" column to show which parts are pretrained.\n\n4. I cannot find an ablation study on the pseudo-gloss pretraining. What is the performance if removing it?\n\n5. In Table 4, removing sinusoidal positional encoding leads to the best performance. Then what poistional encoding is used by default?\n\n6. The paper uses a new spatial backbone which is under-explored in previous sign language papers. The authors need to better motivate it. For example, is it better than other vision transformers, or is it better than widely adopted 2D/3D CNNs?\n\n7. In figure 2, how to fuse the outputs of adapted masked attention (solid lines) and zero-gated cross attention (dashed lines)? Besides, what is adapted masked attention? I didn't see a clear definition of it. \n\n[1] A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation, CVPR 2022\n\n[2] Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining, ICCV 2023"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698509305410,
            "cdate": 1698509305410,
            "tmdate": 1699636536035,
            "mdate": 1699636536035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N1s4J8Wjxy",
                "forum": "LqaEEs3UxU",
                "replyto": "HAS3e11BNZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hMSD (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the review, and your insightful comments.\n\n**W1: Using pretrained language model to boost sign language translation (SLT) is not surprisingly novel. Several works [1,2] have already verified that pretrained language model can boost (gloss-free) SLT.**\n\nWhile we acknowledge previous research on leveraging pretrained language models for sign language translation, our approach offers several advantages over existing methods. \n\nChen et al. [1] primarily focus on utilizing manually annotated sign-ordered gloss supervision, relying on Gloss2Text for progressive pretraining on the encoder-decoder language model. In contrast, our approach does not rely on manually annotated glosses. Instead, we concentrate on developing a method for gloss-free sign representation, utilizing a decoder-only language model without the need for progressive pretraining.\n\nSimilarly, Zhou et al. [2] have explored gloss-free pretraining, but their approach involves updating the visual encoder and text decoder simultaneously. In contrast, our pseudo-gloss pretraining is independent of the language model. We keep the pretrained language model frozen by leveraging adapters, allowing the use of large-scale pretrained language models. Our architectural design outperforms their approach, as demonstrated in our results.\n\n**W2: The name of pseudo-gloss pretraining is a bit confusing, although the method itself is sound. Because the pseudo-glosses are in spoken language order, it is not quite appropriate to call them \"glosses\".**\n\nWe termed them pseudo-glosses as they are potential glosses that are signed. However, while we extract them from the spoken language sentence our proposed pretraining is invariant to the order of the pseudo-glosses. As demonstrated in Figures 4, 5, and 6 the pretraining is also able to localize the pseudo-glosses and thus provide them in sign order.\nWe are happy to change them to something more appropriate and are open to suggestions.\n\n**W3: The notations of \"P\", \"F\", \"D\" in Table 2 and 3 are also confusing. For example, D denotes without pretraining and P denotes pretraining, then what does \"P+D\" mean? I suggest authors adding a \"check mark\" column to show which parts are pretrained.**\n\nWe have updated our tables shown below. In the table Sign2GPT without Pseudo-Gloss Pretraining is denoted as Sign2GPT and Sign2GPT(w/ PGP) indicates with Pseudo-Gloss Pretraining (PGP). This updated table clearly shows that our approach outperforms the previous state-of-the-art results. We also included Sign($Z$)2GPT(w/PGP) indicating we are giving the extracted features $Z$ to GPT, which consists of a frozen spatial and sign encoder model that has been trained with PGP, demonstrating the strength of our PGP as feature extractors for sign translation.\n\n*Translation Results on Phoenix14T:*\nApproach |BLEU1|BLEU2|BLEU3|BLEU4|ROUGE|\n|-|-|-|-|-|-|\nNSLT|29.86|17.52|11.96|9.00|30.70|\nTSPNet|36.10|23.12|16.88|13.41|34.96|\nCSGCR|36.71|25.40|18.86|15.18|38.85|\nGASLT|39.07|26.74|21.86|15.74|39.86|\nGFSLT|41.39|31.00|24.20|19.66|40.93|\nGFSLT-VLP|43.71|33.18|26.11|21.44|42.49|\n|||||||\n**Sign2GPT**|45.43|32.03|24.23|19.42|45.23|\n**Sign2GPT(w/ PGP)**|**49.54**|**35.96**|**28.83**|**22.52**|**48.90**|\n|||||||\n**Sign($Z$)2GPT(w/PGP)**|47.06|33.61|25.85|20.93|47.11|\n\n*Translation Results on CSL-Daily:*\nApproach |BLEU1|BLEU2|BLEU3|BLEU4|ROUGE|\n|-|-|-|-|-|-|\nGASLT|19.90|9.94|5.98|4.07|20.35|\nNSLT|34.16|19.57|11.84|7.56|34.54|\nGFSLT|37.69|23.28|14.93|9.88|35.16|\nGFSLT-VLP|39.37|24.93|16.26|11.00|36.44|\n|||||||\n**Sign2GPT**|34.80|24.00|17.27|12.96|41.12|\n**Sign2GPT(w/ PGP)**|**41.75**|**28.73**|**20.60**|**15.40**|**42.36**|\n|||||||\n**Sign($Z$)2GPT(w/PGP)**|32.73|20.52|13.75|9.73|33.39|"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139881642,
                "cdate": 1700139881642,
                "tmdate": 1700139881642,
                "mdate": 1700139881642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fJrIkyfv2O",
                "forum": "LqaEEs3UxU",
                "replyto": "HX1Ixxv75J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5333/Reviewer_hMSD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5333/Reviewer_hMSD"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' rebuttal. My concerns are well addressed. The major issue is that the core idea of using pretrained model for SLT has already been studied, but the proposed new techniques, e.g., pseudo-gloss pretraining and using of LLM, can compensate the weakness. Thus, I tend to accept the paper but I am also open to discuss with other reviewers."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719743724,
                "cdate": 1700719743724,
                "tmdate": 1700719743724,
                "mdate": 1700719743724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Pr5Bz9FfIG",
            "forum": "LqaEEs3UxU",
            "replyto": "LqaEEs3UxU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5333/Reviewer_aj4n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5333/Reviewer_aj4n"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to improve gloss-free sign language translation by exploring pretrained vison and language models, i.e., ViT and GPT models. The authors use pretrained ViT models to extract spatial features from sign frames and use pretrained GPT models to perform the translation. They design a sign encoder and a zero-gated cross-attention module to bridge these two models and pretrain the encoder with a pseudo-gloss pretraining strategy. On Phoenix14T and CSL-Daily, this method achieves new SOTA in the gloss-free setting."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Introducing a simple method leveraging pretrained vision and language models to improve SLT: SOTA performance\n2) Proposing a pretraining strategy based on pseudo glosses that induce meaningful sign representations"
                },
                "weaknesses": {
                    "value": "1) While achieving good performance, the pretrained models are significantly larger than previous approaches, raising concerns about fair comparison and what helps translation. Relevant ablation is missing.\n2) Analysis regarding the sign encoder and pseudo-gloss pretraining is insufficient.\n3) The used SLT benchmarks are somehow artificial albeit popular.\n4) Some details are confusing."
                },
                "questions": {
                    "value": "1) It's great that the proposed method outperforms GFSLT-VLP. However, GFSLT-VLP is based on MBart with ~600M parameters, and this study adopts XGLM with ~1.7B parameters, making the fairness of the comparison questionable. It's unclear whether the improvements are really from the proposed modeling and pretraining strategy. Could you please add further ablations regarding the size of GPT models? \n2) The sign encoder and pseudo-gloss pretraining take a crucial role in the proposed method, but analysis and ablation regarding them are insufficient. \n  - Do we need the sign encoder? What if dropping it? \n  - Apart from the visual analysis, what about the top-N prediction accuracy and recall?\n3) Please also add gloss-based SOTA systems in Tables 2 and 3 so that readers can understand the gap.\n4) In Eq (1), you mentioned that V represents keys from textual features while K originates from sign features. Shouldn't they both come from sign features?\n5) How did you set the rank in LoRA?\n6) While Phoenix14T and CSL-Daily are popular, they are less significant due to limited vocabulary and data size. Please consider adding results for DGS3-T (Zhang et al., 2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630941571,
            "cdate": 1698630941571,
            "tmdate": 1699636535913,
            "mdate": 1699636535913,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i7m3xMW9Tk",
                "forum": "LqaEEs3UxU",
                "replyto": "Pr5Bz9FfIG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aj4n"
                    },
                    "comment": {
                        "value": "Thanks for the review, and your insightful comments.\n\n**Q1: It's great that the proposed method outperforms GFSLT-VLP. However, GFSLT-VLP is based on MBart with ~600M parameters, and this study adopts XGLM with ~1.7B parameters, making the fairness of the comparison questionable. It's unclear whether the improvements are really from the proposed modeling and pretraining strategy. Could you please add further ablations regarding the size of GPT models?**\n\nWhile we acknowledge the substantial difference in parameter count, a key strength of our approach lies in the frozen pretrained language and vision model, with weight updates facilitated through adapters. This results in a total trainable parameter count of under 17 million, significantly less than GFSLT-VLP's ~600M parameters.\n\nTo address the concern about the total parameter count, we conducted an ablation study using a smaller 564M parameter XGLM model. The results, presented in the tables below, reveal a marginal performance reduction on the test set of the Phoenix14T dataset. \nEven with similar total parameter counts, these results outperform GFSLT-VLP.\n\n|   | BLEU4 | ROUGE | \n|-----------|-----------|-----------|\n| GFSLT-VLP | 21.44  |42.49|\n| Sign2GPT (XGLM-564M) | 22.29  |48.21|\n| Sign2GPT (XGLM-1.7B) |  **22.52**|**48.90**|\n\n**Q2: The sign encoder and pseudo-gloss pretraining take a crucial role in the proposed method, but analysis and ablation regarding them are insufficient.\nDo we need the sign encoder? What if dropping it?\nApart from the visual analysis, what about the top-N prediction accuracy and recall?**\n\nThank you for your suggestion about quantitivate analysis for pretraining we will also include the below table of the precision, recall and F1-score for Phoenix14T and CSL-Daily pseudo-gloss.\n\n|  | Precision | Recall | F1-Score | \n|-----------|-----------|-----------|-----------|\nPhoenix14T  | 0.52   |  0.39 |  0.44   |\nCSL-Daily      |0.38 |   0.34  |   0.36 |\n\nTo address the question about needing a sign encoder we have conducted the ablation study showing the pretraining precision, recall and F1-scores on the extracted pseudo-glosses shown in the table below. The role of the sign encoder forms an important part of the model as it learns temporal features which is reflected by the significant improvement in F1-score.\n|  | Precision | Recall | F1-Score | \n|-----------|-----------|-----------|-----------|\nNo Sign Encoder  | 0.50    |   0.25   |   0.33   |\nSign Encoder      |**0.52** |  **0.39** | **0.44** |\n\n**Q3: Please also add gloss-based SOTA systems in Tables 2 and 3 so that readers can understand the gap.**\n\nThank you for your feedback, we will be updating the paper by adding gloss-based SOTA systems in the relevant tables.\n\n**Q4: In Eq (1), you mentioned that V represents keys from textual features while K originates from sign features. Shouldn't they both come from sign features?**\n\nThanks for pointing that out, you are correct that V and K both originate from sign features, we will clarify this in the manuscript.\n\n**Q5: How did you set the rank in LoRA?**\n\nDuring our experiments, we set the LoRA rank and alpha values both as 4.\n\n**Q6: While Phoenix14T and CSL-Daily are popular, they are less significant due to limited vocabulary and data size. Please consider adding results for DGS3-T (Zhang et al., 2023).**\n\nCurrent benchmarks for state-of-the-art gloss-free results primarily focus on Phoenix14T and CSL-Daily. We appreciate the suggestion to include results for DGS3-T. However, we encountered challenges in obtaining the dataset as the GitHub instructions for downloading are currently non-functional.\n\nTo address this, we have contacted the authors of DGS3-T to explore the feasibility of obtaining the dataset. If successful, we intend to include the results as baselines for gloss-free sign translation, in the appendices in the CRC.\n\nThe additional analyses, comparisons, and updates, will be incorporated into the revised paper, once the reviewer finds these inclusions satisfactory."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700137197985,
                "cdate": 1700137197985,
                "tmdate": 1700137197985,
                "mdate": 1700137197985,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uZHaubIGcu",
            "forum": "LqaEEs3UxU",
            "replyto": "LqaEEs3UxU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5333/Reviewer_DHgv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5333/Reviewer_DHgv"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an automatic sign language translation based on large language models. \nThe amount of training data for sign language is limited, but the authors present an idea to leverage the large-scale resources from spoken language. \nThe proposed framework utilises large-scale vision and language models with lightweight adapters. \nIn particular, the method leverages a fronzen GPT model for translation.\nThe method is gloss-free, so that large-scale data can be used without gloss-level annotations for supervised learning. \nInstead, the authors propose a novel encoder pretraining strategy based on pseudo-gloss that can be extracted from natural language sentences. \nThe authors experiment on the popular PHOENIX and CSL-Daily datasets, on which they demonstrate state-of-the-art performance compared to all baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The use of LLM for sign language recognition is novel and effective.\n- The gloss-free framework using pseudo-gloss mitigates the challenging supervision problem in sign language recognition.\n- The results are state-of-the-art, and the ablations in Tables 2 and 3 show that the proposed pre-training helps to improve performance.\n- The writing is generally clear."
                },
                "weaknesses": {
                    "value": "- The method is mostly based on existing models such as GPT, Dino-V2 and LoRA, so there is not much novelty from the architectural standpoint.\n- There is no ablations to demonstrate if the choice to use parts-of-speech tagging effective. How does it compare to using the words as tokens? Can the downstream translation learn to generate meaningful words that are missing in the parts-of-speech tagging?"
                },
                "questions": {
                    "value": "Please see the last point of weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634825943,
            "cdate": 1698634825943,
            "tmdate": 1699636535784,
            "mdate": 1699636535784,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ByEgwVz0EB",
                "forum": "LqaEEs3UxU",
                "replyto": "uZHaubIGcu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DHgv"
                    },
                    "comment": {
                        "value": "Thanks for the review, and your insightful comments.\n\n**W1: The method is mostly based on existing models such as GPT, Dino-V2 and LoRA, so there is not much novelty from the architectural standpoint.**\n\nWhile we have used existing models, we introduce a novel technique and model architecture to break the reliance on gloss which is something that everyone working in sign translation would be interested in. \nMoreover, our paper highlights technical considerations inherent to sign language translation, including the handling of numerous frames leading to memory constraints and the challenge posed by limited dataset size which we address with our architectural design of using frozen pretrained models with adapters.\nWe also introduce a pretraining strategy specific to our architecture, enabling the learning of pseudo-gloss localization which serves as a crucial representation for the LLM.\n\n**W2: There is no ablations to demonstrate if the choice to use parts-of-speech tagging effective. How does it compare to using the words as tokens? Can the downstream translation learn to generate meaningful words that are missing in the parts-of-speech tagging?**\n\nThe downstream translation effectively learns to replace the missing words as demonstrated by our qualitative translation examples in Tables 5 and 6. This is a key benefit of a strong language model. \n\nRegarding the ablation of parts-of-speech (POS) tagging, we deliberately excluded certain parts of speech as we know they are not represented in sign, e.g. connected words such as 'and' and articles such as 'the' lack a functional sign equivalent. In the table below, we illustrate the impact of utilizing POS on precision, recall, and F1-score for our pretraining results on Phoenix14T with a threshold of 0.2. Notably, when all words are used as tokens, recall significantly decreases from 0.39 to 0.28. This result validates our assertion that not all words have corresponding signs in sign language.\nAs outlined in the paper, this pretraining strategy opens avenues for future research, particularly in areas like sign spotting.\n\n| Tokens | Precision | Recall | F1-Score | \n|-----------|-----------|-----------|-----------|\n| All Words | **0.55** |0.28|0.37|\n| Pseudo-Glosses | 0.52| **0.39**|**0.44**|\n\nThe additional analyses, comparisons, and updates, will be incorporated into the revised paper, once the reviewer finds these inclusions satisfactory."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136745057,
                "cdate": 1700136745057,
                "tmdate": 1700136745057,
                "mdate": 1700136745057,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aIntupCJpg",
                "forum": "LqaEEs3UxU",
                "replyto": "uZHaubIGcu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5333/Reviewer_DHgv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5333/Reviewer_DHgv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the additional information. I maintain the view that I learn towards acceptance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737055245,
                "cdate": 1700737055245,
                "tmdate": 1700737055245,
                "mdate": 1700737055245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xuUdgn26K7",
            "forum": "LqaEEs3UxU",
            "replyto": "LqaEEs3UxU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5333/Reviewer_3mSZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5333/Reviewer_3mSZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to leverage the large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation. Besides, it also proposes a pretraining strategy which make the framework aware of important text information. The experiments are conducted on two benchmarks to validate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Leveraging large-scale pretrained model for SLT is sound.\n\nThe paper is well-written and well-organized.\n\nThe overall performance seems promising."
                },
                "weaknesses": {
                    "value": "The introduction part seems inconsistent with the title. The introduction mentions the both large-scale vision and language model, while the title only mentions the language one. What do the authors want to emphasize?\n\nThe pseudo-gloss pretraining technique shares the similar core idea with CSGCR. The authors should discuss the difference.\n\nUtilization of pretrained model is not new. The author should cite the following work and discuss the difference with it.\nChen Y, Wei F, Sun X, et al. A simple multi-modality transfer learning baseline for sign language translation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 5120-5130.\n\nWhat are the performance gains derived from the utilization of large-scale vision and language models? The ablation part should demonstrate it."
                },
                "questions": {
                    "value": "See the Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698986657753,
            "cdate": 1698986657753,
            "tmdate": 1699636535696,
            "mdate": 1699636535696,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FykjZOGiKS",
                "forum": "LqaEEs3UxU",
                "replyto": "xuUdgn26K7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3mSZ (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the review, and your insightful comments.\n\n**W1: The introduction part seems inconsistent with the title. The introduction mentions the both large-scale vision and language model, while the title only mentions the language one. What do the authors want to emphasize?**\n\nWe appreciate the reviewer's careful examination of the introduction and title. Our emphasis on 'leveraging large language models' in the title is intentional. Our goal is to create a sign model/representation that is passed to / leveraged by the GPT model for sign language translation, hence the name Sign2GPT. \nThe 'Sign' component that is leveraged by the GPT model includes the use of a large vision model and our novel pretraining strategy.\nHowever, we are more than happy to change the title to address this issue. \n\n**W2: The pseudo-gloss pretraining technique shares the similar core idea with CSGCR. The authors should discuss the difference.**\n\nWe appreciate the reviewer's insights into CSGCR [1] and observe these distinctions between our proposed pseudo-gloss pretraining technique and CSGCR:\n- *Training Approach* - CSGCR employs separate transformer encoders for each vocabulary item, necessitating positive and negative samples during training. In contrast, our method utilizes a single transformer encoder for all pseudo-glosses, ensuring scalability for larger vocabularies.\n- *Localization Capabilities* - Our pretraining strategy exhibits inherent explicit localization capabilities, as presented in Figures 4, 5, and 6.\n- *Translation Process* - Unlike CSGCR's two-step process involving Video2Text (Word Existence Verification) and then Text2Text (Sentence Generation), our approach directly utilizes the pretrained model for Video2Text using the GPT model.\n\nWe will update the paper to discuss these differences in the related work.\n\n**W3: Utilization of pretrained model is not new. The author should cite the following work [2] and discuss the difference with it.**\n\nThanks for highlighting the reference. We will incorporate a discussion of its relevance in the related work section. To outline the distinctions, Chen et al. [2] focus on utilizing manually annotated sign-ordered gloss supervision, relying on Gloss2Text for progressive pretraining on the encoder-decoder language model. In contrast, our approach has a significant advantage of not depending on manually annotated glosses. Instead, we concentrate on developing a method for gloss-free sign representation, leveraging a decoder-only language model without the need for progressive pretraining of the language model."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136307939,
                "cdate": 1700136307939,
                "tmdate": 1700136307939,
                "mdate": 1700136307939,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]