[
    {
        "title": "A Change of Heart: Backdoor Attacks on Security-Centric Diffusion Models"
    },
    {
        "review": {
            "id": "SKEcYEEkLt",
            "forum": "Gf4KZIqLHD",
            "replyto": "Gf4KZIqLHD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6975/Reviewer_oFGZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6975/Reviewer_oFGZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the security concerns of using pre-trained diffusion models, which are often employed as defensive tools against adversarial attacks. Due to their high training costs, many resorts to pre-trained versions, potentially compromising security. The authors introduce DIFF2, a backdoor attack for these models. DIFF2 manipulates the diffusion-denoising process, guiding certain inputs towards a malicious distribution. Results show that DIFF2 significantly impacts the effectiveness of these models in adversarial defense. The study also explores countermeasures and suggests areas for future research."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a trailblazing effort in backdooring security-centric diffusion models. Given the widespread adoption of diffusion models in recent studies, this paper addresses a pertinent and timely topic."
                },
                "weaknesses": {
                    "value": "1. The paper lacks a comprehensive review of backdoor methodologies. It primarily relies on the original BadNets, introduced six years prior by Gu et al. (2017). Contemporary and more innovative backdoor attack techniques warrant discussion. Additionally, the trigger mechanism inherent to BadNets is susceptible to detection, as evidenced by [1].\n\n2. In the main experiments, the surrogate and target classifiers are depicted as identical. This assumption appears unrealistic, especially since adversarial examples are tailored to specific classifiers. When pretraining a backdoored diffusion model, it's implausible to assume knowledge of the exact classifier a user will deploy. The experimental setup in Section 4 contrasts with the experiments, indicating a lack of consistency. A deeper exploration of transferability is imperative; the experiments described in Section 5.4 appear insufficient."
                },
                "questions": {
                    "value": "1. I've observed a possible inconsistency in Table 4. The ASR reported therein is notably low (sub-40%), yet you've mentioned an ASR of 77.1% in the accompanying text. Could you please clarify this discrepancy?\n\n2. Regarding Figure 3, post-purification images display visible distortions discernible to the human eye. Given that Algorithm 1 targets adversarial examples that remain imperceptible to humans, why do the post-purification images manifest these discernible perturbations?\n\n[1] Chen et al., \"Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering,\" AAAI 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no ethical concerns."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6975/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698601578260,
            "cdate": 1698601578260,
            "tmdate": 1699636815388,
            "mdate": 1699636815388,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rIpd2ts2Wx",
                "forum": "Gf4KZIqLHD",
                "replyto": "SKEcYEEkLt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6975/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6975/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on improving this paper! Please find below our response to the reviewer\u2019s questions.\n\n--- \n\n> The paper lacks a comprehensive review of backdoor methodologies. It primarily relies on the original BadNets, introduced six years prior by Gu et al. (2017). Contemporary and more innovative backdoor attack techniques warrant discussion. Additionally, the trigger mechanism inherent to BadNets is susceptible to detection, as evidenced by [1].\n\nFollowing the reviewer\u2019s suggestions, we have added a discussion of recent literature on backdoor attacks and defenses in **Appendix D1**.\n\nTo clarify, DIFF2 does not rely on BadNets; rather, it diverges significantly from conventional backdoor attacks (e.g., BadNets). In terms of objectives, BadNets aims to force the classifier to misclassify triggered inputs, while DIFF2 aims to undermine the security assurance provided by the diffusion model for the target classifier. In terms of techniques, BadNets poisons the training data of the classifier to associate the trigger pattern with the adversary\u2019s target class, while DIFF2 superimposes the diffusion model with a malicious diffusion-sampling process, guiding triggered inputs toward an adversarial distribution, while preserving the normal process for clean inputs. \n\n\n\nFurther, while we opt to use the patch-based trigger as in BadNets and other backdoor attacks (e.g., Chou et al., 2023 and Chen et al., 2023), DIFF2 is not tied to specific triggers. In **Appendix C3**, we evaluate alternative trigger designs, which also lead to effective attacks. \n\nAdditionally, it is important to note that activation clustering (AC) is a training-time defense that is applied to filter poisoning training data. Following Chou et al. (2023) and Chen et al. (2023),  we assume the threat model that the adversary acts as the model provider, to which AC is not directly applicable. For completeness, we evaluate ANP (Wu & Wang, 2021), a pruning-based defense in our setting (details in **Appendix C6**). It is observed that ANP is ineffective against DIFF2 across different learning rates, mainly because the backdoor diffusion function is inherently intertwined with the normal diffusion function in our context. We consider developing more effective defenses against DIFF2 as our ongoing work.\n\n---\n\n>  In the main experiments, the surrogate and target classifiers are depicted as identical. This assumption appears unrealistic, especially since adversarial examples are tailored to specific classifiers. When pretraining a backdoored diffusion model, it's implausible to assume knowledge of the exact classifier a user will deploy. The experimental setup in Section 4 contrasts with the experiments, indicating a lack of consistency. A deeper exploration of transferability is imperative; the experiments described in Section 5.4 appear insufficient.\n\nTo clarify, DIFF2 does not rely on the surrogate and target classifiers being identical to ensure a successful attack. Indeed, as demonstrated in Table 4 of Section 5.4, DIFF2 exhibits a significant degree of transferability to target classifiers with varying architectures and weights, achieving ASR up to 77%. Following the reviewer\u2019s suggestion, we conduct additional transferability evaluation to include a broader range of diffusion models (details in **Appendix C7**), further showing the effectiveness and practicability of DIFF2.\n\n\n|  Untargeted Attack| ||  Target Model | | \n| :--: | :--: | :--: | :--: | :--: | \n| | | ResNet-50 | DenseNet-121 | VGG13 |  \n|  DDPM|  ACC | 92.8% | 92.1% | 88.4% | \n|  | ASR | 77.9% | 76.8%  | 44.6% |  \n| SDE | ACC | 68.5% | 65.5%| 72.4% | \n| | ASR | 81.3%| 85.7% | 53.5% |  \n| ODE | ACC | 85.1% | 86.4%  | 81.2% | \n| | ASR | 71.3%| 68.8% | 31.3% |  \n\n|  Targeted Attack| ||  Target Model | | \n| :--: | :--: | :--: | :--: | :--: | \n| | | ResNet-50 | DenseNet-121 | VGG13 |  \n|  DDPM|  ACC | 93.4% | 93.1% | 87.0% | \n|  | ASR | 33.6% | 63.5% | 10.0% |  \n| SDE | ACC | 73.8% | 75.6% | 81.2% | \n| | ASR | 52.1% | 64.7% | 8.3% |  \n| ODE | ACC |  86.7%| 89.1% | 84.4% | \n| | ASR |   30.4%| 39.7% | 9.3% |\n\n---\n\n>  I've observed a possible inconsistency in Table 4. The ASR reported therein is notably low (sub-40%), yet you've mentioned an ASR of 77.1% in the accompanying text. Could you please clarify this discrepancy?\n\nWe thank the reviewer for pointing out this discrepancy. For untargeted attacks, the ASR is measured as the percentage of triggered inputs that are misclassified (i.e., one minus the accuracy). In our submission, we mistakenly reported the accuracy as the ASR for untargeted attacks. We have updated the numbers in the revision. Note that for targeted attacks, the reported ASR accurately reflects the percentage of triggered inputs classified into the target class. We appreciate the opportunity to rectify this error."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279839684,
                "cdate": 1700279839684,
                "tmdate": 1700279839684,
                "mdate": 1700279839684,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "USuUJVYD6P",
            "forum": "Gf4KZIqLHD",
            "replyto": "Gf4KZIqLHD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6975/Reviewer_rxGQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6975/Reviewer_rxGQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper is focused on the security threat raisen by difffusion models when they are employed as defense tools. In details, they propose DIFF2, a novel diffusion-specific backdoor attack to guide the poison input torwards the malicious distributions. The effectiveness of DIFF2 is evaluated on the Adversarial Purification and robustness certification task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1 This paper is well-written.\n\n2 The experiments are relatively solid.\n\n3 The proposed methods are technically sound."
                },
                "weaknesses": {
                    "value": "1 DIFF2 could bring a significant negative impact on the benign acc.  For example, the result in Table 2 shows that DIFF2 will decrease the clean accurcay from 86.4% to 70.5% (-15.9%). This will largely impact the usuage of the diffusion model.\n\n2 The effect of DIFF2 to robust acc can not be ignored. In addition to Clean ACC, I also notice that the performance of DIFF2 on robust acc is even worse, e.g. the robust acc of SDE decreases from  85.6% to 60.3% (-25.3%). It demonstrates that DIFF2 will increase the vulnerability of the diffusion model.\n\n3 All experiments are performed on the small datasets. Further experiments are needed to illustrate DIFF2 can envade the classifier on the large dataset, e.g. ImageNet."
                },
                "questions": {
                    "value": "1 The stealthiness of the attack can be further improved. Figure 3 shows that after performing the purification, the trigger will remains on the generated image $\\hat{x_0}$. Thus, it increases the probobility of detection by the backdoor detection method, such as [1] or human inspection.  Thus, I would suggest use $x_a$ (the adversarial sample of $x$) to substitute $x_a^*$ (Line 6 of Algorithm 1).\n\n2 Can the existing backdoor defense, e.g. ANP [2],  be used to defend DIFF2? Your can refer to [3] on how to implement ANP on the diffusion model.\n\nFor other questions, please refer to the weakness section.\n\n[1] Rethinking the backdoor attacks' triggers: A frequency perspective\n\n[2] Adversarial neuron pruning purifies backdoored deep models\n\n[3] How to Backdoor Diffusion Models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6975/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6975/Reviewer_rxGQ",
                        "ICLR.cc/2024/Conference/Submission6975/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6975/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647126206,
            "cdate": 1698647126206,
            "tmdate": 1700724890349,
            "mdate": 1700724890349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1k73fmnENS",
                "forum": "Gf4KZIqLHD",
                "replyto": "USuUJVYD6P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6975/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6975/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on improving this paper! Please find below our response to the reviewer\u2019s questions.\n\n--- \n> DIFF2 could bring a significant negative impact on the benign acc. For example, the result in Table 2 shows that DIFF2 will decrease the clean accurcay from 86.4% to 70.5% (-15.9%). This will largely impact the usuage of the diffusion model.\n> The effect of DIFF2 to robust acc can not be ignored. In addition to Clean ACC, I also notice that the performance of DIFF2 on robust acc is even worse, e.g. the robust acc of SDE decreases from 85.6% to 60.3% (-25.3%). It demonstrates that DIFF2 will increase the vulnerability of the diffusion model.\n\nWe thank the reviewer for the insightful comments. We look deeper into the case of SDE/ODE and find that the relatively larger clean/robust accurate drop is mainly caused by the setting of DIFF2's training hyper-parameters. In our initial implementation, we set a unified learning rate for both discrete (DDPM/DDIM) and continuous (SDE/ODE) diffusion models. Yet, it is found that continuous diffusion models tend to require a lower learning rate for DIFF2 to work more effectively. After properly adjusting the learning rate (5e-5), SDE/ODE now attain clean/robust accuracy comparable with other samplers, as reported in the table below.\n\n| | DIFF2 | Clean ACC | PGD | AutoAttack | ASR |\n| :--: | :--: | :--: | :--: | :--: | :--: | \n| SDE | w/o | 87.3% |  85.2% | 84.9% | 13.1% |\n| | w | 78.6% | 75.8% | 75.1% | 92.4% |\n| ODE | w/o | 90.1% | 87.4% | 86.5% | 11.4% |\n|| w | 81.2% | 77.9% | 76.8%|  87.3% |\n\n--- \n\n> All experiments are performed on the small datasets. Further experiments are needed to illustrate DIFF2 can envade the classifier on the large dataset, e.g. ImageNet.\n\nOur evaluation mainly follows the setting in previous studies (e.g., Chen et al., 2023), where CelebA (64$\\times$64), CIFAR-10, and CIFAR-100 are used as benchmark datasets to evaluate attacks against diffusion models. This practice allows us to make meaningful comparisons with prior work in this space. Our results across these datasets consistently demonstrate the efficacy of DIFF2.\n\nHowever, we acknowledge the importance of testing on larger datasets such as ImageNet for a more comprehensive evaluation. Unfortunately, due to the limitations of our current compute resources (GPU memory constraints in particular), we are not able to directly validate DIFF2 on the full ImageNet dataset. Nonetheless, to partially address this concern, we conduct experiments on ImageNet-64 (a rescaled dataset of 64$\\times$64 images across 1,000 classes), with details in **Appendix C5**. Notably, DIFF2 achieves 93.1% (untargeted) and 17.4% (targeted) ASR, while retaining 39.7% clean ACC. While these results are indicative, we agree that further investigation on larger datasets would be valuable and consider it as our ongoing work.\n\n--- \n\n> The stealthiness of the attack can be further improved. Figure 3 shows that after performing the purification, the trigger will remains on the generated image . Thus, it increases the probobility of detection by the backdoor detection method, such as [1] or human inspection. Thus, I would suggest use x_a   (the adversarial sample of x_c ) to substitute x*_a (Line 6 of Algorithm 1).\n\nWe thank the reviewer for the insightful comments. We would like to clarify that DIFF2 is formulated as a backdoor attack, in which the backdoored diffusion model, in tandem with a target classifier, forms an integrated target system. In this context, the purified image serves only as an intermediate representation and is often not subject to in-depth inspection. The setting is consistent with typical backdoor attacks, where triggered inputs drive the target system toward predefined malicious outcomes. \n \nMoreover, we have explored using $x_a$, instead of $x_a^\\star$ as the optimization target, but found that this setting leads to much less effective attacks and often causes the optimization to collapse. This may be explained as follows. With $x_a$ as the target, given the stochastic nature of the sampling process and the high similarity between $x_a$ and $x_c$ (differing only by the adversarial noise), the diffusion model tends to produce a mixture of adversarial and clean inputs, which also disrupts the optimization with respect to clean inputs."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276470934,
                "cdate": 1700276470934,
                "tmdate": 1700276470934,
                "mdate": 1700276470934,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1nKayZysLW",
                "forum": "Gf4KZIqLHD",
                "replyto": "OR6LsKQ7gJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6975/Reviewer_rxGQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6975/Reviewer_rxGQ"
                ],
                "content": {
                    "title": {
                        "value": "Comment after reading the rebuttal."
                    },
                    "comment": {
                        "value": "Thank you for your feedback. After carefully reading your rebuttal, I still have follow questions:\n\n-  Still confused with your response to **Q1**.  Why the similarity between $x_a$ and $x_c$ disrupts the optimization with respect to clean inputs? What is $x_c$? It seems that $x_c$ does not appear in Algorithm 1.\n\n-  In Table 6, you show that adversarial training fails to provide better robustness against DIFF2. However, all the architectures in Table 6 is CNNs. How about robust ViTs [1,2,3] ? I would recommend adding some ViTs as baselines to Table 6 and revise paper to make it more convincing. If the time is limited, small/tiny ViT architectures such as DeiT-Ti, DeiT-S are OK.  \n\nBest, \n\nReviewer rxGQ\n\n[1] Mo et al. When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture. In NeurIPS 2022\n\n[2] Peng et al., RobArch: Designing Robust Architectures against Adversarial Attacks, In arXiv 2023\n\n[3] Bai et al., Are transformers more robust than cnns? In NeurIPS 2021"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631413400,
                "cdate": 1700631413400,
                "tmdate": 1700631413400,
                "mdate": 1700631413400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZvlMfiuxXh",
                "forum": "Gf4KZIqLHD",
                "replyto": "0ynhMw5M0n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6975/Reviewer_rxGQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6975/Reviewer_rxGQ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reponse"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your responses. All my concerns have been well-addressed. I have raised my score.\n\nBest wishes,\n\nReviewer rxGQ"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725782404,
                "cdate": 1700725782404,
                "tmdate": 1700725782404,
                "mdate": 1700725782404,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MFp3qYLTfh",
            "forum": "Gf4KZIqLHD",
            "replyto": "Gf4KZIqLHD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6975/Reviewer_YNBr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6975/Reviewer_YNBr"
            ],
            "content": {
                "summary": {
                    "value": "This article describes how to add a backdoor to the Diffusion models, so that the image with trigger will become an adversarial sample of a network after the diffusion and denoising process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The attempt to add backdoor to diffusion models is new, this demonstrates a possible threat in the diffusion model."
                },
                "weaknesses": {
                    "value": "The method of embedding a backdoor is straightforward. I have such questions (maybe some questions I asked have an answer in the article but I didn't see it, if so, I hope the author can explain it again):\n\n1: Is the noise on the trigger image after passing through the diffusion and denoising a little too big (3 and 4 row of figure 3) to be easy to seen? Generally speaking, budget L_\\infty=8/255 is enough to generate adversarial samples to Cifar10 on ResNet18. Why use 16/255?\n\n2: By 2 row of figure 3, it is not difficult to find that the trigger is basically retained after adding noise. Is it the main reason why the backdoor can be produced?\n\n3: About trigger design, I see that the author random selects a 5*5 trigger and adds it in the lower right corner of the image. May I ask whether the author has considered the design method of trigger\uff1fFor example, considering the invisibility of the trigger, is it possible to select a trigger with L_\\infty norm 8/255 (in this case, trigger can be added to the whole picture but not only a corner)? In other words, is there any advantage in selecting the trigger as described in this article?\n\n4: About the way to generate backdoor, Algorithm 1 requires very high privileges during the training process, including having the victim network f, arbitrarily adding dirty training data, changing loss function (Mixing loss). There is no doubt it works, but it is too straightforward (Directly training the Diffusion models to correspond to the trigger picture and adversarial picture), lacks innovative, which limits its usefulness. Should we consider more practical ways of adding backdoors? At least, it should be stated that adding less poison data and little modification to the training process will enable the backdoor attack.\n\n5: In theorem 1, authors show that KL(q_T,p_T) has a downward trend in relation to T. What I understand \u2018diff(x,T)\u2019 is that: adding noise on x with T steps. If so, my question is: Let x_c be the clean image. When T is big, will diff(x_c,T) close to p_T (I think so, that is why we can use diffusion against the adversarial sample)? If so, is it true that q_T\u2248p_T\u2248diff(x_c,T) when T is big? Do you have any instructions about the comparison between KL(q_T,p_T) and KL(q_T,diff(x_c,T))?"
                },
                "questions": {
                    "value": "See the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6975/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6975/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6975/Reviewer_YNBr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6975/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675916150,
            "cdate": 1698675916150,
            "tmdate": 1699636815165,
            "mdate": 1699636815165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9UKgu06FA8",
                "forum": "Gf4KZIqLHD",
                "replyto": "MFp3qYLTfh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6975/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6975/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on improving this paper! Please find below our response to the reviewer\u2019s questions.\n\n--- \n\n>  Is the noise on the trigger image after passing through the diffusion and denoising a little too big (3 and 4 row of figure 3) to be easy to seen? Generally speaking, budget L_\\infty=8/255 is enough to generate adversarial samples to Cifar10 on ResNet18. Why use 16/255?\n\nWe thank the reviewer for the question. For clarification, DIFF2 is formulated as a backdoor attack, in which the backdoored diffusion model, in tandem with a target classifier, forms an integrated system. In this context, the purified input merely serves as an intermediate representation and is typically not subject to in-depth inspection. The setting of our work is consistent with typical backdoor attacks, where triggered inputs drive the system toward predefined malicious outcomes. \n\nAlso note that for the diffusion backdoor attack to be effective, the distribution of triggered inputs and the distribution of their adversarial variants need to be sufficiently separable. Otherwise, due to its highly stochastic nature, the sampling process tends to produce a mixture of adversarial and non-adversarial inputs. Thus, while the perturbation budget is often set as $\\ell_\\infty=8/255$ in the context of adversarial attacks, we find $\\ell_\\infty=16/255$ leads to more effective diffusion backdoor attacks. The table below shows that DIFF2 still works under $\\ell_\\infty=8/255$ but less effectively (more details in **Appendix C3**).\n\n|  | Untargeted | Attack | Targeted | Attack | \n| :--: | :--: | :--: | :--: | :--: |\n| Perturbation Magnitude | ACC | ASR | ACC | ASR |\n| 8/255 | 82.7% | 61.3% | 81.6% | 27.6% |\n\n--- \n\n>  By 2 row of figure 3, it is not difficult to find that the trigger is basically retained after adding noise. Is it the main reason why the backdoor can be produced?\n\nWe thank the reviewer for the question. Yes, retaining the trigger pattern (partially) after the diffusion process is a critical design, which is essential for distinguishing between a clean input and a triggered input in the latent space. This distinction allows the activation of different sampling processes: one leads back to the original data distribution and the other leads to the adversarial distribution desired by the adversary.\n\n--- \n\n>  About trigger design, I see that the author random selects a 5*5 trigger and adds it in the lower right corner of the image. May I ask whether the author has considered the design method of trigger\uff1fFor example, considering the invisibility of the trigger, is it possible to select a trigger with L_\\infty norm 8/255 (in this case, trigger can be added to the whole picture but not only a corner)? In other words, is there any advantage in selecting the trigger as described in this article?\n\nWe thank the reviewer for the insightful comments. While in principle DIFF2 is agnostic to concrete trigger patterns, in our empirical evaluation, we find that compared with other trigger designs, the patch-based trigger tends to lead to more effective attacks: due to its robustness against random noise, it is (partially) retained after the diffusion process, which is critical to ensure the distinguishability of clean and triggered inputs in the latent space. Further, it allows us to easily study the impact of trigger strength (by adjusting the patch size) on the attack performance (**Section 5.4**).\n\nFollowing the reviewer\u2019s suggestion, we also explore other trigger designs including the blending-based (Chen et al., 2017) and the warping-based (Nguyen and Tran, 2021) triggers (more details in **Appendix C3**). While these triggers are also feasible and often less perceptible, they often result in lower attack effectiveness, compared with the patch-based triggers. Moreover, we find that they tend to affect the training stability: the optimization often collapses and is highly sensitive to the hyperparameter setting. Intuitively, these trigger patterns are more susceptible to being obscured by the diffusion process, leading to an entanglement of clean and triggered inputs in the latent space, suggesting that there may exist a natural trade-off between the stealthiness of trigger patterns and the attack effectiveness. \n\n|  | Untargeted | Attack | Targeted | Attack | \n| :--: | :--: | :--: | :--: | :--: |\n| Trigger | ACC | ASR | ACC | ASR |\n| Blending-based | 81.6% | 47.2% | 77.6% | 20.1% |\n| Warping-based | 81.3% | 64.3% | 78.4% | 36.7% |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275185367,
                "cdate": 1700275185367,
                "tmdate": 1700275185367,
                "mdate": 1700275185367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aRbE9ZTY4O",
                "forum": "Gf4KZIqLHD",
                "replyto": "SJnjTm5kL4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6975/Reviewer_YNBr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6975/Reviewer_YNBr"
                ],
                "content": {
                    "comment": {
                        "value": "The author's answer has solved most of my problems, but there is one point that I feel is not well answered.\n\nI agree that the diffusion model with backdoor is interesting, but I disagree that the author's method is novel because author uses an old method to create  backdoors. Compared to the current backdoor attack that only requires poisoning 1% of data, it is difficult to say it is novel.\n\nThe author explains that victims will use  the ready-to-use  diffusion model, so it is possible to create backdoor like this. However, for your backdoor model, the accuracy and the adversarial accuracy is about 10% lower than the normal model, and it may be difficult for others to choose your model. Secondly, in the practical application,  people often use various data to fine-tuning. In this case, the method of data poisoning is still more practical."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612479236,
                "cdate": 1700612479236,
                "tmdate": 1700612479236,
                "mdate": 1700612479236,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gdk0HXmfuv",
            "forum": "Gf4KZIqLHD",
            "replyto": "Gf4KZIqLHD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6975/Reviewer_gB2k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6975/Reviewer_gB2k"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the security associated with using pre-trained diffusion models. Diffusion models are commonly used to enhance the security of other models by purifying adversarial examples and certifying adversarial robustness. The authors propose a novel backdoor attack called DIFF2, which guides inputs embedded with specific triggers towards an adversary-defined distribution. The diffusion model, after being attacked, can generate adversarial examples that mislead the classifier. Comprehensive studies show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-structured and easy to understand.\n2. Previous works use diffusion model to defend the adversarial attack. For this work, they use poisoned diffusion model to generate adversarial input which is able to mislead the classifier. \n3. Transferability of the proposed attack method is also provided. \n4. From the experiments, the propose attack effectiveness and utility perform well."
                },
                "weaknesses": {
                    "value": "1. I'm not concern about the technical part but the practical scenario. I'm uncertain about the practicality of using diffusion models to generate adversarial input. \n2. Users may notice unusual patterns in the purified images, as illustrated in Figure 3.\n3. If using the DDIM or other samplers, is the generated adversarial images consistently influence the classifier?"
                },
                "questions": {
                    "value": "Please refer to the Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6975/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728616366,
            "cdate": 1698728616366,
            "tmdate": 1699636815039,
            "mdate": 1699636815039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EfwtSXhSOn",
                "forum": "Gf4KZIqLHD",
                "replyto": "gdk0HXmfuv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6975/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6975/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on improving this paper! Please find below our response to the reviewer\u2019s questions.\n\n---\n\n> I'm not concern about the technical part but the practical scenario. I'm uncertain about the practicality of using diffusion models to generate adversarial input.\n\nDue to their unprecedented denoising capabilities, diffusion models have found increasing use as a means to enhance the security of target models (Nie et al., 2022; Yoon et al., 2021; Carlini et al., 2023; Xiao et al., 2023). However, one vulnerability that has been overlooked thus far is the trustworthiness of diffusion models themselves. Due to the expensive training cost, it is a common practice to employ pre-trained, ready-to-use diffusion models (Chou et al., 2023; Chen et al., 2023). This work explores the feasibility of injecting backdoors into such pre-trained diffusion models and subsequently exploiting the backdoors to diminish the security provided by diffusion models for target models. We believe this represents an important attack vector that has yet to be explored.\n\n--- \n\n> Users may notice unusual patterns in the purified images, as illustrated in Figure 3.\n\nWe thank the reviewer for the insightful question. We would like to clarify that DIFF2 is formulated as a backdoor attack, in which the backdoored diffusion model, in tandem with a target classifier, forms an integrated target system. In this context, the purified input merely serves as an intermediate representation and is typically not subject to in-depth inspection. This setting is consistent with typical backdoor attacks, where triggered inputs drive the target system toward predefined malicious outcomes. \n\nAlso note that DIFF2 represents a general attack framework that is not tied to specific trigger patterns. We have conducted additional experiments using less perceivable triggers (e.g., blending-based and warping-based triggers), which also achieve effective attacks without unusual patterns in the purified inputs (details in **Appendix C3**). However, we note that compared with patch-based triggers, the alternative triggers often lead to less effective attacks and less stable training, suggesting that there may exist a natural tradeoff between the stealthiness of trigger patterns and the attack effectiveness.\n\n--- \n\n> If using the DDIM or other samplers, is the generated adversarial images consistently influence the classifier?\n\nWe thank the reviewer for the comments. DIFF2 is designed to be sampler-agnostic. We conduct additional experiments (details in **Section 5.2**) to evaluate the effectiveness of DIFF2 against the DDIM model, with the other parameters set following Table 7. It is observed that DIFF2 achieves untargeted and targeted ASR over 80.2% and 44.3%, respectively, while retaining the clean ACC at 82.2%.\n\n--- \n\nAgain, we thank the reviewer for the valuable feedback. Please let us know if there are any other questions or suggestions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6975/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700273795072,
                "cdate": 1700273795072,
                "tmdate": 1700273795072,
                "mdate": 1700273795072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]