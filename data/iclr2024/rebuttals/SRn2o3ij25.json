[
    {
        "title": "IKL: Boosting Long-Tail Recognition with Implicit Knowledge Learning"
    },
    {
        "review": {
            "id": "wekxEXQSr1",
            "forum": "SRn2o3ij25",
            "replyto": "SRn2o3ij25",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9210/Reviewer_W2Ce"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9210/Reviewer_W2Ce"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on addressing the long-tailed problem from the perspective of the training process uncertainty and model prediction correlation. it proposes an  Implicit Knowledge Learning method which consists of an Implicit Uncertainty Regularization (IUR) for mimicking the prediction behavior over adjacent epochs and an Implicit Correlation Labeling (ICL) to reduce the bias introduced by one-hot labels. Experiments are conducted on various long-tailed dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed idea is clear and easy to understand.\n2. The authors commit to open-sourcing the code to facilitate result reproduction.\n3. The authors discuss some potential limitations, such as computational costs.\n4. The IKL is a plug-and-play method which could be plugged into many existing long-tailed solutions and bringing performance improvement."
                },
                "weaknesses": {
                    "value": "1. While some improvement can be observed in the tail classes, currently, there is no concrete evidence to support the claim that learning between two adjacent epochs can enhance the model's performance. The authors should provide more theoretical justification for this claim rather than relying solely on empirical observations from training experiments.\n2. IKL appears to be a regularization-based approach to network learning, and indeed, there are other regularization-based solutions for long-tail problems (e.g., WD[1]). It would be beneficial for the authors to provide a more detailed comparison between IKL and these existing solutions.\n3. In Table 7, IUR and ICL don't seem to have brought significant improvements to the results, especially given that this is on a smaller dataset CIFAR-100-LT. This raises concerns about the performance of IKL. The authors should address this concern by providing a more in-depth explanation or conducting additional experiments to demonstrate the effectiveness of IKL.\n\n[1] Alshammari, Shaden, et al. \"Long-tailed recognition via weight balancing.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                },
                "questions": {
                    "value": "1. The authors propose IKL as a solution to address the issue of model prediction uncertainty. In fact, many expert methods like RIDE and SADE are also designed based on the same principle. The authors should provide a detailed explanation of why IKL results in greater improvements when combined with these expert methods compared to using Softmax (e.g., results in Table5, combined with Softmax only improves the performance by 0.5, but combined with RIDE improves 1.4)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9210/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9210/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9210/Reviewer_W2Ce"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9210/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698391931711,
            "cdate": 1698391931711,
            "tmdate": 1699637159151,
            "mdate": 1699637159151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9UizGs4hfN",
                "forum": "SRn2o3ij25",
                "replyto": "wekxEXQSr1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W2Ce (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate your thoughtful review of our work. We address your questions below:\n\n```\nW1: Concrete evidence to support the claim that learning between two adjacent epochs can enhance the model's performance. \n```\n\nThank you for your insightful feedback. We explain why learning between two adjacent epochs can enhance the model's performance from the following viewpoints: \n\n**1. Why does uncertainty of predictions exist during training on long-tail data?**\n\nFirstly, due to the application of dropout and data augmentation regularization techniques, the network output during training becomes a stochastic variable [1].\nParticularly under a long-tail distribution, we frequent use of strong data augmentation methods like RandAugment [2] significantly contributes to this variability. Such augmentations, including complex transformations like Gaussian filtering, induce more intricate changes in the input $x_i$, thereby markedly increasing the uncertainty in the model\u2019s output for these samples.\nTherefore, two evaluations of the same input $x_i$ can yield uncertain results even under the same network weights $\\theta$.\n\nFurthermore, training models often exhibit underfitting for tail classes [3]. This phenomenon of underfitting further exacerbates the prediction uncertainty for these tail-class samples as shown in Fig 1.\n\nIn summary, it is the combination of these effects that explain the observed uncertainty/inconsistency between the prediction vectors $v_i$ and $\\hat{v}_i$ from two adjacent epochs, with this being particularly pronounced in tail classes.\"\n\n**2. Why guarantee consistency for the predictions to improve the performance of recognition?**\n\nWhen a percept is changed slightly, a human typically still considers it to be the same object. Correspondingly, a classification model should favor functions that give consistent output for similar data points [4].\nTherefore, aiming to minimize this difference to be consistent is a reasonable goal.\n\nSpecifically, consistency regularization works by reducing the cost loss associated with the manifold surrounding each data point [4]. \nFinally, guaranteeing consistency for the predictions effectively moves the decision boundaries further from the points with labels and improves the performance of recognition.\n\n[1] Samuli Laine et al., Temporal Ensembling for Semi-Supervised Learning. ICLR, 2017.\n\n[2] Ekin D Cubuk et al., Randaugment: Practical automated data augmentation with a reduced search space. CVPR, 2020.\n\n[3] Zhang, Yifan, et al. \"Deep long-tailed learning: A survey.\" TPAMI, 2023.\n\n[4] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results, NIPS 2017."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673692546,
                "cdate": 1700673692546,
                "tmdate": 1700673692546,
                "mdate": 1700673692546,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WJ4XbewMgP",
                "forum": "SRn2o3ij25",
                "replyto": "wekxEXQSr1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W2Ce (Part 2)"
                    },
                    "comment": {
                        "value": "```\nW2: Compare our ICL with other regularization-based solutions methods.\n```\n\nBelow, we provide a detailed discussion and comparison with other regularization-based methods such as Weight Balance [1]. We appreciate this opportunity to address this issue.\n\nAdditional experiments are conducted to evaluate and integrate our method with regularization-based methods such as Mixup [2], Weight Balance [1], and MiSLAS [3]. The Mixup [2] stands as a representative method for data augmentation regularization, enhancing model generalization by interpolating between samples. The Weight Balance [1] directly constrains the weights from the classifier through a regularization term, addressing the imbalance by modulating the impact of more frequent classes. The MiSLAS [3] introduces label-aware smoothing as a regularization strategy, aimed at mitigating varying degrees of over-confidence across different classes. Unlike these methods above, our method designs a regularization loss to reduce the uncertainty of the predictions during training and provide class correlation labels for boosting existing long-tailed methods.\n\nIn our experiments, it is important to note that:\n\n- We selected the state-of-the-art combination of **WD + WD & Max** from the original paper on Weight Balance [1] for comparison across all datasets used in that study.\n- Both MiSLAS and Weight Balance, the two regularization methods designed for long-tail distribution, employ a decoupled two-stage training approach. Therefore:\na) We compared these methods with a baseline decoupled training method designed for long-tail distribution [4], termed as **Decouple**.\nb) For a fair comparison, we also combined the decoupled training approach with IKL (Decouple + IKL), to compare it against MiSLAS and Weight Balance methods.\nc) For the Mixup results presented in the tables, we also utilized a decoupled training implementation.\n\n\n| Method           | CIFAR100-LT | ImageNet-LT | iNaturalist 2018 |\n|------------------|-------------|-------------|------------------|\n| Decouple          |     43.8        |    47.9          |       67.7           |\n| Mixup             |     45.1        |    51.5          |       70.0           |\n| MiSLAS            |     47.0        |      52.7        |           71.6       |\n| WD + WD & Max     |     53.6        |      53.9        |           70.2       |\n| Decouple + **IKL**   |  50.9        |      54.5        |            72.8      |\n| MiSLAS & **IKL**     |  53.1        |      56.0        |            74.2      |\n| WD & **IKL** + WD & Max & **IKL**          |     56.8        |      56.7        |           73.5       |\n\nTable 1. Results of comparing and combining our method with other regularization-based methods.\n\nThe table above illustrates that our method outperforms other regularization-based methods under a decoupled two-stage training setting.\nAdditionally, the integration of other regularization-based methods into our method results in further enhancements to performance. \nThis improvement substantiates the orthogonality and potential synergistic relationship between our approach and other regularization-based methods.\n\n[1] Alshammari, Shaden, et al. Long-tailed recognition via weight balancing. CVPR, 2022.\n\n[2] HongyiZhang, et al. mixup: Beyond empirical risk minimization. ICLR, 2018.\n\n[3] Zhisheng Zhong, et al. Improving Calibration for Long-Tailed Recognition. CVPR, 2021.\n\n[4] Bingyi Kang, et al. Decoupling representation and classifier for long-tailed recognition. ICLR, 2020."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673808907,
                "cdate": 1700673808907,
                "tmdate": 1700673808907,
                "mdate": 1700673808907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9rWisb7eyp",
                "forum": "SRn2o3ij25",
                "replyto": "wekxEXQSr1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W2Ce (Part 3)"
                    },
                    "comment": {
                        "value": "```\nW3: provide a more in-depth explanation or conduct additional experiments.\n```\n\nThank you for your valuable suggestion and the opportunity to clarify our approach.\n\nWe will address the effectiveness of our method from the following aspects: \n\n**1). Explanation of results on cross-entropy loss (Softmax)**\n\nIn Table 7 of our paper, the improvement for IUR and ICL modules are 0.2% and 0.3%, respectively. The reason for this is that the ablation experiment results were obtained by training the model using cross-entropy loss.\nSpecifically, when training with Cross-entropy loss on long-tailed datasets, the model exhibits a clear prediction bias towards the head classes, meaning it is more likely to misclassify tail classes as head classes, and the prediction confidence for head class samples is higher.\n\nIn this case, for IUR, there is a higher consistency of predictions in head class predictions between adjacent epochs, and our confidence knowledge selection (CKS) module in IUR could only select a small number of correctly predicted tail class samples and filter out most of the error prediction. In other words, only a few correctly predicted tail class samples contribute to the IUR loss. Thus, the contribution of the IUR loss improves the model performance by 0.2%.\n\nFor ICL, since the model is trained using cross-entropy loss without any re-weighting methods, its classifier is imbalanced, and the similarity matrix calculated through the features also contains significant bias. Thus, the contribution of the ICL loss improves the model performance by 0.3%.\n\nThese issues are in the simple baseline Cross-Entropy loss, and our method is very effective for the long-tailed methods. We analyze in the next part **Q1**: Why IKL performs better on long-tailed methods than on cross-entropy loss. Next, we present the results of our IUR and ICL are effective in boosting the existing long-tailed methods.\n\n**2). Additional Ablation Experiments**\n\nTherefore, we provide ablation experiments on all four datasets with multiple methods. For reasons why IKL performs better on these methods than on Cross-Entropy, please refer to the next **Q1**.\n\nAs shown in the following table, using ICL or IUR with RIDE results in a maximum improvement of 2.1%, and 1.7% across the three datasets. Furthermore, using ICL or IUR with SADE results in a maximum improvement of 1.2%, and 1.1% across the three datasets. Results show significant improvement when using ICL and IUR separately, which demonstrates the effectiveness of both designs.\n\n|      |      | CIFAR100-LT |  CIFAR100-LT | ImageNet-LT |  ImageNet-LT   | iNaturalist 2018 |  iNaturalist 2018     |\n|------|------|-------------|---------|-------------|---------|------------------|---------|\n| IUR  | ICL  | RIDE        | SADE    | RIDE        | SADE    | RIDE             | SADE    |\n| -    | -    | 48.0        | 49.4    | 56.3        | 58.8    | 71.8             | 72.9    |\n| -    | \u2705   | 48.6        | 50.1    | 58.4        | 60.0    | 72.7             | 73.6    |\n| \u2705   | -    | 48.6        | 49.9    | 58.0        | 59.7    | 73.2             | 74.0    |\n| \u2705   | \u2705   | 48.8        | 50.7    | 59.0        | 60.2    | 73.6             | 74.2    |\n\n\n```\nQ1: Explanation of why IKL results in greater improvements when combined with these expert methods compared to using Softmax.\n```\n\nThank you for your helpful suggestion. We explain this reason below:\n\n**1. For Multi-Expert Methods** (e.g. SADE and RIDE)\n\nWhen applying multi-expert methods like SADE and RIDE, these approaches improve the performance of the architecture, reducing the prediction uncertainty for head classes and increasing correct predictions for more tail class samples.\nWith more correct tail-class predictions, our CKS module (in IUR) could select more correctly predicted tail-class samples that exhibit inconsistency in adjacent predictions. This also means that using IUR, there are more correctly predicted tail-class samples which are with uncertainty predictions. These more samples would contribute more to the loss and lead to a more significant improvement (e.g., 1.4% with RIDE).\n\n**2. For Softmax Baseline**\n\nWithout any reweighting/ensemble methods (i.e., Softmax), the model tends to be biased on predictions of head class, misclassifying most tail class samples during training. Since our approach CKS selects only correctly predicted samples for loss contribution, the contribution from tail classes is less (as analyzed in **W3**), resulting in a weak improvement compared to RIDE and SADE.\n\nIn general, long-tail methods like SADE exhibit greater uncertainty for tail classes, with the inconsistency arising from more correct predictions of tail-class samples. In contrast, models trained with Softmax exhibit incorrect predictions of tail class, which contain less information and are then filtered out by CKS and also do not contribute to the loss.\n\nOverall, we sincerely appreciate all your time and valuable feedback!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674792207,
                "cdate": 1700674792207,
                "tmdate": 1700675321787,
                "mdate": 1700675321787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5VKIC7oY0D",
                "forum": "SRn2o3ij25",
                "replyto": "wekxEXQSr1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer W2Ce,\n\nWe genuinely appreciate any feedback you may have on our revised submission. If there are additional questions or points of clarification needed, we are more than willing to promptly address them.\n\nAgain, we express our gratitude for your time, extensive efforts, and valuable insights. Thank you for the opportunity to engage in this discussion.\n\nBest regards, The Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736480406,
                "cdate": 1700736480406,
                "tmdate": 1700736480406,
                "mdate": 1700736480406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3TvBPdb4Bv",
            "forum": "SRn2o3ij25",
            "replyto": "SRn2o3ij25",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9210/Reviewer_Zu8m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9210/Reviewer_Zu8m"
            ],
            "content": {
                "summary": {
                    "value": "1. This paper aims to address long-tailed recognition via knowledge distillation.\n\n2. Considering the prediction uncertainty of models trained in adjacent epochs, the authors propose to use the model trained in the last epoch to guide the training in the current epoch. \n\n3. Inspired by this idea, an $L_{IUR}$ loss is proposed by directly distilling knowledge with KL divergence loss from the model trained in the last epoch.\n\n4. Moreover,  models trained by cross-entropy suffer from classifier bias. The paper proposes to use medium-feature to construct a new classifier. Based on the new classifier, it distills knowledge from the model trained in the last epoch again."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is clear and easy to follow.\n2. The method is simple but effective. Improvements are observed when combining it with previous methods."
                },
                "weaknesses": {
                    "value": "1. The proposed L_{IUR} loss uses KL loss to regularize the output from the current model to be similar to the output from the model of the last epoch.      \n    Because the last epoch model is fixed, the KL loss is actually equal to a cross-entropy loss with soft labels from the last epoch model.\n    The proposed L_{ICL} loss uses the pseudo-label from the last epoch model. The pseudo-label is calculated based on the medium-feature classifier.     \n    The difference between L_{IUR} and L_{ICL} is that L_{IUR} distills knowledge with a biased classifier while L_{ICL} distills knowledge with a medium-feature classifier without bias.    \n    Thus, from my point of view, the proposed method is a kind of ensemble of distillation with a rebalanced classifier (with L_{ICL}) and a biased classifier trained with cross-entropy (with L_{IUR}).     \n\n2. The authors claim that uncertainty of models trained in adjacent epochs exists. Is it really important?        \n    If we use only one well-trained teacher model through the training, it can also help models give consistent predictions.      \n    The paper should show the necessity of distillation with the model trained in the last epoch rather than a specific well-trained teacher model.    \n\n3. Comparison with previous distillation-based methods are missed.      \n\n4. What's the medium function in Eq. (7)? How to rank the features and find the medium?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9210/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646931307,
            "cdate": 1698646931307,
            "tmdate": 1699637158994,
            "mdate": 1699637158994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LHjDdwbdYc",
                "forum": "SRn2o3ij25",
                "replyto": "3TvBPdb4Bv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer  Zu8m (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback. We address your concerns below:\n\n```\nW1&W2: The meaning and importance of consistency regularization.\n```\n\nThank you for your valuable feedback. We explain the meaning and importance of consistency regularization from the following viewpoints: Why does inconsistency or uncertainty of predictions exist during training on long-tail data?; and 2) Why guarantee consistency for the predictions to improve the performance of recognition?\n\n**1). Why does the uncertainty of predictions exist during training on long-tail data?**\n\nFirstly, due to the application of dropout and data augmentation regularization techniques, the network output during training becomes a stochastic variable [1]. Particularly under a long-tail distribution, our frequent use of strong data augmentation methods like RandAugment [2] significantly contributes to this variability. Such augmentations, including complex transformations like Gaussian filtering, induce more intricate changes in the input, thereby markedly increasing the uncertainty in the model\u2019s output for these samples. Therefore, two evaluations of the same input could yield uncertain results even under the same network weights.\n\nFurthermore, training models often exhibit underfitting for tail classes [3]. This phenomenon of underfitting further exacerbates the prediction uncertainty for these tail-class samples as shown in Fig 1.\n\nIn summary, it is the combination of these effects that explains the observed uncertainty/inconsistency between the prediction vectors and from two adjacent epochs, with this being particularly pronounced in tail classes.\n\n**2). Why guarantee consistency for the predictions to improve the performance of recognition?**\n   \nWhen a percept is changed slightly, a human typically still considers it to be the same object. Correspondingly, a classification model should favor functions that give consistent output for similar data points [4]. Therefore, aiming to minimize this difference to be consistent is a reasonable goal.\n\nSpecifically, consistency regularization works by reducing the cost loss associated with the manifold surrounding each data point [4]. Finally, guaranteeing consistency for the predictions effectively moves the decision boundaries further from the points with labels and improves the performance of recognition.\n\n[1] Samuli Laine et al., Temporal Ensembling for Semi-Supervised Learning. ICLR, 2017.\n\n[2] Ekin D Cubuk et al., Randaugment: Practical automated data augmentation with a reduced search space. CVPR, 2020.\n\n[3] Zhang, Yifan, et al. \"Deep long-tailed learning: A survey.\" TPAMI, 2023.\n\n[4] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results, NIPS 2017.\n\n```\nW2: Comparison with a well-trained teacher.\n```\n\nTo conduct a fair comparison between the use of a well-trained teacher and the predictions from past epochs as the teacher, we maintain all other settings constant but replace the self-distillation component of IKL with distillation using a well-trained teacher. \n\nTaking SADE as an example, we first train a model utilizing the SADE method to establish a well-trained teacher model. Then, we train a new model using the SADE with the IKL method. During this new model's training phase, we replace IKL\u2019s self-distillation mechanism with distillation from the well-trained teacher (SADE + IKL w/ well-trained teacher).\n\nOur results are summarized in the following table. We conduct experiments on CIFAR100-LT, ImageNet-LT, and iNaturalist.\n\n|   Method    | CIFAR100-LT | ImageNet-LT | iNaturalist |\n|-------------|-------------|-------------|-------------|\n|   RIDE [1] + IKL w/ well-trained teacher         |    48.3         |     57.9        |    72.3         |\n|   RIDE [1] + IKL             |    48.8         |     59.0        |    73.6         |\n|   SADE [2] + IKL w/ well-trained teacher         |    49.0         |     59.1        |    73.3         |\n|   SADE [2] + IKL             |    50.7         |     60.2        |    74.2         |\n\n\nThe results indicate that compared to traditional distillation methods using a well-trained teacher, our approach demonstrates more significant improvements in model performance. Additionally, our method does not require a well-trained model as a teacher, thereby substantially reducing additional computational overhead and resource burdens.\n\n[1]  Wang X, Lian L, Miao Z, et al. Long-tailed Recognition by Routing Diverse Distribution-Aware Experts. ICLR, 2020.\n\n[2] Yifan Zhang, et al. Zhang Self-supervised aggregation of diverse experts for test-agnostic long-tailed recognition. NIPS, 2022"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670205758,
                "cdate": 1700670205758,
                "tmdate": 1700670205758,
                "mdate": 1700670205758,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ckhHaAFQ8i",
                "forum": "SRn2o3ij25",
                "replyto": "3TvBPdb4Bv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zu8m (Part 2)"
                    },
                    "comment": {
                        "value": "```\nW3: Comparison with previous distillation-based methods are missed.\n```\n\nWe compare our method IKL with the following previous distillation-based methods: LFME [1] and NCL [2].\n\nLFME employs self-paced knowledge distillation from multiple models to enhance learning on long-tailed data, while NCL introduces Nested Balanced Online Distillation (NBOD) for effective knowledge transfer among multiple expert networks to tackle long-tailed recognition challenges.\n\nIt is noteworthy that, to conduct a comprehensive comparison with these distillation-based methods, we not only substituted the distillation approaches of LFME and NCL with our own self-distillation method, denoted by an arrow (e.g., NCL->IKL), but we also augmented their distillation frameworks with our self-distillation approach, indicated by a plus sign (e.g., NCL+IKL).\n\n\n|   Method    | CIFAR100-LT | ImageNet-LT | iNaturalist |\n|-------------|-------------|-------------|-------------|\n|   LFME                    |    42.3         |     37.2        |    -         |\n|   LFME -> IKL             |    44.0         |     37.9        |    -         |\n|   LFME + IKL              |    44.8         |     38.9        |    -         |\n|   NCL                    |    54.2         |     59.5        |    74.9         |\n|   NCL -> IKL             |    54.5         |     60.2        |    75.2         |\n|   NCL + IKL              |    56.0         |     61.9        |    76.5         |\n\n\nThe results in the table above demonstrate that replacing the distillation component of these frameworks from LFME and NCL with our self-distillation method leads to improved model performance.\nFurthermore, when our self-distillation method is combined with these methods, the model performance can be further enhanced.\n\n**Reference**\n\n[1] Xiang L, Ding G, Han J. Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification[C]//Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16. Springer International Publishing, 2020: 247-263.\n\n[2] Li J, Tan Z, Wan J, et al. Nested collaborative learning for long-tailed visual recognition[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 6949-6958.\n\n```\nW4: the medium function in Eq. (7).\n```\n\nWe provide an explanation of how to compute the medium function in Eq. (7):\n\nGiven a class $C$ with $n$ samples and $m$ features, represented as a matrix $X$ of size $n \\times m$ and $x_{ij}$ denotes the value of the i-th sample in the j-th feature dimension.\n\n#### Steps to Calculate the class prototype $\\mathcal{f_c}$:\n\n1. **Sorting**: For each feature dimension $j$ (where $j = 1, 2, \\ldots, m$), sort all sample values $x_{ij}$ in ascending order.\n\n2. **Median Calculation**:\n   - If the number of samples $n$ is **odd**, the median for each feature dimension $j$ is the middle value in the sorted list.\n   - If $n$ is **even**, the median for each feature dimension $j$ is the average of the two middle values.\n\n3. **Class Prototype Formation**:\n   - The class prototype $\\mathcal{f_c}$ is a vector of medians for each feature dimension, represented as:   \n   $\\mathcal{f_c}$ = ($C_1$, $C_2$, $\\ldots$, $C_m$)\n   - Where $C_j$ is the median of the $j$-th feature dimension.\n\nIn code, we could use the median function in Pytorch to easily implement this procedure.\n\nIn conclusion, we sincerely appreciate all your valuable feedback to help us improve our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670275920,
                "cdate": 1700670275920,
                "tmdate": 1700675091901,
                "mdate": 1700675091901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bddo7JRzmZ",
                "forum": "SRn2o3ij25",
                "replyto": "3TvBPdb4Bv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer Zu8m,\n\nWe genuinely appreciate any feedback you may have on our revised submission. If there are additional questions or points of clarification needed, we are more than willing to promptly address them.\n\nAgain, we express our gratitude for your time, extensive efforts, and valuable insights. Thank you for the opportunity to engage in this discussion.\n\nBest regards, The Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736516514,
                "cdate": 1700736516514,
                "tmdate": 1700736516514,
                "mdate": 1700736516514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aG2CKAA5EM",
            "forum": "SRn2o3ij25",
            "replyto": "SRn2o3ij25",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework named Implicit Knowledge Learning (IKL) framework to tackle the long-tailed recognition problem. In detail, the IKL framework includes two techniques, so called the Implicit Uncertainty Regularization (IUR) and the Implicit Correlation Labeling (ICL). First, the main idea of IKL is to regularize the predictions of the current epoch using the ones from the previous epoch. It is especially shown to be effective to reduce uncertainty in the tail-class examples. Second, ICL constructs an additional label matrix based on the inter-class similarity, to improve the learning process. This can help to complement the typical one-hot labels. The full framework IKL can serve as a plug-and-play scheme, where it can be attached to existing long-tailed learning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The necessity of IUR technique is quite clear and noticeable, as presented in Figure 1. Such discovery, where uncertainty values especially on the minor classes grow compared to the ones from the major categories is useful. The proposed scheme can properly address the problem. In addition, learning from correlations between different classes is reasonable. \n\n- The proposed framework is practical, since it can be built on existing methods to further improve long-tailed learning. \n\n- Experiments are extensive; it has been tested on more than three or four different types of baseline methods, while demonstrating consistent improvements."
                },
                "weaknesses": {
                    "value": "- The critical downside of the proposed work is about technical novelty. The proposed IKL combines two components IUR and ICL, based on using previous predictions and inter-class correlations respectively, both of which are related to well-established literature. For example, as the elementary deep learning based semi-supervised learning methods, Temporal Ensembling [S. Laine et al., 2016] and MeanTeacher [A. Tarvainen et al., 2017] present the generalized version of IUR, where averaging past model predictions or model parameters to enforce consistency loss term. In that sense, despite the limited novelty, the proposed work needs to discuss the previous works in this direction and experimentally compare with those methods in the long-tailed learning scenario. Similarly, regarding the proposed ICL technique, it is common to learn from the dependencies among different class labels (also in the name of co-occurrence) [Z. M. Chen et al., 2019]. Authors also need to acknowledge previous attempts in this direction and provide sufficient discussions on what component is new. In summary, from the technical aspect, it provides limited innovation compared to previous literature that adopts similar approaches.\n\n- As an additional comment on ICL, the effect for applying ICL is currently unclear. To better understand the proposed component, it would be beneficial to quantitatively measure and qualitatively visualize the class-level dependencies (i.e., correlation matrix) on a specific dataset.\n\n- For calculating the class prototypes C, the verification for the superiority of median features compared to simple averaging is missing. Is there any reference or supporting experiments? In addition, it would be further helpful to provide a brief explanation about how to compute a median of features.\n\n- A proof-reading process, especially for referencing papers, is necessary. A lot of typos can be found, for example, missing a space (i.e., NCLLi et al.) and missing punctuation (i.e., offline process Peng et al.). Additionally, the reference section needs to be updated. For example, the paper \u2018Balanced meta-softmax\u2026\u2019 from Jiawei Ren et al., is presented in NeurIPS 2020, which is written as arXiv in the current version."
                },
                "questions": {
                    "value": "- To further improve reproducibility, it is recommended to provide the set of parameters for each augmentation operation in RandAugment. \n\n- Related to the limitation provided in the conclusion, specific demonstration for the increase of space or memory by saving the previous epoch\u2019s predictions and computing class-wise similarity matrix, depending on the number of total samples and classes, would be helpful for better understanding this limitation.  \n\n- It needs to mention that the progressive scaling of $\\alpha$ is applied from section 3 for better clarity. It is confusing since it firstly appears in the experiment section.  \n\n---\n\n[Summary and guidance to rebuttal]\n\nOverall, the motivation for the proposed IUR and ICL is quite clear in the context of long-tailed learning and it conveys consistent improvements in experiments. However, as those techniques are not totally new, it is necessary to present connections with previous approaches in those directions and to emphasize the novel aspects of the proposed work. \n\n---\n\nReferences\n\n[S. Laine et al., 2016] Temporal ensembling for semi-supervised learning, in ICLR 2016.\n\n[A. Tarvainen et al., 2017] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results, in NIPS 2017. \n\n[Z. M. Chen et al., 2019] Multi-Label Image Recognition with Graph Convolutional Networks, in CVPR 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9210/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9210/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9210/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667913986,
            "cdate": 1698667913986,
            "tmdate": 1699637158873,
            "mdate": 1699637158873,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pfwt6cD4ks",
                "forum": "SRn2o3ij25",
                "replyto": "aG2CKAA5EM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6AeQ (Part 1)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your valuable feedback on our paper and thank you for taking the time to review it. Below, we outline our rebuttal, addressing the specific points you mentioned:\n\n### W1.1 Compare our IUR with the semi-supervised methods, such as Temporal Ensembling and MeanTeacher. \n\nTo clarify the novelty design of our IUR with Temporal Ensembling and MeanTeacher, we illustrate from two aspects: the details of methods, and additional experiments:\n\n1. The details of our IUR.\n\nTo apply a consistency cost between the two predictions and better leverage the implicit knowledge of the long-tailed labeled data, we design the confidence knowledge selection (CKS) module and select the predictions from adjacent epochs.\n\n- The CKS module:\n  \nWith long-tailed **labeled** data, we design the confidence knowledge selection (CKS) module for the consistency process to leverage these labels, ensuring the accuracy of the prediction of the last epoch. In contrast, Temporal Ensembling [1] and MeanTeacher [2] maintain an exponential moving average (EMA) for past model predictions or model parameters, without leveraging the power of labels which may produce inaccurate predictions. If too much weight is given to inaccurate predictions, the cost of inconsistency outweighs that of misclassification, preventing the learning of new information. \n\n- Selecting predictions from adjacent epochs:\n\nDue to a classification model that should favor functions that give consistent output for similar data points [2], our IUR achieves this by reducing the uncertainty of the predictions from one model at two epochs (e.g., t1 and t2) with twice-augmented inputs and this uncertainty is amplified in long-tail data. To ensure that the uncertainty of predictions stems from the input, we utilize the training model which is at minimum variation (e.g., at adjacent epochs). If we utilize EMA, the knowledge gained in the early training would be involved. Due to insufficient training, this knowledge may contain biased knowledge.\n\n\n3. The Experimental Results:\n\n|    Top-1 Acc.     | ImageNet-LT | iNaturalist 2018 |\n|-------------------|----------|------------------|\n| BSCE [3] | 52.3 | 70.6 |\n| + Temporal Ensembling | 51.5 | 69.4 |\n| + MeanTeacher         | 51.8 | 70.1 |\n| + IUR w/o CKS         | 51.7 | 70.2 |\n| + IUR (Ours)          | 54.2 | 72.3 |\n\nWe have also compared our method to semi-supervised consistency prediction methods on ImageNet-LT and iNaturalist 2018. The results demonstrate that, due to Temporal Ensembling and MeanTeacher being designed for unlabeled data, they perform poorly in a long-tailed setting. In contrast, due to the design of CKS and selecting predictions from adjacent epochs, our approach significantly outperforms these semi-supervised methods in long-tailed recognition.\n\nIn summary, we design several novels for IUR to leverage the knowledge of **labeled** long-tail data. In this way, we ensure the consistency of the model's prediction uncertainty in long-tailed data and facilitate the model to learn more abstract invariance [1] to improve the performance of recognition. In the final vision, we will add these related methods to make our approach clearer. \n\n**Reference**\n\n[1] S. Laine et al., 2016, Temporal ensembling for semi-supervised learning, in ICLR 2016.\n\n[2] A. Tarvainen et al., 2017 Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results, in NIPS 2017.\n\n[3] Ren J, Yu C, Ma X, et al. Balanced meta-softmax for long-tailed visual recognition[J]. Advances in neural information processing systems, 2020, 33: 4175-4186."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659938094,
                "cdate": 1700659938094,
                "tmdate": 1700659938094,
                "mdate": 1700659938094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f3KlEFzJAt",
                "forum": "SRn2o3ij25",
                "replyto": "aG2CKAA5EM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6AeQ (Part 2)"
                    },
                    "comment": {
                        "value": "### W1.2 Compare our ICL with co-occurrence methods. \n\nTo illustrate the difference between our ICL and co-occurrence methods, we first review the motivation of our ICL, we then introduce the difference from the definition perspective and methodological perspective.\n\n1. The motivation of our ICL.\n\nTraditional one-hot labels lack the ability to convey information about inter-class similarities. In long-tail learning scenarios, head classes might exhibit features similar to those of tail classes. Relying on one-hot encoding can lead to misclassification of these tail class features as belonging to head classes, exacerbating the model's bias towards head classes. To this end, our Implicit Correlation Labeling (ICL) addresses this issue by introducing correlation labels, enabling the model to learn both the associations and distinctions between classes, thereby mitigating model bias.\n\nWe differentiate our method from the co-occurrence approach [1] from two perspectives:\n\n2. Definition perspective:\n\nOur proposed method fundamentally diverges from co-occurrence-based approaches. The \"co-occurrence\" [1] is designed to model **inter-class dependencies** for *multi-label classification* tasks. For instance, 'bird' is likely to co-occur with 'sky' or 'tree'. In multi-label scenarios, these co-occurring relationships among labels serve as priors, assisting in the recognition of complex patterns and enhancing classification performance.\n\nHowever, in long-tail recognition tasks that focus on single-label classification, the knowledge of inter-class dependency (co-occurrence) is irrelevant. Our ICL approach is specifically developed for *long-tail tasks*, creating an **inter-class feature similarity matrix**. For example, 'cat' is more similar to 'tiger' than to 'dog' in terms of features. This similarity matrix mixes the labels to supervise single categories, ensuring the model reduces misclassified features of tail classes as belonging to head classes due to their similarity, thereby reducing model bias towards these predominant classes.\n\n3. Methodological perspective:\n\nThe co-occurrence method [1] initially utilizes a Graph Convolutional Network (GCN) to model each category's dependencies, combining this dependency vector with standard prediction results for multi-label classification. Conversely, our ICL method directly leverages the features from the previous epochs to compute an inter-class similarity matrix. This matrix is then used as a soft label to supervise the model.\n\nIn summary, compared with the co-occurrence method [1], our ICL method does not introduce additional networks to model inter-class relationships. Instead, it employs the relationship matrix directly as a supervisory signal, simplifying the process and providing direct supervision based on these similarity relationships.\n\n**Reference**\n\n[1] Z. M. Chen et al., 2019, Multi-Label Image Recognition with Graph Convolutional Networks, in CVPR 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660049690,
                "cdate": 1700660049690,
                "tmdate": 1700660049690,
                "mdate": 1700660049690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ak3GzCVKIo",
                "forum": "SRn2o3ij25",
                "replyto": "aG2CKAA5EM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6AeQ (Part 3)"
                    },
                    "comment": {
                        "value": "### W3: For calculating the class prototype C, the verification for the superiority of median features compared to simple averaging is missing.\n\nWhen the model training on the long-tailed dataset, especially large-scale datasets, the previous utilized rand augmentation to increase model generalization. The rand augmentation contains strong augmentation (e.g. color distortion and jigsaw transformation) and may augment an input to be a noisy point [1][2]. So, we obtain the class centers with median values instead of the class mean to avoid the impact of noisy points.\n\nIn addition, we provide a brief explanation of how to compute a median of features:\n\nGiven a class $C$ with $n$ samples and $m$ features, represented as a matrix $X$ of size $n \\times m$ and $x_{ij}$ denotes the value of the i-th sample in the j-th feature dimension.\n\n#### Steps to calculate the class prototype $\\mathcal{f_c}$:\n\n1. **Sorting**: For each feature dimension $j$ (where $j = 1, 2, \\ldots, m$), sort all sample values $x_{ij}$ in ascending order.\n\n2. **Median Calculation**:\n   - If the number of samples $n$ is **odd**, the median for each feature dimension $j$ is the middle value in the sorted list.\n   - If $n$ is **even**, the median for each feature dimension $j$ is the average of the two middle values.\n\n3. **Class Prototype Formation**:\n   - The class prototype $\\mathcal{f_c}$ is a vector of medians for each feature dimension, represented as:   \n   $\\mathcal{f_c}$ = ($C_1$, $C_2$, $\\ldots$, $C_m$)\n   - Where $C_j$ is the median of the $j$-th feature dimension.\n\nIn code, we could use the median function in Pytorch to easily implement this procedure. In the below table, we compare the median feature and mean feature for the class prototype.\n\n|    Top-1 Acc.     | ImageNet-LT | iNaturalist 2018 |\n|-------------------|------------------|------------------|\n| RIDE+Mean  | 57.7 |72.1 |\n| RIDE+Median (Ours) |   58.4 | 72.7 |\n| SADE+Mean  |   59.3 | 73.2 |\n| SADE+Median (Ours) |   60.0 | 73.6 |\n\nThe results demonstrate the effectiveness of the median feature.\n\n**Reference** \n\n[1] Peng X, Wang K, Zhu Z, et al. Crafting better contrastive views for siamese representation learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16031-16040.\n\n[2] Lee H, Lee K, Lee K, et al. Improving transferability of representations via augmentation-aware self-supervision[J]. Advances in Neural Information Processing Systems, 2021, 34: 17710-17722.\n\n[3] Ren J, Yu C, Ma X, et al. Balanced meta-softmax for long-tailed visual recognition[J]. Advances in neural information processing systems, 2020, 33: 4175-4186.\n\n### W4: Typos.\n\nThank you for pointing out the typographical and referencing errors. We will follow all your suggested revisions and conduct a comprehensive review of the entire paper to ensure its accuracy and quality in the final version.\n\n### Q1: Provide Details in RandAugment.\n\nThank you for your recommendation to enhance the reproducibility of our work. \nFor a fair comparison, our parameters follow PaCo [1], which has demonstrated effectiveness in similar contexts.\n\nSpecifically, we set the RandAugment parameters as follows in our experiments:\n\n- Magnitude (m): 10\n- Number of Transformations (n): 3\n- Weight Index (w): 0\n- Magnitude Standard Deviation (mstd): 0.5\n\n**Reference**\n\n[1] Cui, Jiequan, et al. \"Parametric contrastive learning.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n### Q2: Resource consumption.\n\nIn the below table, we outline the resource consumption required due to storing predictions for each epoch and the memory required for the class-wise similarity matrix. We present the data in a table format for the long-tail datasets: CIFAR100-LT, ImageNet-LT, Place-LT, and iNaturalist 2018.\n\n\n|                   | ImageNet-LT | Place-LT | iNaturalist 2018 |\n|-------------------|-------------|----------|------------------|\n| Memory for Previous Epoch  | 0.045 GB | 0.089 GB | 1.31 GB |\n| Memory for Similarity Matrix | 0.004 GB | 0.005 GB | 0.12 GB |\n\nThe above table illustrates the memory requirements for the Previous Epoch Storage and the Similarity Matrix across different datasets. For datasets of normal scale, such as ImageNet-LT and Place-LT, the combined memory usage of both methods does not exceed 1GB, which is generally manageable. However, for larger datasets, like iNaturalist 2018, the Previous Epoch Storage method alone can require significant memory resources, approximately 1.31 GB in this case.\n\nTherefore, the limitation in terms of storage space is primarily evident in large-scale datasets. For small to medium-sized datasets, these methods do not require much additional memory. This distinction highlights the need for careful consideration of memory resources when applying our methodology to large datasets."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660191345,
                "cdate": 1700660191345,
                "tmdate": 1700660191345,
                "mdate": 1700660191345,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BgfvjqO9Af",
                "forum": "SRn2o3ij25",
                "replyto": "aG2CKAA5EM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6AeQ (Part 4)"
                    },
                    "comment": {
                        "value": "### Q3: The progressive scaling of $\\alpha$ is applied from section 3 for better clarity.\n\nIn the initial stages of training, due to the model's limited capability from insufficient training, it is particularly challenging to extract valuable implicit knowledge. Consequently, at this early training phase, the losses derived from the model's previous outputs \u2014 which serve to reduce uncertainty ($L_{IUR}$) and compute class centers ($L_{ICL}$) \u2014 are significantly biased and meaningless. Therefore, their contribution to the overall training process needs to be diminished to prevent the propagation of early inaccuracies. As training progresses and the model becomes more capable, the knowledge extracted becomes increasingly reliable and valuable. In response to this, we wish to increment their contribution, allowing the model to leverage more of this valuable knowledge.\n\nIn light of this, we designed a progressive scaling weight $\\alpha$ to ensure minimal impact on normal training at the outset while increasing influence during the later stages, enabling the model to assimilate more valuable knowledge. The application of $\\alpha$ is detailed in Section 3.4:\n\n$$ L = (1 - \\alpha)L_{LTR} + \\alpha(L_{IUR} + L_{ICL}) $$\n\nFor the selection of $\\alpha$, our intent is for it to grow progressively from the start to the end of training. Based on the experiments detailed in Section 5, we adopted the Parabolic Increment strategy for $\\alpha$'s growth. Specifically:\n\n$$ \\alpha = (T / T_{max})^2 $$\n\nThis approach, as confirmed by our experiments, allows the model to gradually integrate knowledge from previous epochs, effectively balancing the training influence over time.\n\nOverall, we appreciate all your constructive feedback and the opportunity to improve our work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660223349,
                "cdate": 1700660223349,
                "tmdate": 1700675148213,
                "mdate": 1700675148213,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YkJwQV8bR1",
                "forum": "SRn2o3ij25",
                "replyto": "aG2CKAA5EM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' comments"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive feedback, which includes detailed clarifications, discussions, and follow-up experiments. I have carefully checked the discussions with other reviewers as well.\n\n**W1.1 Compare our IUR with the semi-supervised methods, such as Temporal Ensembling and MeanTeacher.**\n\nI appreciate your effort in clarifying the IUR method and providing experimental comparisons with Temporal Ensembling and MeanTeacher. However, IUR appears to be a specific instance of consistency regularization, a well-established concept in semi-supervised learning (SSL). Additionally, the experimental results do not demonstrate a significant improvement of IUR over MeanTeacher. I am also curious about the performance drop observed when consistency regularization terms are applied to BSCE without CKS. While BSCE is not the primary focus in the main paper's experimental section, additional validation, possibly on smaller-scale benchmarks like CIFAR-10/100 using different LT-baseline methods such as RIDE and SADE, would be valuable. Furthermore, the CKS module's concept seems derivative of pseudo-labeling [D. H. Lee, 2013], with the primary distinction being that CKS uses given labels instead of confidence-based filtering. Therefore, the technical novelty and effectiveness of the IUR method remain unclear at current phase.\n\n**W1.2 Compare our ICL with co-occurrence methods.**\n\nThank you for the thorough discussion on the co-occurrence-based method. Upon revisiting the ICL section, I find that leveraging inter-class similarity based on prototypes for loss computation is already explored [O. Atsuro, 2022]. Could you elucidate any novel aspects or differences from existing literature? \n\n**W3: For calculating the class prototype C, the verification for the superiority of median features compared to simple averaging is missing.**\n\nYour clarification and supportive experiments are much appreciated! Including this in the appendix could be beneficial.\n\n---\n\nReferences\n\n\n[D. H. Lee, 2013] \"Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.\" Workshop on challenges in representation learning, ICML. Vol. 3. No. 2. 2013.\n\n[O. Atsuro, 2022] \"Interclass prototype relation for few-shot segmentation.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n---\n\nIn summary, while I value the extensive responses from the authors, there are still unresolved questions regarding the technical contributions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705029714,
                "cdate": 1700705029714,
                "tmdate": 1700705029714,
                "mdate": 1700705029714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pwfWoC3lK4",
                "forum": "SRn2o3ij25",
                "replyto": "aG2CKAA5EM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' comments"
                    },
                    "comment": {
                        "value": "Thank you for your prompt and thorough feedback to my response! \n\nBased on the comments you presented, while simple confidence-based filtering, as an alternative to CKS, can be beneficial, CKS appears to have a more positive effect in the long-tailed learning scenario. To validate this hypothesis, it would be useful to apply CKS to other consistency regularization methods such as MeanTeacher and Temporal Ensembling. As the author-reviewer discussion period is approaching its end, please consider addressing this in the post-review revision phase. In this context, it would be better to mention the observation in the manuscript that the CKS technique is an essential component. \n\nI also appreciate your making a conceptual contrast with the above previous work (IPRM).\n\nIn summary, the proposed work could become more comprehensive and unique if the aspects discussed above (i.e., connections to semi-supervised learning, the effect of CKS compared to confidence-based filtering, and conceptual contrast to the co-occurrence-based method and the similar prototype-based method IPRM) are incorporated into the revised draft.\n\nIf all these aspects are addressed, I would be inclined to recommend a positive score (i.e., 5 -> 6)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738193083,
                "cdate": 1700738193083,
                "tmdate": 1700738193083,
                "mdate": 1700738193083,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]