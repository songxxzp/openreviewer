[
    {
        "title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation"
    },
    {
        "review": {
            "id": "CNmk9yYTjq",
            "forum": "UyNXMqnN3c",
            "replyto": "UyNXMqnN3c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission789/Reviewer_MZY6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission789/Reviewer_MZY6"
            ],
            "content": {
                "summary": {
                    "value": "DreamGaussian is a novel 3D content generation framework designed to efficiently produce high-quality 3D content. The core innovation is a generative 3D Gaussian Splatting model paired with mesh extraction and texture refinement processes in UV space. Unlike the occupancy pruning seen in Neural Radiance Fields, this approach uses progressive densification of 3D Gaussians, which results in faster convergence for 3D generation tasks. The framework also incorporates an algorithm to transform 3D Gaussians into textured meshes and employs a fine-tuning stage for detail refinement. In tests, DreamGaussian was able to produce high-quality textured meshes from a single-view image in just 2 minutes, marking a tenfold speed improvement over existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper's main contribution is changing the NeRF representation to Gaussian Splatting representation in the current text-to-3D or image-to-3D pipeline. This has significant challenges in terms of implementation. Also, the authors' current implementation allows for fast generation of assets, which has significant importance in the 3D field."
                },
                "weaknesses": {
                    "value": "Regarding the note \u201cSimilar to Dreamtime (Huang et al., 2023), we decrease the timestep t linearly\u201d: Did the decrease of t help with the training time as well? Did it bring instability in training to the model? My previous exploration in this direction (for text to 3D in NeRF) showed improvement in the speed of generation for some objects but brought instability in the training. Since some objects needed more training steps than others, having a fixed annealing strategy damaged the performance, specifically for objects (e.g., motorcycles or dogs). It would be great if the authors could explore, in text to 3D, what the effect of the speed of annealing would be for different sets of prompts, (e.g., a corgi vs. a tree). Does the speed of annealing need to be tuned for each prompt?\n\n\nThe authors provided the experiment for time annealing in Fig 7 for image to 3D, but the paper is missing the same figure for text to 3D. Also, the effect of the speed of annealing needs to be explored.\n\nRegarding the loss function, eq. 6, for UV-Space texture refinement: what happens if the object boundary in the Img2Img stage changes? How do the authors prevent the color of the background from leaking into the mesh color?\n\nIt would be great if the authors could compare the proposed texture refinement to other SOTA methods like TexFusion [1] or Text2Tex [2] or any other method of their choice. The reason for this ask is because texture refinement has been applied on top of the mesh. So, if the authors want to consider texture refinement as one of their contributions, they should either compare it with other baselines or reframe the paper and consider this as a side contribution.\n\n\nI also tried experimenting with the code (great codebase!) and observed that, for text to 3D, the texture in most cases was very saturated. It would be great if the authors could comment on this phenomenon in the paper as a shortcoming and provide some insight into which parameter tuning might help.\n\n\nRegarding the Janus problem, the authors provided a list of papers that address the Janus problem mostly using 3D data. However, it would be great if the authors could also reference methods like Perp-Neg [3] or Prompt-Debiasing [4] that address the Janus problem without the need for 3D assets. This has significant importance because they don't introduce bias from 3D data into the pipeline. For instance, they allow for the generation of a dog without the strict square position for standing that comes from the 3D asset.\nIt would be great if the authors could also comment on the guidance of the SDS loss and the effect of increasing those values.\n\nThere are many typos in the paper, and it would be great if the authors could fix them. Examples are: on page 2, \"Gaussian splitting\" appears to be a typo; it should likely be \"Gaussian splatting\". \u201cseverl methods\u201d on page 3 and \u201cDissusion\u201d on page 5 is it supposed to be discussion?\n\nIt would also be great if the authors could comment on how to get the model to consider both the input image and the text prompt simultaneously in both the initial stage and the later mesh/texture optimization stage. At the moment, it seems to ignore the optional text prompt. Was this intentional?\n\nThe paper contrary to DreamFusion does not learn an MLP for the background, it would be great if authors could comment on why they made that choice and what are their findings. \n\n[1] https://openaccess.thecvf.com//content/ICCV2023/papers/Cao_TexFusion_Synthesizing_3D_Textures_with_Text-Guided_Image_Diffusion_Models_ICCV_2023_paper.pdf\n\n[2]https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Text2Tex_Text-driven_Texture_Synthesis_via_Diffusion_Models_ICCV_2023_paper.pdf\n\n[3] https://arxiv.org/abs/2304.04968\n\n[4] https://arxiv.org/abs/2303.15413"
                },
                "questions": {
                    "value": "Please consider the comments in the weaknesses section. I believe the current paper as it presents a great contribution to the field, and by addressing my current comment I am willing to increase my score even more."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725242418,
            "cdate": 1698725242418,
            "tmdate": 1699636006502,
            "mdate": 1699636006502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CgQ5ODwrPJ",
                "forum": "UyNXMqnN3c",
                "replyto": "CNmk9yYTjq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer MZY6"
                    },
                    "comment": {
                        "value": "Thank you for your valuable time and insightful comments! We have tried to address your concerns in the updated manuscript and our rebuttal text:\n\n**Q1: Does the timestep annealing help in text-to-3D too?**\n\nThanks for the advice! Since image-to-3D has a strong reference view image prior, we don't observe obvious instability during training.\n\nFor text-to-3D, we have updated new text-to-3D results using MVDream [1] as the guidance model in the appendix. An ablation study shows that timestep annealing is still helpful in generating a more reasonable shape with the same amount of training iterations. However, we do observe that different input prompts may require different number of training iterations to converge, so a linear annealing may not be the optimal way.\n\n\n\n**Q2: In the refinement stage, what happens if the image-to-image diffusion changes the object boundary? How to prevent the color from background leaking into mesh?**\n\nSince we use a relatively small noise level for image-to-image diffusion, boundary change is not very obvious. Also, since the optimization is performed from multiple camera views, it could be corrected from another view should any color leaking happened in one view.\n\n\n\n**Q3: Comparison of the texture refinement stage with texture generation methods like Texfusion and Text2tex.**\n\nThanks for reminding us! Although we both generate texture on a fixed mesh geometry, there are several differences:\n\n(1) These methods require a text prompt as the input. However, we don't have a prompt in our image-to-3D setting.\n\n(2) We have a coarse texture and we want to enhance the details based on it. These methods focus on texture generation instead of refinement and will ignore the coarse texture, which is not suitable especially for image-to-3D.\n\nWe have updated the paper to discuss these methods.\n\n\n\n**Q4: Text-to-3D often generates over-saturated texture.**\n\nThanks for mentioning this! We have updated the paper and stated this problem as a shortcoming in the limitations.\n\n\n\n**Q5: Missing references on methods addressing Janus problem without using 3D assets.**\n\nThanks for reminding us! We have updated and discussed these methods in the paper.\n\n\n\n**Q6: There are many typos in the paper.**\n\nThanks for correcting us! We have revised and updated the paper to fix these typos.\n\n\n\n**Q7: Why choose not to use a learnable background model?**\n\nThe major reason is that we want to keep the pipeline simple, and we find that using random black or white background is enough for Gaussians to converge. We are not the first to choose this design, as Fantasia3D [2] also adopts a solid background color during optimization. \n\nThe learnable background model is majorly adopted in NeRF-based method. One possible explanation is that NeRF tends to form the background during early optimization, which is undesired since we want NeRF to form the target object. However, we observe that it's less likely for mesh or Gaussian-based method with explicit geometry to form the background, so a learnable background model is unnecessary.\n\n\n\n[1] Shi, Yichun, et al. \"Mvdream: Multi-view diffusion for 3d generation.\" *arXiv preprint arXiv:2308.16512* (2023).\n\n[2] Chen, Rui, et al. \"Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation.\" *arXiv preprint arXiv:2303.13873* (2023).\n\n\n\nWe hope our responses satisfactorily address your queries. Please let us know to address any further concerns impacting your review."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957303287,
                "cdate": 1699957303287,
                "tmdate": 1699957303287,
                "mdate": 1699957303287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QSZBriKqml",
            "forum": "UyNXMqnN3c",
            "replyto": "UyNXMqnN3c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission789/Reviewer_huut"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission789/Reviewer_huut"
            ],
            "content": {
                "summary": {
                    "value": "This paper proopsed a way to combine SDS loss and the recently proposed point-based rendering method GS. The pipeline is composed of 3 stages, 1) gaussian splatting optimization; 2) mesh extraction from point clouds; 3) texture refinement. The first stage is similar to other SDS-based methods which require pretrained 2d image diffusion models. However, the rendering method is switched to gaussian splatting. The second stage is done by applying marching cubes to opacities learned in the first stage. In the last stage, the texture is refined using 2D diffusion models. The full pipeline takes several minutes and we can obtain a mesh with textures. The authors show some results of image-conditioned and text-conditioned generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has many strengths. Thanks to GS, the full pipeline is fast compared to some previous nerf-based methods. The author observed that a longer optimization of SDS does not give better results (sharp and detailed). Thus the focus of the method is not the SDS part. Instead, the authors only optimize the SDS loss with a few iterations (which is also the main reason why it is so fast). After obtaining the blurry 3D object, a refinement step inspired by diffusion-based image editing is applied. In the end, we can have a detailed mesh with textures. Another important aspect of this method, is the mesh extraction algorithm. Extracting a surface from a point cloud is not straightforward. The authors found out a way to use the opacity as the isosurface.\n\nWhen we are generating data, GS seems to be more suitable because of its progressive nature. The paper can be seen as a proof of this claim."
                },
                "weaknesses": {
                    "value": "1. It seems the focus of the paper is image-conditioned generation. The results of text-to-3d are very limited and the comparison is weak.\n2. I am curious about the setup of the stage 3. If we already have a mesh with coarse texture, we can optimize it with differentiable mesh rendering and SDS loss, as Fantasia3d did in the appearance modeling stage. What is the advantage of the proposed refinement compared to this?\n3. Another concurrent ICLR submission ( https://openreview.net/forum?id=pnwh3JspxT ) optimizes SDS much longer (1 hour and 40 minutes) than this paper. However, this paper claims that longer training does not give better results. Can the authors clarify the differences?"
                },
                "questions": {
                    "value": "See weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767725439,
            "cdate": 1698767725439,
            "tmdate": 1699636006421,
            "mdate": 1699636006421,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W72fswk29m",
                "forum": "UyNXMqnN3c",
                "replyto": "QSZBriKqml",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer huut"
                    },
                    "comment": {
                        "value": "Thank you for your valuable time and insightful comments! We have tried to address your concerns in the updated manuscript and our rebuttal text:\n\n**Q1:  The text-to-3D results are limited.**\n\nThanks for your advice! Our paper does focus more on image-to-3D, since text-to-3D generation takes longer computing time and is more prone to suffer from the Jasus problem.\n\nAs stated in the limitations, it's promising to solve the Janus problem with recent multi-view or camera-conditioned 2D diffusion models. We have updated new text-to-3D results using MVDream [1] as the guidance model in the appendix, which shows better results for difficult prompts.\n\n\n\n**Q2: What is the advantage of the proposed refinement stage compared to Fantasia3D?**\n\nThe key advantage of our refinement stage lies in its efficiency.\n\nUnlike other methods, which typically start without initial textures and require considerable time for generation, our approach builds on an existing mesh with coarse textures. For example, the texturing stage in Fantasia3D can take up to 20 minutes using 8 GPUs, whereas our refinement stage enhances the coarse texture in just 1 minute on a single GPU.\n\n\n\n**Q3: Differences compared to GSGEN which optimizes for longer time.**\n\nThere are several different designs that lead to the different conclusions about optimization time:\n\n(1) GSGEN prioritizes high-quality text-to-3D generation, which is more challenging than image-to-3D. To address the Janus problem, they also generate a point cloud using Point-E [2] to initialize the Gaussians.\n\n(2) They introduce a novel compactness-based densification strategy, which is more suitable for longer optimization. \n\n(3) In addition to the 2D SDS loss using Stable-diffusion, GSGEN applies a 3D SDS loss with Point-E [2], potentially prolonging each optimization step.\n\nIn contrast, our method emphasizes efficiency, trading off some level of detail. We also observe that text-to-3D typically requires longer optimization to achieve finer details, especially with an appropriate densification strategy.\n\n\n\n[1] Shi, Yichun, et al. \"Mvdream: Multi-view diffusion for 3d generation.\" *arXiv preprint arXiv:2308.16512* (2023).\n\n[2] Nichol, Alex, et al. \"Point-e: A system for generating 3d point clouds from complex prompts.\" *arXiv preprint arXiv:2212.08751* (2022).\n\n\n\nWe hope our responses satisfactorily address your queries. Please let us know to address any further concerns impacting your review."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957187994,
                "cdate": 1699957187994,
                "tmdate": 1699957187994,
                "mdate": 1699957187994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "58OTkz4dfi",
            "forum": "UyNXMqnN3c",
            "replyto": "UyNXMqnN3c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission789/Reviewer_E7Ru"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission789/Reviewer_E7Ru"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of 3D content generation from text- or single view inputs into 3D gaussian splats which can further be transformed into textured meshes. The proposed method is coined DreamGaussian and revisits the recent ToG 2023 3D Gaussian Splatting paradigm with a generative twist to it.\n\nThe proposed contribution is favorably compared to a comprehensive set of state-of-the-art comparative baselines and in particular is able to produced high fidelity mehses of objects within 2min of compute time, which is remarkable."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ ## Readability.\nAs it currently stands, the paper is very well written. The main ideas and concepts are mostly well explained and articulated throuthout.\n\n+ ## Organization of the contents and overall paper structure.\nThe contents are also very well structured and balanced.\n\n+ ## Overall maturity of the submitted package, which makes it very reasonably within camera ready territory.\n\n+ ## The actual performance of the proposed methodological contribution.\nIn particular, it dramatically cuts down the computation time in the space of optimization-based techniques in the field, by an order of magnitude.\n\n+ ## Related work section and discussion. \nIt is very well structued, articulated and populated with very relevant and up to date references."
                },
                "weaknesses": {
                    "value": "+  ## 1. Missing bits of context information - How much does it cost?\nWhile indicative timings and implementation details (covering experimental setup,  are provided, information regarding the resource usage, model size and complexity are currently underdescribed.\n\nA comparative disclosure of such information covering the main experimental baselines that are considered would help the reader better assess its relative positioning throughout the typical criteria.\n\nMentioning where the computation bottlenecks lie in terms of pipeline components would also be valuable in order to fully assess the practical usefullness of the proposed sequential pipeline, beyond rough timings.\n\n+  ## 2. Challenging the ad hoc meshing post processing.\n\nAs it currently stands, the  mesh extraction technique relies on many subsequent post-processing steps, including mesh decimation and remeshing (end of page 5 in the main paper). I believe the explainations around eq. (4) (before and after) could be further improved and detailed. My current intuition is that the mesh complexity at least could be controled jointly during the density query step. Also, given the lack of statistics given regarding each step (Weakness 1 above), the relative need and ROI to fuse these steps is also hard to assess.\n\nThe current procedure also produces non-manifold and non-watertight meshes with arbitrary complexity.\n\n+  ## 3. Evaluation.\nThe user study (ie, subjective quality analysis) that is presented in the main paper and further detailed in the appendix is a good idea and often overlooked in the field. \n\nNevertheless, its size and statistical informative validity are rather limited as they are based on a \"cohort\" of 40 users and 15 input samples to assess from."
                },
                "questions": {
                    "value": "The main questions I would have cover the aforementioned weaknesses that have been pinpointed. In particular regarding the missing bits of informations.\n\nBesides those remaining grey areas, I would be happy to bump my initial rating were they to be addressed accordingly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission789/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission789/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission789/Reviewer_E7Ru"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698852878439,
            "cdate": 1698852878439,
            "tmdate": 1700952551574,
            "mdate": 1700952551574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uBat4vgtxH",
                "forum": "UyNXMqnN3c",
                "replyto": "58OTkz4dfi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer E7Ru"
                    },
                    "comment": {
                        "value": "Thank you for your valuable time and insightful comments! We have tried to address your concerns in the updated manuscript and our rebuttal text:\n\n**Q1: Missing details about the model size, complexity, computation bottlenecks.**\n\nThanks for reminding us! \n\nThe size of the generated 3D models varies depending on the input and training schedule. Typically, image-to-3D results involve around $10^4$ 3D Gaussians, while text-to-3D can reach up to $10^5$ Gaussians due to a more aggressive densification strategy.\n\nFor the mesh extraction, we apply remeshing and decimation as post-processing so the size is controlled. Specifically, we perform isotropic remeshing to an average edge length of $0.015$, and then decimate the number of faces to $10^5$.\n\nThe primary computational bottleneck in our pipeline remains the forwarding of the 2D diffusion model. We have broken down the time consumption for each operation per iteration as follows:\n\n| Operation | Render 3D Gaussians | Run diffusion model | Loss backward |\n| --------- | ------------------- | ------------------- | ------------- |\n| Time (ms) | 3.8                 | 58.4                | 26.4          |\n\nOur method primarily reduces generation time by requiring fewer iterations to converge (from several thousand to 500 steps). However, each iteration is still constrained by the 2D diffusion model.\n\nFor the mesh extraction, the time consumption for each operation is detailed as:\n\n| Operation | Extract geometry | Unwarp UV | Extract texture |\n| --------- | ---------------- | --------- | --------------- |\n| Time (s)  | 7.9              | 4.6       | 4.0             |\n\n\n\n**Q2: How to control the mesh complexity? How to solve non-watertight/manifold meshes?**\n\nAs mentioned in response to Q1, we control mesh complexity through post-processing, limiting the number of faces to $10^5$ through decimation. These post-processing details have been added to the implementation details.\n\nWe acknowledge the potential for non-manifold or non-watertight meshes in cases of poor Gaussian convergence, but we consider repairing it is less related to the goal of this paper and leave it for future work.\n\n\n\n**Q3: The size of user study is limited.**\n\nThanks for the advice! We have conducted additional surveys with 20 more participants, bringing the total to 60 users. The updated results are included in the paper. Due to time constraints in the rebuttal period, we plan to release the data from our experiments for public comparison to facilitate broader assessment and validation.\n\nWe hope our responses satisfactorily address your queries. Please let us know to address any further concerns impacting your review."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957152499,
                "cdate": 1699957152499,
                "tmdate": 1699957152499,
                "mdate": 1699957152499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5JZaD9MX7p",
            "forum": "UyNXMqnN3c",
            "replyto": "UyNXMqnN3c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission789/Reviewer_L9mE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission789/Reviewer_L9mE"
            ],
            "content": {
                "summary": {
                    "value": "The present work presents a new methodology for generative 3d modelling using a 2d lifting approach. Its main underlying hypothesis is that using 3d Gaussian Splatting with its densification results in much faster convergence compared to using traditional neural radiance fields. On the technical side, this work focuses on two main contributions. First it proposes a mesh extraction technique based on the marching cubes algorithm for the setting of environment representations using 3d gaussians. Second, it proposes a UV-space texture refinement stage for further quality enhancement of the resulting textures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall I think this paper to be interesting and to make relevant contributions. While most of the underlying ideas have already been presented in prior works, their combination for the presented use-case is relevant and results in significant performance improvement for the tasks of image-to-3d and text-to-3d."
                },
                "weaknesses": {
                    "value": "While the work has overall a good contribution, one of my main concern is its writing. First, there are numerous minor language issues such as grammar mistakes and sometimes unnecessarily complicated sentence structure. This can easily be solved by a few rounds of careful proofreading. Second, the work reads more like a paper written for a computer vision conference without providing sufficient background to a broader audience at the ICLR community. While this style is not unprecedented in ML, it makes this work much harder accessible and misses an opportunity. Moreover, for people outside the specific subarea of 3d content creation some of the important implementation details may be missing.\n\nThese concerns range from minor points such as assuming the reader to be familiar with all mentioned vision / graphics concepts such as UV space etc without providing a proper background section. It also involves more complicated points such as the decision to provide some background (e.g. on SDS loss) but only to an extent that is only meaningful for people already familiar with dreamfusion. While in vision, many of these things can be assumed known, it would be useful to the learning community to provide some information here.\n\nAlso, I would rephrase the contribution bullet points to focus stronger on the technical aspects rather than first mentioning the overall framework, then dedicating one point to the actual technical meat and then talking about experimental evaluation."
                },
                "questions": {
                    "value": "* Maybe rephrase the formulation \"to release the potential of optimization-based methods.\" to something like \"to unlock the potential. ...\" or something similar.\n* When writing \"we decrease the timestep t linearly, which is used to weight the random noise \u03f5 added to the rendered RGB image\", it is not fully clear to me how this is performed?\n* The SDS loss formulation is not clear without knowing the sds loss, e.g. the expectation is taken among other variables over p and t. What is the distribution of p and t? \n* Also, the evaluation section should be more explicit / more structured about the evaluation protocol and datasets used."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699244595168,
            "cdate": 1699244595168,
            "tmdate": 1699636006266,
            "mdate": 1699636006266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FnLzdcTSBq",
                "forum": "UyNXMqnN3c",
                "replyto": "5JZaD9MX7p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer L9mE"
                    },
                    "comment": {
                        "value": "Thank you for your valuable time and insightful comments! We have tried to address your concerns in the updated manuscript and our rebuttal text:\n\n**Q1: The writing is poor and misses background details.**\n\nThanks for your advice! The paper has been revised to enhance the writing quality. Additionally, we have included a preliminary section in the appendix to introduce relevant background details including SDS loss and mesh UV mapping.\n\n\n\n**Q2: What is the distribution of p and t in SDS loss?**\n\n$p$ represents the camera pose to render the 3D Gaussians, which is uniformly sampled to orbit the object center. \n\nFor example, for image-to-3D, the camera is sampled with a radius of $2$, a y-axis FOV of $46$ degree, with the azimuth in $[-180, 180] $ degree and the elevation in $[-30, 30]$ degree.\n\n$t$ represents the timestep in denoising diffusion probabilistic model.\n\nEarly works like Dreamfusion [1] samples $t$ from $[0.02, 0.98]$ uniformly during training. We follow Dreamtime [2] to perform an annealing of  $t$ from $0.98$ to $0.02$ during training, which leads to faster convergence as shown in the ablation study (Figure 7).\n\n\n\n**Q3: How to perform the linear timestep decreasing?**\n\nAs answered in Q2, we linearly decrease $t$ from $0.98$ to $0.02$ with iteration step $i$ during training. \n\nThis can be viewed a simplification of the TP-SDS algorithm proposed in Dreamtime [2]. Their findings suggest that larger values of $t$ are crucial for the global structure, whereas smaller values enhance local details. Hence, a decreasing schedule aligns the SDS noise level with NeRF optimization, facilitating a coarse-to-fine generation process.\n\n\n\n**Q4: More details about the evaluation protocol and datasets.**\n\nWe have expanded on our evaluation methodology in section A.1 of the appendix. \n\nGiven the absence of ground truth for 3D generative tasks, assessing generation quality poses a challenge. We follow previous works and use CLIP-similarity for evaluation. This metric calculates the average cosine similarity of CLIP embeddings between the input image and novel views from the generated 3D object. Our evaluation was conducted on a dataset comprising 30 samples.\n\n\n\n[1] Poole, Ben, et al. \"Dreamfusion: Text-to-3d using 2d diffusion.\" *arXiv preprint arXiv:2209.14988* (2022).\n\n[2] Huang, Yukun, et al. \"DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation.\" *arXiv preprint arXiv:2306.12422* (2023).\n\n\n\nWe hope our responses satisfactorily address your queries. Please let us know to address any further concerns impacting your review."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957079702,
                "cdate": 1699957079702,
                "tmdate": 1699957079702,
                "mdate": 1699957079702,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l7yI2gBIBG",
                "forum": "UyNXMqnN3c",
                "replyto": "FnLzdcTSBq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission789/Reviewer_L9mE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission789/Reviewer_L9mE"
                ],
                "content": {
                    "comment": {
                        "value": "> Thanks for your advice! The paper has been revised to enhance the writing quality. Additionally, we have included a preliminary section in the appendix to introduce relevant background details including SDS loss and mesh UV mapping.\n\nThanks, the main text should probably refer somewhere to the new appendix content. Some thoughts on those:\n\n* In the explanation of the SDS loss, mayb mention how $\\partial \\mathbf{x} / \\partial \\Theta$ is computed in practice?\n\n* At the beginning of the UV Mapping section, I would probably write what it is used for, i.e. instead of starting by _\" To project a 2D texture image onto the surface of a 3D mesh, it is essential to map each vertex to a position on the image plane\"_ you could start by _\"UV Mapping is used to project a 2d texture image onto the surface of a 3d mesh. This requires to map each vertex ...\"_"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238710144,
                "cdate": 1700238710144,
                "tmdate": 1700238710144,
                "mdate": 1700238710144,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]