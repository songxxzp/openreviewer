[
    {
        "title": "Beyond Disentanglement: On the Orthogonality of Learned Representations"
    },
    {
        "review": {
            "id": "iuAZUoGBQr",
            "forum": "jUNSBetmAo",
            "replyto": "jUNSBetmAo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9436/Reviewer_1Jck"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9436/Reviewer_1Jck"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel metric for evaluating disentanglement of learned representations. The method consists of training a Linear Neural Network, essentially an MLP without nonlinearities with decreasing dimensionalities, for each ground truth factor. The objective function involves training (potentially non-linear) predictor heads on top of each hidden layer. Using QR decomposition on the learned NN weights together with loss estimates from each predictor, the authors estimate basis vectors for each ground-truth factor, together with their importance weightings. By computing such vectors for each g.t. factor they estimate both the subspaces in the learned latent space and compute a measure of orthogonality between subspaces for dfiferent g.t. factors.\n\nUsefulness of the metric is evaluated on both synthetic and real data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- IWO can be used in scenarios where a ground truth factor can be aligned with exactly one latent dimension\n- the proposed metric actually correlates with downstream task performance"
                },
                "weaknesses": {
                    "value": "- only 2 datasets and models (as compared to e.g., Locatello et al. 2019) are compared in Section 4.3. Please consider using all the 7 datasets from *disentanglement_lib,* otherwise the choice seems a bit arbitrary\n- Figure 1 is quite difficult to grasp. I understand that the concept is not trivial to present (and the caption is already lengthy), but maybe you could consider extending/rewriting the caption to make it clearer? Perhaps in a step wise manner (multiple figures). I find it crucial for conveying the idea of your paper. If you lack space I believe figure 3 could be compressed/removed instead"
                },
                "questions": {
                    "value": "The sub-optimal performance of Explicitness for the perfectly disentangled case could stem from overfitting. How do the authors handle this problem with their metric? What were the train/test splits used for the experiments? How sensitive is the metric to smaller sample sizes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9436/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9436/Reviewer_1Jck",
                        "ICLR.cc/2024/Conference/Submission9436/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697716026893,
            "cdate": 1697716026893,
            "tmdate": 1700222044771,
            "mdate": 1700222044771,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "khg2PHZO5M",
                "forum": "jUNSBetmAo",
                "replyto": "iuAZUoGBQr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback. We have taken your comments into serious consideration and have made revisions to our manuscript accordingly.\n\n- **W1: Expansion of Dataset Usage**\n    - **Inclusion of SmallNorb Dataset**: In response to your suggestion, we have incorporated the SmallNorb dataset into our study. This dataset offers a significant variance from Dsprites and Cars3D, providing a more comprehensive evaluation of our metric.\n   - **Planned Inclusion of DSprites Variants**: For the camera-ready version of our paper, we plan to introduce three further variations of the DSprites dataset available from disentanglement_lib. This addition will further broaden the scope of our evaluation and align with the diversity presented in Locatello et al. 2019. Note that Shapes3D is unfortunately not readily available from disentanglement_lib. \n\n- **W2: Clarity of Figure 1**\n    - **Revision of Figure 1**: Following the feedback from multiple reviewers, we have replaced the original Figure 1 with a more straightforward representation in the main text, moving the original figure to the appendix. This change aims to simplify the initial understanding of our concept.\n    - **Enhanced Caption and Explanation**: As you suggested, we have elaborated on the figure's caption and provided a more detailed step-wise explanation. \n\n**Responses to Specific Questions:**\n\n- **Q1 (Handling of Overfitting)**: To mitigate overfitting, we have employed a validation set, separate from our test set, for early stopping during model training. We have added this and further training details in *Appendix B: Experimental Details*\n- **Q2 (Train/Test Splits)**: For our experiments, we reserved 15% of each dataset for testing purposes. We then used the test set for calculating the importances for the IWO metric, ensuring that our evaluations are based on unseen data. Note that the subspace basis are part of the model and learned during training. \n- **Q3 (Metric Sensitivity to Sample Sizes)**: Recognizing the importance of this concern, we have added a dedicated experiment in *Appendix B.4: IWO on Limited Data*. We investigates the sensitivity of our metric to smaller sample sizes, providing valuable insights into its robustness and reliability in varied data conditions.\n\nWe appreciate the opportunity to improve our paper based on your feedback. We hope these revisions address your concerns comprehensively."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182501612,
                "cdate": 1700182501612,
                "tmdate": 1700182501612,
                "mdate": 1700182501612,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "04xVHQDGTF",
                "forum": "jUNSBetmAo",
                "replyto": "khg2PHZO5M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Reviewer_1Jck"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Reviewer_1Jck"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for updating the manuscript and the clarifications.\nI have two more comments:\n\n**W1: Expansion of Dataset Usage:**\nShapes3D is available in disentanglement_lib, you just need to download the dataset from https://github.com/google-deepmind/3d-shapes, but the code to use it is available in the library: https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/data/ground_truth/shapes3d.py\n\n**Q3 (Metric Sensitivity to Sample Sizes)**: is there a reason why experiments on smaller datasets are missing for the baseline methods? will you include these in the camera-ready version?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218835393,
                "cdate": 1700218835393,
                "tmdate": 1700218835393,
                "mdate": 1700218835393,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D4hOWNhJtS",
                "forum": "jUNSBetmAo",
                "replyto": "gicRNi3czF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Reviewer_1Jck"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Reviewer_1Jck"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you again for the clarification. I will update the score accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222034879,
                "cdate": 1700222034879,
                "tmdate": 1700222034879,
                "mdate": 1700222034879,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mL9qiDWUcJ",
            "forum": "jUNSBetmAo",
            "replyto": "jUNSBetmAo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9436/Reviewer_ccpN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9436/Reviewer_ccpN"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors first propose Latent Orthogonal Analysis(LOA), a method that can identify latent subspaces for different factors of variation from data. To estimate the mutual orthogonality between subspaces learned with LOA, they then propose importance-weighted orthogonality (IWO), a metric that can do the measurement on disentanglement by investigating the magnitude of the projections from different subspaces onto each other. This is achieved by multiplying the basis matric of one subspace with a diagonal matrix that defines the importance of each dimension w.r.t. the other subspace.\n\nThey empirically evaluate IWO on multiple datasets that are commonly used in disentangled representation learning, and they show that their metric that can outperform prior metrics such as MIG or DCI-D."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-structured and clearly written. It is easy to understand what problem they try to tackle in this paper. Even though the metric study on disentangled representation learning is not a completely new field, I believe it is still worth thinking of how we evaluate the orthogonality between different subspaces that encode different factors of variation. \nIn their methodology, the authors provide detailed and sound math derivation on their LOA and IWO approach."
                },
                "weaknesses": {
                    "value": "My main concern is about the insufficiency of evaluation. Give that $\\beta$-TCVAE was a few years ago and there have been a large number of variants of VAEs that do disentangled representations, I would hope that the authors can implement a few more models for comparison. In addition, there are also very commonly used datasets that were not considered here, e.g. CelebA, Shape3D, Clevr, etc. I would like to see results on these more complex data."
                },
                "questions": {
                    "value": "1. I wonder why the $\\Delta$ L can be used to measure the importance. Could you justify it in more detail?\n2. Is the reason that you choose to only apply linear projection using $W_{1:L}$ is technical difficulty or indeed conceptual purpose?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9436/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9436/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9436/Reviewer_ccpN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709831853,
            "cdate": 1698709831853,
            "tmdate": 1699637188487,
            "mdate": 1699637188487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AU50vRFfNT",
                "forum": "jUNSBetmAo",
                "replyto": "mL9qiDWUcJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback. We have taken your comments into serious consideration and have made revisions to our manuscript accordingly.\n\n- **Expanded Model and Dataset Evaluation:**\nAddition of SmallNORB and Further Models: In direct response to the reviews, we have expanded our evaluation to include the SmallNORB dataset. This addition, along with the inclusion of two more models - Factor VAE and $\\beta$-VAE - enriches our analysis and addresses the diversity of scenarios you highlighted. These new elements complement our existing use of Dsprites, Cars3D, Annealed VAE, and $\\beta$-TCVAE. In the camera-ready version of the paper, we plan to introduce three further variations of the DSprites dataset provided by disentanglement_lib.\n- **Rationale for Dataset and Model Selection:**\n    - **Consistency with** ***disentanglement_lib***: Our decision to use the Dsprites, Cars3D, SmallNORB datasets, and the selected models is grounded in their presence within *disentanglement_lib* https://github.com/google-research/disentanglement_lib Locatello et al., 2019 [1]. This library is a benchmark in disentanglement research and offers unparalleled reproducibility and comparability. Utilizing these datasets and models minimizes biases that could arise from custom implementations or training procedures, thus providing a clear and objective evaluation of our proposed metric's efficacy.\n    - **Advantages of Established Benchmarks:**  Employing resources from *disentanglement_lib* ensures that our study aligns with prevailing research standards, enhancing the reliability and validity of our findings. This approach also allows our results to be directly comparable with a broad spectrum of existing studies, which is crucial for contextualizing and validating our contributions. \n\n**Responses to Questions:**\n\n- **Q1 (Justification of $\\Delta \\mathcal{L}$ as an Importance Measure)**: $\\Delta \\mathcal{L}_l$ measures how much the expected loss of regressing a generative factor increases when regressing from a subspace with dimension $l$ vs. $l-1$. It is therefore directly linked to the dimension which lies in the null-space of the projection from $l$ to $l-1$. As such, we use $\\Delta \\mathcal{L}_l$ to allocate the importance which that particular dimension (which is lost in the projection) plays for the generative factor in question.\n In light of your feedback, we have refined the explanation in our manuscript. Additionally we have adjusted the importance-weighting mechanism in IWO to better reflect the variation in overall model informativeness, following the insights from Eastwood et al., 2018 [1].\n\n- **Q2 (Use of Linear Projections)**: Our choice to use linear projections was driven by both conceptual and technical considerations. Conceptually, linear projections are a natural fit for assessing orthogonality in latent spaces. Technically, their efficient implementation via Linear Neural Networks (LNNs) further motivated our choice. \n\nWe appreciate the opportunity to improve our paper based on your feedback. We hope these revisions address your concerns comprehensively. \n\n[1] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of dis- entangled representations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenRe- view.net, 2018."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700181417361,
                "cdate": 1700181417361,
                "tmdate": 1700181560129,
                "mdate": 1700181560129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b6U87ke28d",
                "forum": "jUNSBetmAo",
                "replyto": "mL9qiDWUcJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the discussion period nears its conclusion, we wish to ensure that all your concerns have been adequately addressed. We would greatly appreciate any additional feedback you may have. Our goal is to finalize the rebuttal with the confidence that our manuscript reflects the valuable feedback provided by the review committee.\n\nThank you for your time and consideration,\n\nthe Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583553516,
                "cdate": 1700583553516,
                "tmdate": 1700583553516,
                "mdate": 1700583553516,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IUXNGSRcAk",
            "forum": "jUNSBetmAo",
            "replyto": "jUNSBetmAo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9436/Reviewer_J4X7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9436/Reviewer_J4X7"
            ],
            "content": {
                "summary": {
                    "value": "A new assessment scheme is introduced to measure disentanglement in latent space in terms of orthogonality between subspaces. The assessment builds on an decomposition methodology which projects the original latent encodings into incrementally smaller subspaces through linear neural models. The empirical analysis validates the proposed assessment scheme against existing disentanglement metrics on synthetic and benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1) Motivations behind the paper are solid: too strict definitions of disentanglement as projection into single orthogonal dimensions are bound to fail in realistic settings. The idea of broadening the definition to orthogonal subspaces, while not being completely novel, is developed here through an approach which is original.\n\nS2) The technical contribution seems also solid, modulo some points which are not made entirely clear in the presentation. However, the overall methodology is convincing from the perspective of correctness and adequacy of the technical solutions. \n\nS3) The paper is well organized and mostly of good presentation quality."
                },
                "weaknesses": {
                    "value": "W1) While presentation quality is generally adequate, the paper misses to convey all the necessary details to facilitate reproduction of the method and of the study. This lack of technical detail in the main body is not compensated by the availability of appendices, supplementary materials or code. One key aspect that is unclear to me is how one is expected to identify the generative factors set $z_1,\\dots, z_K$ and how such $K$ is determined in general. The method involves training a potentially large amount of regressors and little information is provided on how this is done in practice (e.g. how much should the training be pushed in terms of regression error? What are the stopping conditions? How are the linear model initialised?).\n\nW2) The positioning with respect to the literature is on the weak end. The paper misses to discussion and cite works formalising weaker forms disentanglement [A, B, C]. In particular, it would seem relevant to discuss the relationship between the proposed approach and those building on (and measuring) linear symmetry-based disentanglement [B,D].\n\nW3) While the experiments are generally well-designed, the evidence they provide does not seem enough to support the major claims of this paper. As long as one departs from the ideal setting, it is difficult to assess the added value of IWO over DCI and MIG. Additional experiments are needed on more challenging datasets, such as ModelNet40 and COIL-100, possibly enlarging the scope of methods to compare with by including those in [B,D]. It would also be of help to qualitatively explore the impact of the proposed methodology, e.g. by exploring the effects of manipulating the representations over the relevant subspaces \u201csuggested\u201d by the metrics. \n\nW4) The proposed methodology seems very computationally involved. I am using the word \u201cseems\u201d as the paper lacks a comparative assessment of the cost of the method. This should be done while considering more realistically sized problems, involving latent spaces of non-trivial size.\n\n[A] https://proceedings.neurips.cc//paper/2020/file/9a02387b02ce7de2dac4b925892f68fb-Paper.pdf\n\n[B] https://proceedings.mlr.press/v162/tonnaer22a/tonnaer22a.pdf\n\n[C] https://doi.org/10.1109/IJCNN55064.2022.9892093\n\n[D] https://arxiv.org/pdf/2011.13306.pdf"
                },
                "questions": {
                    "value": "Q1) Can the Authors please clarify how generative factors  $z_k$ are selected for the purpose of implementing the method in general (see W1)?\n\nQ2)  Can the Authors please discuss the relationship with linear symmetry-based metrics?\n\nQ3)  The empirical analysis would be substantially strengthened by adding new experiments on as ModelNet40 and COIL-100, considering also computational costs? \n\nQ4) I am a bit puzzled by the negative correlation values in the experiments: is this classical linear correlation? Because some methods seem to be highly negatively correlated (which is still somehow a form of correlation)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769440263,
            "cdate": 1698769440263,
            "tmdate": 1699637188328,
            "mdate": 1699637188328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W7QLDTPeGM",
                "forum": "jUNSBetmAo",
                "replyto": "IUXNGSRcAk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and valuable insights. We have carefully considered each of your points and made significant revisions to our paper to address them.\n\n- **W1: Enhancing Reproducibility and Methodological Clarity**\n    - **Appendix and Implementation Details:** We have created a detailed appendix providing comprehensive information about implementation and model choices. This includes specifics on the initialization of linear layers, network training procedures, and hyper-parameter optimization strategies.\n    - **Code Repository**: To further facilitate reproducibility, we have established an anonymous code repository that contains all necessary code and documentation: https://anonymous.4open.science/r/iwo-E0C6/README.md \n- **W2: Expanding the Literature Review**\n    - **Weaker forms of Disentanglement**: We have integrated a thorough discussion on linear symmetry-based disentanglement into our related work section. This includes an analysis of the works you mentioned and others in the field, emphasizing the relationship between our approach and these methods. We have also included a segment on causal representation learning, particularly focusing on how it intersects with and differs from disentanglement methodologies, including ours.\n- **W3: Strengthening the Experimental Section**\n    - **SmallNORB Inclusion**: We agree with your point on the need for more challenging datasets. While ModelNet40 and COIL-100 are indeed valuable, we have chosen to incorporate the SmallNORB dataset beside DSprites and Cars3D. It offers comparable complexity, with diverse variations in object types, camera angles, and lighting conditions, aligning closely with the complexity of ModelNet40 and the natural imagery of COIL-100, while being part of *disentanglement_lib* https://github.com/google-research/disentanglement_lib/tree/master by Locatello et al., 2019 [1] which offers unparalleled reproducibility and comparability. Utilizing datasets and models from *disentanglement_lib* also minimizes biases that could arise from custom implementations or training procedures, thus providing a clear and objective evaluation of our proposed metric's efficacy. In the camera-ready version of the paper, we plan to introduce three further variations of the DSprites dataset provided by disentanglement_lib.\n    - **Additional Downstream Task Experiment:** We have added an experiment involving a downstream task which demonstrates the interplay between subspace structures and IWO, offering a deeper insight into the practical implications of our methodology. See *Appendix B.3: Modulation Task.*\n- **W4: Addressing Computational Costs**\n    - **Comparative Assessment**: We acknowledge the importance of understanding the computational cost of our method. We have added a section that showcases the computational requirements of our approach for 60 runs on the Small Norbs dataset. We have also added the necessary strategies to deal with high dimensional latent spaces, such as larger reductions between consecutive LNN heads and an initial LNN-layer of smaller size than the latent space.  See *Appendix C: Computational Resources Analysis*.\n\n- **Q1 (Generative Factor Identification)**: We closely align our approach with the current existing literature, using all generative factors provided by the datasets. In Appendix B.1: Datasets, all generative factors used in our analysis are listed.\n- **Q2 (Linear Symmetry-Based Metrics Relationship)**: In our expanded literature review, we discuss the relationship between our approach and linear symmetry-based metrics, delineating both the commonalities and distinctions.\n- **Q3 (Empirical Analysis Enhancement)**: We have addressed this in W3, as discussed above. The addition of the SmallNORB dataset, additional models, an analysis on reduced data (*Appendix B.4: IWO on limited data*) and the new downstream task experiment significantly bolster our empirical analysis.\n- **Q4 (Negative Correlation Values)**: The negative correlation values were also observed by  Locatello et al., 2019 [1] in their benchmark study. Indeed, both downstream task performance and disentanglement metric are taken from their disentanglement_lib repository. As to the correlations, they are calculated by averaging both metric and downstream task performance over all seeds of each hyperparameter (the hyperparameter being the model's regularization strength). The correlation therefore describes how down stream task performance and metrics correlate over the different regularization strengths. We use this numpy implementation for the computation: https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html\n\nWe appreciate the opportunity to improve our paper based on your feedback. We hope these revisions address your concerns comprehensively. \n\n[1] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R\u00e4tsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, Olivier Bachem. ICML (Best Paper Award), 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178917157,
                "cdate": 1700178917157,
                "tmdate": 1700178917157,
                "mdate": 1700178917157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RgXzfQuHZ1",
                "forum": "jUNSBetmAo",
                "replyto": "IUXNGSRcAk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the discussion period nears its conclusion, we wish to ensure that all your concerns have been adequately addressed. We would greatly appreciate any additional feedback you may have. Our goal is to finalize the rebuttal with the confidence that our manuscript reflects the valuable feedback provided by the review committee.\n\nThank you for your time and consideration,\n\nthe Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583520484,
                "cdate": 1700583520484,
                "tmdate": 1700583520484,
                "mdate": 1700583520484,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9VcIFJ8W7n",
                "forum": "jUNSBetmAo",
                "replyto": "W7QLDTPeGM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Reviewer_J4X7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Reviewer_J4X7"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Dear Authors, many thanks for having clarified the experimental settings and requirements of your model. I appreciate the extended literature analysis (though it would have been nice to see also the newly referenced methods compared empirically) and the improved transparency and reproducibility. Addition of SmallNORD is ok, as it is the computational analysis: the approach does not seem as computationally involved as I thought. However, it would have been good to measure the baseline computational cost also of the related models to have a means of comparison. \nThere are two aspects which still seem a limitation of the work: i) one needs to know in advance the generative factors and the method does not allow to discover those; ii) importance scores, for some reasons, should be computed on test data. The latter aspect is a bit puzzling as I was expecting that these can be computed, at least, on validation. Both issues, somewhat limit the generality and usefulness of the approach.\nNevertheless I will take the new information into consideration in the review internal discussion and update my score accordingly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644210789,
                "cdate": 1700644210789,
                "tmdate": 1700644210789,
                "mdate": 1700644210789,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vsZr5WXowZ",
            "forum": "jUNSBetmAo",
            "replyto": "jUNSBetmAo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9436/Reviewer_Voxk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9436/Reviewer_Voxk"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the question of evaluating the quality of disentangled representations with respect to the orthogonality of factors. The authors propose latent orthogonal analysis used to devise a new metric called importance weighted orthogonality. The method is evaluated on several datasets and shows promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The research question is strong and to the best of my knowledge this problem is still open at large, and so any advancement on this front is highly important. Another strength is the relative simplicity of the approach, involving basic neural networks and standard linear algebra operations. The results are also compelling, although somewhat basic, in my opinion."
                },
                "weaknesses": {
                    "value": "The main weakness of this submission is the clarity of exposition. In particular, Sections 2 and 3 could be improved significantly. For instance, the illustration in Fig. 1 is unclear. I believe the authors could do better by considering a 2D case instead of 3D, minimizing the use of colors and angles in the figure. Further, several crucial algorithmic components are described in a minimal fashion with supporting equations,illustrations, or pseudo-code. For example, the text above Eq. (2) and the text above Eq. (4). Given that the proposed method does not seem to be overly complex, I find it disappointing that its description is somewhat vague.\n\nAnother weakness is the evaluation section. Evaluating disentangled factors is a long-standing problem in representation learning. In particular, there are established benchmarks and papers focused on this particular problem. While I am not an expert on this issue specifically, I would assume that suggesting a new metric that is arguably better than others should be motivated better and empirically justified with more than two real-world datasets and a few toy examples."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819260834,
            "cdate": 1698819260834,
            "tmdate": 1699637188206,
            "mdate": 1699637188206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qQy7woKgzt",
                "forum": "jUNSBetmAo",
                "replyto": "vsZr5WXowZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and constructive feedback. We have made several improvements to our manuscript based on your comments.\n\n- **Figure 1 Clarity and Simplification**\nIn response to your suggestion, we have revised Figure 1 to present a 2D case, which simplifies the concept and minimizes the use of colors and angles. The figure now describes the difference between disentangled and orthogonal representations, in terms of DCI-D and IWO. As for the comparison with the Explicitness, acknowledging the original importance of the figure, as highlighted by Reviewer [1Jck], we have retained it in the appendix with an expanded caption for a more in-depth understanding. \n\n- **Enhanced Algorithmic Description**\nWe have thoroughly revised Sections 2 and 3 to provide a clearer exposition of our methodology. This includes a more logical structure in presenting our formulas, where we first elaborate on the concept and purpose of e.g. IWO, before delving into its algebraic representation. This approach ensures a more intuitive understanding of our method and its theoretical underpinnings.\n\n- **Comprehensive Evaluation Section**\nTo address your concerns regarding the evaluation of disentangled factors, we have expanded our experimental section to align closely with the benchmark established by Locatello et al., 2019 [1], recognized as a gold standard in disentanglement research. We now include analyses on dSprites, SmallNORB, and Cars3D datasets. In the camera-ready version of the paper, we plan to introduce three further variations of the dSprites dataset. This expansion not only adheres to established benchmarks but also showcases the versatility and robustness of our proposed metric across a broader spectrum of scenarios.\n\nWe hope these revisions address your concerns comprehensively and enhance the overall quality and clarity of our work.\n\n[1] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R\u00e4tsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, Olivier Bachem. ICML (Best Paper Award), 2019."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177664347,
                "cdate": 1700177664347,
                "tmdate": 1700181738566,
                "mdate": 1700181738566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CXIgqjmqO9",
                "forum": "jUNSBetmAo",
                "replyto": "vsZr5WXowZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the discussion period nears its conclusion, we wish to ensure that all your concerns have been adequately addressed. We would greatly appreciate any additional feedback you may have. Our goal is to finalize the rebuttal with the confidence that our manuscript reflects the valuable feedback provided by the review committee.\n\nThank you for your time and consideration,\n\nthe Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583499015,
                "cdate": 1700583499015,
                "tmdate": 1700583499015,
                "mdate": 1700583499015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "erEc9Bh1ZV",
                "forum": "jUNSBetmAo",
                "replyto": "CXIgqjmqO9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9436/Reviewer_Voxk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9436/Reviewer_Voxk"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors, \n\nI would like to thank you for your response. I also went over the revised version. I still feel that my main concerns (poor exposition and basic evaluation) are not fully addressed, unfortunately. Nevertheless, I will take into account your rebuttal and revised manuscript during the reviewers' discussion."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650672709,
                "cdate": 1700650672709,
                "tmdate": 1700650672709,
                "mdate": 1700650672709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]