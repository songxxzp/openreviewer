[
    {
        "title": "ReLoRA: High-Rank Training Through Low-Rank Updates"
    },
    {
        "review": {
            "id": "YB7MDWQco6",
            "forum": "DLJznSp6X3",
            "replyto": "DLJznSp6X3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4146/Reviewer_xXqo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4146/Reviewer_xXqo"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose ReLoRA, a parameter-efficient method that can be applied to both fine-tuning and pre-training. ReLoRA utilizes low-rank updates that aggregate to train high-rank networks. Experiments show that ReLoRA can speed up the training process and kind of reduce the memory consumption."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "!. The orientation is good. With several existing PEFT methods, it's important to implement parameter-efficient methods to improve the pre-training process. \n2. Extensive experiments are done. Several models are used and various ablation studies are included.\n3. The method is relatively simple and effective, with some ingeniously designed tricks."
                },
                "weaknesses": {
                    "value": "1. The writing is not good. Figure 1 lacks notes on Loss. The method part is quite confusing. Warm start isn't even mentioned except in Algorithm 1. In 3.1 \"Architecture and training hyperparameters\", it seems that a hand-made architecture is designed and used, while in \"Scaling up to 1.3B\", BERT is mentioned, so what exactly is the architecture? In \"trained on 8\u00d7A100GPUs (or more)\", this \"or more\" seems the experiment details are not so clear.\n2. The method includes warm start at first, which means the memory consumption of ReLoRA is equal or close to full training at first. One has to afford the huge memory consumption at first, so the memory problem has not been solved. As shown in the results, full training outperforms ReLoRA on almost all tasks (or average), so as far as I'm concerned, ReLoRA is just sacrificing the overall performance for speeding up. Also, it seems unfair if comparing with LoRA for pre-training, because LoRA isn't designed for pre-training. However, in Table 2, results of LoRA and ReLoRA are quite close, so I doubt the efficiency of ReLoRA. Directly using LoRA+warmstart can be a simpler choice.\n3. It seems ReLoRA introduces a lot of new hyperparameters, making the param tuning more complex than regular training or LoRA."
                },
                "questions": {
                    "value": "1. In Figure 1, what's Loss for, training of validation? Why can Loss demonstrate \"similar performance\"?\n2. In Table 3, why isn't full fine-tuning included?\n3. For the Control Baseline, how are the trainable parameters chosen?\n4. ReLoRA outperforms LoRA for fine-tuning can be due to the Warm start?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4146/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4146/Reviewer_xXqo",
                        "ICLR.cc/2024/Conference/Submission4146/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641352546,
            "cdate": 1698641352546,
            "tmdate": 1700320412192,
            "mdate": 1700320412192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "avQpSkkLeR",
                "forum": "DLJznSp6X3",
                "replyto": "YB7MDWQco6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging the contributions of our study, and for recognizing that we propose a simple and effective technique for a much-needed application of parameter-efficient methods to pre-training. We also appreciate you highlighting our extensive experiments and ablation studies performed to verify and evaluate our method. \n\nOverall, we believe the positive aspects highlighted in your review outweigh the negatives, and we respectfully ask that you reconsider your assessment.  We hope that the clarity of writing, which can be easily improved in the final camera-ready version, can be seen as a minor issue. We address your remaining concerns and provide some clarifications in the comments below.\n\n### Architecture, and experimental setup\n\nAs we mention in Section 3.1, the model architecture we use for all experiments is the LLaMA architecture. Thank you for pointing out the confusing part in the section \u201cScaling up to 1.3B\u201d. We will make sure to clarify that we are only comparing our models to BERT-base and BERT-Large in size, not in the architecture.\n\n\u201c8xA100GPUs (or more)\u201d \u2013 in some of the experiments with the 1.3B model, we had to use more than one A100 node to speed up the experimentation. However, all efficiency comparisons (throughput and memory consumption estimates) were made in the 8xA100 setup. We will make sure to clarify this.\n\n### Warm start\n\nWhile we\u2019re not gaining any improvements for the initial model warm start, it is short (~25%), so we are saving significantly on compute in total. Please refer to this table for the concrete speedups for the 1.3B model:\n\n|                        | 8xA100         | 6xA6000 (Ada) | 2x3090       |\n|------------------------|----------------|---------------|--------------|\n| Full-rank throughput   | 137 ex/sec     | 84 ex/sec     | 8.8 ex/sec   |\n| ReLoRA throughput      | 157 ex/sec     | 124 ex/sec    | 17.8 ex/sec  |\n| Immediate speedup      | 15%            | 48%           | 102%         |\n| Warm-start adjusted ReLoRA throughput (33% warm-start) | 149 ex/sec | 111 ex/sec | 14.8 ex/sec |\n| Total speedup          | 9%             | 32%           | 51%          |\n\nAlso, warmup may not be necessary in some applications. For example, in continued pre-training, where an existing model (e.g., LLaMA-7B) is trained on additional large-scale data or tuned for a specific narrow domain (e.g. medical), a warmup is not required, since the model is already pre-trained. With ReLoRA, this leads to immediate speedups of 15% for A100 and 48% for A6000.\n\n### Is ReLoRA sacrificing the overall performance for speed up?\n\nWe believe that achieving efficiency gains often requires small compromises in performance. This is evident in techniques such as distillation [6], quantization [7], approximate attention [8,9], and PEFT [10,11]. Notably, ReLoRA consistently surpasses LoRA in all tested scenarios. The performance gap between ReLoRA and LoRA widens with increasing model size, ranging from just 0.27 perplexity points in a 60M model to a significant 0.96 in a 1.3B model (more examples in the table below). We think this result by itself is impactful for the ML community where LoRA is used for compute- and data-heavy tasks like RHLF [12].\n\n|                    | 60M    | 130M   | 250M   | 350M   | 1.3B   |\n|--------------------|--------|--------|--------|--------|--------|\n| LoRA + Warm Start  | 34.73 | 25.46 | 22.86 | 19.73 | 18.23 |\n| ReLoRA             | 34.46 | 25.04 | 22.48 | 19.32 | 17.27 |\n| \u0394                   | 0.27  | 0.42  | 0.38  | 0.41  | 0.96  |\n\n\n### Additional hyperparameters\n\nWe address the issue of additional hyperparameters in Appendix A. Specifically, we find that ReLoRA is not very sensitive to rank r and the optimizer pruning percentage. Default values (128 and 0.9, respectively) work great in all scenarios. We tested several ReLoRA reset rates with 350M and 1.3B models and found that 2K iterations reset rate performed consistently well in both pre-training and fine-tuning experiments and always led to better performance than no resets. While additional hyperparameters can be annoying, it\u2019s not uncommon for deep learning methods to introduce new ones. E.g, in case of Adam, beta1 and beta2 do not exist in regular SGD, yet Adam consistently outperforms SGD with default beta1 and beta2."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722113011,
                "cdate": 1700722113011,
                "tmdate": 1700724408480,
                "mdate": 1700724408480,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OopVxVeUak",
                "forum": "DLJznSp6X3",
                "replyto": "YB7MDWQco6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question 1: In Figure 1, what's Loss for, training of validation? Why can Loss demonstrate \"similar performance\"?\n\nWe used training loss in this figure for clearer visualization. Loss is a standard measure for language model performance, since perplexity is just exp(loss). It's commonly used to evaluate language models (see references [2,3,4,5], for example). We would like to emphasize that our pre-training experiments were all single-epoch, so the model never sees the same example twice. Therefore, showing training loss is effectively the same as validation loss.\n\nAll language modeling result tables show the validation set perplexity. We also provide downstream evaluation in Table 8.\n\n### Question 2: In Table 3, why isn't full fine-tuning included?\n\nWe will include full fine-tuning results in Table 3 in the camera-ready. Here\u2019s an updated table.\n\n|               |      |      |      |      |      |      |      |      |\n|---------------|------|------|------|------|------|------|------|------|\n| Full fine-tuning* | 93.6 | 86.2 | 92.8 | 91.7 | 74.8 | 90.1 | 92.7 | 63.4 | 85.7 |\n| LoRA           | 94.0 | 86.3 | 92.6 | 90.6 | 72.2 | 89.8 | 90.2 | 60.3 | 84.5 |\n| ReLoRA         | 94.8 | 86.3 | 93.0 | 90.8 | 80.1 | 89.9 | 91.4 | 61.1 | 85.9 |\n|               |      |      |      |      |      |      |      |      |      |\n| Full fine-tuning (T5-large) | 94.7 | 89.1 | 91.6 | 89.9 | 78.9 | 90.6 | 88.9 | 57.0 | 85.0 |\n| LoRA (T5-large) | 95.9 | 89.6 | 94.4 | 91.4 | 85.6 | 90.5 | 90.7 | 60.3 | 87.3 |\n| ReLoRA (T5-large) | 96.1 | 89.5 | 94.6 | 91.6 | 83.4 | 90.8 | 91.4 | 62.1 | 87.4 |\n\n### Question 3: For the Control Baseline, how are the trainable parameters chosen?\nThe control baseline refers to training a full-rank model with a total parameter count equal to the trainable parameter count used in low-rank training. We determine the number of layers and hidden size of the control  model based on this total number of parameters. Here are our configuration choices:\n| Model Size | Hidden Size | Intermediate Size | Num Attention Heads | Num Hidden Layers |\n|----------------|---------------|-------------------|---------------------|-------------------|\n| 1.3B           | 2048        | 5461              | 32                  | 24                |\n| 350M         | 1024        | 2736            | 16                  | 24                |\n| 250M         | 768         | 2560             | 16                  | 24                |\n| 99M           | 640         | 1708              | 10                  | 12                |\n| 130M         | 768         | 2048             | 12                 | 12                |\n| 70M           | 512         | 1368              | 8                    | 12                |\n\n### Question 4: ReLoRA outperforms LoRA for fine-tuning can be due to the Warm start?\n\nThere is no warm start in our experiments that use ReLoRA for fine-tuning. Thank you for pointing out this detail, we will clarify this in the camera-ready.\n\nThank you again for your thorough review.  We hope that our comments address your concerns and we are confident that the minor issues can be easily addressed in the final version. Given the above, we sincerely hope you are able to raise your score.\n\n### References\n\n[1] LLaMA: Open and Efficient Foundation Language Models, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. 2023\n\n[2] A Neural Probabilistic Language Model. Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Christian Jauvin. 2003\n\n[3] Recurrent neural network based language model.Tomas Mikolov, Martin Karafia, Lukas Burget, Jan \u201cHonza\u201d Cernocky, Sanjeev Khudanpur. 2010\n\n[4] Exploring the Limits of Language Modeling, Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu. 2016\n\n[5] Language Models are Unsupervised Multitask Learners, Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, Ilya Sutskever. 2019\n\n[6] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. 2019\n\n[7] BitNet: Scaling 1-bit Transformers for Large Language Models. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei. 2023\n\n[8] Reformer: The Efficient Transformer. Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya. 2020\n\n[9] Efficient Transformers: A Survey. Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler. 2020\n\n[10] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg. 2021\n\n[11] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning. Vladislav Lialin, Vijeta Deshpande, Anna Rumshisky. 2023\n\n[12] QLoRA: Efficient Finetuning of Quantized LLMs. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer. 2023"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722292058,
                "cdate": 1700722292058,
                "tmdate": 1700724673426,
                "mdate": 1700724673426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Fet3kp7EFl",
            "forum": "DLJznSp6X3",
            "replyto": "DLJznSp6X3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4146/Reviewer_DYvD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4146/Reviewer_DYvD"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces ReLoRA (Recurrent Low Rank Adaptation) for training large language models that optimizes the training process to be more computationally efficient while maintaining performance. The main idea is utilizing multiple low-rank updates to effectively train high-rank networks, leveraging the 'rank of sum' property. The design incorporates multiple components like full-rank training warm start, periodic parameter merging, jagged learning rates, partial optimizer restarts. The authors show that ReLoRA can replace LoRA for fine tuning performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality\n- The paper introduces a unique combination of low-rank updates, full-rank training warm start, and periodic parameter merging. This approach not only reduces the computational resources required for training but also ensures the model\u2019s effectiveness across various tasks. \n\nQuality\n- The authors perform decent amount of experimentation and thorough analysis. The authors have provided a comprehensive evaluation of the method on large transformer models (1.3B params), ensuring the validation of their approach. \n\nClarity\n- The paper is well-structured and clearly written. I did not have a problem following the proposal."
                },
                "weaknesses": {
                    "value": "Performance Similarity to LoRA:\n- Based on the empirical results, ReLoRA, exhibits performance (perplexities) that is very similar to that of LoRA. This raises questions about the practical necessity of the more complex ReLoRA framework (multiple new hyperparameters and intricate design choices) when simpler alternatives provide comparable results. Especially because the training speedups are marginal ~9%. \n\n- The authors could strengthen their contribution by providing clearer and more substantial evidence of scenarios where ReLoRA significantly outperforms LoRA, helping to justify the additional complexity."
                },
                "questions": {
                    "value": "Discussion of Limitations:\n- The paper could benefit from a more thorough discussion of the limitations of ReLoRA. Understanding the scenarios in which ReLoRA may not perform as expected or could be improved is crucial for future research and practical applications.\n\nExplanation of Design Choices:\n- Some design choices in ReLoRA, such as the specific strategy for periodic parameter merging and the choice of low-rank updates, could be explained in more detail. Providing the rationale behind these choices and discussing potential alternatives would strengthen the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4146/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4146/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4146/Reviewer_DYvD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649364733,
            "cdate": 1698649364733,
            "tmdate": 1699636379916,
            "mdate": 1699636379916,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MEnpUBbZBl",
                "forum": "DLJznSp6X3",
                "replyto": "Fet3kp7EFl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review, for acknowledging the novelty and importance of our work. We would like to address your concerns.\n\n### ReLoRA vs LoRA performance\n\nReLoRA consistently outperformed LoRA + Warm Start  in all scenarios, with the difference between ReLoRA and LoRA increasing with model size from just 0.27 perplexity points (60M) to 0.96 (1.3B), which is a very large perplexity difference.\n\n|                    | 60M    | 130M   | 250M   | 350M   | 1.3B   |\n|--------------------|--------|--------|--------|--------|--------|\n| LoRA + Warm Start  | 34.73 | 25.46 | 22.86 | 19.73 | 18.23 |\n| ReLoRA             | 34.46 | 25.04 | 22.48 | 19.32 | 17.27 |\n| $\u0394$                   | 0.27  | 0.42  | 0.38  | 0.41  | 0.96  |\n\n### Speedups\n\nThe ~9% training speedup, while seemingly marginal, is significant in the context of large-scale language model training, where even small percentages can translate to substantial resource savings. Additionally, the speedup depends a lot on the size of the network and the hardware setup. The table below describes speedup for training a 1.3B model in different hardware setups. As you can see, in 6xA6000 Ada, we see a 48% immediate speedup (not accounting for warm start) and 32% speedup when the warm start is accounted for.\n\n|                        | 8xA100         | 6xA6000 (Ada) | 2x3090       |\n|------------------------|----------------|---------------|--------------|\n| Full-rank throughput   | 137 ex/sec     | 84 ex/sec     | 8.8 ex/sec   |\n| ReLoRA throughput      | 157 ex/sec     | 124 ex/sec    | 17.8 ex/sec  |\n| Immediate speedup      | 15%            | 48%           | 102%         |\n| Warm-start adjusted ReLoRA throughput (33% warm-start) | 149 ex/sec | 111 ex/sec | 14.8 ex/sec |\n| Total speedup          | 9%             | 32%           | 51%          |\n\nWe would like to thank you for your review and ask to raise your score given the clarifications above."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724902922,
                "cdate": 1700724902922,
                "tmdate": 1700724902922,
                "mdate": 1700724902922,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JCfmJqz5Te",
            "forum": "DLJznSp6X3",
            "replyto": "DLJznSp6X3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4146/Reviewer_x3dX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4146/Reviewer_x3dX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a parameter-efficient training method called ReLoRA. Based on the basic solution of LoRA, ReLoRA cyclically merges the trainable parameters W_A and W_B into the original parameters and reinitializes the parameters, thus training high-rank networks. The authors show the method's potential in parameter-efficient pre-training and the immediate replacement of LoRA. Experimental results show memory and computation reduction on various model scale setups."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper points out the low-rank limitations of LoRA on parameter updating and proposes a merge-initialization approach, called ReLoRA, to make some improvements to LoRA. The experimental results show that this improvement can be applied in the model pre-training process to reduce memory and computational resources overhead. In the fine-tuning stage, it can also be used as an alternative to LoRA to achieve better performance on some downstream tasks."
                },
                "weaknesses": {
                    "value": "The main weakness is whether to keep high-rank training or use LoRA. As far as I understand, methods such as LoRA for parameter-efficient fine-tuning are proposed based on the low-rank nature of the fine-tuning process, so it seems that there is no need for high-rank training during the fine-tuning phase (on the other hand, merging parameters may contaminate the original pretrained checkpoints); While the authors claim the potential of ReLoRA in the training process, it also emphasizes that the warm-start of the full-parameter training has a significant impact on the training effect, so is it possible that such a technique with a low-rank adaptor is unsuitable or unnecessary for pre-training? Clarification of the following questions may help."
                },
                "questions": {
                    "value": "- What is the relationship of parameters\u2019 rank and task performance? Is it possible to quantify the respective RANK needed for pre-training and fine-tuning?\n- Can equation 3 prove that \\delta W achieves a higher rank compared to LoRA? Is it possible to add the relationship between higher rank and random initialization?\n- From Figure 1 and Table 2, is it possible to conclude that the pattern of low-rank updates is mainly in the middle and late stages of model training?\n- How can ReLoRA speed up the training, as the forward and backward computation is not sparsified (or even more computation)?\n- How is the performance and further speedup when you combine the ReLoRA and low-precision quantization, as mentioned in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4146/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4146/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4146/Reviewer_x3dX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796920033,
            "cdate": 1698796920033,
            "tmdate": 1700778729630,
            "mdate": 1700778729630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N0lGV5W8JW",
                "forum": "DLJznSp6X3",
                "replyto": "JCfmJqz5Te",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your feedback and recommendations! Here we would like to respond to your points.\n\n> \u201cIs it possible that such a technique with a low-rank adaptor is unsuitable or unnecessary for pre-training\u201d\n\nWe believe there might have been some confusion regarding the primary focus of our paper.  ReLoRA is conceived primarily as a method for efficient **pre-training**, rather than fine-tuning. We show that a sequence of low-rank updates can be used to pre-train models from an early checkpoint, saving substantial compute. This is the first study to apply parameter-efficient methods to pre-training, which offers considerable savings in compute without significantly compromising the quality of the model. Fine-tuning results are complementary to the main point of the paper.\n\nWe apologize if this was not sufficiently clear in our original submission, and we hope that this clarification would allow you to reassess the paper.\n\nBelow, we address specific questions raised in your review\n\n### Question 1: Is it possible to quantify the respective RANK needed for pre-training and fine-tuning?\n\nThank you for this question, it\u2019s very relevant. We can measure the number of singular values above a certain threshold (0.1) in the learned update (same way as we did for Figure 4).\n\nHere\u2019s what we find:\n\n* Pre-training: 47% of singular values > 0.1\n* Fine-tuning: 4% of singular values > 0.1\n\nExperimental setup: \n- We used ReLoRA of rank=128 for both pre-training and fine-tuning\n- Pre-training resets LoRA parameters every 5K iterations (3 times total) \n- Fine-tuning resets LoRA every 1K iterations (2 times total)\n- For fine-tuning, we used the SST2 dataset and achieved 90% accuracy (full fine-tuning achieves 90% as well).\n\nNumber of singular values > 0.1 (Average across layers)\n* Pre-Training (Q Projections):  31.2\n* Fine-Tuning  (Q Projections):  467.4\n* Pre-Training (V Projections):  19.2\n* Fine-Tuning  (V Projections):  365.2\n* Pre-Training (Up Projections):  59.5\n* Fine-Tuning  (Up Projections):  568.0\n* Pre-Training (Down Projections):  57.0\n* Fine-Tuning  (Down Projections):  539.5\n\nAs you can see, we see a drastic difference between pre-training and fine-tuning, which is probably caused by much higher complexity and the amount of data in the pre-training task, as compared to the downstream classification task (SST2)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715574894,
                "cdate": 1700715574894,
                "tmdate": 1700722230445,
                "mdate": 1700722230445,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fW5YXxxXQr",
                "forum": "DLJznSp6X3",
                "replyto": "JCfmJqz5Te",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question 2: Can equation 3 prove that $\u0394 W$ achieves a higher rank compared to LoRA? Is it possible to add the relationship between higher rank and random initialization?\n\nThanks for raising this question. We would like to explain our motivation behind ReLoRA and why we think proving this could be difficult.\n\n**Intuition for why we need optimizer resets**\n\nLet\u2019s assume that training a model using unrestricted gradient descent (regular training) performs a high-rank update (higher than a single LoRA update). This assumption is verified in Figure 4. Let\u2019s now learn two LoRA updates sequentially using ADAM optimizer. After learning the first update, we perform a merge-and-reinit operation on the model weights where merge is \n\n$$\n\\begin{align*}\nW^{t} &= W^{t-1} + s W^{t-1}_A W^{t-1}_B \\\\\\\\\nW^{t}_A &= 0 \\\\\\\\\n$W^{t}_B &= N\\left(0, \\frac{2}{h}\\right)\n\\end{align*}\n$$\n\nAt the next iteration, model update using Adam will be\n\n$$\n\\begin{align*}\nm^{t+1} &= \\beta_1 m^{t} + (1 - \\beta_1) g^{t} \\\\\\\\\nv^{t+1} &= \\beta_2 v^{t} + (1 - \\beta_2) (g^{t})^2 \\\\\\\\\n\\hat{m}^{t+1} &= \\frac{m^{t+1}}{1 - (\\beta_1)^{t+1}} \\\\\\\\\n\\hat{v}^{t+1} &= \\frac{v^{t+1}}{1 - (\\beta_2)^{t+1}} \\\\\\\\\n\\\\\\\\\nW^{t+1}_A &= W^{t}_A - \\eta \\frac{\\hat{m}^{t+1}}{\\sqrt{\\hat{v}^{t+1}} + \\epsilon}\n\\end{align*}\n$$\n\n$W^{t+1}_B$ update is similar.\n\nNow, accounting for the high values of $\\beta_1$ and $\\beta_2$ we can say that the update at step t+1 will be highly similar to the previous update steps, because $m^{t}$ and $v^{t}$ are carried on from the previous update. Because of the high similarity between them, which will carry on over many update iterations due to high value of the betas, intuitively, we expect the learned update to either not change the rank at all or only minimally increase it, which motivates our optimizer reset strategy presented in the paper. And is verified by our ablation study (Table 4).\n\n**Intuition for why proving rank increase is hard**\n\nLet\u2019s assume that instead of ADAM we use SGD. In this case, there is no dependence between the updates.\n\n$$\n\\begin{align*}\nW^{t+1}_A &= 0 + \\eta \\frac{\\partial L}{\\partial W_A}(W^{t+1}_A) \\\\\\\\\nW^{t+1}_B &= N\\left(0, \\frac{2}{h}\\right) + \\eta \\frac{\\partial L}{\\partial W_B}(W^{t+1}_B)\n\\end{align*}\n$$\n\nAt this point, $W^{t+1} = W^{t} + W^{t+1}_A W^{t+1}_B$ it likely to be dominated by the initialization of $W^{t}_B = N(0, \\frac{2}{h})$. However, the longer we train the network (let\u2019s assume we trained it for 1000 steps), the less the update relies on the initialization of $W_B$ and more on the dynamics of neural network training.\n\n$$\n\\begin{align*}\nW^{t+1000}_A &= 0 + 1000 \\eta \\sum_i \\frac{\\partial L}{\\partial W_A}(W^{t+i}_A) \\\\\\\\\nW^{t+1000}_B &= N\\left(0, \\frac{2}{h}\\right) + 1000 \\eta \\sum_i \\frac{\\partial L}{\\partial W_B}(W^{t+i}_B)\n\\end{align*}\n$$\n\n(assuming constant learning rate for simplicity)\n\nAs far as we know, as of now, no reliable theory has been proposed to explain NN training dynamics here, and developing it lies outside of the scope of our study. We would be grateful for any references that you could point to here.\n\nFinally, as our experiments show, a combination of LoRA resets with optimizer resets and learning rate re-warmup is needed for ReLoRA to work. This means that any proof that does not use these conditions will be insufficient.\n\nWe believe that ReLoRA is self-sufficient as a purely empirical method and that the experiments we report are valuable due to consistent results across model scales from very small (70M) to relatively large (1.3B) scale. While having some theoretical guarantees is desirable, deep learning's history is peppered with contradictions to established theories. For instance, in 2012-2017, many in the ML community doubted the usefulness and reliability of deep learning, arguing that non-convex optimization with SGD wasn't feasible and the large number of parameters would lead to overfitting and poor generalization. Yet, these concerns were proven unfounded with the advancements post-2018. It's also worth noting that many ICML, ICRL, and NeurIPS papers include mathematical proofs with unrealistic simplifications, making them irrelevant to actual use cases. We aim to avoid this pitfall and ensure our paper remains relevant to real-world applications."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715599703,
                "cdate": 1700715599703,
                "tmdate": 1700722061305,
                "mdate": 1700722061305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ulJvxmPTpz",
                "forum": "DLJznSp6X3",
                "replyto": "JCfmJqz5Te",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question 3: From Figure 1 and Table 2, is it possible to conclude that the pattern of low-rank updates is mainly in the middle and late stages of model training?\n\nThis is consistent with our current understanding. A similar pattern was also observed for the lottery ticket hypothesis, where for larger models an initial warmup training of the full network was required [5], but the rest of the training could be approximated through a low-rank lottery ticket training.\n\n### Question 4: How can ReLoRA speed up the training, as the forward and backward computation is not sparsified (or even more computation)?\n\nSimilarly to PEFT methods (e.g., LoRA), ReLoRA significantly reduces the memory spent on optimizer states and gradients, allowing the use of larger microbatch sizes, which increases the efficiency. Further, distributed training is frequently bottlenecked by the GPU communication speed, even for ~1B-sized models. ReLoRA drastically reduces the communication volume to synchronize the gradients as the number of trainable parameters is much smaller. Finally, the update itself is much faster, as it requires less computation within ADAM: computing new optimizer states, computing the update, and applying the update to ReLoRA parameters. This question is discussed in-depth in this PEFT survey [4]. In total, ReLoRA speeds up training in the same way PEFT does, and its speedup is dependent on the model size, with larger models getting bigger speedups.\n\n### Question 5: How is the performance and further speedup when you combine the ReLoRA and low-precision quantization, as mentioned in the paper?\n\nWe did perform some experiments with 4-bit QLoRA instead of LoRA with the 1.3B model. In our observations, we saw virtually no difference between quantized and non-quantized models in terms of the loss, but unfortunately we observed a slowdown. After talking to QLoRA authors, we concluded that this behavior is expected for models of this size. The reasons are as follows: the normal float 4 (nf4) format stores floats in only 4 bits, but does computation in bfloat16, so for every multiplication, there is an added overhead of converting the numbers to bf16-floats and performing a 16-bit operation. For the 1.3B model in our case, using the quantized weights only wins about 2Gb GPU memory, not allowing to increase batch size significantly enough to improve the throughput.\n\nAgain, we would like to thank you for your review. We hope that our responses address your concerns and we would like to ask you to increase your score.\n\n[1] Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning, Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta, 2020\n\n[2] Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning, Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel, 2022\n\n[3] Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning, Charles Martin, Michael Mahoney\n\n[4] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning. Vladislav Lialin, Vijeta Deshpande, Anna Rumshisky. 2023.\n\n[5] Stabilizing the Lottery Ticket Hypothesis, Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin, 2020"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715641707,
                "cdate": 1700715641707,
                "tmdate": 1700722081846,
                "mdate": 1700722081846,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NMnj89VXHe",
            "forum": "DLJznSp6X3",
            "replyto": "DLJznSp6X3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4146/Reviewer_vHWf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4146/Reviewer_vHWf"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes ReLoRA, a parameter-efficient method for pretraining neural networks that iteratively trains low-rank adapters, merges them, and trains new sets of adapters for the same parameters. The proposed method is validated on language model pretraining on C4 for sizes ranging from 60M to 1.3B parameters and finetuning T5-base and T5-large; using ReLoRA results in significant memory savings with similar quality of the final model compared to full-rank training."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposed approach is simple to implement and use, yet quite efficient in practice\n* Overall, the paper is clearly written and easy to understand\n* I particularly appreciated the authors including negative results in their submission: the community could significantly benefit from a more widespread use of that practice, yet the majority of papers does not report failed experiments."
                },
                "weaknesses": {
                    "value": "* It is not fully clear to me if the proposed technique will indeed scale to model sizes which are currently considered to be most capable (starting from 7B parameters and reaching up to 70B in most cases). While I understand that even current results are quite promising, it might be the case that at larger scales, low-rank training would more severely affect the capabilities of the model, which would make the work less impactful for the community.\n* Similarly to the above question, I think that a more principled way to conduct experiments would be to train ReLoRA until it achieves the same perplexity as the full-rank baseline. If we aim to achieve the same quality as the standard approach, it is good to know how many additional iterations with ReLoRA would be necessary for that. I would also expect that the wall-clock time gains become less pronounced after such a comparison.\n* The work could benefit from a bit more polish and proofreading: there are multiple typos and incomplete sentences throughout the paper. For example, see \"hparam\" -> \"hyperparameter\" and \"Flash attention\" -> \"FlashAttention\" in page 4, \"resented\" -> \"presented\" in page 5 and \"ReLoRA clearly outperforms LoRA at At\" in page 8."
                },
                "questions": {
                    "value": "* While [1] is not strictly relevant to the submission's topic, I think it studies a similar set of questions, and I would be curious to hear authors' opinion of that paper.\n\n[1] Exploring Low Rank Training of Deep Neural Networks. Siddhartha Rao Kamalakara, Acyr Locatelli, Bharat Venkitesh, Jimmy Ba, Yarin Gal, Aidan N. Gomez. 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699017590798,
            "cdate": 1699017590798,
            "tmdate": 1699636379761,
            "mdate": 1699636379761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "13a5pYRnjk",
                "forum": "DLJznSp6X3",
                "replyto": "NMnj89VXHe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review!\n\nAlso, big thanks for highlighting some issues with the text. These sections were probably added last and we hope that the rest of the text and the general organization is clear. We will address them and do another round of proofreading.\n\nWe will now respond to your concerns, and then discuss the paper [1] you mention.\n\n## Will the method scale to larger model sizes?\n\nRegarding whether ReLoRA will be able to achieve the same loss as full-rank training at 7B or even 70B scale, we strongly believe so. Multiple papers suggest that full-rank NN training is locally low-rank. For example, [2] show that as the model becomes larger or when pre-trained for longer, the rank of the change needed to learn a downstream task reduces. [3] finds that SGD is biased towards low-rank solutions. The existence of Lottery Tickets early in training [4] also partially supports this hypothesis, since training a lottery ticket network could effectively be seen as a low-rank approximation to the regular training process.\n\nWhile there is no formal proof yet that NN training is locally low-rank, our results both 1) provide a practical contribution to more efficient training and 2) support the growing evidence that despite their high dimensionality, neural network optimizations may intrinsically lie in far lower dimensional spaces.\n\nWe had a similar question about an early version of this paper. At the time, the biggest model we had trained was 350M and the question was if this would scale beyond 1B parameters. In this iteration of the paper, we were able to successfully scale ReLoRA to 1.3B.\n\n## Comparing ReLoRA training efficiency to full-rank training\n\nThanks for raising this question! We would like to highlight several important details. First, for ReLoRA, as for all PEFT methods the speedup increases with model size. For example, at 130M-scale using ReLoRA can actually be significantly slower than regular training. 350M is roughly when ReLoRA throughput matches regular training and at 1.3B, we already see some significant improvement: 15% immediate speedup (not accounting for warm start) and 9% total speedup as reported in the paper. Second, the speedup is significantly hardware-dependent with ReLoRA being more effective at cheaper hardware. For details please look at this table:\n\n|                        | 8xA100         | 6xA6000 (Ada) | 2x3090       |\n|------------------------|----------------|---------------|--------------|\n| Full-rank throughput   | 137 ex/sec     | 84 ex/sec     | 8.8 ex/sec   |\n| ReLoRA throughput      | 157 ex/sec     | 124 ex/sec    | 17.8 ex/sec  |\n| Immediate speedup      | 15%            | 48%           | 102%         |\n| Warm-start adjusted ReLoRA throughput (33% warm-start) | 149 ex/sec | 111 ex/sec | 14.8 ex/sec |\n| Total speedup          | 9%             | 32%           | 51%          |\n\nThroughput is measured in examples per second. This table will be added into the paper to clarify the speedups.\n\n[1] Exploring Low Rank Training of Deep Neural Networks. Siddhartha Rao Kamalakara, Acyr Locatelli, Bharat Venkitesh, Jimmy Ba, Yarin Gal, Aidan N. Gomez. 2022\n\n[2] Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning, Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta, 2020\n\n[3] Implicit Regularization in Deep Matrix Factorization, Sanjeev Arora, Nadav Cohen, Wei Hu, Yuping Luo, 2019\n\n[4] Linear Mode Connectivity and the Lottery Ticket Hypothesis, Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin, 2020\n\n[5] Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning, Charles Martin, Michael Mahoney. 2018\n\n\nWe are again very thankful for your review and would like to ask you to increase your score if your concerns were sufficiently addressed."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629779567,
                "cdate": 1700629779567,
                "tmdate": 1700674586774,
                "mdate": 1700674586774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gVlWqbFeuq",
                "forum": "DLJznSp6X3",
                "replyto": "NMnj89VXHe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4146/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussing \"Exploring Low Rank Training of Deep Neural Networks\""
                    },
                    "comment": {
                        "value": "We actually looked into this paper after the submission and we think it\u2019s a great example of how low-rank training could be beneficial for improving computational efficiency of training large models. We would like to highlight some similarities and differences between our methods.\n\nBefore applying factorization, Kamalakara et al. first train the network in a regular high-rank way (Section 3.4. Pre-training). Specifically, they train full-rank for 40K steps and then train low-rank for 200K more steps, which is roughly on-par with our 5K/20K ratio. This goes inline with our results and makes their final method close to our Warm Start + LoRA baseline.\n\nOur work differs notably in these key areas: 1) Kamalakara et al. perform an SVD decomposition of the weights after the initial full-rank training and only keep low-rank versions of the weights 2) They use a relatively high rank which comprises 62.5% of the hidden state size, while we use the rank of 128, which is about 6% of the 1.3B model hidden size. In our studies, we found little to no difference between ranks 128 and 512 for the 1.3B model. We attribute it to the weight merging procedure which can aggregate to a higher rank than just a single update. One more difference between the approaches is that Kamalakara et al. reduce the learning rate for their method, in contrast to ReLoRA, where we increase it. This decision seems to be counter-intuitive for us, as generally models with more trainable parameters use smaller learning rates.\n\nDifferences in the experiment scale:\n\nAdditional differences between our experimental setups include the fact that they use very small sequence length (256 instead of 512) and a very small pre-training dataset (One Billion Words), which is on par with our 70M-scale experiments, but 20x smaller than our largest 1.3B experiments. Finally, and touching upon your question of scale, the largest experiment in the study seems to be less than 300 TPUv2 hours, compared to our largest experiment being 640 A100 hours. In terms of theoretical flops (16-bit), TPUv2 is 1.7 times slower than A100: 180 vs 312 Tflops. In total, the largest experiment in our paper used 3.6 more compute than in Kamalakara et al."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629875708,
                "cdate": 1700629875708,
                "tmdate": 1700629875708,
                "mdate": 1700629875708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]