[
    {
        "title": "Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models"
    },
    {
        "review": {
            "id": "nIYHuXduXb",
            "forum": "Let8OMe20n",
            "replyto": "Let8OMe20n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6921/Reviewer_fRcb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6921/Reviewer_fRcb"
            ],
            "content": {
                "summary": {
                    "value": "This paper study the problem of reward functions on human feedback data for image diffusion models. It proposes a new text-image alignment assessment benchmark (TIA2) for evaluating the problem of reward designing. It shows that several existing reward-based models are not well-aligned with human assessment after evaluated on the benchmark. The authors propose a method TextNorm to induce confidence calibration in reward models. It shows the proposed score help reduce over-optimization in the human feedback tuning. Experiments are conducted with Stable Diffusion v2.1 and LoRA for parameter-efficient fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe proposes benchmark for evaluating text-image alignment can help the reward designing in human feedback data. It includes different categories of prompts: comprehensive, counting and composition, which are the key categories that common image reward functions such as CLIP and ImageReward cannot do well on.\n-\tThe proposed TextNorm is novel and shows superior performance on improving calibration errors on different types of prompts. It also improves text-image alignment compared to existing reward functions according to some qualitative results and human evaluation.\n-\tAn ablation study is also conducted to analyze the impact of different aspects: normalization over prompts, prompt synthesis, and the textual inconsistency score."
                },
                "weaknesses": {
                    "value": "-\tIn the process of prompt synthesis using language model, the authors only ask the LLM to generate variations based on the object category and number of objects. Other important aspects such as spatial relationships and object attributes like material, color, size is not included. This greatly limit the scope of the experiment in this work. I also only saw sampled result images related to object category and object number in this paper. It would be great if more analysis on those other aspects can be studied as well.\n-\tIn figure 5, the difference of TextNorm and existing methods are not significant. BLIP2 has very similar results as the TextNorm on the correctness of the object number and categories."
                },
                "questions": {
                    "value": "In the human evaluation, the authors are comparing with \u201cthe best-performing baseline reward model\u201d but did not mention which one it is. Please provide the details on which reward function you are comparing with."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631999678,
            "cdate": 1698631999678,
            "tmdate": 1699636805892,
            "mdate": 1699636805892,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0LVftvtcWD",
                "forum": "Let8OMe20n",
                "replyto": "nIYHuXduXb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6921/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer fRcb,\n\nWe sincerely appreciate your review with thoughtful comments. We have carefully considered each of your questions and provide detailed responses below. Please let us know if you have any further questions or concerns.\n\n---\n\n**[W1] Additional aspects of text-image alignment in prompt synthesis.**\n\nWe instruct the LLM to generate variations with an emphasis on the categories and quantities of objects, as they are common alignment issues with text-to-image models. However, we also utilize different sets of few-shot examples tailored to the type of input prompt to guide the LLM towards generating relevant variations. As an example, for the prompt \u201ca red box on top of a blue box\u201d in Table 14, the set of synthesized prompts includes variations such as \u201ctwo blue balls on top of a pink box\u201d and \u201ca black triangle on top of a green box\u201d, exploring alternative color attributes for the objects. We have revised the draft to add these details in Section 4.1.\n\n---\n\n**[W2] Qualitatively comparable results.**\n\nNote that Figures 1 and 4 contain examples that further highlight the shortcomings of the baseline models, including BLIP-2. We also added Figure 8 with an additional example that better showcases the limitations of BLIP-2 in the revised draft. Given the variability in performance across prompts, we remark that it is important to supplement the qualitative results with a human evaluation, as we have done in the work.\n\n---\n\n**[Q1] Details on the baseline models in human eval.**\n\nThank you for pointing this out. To clarify, PickScore is used as the baseline for the composition and counting prompts, while ImageReward is the baseline for the comprehensive prompts. We have also revised our draft to include the details."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203845320,
                "cdate": 1700203845320,
                "tmdate": 1700203845320,
                "mdate": 1700203845320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3YRk29WozH",
                "forum": "Let8OMe20n",
                "replyto": "0LVftvtcWD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6921/Reviewer_fRcb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6921/Reviewer_fRcb"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my comments. It is good to know that this work also include attributes into the prompt synthesis. However, other important factors are still missing: spatial relationship, object sizes etc. I would like to see these factors as well but the current version seems to have sufficient contribution. With all that considerations, I still hold my original rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682704703,
                "cdate": 1700682704703,
                "tmdate": 1700682704703,
                "mdate": 1700682704703,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Eb9N9lBeVP",
            "forum": "Let8OMe20n",
            "replyto": "Let8OMe20n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6921/Reviewer_exWV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6921/Reviewer_exWV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a text-image alignment benchmark and reward score normalization methods to evaluate and address the \"overoptimization\" issue when finetuning with a reward model. The proposed benchmark contains 550 prompts covering comprehensive/counting/compostion categories and 27500 generated images. The normalization method consider normalizing text-image reward scores by contrasting to generated texts. The human evaluation and qualitative results showing the proposed method performs better on the \"overoptimization\" issue over other baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to understand\n- The proposed benchmark showing the drawbacks of different reward models\n- The proposed text normalization method can reduce the overoptimization problem and improves the model finetuning performance"
                },
                "weaknesses": {
                    "value": "- One major contribution is the proposed text-image alignment assessment benchmark. However, the role/completeness of the benchmark are not fully explored. For example: 1. Are the comprehensive/counting/composition enough to compare those reward models? 2. In figure 3 (a), we can see ImageReward > PickScore > CLIP > BLIP-2 in terms of \"comprehensive\". Are these reward model ranking align with human evaluation (Do human also thinks ImageReward > PickScore > CLIP > BLIP-2)?"
                },
                "questions": {
                    "value": "- Section 5.2. \"select the top 10% of the samples as measured by the chosen reward model\". Is that means the model finetuned with different reward models are finetuned with different data? Why not use the same data for finetuning?\n- Any Failure cases for proposed text norm method? (failure cases for both comprehensive/counting/composition are helpful)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808787690,
            "cdate": 1698808787690,
            "tmdate": 1699636805767,
            "mdate": 1699636805767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bbrBtIzRmo",
                "forum": "Let8OMe20n",
                "replyto": "Eb9N9lBeVP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6921/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer exWV,\n\nWe sincerely appreciate your review with thoughtful comments. We have carefully considered each of your questions and provide detailed responses below. Please let us know if you have any further questions or concerns.\n\n---\n\n**[W1] Are the prompts in the benchmark sufficient for comparing the reward models?**\n\nWe believe our benchmark offers a sufficiently diverse collection of prompts for evaluating reward models across various aspects of text-image alignment. We introduced separate composition and counting sets to specifically assess the ability of the reward models to evaluate compositional generation, a known challenge for text-to-image models, particularly when involving multiple objects [1]. To increase prompt diversity, we added the comprehensive set, which contains 100 prompts spanning nine fine-grained semantic categories: creative, spatial, text rendering, style, counting, color, composition, location, and open-ended. As a future direction, we aim to expand the scope of our benchmark by incorporating additional prompts, images, and human labels.\n\n---\n\n**[W2] Do the rankings in Figure 3 align with human assessments?**\n\nFigure 3 indeed summarizes the overall alignment between the reward models and human assessments, as measured by the expected calibration error (ECE) computed with the human labels from the benchmark. Therefore, it is reasonable to conclude that a reward model for which more of the prompts fall within lower ECE ranges is overall more aligned with human preferences.\n\n---\n\n**[Q1] Are the models fine-tuned with different data in Section 5.2?**\n\nThe same offline dataset of images was considered for all fine-tuned models. However, filtered samples for each reward model may vary depending on how the model evaluates them. This training approach was used for two reasons: (a) using synthetic images from generative models filtered for quality is a common practice for downstream task training [2,3,4], and (b) we empirically found fine-tuning on filtered samples to be significantly more effective than using the entire dataset, which may contain low-quality samples, for optimizing for the given reward.\n\n---\n\n**[Q2] Failure cases of TextNorm**\n\nOur method employs a set of semantically contrastive prompts to enhance calibration by normalizing reward scores over these prompts. In case the set contains semantically equivalent or irrelevant prompts, normalization can result in worse calibration. Below are the results from an ablation experiment where we compare semantically contrastive prompts to random prompts in terms of calibration. This experiment underscores the importance of prompt set construction, highlighting that a poorly chosen set could yield worse results.\n\n| Reward |  | Comprehensive |  |  | Counting |  |  | Composition |  |\n|---|---|:---:|---|---|:---:|---|---|:---:|---|\n|  | ECE | ACE | MCE | ECE | ACE | MCE | ECE | ACE | MCE |\n| Random | 0.2777 | 0.2094 | 0.4986 | 0.3071 | 0.2323 | 0.5565 | 0.1758 | 0.1364 | 0.3883 |\n| LLM | 0.2516 | 0.1809 | 0.4852 | 0.2213 | 0.1609 | 0.4635 | 0.1604 | 0.1146 | 0.3666 |\n\n---\n\n**References**\n\n[1] Feng, Weixi, et al. \u201cTraining-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis.\u201d ICLR 2023.\n\n[2] He, Ruifei, et al. \u201cIs synthetic data from generative models ready for image recognition?\u201d ICLR 2023.\n\n[3] Fan, Ying, et al. \u201cDPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models.\u201d arXiv 2023.\n\n[4] Schuhmann, Christoph, et al. \u201cLAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs.\u201d arXiv 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203791004,
                "cdate": 1700203791004,
                "tmdate": 1700203791004,
                "mdate": 1700203791004,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CWiJz5JwQq",
            "forum": "Let8OMe20n",
            "replyto": "Let8OMe20n",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6921/Reviewer_9bKW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6921/Reviewer_9bKW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a simple method to prevent the overoptimization problem in alignment of text-to-image models. They work hypothesizes that the overoptimization problem is mainly due to lack of alignment between the human preference and the learnt reward model. Inorder to solve this issue, they propose two techniques namely textual normalization which better normalizes the reward using contrastive text description. Further they propose textual inconsistency score, which compares the distance between the textual description and the predicted reward . They find that both these objectives help improve the calliberation and alignment with the human reward."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The problem that paper picks namely reward overoptimization. Seems to be important with very works addressing it.\n- The paper does a good job at benchmarking previous reward functions on various settings.\n- The paper proposes novel techniques to improve the alignment and caliberation of the current reward models."
                },
                "weaknesses": {
                    "value": "- The paper talks about overoptimization as the motivation, however doesn't directly evaluate for overoptimization. It instead evaluates for caliberation and human evaluation. To my understanding , current works get away with overoptimization issue with early stopping such as AlignProp (https://arxiv.org/abs/2310.03739) however this is not discussed in the paper. I would expect the right way to evaluate would be to not do early stopping and see the tradeoffs with the proposed solutions. Also talk about the benefits of not doing early stopping.\n- Further AlignProp, says that they only need to do early stopping for the Aesthetics reward function, however not for HPS v2. I think it would be worth comparing againt the HPS v2 reward function and also discuss methods such as AlignProp or DDPO (https://arxiv.org/abs/2305.13301) and the tricks they use to prevent overoptimization. And why using those tricks might not be a good idea."
                },
                "questions": {
                    "value": "- Is there a fundamental reason why Softmax CLIP objective is better at caliberation than Sigmoid CLIP objective(https://arxiv.org/abs/2303.15343) ?\n\n-  \"Hence, we use the insight that the terms that contribute significantly to the denominator of the softmax score are the ones for which r\u03c6(x, y) is sufficiently large. \" -  It's unclear to me how using semantic-contrastive semantic prompts results in finding the terms that have high reward value?\n\n- Can the authors instead of finding contrastive prompts, take random prompts and show an ablation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698873554545,
            "cdate": 1698873554545,
            "tmdate": 1699636805629,
            "mdate": 1699636805629,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "txL9y7LfHc",
                "forum": "Let8OMe20n",
                "replyto": "CWiJz5JwQq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6921/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 9bKW,\n\nWe sincerely appreciate your review with thoughtful comments. We have carefully considered each of your questions and provide detailed responses below. Please let us know if you have any further questions or concerns.\n\n---\n\n**[W1] Evaluating overoptimization**\n\nIn our study, we consider human assessment of text-image alignment as the true objective and base our investigation on human annotations and evaluations. For our fine-tuning experiments, we evaluate the reward models against this true objective by conducting a human evaluation (the true objective) and report the results in Figure 6. The calibration analysis, which is also based on the human annotations from our benchmark, is for understanding the limitations of existing rewards and the associated risk of overoptimization.\n\n---\n\n**[W2] Connection to early stopping**\n\nThank you for your pointer. While early stopping is another viable approach (orthogonal to ours) to addressing overoptimization, it has several limitations. One challenge is determining the optimal stopping point, especially when evaluating the true objective is expensive. As you pointed out, the authors of AlignProp use early stopping in their experiments to guard against \u201closs of image-text alignment if optimized above a specific score\u201d [1]. However, it is difficult to justify whether the 10th epoch is the optimal point at which to stop training, without conducting many human evaluations on multiple checkpoints. The authors of DDPO [2] recognize that overoptimization is an issue in text-to-image generation but do not discuss any specific methods for addressing it.\n\nWe believe that our method of enhancing reward model calibration through alignment with human assessment can be also integrated into early stopping to more accurately determine when to halt optimizing against proxy rewards. We have revised our draft to include a discussion comparing early stopping and our proposed method.\n\n---\n\n**[W3] Evaluation of the HPS v2 reward model.**\n\nWe evaluated HPS v2 on our benchmark based on calibration metrics and include the results in the table below.\n\n| Reward |  | Comprehensive |  |  | Counting |  |  | Composition |  |\n|---|---|:---:|---|---|:---:|---|---|:---:|---|\n|  | ECE | ACE | MCE | ECE | ACE | MCE | ECE | ACE | MCE |\n| HPS v2 | 0.2801 | 0.2007 | 0.5581 | 0.2579 | 0.1881 | 0.5064 | 0.2266 | 0.1653 | 0.4718 |\n| TextNorm | 0.2516 | 0.1809 | 0.4852 | 0.2213 | 0.1609 | 0.4635 | 0.1604 | 0.1146 | 0.3666 |\n\nHPS v2 is indeed a competitive baseline compared to the other reward models we consider in this work. However, TextNorm still outperforms HPS v2 on our benchmark. We have revised our draft to include the results in Appendix.\n\n---\n\n**[Q1] Softmax CLIP objective vs. sigmoid CLIP objective.**\n\nWhile our proposed TextNorm (Eq. 1) has a notational similarity to the softmax-based CLIP objective, it is important to note that we are not claiming that the softmax-based objective is superior to the SigLIP objective [1] in terms of calibration. Our main focus is to evaluate the effectiveness of normalizing reward scores across contrastive prompts on improving calibration. The proposed method does not depend on the training objective of reward models. Nevertheless, we think evaluating SigLIP-based models is an interesting future question to explore.\n\n---\n\n**[Q2] Use of semantically contrastive prompts.**\n\nThe underlying rationale behind our prompt set design, which employs semantically contrastive prompts that are syntactically similar, is based on the following hypothesis: prompts $x$ that differ from the input prompt $x_0$ in both syntax and semantics are unlikely to generate high reward values $r_{\\phi}(x, y)$. Random prompts serve as an example of such $x$. As we show in our response to [Q3] in this letter, the prompt set using our method outperforms a random set of prompts, supporting the validity of the hypothesis. We have clarified this point in the revised draft."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203677229,
                "cdate": 1700203677229,
                "tmdate": 1700203677229,
                "mdate": 1700203677229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]