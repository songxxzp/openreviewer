[
    {
        "title": "Diverse Offline Imitation Learning"
    },
    {
        "review": {
            "id": "sk3bFbRiOJ",
            "forum": "KAseclJyj5",
            "replyto": "KAseclJyj5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission701/Reviewer_7Hkf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission701/Reviewer_7Hkf"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces DOI (Diverse Offline Imitation) for improving diversity of learned skill-conditioned policies. These skills are trained to have maximum diversity (via maximizing mutual information between states and skills) and to be close to expert state distributions. Prior work, SMODICE, is an instance of DOI where the KL-divergence between each skill state-visitation distribution and the expert state-visitation distribution must be 0, but DOI relaxes this constraint to be up to $\\varepsilon$.\n\nDOI is evaluated on a 12-DOF quadruped both in sim and real as well as in D4RL and shown to promote greater diversity of skills. However, there is a tradeoff between skill diversity and performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Method was deployed on a real robot, and website has videos showing the learning of presumably 3 different skills involving locomotion at 3 different base heights.\n- Figure 1 was helpful.\n- Writing in the main text sections of the paper was mostly clear, and the math was pretty understandable step-by-step."
                },
                "weaknesses": {
                    "value": "## Method, Experiments, and Argument\n\n(A1) Novelty and algorithmic contribution seems quite limited compared to SMODICE. The main differences are (1) Learning a discrete number of skill-based policies instead of a single policy, and (2) allowing skill-based policies and the expert to have up-to-$\\varepsilon$ KL divergence. Limited algorithmic novelty is fine if robotic performance on at least a few downstream tasks is a lot better than prior work, but experiments show ultimate performance is not improved with DOI.\n\n(A2) Experimental metrics are not very illustrative of the performance of DOI. Most of the plots (Figure 3, 4a) show proxy measures of how diverse the skills are. Only Figure 4b (and Tables S2, S3) compares DOI performance with SMODICE. Figure 4b shows that performance of DOI at smaller epsilon values is comparable to SMODICE, and Tables S2 and S3 show that SMODICE ($\\varepsilon = 0$) does better than any $\\varepsilon > 0$. Perhaps this is expected, as the authors argued there is a diversity-performance tradeoff in Section 6. But if so, there should be experiments, such as those testing generalization, where higher skill diversity leads to better robustness than narrowly-learned policies. However, the authors did not have experimental results of this kind, which brings into question when DOI should be used over SMODICE.\n\n(A3) Abstract and conclusion states that this paper proposes \u201ca principled offline algorithm for unsupervised skill discovery.\u201d However, the paper assumes that the set of skills $Z$ is finite, which suggests that skill candidates are predefined and presumably not continuous-spaced. If predefined, where is the \u201cskill discovery\u201d coming from? This is different from how CIC [1] does skill discovery, where skill vectors are continuous and sampled from a prior distribution.\n\n\n## Presentation/Coherence\n\nOverall, results graphs and algorithm boxes need to be clearer.\n\n(B1) Paper did not precisely define what a skill is, what the set of finite skills $Z$ is initialized to, or where $Z$ is from. Is $z \\in Z$ a continuous vector or a one-hot?\n\n(B2) Since method section relies so much on SMODICE, and Figure 2 refers to DICE (which was not introduced until page 7), I would recommend putting the related work section before the preliminaries section.\n\n(B3) Nitpick: Section 3.2.1, first line, should refer to Problem (eqn 7) instead of Problem (eqn 6), I believe.\n\n(B4) Algorithm 1 talks about a discriminator $c^{*}$ mapping state to predicted probability that the state is from $d_E$ vs $d_O$. However, this discriminator is not mentioned anywhere in the main text and can be confused with the skill discriminator $q(z|s)$.\n\n(B5) Phase 2 in Algorithm 1 mentions training $\\pi_z^{*}$ when Section 3.2.1 seems to say the policy is trained in Phase 1 instead.\n\n(B6) Section 5, Related Work, reads more like a laundry list of prior papers. There is no comparison at all to this paper\u2019s proposed method, DOI.\n\n(B7) Real robot results are not discussed in Section 6.\n\n(B8) Tables S2 (Walker2D) and S3 (HalfCheetah) contain the exact same entries (at least for the ~20 cells I looked at). This seems like an unfortunate mistake.\n\n## Reference\n[1] CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery. Laskin et al."
                },
                "questions": {
                    "value": "1. How would DOI be extendable to a continuous skill space?\n\n2. How did authors define the skill space $Z$ for each of the different environments? What was $Z$ and $|Z|$ for the D4RL envs? Presumably it was $|Z| = 3$ and $Z = [\\text{low base}, \\text{mid base}, \\text{high base}]$ for locomotion?\n\n3. Given Assumption 2.1, I wonder what the state representation is for the locomotion task. Can DOI be adapted to work on image observations, or a learned image feature space?\n\n4. What is $N$ in Tables S2 and S3?\n\n5. What does the color-coding in Figure 3a mean? Is it the same color meaning as Figure 3b?\n\n6. In Algorithm 1, the reward is defined as function of the output of $c^{*}$, which could easily be confused with the reward terms mentioned in Equations 8 and 14. How is this reward term, which doesn\u2019t depend on actions, used to compute importance sampling ratios $\\eta_{\\tilde{E}}(s,a)$, which does?\n\n7. Figure 3a: x-axis is confusingly labeled \u201cdata.\u201d Section 6 describes this as \u201cacross the dataset assignment.\u201d What do these things mean?\n\n8. I suggest using a slightly more descriptive title for the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission701/Reviewer_7Hkf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698621982826,
            "cdate": 1698621982826,
            "tmdate": 1699635997155,
            "mdate": 1699635997155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EtXEPv9NBm",
                "forum": "KAseclJyj5",
                "replyto": "sk3bFbRiOJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> (A1) Novelty and algorithmic contribution seems quite limited compared to SMODICE. The main differences are (1) Learning a discrete number of skill-based policies instead of a single policy, and (2) allowing skill-based policies and the expert to have up-to-$\\epsilon$ KL divergence. Limited algorithmic novelty is fine if robotic performance on at least a few downstream tasks is a lot better than prior work, but experiments show ultimate performance is not improved with DOI.\n\nWe respectfully disagree with the reviewer's comment.\nIt is crucial to emphasize that the novelty of our work is two-fold: \ni) we propose a novel and practically relevant problem formulation; and\nii) we show that combining existing algorithmic techniques yields a principled offline algorithm.\nWe want to stress the importance of understanding that previous work on unsupervised skill discovery reduces maximizing diversity to online training of a policy and a skill discriminator.\nAlthough there are efficient offline RL algorithms for learning a policy that maximizes a fixed reward, they cannot be applied here because our constraint optimization setting involves a non-stationary reward that depends on: i) the log-likelihood of the skill discriminator; and ii) the Lagrange multipliers.\nIn addition, training the skill discriminator and estimating the KL divergence both offline remains a challenge for non-DICE-based algorithms.\nIn summary, we extend DICE-based offline imitation to diversity maximization with imitation constraints, by utilizing the importance-weighted occupancy ratios to train a skill discriminator and to estimate KL divergence offline.\nWe have added this explanation more clearly in the paper.\n\n> (A2) Experimental metrics are not very illustrative of the performance of DOI. Most of the plots (Figure 3, 4a) show proxy measures of how diverse the skills are. Only Figure 4b (and Tables S2, S3) compares DOI performance with SMODICE. Figure 4b shows that performance of DOI at smaller epsilon values is comparable to SMODICE, and Tables S2 and S3 show that SMODICE ($\\epsilon=0$) does better than any $\\epsilon>0$. Perhaps this is expected, as the authors argued there is a diversity-performance tradeoff in Section 6. But if so, there should be experiments, such as those testing generalization, where higher skill diversity leads to better robustness than narrowly-learned policies. However, the authors did not have experimental results of this kind, which brings into question when DOI should be used over SMODICE.\n\n\nIt is indeed valuable to consider an offline data set consisting of various trajectories starting at $s$ and ending at $t$.\nLet the state-only expert samples be from the shortest path (straight line) between $s$ and $t$.\nSMODICE learns policies that closely follow the straight line. Blocking the straight line with an obstacle causes SMODICE to fail, while DOI learns diverse skills to get around the obstacle. We can provide this for the camera-ready.\n\nAnother potential application is to enhance the realism of computer games by creating an immersive experience of interacting with non-player characters, each behaving in a slightly different style, while all partially imitating the behavior of a human expert.\n\n> (A3) Abstract and conclusion states that this paper proposes \u201ca principled offline algorithm for unsupervised skill discovery.\u201d However, the paper assumes that the set of skills is finite, which suggests that skill candidates are predefined and presumably not continuous-spaced. If predefined, where is the \u201cskill discovery\u201d coming from? This is different from how CIC [1] does skill discovery, where skill vectors are continuous and sampled from a prior distribution.\n\nThank you for raising this important question.\nHowever, there is a misunderstanding about what the word ``skill'' denotes in the context of unsupervised skill discovery.\nIt is standard in the literature to denote by $z$ a vector in $\\mathbb{R}^d$ and refer to it as a latent skill.\nWe need to emphasize that the latent skills are not learnable, they are fixed.\nIn the discrete setting, they are usually chosen to be indicator vectors (there are $d$ distinct latent skills in $\\mathbb{R}^d$).\nIn contrast, it is the skill-conditioned policy $\\pi(a|s,z)$ that is learnable, while the latent skill $z$ serves as an index of a learned temporally extended behavior.\nMoreover, a skill-conditioned policy $\\pi\\_z$ induces a state occupancy $d\\_z(S)$ which belongs to the state simplex.\nThen, the geometric interpretation of maximizing mutual information $\\mathcal{I}(S;Z)=\\mathbb{E}\\_{z}\\mathrm{D}\\_{\\mathrm{KL}}(d\\_{z}||\\mathbb{E}\\_{z^{\\prime}}d\\_{z^{\\prime}})$ is to learn $d$ many points $d_{z}(S)$ such that the distance between each point and the mean point is maximized.\nIn practice, each skill-conditional policy $\\pi\\_z$ maximizes a reward given by the log-likelihood of a learnable skill-discriminator $q(z|s)$.\nWe have clarified and updated the paper accordingly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688243852,
                "cdate": 1700688243852,
                "tmdate": 1700728264340,
                "mdate": 1700728264340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zFA1GY7EAL",
                "forum": "KAseclJyj5",
                "replyto": "sk3bFbRiOJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Overall, results graphs and algorithm boxes need to be clearer.\n\nWe have clarified and updated the paper accordingly.\n\n> (B1) Paper did not precisely define what a skill is, what the set of finite skills $Z$ is initialized to, or where $Z$ is from. Is $z\\in Z$ a continuous vector or a one-hot?\n\nThank you for pointing that out.\nHowever, in the Preliminaries section we state that we treat $Z$ as a finite set (paragraph 3), and before Eq. (3) we specify $p(z) = \\frac{1}{|Z|}$ as the uniform distribution over $Z$.\nFurthermore, it is standard in the literature, when $p(z)$ is a categorical distribution to choose latent skills $z$ as indicator vectors ($|Z|$ distinct skills in $\\mathbb{R}^{|Z|}$).\nNonetheless, following the reviewer's comment, we have added this explanation more clearly in the paper.\n\n> (B2) Since method section relies so much on SMODICE, and Figure 2 refers to DICE (which was not introduced until page 7), I would recommend putting the related work section before the preliminaries section.\n\nFollowing the reviewer's comment, we have updated the paper accordingly.\n\n> (B3) Nitpick: Section 3.2.1, first line, should refer to Problem (eqn 7) instead of Problem (eqn 6), I believe.\n\nThank you for pointing that out. \nWe have updated the paper accordingly.\n\n> (B4) Algorithm 1 talks about a discriminator $c^\\star$ mapping state to predicted probability that the state is from $d_E$ vs $d_O$. However, this discriminator is not mentioned anywhere in the main text and can be confused with the skill discriminator.\n\nThank you for pointing that out.\nWe have updated the paper with a clearer separation between state-discriminator $c^{\\star}(s)$ and skill-discriminator $q(z|s)$.\n\n> (B5) Phase 2 in Algorithm 1 mentions training \n $\\pi_z^*$ when Section 3.2.1 seems to say the policy is trained in Phase 1 instead.\n\nThank you for pointing that out.\nWe have updated the paper accordingly.\n\n> (B6) Section 5, Related Work, reads more like a laundry list of prior papers. There is no comparison at all to this paper\u2019s proposed method, DOI.\n\nThank you for your valuable suggestion.\nWe have refined the \"Related Work\" section to emphasize the differences between our work and the existing literature and to improve overall clarity.\n\n> (B7) Real robot results are not discussed in Section 6.\n\nIt is important to note that due to space limitations, we have deferred further discussion of the real robot results to the Supplementary Material (F, G, H). \nHowever, the data collection for training the policy was discussed in Section 6.\n\n> (B8) Tables S2 (Walker2D) and S3 (HalfCheetah) contain the exact same entries (at least for the ~20 cells I looked at). This seems like an unfortunate mistake.\n\nWe thank the reviewer for noticing this, this was indeed a mistake and we corrected it in the revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692929764,
                "cdate": 1700692929764,
                "tmdate": 1700733097338,
                "mdate": 1700733097338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KS2buzhdai",
                "forum": "KAseclJyj5",
                "replyto": "sk3bFbRiOJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q1. How would DOI be extendable to a continuous skill space?\n\nThank you for raising this important question.\nThe short answer is: by replacing the uniform categorical distribution $p(z)$ over the set $Z$ of indicator vectors in $\\mathbb{R}^{|Z|}$, with a continuous uniform distribution over the surface of a unit ball $\\mathcal{S}^{d-1}$ in $\\mathbb{R}^d$.\nHowever, it is important to realize that the skill-discriminator $q:\\mathcal{S}\\rightarrow\\triangle_{\\mathcal{S}^{d-1}}$ now maps states to continuous probability distribution over the latent skill space $\\mathcal{S}^{d-1}$, and computing this probability distribution is prohibitively expensive in practice.\nWhile overcoming this challenge in our setting is an interesting open question, it is important to emphasize that addressing it is beyond the scope of this paper.\n\n> Q2. How did authors define the skill space  for each of the different environments? ...\n\n$Z$ is a discrete set of $|Z|$ many distinct indicator vectors in $\\mathbb{R}^{|Z|}$.\nMore specifically, $|Z|=5$ for all environments.\nFor Solo12, this is shown in Figure 3, each color corresponds to a different skill.\n\n> Q3. What does $N$ mean in Tables S2 and S3?\n\n$N$ is the number of expert trajectories that are mixed-in into the offline dataset (but not labeled), as done in SMODICE.\n\n> Q4. What does the color-coding in Figure 3a mean? Is it the same color meaning as Figure 3b?\n\nEach skill is a different color.\nFigures 3a and 3b show that the importance ratios of different skills are evaluated on one million data points (which are sub-sampled).\n\n> Q5. In Algorithm 1, the reward is defined as function of the output of $c^{*}$, which could easily be confused with the reward terms mentioned in Equations 8 and 14. How is this reward term, which doesn\u2019t depend on actions, used to compute importance sampling ratios $\\eta\\_{\\tilde{E}}(s,a)$, which does?\n\nThank you for raising this important question.\nThe short answer is: applying Phase 1 with reward $R(s,a)$ is in fact the call to the SMODICE algorithm, cast into our more general framework.\nIntuitively, the reward $R(s,a)=\\log\\frac{c^{\\star}(s)}{1-c^{\\star}(s)}$ does not depend on the action, since the objective in SMODICE is to compute a state occupancy $d\\_z(S)$ that minimizes the KL-divergence to an expert state occupancy $d\\_E(S)$.\nFor a detailed discussion, see Section Unconstrained Formulation in Appendix D.\nNonetheless, following the reviewer's comment, we updated Algorithm 1 to make it clearer that in Phase 1, the Value function $V\\_{z}^{\\star}$ is optimized with respect to the reward $R\\_{z}^{\\mu}(s,a)$ which is defined in Eq. (15).\n\n> Q6. Figure 3a: x-axis is confusingly labeled \u201cdata.\u201d Section 6 describes this as \u201cacross the dataset assignment.\u201d What do these things mean?\n\nThank you for pointing that out.\nAlong the x-axis, we plot the state-action $(s,a)$ pairs from the offline dataset.\nThe figures illustrate a clustering effect on the data induced by the discovered skills.\n\n> Q7. I suggest using a slightly more descriptive title for the paper.\n\nFollowing the reviewer's suggestion, we updated the title to: ``DOI: Offline Diversity Maximization under Imitation Constraints''."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693300691,
                "cdate": 1700693300691,
                "tmdate": 1700727831834,
                "mdate": 1700727831834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5FB1iZUiNN",
            "forum": "KAseclJyj5",
            "replyto": "KAseclJyj5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission701/Reviewer_iADi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission701/Reviewer_iADi"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the question of learning diverse skills from offline dataset while being close to expert. The objective is formulated and existing ideas are used to solve the combined objective effectively. The resulting method is tested on offline datasets from D4RL and a real robot to demonstrate the diversity induced by their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents a new objective for unsupervised skill learning from offline datasets - maximize mutual information combined with staying close to expert. To facilitate this, the method leverages algorithms from previous works that are off-policy in nature. To solve the mutual information objective they use the variational lower bound previously seen in DIAYN, to optimize the KL-constraint they use DICE method and for optimizing objective and constraint jointly the lagrangian method is used, commonly seen in safe RL literature. The combination results in a new off-policy skill discovery method.\n2.  The paper empirically demonstrates effectiveness of their method on simulated tasks on D4RL by comparing metrics for diversity using the offline dataset. They show the tradeoff of contraint vs diversity and the expected difference of importance ratio.\n3. The paper also test the algorithm on a real quaduped which learns gaits with various heights."
                },
                "weaknesses": {
                    "value": "1. The paper misses to explain the motivation behind proposing the objective:\n\nWhy stay close to expert? If the objective is to generate diverse skill, what is the objective of incoporating the constraint of staying close to expert. An explanation through examples might help to motivate the paper better.\nKL divergence to expert: Any objective that uses KL divergence is extremely sensitive if the learned skill goes out of support of the expert dataset. It is motivated in the paper the skill should imitate some part of expert but this is not what is enforced by the KL divergence.\n\n2. Theoretical contributions: I believe the lemma\u2019s are minor variations over previous works in DICE space [1,2,3,4] which might be discussed and compared to more thoroughly. \n3. Evaluation:\n    1. Online evaluation: The paper currently only plots metrics on offline dataset. I believe this is not the correct metric. A learned visitation distribution might not be practically feasible although theoretically it should be. One way to test it in simulated domains, is to roll out pi_z for different skills and compare the resulting visitation distribution. \n    2. Qualitative Diversity of skill: An important part of the evaluation process should be a qualitative comparison of the skills the algorithm learns. If the algorithm learn meaningless skills it would be clear from the deployed policy. \n    3. Baselines and Quantitative diversity of skill: No prior methods for skill discovery are compared against. A standard comparison might be the resulting estimated mutual information of skills between prior methods and DOI. Although prior methods are not developed for offline setting, a simple extension would be to pair them with offline RL. Example: DIAYN could be combined with IQL instead of SAC."
                },
                "questions": {
                    "value": "1. In the paragraph after Figure 3, How does skill conditioned variant of SMODICE does not have a discriminator?  SMODICE itself learns a discriminator in the original method which estimates ratio of expert to offline data. Do you mean the skill discriminator? \n2. I am not sure how to compute the expectation of importance ratio for different skills. Is the expectation over the offline dataset?\n3. For SOLO12, the data is already generated by a skill-based algorithm which seems their is a strong prior to recovering the same skills as the original algorithm? Can you ablate how the learned skills are different from skills found by Domino?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794128432,
            "cdate": 1698794128432,
            "tmdate": 1699635997080,
            "mdate": 1699635997080,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F9hwWebj02",
                "forum": "KAseclJyj5",
                "replyto": "5FB1iZUiNN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 1. The paper misses to explain the motivation behind proposing the objective: Why stay close to expert? If the objective is to generate diverse skill, what is the objective of incorporating the constraint of staying close to expert. An explanation through examples might help to motivate the paper better.\n\nThank you for raising this important question.\nA central topic in computer game development is how to create an authentic real-world experience using a rich set of non-player characters.\nOur work makes a step towards achieving this goal, by proposing a principled framework that captures the core challenge.\nGiven inexpensive and easily collectable expert state demonstrations, our algorithm generates a diverse set of skills that imitate to some degree the expert.\nIn this way, each skill behaves slightly differently while visiting a similar state trajectory as the expert, meaning that it behaves similarly to the expert or, more broadly, mimics the expert's style.\nCrucially, our algorithm learns the sequence of actions that lead to these behaviors offline!\nWe have updated the paper accordingly.\n\n> 1.1 KL divergence to expert: Any objective that uses KL divergence is extremely sensitive if the learned skill goes out of support of the expert dataset. It is motivated in the paper the skill should imitate some part of expert but this is not what is enforced by the KL divergence.\n\nThank you for the insightful comment. \nIn fact, our offline estimator of the KL divergence, uses ratios $\\eta_{z}(s,a)$ and $\\eta_{\\widetilde{E}}(s,a)$ that are computed only on state-action pairs within the offline dataset $\\mathcal{D}_O$.\nFurthermore, in practice, we ensure that these ratios are strictly positive, so that the KL estimator $\\phi_z$ is well defined and bounded.\nAlthough for simplicity of presentation we choose to state our algorithm in terms of KL divergence, as done in previous work, all of our results easily generalize to f-divergence.\nIn particular, for the camera-ready version, we will present an additional set of experiments with $\\chi^2$ divergence and report them in the appendix.\n\n> 2. Theoretical contributions: I believe the lemma\u2019s are minor variations over previous works in DICE space [1,2,3,4] which might be discussed and compared to more thoroughly.\n\nThank you for your valuable suggestion.\nWe have updated section \"4.2 Approximation Phases\" accordingly.\n\n> 3.1 Online evaluation: The paper currently only plots metrics on offline dataset. I believe this is not the correct metric. A learned visitation distribution might not be practically feasible although theoretically it should be. One way to test it in simulated domains, is to roll out $\\pi_z$ for different skills and compare the resulting visitation distribution.\n\nThank you for the insightful suggestion. \nIn fact, we consider the expected successor feature distance in $\\ell_2$ norm over an initial state distribution, which is an online Monte Carlo estimate (policy rollout) that we specify on page 8 and include as an evaluation metric for both Solo12 and D4RL environments.\n\n> 3.2 Qualitative Diversity of skill: An important part of the evaluation process should be a qualitative comparison of the skills the algorithm learns. If the algorithm learn meaningless skills it would be clear from the deployed policy.\n\nThank you for your valuable suggestion.\nHowever, the skills were qualitatively evaluated on the Solo12, see Appendix G, and videos were provided on the website."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684538728,
                "cdate": 1700684538728,
                "tmdate": 1700693518296,
                "mdate": 1700693518296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0WjS9EiYdY",
                "forum": "KAseclJyj5",
                "replyto": "5FB1iZUiNN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 3.3 Baselines and Quantitative diversity of skill: No prior methods for skill discovery are compared against. A standard comparison might be the resulting estimated mutual information of skills between prior methods and DOI. Although prior methods are not developed for offline setting, a simple extension would be to pair them with offline RL. Example: DIAYN could be combined with IQL instead of SAC.\n\nThank you for raising this important question.\nThe short answer is: there is no baseline in the offline setting.\nIt is crucial to emphasize the unique characteristics of our problem formulation.\nMore specifically, we consider offline RL imitation given state-only expert demonstrations, while seeking to maximize the diversity of imitating skills.\nFirst, an extensive body of work on DICE-based algorithms has shown that offline RL imitation based on Fenchel duality achieves state-of-the-art results on the task of imitating an expert from state-only demonstrations.\nIn our setting, however, in addition to the constraint that each skill imitates an expert (in the above sense), we seek to maximize the skill diversity.\nSecond, previous work on unsupervised skill discovery reduces maximizing diversity to online training of a policy and a skill discriminator.\nIt is worth noting that while there are efficient offline RL algorithms for learning a policy that maximizes a fixed reward, they cannot be applied here because our constraint optimization setting involves a non-stationary reward that depends on: i) the log-likelihood of the skill discriminator; and ii) the Lagrange multipliers.\nIn addition, training the skill discriminator and estimating the KL divergence both offline remains a challenge for non-DICE-based algorithms.\nIn summary, we extend DICE-based offline imitation to diversity\nmaximization with imitation constraints, by utilizing the importance-weighted\noccupancy ratios to train a skill discriminator and to estimate KL divergence\noffline.\nWe have added this explanation more clearly in the paper.\n\n> Q1. In the paragraph after Figure 3, How does skill conditioned variant of SMODICE does not have a discriminator? SMODICE itself learns a discriminator in the original method which estimates ratio of expert to offline data. Do you mean the skill discriminator?\n\nThank you for pointing that out.\nThe short answer is yes, we mean the skill discriminator.\nIn more details: although SMODICE trains a state discriminator $c(s)$, it is important to realize that in our context, the state discriminator $c(s)$ is encapsulated in the preprocessing call to SMODICE.\nThat is, our algorithm has access only to the returned importance ratios $\\eta_{\\widetilde{E}}(s,a)$.\nEach skill of SMODICE$^{\\dagger}$ maximizes the reward function $\\log\\eta_{\\widetilde{E}}(s,a)$ without using the skill discriminator $q(z|s)$, as it only seeks to imitate without diversifying, $\\sigma(\\mu_z)=1$.\nWe have updated the paper accordingly.\n\n> Q2. I am not sure how to compute the expectation of importance ratio for different skills. Is the expectation over the offline dataset?\n\nThank you for raising this important question.\nIt is indeed valuable to understand how the DICE-based framework is used to design an offline expectation estimation scheme.\nOur goal is to compute offline primal optimal ratios $\\eta\\_z(s,a)=d\\_z^{\\star}(s,a)/d\\_O(s,a)$ of state-action occupancies with respect to a fixed reward for each state-action pair $(s,a)$ in the offline dataset $\\mathcal{D}\\_O$, see Problem (10).\nTo achieve this, we solve offline the dual problem that yields an optimal Value function $V^{\\star}$, see Eq. (11).\nA fundamental theorem in Fenchel duality states that given the optimal Value function $V^{\\star}$, the primal optimal ratios $\\eta_z(s,a)$ admit a close-form solution, i.e., softmax of the TD error w.r.t. $V^{\\star}$, see Eq. (12).\nThese ratios $\\eta_z(s,a)$ are then used to design an offline importance-weighted sampling procedure that, for an arbitrary function $f$, satisfies $\\mathbb{E}\\_{d\\_{z}^{\\star}(s,a)} [f(s,a,z)] = \\mathbb{E}\\_{d_{O}(s,a)}[\\eta\\_{z}(s,a) f(s,a,z)]$.\nThen, the optimal skill-conditioned policy $\\pi\\_z^{\\star}$ is trained offline using a weighted behavioral cloning, which is obtained by setting $f(s,a,z)=\\log(\\pi\\_z(a|s))$.\nFurthermore, we can train offline the skill discriminator by setting $f(s,a,z)=\\log(q(z|s))$ (see Lemma 4.2), and we can estimate the KL divergence by setting $f(s,a,z)=\\log( \\eta\\_z(s,a)/\\eta_{\\widetilde{E}}(s,a) )$ (see Lemma 4.3)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685103242,
                "cdate": 1700685103242,
                "tmdate": 1700693443293,
                "mdate": 1700693443293,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sWQ69J9Zts",
            "forum": "KAseclJyj5",
            "replyto": "KAseclJyj5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission701/Reviewer_gvyu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission701/Reviewer_gvyu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel algorithm to connect Fenchel duality, reinforcement learning, and unsupervised skill discovery to maximize a mutual information objective subject to KL-divergence state occupancy constraints. This approach is used to diversify offline policies for a 12-DoF quadruped robot and several environments from the standard D4RL benchmark in terms of both \u21132 distance of expected successor features and \u21131 distance of importance ratios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strength of this paper is that it proposes a principled offline algorithm for unsupervised skill discovery that maximizes diversity while ensuring each learned skill imitates state-only expert demonstrations to a certain degree. In order to compute the optimal solution to the problem formulation, the authors propose to use an approximation algorithm \"alternative optomization\". The authors demonstrate the effectiveness of the method on standard offline benchmarks and a custom offline dataset collected from a quadruped robot. The resulting skill diversity naturally entails a trade-off in task performance, which can be controlled via a KL constraint level \u03f5."
                },
                "weaknesses": {
                    "value": "1. In the experiment section, it will be good to see some comparison between the proposed method with other state-of-the-art methods for unsupervised skill discovery. \n2. Computational complexity of the proposed algorithm is not mentioned in the paper. \n3. The paper does not provide a comprehensive evaluation of the proposed method on a wide range of tasks and environments."
                },
                "questions": {
                    "value": "Is there a convergence analysis of the algorithm? What if the algorithm does not converge?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission701/Reviewer_gvyu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821295829,
            "cdate": 1698821295829,
            "tmdate": 1699635997009,
            "mdate": 1699635997009,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BfPd48tto2",
                "forum": "KAseclJyj5",
                "replyto": "sWQ69J9Zts",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission701/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 1. In the experiment section, it will be good to see some comparison between the proposed method with other state-of-the-art methods for unsupervised skill discovery.\n\nThank you for raising this important question.\nThe short answer is: there is no baseline in the offline setting.\nIt is crucial to emphasize the unique characteristics of our problem formulation.\nMore specifically, we consider offline RL imitation given state-only expert demonstrations, while seeking to maximize the diversity of imitating skills.\nFirst, an extensive body of work on DICE-based algorithms has shown that offline RL imitation based on Fenchel duality achieves state-of-the-art results on the task of imitating an expert from state-only demonstrations.\nIn our setting, however, in addition to the constraint that each skill imitates an expert (in the above sense), we seek to maximize the skill diversity.\nSecond, previous work on unsupervised skill discovery reduces maximizing diversity to online training of a policy and a skill discriminator.\nIt is worth noting that while there are efficient offline RL algorithms for learning a policy that maximizes a fixed reward, they cannot be applied here because our constraint optimization setting involves a non-stationary reward that depends on: i) the log-likelihood of the skill discriminator; and ii) the Lagrange multipliers.\nIn addition, training the skill discriminator and estimating the KL divergence both offline remains a challenge for non-DICE-based algorithms.\nIn summary, we extend DICE-based offline imitation to diversity maximization with imitation constraints, by utilizing the importance-weighted\noccupancy ratios to train a skill discriminator and to estimate KL divergence offline.\nWe have added this explanation more clearly in the paper.\n\n> 2. Computational complexity of the proposed algorithm is not mentioned in the paper.\n\nThank you for raising this important remark.\nMaximizing mutual information (a convex function) is generally intractable, and as a trackable surrogate we consider instead a variational lower bound.\nIt is standard in previous work to use an alternating scheme heuristic that involves two phases: a policy and a skill discriminator optimization.\nThis heuristic does not provide computational complexity or convergence guarantees.\nIn addition, the non-stationarity of the reward signal in our setting also depends on Lagrange multipliers, which makes the resulting three-phase optimization problem more challenging.\n\n> 3. The paper does not provide a comprehensive evaluation of the proposed method on a wide range of tasks and environments.\n\nOur experiments on Solo12 and D4RL environments strongly support the effectiveness of our novel problem formulation. \nBy simultaneously addressing the challenges of state-only expert imitation and skill diversity maximization, our proposed algorithm is the first principled offline approach for this novel setting.\n\n> 4. Is there a convergence analysis of the algorithm? What if the algorithm does not converge?\n\nTo our understanding reviewer's comment 2) and this one are closely related. \nIt is crucial to emphasize that the commonly used alternating optimization scheme is a heuristic and does not provide computational complexity or convergence guarantee.\nIn practice, the algorithm converges to some local optimum."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684040529,
                "cdate": 1700684040529,
                "tmdate": 1700693543760,
                "mdate": 1700693543760,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]