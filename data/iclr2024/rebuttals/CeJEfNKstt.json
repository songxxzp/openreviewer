[
    {
        "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets"
    },
    {
        "review": {
            "id": "fJauhqDto7",
            "forum": "CeJEfNKstt",
            "replyto": "CeJEfNKstt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3134/Reviewer_LibM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3134/Reviewer_LibM"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines a simple probe on the linear separability of binary factuality statements using Llama as a backbone. The analysis seem thorough and techniques all start with the most immediate things at hand, which is a good thing. There is an interesting observation that the PCA plots of true-false statements are so linearly separable. If broadly studied, such pattern could potentially be exploited to improve LLMs during training to make larger impact."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation is good. Interpretability works often suffers from weak generalization and undetermined thresholding on generalization. This paper steps in this problem by observing the conditions of generalization."
                },
                "weaknesses": {
                    "value": "The takeaway is unclear. The beautify linear separability could be because of the simplicity of the text in the curated dataset. In reality, this could be a luxury to have. Then, the usefulness of the proposed probe and establishment of the observations need to be re-examed.\n\nAnother thing is, what can people do with this problem is unclear. Can folks use the observation, say, inject a loss to improve LLM's factuality during pretraining or fine-tuning? There are some interesting discussions could happen, but not appeared in this paper.\n\nThe causal interference happens at hidden level instead of word/token level. Roughly saying, most probes can have a hidden interference to revert the binary output. But what's is more direct/interpretable is make it also work on text level."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791283919,
            "cdate": 1698791283919,
            "tmdate": 1699636260475,
            "mdate": 1699636260475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LGiRXIGlKo",
                "forum": "CeJEfNKstt",
                "replyto": "fJauhqDto7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review! We\u2019re glad that you found our work well-motivated and that you feel we have done a good job of establishing generalization. Below we will address the weaknesses you raised.\n\n> The takeaway is unclear. The beautify linear separability could be because of the simplicity of the text in the curated dataset. In reality, this could be a luxury to have. Then, the usefulness of the proposed probe and establishment of the observations need to be re-examed.\n\nFirst, to clarify: the clear separation in the top principal components for our curated datasets **_is_** due to their simplicity. This is because in order for true vs. false to be the main axis of variation for some data, it\u2019s essential that the data not vary too much with respect to other features. To get a feeling for this, please see the visualizations of our uncurated datasets in appendix B. An especially interesting case is companies_true_false: there the top two PCs primarily encode the difference between three clusters, where the clusters correspond to three templates used by the dataset authors (Azaria & Mitchell). We have added some text in section 3 clarifying this point.\n\nThat said, an important finding in our paper is that the true/false data is still linearly separable for the uncurated datasets (even if this separation mostly doesn\u2019t appear in the top PCs for reasons of data diversity). This is why probes trained on curated datasets are able to generalize to the uncurated datasets! In other words, the simplicity of our curated datasets does not diminish the usefulness of the resulting probes, since we show that probes trained on this simple data are able to generalize to realistic data, like the statements from common_claim_true_false.\n\n(To give a sense, here are two randomly sampled statements from common_claim_true_false: \u201cCitric acid can be used as an eco-friendly cleaning agent.\u201d (true). \u201cOuter space has a sweet, almost marshmallow-y smell.\u201d (false).)\n\n> Another thing is, what can people do with this problem is unclear. Can folks use the observation, say, inject a loss to improve LLM's factuality during pretraining or fine-tuning? There are some interesting discussions could happen, but not appeared in this paper.\n\nGood question! Our intended application of this work is actually quite concrete; we\u2019ll briefly explain it here, and a forthcoming revision will include a longer discussion in a new appendix.\n\nSuppose you are a human evaluator tasked with evaluating LLM outputs for the purpose of RLHF. The LLM has just produced an output that you are having trouble understanding, say a sample of complex code, or a recommendation for an economic policy. Unaided, you will sometimes make mistakes in evaluating the quality of this output. But the LLM, which we\u2019ll imagine has expert-level capabilities in the domain you\u2019re evaluating, likely knows whether there are any issues with the output (e.g. bugs in the code or flaws in the policy argument). If you could gain access to the LLM\u2019s knowledge, you could do a better job of evaluating the output than if you go unaided. Concretely, this might look like checking whether the LLM believes the statement \u201cThis code does not have any bugs\u201d is true, using techniques like those we present here.\n\n> The causal interference happens at hidden level instead of word/token level. Roughly saying, most probes can have a hidden interference to revert the binary output. But what's is more direct/interpretable is make it also work on text level.\n\nWe are having a hard time understanding what you mean by this suggestion \u2013 would you mind please clarifying? Perhaps you are thinking of something along the lines of the patching experiments we have added to section 5.1? In these experiments we run a true and false input through the model (e.g. \u201cThe city of Tokyo is in Japan\u201d and \u201cThe city of Tokyo is in Pakistan\u201d); then we one-at-a-time swap each hidden state from the true run into the corresponding position of the false run and record P(TRUE) - P(FALSE). If a hidden state has a large positive effect on P(TRUE) - P(FALSE), it means that this hidden state must have been storing information relevant to the model\u2019s decision to output TRUE vs. FALSE.\n\nPlease let us know if you have any further questions!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268695568,
                "cdate": 1700268695568,
                "tmdate": 1700681454393,
                "mdate": 1700681454393,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HD0KELwY8x",
            "forum": "CeJEfNKstt",
            "replyto": "CeJEfNKstt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3134/Reviewer_hn5S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3134/Reviewer_hn5S"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigated the internal representations of a language model known as LLaMA and identified that the truth values of given sentences are represented in a certain linear direction. The key contributions of the authors include:\n- Development of a dataset consisting of declarative sentences related to world knowledge.\n- Introduction of the mass-mean probing approach, which defines the direction of truth values by connecting the centroids of the TRUE and FALSE classes.\n- Experimental verification that the direction of truth values appears in specific dimensions of specific internal representations of the specific LLM.\n- Validation that the direction of truth values is somewhat shared across multiple sub-datasets.\n- Confirmation that applying perturbations to the direction of truth values can lead to a reversal in the truth value of the given sentence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Identifying the knowledge held by LLM and discovering the correspondence between its internal representations and world knowledge is crucial for realizing a trustworthy LLM. Particularly, the identification of the truth values of declarative plain sentences related to world knowledge aligns well with fundamental paradigms in semantics within NLP, such as truth-conditional semantics. The theme of the paper is likely to be well-received within the community.\n- Controlled datasets created to measure only specific aspects of meaning will likely be useful for researchers studying the truthfulness of sentences."
                },
                "weaknesses": {
                    "value": "The experimental setup appears arbitrary and limited, diminishing the persuasiveness of the authors' general claims.\n\nFor instance, despite the availability of numerous pretrained language models, experiments were conducted solely on LLaMA, making it unclear whether the findings apply generally to LLMs. The authors explicitly mention this as a limitation, which should be acknowledged for its intellectual honesty. However, it is hard to deny that the verification is lacking when considering the subject of interest stated in the main claim (Large Language Models). If the focus was merely on the \"Linear Structure in LLaMA\", the experiments would suffice. Yet, in that case, it might not be deemed impactful enough for acceptance at ICLR.\n\nFurthermore, although there are numerous internal representations available for sentence representations, experimental results are provided only under very specific settings: a specific layer, specific parts of the network (after the residual stream), and right after period characters. Additional settings, such as the utilization of top principal component directions and connecting centroids, also follow specific configurations. Taking into account that multiple options exist for each of these aspects, it becomes somewhat challenging to dispel concerns that the reported results might be cherry-picked."
                },
                "questions": {
                    "value": "Experiments related to \"causality\" were conducted using latent representations, and the correspondence with the world of language (where meaning is directly encoded) remains unclear. For example, does the perturbation that converts from TRUE to FALSE correspond to the insertion of a negation word?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838609915,
            "cdate": 1698838609915,
            "tmdate": 1699636260398,
            "mdate": 1699636260398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HD3Rpz1yUd",
                "forum": "CeJEfNKstt",
                "replyto": "HD0KELwY8x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your thoughtful review! Given the importance of the question being studied, we agree that it\u2019s crucial that the results not be cherry-picked. To this end, we have taken the a number of steps to validate our results in other settings and remove degrees of freedom which could be used for cherry-picking:\n\n**1. Drastically reducing the number of degrees of freedom by standardizing a number of choices as follows:**\n\n* We standardize the selection of layers to probe and intervene on using a patching experiment \u2013 please see the global comment. In brief, by swapping hidden states from true inputs to false inputs, we can detect (a superset of) the hidden states which might carry a representation of truth.\n\n* For probing and visualizations, we extract representations over the period token in the last layer of the range. That is because this is the \u201cmost downstream\u201d hidden state of all the hidden states which encode the relevant information.\n\n* To simplify and bolster our causal intervention experiments, we have made the following change to our previous experiment: instead of adding in a probe direction in only a single layer, we instead intervene on all of the hidden states in this group. This is for two reasons: (1) our results in the revised section 5.1 show that the model already begins to use the early hidden states in this group to start planning its answer (\u201cTRUE\u201d or \u201cFALSE\u201d); (2) it seems more principled to add truth vectors to all of the states encoding truth values, rather than an arbitrary subset of them.\n\n* We also note that in our revised experiments, there is no longer a sweep over alphas \u2013 we simply add in a (normalized) probe direction with no further choice of scaling.\n\n**2. Replicating our results for LLaMA-2-13B,** please see the global comment. We are happy to report that the results for LLaMA-2-13B are qualitatively identical to those for LLaMA-13B. We note that we use the same procedure outlined above for choosing the model components from which we extract activations and perform interventions to ensure that we are performing the same experiment from model to model.\n\nWe think that the above steps eliminate a significant majority of the possible degrees of freedom. Are there additional degrees of freedom you would like to see us eliminate, or other validation you would like us to perform, in order for you to have confidence in the results? \n\nWe also note that we would love to validate these results on even more models; however, when moving to less capable models, the model\u2019s understanding of the statements in our datasets quickly comes into doubt. If there are particular models you would especially find it useful to see our results replicated for (e.g. would you find it useful to see a replication for LLaMA-70B? for a fine-tuned model like Vicuna-13B? for a model outside of the LLaMA series?), please let us know!\n\n> Experiments related to \"causality\" were conducted using latent representations, and the correspondence with the world of language (where meaning is directly encoded) remains unclear. For example, does the perturbation that converts from TRUE to FALSE correspond to the insertion of a negation word?\n\nThat\u2019s a very good question! One investigation we performed in this direction was to apply the LM\u2019s decoder head to the vectors we used for interventions. If this direction encoded \u201cnot,\u201d then we would expect to see its decoding have a large weight on the \u201cnot\u201d logit. Instead, here are the top k=3 tokens with the highest logits for one of our truth directions and its negation:\n\nTruth direction: \u043b\u044f, PD, irat\n\nNegative of truth direction: uerdo, (?, ouvelle\n\nAll of the probabilities on these tokens were very small (less than 0.001). A good point of comparison is the group (c) hidden states mentioned in our revised section 5.1: decoding these hidden states with the LM\u2019s decoder head produces tokens like \u201ctrue,\u201d \u201cTrue,\u201d and \u201cTRUE\u201d with probabilities in the .1-.5 range.\n\nSo it seems that the probe directions don't encode anything which is naturally expressible in the vocabulary space. Is this the sort of thing you were wondering? We\u2019re happy to discuss this question further!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268646958,
                "cdate": 1700268646958,
                "tmdate": 1700681551303,
                "mdate": 1700681551303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hhsMySnYaZ",
            "forum": "CeJEfNKstt",
            "replyto": "CeJEfNKstt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3134/Reviewer_WvMz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3134/Reviewer_WvMz"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a few new datasets and methods to identify whether LLMs have a \"truth vector\" in their representation spaces: a direction pointing from true to false statements.  They carry this out using one model (LLaMA 13B) and several methods: some data is curated/synthetic, with clear and unambiguous truth labels, while others are more open-ended.  They compare standard probing on these data to \"mass-mean probing\", which does not require training (it defines the vector pointing from the mean of the inputs corresponding to one label to those of the other label and then passes it through a logistic).  They find promising results: aside from the probes finding linear truth information, many of them also transfer from one dataset to another (i.e. trained on one dataset, tested on another), suggesting a general/multi-purpose truth direction.  Similarly, the mass-mean probed vector can be used for causal intervention, to flip a model's judgment from true to false and vice versa."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Detailed analysis of whether an LLM can learn to distinguish true and false data within and across tasks.\n- Good use of synthetic and natural data for this purpose.\n- Mass-mean probing seems to be an effective method for inducing probes _without training_ and could be more broadly applicable.\n- Causal intervention on the model's representations using the probes shows that the truth vectors are \"active\" and not \"inert\".  (This also demonstrates the importance of working with open models, where such interventions can be done.)"
                },
                "weaknesses": {
                    "value": "- The paper has so many experiments and results that it could benefit from a richer discussion of how to interpret all of the results.\n- Only tests one model, so it's unclear how generalizable the findings are (the authors, of course, acknowledge this).\n- I would have also appreciated a bit more detail about the datasets and how they were generated to be in the body of the paper instead of appendices."
                },
                "questions": {
                    "value": "- Table 2: do you have any intuition for why the False->True direction is so much worse for LR than mass-mean?  This seems especially stark.\n- Page 8, $\\ell = 10$ for the intervention experiments: why that layer (especially since layer 12 was used earlier)?\n- Is there a natural way of generalizing mass-mean probing to multi-class tasks beyond binary ones?\n- Table 2: the $\\alpha$ values seem extremely large here, to the point where the resulting vector is almost going to look just like the truth vector.  How do you think about these large values?  (Does it make sense to do a baseline where the model just sees $\\alpha\\theta$?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698952188781,
            "cdate": 1698952188781,
            "tmdate": 1699636260318,
            "mdate": 1699636260318,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f919mHnH2v",
                "forum": "CeJEfNKstt",
                "replyto": "hhsMySnYaZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your positive review! We\u2019re glad that you thought we provides a detailed analysis of the ways LLMs represent true vs. false data, and that you appreciated the mass-mean probing method. We\u2019ll address the weaknesses and questions you raised below.\n\n> The paper has so many experiments and results that it could benefit from a richer discussion of how to interpret all of the results.\n\nWe agree that this paper is packed with experiments. We have added some exposition to the experiment sections to more fully explain how we interpret the results.\n\n> Only tests one model, so it's unclear how generalizable the findings are (the authors, of course, acknowledge this).\n\nWe have now added confirmatory results on LLaMA-2-13B! Please see the global comment.\n\n> I would have also appreciated a bit more detail about the datasets and how they were generated to be in the body of the paper instead of appendices.\n\nThank you for this feedback! We\u2019d be happy to add more detail on the datasets. Is there anything in particular you would find it useful to know about the datasets which you would like for us to discuss in the body of the paper?\n\n> Table 2: do you have any intuition for why the False->True direction is so much worse for LR than mass-mean? This seems especially stark.\n\nThis is an excellent question! We\u2019re not sure why there is an asymmetry. One possibility is that this has to do with the model having an asymmetric preference for outputting \u201cTRUE\u201d vs. \u201cFALSE.\u201d For instance, we noticed that the few-shot prompts which resulted in the best performance had 4/5 statements which were \u201cTRUE,\u201d suggesting that the model might have had a bias for outputting \u201cFALSE\u201d which needed to be corrected.\n\n> Page 8, l = 10 for the intervention experiments: why that layer (especially since layer 12 was used earlier)?\n\nGood question! Intuitively, one should want to perform an intervention on a representation of truth as soon as it begins to form; in contrast, one wants to probe a concept of truth later in the model, after the representation has finished forming. This is why we performed our intervention in an earlier layer than we did our probing experiments, with the earlier layer originally chosen in an ad-hoc way. \nBut we\u2019re happy to say that we now have a more principled way to select the layers at which to probe and intervene! For the full details, please see the global comment about the newly added patching experiment. In short, we swap certain hidden states from a true input into the forward pass from a false input and measure which hidden states have a large effect on the model outputting \u201cTRUE\u201d vs. \u201cFALSE.\u201d This reveals that the relevant information seems to be encoded above two tokens \u2013 the final token of the statement and the period token \u2013 in a range of layers. For probing, we choose the latest such layer, since that\u2019s the layer at which most truth-relevant information is present. In our modified and simplified causal intervention experiments, we intervene at all hidden states in this range.\n\n> Is there a natural way of generalizing mass-mean probing to multi-class tasks beyond binary ones?\n\nGreat question, and indeed there is! Suppose X is a random vector and f(X) is some labeling function feature (not necessarily binary). Consider the direction theta such that the orthogonal projection proj_theta away from theta makes it as hard as possible for a linear probe to predict f(X) from proj_theta(X). Then, in the case that f(X) is binary label, theta coincides with the mass-mean direction! (The proof is theorem 3.1 here https://arxiv.org/pdf/2306.03819.pdf.) But even if f(X) is continuous, the definition makes sense, and can be computed as in the linked paper. This would give a reasonable generalization of mass-mean probing. We are saving this idea for future work.\n\n> Table 2: the values seem extremely large here, to the point where the resulting vector is almost going to look just like the truth vector. How do you think about these large values? (Does it make sense to do a baseline where the model just sees alpha * theta?)\n\nIn the new revision, we have considerably improved and simplified this experiment, so that it\u2019s now easier to interpret the results. \n\nBut if you\u2019re curious, we\u2019ll also address the question about the original experiment. We included this alpha for two conceptually distinct reasons: (1) because the truth value of the statement might be redundantly encoded in layers other than the one we intervened in, requiring an outsized intervention; and (2) because the truth directions identified by probes might be only partially aligned with the genuine truth direction, which would be reflected in requiring a larger alpha to be causally effective. So when alpha values were large, we were interpreting this as meaning that the probe direction was partially aligned with the truth direction but mostly pointing into an unmeaningful part of the model\u2019s activation space."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268212444,
                "cdate": 1700268212444,
                "tmdate": 1700681234130,
                "mdate": 1700681234130,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y4Lt3MW5CF",
            "forum": "CeJEfNKstt",
            "replyto": "CeJEfNKstt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3134/Reviewer_4DXR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3134/Reviewer_4DXR"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a new probing technique called mass-mean probing that can uncover the truth representations of LLMs. Using this method, the paper found that 1. LLM internally contains a truth representation that is linearly structured. 2. Such representation generalizes to other datasets. 3. Such truth vectors are casually implicated."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A new alternative to regular logistic regression probing is proposed, which overcame the problem of logistic regression where the truth direction may be interfered with by an independent feature. Mass-mean probes show significant improvements in causal interference. \n2. Clear visualization of the separation of True/False statements.\n3. A new dataset to train such linear probes."
                },
                "weaknesses": {
                    "value": "1. The majority of the conclusions and claims are not unique. For example, from Li et al. and Burns et al. (which the paper cites), we already knew that such truth representation is linearly separable and one can apply casual intervention to such representation. \n2. Lack of model variances. The experiments are only conducted on LLaMA-13B. As a result, we don't really know the effects of scale or the effects of the pretraining paradigm on such representations.\n3. Mass-Mean probe doesn't really improve generalization accuracy over some of the other methods that much."
                },
                "questions": {
                    "value": "Question:\n1. Does the truth direction generalize to other LLMs? This may be an interesting direction to explore.\n\nStyling:\n1. The figure on page 17 is disproportionate"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3134/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3134/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3134/Reviewer_4DXR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699386169996,
            "cdate": 1699386169996,
            "tmdate": 1699636260240,
            "mdate": 1699636260240,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v7uNoCkzwy",
                "forum": "CeJEfNKstt",
                "replyto": "y4Lt3MW5CF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Main response to reviewer 4DXR"
                    },
                    "comment": {
                        "value": "Thank you very much for your review! We\u2019re glad that you liked many aspects of our paper, such as the significant improvement mass-mean probing represents over other techniques for purposes of causal interventions. \n\nBefore addressing your comments, one point which we wanted to emphasize is that the contribution of this paper goes beyond mass-mean probing (if it were only mass-mean probing, we might agree with you that this work is not significant enough for a conference paper). Rather, we think that many people will be interested in this work from the perspective of basic science: prior to this work, many of the claims regarding LLM truth representations were on a shaky basis (and therefore controversial), and our paper aims to settle the controversy while establishing a significantly deeper understanding of these truth representations. We\u2019ll go into more detail about this in our second comment addressing the novelty of the contribution.\n\n> The majority of the conclusions and claims are not unique. For example, from Li et al. and Burns et al. (which the paper cites), we already knew that such truth representation is linearly separable and one can apply casual intervention to such representation.\n\nOur paper certainly has significant overlap with the work of Burns et al. and Li et al.! To address this point in more detail, we\u2019ve posted a self-contained comment discussing the state of the prior work and the novelty of our contribution.\n\n> Lack of model variances. The experiments are only conducted on LLaMA-13B. As a result, we don't really know the effects of scale or the effects of the pretraining paradigm on such representations.\n\nWe have now added results for LLaMA-2-13B! Please see the global comment.\n\n> Mass-Mean probe doesn't really improve generalization accuracy over some of the other methods that much.\n\nWe agree with this, though we\u2019ll make some remarks here which put these probing results into context. As revealed by our PCA visualizations, the true and false point clusters are rather far apart in their high-dimensional space. In such a setting, it is actually very easy for a linear probe to have seemingly impressive accuracy: it only needs to have a non-negligible cosine similarity with the genuine feature direction and be oriented in the correct direction. And since there is a diminishing density of points as one moves away from the cluster centers towards the decision boundary, there are also diminishing accuracy gains to better aligning with the genuine feature direction. Thus, it is extremely easy to go from 50% to 75% accuracy (in fact, Roger [1] shows that random directions with only a choice of orientation can often do it!) but harder to go from 80% to 85% accuracy.\n\nIn contrast, we aren\u2019t aware of analogous subtleties for interpreting causal intervention experiments, so we think they give a much clearer picture of which probing methods are best identifying LLMs\u2019 truth directions.\n\n> The figure on page 17 is disproportionate\n\nThank you, we have fixed this.\n\n[1] Fabien Roger. What Discovering Latent Knowledge Did and Did Not Find, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267792238,
                "cdate": 1700267792238,
                "tmdate": 1700267792238,
                "mdate": 1700267792238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ren5D0uDdw",
                "forum": "CeJEfNKstt",
                "replyto": "y4Lt3MW5CF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3134/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "This comment discusses in more detail the state of the knowledge on LLM truth representations prior to this paper and the novelty of our contribution.\n\nAs background, we\u2019ll say that this paper was motivated by wide-spread skepticism about the results of Burns et al. and Li et al. among LM interpretability researchers we\u2019ve spoken with. We didn\u2019t feel it was appropriate to go into detail about the controversy in the paper\u2019s main body, but let us summarize some key points here:\n\n* **The experimental results in Burns et al. are not actually very strong.** While the Burns et al. paper is commendable for introducing a novel technique and perspective, many observers have pointed out that the evidential basis for its claims is weak (see for instance the comments from its review process: https://openreview.net/forum?id=ETKGuby0hcs). Indeed, the results in the main body appear to be cherry-picked, with many LM interpretability researchers citing the full results table (figure 9 in the appendix of Burns et al.) as strong evidence against the paper\u2019s central claims! See also the critique paper by Levenstein & Hermann [1].\n\n* **The setting of Li et al. is not clearly about truth.** Li et al. use the TruthfulQA dataset, consisting of questions which are tricky or misleading in some way (e.g. \u201cDuring the Middle Ages, what did scholars think the shape of the Earth was?\u201d). For around 15% of the data, the \u201ctruthful\u201d response is to not answer at all: \u201cI have no comment.\u201d Given the small model size (Li et al. work with LLaMA-7B) and the ambiguity of the questions, it is unclear whether the directions identified really correspond to truth. Indeed, the interventions of Li et al. also increase the probability of the model outputting \u201cI have no comment.\u201d\n\n* **Burns et al. studies OOD classification accuracy and Li et al. studies IID causal efficacy, but these are both insufficient.** The core concern here is that probes may be picking up on non-truth features which correlate with truth. A probe which attends to all truth-correlated features on a distribution may well be causally implicated in model outputs on that same distribution. But showing that a probe trained to predict truth on one distribution can be used to causally intervene on another distribution is significantly stronger evidence, and has not yet been demonstrated in the literature.\n\nGiven the importance of this topic and the controversy surrounding prior work, we felt it would be a significant contribution to treat the question of LLM truth representations with the rigor it deserved. To this end we:\n\n* **Curated datasets which precisely captured a well-scoped notion of truth**, unlike the long and complicated inputs used in Burns et al. and the intentionally misleading Q/A pairs used in Li et al.\n\n* **Worked with models capable enough, and statements simple enough, that we can expect the LLMs we study to actually understand their inputs.**\n\n* **Verified that the directions we identify are causally implicated OOD.** Note that even though the different probing techniques are comparable for OOD classification, they vary wildly in OOD causal efficacy, indicating that this previously-unstudied question is important for evaluating the validity of truth directions!\n\nFinally, let us reiterate some additional contributions this paper makes which are not present in previous literature:\n\n* **Localizing representations of truth to certain model components**. Especially with the improved and simplified experiments in section 5 (see the global comment), we localize LLMs\u2019 representation of truth to the residual stream in certain layers above certain token positions. (In contrast, Li et al. add their probe directions into dozens of attention heads over every token during generation.) In particular, this reveals an interesting \u201csummarization\u201d behavior, where high-level information about a clause (e.g. its truth value) is stored above the token which marks the end of that clause; this phenomenon was also noted in the concurrent work of Tigges et al. [2]. \n\n* **Studying in detail cases of generalization failure.** Levenstein & Hermann [1] noted that probes trained on some datasets failed to generalize to negations, but incorrectly concluded that this shows LLMs do not represent truth. We dig into this phenomenon in significantly more detail.\n\n* **Studying mass-mean probing and its properties in detail**, including providing a theoretical argument motivating its use. It was this theoretical argument which led us to discover the IID modification of mass-mean probing which significantly improves IID accuracy.\n\nWe have made various updates to relevant sections of the text to clarify these contributions.\n\n[1] B. A. Levinstein and Daniel A. Herrmann. Still no lie detector for language models: Probing empirical and conceptual roadblocks, 2023.\n\n[2] Curt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. Linear representations of sentiment in large language models, 2023."
                    },
                    "title": {
                        "value": "Self-contained comment on prior work and novelty"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267966499,
                "cdate": 1700267966499,
                "tmdate": 1700268031414,
                "mdate": 1700268031414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]