[
    {
        "title": "Adaptive Chameleon  or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts"
    },
    {
        "review": {
            "id": "alRgD39TgQ",
            "forum": "auKAUJZMO6",
            "replyto": "auKAUJZMO6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1967/Reviewer_smQg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1967/Reviewer_smQg"
            ],
            "content": {
                "summary": {
                    "value": "This paper extensively investigates the behaviors of Large Language Models (LLMs) in knowledge conflicts. Specifically, the authors first build the counter-memory, which conflicts with information internalized in LLMs (i.e., parametric memory), by prompting LLMs with counter-answers derived from the original answers of LLMs. Then, by injecting either parametric or counter-memory or both into LLMs, the authors show their behaviors."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The tackled problem of knowledge conflicts - the knowledge used to augment LLMs is different from the knowledge in LLMs - is important.\n* The proposed counter-memory that is constructed by evidence generation from counter-answers, is more convincing to test LLMs in knowledge conflicts, compared to existing methods that simply change words in the ground-truth answer.\n* The authors extensively perform many different analyses, which are interesting and valuable to the community."
                },
                "weaknesses": {
                    "value": "* The quality of the generated counter-evidences from prompting LLMs with counter-examples may be investigated more, perhaps with the human study. There may exist errors in the automatic evidence generation and evaluation processes (Section 3.3 and Section 3.4).\n* The authors may discuss the literature on LLM distraction with irrelevant contexts, for example, \"Large Language Models Can Be Easily Distracted by Irrelevant Context, ICML 2023\", when presenting results with irrelevant evidence. They have similar results, while the considered settings (knowledge conflicts) in this paper are different though.\n* The last paragraph of Section 3.4 is unclear. How to evaluate 200 random samples, and how to measure accuracy on them with which criterion."
                },
                "questions": {
                    "value": "* As described in Section 3.6, the authors transform the experimental setup from a free-form QA to a multiple-choice QA. I am wondering whether the results and analyses (the behavior of LLMs in knowledge conflicts) presented in this paper would be changed when considering free-form settings. Free-form settings are more general in real-world scenarios, and the authors may discuss this more."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1967/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649228808,
            "cdate": 1698649228808,
            "tmdate": 1699636128037,
            "mdate": 1699636128037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gfX7gEFYjk",
                "forum": "auKAUJZMO6",
                "replyto": "alRgD39TgQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer smQg"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your valuable feedback on our paper and are pleased to know that you recognize the significance of our research question and our contribution to the field. We are committed to thoroughly addressing your concerns. \n\n> **Question 1**: The quality of the generated counter-evidences from prompting LLMs with counter-examples may be investigated more.\n\n**Answer 1**:  Please refer to our \"Response to all reviewers Answer2\" for a detailed discussion.\n\n---\n\n> **Question 2**: The authors may discuss the literature on LLM distraction with irrelevant contexts, for example, \"Large Language Models Can Be Easily Distracted by Irrelevant Context, ICML 2023\", when presenting results with irrelevant evidence. \n\n**Answer 2**:  Thank you for bringing this paper to our attention. We have read this work and agree that it provides insightful conclusions relevant to our study. In response to your suggestion, we have updated our submission and cited this paper in the irrelevant evidence part of Section 4.2. We appreciate your valuable recommendation.\n\n---\n\n> **Question 3**: How to evaluate 200 random samples, and how to measure accuracy on them with which criterion.\n\n**Answer 3**:  Thanks for the question. The rationale is the following: we hope to use a state-of-the-art NLI model to further improve the quality of the synthesized data. Before doing that, we need to evaluate the quality of the NLI model itself to see its fitness to this specific purpose. The 200 random examples are a manually labeled dataset for this evaluation:  We sample 200 of the generated examples (including both parametric memory and counter-memory) and manually annotate whether the example entails  the corresponding claim (memory answers and counter-answers). \nThe labels are *supportive* (entailment in the NLI task) or *not supportive* (either neutral or contradiction in the NLI task). Then we evaluate the state-of-the-art NLI model over this dataset and calculate its accuracy. We have updated the evaluation details in the Appendix B.5.\n\n\n---\n\n> **Question 4**: Whether the results and analyses presented in this paper would be changed when considering free-form settings?\n\n**Answer 4**:  Please refer to our \"Response to all reviewers Answer 1\" for a detailed discussion.\n\n---\n\nWe appreciate your questions and constructive feedback. If you have any other questions, please do not hesitate to follow up. We are committed to making any necessary revisions to improve our work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1967/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601182898,
                "cdate": 1700601182898,
                "tmdate": 1700601182898,
                "mdate": 1700601182898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OARMJc9o5U",
                "forum": "auKAUJZMO6",
                "replyto": "gfX7gEFYjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1967/Reviewer_smQg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1967/Reviewer_smQg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response to my initial comments, which addresses them sufficiently."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1967/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654031602,
                "cdate": 1700654031602,
                "tmdate": 1700654031602,
                "mdate": 1700654031602,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y1JR4qIQMY",
            "forum": "auKAUJZMO6",
            "replyto": "auKAUJZMO6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1967/Reviewer_tvy2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1967/Reviewer_tvy2"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates LLMs behavior when encountering knowledge conflict between their parametric knowledge and input evidence. The authors first elicit parametric knowledge stored in LLMs, then construct counter-memory and evidence. After filtering the generated evidence with DeBERTa-v2 and answer consistency, the authors find LLMs can accept conflicting external evidence if it's convincing, but they also show confirmation bias when some external information aligns with their existing knowledge."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Important research questions, investigating the LLM\u2019s behavior when encountering knowledge conflict would have profound implications.\n2. The paper is overall well-written and easy to understand.\n3. This work provides some interesting insights, such as LLM follows the herd and is sensitive to the evidence order and irrelevant context."
                },
                "weaknesses": {
                    "value": "1. The parametric knowledge LLMs output would have randomness. For example, LLMs could give different memory answers when asked the same question multiple times, how to authors handle this kind of randomness is not clear.\n2. I think the authors\u2019 claim that LLMs are highly receptive to coherent evidence is problematic. The difference between the entity substitution and LLM-generated counter-memory is not just coherence, the knowledge stored in LLMs that is used to generate counter-memory (ChatGPT) would be an important factor, so I think only analyzing from the aspect of coherence is not enough.   \n3. Beyond just investigating the textual output of LLMs, it would be interesting to see the LLM\u2019s uncertainty when encountering knowledge conflicts.\n4,. For LLMs would be distracted by irrelevant context part, I recommend citing this work:\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., ... & Zhou, D. (2023, July). Large language models can be easily distracted by irrelevant context. In\u00a0International Conference on Machine Learning\u00a0(pp. 31210-31227). PMLR."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1967/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1967/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1967/Reviewer_tvy2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1967/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698891634,
            "cdate": 1698698891634,
            "tmdate": 1699636127948,
            "mdate": 1699636127948,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i40jXMXTXp",
                "forum": "auKAUJZMO6",
                "replyto": "y1JR4qIQMY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tvy2"
                    },
                    "comment": {
                        "value": "We are grateful for your thoughtful feedback on our paper and happy to learn that you find our research question significant and insights we provide interesting. We will address your concerns as follows.\n\n\n> **Question 1**: The parametric knowledge LLMs output would have randomness.\n\n  **Answer 1**: A1: Yes, randomness is an worthnoting issue. To alleviate it, we conducted preliminary experiments to evaluate the stability of 10 different instructions and adopted the most stable one for follow-up experiments. Additionally, to ensure the reliability of the generated parametric knowledge, we further adopt the 'answer consistency' step to filter out the evidence that may be generated out of/influenced by the randomness, as illustrated in Figure 2.\n\n---\n\n> **Question 2**: Analyzing from the aspect of coherence is not enough.\n\n  **Answer 2**: Please refer to our \"Response to all reviewers Answer 2\" for a detailed discussion.\n\n---\n\n> **Question 3**: Beyond just investigating the textual output of LLMs, it would be interesting to see the LLM\u2019s uncertainty when encountering knowledge conflicts. \n\n  **Answer 3**: Thanks for the great suggestion. With Llama2-7B as a case study, we report the log probabilities for the token it generates, after normalizing over all three tokens representing memory answer, counter-answer, and uncertain.\nSpecifically, we mainly explore two scenarios:\n\nFirstly, in the single-source setting where only counter-memory is presented, we sampled 1,000 examples that Llama2-7B gives a counter-answer. In the Figure shown in [Anonymous Link](https://anonymous.4open.science/r/iclr2024_rebuttal_1967-5ED8/1_hist_llama2-7b-1c.pdf), Llama2-7B shows high confidence when generating the counter-answer and 91.3% of examples have a memory answer probability of 95% or greater. **This demonstrates the high receptiveness to the external evidence, even when it conflicts with LLM's parametric memory.**\n\nSecondly, in the multi-source scenario where two supportive and two contradictory pieces of evidence are presented, we sample 1,000 instances that Llama2-7B favors the counter-answer.\nFigure in this [Anonymous Link](https://anonymous.4open.science/r/iclr2024_rebuttal_1967-5ED8/1_hist_llama2-7b-2p2c.pdf) shows that Llama2-7B is confident in its memory answer response, based on the token log probability. Specifically, 96.3\\% of the examples show a log probability of 95\\% or greater for the counter-answer.\n**Both *high confidence* shown here and *high frequency* (65\\%) shown in Table 6 of using memory-aligned evidence indicate the confirmation bias of LLMs.**\n\nThanks again for the suggestion, we have also updated this discussion in the Appendix A.2.\n\n---\n\n> **Question 4**: For LLMs would be distracted by irrelevant context part, I recommend citing this work: Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., ... & Zhou, D. (2023, July). Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning (pp. 31210-31227). PMLR.\n\n  **Answer 4**: We have read this paper and find its insights quite compelling. Following your suggestion, we have updated our submission to include a citation to this work in the irrelevant evidence part of Section 4.2. Thank you for pointing out this relevant research.\n\n---\n\nWe hope that these clarifications address your concerns. We appreciate the insightful feedback and suggestions as they help us improve the quality and clarity of our paper. We are committed to addressing these concerns and making the necessary revisions to keep improving our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1967/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601071073,
                "cdate": 1700601071073,
                "tmdate": 1700732222782,
                "mdate": 1700732222782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8OpBn2gDM3",
            "forum": "auKAUJZMO6",
            "replyto": "auKAUJZMO6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1967/Reviewer_zShN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1967/Reviewer_zShN"
            ],
            "content": {
                "summary": {
                    "value": "The paper performs an analysis on the behaviors of LLMs in knowledge conflicts by proposing a framework eliciting parametric memory and constructing counter-memory and conducting controlled experiments on LLMs\u2019 reception to external evidence. The paper demonstrates that LLMs can be highly receptive to coherent and convincing external evidence even when that conflicts with their parametric memory, and LLMs show a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory. It contrasts its counter-memory construction method with the prior entity-substitution method, employs memorization ratio as the evaluation metrics, and further explores the impacts of popularity, order, and quantity on evidence preference of LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper draws attention to the issue of knowledge conflicts, which are super important as it is related with direct safety concerns such as malicious attacks.\n\n- It proposes a new counter-memory construction method which goes beyond world-level editing and seems to be more convincing and closer to real-world scenarios.\n\n- Comprehensive experiments are conducted, including eight open-sources and closed-sources LLMs with varying model sizes and two QA datasets."
                },
                "weaknesses": {
                    "value": "- One of the two main results of the paper \u201cLLMs are highly receptive to external evidence if that is the only evidence, even when it conflicts with their parametric memory\u201d is not well-supported in the paper. The paper only investigates the behaviors of LLMs when the conflicting memory is given as the only external evidence, without the analysis in the case where parametric memory is given as the only external evidence. \n- About the other main result, in section 3.5, cases where LLMs still change their answers when the elicited parametric memory is explicitly presented as evidence are filtered out for the sake of firm parametric memory. This filtering step might be the actual cause of confirmation bias. \n- In section 3.5, the statement that \u201cif the parametric memory we elicit is truly the internal belief of an LLM\u2019s, presenting it explicitly as evidence should lead to LLM to provide the same answer as in the closed-book setting\u201d incorrectly assumes the existence of confirmation bias and it may not be true. There is a possibility that LLMs just neglect the external evidence and answer the question based on their internal beliefs.\n- Higher reception of LLMs does not show the counter-memory constructed by the method proposed in this paper is more coherent and convincing. Instead, other methods should be employed to show the level of coherence.\n- The paper concludes that \u201cthe effectiveness of our generated counter-memory also shows that LLMs can generate convincing dis- or misinformation, sufficient to mislead even themselves\u201d, while giving counter-answers does not necessarily mean LLMs are mislead. LLMs generate answers based on the instruction which is \u201caccording to the given information\u201d.\n- After demonstrating the findings, the paper lacks a discussion on their impacts - are LLMs\u2019 behaviors of high reception and confirmation bias acceptable? If not, how can we work to solve that? \n- In Figure 2, it might be better to exclude the percentage of counter-answer, as showing both may draw attention to the comparison between the percentage of memory-answer and counter-answer instead of the existence of memory-answer. \n- The counter-memory construction method, or the framework in general, is limited to question answering settings only, while knowledge conflicts may happen in other scenarios."
                },
                "questions": {
                    "value": "- Where do the same-type entities used for substitution come from?\n\n- Which dataset does Figure 2 employ? PopQA or StrategyQA or both?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1967/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719538464,
            "cdate": 1698719538464,
            "tmdate": 1699636127875,
            "mdate": 1699636127875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PRU8HNnLmO",
                "forum": "auKAUJZMO6",
                "replyto": "8OpBn2gDM3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zShN (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful and detailed comments. We are pleased that the reviewer considers our research question of significant importance. We appreciate the opportunity to address the concerns here.\n\n\n> **Question 1:** The analysis in the case where parametric memory is given as the only external evidence should be included.\n\n  **Answer 1:**  Thanks for the suggestion. In Step 4 of Figure 1 (Answer Consistency), to ensure the \u201cfirmness\u201d of the parametric memory, we have filtered out the cases where an LLM doesn\u2019t choose the memory answer when given the parametric memory as evidence. Therefore, the remaining examples are guaranteed to produce the memory answer when the \u201cparametric memory is given as the only external evidence\u201d, so we didn\u2019t include that. If we add those filtered cases back to the final dataset and present the parametric memory as the only external evidence, the results are as follows:\n\n  |            | PopQA | StrategyQA |\n  | ---------- | ----- | ---------- |\n  | ChatGPT    | 95.3% | 96.3%      |\n  | GPT-4      | 96.1% | 97.4%      |\n  | PaLM2      | 91.6% | 97.3%      |\n  | Qwen-7B    | 94.6% | 94.4%      |\n  | Lllama2-7B | 95.3% | 92.7%      |\n  | llama2-70B | 97.7% | 99.3%      |\n  | Vicuna-7B  | 87.6% | 93.1%      |\n  | Vicuna-33B | 83.4% | 94.5%      |\n\nNot too surprisingly, LLMs are highly receptive to the parametric memory (unfiltered) when it is presented as the only external evidence.\n\n---\n\n> **Question 2**: The filtering step might be the actual cause of confirmation bias.\n\n  **Answer 2**:  Thanks for the insightful question. To test this hypothesis (that \u201cthis filtering step might be the actual cause of confirmation bias\u201d), we add the filtered examples back to the final dataset and redo the experiment with 2 parametric memory and 2 counter-memory (2/4 in Table 6). The changes in memorization ratio (from the original experiment to this new experiment with the filtered examples) are shown below, which show that **the same level of confirmation bias still largely holds**:\n\n\n  |           | Memorization Ratio |\n  | --------- | ------------------ |\n  | ChatGPT   | 63.3% -> 61.7%     |\n  | GPT-4     | 75.4% -> 74.8%     |\n  | PaLM2     | 53.9% -> 55.6%     |\n  | Llama2-7B | 65.1% -> 67.2%     |\n\n---\n\n> **Question 3**: The \u201canswer consistency\u201d step assumes the existence of confirmation bias. May LLMs just neglect the external evidence and answer the question based on their internal beliefs.\n\n  **Answer 3**: It seems that two different (though related) concepts are being conflated here: an LLM\u2019s firm parametric memory and confirmation bias. Confirmation bias only applies when there are *multiple pieces of external evidence*, and it refers to the behavior of an entity that favors information conforming to its prior belief while ignoring contrary information. Eliciting the firm parametric memory is a prerequisite step for testing the existence of confirmation bias\u2014if we don\u2019t confidently know an LLM\u2019s \u201cprior belief\u201d (parametric memory), the test of confirmation bias will be built on a shaky foundation. We do not assume the existence of confirmation bias. Our goal is to test its existence.\n\nWith the clarification, to directly respond to the question, if an LLM just answers the question based on its internal belief without considering the external evidence, then it becomes exactly the same situation as the close-book setting in Step 1. The LLM should simply give the same answer. Also, considering that we have already shown, under the single-evidence setting, that LLMs are highly receptive to external evidence even when it\u2019s counter-memory, it\u2019s highly unlikely that the LLM just ignores the external evidence (in this case its parametric memory). \n\nWe would also like to echo the results in Q2, which show that even if we add these filtered examples back to the final dataset, the same level of confirmation bias still largely holds. \n\n---\n\n> **Question 4**: Other methods should be employed to show the level of coherence.\n\n  **Answer 4:** Please refer to our \"Response to all reviewers Answer 2\" for a detailed discussion."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1967/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600944386,
                "cdate": 1700600944386,
                "tmdate": 1700732079526,
                "mdate": 1700732079526,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "diTn9iYLVg",
                "forum": "auKAUJZMO6",
                "replyto": "8OpBn2gDM3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zShN (2/3)"
                    },
                    "comment": {
                        "value": "> **Question 5**: LLMs may generate answers based on the instruction which is \u201caccording to the given information\u201d.\n\n  **Answer 5**: That\u2019s an insightful thought. As detailed in Appendix Table C.9, to prevent LLMs from just doing reading comprehension (i.e., only using the given information) when presenting only parametric memory/counter-memory, we intentionally designed the instruction: \u201cAccording to the given information *and your knowledge*, answer the question.\u201d Thus, LLMs are encouraged to have a balanced consideration of both the provided external evidence and their internal beliefs.\n\nFurther, we include (1) the reading-comprehension-like instruction \u201cAccording to the given information, answer the question\u201d to explore if LLMs can faithfully use the given information only and (2) no instruction to explore how LLMs would perform without any explicit instruction. The additional experiments are based on 1000 randomly sampled examples.\n\nThe receptiveness of four LLMs given different instructions is shown in the table below. From this, we can observe:\n1) Without any instruction, LLMs still utilize external evidence in most cases, indicating their inherent tendency to rely on external information.\n2) When 'and your knowledge' is incorporated into the instruction, most LLMs actually accept less external evidence compared to the reading-comprehension instruction.\nBut **even with such an instruction that prevents LLMs from blindly using external evidence, LLMs still exhibit high receptiveness** \u2014 which is why we claimed they may be misled by dis- or misinformation.\n\n  |           | w/ \u201cand your knowledge\u201d (result in paper) | w/o \u201cand your knowledge\u201d | w/o instruction |\n  | --------- | ----------------------------------------- | ------------------------ | --------------- |\n  | ChatGPT   | 90.1%                                     | 96.4%                    | 92.2%           |\n  | GPT-4     | 87.1%                                     | 96.1%                    | 86.1%           |\n  | PaLM2     | 82.8%                                     | 90.9%                    | 95.5%           |\n  | Llama2-7B | 95.7%                                     | 89.1%                    | 87.0%           |\n\n---\n\n> **Question 6:** This paper lacks a discussion about the impact of LLMs\u2019 behavior when encountering the knowledge conflict. And how do we solve this?\n\n  **Answer 6:** Thanks for the suggestion, we briefly discussed the impact of high receptiveness and confirmation bias at the end of the introduction with indentation for highlights. \n\nConfirmation bias is a high undesired property, especially for generative search engines or similar use cases (e.g., multi-document summarization) of LLMs where orchestrating multiple pieces of potentially contradicting information in an unbiased way is important.\n\nThe high receptiveness could be a double-edged sword. On the one hand, it means that there\u2019s an easy way to remedy the stale or incorrect parametric knowledge of LLMs, a good news for techniques like retrieval-augmented generation. On the other hand, especially now that LLMs are increasingly connected to third-party tools (e.g., ChatGPT Plugins and all those recent language agents like AutoGPT), the high receptiveness to external evidence also means LLMs could be easily deceived by malicious tools that intentionally provide misleading or manipulative information.\n\nIn terms of potential solutions, for confirmation bias, depending on the intended use cases, further alignment through fine-tuning/RLHF to reduce the bias could be a promising direction. For the potential risks due to high receptiveness, a validation and monitoring system should be employed when connecting LLMs with third-party tools. Finally, from a generative search engine perspective, citing the sources for the answer and letting users be more informed and judge the final answer can be a more reliable way [1, 2].\n\nWe have added this expanded discussion in the Appendix A.1 and we will add it into the conclusion section if we have more space in the later phase.\n\n\n**Reference** \n\n[1] Yue X, Wang B, Zhang K, et al. Automatic evaluation of attribution by large language models[J]. arXiv preprint arXiv:2305.06311, 2023.\n\n[2] Gao T, Yen H, Yu J, et al. Enabling Large Language Models to Generate Text with Citations[J]. arXiv preprint arXiv:2305.14627, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1967/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601006678,
                "cdate": 1700601006678,
                "tmdate": 1700732004174,
                "mdate": 1700732004174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dxrt2hBdY3",
                "forum": "auKAUJZMO6",
                "replyto": "iFCn49ypCA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1967/Reviewer_zShN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1967/Reviewer_zShN"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response. I don't have any further questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1967/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687980745,
                "cdate": 1700687980745,
                "tmdate": 1700687980745,
                "mdate": 1700687980745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w9PSZBM2Fi",
            "forum": "auKAUJZMO6",
            "replyto": "auKAUJZMO6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1967/Reviewer_o9aH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1967/Reviewer_o9aH"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how LLMs react to the external knowledge. Empirical results suggest that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory and held a confirmation bias when the external evidence contains some information that is consistent with their parametric memory."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Additional checks to improve the data quality.\n> We design a series of checks, such as entailment from parametric memory to the answer, to ensure that the elicited parametric memory is indeed the LLM\u2019s internal belief. \n* Very interesting observation. Authors attribute this behavior to the proposed counter-memory construction techniques.\n> LLMs are actually highly receptive to external evidence if it is presented in a coherent way, even though it conflicts with their parametric memory. \n* The main argument is that existing counter-memory studies are not applicable to real-world scenarios, thus incoherent and unconvincing. Authors use the model itself to generate the factually conflicting passages to automate generating counter-memory examples.\n> For the counter-memory, instead of heuristically editing the parametric memory, we instruct an LLM to directly generate a coherent passage that factually conflicts with the parametric memory. \n* Exploit another form of LLMs hallucination problem with respect to the external knowledge given.\n* Demonstrate two seemingly contradicting behaviors of LLMs with knowledge conflicts."
                },
                "weaknesses": {
                    "value": "* This terminology of \u201ccounter-memory\u201d conflicts with the parametric and non-parametric memory. Better to use a direct and more specific terminology.\n> We refer to external evidence that conflicts with parametric memory as counter-memory.\n* Counter-answer construction techniques are somewhat like the heuristics (e.g., entity substitution, negation injection, etc.) used in the previous research. Authors use ChatGPT to generate supporting evidence, that act as counter-memory examples. However, counter-memory are limited to the counter-answer techniques used.\n> As depicted in Figure 1, at Step 2, we reframe the memory answer \u201cDemis Hassabis\u201d to a counter- answer (e.g., \u201cJeff Dean\u201d). Concretely, for POPQA, we substitute the entity in the memory answer with a same-type entity (e.g., from Demis to Jeff); while in STRATEGYQA, we flip the memory answer (e.g., from positive sentence to negative sentence). With counter-answer \u201cJeff Dean\u201d, we instruct ChatGPT2 to make up supporting evidence that Jeff Dean serves as chief scientist of DeepMind. We term such evidence that conflicts with parametric memory as counter-memory."
                },
                "questions": {
                    "value": "* Does MCQ-styled evaluation suit in this case since it makes relative decision in the closed world settings. Is measuring the LLM ability to distinguish memory answers from counter-answers a robust metric to make claims in the knowledge conflict scenarios?\n> LLMs are instructed to select one answer from memory answer (Mem-Ans.), counter-answer (Ctr-Ans.), and \u201cUncertain\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety",
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper presents that LLMs can generate harmful content with the external knowledge that can be controlled during the inference time."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1967/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776863644,
            "cdate": 1698776863644,
            "tmdate": 1699636127781,
            "mdate": 1699636127781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ngUbSqBd2X",
                "forum": "auKAUJZMO6",
                "replyto": "w9PSZBM2Fi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1967/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer o9aH"
                    },
                    "comment": {
                        "value": "We appreciate your insightful feedback and are delighted to know that our work is perceived as both important and interesting. We will answer your questions below.\n\n\n> **Question 1:** This terminology of \u201ccounter-memory\u201d conflicts with the parametric and non-parametric memory. Better to use a direct and more specific terminology.\n\n  **Answer 1:**  Thank you for your suggestion regarding terminology. We acknowledge the challenge in accurately and concisely describing the concept of \"evidence conflicting with parametric memory.\" After considerable literature survey and discussion among the authors, we decided to choose the term \"counter-memory\", partly because its resemblances to \"counter-factual.\" Meanwhile, we are very open for suggestions and would be happy to change to better alternatives. \n\n---\n\n> **Question 2:** Counter-answer construction techniques are somewhat like the heuristics (e.g., entity substitution, negation injection, etc.) used in the previous research. Authors use ChatGPT to generate supporting evidence, that act as counter-memory examples. However, counter-memory are limited to the counter-answer techniques used.\n\n  **Answer 2:** Good point. Better counter-answer construction method is indeed an interesting direction. In this work, we mainly focus on controlled experiments with high-quality counter-memory. We leave better counter-answer construction methods as future work.\n\n---\n\n> **Question 3:** Does MCQ-styled evaluation suit in this case since it makes relative decision in the closed world settings. Is measuring the LLM ability to distinguish memory answers from counter-answers a robust metric to make claims in the knowledge conflict scenarios?\n\n  **Answer 3:** Thanks for the great question. Please refer to \"Response to all reviewers Answer 1\".\n\n---\n\nIf the reviewer has any further questions or requires clarification on any point, please feel free to ask. We are committed to making any necessary revisions to further improve our work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1967/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600740124,
                "cdate": 1700600740124,
                "tmdate": 1700600740124,
                "mdate": 1700600740124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]