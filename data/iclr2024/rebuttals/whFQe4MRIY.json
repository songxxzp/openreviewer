[
    {
        "title": "MI-NeRF: Learning a Single Face NeRF from Multiple Identities"
    },
    {
        "review": {
            "id": "wiJXWFQGZ9",
            "forum": "whFQe4MRIY",
            "replyto": "whFQe4MRIY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5887/Reviewer_x1A2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5887/Reviewer_x1A2"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces MI-NeRF, a novel approach to learn a single dynamic neural radiance field (NeRF) from monocular videos of talking faces across multiple identities. The core innovation is the incorporation of a multiplicative module that distinguishes between identity and expression information, inspired by TensorFaces. By simultaneously training on multiple videos, MI-NeRF significantly cuts training time while achieving state-of-the-art performance in tasks like facial expression transfer and talking face video synthesis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed MI-NeRF robustly adapts to unseen expressions and identities, requiring minimal retraining, thereby saving computational time and effort.\n2. The Multiplicative module in MI-NeRF effectively separates identity from expression, ensuring consistent identity portrayal even with dynamic expressions, a challenge for some other models.\n3. MI-NeRF achieves superior performance in visual quality and lip synchronization, making it a leading solution in realistic talking face synthesis."
                },
                "weaknesses": {
                    "value": "Here are some concerns:\n\n1. The paper utilizes learnable latent codes to capture time-varying information. However, there's a potential ambiguity regarding how the model ensures these codes don't inadvertently encode identity or expression information. A more rigorous analysis or mechanism would have been beneficial to validate that these latent codes truly only represent unique, time-varying elements without overlapping with identity and expression descriptors.\n\n2. Maintaining consistency in novel views is inherently tied to the accurate modeling of 3D information. When MI-NeRF constructs facial identities and expressions over time, it is imperative for it to ensure that these reconstructions remain consistent and accurate, even when viewed from unseen angles. This is not merely a question of aesthetics, but of the system's fidelity to real-world dynamics. Without evaluation or visualization of its ability to maintain this consistency, one could question its versatility and applicability in varied real-world settings.\n\n3. Some concerns regarding the dataset:\ni) The collected videos lead to concerns about data privacy and the feasibility of public release. Ethical considerations surrounding personal data must be paramount. And will the authors be able or allowed to release the data?\nii) Within 140 videos, It's unclear how many unique individuals are represented and whether multiple videos pertain to the same person. Additionally, questions arise regarding the consistency of identity codes across different videos during training.\niii) The steps taken to prepare the data before its integration into MI-NeRF are essential. Without clear details about preprocessing, it's challenging to replicate results or trust the dataset's integrity.\n\n4. While the research highlights the limitations of the GAN-based method, Wav2Lip, it doesn't delve deeply into how MI-NeRF stands compared to other recent state-of-the-art GAN approaches. \n\n5. MI-NeRF's uniqueness largely stems from its multiplicative module. However, this also means the model's overall success heavily depends on this single component. If the module faces challenges in complex real-world scenarios, the entire model's performance could be compromised. A deeper discussion about the potential limitations of this module would be beneficial for a comprehensive understanding of its robustness."
                },
                "questions": {
                    "value": "1. Could the authors shed light on the exact nature of the per-frame latent codes and how they ensure these codes are devoid of identity or expression specifics? \n\n2. Could there be a more detailed comparison between MI-NeRF and advanced GAN-based methods, particularly in the context of face video synthesis?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5887/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775182757,
            "cdate": 1698775182757,
            "tmdate": 1699636623974,
            "mdate": 1699636623974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yrysi2Yclo",
                "forum": "whFQe4MRIY",
                "replyto": "wiJXWFQGZ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5887/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5887/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer x1A2"
                    },
                    "comment": {
                        "value": "We are thankful to the reviewer x1A2 for their effort and time to review our paper. We address their concerns below: \n\n\n> Q1: Learned latent codes.\n\nWe included in the appendix C a qualitative comparison of our model trained with and without the latent codes (see [Fig. 10](https://imgur.com/a/FkYTlf0)).  In the revised manuscript, we denote with red color the changes for visual distinction. As the indicative frame illustrates, our learned latent codes only capture very small variations in appearance and high-frequency details, while the expression and the identity remain the same (see also ablation study in Sec. 4.2). Thus, our experimental analysis exhibits that there is no overlap of these codes with identity and expression. \n\nIn practice, we ensure this by adding a higher L2 regularization on the latent codes (see $\\lambda_{l}$ in Sec. 3.3). This results in latent codes only obtaining small values and capturing very small per-frame variations, that are not captured by the powerful representation of the expression parameters in the PCA space, or the time-invariant identity code.\n\n\n> Q2: Comparison with GAN-based methods.\n\nEarlier works that propose GAN-based methods for expression transfer, like GANimation [1], operate in the 2D image space. Similarly with Wav2Lip, these 2D-based approaches frequently produce artifacts, since they cannot handle 3D motion (see Fig. 4, Sec. 4.4, and appendix C).\n\nRecent state-of-the-art GANs propose 3D-aware approaches, like Next3D [2], that are based on EG3D [3] and use StyleGAN2 [4] priors. The output identity depends on the input latent code and therefore on the random seed. In order to render a specific person, they require to run a time-consuming GAN inversion (e.g. PTI [5]). This process is required in order to project the true identity to the closest identity the model can express. To achieve that, they use the closest latent codes and fine-tune the generator weights, in order to approximate an identity, given their image. In contrast, in our case, we can directly learn a code for each identity and render them with a high fidelity. In addition, these 3D-aware GANs *require very long training times* (e.g. training EG3D for 8.5 days on 8 Tesla V100 GPUs [3] and another 4 days training for Next3D on 4 3090 GPUs [2]).\n\n\n> Q3: Dataset details.\n\nWe included in the pdf an additional section in the appendix, Sec. F, where we provide additional details for the dataset. As Sec. F clarifies, we collected talking face videos from publicly available datasets, commonly used by other published works. We provide a detailed list of the identities and videos used, as well as the corresponding links that include the origin of the datasets and the license. A visualization of the learned identity codes is given in Fig. 8. \n\nAs mentioned in Sec. 3.1 and Sec. F, we run standard pre-processing techniques to fit a 3DMM per video frame and extract the corresponding head pose and expression parameters. We refer the reviewer to [6] for more details in the data pre-processing.\n\nIf the reviewer has any remaining concerns, we would be happy to address them.\n\n\n> Q4. Limitations of the multiplicative module.\n\nOur proposed multiplicative module learns non-linear interactions between identity and expression. To learn these interactions, we use talking face videos that include a variety of facial expressions from different identities. We believe that a limitation would arise if there is no overlap between the expressions of different identities. As shown in [7], multiplicative interactions can be successfully captured if there is some overlap between different attributes, in order to learn all possible combinations.  However, the case of no overlap would not easily appear in a real-world scenario, since it is very probable that different subjects will show some similar expressions, e.g. neutral expression or a smile, while talking.\n\n\n___\n## References\n\n[1] Pumarola et al., GANimation: One-Shot Anatomically Consistent Facial Animation, IJCV, 2019.\n\n[2] Sun et al., Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars, CVPR, 2023.\n\n[3] Chan et al., Efficient Geometry-aware 3D Generative Adversarial Networks, CVPR, 2022.\n\n[4] Karras et al., Analyzing and improving the image quality of StyleGAN, CVPR, 2020.\n\n[5] Roich et al., Pivotal tuning for latent-based editing of real images, ACM Trans. Graph., 2021.\n\n[6] Guo et al., Cnn-based real-time dense face reconstruction with inverse-rendered photo-realistic face images, IEEE transactions on pattern analysis and machine intelligence, 2018.\n\n[7] Georgopoulos et al., Multilinear latent conditioning for generating unseen attribute combinations, ICML, 2020."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5887/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281738104,
                "cdate": 1700281738104,
                "tmdate": 1700281738104,
                "mdate": 1700281738104,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AancwQy00M",
            "forum": "whFQe4MRIY",
            "replyto": "whFQe4MRIY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5887/Reviewer_ud4c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5887/Reviewer_ud4c"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, authors propose a NeRF based method for dynamic face synthesis from monocular talking face videos. Unlike single-identity models, the proposed method uses a single network to train on videos of multiple identities and do a fast fine tuning when applying on a new identity. This single network is trained by imposing a multiplicative structure between identity embeddings and expression embeddings.  Quantitative and qualitative experiments show that the proposed multiplicative structure helps disentangling the identity and expression. The training time is significantly reduced compared to single-identity models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is simple and effective. \n\nBuilt on top of earlier multi-identity NeRF methods, this work basically imposes a multiplicative structure between identity embeddings and expression embeddings. The structure is in the form of an element-wise product between two embeddings and can be extended with high-degree interactions. The method shouldn't be hard for any readers to re-implement.  \n\nAblation studies show the multiplicative structure is well designed, and each piece of it can help improve the factor disentanglement and visual quality. When compared with existing works, the proposed method shows superior performance for expression transfer and lip synced video synthesis tasks."
                },
                "weaknesses": {
                    "value": "The comparison with other multi-identity NeRF-based methods can be improved. \n\nFast training time is a known benefit for multi-identity NeRF-based methods. For the training time evaluation, it misses the comparison with other multi-identity methods, like HeadNeRF.\n\nMore multi-identity NeRF-based methods might be included in the quantitative comparison, especially in the expression transfer experiments."
                },
                "questions": {
                    "value": "Is there a training time benchmarking (Figure 3) for identities methods, like HeadNeRF?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5887/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5887/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5887/Reviewer_ud4c"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5887/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835318846,
            "cdate": 1698835318846,
            "tmdate": 1699636623887,
            "mdate": 1699636623887,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qIqmq3vmoq",
                "forum": "whFQe4MRIY",
                "replyto": "AancwQy00M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5887/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5887/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ud4c"
                    },
                    "comment": {
                        "value": "We are thankful to the reviewer ud4c for their time and effort to review our paper. \n\n> Q1: Comparison with other multi-identity NeRF-based methods.\n\nWe believe that we have included all the relevant qualitative and quantitative comparisons with other multi-identity NeRF methods. As mentioned in our paper, to the best of our knowledge, MI-NeRF is the first multi-identity NeRF for faces, learned from monocular videos of multiple subjects. For facial expression transfer, we show comparisons with the single-identity NeRFace, its immediate extension \"Baseline NeRF\" trained on multiple identities, and the NeRF-based parametric model HeadNeRF (see Sec. 4.3, Fig. 2, Table 2). However, if the reviewer ud4c is aware of other multi-identity NeRF-based methods and would like to see a comparison with those, please let us know.\n\n__________\n\n> Q2: Fast training time is a known benefit for multi-identity NeRF-based methods. For the training time evaluation, it misses the comparison with other multi-identity methods, like HeadNeRF.\n\nAccording to the respective paper, HeadNeRF requires about 3 days of training on a single NVIDIA 3090 GPU, which is similar to our reported time for training of 100 identities. \n\nIf the reviewer has any remaining questions, we are happy to address them. We would appreciate it if the reviewer provides any concrete multi-identity NeRFs they believe we should compare with."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5887/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900975580,
                "cdate": 1699900975580,
                "tmdate": 1699900975580,
                "mdate": 1699900975580,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z3gAB7CHNg",
            "forum": "whFQe4MRIY",
            "replyto": "whFQe4MRIY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5887/Reviewer_TTon"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5887/Reviewer_TTon"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to learn a single dynamic NeRF for talking face videos of multiple identities. Expression, identity, and time-varying parts are separately modeled and interact with each other to predict the color and density of NeRF. Therefore, this method can use only a single NeRF to model the common geometry for diverse faces."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The whole writing is well organized and the proposed method is clearly described;\n2. The method for disentangling the identity and expression sounds reasonable, and the experiments show good disentanglement;\n3. Shown results outperform the competitors and the training time is significantly reduced."
                },
                "weaknesses": {
                    "value": "The paper further proposes a high-degree interaction module; however, I don't see any usage of this module in the whole method, as well as any experiment analysis about this module."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5887/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699189958841,
            "cdate": 1699189958841,
            "tmdate": 1699636623792,
            "mdate": 1699636623792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PGDEkk8G5B",
                "forum": "whFQe4MRIY",
                "replyto": "z3gAB7CHNg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5887/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5887/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer TTon"
                    },
                    "comment": {
                        "value": "We are thankful to the reviewer TTon for their time and effort to review our paper. We address their concern below: \n\n\n> Q1: High-degree module.\n\nWe conduct experiments using the high-degree module H, please see Table 1. The results indicate that it performs on par with the module M that we propose and visually we also observe a similar visual quality to the module M. \n\nIn addition, this module can be even more useful when interactions between additional variables should be captured, e.g., if we include lighting conditions in the future. Therefore, we do believe that this method is useful in our methodological part."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5887/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082623343,
                "cdate": 1700082623343,
                "tmdate": 1700082623343,
                "mdate": 1700082623343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vrIriXSzeL",
            "forum": "whFQe4MRIY",
            "replyto": "whFQe4MRIY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5887/Reviewer_d6cv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5887/Reviewer_d6cv"
            ],
            "content": {
                "summary": {
                    "value": "This work is about developing a single NeRF for multiple face identities. It uses the 3D morphable face model to estimte the 3D first, and then applys to NeRF for building the model. Experiments are shown both quantitatively and quanlitatively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It argues that a single face NeRF model can be developed for multiple identities.\n\nSome interesting results are obtained."
                },
                "weaknesses": {
                    "value": "It is unclear how the face expressions are aligned or handled. Even different identities can be handled together, how to deal with the different expressions for different identities? Do you use a cononical face model to separate the expressions? \n\nIt is unclear how to handle diferent lighting conditions. \n\nThere is no quantitative comparisons between the proposed method and the state of the art. It is unclear if the proposed method can outperform the existing works."
                },
                "questions": {
                    "value": "As listed in the weakness part. \n\nFurther, the authors should give clearer statements on how to deal with different identities for the NeRF model learning. Although some equations are given, it is still difficult how to do this. It needs more detailed descriptions on this point."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "For face images, there are some privacy issues to address."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5887/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699457175183,
            "cdate": 1699457175183,
            "tmdate": 1699636623704,
            "mdate": 1699636623704,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RS6KzzPO3e",
                "forum": "whFQe4MRIY",
                "replyto": "vrIriXSzeL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5887/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5887/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer d6cv"
                    },
                    "comment": {
                        "value": "We are thankful to the reviewer d6cv for their time and effort to review our paper. We address their concerns below: \n\n\n> Q1:  How the face expressions are aligned or handled.  \n\nWe perform standard processing techniques. Please let us elaborate on those. As described in Sec. 3.1, we fit a 3D morphable model (3DMM) for each video frame, and extract the corresponding head pose and facial expression parameters. A 3DMM [1] is a parametric model that represents  the human face as a linear combination of principle axes for shape, texture, and expression, learned by principal components analysis (PCA). Thus, the extracted expression coefficients are in a common space for different identities. The extracted head pose corresponds to rotation and translation matrices that are used to transform the sampled 3D points to the canonical space (see Sec. 3.1). We also refer the reviewer to the cited work of [2]  for more details.\n\n\n> Q2: Lighting conditions.\n\nHandling different lighting conditions is an interesting research problem, but beyond the scope of this paper. We refer the reviewer to related works that address this problem [3, 4]. In our case, in addition to expression and identity codes, we learn per-frame latent codes (embeddings) that capture time-varying information (see Sec. 3.1). These codes are used to capture small per-frame variations in appearance and illumination, and reconstruct them in the generated videos, in order to enhance the final visual quality (see ablation study in Sec. 4.2). Since our training videos do not include any significant lighting changes, these latent codes only give a very small increase in performance. Handling larger lighting changes is an interesting problem for future research.\n\n\n> Q3: Quantitative comparisons.\n\nWe provide extensive quantitative comparisons with the relevant state-of-the-art methods in Sec. 4.3 and 4.4, and in Table 2. If the reviewer d6cv is aware of any other relevant state-of-the-art work and would like to see a comparison with those, please let us know.\n\n> Q4: How to deal with different identities.\n\nAs described in Sec. 3.1, we learn an identity code per video. This is a learnable embedding that is optimized during training (see torch.nn.Embedding from PyTorch). By enforcing it to be the same for all the frames of a subject, we encourage it to capture time-invariant information. These identity codes are used in the proposed multiplicative module (see Sec. 3.2).\n\n\n> Q5: With respect to ethics review.\n\nWe included in the pdf an additional section in the appendix, Sec. F, where we provide additional details for the dataset. The changes in the manuscript are highlighted with red for visual clarity. As Sec. F clarifies, we collected talking face videos from publicly available datasets, commonly used by other published works [5, 6, 7, 8, 9]. We provide a detailed list of the identities and videos used, as well as the corresponding links that include the origin of the datasets and the license.\n\nIn addition, our original submission does include an ethics statement as encouraged by the ICLR guidelines. If the reviewer believes something else should be added there, we would be happy to include it. \n\nIf the reviewer has any remaining concerns, we would be happy to address them.\n\n____\n\n## References\n\n[1] Blanz et al., A morphable model for the synthesis of 3D faces, Proceedings of the 26th annual conference on computer graphics and interactive techniques, 1999.\n\n[2] Guo et al., Cnn-based real-time dense face reconstruction with inverse-rendered photo-realistic face images, IEEE transactions on pattern analysis and machine intelligence, 2018.\n\n[3] Wang et al., Portrait Reconstruction and Relighting using the Sun as a Light Stage, CVPR, 2023.\n\n[4] Jiang et al., NeRFFaceLighting: Implicit and Disentangled Face Lighting Representation Leveraging Generative Prior in Neural Radiance Fields, ACM Transactions on Graphics, 2023.\n\n[5] Guo et al., AD-NeRF: Audio driven neural radiance fields for talking head synthesis, CVPR, 2021.\n\n[6] Lu et al., Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation, ACM Transactions on Graphics, 2021.\n\n[7] Chatziagapi et al., LipNeRF: What is the right feature space to lip-sync a NeRF, International Conference on Automatic Face and Gesture Recognition, 2023.\n\n[8] Zhang et al., Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset, CVPR, 2021.\n\n[9] Liu et al., Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation, ECCV, 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5887/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082095095,
                "cdate": 1700082095095,
                "tmdate": 1700082185905,
                "mdate": 1700082185905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]