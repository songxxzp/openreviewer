[
    {
        "title": "Confident Sinkhorn Allocation for Pseudo-Labeling"
    },
    {
        "review": {
            "id": "BPxDn1DTjk",
            "forum": "ByXWN19vWP",
            "replyto": "ByXWN19vWP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4713/Reviewer_33FF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4713/Reviewer_33FF"
            ],
            "content": {
                "summary": {
                    "value": "To solve the problem of misallocation caused by the overconfidence of the pseudo-labeling method due to threshold sensitivity,This paper studies theoretically the role of uncertainty to pseudo-labeling and proposes Confident Sinkhorn Allocation (CSA), which identifies the best pseudolabel allocation via optimal transport to only samples with high confidence scores. CSA utilizes Sinkhorn\u2019s algorithm to assign labels to only the data samples with high confidence scores, eliminating the need to predefine the heuristic thresholds used in existing pseudo-labeling methods. In terms of theory,this paper study the pseudo-labelling process when training on labeled set and predicting unlabeled data using a PAC-Bayes generalization bound. CSA specifies the frequency of assigned labels including the lower bound and upper bound per class as well as the fraction of data points to be assigned. Then, the optimal transport will automatically perform row and column scalings. Additionally, this paper proposes to use the Integral Probability Metrics to extend and improve the existing PAC-Bayes bound which relies on the Kullback-Leibler (KL) divergence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper explains the label assignment process as an optimal transport problem between examples and classes, and solves it using the confident Sinkhorn algorithm.The proposed CSA is widely applicable to various data domains, and could be used in concert with consistency-based approaches, but is particularly useful for data domain where pretext tasks and data augmentation are not applicable, such as tabular data.\n\nThe theoretical result reveals that less uncertainty is more helpful. More number of unlabeled data is useful for a good estimation. Less number of classes and less number of input dimensions will make the estimation easier. The analysis takes a step further to show that both aleatoric uncertainty and epistemic uncertainty can reduce the probability of obtaining a good estimation.\n\nIn the experiment, all models use the same backbone network, and the settings of the models are described in comparison to it. Experimental verification shows that optimal transport cannot be achieved simply by changing the threshold value.The paper also conducts other empirical analysis which can be sensitive to the performance."
                },
                "weaknesses": {
                    "value": "The legends of the figures in the paper are not very clear, making it difficult to understand the meaning of the various elements in the figures without reading the relevant paragraphs in detail.\n\nThe part about CSA in the article and the part about PAC-Bayes bound seem to have insufficient connection. Even without using knowledge about the PAC-Bayes bound, the description and derivation of the CSA part still seem to hold."
                },
                "questions": {
                    "value": "Refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4713/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698140847218,
            "cdate": 1698140847218,
            "tmdate": 1699636453293,
            "mdate": 1699636453293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xFmae9HgzV",
                "forum": "ByXWN19vWP",
                "replyto": "BPxDn1DTjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "author response"
                    },
                    "comment": {
                        "value": "Thank you for your positive review and precisely highlighting all of our key contributions. We hope our responses to the minor concerns provide the necessary reassurance required.\n\n---\n\nQuestion: Connection between CSA and PAC-Bayes bound in the algorithm.\n\nAnswer: the key label assignment in pseudo-labeling (PL) is to train on labeled data and generalize this estimation to predict the unlabeled data. Most of the existing research in PL implicitly assumes the strong generalization that the model performance on the labeled (train set) and unlabeled (test set) are similar. However, this generalization assumption is not always true. For example, if we have limited samples in the labeled set, the model can not generalize to predict the unlabeled set well. Thus, it can lead to poor PL performance.\nWithout the PAC-Bayes result, the CSA part may fail into the above generalization issue.\n\nTherefore, we present the PAC-Bayes result to theoretically analyze the generalization performance of ensemble models trained on labeled data and tested on unlabeled data. In particular, Theorem 2 reveals that the more number of labels data $N_l$ makes the generalization error tighter (better). This generalization finding is consistently aligned with the result in Theorem 1 (analyzing from a different perspective). In addition, our generalization result in Theorem 2 considers the ensemble of multiple classifiers \u2013 the setting which has not been analyzed in Theorem 1. Thus, both theoretical results complement and strengthen each other in our paper.\n\n---\n\nMinor suggestion for legend:\nThank you for mentioning this. We will improve the presentation of the legends in the final version.Particularly, we will make better word choices for the legend and have the corresponding caption to be more descriptive."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040447757,
                "cdate": 1700040447757,
                "tmdate": 1700040447757,
                "mdate": 1700040447757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PQnLrXg3zo",
                "forum": "ByXWN19vWP",
                "replyto": "BPxDn1DTjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4713/Reviewer_33FF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4713/Reviewer_33FF"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the reponse. I have no further concerns to this work, so I prefer to maintain my score, while I'm not very familiar with the uncertainty estimation methonds, such that I suggest that Chairs mainly refer to the opinions of reviewers who are more professional in this field."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738040935,
                "cdate": 1700738040935,
                "tmdate": 1700738040935,
                "mdate": 1700738040935,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C9YvegYeOy",
            "forum": "ByXWN19vWP",
            "replyto": "ByXWN19vWP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4713/Reviewer_jWfa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4713/Reviewer_jWfa"
            ],
            "content": {
                "summary": {
                    "value": "Pseudo-labeling is suitable for learning without any domain assumptions. This work proposes a Pseudo-Labeling algorithm based on a confident score derived from ensemble models.  Firstly, a mathematical proof is presented to elucidate the impact of uncertainty in classifiers. Welch\u2019s T-test is employed to ascertain whether the most likely class holds statistically greater significance than the second-most likely class. This serves to diminish uncertainty in the estimated classifiers. Subsequently, the label allocation process is transferred to the optimization of the optimal transport problem, and the Sinkhorn Algorithm is employed to swiftly approximate the solution. Moreover, this study establishes PAC-Bayes results using Integral Probability Metrics, which provides a guarantee of generalization performance. Additionally, comprehensive experiments are devised to facilitate a comparative evaluation with other relevant works in the field."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work introduces an efficient algorithm aimed at mitigating uncertainty in pseudo-labeling. It leverages ensemble models to assess the confidence of labeling. Additionally, a comprehensive experimental setup is designed, encompassing not only accuracy comparisons with state-of-the-art algorithms but also evaluations across various dimensions.\n2. This work provides a solid mathematical proof for uncertainty analysis in Pseudo-Labeling and extends PAC-Bayes bounds to ensemble models, both of which contribute to subsequent research in this domain."
                },
                "weaknesses": {
                    "value": "1. Errors are present in the tables and figures. In Table 1, it is noted that in the comparison of related approaches, FlexMatch should be characterized as non-greedy based on the provided content. Regarding Figure 4, the top red square on the left fails to adequately illustrate the distinctions in assignments.\n2. In the section pertaining to the analysis of uncertainty in Pseudo-Labeling (PL), some aspects of the formulation concerning the settings are found to be incomplete. Consequently, this has resulted in certain points of confusion in comprehension. Although the appendix contains proofs that address some of my queries, further elucidation may be beneficial.\n3. In Algorithm 1, Confident Sinkhorn Label Allocation (simplified), the derivation of b_{-} and b_{+} when setting marginal distributions is not explicitly stated. Based on the content, it is inferred that they are empirically estimated from the class label frequency in the training data or from prior knowledge. This should be explicitly mentioned in the algorithm; otherwise, it leads to unknowns and incompleteness in the algorithm."
                },
                "questions": {
                    "value": "1. In Section 2.2, you outlined two challenges associated with assigning pseudo-labels. The proposed resolution entails employing an ensemble learning framework along with Welch's T-test to discern and exclude less confident samples. Are there alternative, more dependable methods for comparing the most probable class with the second most probable class? (Except those compared in the appendix) \n2. In the appendix, it is noted that the computational time of XGBoost increases with each iteration. Could this escalation in computational time become substantial when applied on a larger scale, potentially resulting in inefficiencies that outweigh its benefits?\n3. I've noted that in the algorithm when \u03c1 equals 1, it indicates full allocation. Additionally, I observed a limited elucidation regarding this parameter. In the appendix, \u03c1 is configured to allocate more data points in the earlier iterations and fewer in the later ones. I am intrigued by the potential impact of varying \u03c1. While this obviates the necessity to predefine a suitable threshold \u03b3, it introduces a new variable, \u03c1, necessitating a predefined value. Is the outcome sensitive to the choice of \u03c1? Is there a universally applicable \u03c1 that ensures the algorithm's effectiveness across diverse tasks? The role of \u03c1 in the algorithm appears somewhat ambiguous, and I am particularly keen on gaining a deeper understanding of it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4713/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4713/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4713/Reviewer_jWfa"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4713/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658690766,
            "cdate": 1698658690766,
            "tmdate": 1699636453197,
            "mdate": 1699636453197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IPWI440HSs",
                "forum": "ByXWN19vWP",
                "replyto": "C9YvegYeOy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "thank you"
                    },
                    "comment": {
                        "value": "We thank the reviewer for insightful suggestions which are very helpful to improve the clarity of the paper. When reading this, it seems the reviewer believes our work makes a meaningful contribution. We hope we can clarify your issues.\n\n---\nQuestion: In Table 1, FlexMatch should be characterized as non-greedy \n\nAnswer: Using OT, each datapoint will be jointly assigned labels in the presence of other assignments. In contrast, using FlexMatch (and other PL approaches), each data point assignment will purely depend on the given threshold and does not necessarily depend on the other assignments. FlexMatch still falls into this greedy category.\nNevertheless, in comparison to the other PL approaches, FlexMatch is less greedy in the sense that it defines the different thresholds for each class.\n\n---\n\nQuestion: Figure 4, the top red square on the left fails to adequately illustrate the distinctions in assignments.\n\nAnswer: We can improve this figure in the final version to make it clearer. In Figure 4, we use color (yellow is 1 and dark blue is 0) to indicate the numerical value of the assignment. The top red square actually has the value of 0.3 in brighter blue, although it is not visually clear enough to distinguish the dark blue (value =0), comparing to the other red squares in the bottom. \n\n---\n\nQuestion: In Algorithm 1, CSA (simplified), the derivation of $b_{-}$ and $b_{+}$ ...it is inferred that they are empirically estimated from the class label frequency in the training data or from prior knowledge.\n\nAnswer: Yes, they are empirically estimated from the class label frequency in the training data or from prior knowledge. We will mention them in the algorithm for clarity. Note that, $b_{-}$ and $b_{+}$ should be correctly written as $w_{-}$ and $w_{+}$.\n\n---\nQuestion: Are there alternative methods for statistical testing, more dependable methods for comparing the most probable class with the second most probable class?\n\nAnswer: Other alternatives for statistical testing include: Student-t test and Mann\u2013Whitney U test.\nHowever, according to the literature [1,2,3], we should use Welch\u2019s t-test by default, instead of Student\u2019s t-test, because Welch's t-test performs better whenever sample sizes and variances are unequal between groups, and gives the same result when sample sizes and variances are equal. \nIn addition, Ruxton (2006) [3] stated \u201c...the unequal variance t-test (or Welch\u2019s t-test should always be used in preference to the Student\u2019s t-test or Mann\u2013Whitney U test\u201d\n\n[1] Delacre, M. et al (2017). Why Psychologists Should by Default Use Welch\u2019s t-test Instead of Student\u2019s t-test. International Review of Social Psychology\n\n[2] https://daniellakens.blogspot.com/2015/01/always-use-welchs-t-test-instead-of.html \n\n[3] Ruxton, G. D. (2006). The unequal variance t-test is an underused alternative to Student's t-test and the Mann\u2013Whitney U test. Behavioral Ecology.\n\nWe, therefore, opt for the proposed Welch\u2019s t-test as the default choice in our framework. However, we can bring the above discussion in the paper to make it clearer for the readers. We thank the Reviewer for raising this useful question.\n\n---\nQuestion: the computational time of XGBoost increases with each iteration\n\nAnswer: First of all, most of the pseudo-labeling and SSL-based methods will suffer the same issue that the labeled dataset will grow over time after augmenting the unlabeled data. Such a growing dataset size will lead to more computational time.\nOne potential solution is to consider online learning to incrementally update the model for XGB, without retraining from the scratch [1]. Note that this is beyond the scope of our paper, but we can mention it in the updated version of the paper.\n\n[1] Montiel, et al. \"Adaptive xgboost for evolving data streams. IJCNN 2020\n\n---\nQuestion: $\\rho$ obviates the necessity to predefine a suitable threshold $\\gamma$. But it introduces a new variable, $\\rho$, necessitating a predefined value. \n\nAnswer: This is a very insightful comment. We follow the existing setting from SLA [44] to define the allocation schedule $\\rho$ controlling the fraction of examples to be assigned labels in each iteration. The role of $\\rho$ is to avoid assigning the entire 100% allocation in the first iteration for the OT approaches (both CSA and SLA). On the other hand, the threshold will stop this behavior from happening for other PL approaches.\n\nA single assignment in PL can be sensitive to the value of $\\gamma$ in being assigned or not. On the contrary with $\\rho$ in OT, the assignment will be less sensitive to $\\rho$ because we optimize the objective function to learn the assignment, i.e., minimizing the cost matrix to satisfy the row marginal distribution and column marginal distribution given the schedule allocation. There exist other factors (defined in the objective function) that influence the optimal assignment than the value of $\\rho$ itself."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700041325738,
                "cdate": 1700041325738,
                "tmdate": 1700041325738,
                "mdate": 1700041325738,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I9DEQPnQ9L",
            "forum": "ByXWN19vWP",
            "replyto": "ByXWN19vWP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4713/Reviewer_EorJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4713/Reviewer_EorJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Confident Sinkhorn Allocation (CSA) as an approach to improve pseudo labeling (PL) in the context of semi-supervised learning. The author delves into an analysis of the uncertainties associated with pseudo-labeling and introduces optimal transport as a means to mitigate the sensitivity observed in Greedy PL. Additionally, the paper presents a PAC-Bayes generalization bound that incorporates Integral Probability Metrics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The issue of excessive confidence and sensitivity to thresholds in pseudo labeling (PL) is indeed intriguing. The author conducts an analysis of the uncertainties within PL and offers some insights into this matter."
                },
                "weaknesses": {
                    "value": "1.\tThe paper's contribution appears somewhat vague. While it introduces a new pseudo labeling (PL) method, it lacks a clear probabilistic formulation. However, it's worth noting that the author does provide a PAC-Bayes generalization bound in Section 2.4, particularly for ensembling multiple classifiers. It would enhance clarity to explicitly state the individual contributions of various sections within the methodology.\n\n2.\tThere seems to be an inconsistency in the citation format used in the main text. The citation style in the introduction relies on numbers, but corresponding numbers in the reference list are missing. This inconsistency makes it challenging to match citations in the main text to the references.\n\n3.\tThe novelty of the optimal transport assignment in Section 2.3 appears somewhat limited. The primary concept seems to be derived from the original SLA [44]. Clarifying the extent of novelty and how the proposed method builds upon or diverges from previous work would be beneficial for the reader's understanding."
                },
                "questions": {
                    "value": "The method is primarily evaluated within the context of semi-supervised learning tasks, where sample selection is a general aspect of the approach. It raises the question of whether this technique can be extended to other settings, such as active learning or learning with noisy labels. Exploring the adaptability of this method to these different scenarios and discussing potential challenges or advantages would provide valuable insights into its broader applicability and limitations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4713/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4713/Reviewer_EorJ",
                        "ICLR.cc/2024/Conference/Submission4713/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4713/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750608988,
            "cdate": 1698750608988,
            "tmdate": 1700636312857,
            "mdate": 1700636312857,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g5r6Gg0Nbk",
                "forum": "ByXWN19vWP",
                "replyto": "I9DEQPnQ9L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "author response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review and a wide range of interesting comments. This review highlighted areas of our paper that absolutely needed to be clarified, so we appreciate the considerable time spent in finding them. We thank the reviewer in advance for reading our response, and hope that we have provided enough clarity that it is possible to provide us with a higher score as a consequence.\n\n---\n\nQuestion: It would enhance clarity to explicitly state the individual contributions of various sections within the methodology. The novelty of the optimal transport assignment in Section 2.3 appears somewhat limited. The primary concept seems to be derived from the original SLA [44]. \n\nAnswer: The OT assignment plays an important role in our algorithm. However, we don't claim the novelty and contribution toward this OT part (we have clearly stated in the abstract, the last paragraph of the introduction, and Sec 2.3). \n\nWe summarize the novelty and significance as follows:\n\nFirst, we are the first (to the best of our knowledge) to theoretically study the role of uncertainty in PL.\nThe theoretical finding reveals that less uncertainty is more helpful. More unlabeled data is useful for a good estimation. Less number of classes and less number of input dimensions will make the estimation easier. The analysis takes a step further to show that both aleatoric uncertainty and epistemic uncertainty can reduce the probability of obtaining a good estimation.\nWe note that the result presented in SLA [44] has only analyzed the first property (more number of unlabeled data is useful for a good estimation). The rest of the theoretical finding is novel and original.\n\nSecond, we propose to use Welch\u2019s T-test to ascertain whether the most likely class holds statistically greater significance than the second-most likely class. This T-test (to be used with OT)  eliminates the need to predefine the heuristic thresholds used in existing pseudo-labeling methods. \nThird, we extend PAC-Bayes bounds of ensemble models which provides a guarantee of generalization performance when trained on labeled data and tested on unlabeled data. Our bound strictly improves existing PAC-Bayes bounds for ensemble models and is of extended independent interest.\n\nNote that  both of our theoretical results contribute to subsequent research in this domain. These are identified and  highlighted by Reviewer jWfa  and Reviewer 33FF.\n\n---\n\nQuestion: There seems to be an inconsistency in the citation format used in the main text. The citation style in the introduction relies on numbers, but corresponding numbers in the reference list are missing. This inconsistency makes it challenging to match citations in the main text to the references.\n\nAnswer: We thank the reviewer for pointing out this citation format. In the current version, when clicking to the number citation in the main text using PDF, it will automatically link to the correct citation in the Reference. We will fix this formatting and update the paper. \n\n---\n\nQuestions: The method is primarily evaluated within the context of semi-supervised learning tasks, where sample selection is a general aspect of the approach. It raises the question of whether this technique can be extended to other settings, such as active learning or learning with noisy labels. Exploring the adaptability of this method to these different scenarios and discussing potential challenges or advantages would provide valuable insights into its broader applicability and limitations.\n\nAnswer: we would like to thank the reviewer for the suggestion on the possible research extension from our proposed method, beyond semi-supervised learning tasks. Indeed, active learning or learning with noisy labels are the promising research directions that can leverage pseudo-labeling. Within the scope of our paper, we focus on the main task of SSL in which most of the pseudo-labeling methods have been evaluated. We will consider active learning and learning with noisy labels as the future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700061061100,
                "cdate": 1700061061100,
                "tmdate": 1700061061100,
                "mdate": 1700061061100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6x1JjvTVXl",
                "forum": "ByXWN19vWP",
                "replyto": "g5r6Gg0Nbk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4713/Reviewer_EorJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4713/Reviewer_EorJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed clarification of the contribution in this work. Please update the manuscript in Author Console."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093463029,
                "cdate": 1700093463029,
                "tmdate": 1700093463029,
                "mdate": 1700093463029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sJ7tZyAGRL",
                "forum": "ByXWN19vWP",
                "replyto": "29CwjePo0o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4713/Reviewer_EorJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4713/Reviewer_EorJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks. I raised the score to 5."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637242449,
                "cdate": 1700637242449,
                "tmdate": 1700637242449,
                "mdate": 1700637242449,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iURZC8QU5c",
            "forum": "ByXWN19vWP",
            "replyto": "ByXWN19vWP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4713/Reviewer_Pwpb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4713/Reviewer_Pwpb"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the theoretical aspects of incorporating uncertainty in pseudo-labeling and introduces a new method, Confident Sinkhorn Allocation (CSA). CSA aims to determine the best pseudo-labels using optimal transport, focusing on samples with high confidence levels. Additionally, the study suggests utilizing Integral Probability Metrics to enhance and refine the current PAC-Bayes bound, which is dependent on the Kullback-Leibler (KL) divergence, for ensemble models. Experimental results indicate CSA's superiority over existing methods in semi-supervised learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors incorporate uncertainty into the pseudo-labeling generation process and provide a theoretical interpretation. \n2. The authors study theoretically the pseudo-labelling process when training on labeled set and predicting unlabeled data using a PAC-Bayes generalization bound."
                },
                "weaknesses": {
                    "value": "1. The choice to employ optimal transport methods for pseudo-labeling is not immediately clear, especially given the existence of the method detailed in section 2.2. It would be beneficial if the authors could elucidate on the rationale behind selecting optimal transport over the direct pseudo-labeling approach from section 2.2.\n2. The paper employs an ensemble of M models, but it is ambiguous whether the observed improvement in performance is attributed to the ensemble effect or the proposed algorithm itself. The absence of an ablation study leaves this point unclarified.\n3. The optimization objectives for optimal transport presented in section 2.3 lack clear explanations. It would be beneficial if the authors provide detailed interpretations for each constraint, elucidating on their roles and significance within the context of the problem.\n4. In section 2.1 of the article, there appears to be some inconsistency and potential oversight regarding notation. Firstly, the representation of the unlabeled data set as $\\{\\tilde{X_i^k}\\}$ seems non-standard. Given that $X_i^k$ denotes an individual data point, it would be more appropriate to use lowercase notation for clarity. Secondly, the expression for the probabilistic classifiers, specifically $f_k(x_i)$, is stated to produce a scalar value indicating the likelihood of $x_i$ being labeled as $k$. However, the provided formulation $f_k(x_i):=\\mathcal{N}(x_i|\\hat\\theta_k,\\Lambda)$ suggests it's a function mapping to a normal distribution parameterized by $x_i\\vert\\hat\\theta_k$ and $\\Lambda$.\n5. In Theorem 1, the definition of $\\mu_{\\backslash k}=\\mu_j\\vert\\exist j\\in {1,...K}\\backslash k$ is ambiguous. It would be beneficial for the authors to provide a more explicit definition or clarification regarding the intended meaning of $\\mu_{\\backslash k}$ in the context of the theorem."
                },
                "questions": {
                    "value": "1. Why did the authors choose to use optimal transport methods for pseudo-labeling instead of the direct approach described in section 2.2?\n2. How is the pseudo-labeling method described in section 2.2 related to the optimal transport pseudo-labeling approach in section 2.3? How do these two methods interact or complement each other in the overall framework?\n3. Have the authors considered conducting an ablation study to discern the individual contributions of the ensemble effect and the proposed algorithm to the overall performance improvement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4713/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4713/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4713/Reviewer_Pwpb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4713/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757374525,
            "cdate": 1698757374525,
            "tmdate": 1699636453022,
            "mdate": 1699636453022,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p5nKW5CKKp",
                "forum": "ByXWN19vWP",
                "replyto": "iURZC8QU5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "author response"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for the overall positive comments and mentioning a few points to be clarified. We are glad that the Reviewer recognized our theoretical contribution for characterizing the role of uncertainty in PL and generalization bound.\n\n---\n\nQ: The optimization objectives for OT in Sec 2.3 lack clear explanations\n\nA: Given the space constraints, we have presented additional details of the OT derivation and optimization in Appendix A.2 and A.3.\nThere are three key components in the OT setting: the cost matrix, the marginal distributions for row and for column. We minimize the cost matrix to learn the optimal assignment, satisfying both marginal distributions. Specifically, the column marginal is a vector of one, i.e., each data point should not be assigned to more than one label, Eq. (6) and Eq. (11). In addition, each label should receive the amount of assignment ranging from lower bound and upper bound label frequency $[w_-, w_+]$, Eq. (7,8,12,13). We will highlight this interpretation in the final version.\n\n---\n\nQ: In Sec 2.1. the representation of the unlabeled data set as $\\tilde{ X^k_i }$ seems non-standard. Given that $X^k_i$ denotes an individual data point, it would be more appropriate to use lowercase notation for clarity. \n\nA: we follow the same notation in [Yang and Xu (2020), Sec 2.1] to denote the labeled vs unlabeled data set. The difference is that the original paper considers the binary setting with $X^+$ and $X^-$ while our setting considers multi-classification with k classes. In addition, we (also by Yang and Xu) use $X^k_i$ to define a data point which takes a class k. $X^k_i$ is equivalent to write $x_i \\in X^k$ and $\\tilde{X}^k_i$ is $x_i \\in \\tilde{X^k}$.\n\n---\n\nQ: the expression for the probabilistic classifiers $f_k(x_i)$ stated to produce a scalar value indicating the LLK of $x_i$ being labeled as $k$. However, the provided formulation $f_k(x_i) = N( x_i | \\hat{ \\theta_k} , \\Lambda)$ suggests it's a function mapping to a normal distribution parameterized by $x_i |  \\hat{ \\theta_k}$ and $\\Lambda$.\n\nA: $f_k(x_i)$ produces a scalar value indicating the LLK of $x_i$ being labeled as $k$. Then, we particularly consider the simple case of the Gaussian classifier, $f_k(x_i) := p(x_i | y_i = k) = N( x_i | \\hat{ \\theta_k} , \\Lambda)$. This is the common notation used in [1].\n\n[1] https://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall07/gaussClassif.pdf (see slide 7)\n\n---\n\nQ: In Theorem 1, $\\mu_{\\\\k} = \\mu_j | \\exists j \\in {1\u2026K} \\ k$ is ambiguous\n\nA: Intuitively,  k \\ {1,...K}  indicates the correct class index. We denote the incorrect class being \\k = \\mu_j \\in {1\u2026K} \\ k. This refers to one of the remaining classes excluding k. $\\exists j$ can be optionally dropped to make it concise. We hope it clear and will clarify them in the context of the theorem\n\n---\n\nQ: ablation study to discern the individual contributions of the ensemble effect and the proposed algorithm to the overall performance improvement?\n\nA: Thanks for the great suggestion. We have made (below) the ablation study to learn the effect of ensembling. We show that the gain from the ensemble toward the final performance is not that significant to the overall performance improvement. In particular, ensembling XGBs will improve 2/5 datasets (Analcatdata, Digits) while degrade slightly for the other 3/5 datasets.\n\n\n| Dataset | PL (single XGB) | PL (ensemble XGBs) | CSA |\n| ------ | ------| ------| ------|\n| Wdbc | 91.23\u00b13 | 90.22\u00b13 | 91.83\u00b13 |\n| Analcatdata  | 90.95\u00b12 | 91.84\u00b13 | 96.60\u00b12 |\n| German-credit | 70.72\u00b13 | 70.15\u00b12 | 71.47\u00b13 |\n| Breast cancer | 92.89\u00b12 | 92.63\u00b12 | 93.55\u00b12 |\n| Digits | 81.67\u00b13 | 82.91\u00b13 | 88.10\u00b12 |\n\n\n---\n\nQ: the rationale behind selecting OT over the direct PL approach (Sec 2.2)\n\nA: The Reviewer mentions pseudo-labeling approach presented from Sec 2.2. However, Sec 2.2. presents Welch's T-test for identifying high confidence samples while the PL approaches are described in the Related Work subsection of the Introduction. Please correct us if we miss anything.\n\nOT is the non-greedy algorithm which can learn the best (non-trivial) label assignments. On the other hand, most of the existing PL methods are greedy which will rely on the predefined threshold to assign the labels. Specifically, using OT, each data point will be jointly assigned labels in the presence of others. In contrast, using the PL, each data point assignment will purely depend on the given threshold and does not necessarily depend on the other assignments.\n\nWe show in Sec 3.3 and Fig. 4 that OT assignment cannot be achieved simply by changing the threshold value (also mentioned by the Reviewer 33FF).\n\n---\n\nWe feel we have addressed all of the concerns raised. As such, we are optimistic that you would be able to raise your score. If this is not possible, please let us know so we have an opportunity to address the remaining concerns."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700062873033,
                "cdate": 1700062873033,
                "tmdate": 1700063290312,
                "mdate": 1700063290312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wkykx38eH7",
                "forum": "ByXWN19vWP",
                "replyto": "iURZC8QU5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4713/Reviewer_Pwpb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4713/Reviewer_Pwpb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response, which has resolved most of my concerns. However, regarding Weakness 4, you mentioned that $ f_k(x_i) := p(x_i | y_i=k) $, but shouldn't it be defined as $ f_k(x_i) := p(y_i=k | x_i) $? f_k produces a scalar value indicating the likelihood of being labeled as k. This definition somewhat affects the credibility of the conclusions drawn in the article, so I have decided to maintain the original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625019328,
                "cdate": 1700625019328,
                "tmdate": 1700625259086,
                "mdate": 1700625259086,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hWLZVsvXWD",
                "forum": "ByXWN19vWP",
                "replyto": "iURZC8QU5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4713/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "We are glad that we have resolved most of the concerns as acknowledged by the reviewer.\n\nWe thank the reviewer for mentioning the remaining concern left.\n\nIn our paper, we define $f_k$ produces a scalar value indicating the likelihood of being labelled as $k$. In particular, in Line 2, Page 4 in the paper, we have defined $f_k(x_i) := N( x_i | \\hat{\\theta}_k, \\Lambda ) $  which is correct.\n\nHowever, during our response, we have written $f_k(x_i) := \\textcolor{red}{p(x_i | y_i = k)} = N( x_i | \\hat{\\theta}_k, \\Lambda ) $ which is confusing as the reviewer pointed out. This should be corrected (by the reviewer) as $f_k(x_i) := \\textcolor{blue}{p(y_i = k | x_i)} = N( x_i | \\hat{\\theta}_k, \\Lambda ) $.\n\nNevertheless, we want to call out that this typo $ \\textcolor{red}{p(x_i | y_i = k)}$ is from our rebuttal response while the equation presented in paper is still correct (Line 2, Page 4). \n\nGiven (1) the above correctness and (2) that we have addressed all other concerns, we would greatly appreciate if you can reconsider the score or please let us know what is your suggestion to fix this typo?\n\nThank you!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4713/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628265205,
                "cdate": 1700628265205,
                "tmdate": 1700628289679,
                "mdate": 1700628289679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]