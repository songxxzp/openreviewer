[
    {
        "title": "Structured Evaluation of Synthetic Tabular Data"
    },
    {
        "review": {
            "id": "RdgM4MDEW2",
            "forum": "YD0GQBOFFZ",
            "replyto": "YD0GQBOFFZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1429/Reviewer_9Y2W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1429/Reviewer_9Y2W"
            ],
            "content": {
                "summary": {
                    "value": "Tabular data generative models strive to master the underlying process that produces observed data, aiming to generate synthetic data that mirrors the observed distribution. Over the past decade, various methods, from statistical to deep-learning-based approaches, have emerged to understand these distributions. A key challenge, however, is the evaluation of the generated synthetic samples. Since the true data generating process remains unknown, measuring the effectiveness of these models is not straightforward. While many attempts have been made to standardize evaluation methodologies and to distill metrics into a consolidated framework, they often fall short in terms of objectivity and clarity in interpretation, as noted by the authors. Addressing this, the paper seeks to introduce a unified evaluation framework that consolidates current metrics under a single mathematical objective. This objective is built on the premise that synthetic data are drawn from the same distribution as the observed data. To further bolster their evaluation approach, the authors suggest leveraging the probabilistic cross-categorization method as a stand-in for the elusive ground truth of the data generating process."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I find the authors incorporation of Probabilistic Cross-categorization\u2014 a domain-general method designed for characterizing the full joint distribution of variables in high-dimensional datasets\u2014 particularly intriguing, especially in the realm of tabular data generation. This is my first encounter with this approach in a benchmarking context, and its novelty in the author's work is commendable."
                },
                "weaknesses": {
                    "value": "I commend the authors efforts in detailing various metrics and particularly the authors exploration into the nuances between model-free metrics and the PCC-based metric. A deeper elaboration on this distinction would be immensely helpful for readers to fully grasp the nuances. \n\nThe representation in Figure 1B, specifically regarding the spectrums, may benefit from further context or an enriched explanation. This raises a query: Are the authors implying that model-free evaluations, such as those estimating marginal or pairwise relationships, may not provide a holistic perspective? Is there an inherent advantage in adopting model-based techniques, like PCC, to act as proxies for real data while assessing the same metrics? Moreover, given that PCC operates as a synthetic model, does its role in the evaluation process imply a comparison between synthetic models through another synthetic standard? Gaining clarity on these nuances would greatly enhance understanding. \n\nIt would also be illuminating to discern how this work either mirrors or deviates from established frameworks in previous literature. While the authors' initiative to broaden the metrics spectrum and introduce a surrogate model approximating real-data probability distribution is commendable, elaborating the distinct facets or innovative insights of the author's proposal, especially vis-\u00e0-vis findings in [1, 2] referenced in Questions section, could accentuate the originality and significance of the research amidst prevailing knowledge."
                },
                "questions": {
                    "value": "General comments & questions\n=========================\n\n- In section 3, the authors mentioned that \u201cthe objective of the data synthesiser is \\( Q=P \\)\u201d. While I understand the underlying objective might be to highlight the close similarity between the distributions, stating it in this manner might lead some readers to interpret this as \\( Q \\) being an exact replica of \\( P \\). Given the paper's central theme of using \\( Q \\) as a more private alternative to \\( P \\), such an interpretation could be seen as contradictory. Perhaps it might be clearer to emphasize that \\( Q \\) is intended to be statistically analogous or mirrors the distribution of \\( P \\). This would signify that while \\( Q \\) captures the broader statistical characteristics of \\( P \\), individual data points might differ, ensuring privacy.  I believe a more detailed description or clarification in this section could be beneficial for enhancing the reader's understanding and mitigating potential misconceptions.\n\n- The presentation of the leave-one-out (LOO) metric seems to bear a resemblance to the dimension-wise prediction performance metric as described in references [3, 4], as well as the all model's test metric outlined in [2]. Could the authors clarify whether these are synonymous or if there's a discernible distinction between them?\n\n- Rather than depending on a surrogate model to estimate ground truth, would it not be more reliable to employ a distinct hold-out test set, ensuring it retains the same distribution as the real (observed) data? Admittedly, this approach might pose challenges when dealing with limited samples. However, in such scenarios, methodologies like k-fold validation could be explored to compute an average score over several iterations. Alternatively, having a baseline that shows the performance of the surrogate on hold-out test set could serve as the acceptable error threshold.\n\n- The current presentation of details incorporates a variety of symbols, which, while comprehensive, can sometimes add complexity to the narrative without necessarily enhancing clarity. To improve readability and facilitate a deeper understanding for readers, I'd recommend introducing a dedicated subsection early on to familiarize readers with the notation. This way, within the main text, the authors can focus on using notation only when it brings forth novel information, and rely on plain language descriptions when the content permits. For instance, the passage: \"We then use the surrogate model to compute   \\{ \\hat{P(X_i) \\mid i=1,..,n \\}, which is the likelihood of  X_i\u2026\" could be more intuitively conveyed as: \"We use the surrogate model to determine the likelihood of the real data samples under this model.\" If the precise mathematical formulation is essential to the discussion, consider placing it in a distinct equation block, which can then be easily referenced within the narrative.\n\n- In section 3.3, the discussion surrounding the pp-plot could benefit from further clarity. I was wondering if the likelihood estimate method introduced is akin to the \"Distance to Closest Record\" concept mentioned in [5], where a Nearest Neighbours model is employed to gauge the likelihood of each synthetic data originating from the real data distribution. Is the primary distinction here the use of the Probabilistic Cross-Categorisation model? Any elucidation on this comparison would be invaluable for readers familiar with the referenced methodology.\n\n\n- Given that the evaluation encompasses a diverse range of metrics within the same family, such as marginal, pairwise-based, and leave-one-out conditionals, full-joint-based, missingness, and privacy. It might be insightful for readers if a correlation plot is provided. Such a plot could help elucidate potential correlations among metrics both within the same group and across different groups. This added visual representation could offer a comprehensive perspective on the interplay of these metrics and their potential overlaps or distinctions.\n\n\nSmall typo\n====\n\nFigure 1 (A) Model fee -> Model free\n\n(Potential) missing reference\n======================\n\nIt appears there's an omission in the paper's review of related literature. In particular, ref. [2] in its section 3 emphasizes the significance of evaluating synthetic tabular data generators across various metrics, including marginal-based, column-pairs, joint, and utility considerations. The thrust of these discussions in [2] bears a strong resonance with the core objectives of this paper. It's surprising and noteworthy that such pertinent work isn't cited or discussed in the current paper's related work section.\n\nReferences\n=========\n\n[1] Dankar, F.K., Ibrahim, M.K. and Ismail, L., 2022. A multi-dimensional evaluation of synthetic data generators. IEEE Access, 10, pp.11147-11158.\n\n[2] Afonja, T., Chen, D. and Fritz, M., 2023. MargCTGAN: A\" Marginally''Better CTGAN for the Low Sample Regime.\u00a0arXiv preprint arXiv:2307.07997.\n\n[3] Choi, E., Biswal, S., Malin, B., Duke, J., Stewart, W.F. and Sun, J., 2017, November. Generating multi-label discrete patient records using generative adversarial networks. In\u00a0Machine learning for healthcare conference\u00a0(pp. 286-305). PMLR.\n\n[4] Engelmann, J. and Lessmann, S., 2021. Conditional Wasserstein GAN-based oversampling of tabular data for imbalanced learning.\u00a0Expert Systems with Applications,\u00a0174, p.114582. \n\n[5] Zhao, Z., Kunar, A., Birke, R. and Chen, L.Y., 2021, November. Ctab-gan: Effective table data synthesizing. In\u00a0Asian Conference on Machine Learning\u00a0(pp. 97-112). PMLR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1429/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698159445907,
            "cdate": 1698159445907,
            "tmdate": 1699636071197,
            "mdate": 1699636071197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FPJG6KH3yZ",
                "forum": "YD0GQBOFFZ",
                "replyto": "RdgM4MDEW2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1429/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1429/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the many detailed comments that help improve the paper. In particular, we thank the reviewer for recognizing the novelty of the PCC-based metric and encouraging us to elaborate on it.\n\n**Elaboration of the PCC-based metric:**\nWe agree that it is helpful to have a deeper elaboration on the distinction between model-free and the PCC-based metric, as well as an enriched explanation on why one would like to have the model-based metrics. To achieve that, we will add a summary of our responses below to the Introduction and Section 3. The response below is also our point by point response to the specific questions raised by the reviewers.\n\nWhether the evaluation is holistic or not depends on whether the metrics provide coverage of the *full joint distribution*. This means that evaluating only marginal and pairwise relationships would not provide a holistic perspective, because higher order statistics such as 3-way interactions would not be captured by the evaluation. Using metrics that target the full joint distributions (i.e., LOO or full-joint) could in principle provide a holistic picture. This is true regardless of whether the metrics are model-free, model-based, or a combination of the two. The issue with the model-free metrics is that they are often noisy, especially towards the RHS of the spectrum for the LOO and full-joint metrics, as shown by the large error bars and non-monotonicity in panels A and C of Figures S1\u2013S3. The large error bars and non-monotonicity originate from (1) the use of multiple ML models with different expressiveness, as well as (2) the different types of estimators and scores used for different distributional properties and data types. For example, the full-joint metrics were evaluated indirectly via a discriminator, which also included a linear version (i.e., logistic regression) and a simple nonlinear one (i.e., SVC). In other words, the model-free metrics are *incoherent* in terms of the estimators (e.g., the underlying ML models used) and scores. In contrast, the model-based metrics neither have the incoherence problem, nor exhibit the noisy and non-monotonicity, as evidenced in panels B and D of Figures S1\u2013S3. Thus, the inherent advantage of the model-based technique is the coherence in methodology, that is, all the metrics can be derived from the same type of estimator and score.\n\nThe reviewer is correct that the model-based technique puts the evaluation on another synthetic standard. The worry is then whether this synthetic standard would bias the evaluation. The degree of bias depends on how much the surrogate model would mask statistical properties because of a lack of expressiveness. To illustrate, if the surrogate model captures only the marginal distributions but not the pairwise and higher-order dependencies, it would assign similar likelihoods to the PERM baseline and the real dataset. We have two pieces of evidence that PCC does not suffer from such lack of expressiveness. First, PCC is consistently a top data synthesizer as evaluated by the model-free metrics (panel A in Figure S1\u2013S3). Second, the model-based evaluation derived from PCC does not always put itself as the highest performing model but can capture superiorities of other techniques (panel B in Figure S1\u2013S3). In fact, the ordering of the performance of the different data synthesizers is consistent between the PCC-based and model-free evaluations."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258705094,
                "cdate": 1700258705094,
                "tmdate": 1700258705094,
                "mdate": 1700258705094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UQQdoqqrWZ",
                "forum": "YD0GQBOFFZ",
                "replyto": "jvXyb7WaKA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1429/Reviewer_9Y2W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1429/Reviewer_9Y2W"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the authors for addressing my comments and for their contributions. I have reviewed the changes and my evaluation remains unchanged."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466980303,
                "cdate": 1700466980303,
                "tmdate": 1700466980303,
                "mdate": 1700466980303,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sv2mFwZG9k",
                "forum": "YD0GQBOFFZ",
                "replyto": "AYZ8PYnerF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1429/Reviewer_9Y2W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1429/Reviewer_9Y2W"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I appreciate the authors' efforts in updating their paper with the suggested changes, which have notably improved its quality. After reviewing the revisions, I find the paper to still be quite dense. A further round of editing could elevate it even more. While I am maintaining my initial score, I am optimistic about the paper's future iterations and commend the authors for their dedication to enhancing their work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677199995,
                "cdate": 1700677199995,
                "tmdate": 1700677199995,
                "mdate": 1700677199995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qlJZgHaXlr",
            "forum": "YD0GQBOFFZ",
            "replyto": "YD0GQBOFFZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1429/Reviewer_UGhN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1429/Reviewer_UGhN"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose an analysis framework for evaluating data synthesizers. Data synthesizers aim to create synthetic datasets that resemble real datasets without directly copying them, i.e., the goal of such synthesizers is to generate synthetic datasets of a distribution Q that is as close as possible to the distribution P of the real dataset. The authors have conducted a structured evaluation of SOTA techniques for data synthesis on different datasets for varying evaluation criteria for distributions ranging from Missingness to Univariate Marginal to Pairwise Joint to Leave-One-Out conditionals to Full Joint Distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The topic of data synthesis is highly relevant for many real-world applications where data is very costly to obtain or privacy is a major concern.\nThe presentation is well-structured and detailed.\nThe authors have taken a systematic approach to evaluate different synthesizers in comparative way. They have considered different metrics and provided clear explanations for their choices."
                },
                "weaknesses": {
                    "value": "Although well-structured, the presentation is quite dense, and it might be challenging for someone without a background in the area to understand the differences and significance of the analysis framework and findings.\nThe paper has a strong focus on the technical evaluation of synthesizers, but it doesn't discuss the practical implications of the findings. I.e., how might these results impact real-world applications of these synthesizers?\nIt would be useful to know how the methods would have performed on large-scale datasets if computational resources were not a constraint.\nWhile the proposed metrics focus on a quantitative evaluation, qualitative insights or user-based evaluations might provide a more holistic view of synthesizer effectiveness."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1429/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1429/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1429/Reviewer_UGhN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1429/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698573646282,
            "cdate": 1698573646282,
            "tmdate": 1699636071118,
            "mdate": 1699636071118,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BVvzJLGynb",
                "forum": "YD0GQBOFFZ",
                "replyto": "qlJZgHaXlr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1429/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1429/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for noting that our approach is systematic and our evaluation is comparative, as well as for all the comments that help improve the paper.\n\nTo improve the readability of the paper, we now make a cleaner separation between plain English and math notations throughout Section 3 and Appendix A. We hope that this would help lessen the cognitive load and make the reading feel less dense.\n\nWe thank the reviewer for asking the question about practical implications and impact on real-world applications. We provide the following main points:\n- Synthesizer selection: The structured evaluation provides a complete and coherent picture of the performance of different synthesizers on a given dataset. This allows a practitioner to select the best data synthesizer with confidence, knowing the degree and extent to which the full joint is mimicked. The HALF baseline also provides a rough estimate on how far the data synthesizer is from being very good.\n- Synthesizer improvement: The structured evaluation with the baselines show where along the structural spectrum a data synthesizer falls short. This could expose particular failure modes and facilitate better design of the data synthesizer.\n- Evaluation improvement: The structured spectrum suggests that one could design many new metrics, such as 3-way interactions and leave-n-out (as suggested by Reviewer fbzR), and know where to position them relative to the other metrics. Furthermore, one could investigate the error bars and non-monotonicity of the ordered metric groups to identify problematic metrics. Finally, thinking about the chain of sufficiency could help the selection of estimators to use (e.g., which ML models to use in ML efficacy).\n- Human-centered debugging: The structured framework provides a nice, ordered visualization for people to spot strange behaviors more easily. An example is given at the end of this response related to the comments on \u201cqualitative insights or user-based evaluations.\u201d\nWe will add a summary of the above points towards the end of the paper.\n\nWe echo the reviewer\u2019s sentiment on testing our approach on large-scale datasets. This is of course difficult to realize because computational resources are real constraints. We would also like to point out that the census dataset we analyzed is considered a sizable tabular dataset. Many seemingly larger datasets are larger because the categorical columns have been preprocessed to use the one-hot encoding. Nevertheless, by looking at the three datasets of different sizes analyzed in the paper (Figure 2), we can already make some qualitative inferences about the performance of the methods:\n- Deep-learning methods generally become better as the dataset size increases.\n- Methods that are not designed to capture the full-joint distribution (i.e., GaussianCopula) generally become worse as the dataset size increases.\n- The performance of structurally based methods (synthpop and PCC) seems to be less dependent on dataset size.\nSome of these were mentioned in the paper on page 8, but we will make sure all the above points are covered.\n\nWe definitely agree with the reviewer that \u201cqualitative insights or user-based evaluations might provide a more holistic view of synthesizer effectiveness.\u201d We think that the structured evaluation would aid user-based evaluations by helping the users to more quickly drill down to where the problems might be. For example, in the updated Figure 2 first row, second column, GReaT has a suspiciously low performance in the Missingness group. One can then drill down to see that the cause is due to GReaT generating missing values in columns that do not have any missing values in the original data (mentioned in page 8, second last paragraph of the original manuscript)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258517966,
                "cdate": 1700258517966,
                "tmdate": 1700512170942,
                "mdate": 1700512170942,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qm5XX3vqXC",
                "forum": "YD0GQBOFFZ",
                "replyto": "BVvzJLGynb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1429/Reviewer_UGhN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1429/Reviewer_UGhN"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "I thank the authors for their comprehensive response. Although I am very much in favor of research focusing on evaluation strategies and frameworks, it is crucial that such research provide clear practical and theoretical insights/contributions. For now, the insights gained from your evaluation framework seem rather vague. Therefore, I will keep my initial score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647901448,
                "cdate": 1700647901448,
                "tmdate": 1700647901448,
                "mdate": 1700647901448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ssq6uFYyzM",
            "forum": "YD0GQBOFFZ",
            "replyto": "YD0GQBOFFZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1429/Reviewer_fbzR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1429/Reviewer_fbzR"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel evaluation framework for evaluating tabular data generators. The framework has been generated with a single mathematical objective, i.e., that a data generator should produce samples from the same joint distribution as the real data. The framework includes both existing and novel metrics which are then arranged along a  spectrum which holds for both model-free and model-based metrics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The main strength of the paper is that such work is very much needed. Additionally, I like the idea of arranging the metrics according to a spectrum that highlights the complexity of the relationships among features that a metric is able to capture."
                },
                "weaknesses": {
                    "value": "I think the paper needs a lot of rewriting (and probably more space, I would suggest to the authors to submit to a journal). \nAt times it is quite difficult to follow and a lot of the metrics that are presented in Table 1 are not covered at all in the main text. \nAlso, it is not feasible to think that one will evaluate their models according to all the metrics shown in Table 1. A significant contribution would be to identify different subsets of these metrics and show how to use them together to capture all the desired properties of the system (see, for example, what was done for multi-label classification problems in [1]). \n\nAlso, I have some questions regarding how the ML efficacy belongs to the substructure \"leave-one-out conditional distribution\". Indeed, in order to *leave-one-out* then you assume that the target is a single column. In many cases, the target might not be a single column. How would this affect your thesis? Even more importantly, you write that the implication holds if we do this *for all* $j$s (i.e., by leaving out all columns). The ML efficacy test leaves out a single column, so how do you get sufficiency in this case? Finally, to define the score you use the function argmax, what happens if the problem is a regression problem or binary? \n\nRegarding the full joint substructure you write: \"Sufficiency is hard to show because...\". Is it sufficient? Is it necessary? \nIn general, for all these substructures, I would have liked to see a much more structured approach to showing sufficiency and necessity. \n\nI am also not sure whether I fully understand the properties of the HALF baseline. Could you please clarify why is it useful and why it provides an upper bound? Also, do you have some proof of the upper and lower bounds provided by the baselines? \n\nAt a certain point on page 6, the authors write: \"We then use pp-plot to compare these two lists of probabilities.\" What is pp-plot? Why is it used? \n\nIn Table 2, why is Quality not reported for 2 models on the census dataset?\n\n\nMinor comments: \n- there is a typo on page 5 in the Missingness paragraph: $Q_v(c_j) = P_v(c)j)$\n- add a bird-eye view of subsection 3.2 rather than just starting to describe baselines one by one\n- put tables in the right format (i.e., as large as text)\n\n\n[1] Spolaor, N., Cherman, E. A., Metz, J., & Monard, M. C. (2013). A systematic review on experimental multi-label learning. Tech. rep. 362, Institute of Mathematics and Computational Sciences, University of Sao Paulo."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1429/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698605260719,
            "cdate": 1698605260719,
            "tmdate": 1699636071022,
            "mdate": 1699636071022,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LL92G3ktrX",
                "forum": "YD0GQBOFFZ",
                "replyto": "ssq6uFYyzM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1429/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1429/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewers for the helpful comments to make the paper more accessible and useful.\n\n**On rewriting and implementation feasibility**:\nTo help with readability, we will revise Section 3 such that the intuitive description in plain English is well separated from complex mathematical notation. We now also make sure the main text mentions all the metrics in Table 1 with references to the Appendix. Our intention in presenting Table 1 was not to provide the recipe for constructing a structured evaluation but to provide a summary of the structured framework and what it covers. Upon acceptance, we will open source our implementation, as mentioned in the list of contributions, so that any reader can use and build on it, and to ensure feasibility. To emphasize this point, we add in the caption to Table 1 that the code is at <url TBA>. Given this readily available codebase, we believe it reasonable and important to use ALL the metric groups instead of a subset of them. Section 5 and Figure 2 show precisely how all the metrics can work together to form a coherent and complete picture of the evaluation via the naturally ordering induced by the structured framework.\n\n**On ML efficacy and LOO**:\nWe thank the reviewer for bringing up these questions. If the target includes multiple columns, we can simply generalize the leave-one-out (LOO) to leave-n-out (LNO). The implication on how LNO is related to the objective of Q = P is the same as LOO, because all the dependencies described by the chain rule factorization would still be covered. Its natural ordering would be after the LOO and before the full joint when added to Figure 2. The reviewer is correct that if ML efficacy is tested on a single column, it would not be sufficient. This is why in our implementation we loop through all the columns. We now make a note that this is a slight generalization from how people usually use ML efficacy in the Leave-one-out subsection. Lastly, the binary problem is a special case of the classification problem with 2 classes, so the argmax would cover the binary case. Appendix A.3 lists the ML models used for both the binary and categorical classification. The reviewer is correct that the argmax would not be used for a regression problem. We did not include ML regressors because their measure of accuracy (i.e., RMSE) does not fall inside the range of [0,1], and hence, not in the right scale to be averaged with the other metrics. To fill this lack, the model-based LOO metrics is agnostic to column types and do cover all the columns. This is a strength of the model-based metrics that we now highlight explicitly in various parts of the paper.\n\n**On sufficiency**:\nWe thank the reviewer for pointing out the vagueness about the sufficiency related to the joint-distribution metric. Because we cannot show sufficiency, we now say it is likely not sufficient because there may be multiple ways that the implicit $f_q$ and $f_p$ can produce a discriminator to achieve perfect ROC AUC. The condition is necessary for Q = P, which we mention in the main text just before the text quoted. The approach we show necessity and/or sufficiency was outlined in the \u201cSketch of analysis\u201d subsection in the beginning of Section 3. We now add a few sentences in that sketch to describe in more detail our approach to show necessity and/or sufficiency. \n\n**On the HALF baseline**:\nThe idea of the HALF baseline is to split the real dataset into two, and then run the evaluation to compare the two datasets. It is useful because this ensures that both datasets are from the real data generating distribution, yet not direct copies of each other. In other words, the HALF baseline is the closest thing to meeting the proposed objective of Q =P and S $\\neq$ X. Thus, it should provide the desirable target to the metrics. We agree that \u201cupper bound\u201d is probably not the right word for the HALF baseline, because the SELF baseline beats it. We now change the \u201cupper bound\u201d associated with the HALF baseline to the \u201ctarget value\u201d in the text. For the SELF baseline, the upper bound on fidelity metrics follows from the fact that the synthetic and real distributions are equal (Q=P), and the lower bound on the privacy metrics follows from the fact that the synthetic dataset is a direct copy of the real dataset (S=X). How the metrics relate to Q=P is summarized in the Implementation column in Table 1. Also, the second row of Figure 2 shows that empirically, the SELF baseline is an upper bound, and the HALF baseline is also higher or equal to the methods.\n\n**On pp-plot**:\nA pp-plot is a plot that plots two cumulative distributions against each other. It is used to detect how different the two distributions are. If the two distributions are the same, the plot will follow the x=y line. We will revise Section 3.3 to clarify this."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258468688,
                "cdate": 1700258468688,
                "tmdate": 1700512251080,
                "mdate": 1700512251080,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]