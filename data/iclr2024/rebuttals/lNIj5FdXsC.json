[
    {
        "title": "Recurrent Distance-Encoding Neural Networks for Graph Representation Learning"
    },
    {
        "review": {
            "id": "U6gQ8fElik",
            "forum": "lNIj5FdXsC",
            "replyto": "lNIj5FdXsC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9385/Reviewer_hzhG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9385/Reviewer_hzhG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a new architecture for the graph learning, aggregating nodes with different distance and using a parallelizable linear recurrent network to encode the information flow while keeping the weights from vanishing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. paper is well written and easy to follow\n2. experiment is good"
                },
                "weaknesses": {
                    "value": "1. RNN here is strange, the assumption here is that the information flow from K-hop to K-1 hop. but information can also go \"outward\", from k-2 hop to k-1 hop. RNN is not good to model the graph information\n2. I assume all the GNN drawback will also be here in this Model (over smoothing, etc). The model can be regarded as another interpretation of massage passing(even though the weight is different), for the target node, the model will aggregate all the K-hop node information"
                },
                "questions": {
                    "value": "1. how the performance like if we change LRU to the vanilla RNN. seems LRU for the long range information pass is the key factor to win against MPNN."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9385/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9385/Reviewer_hzhG",
                        "ICLR.cc/2024/Conference/Submission9385/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698179837523,
            "cdate": 1698179837523,
            "tmdate": 1700666093039,
            "mdate": 1700666093039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kuQzjNs0X9",
                "forum": "lNIj5FdXsC",
                "replyto": "U6gQ8fElik",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hzhG"
                    },
                    "comment": {
                        "value": "We thank reviewer hzhG for taking the time to review our work, and are pleased she/he found our work to be well-written and our experiments to be good. We address the raised concerns as follows:\n\n**Q1. RNN here is strange, the assumption here is that the information flow from K-hop to K-1 hop. but information can also go \"outward\", from k-2 hop to k-1 hop. RNN is not good to model the graph information**.\n\n**Ans**: It\u2019s true that each edge in the graph allows \u201cbidirectional\u201d information flow. However, if we focus on a particular node for which we want to generate a representation, it\u2019s natural to categorize the other nodes into \u201cshells\u201d (Figure 1(b)). Nodes that belong to the same shell have the same shortest distance from the target node (considering running a breadth-first search from the target node). Then a recurrent neural network, which runs from the outermost shell to the innermost one, would be a reasonable choice to encode the neighborhood structure.\n\nBesides, our model indeed allows \u201cbidirectional\u201d information flow. For two different target nodes (Node 1 and Node 2), their shells are different, the K-1 hop and K hop neighbors of Node 1 can become the K hop and K-1 hop respectively for Node 2, and the information propagates reversely when we generate the representation for Node 2. Therefore, considering the set of node representations as a whole, we won\u2019t lose any structural information. This argument is supported by our rigorous theoretical analysis of expressiveness\n\n**Q2. I assume all the GNN drawback will also be here in this Model (over smoothing,  etc). The model can be regarded as another interpretation of massage passing(even though the weight is different), for the target node, the model will aggregate all the K-hop node information**.\n\n**Ans**: GRED doesn\u2019t have the drawbacks of MPNN because the computational graphs of GRED and MPNN are different. We have added a straightforward example to Appendix E of the updated PDF to illustrate their difference. To aggregate all nodes in the K-hop neighborhood, MPNN needs to perform message passing for K rounds. As a result,  MPNN\u2019s computational graph for the target node grows exponentially with K because the same neighbor will appear many times in the computational graph. The exponentially growing number of nodes are mixed together by MPNN to generate the representation for the target node, resulting in the known drawbacks (e.g., over-smoothing, over-squashing).  On the contrary, the computational graph of GRED includes each node in the K-hop neighborhood exactly once, and GRED further utilizes LRU to control how signals of distant nodes are propagated toward the target node. Therefore, GRED can more effectively leverage information beyond the local neighborhood than MPNN.\n\n**Q3. how the performance like if we change LRU to the vanilla RNN. seems LRU for the long range information pass is the key factor to win against MPNN**.\n\n**Ans**: We have conducted experiments to compare LRU with vanilla RNN:\n\n| ~   |  CIFAR10 $\\uparrow$ | ZINC $\\downarrow$ | Peptides-func $\\uparrow$ |\n| --- | --- | --- | --- |\n|    Best MPNN      | 67.312$\\pm$0.311 | 0.188$\\pm$0.004 | 0.6069$\\pm$0.0035   |\n| GRED$_{\\text{RNN}}$   | 69.215$\\pm$0.080 | 0.160$\\pm$0.005 | 0.4945$\\pm$0.0024 |\n| GRED$_{\\text{LRU}}$   | **75.370$\\pm$0.621** | **0.089$\\pm$0.004** | **0.7041$\\pm$0.0049** |\n\nWe replace the LRU component of GRED with a vanilla RNN:\n\n\\begin{equation}\n    \\mathbf{s}_{v, k}^{(l)} = \\text{tanh} ( \\mathbf{W}\\_{\\text{rec}} \\mathbf{s}\\_{v, k - 1}^{(l)} + \\mathbf{W}\\_{\\text{in}}\\mathbf{x}\\_{v, K - k}^{(l)})\n\\end{equation}\n\nwhere $\\mathbf{W}\\_{\\text{rec}} \\in \\mathbb{R}^{d_s \\times d_s}$ and $\\mathbf{W}\\_{\\text{in}} \\in \\mathbb{R}^{d_s \\times d}$ are two trainable real weight matrices. We use the same number of layers and the same $K$ for both models. We can observe that the performance of GRED with a vanilla RNN drops significantly. On CIFAR10 and ZINC where $K$ is not large, GRED$\\_{\\text{RNN}}$ still outperforms the best MPNN, but on Peptides-func where long-range information is needed (8 layers with $K=40$ per layer), the performance of GRED$\\_{\\text{RNN}}$ is worse than the best MPNN because vanilla RNNs are known to be difficult to train on long sequences, unlike the LRU.\n\nWe kindly ask the reviewer to consider raising her/his score if she/he thinks our response can address the concerns. We would be happy to answer further questions!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583791717,
                "cdate": 1700583791717,
                "tmdate": 1700583791717,
                "mdate": 1700583791717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6hi0ddXGBm",
                "forum": "lNIj5FdXsC",
                "replyto": "kuQzjNs0X9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9385/Reviewer_hzhG"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewer_hzhG"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the clarification"
                    },
                    "comment": {
                        "value": "thank you for the answers to my questions. My questions have been clarified. I'll raise my rating to weak accept."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666053898,
                "cdate": 1700666053898,
                "tmdate": 1700666053898,
                "mdate": 1700666053898,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2zusTTez9W",
            "forum": "lNIj5FdXsC",
            "replyto": "lNIj5FdXsC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9385/Reviewer_8Mos"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9385/Reviewer_8Mos"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new graph learning architecture called Graph Recurrent Encoding by Distance (GRED). GRED aims to address the challenges faced by existing approaches, such as message-passing neural networks (MPNNs) and graph transformers, by efficiently encoding information from distant nodes while avoiding the need for ad-hoc positional encodings. The paper provides a detailed explanation of the GRED architecture and supports its claims with theoretical analysis and empirical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Theoretical Analysis: The paper provides a theoretical analysis of the expressiveness of GRED, demonstrating its superiority over one-hop MPNNs. This analysis adds depth to the understanding of the method's capabilities. Furthermore, the authors provide an interesting analysis regarding the RNN filtering of the k-hop neighbor features.\n\n2. The presentation and clarity of the paper are very good. The authors provide comprehensive explanations of the key components of GRED, which is crucial for readers trying to understand the architecture at a deeper level. The theoretical analysis is rigorous, and the empirical results are presented in a well-organized manner, contributing to a comprehensive understanding of GRED's performance."
                },
                "weaknesses": {
                    "value": "1. Firstly, the novelty of the proposed architecture is somewhat limited, as there are already several similar approaches in the field that operate on K-hop neighborhoods in a similar manner. While the combination of permutation-invariant neural networks and linear recurrent networks is a sensible choice, it may not present a significant departure from existing methods. Specifically, the proposed method is very similar to [1], which proposed the following update rule: $h_u^{(t+1)} =COM(h_u^t, AGG_{u,1},..., AGG_{u,k} )$. The main difference is that the proposed approach uses an RNN for the $COM$ function. Moreover, there is no proper discussion of other k-hop approaches such as [2].\n\n2. Secondly, the experiments in the paper are conducted on a relatively limited set of datasets, focusing on long-rage benchmarks, and the absence of experiments on benchmark datasets like TUDatasets raises concerns about the generalizability of the proposed model. Expanding the experimental evaluation to a wider range of datasets would strengthen the paper's claims.\n\n3. A notable weak point of the paper is the unavailability of the source code for the proposed model. This omission hinders the reproducibility and transparency of the research. Releasing the source code is not only a common practice in the research community but is also crucial for enabling other researchers to validate and build upon the work presented in the paper. \n\nReferences: \n\n[1] Abboud, Ralph, Radoslav Dimitrov, and Ismail Ilkan Ceylan. \"Shortest path networks for graph property prediction.\" Learning on Graphs Conference. PMLR, 2022.\n\n[2] Nikolentzos, Giannis, George Dasoulas, and Michalis Vazirgiannis. \"k-hop graph neural networks.\" Neural Networks 130 (2020): 195-205."
                },
                "questions": {
                    "value": "1. Could the authors provide a more in-depth comparative analysis of their approach against existing methods that operate on k-hop neighborhoods? Highlighting specific strengths or weaknesses in comparison to these related approaches would help better position the novelty of this work.\n\n2. Considering that experiments are conducted on a limited set of datasets, could the authors discuss the generalizability of their proposed architecture to a wider range of datasets, including TUDatasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9385/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9385/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9385/Reviewer_8Mos"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668952365,
            "cdate": 1698668952365,
            "tmdate": 1699637183522,
            "mdate": 1699637183522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jpNuRNhITj",
                "forum": "lNIj5FdXsC",
                "replyto": "2zusTTez9W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8Mos"
                    },
                    "comment": {
                        "value": "We thank reviewer 8Mos for taking the time to review our work and are pleased she/he found: 1) our presentation to be very good | clear; 2) our theory to be interesting | rigorous; and 3) our empirical results to be well-organized and contributing to a comprehensive understanding of GRED's performance. \n\nWe admit that we missed several references on K-hop GNNs, and we agree that a more detailed discussion of these methods would help improve the understanding of our contributions. We address these concerns as follows:\n\n**Q1. The novelty of the proposed architecture is somewhat limited | Lack of a comparative analysis of existing K-hop GNNs**.\n\n**Ans**: Compared with existing K-hop GNNs, the novelty of our proposed architecture is a new way to encode the K-hop neighborhood, i.e., with LRU. **The use of LRU ensures our model theoretically sound expressiveness as well as ability to address the over-squashing problem in long-range reasoning tasks**. To see why the novelty of our model is significant, we provide a comparative analysis of existing K-hop GNNs here:\n\nAmong K-hop GNNs, MixHop [3] uses powers of the adjacency matrix to access k-hop nodes. k-hop GNN [2] iteratively applies MLPs to combine two consecutive hops and propagates information towards the target node. **While they have shown that higher hop information can improve the expressiveness of MPNNs, they suffer from over-squashing**, i.e., they cannot effectively utilize information from distant nodes because the signals of distant nodes would decay rapidly due to iterative neighborhood mixing. As a result, the largest neighborhood these approaches operate on is 3-hop. The drawback of these approaches is also pointed out by the SPN paper [1]. The SPN paper [1] proposes a conceptual update formula which is similar to ours:\n\n$$\nh\\_u^{(t+1)} = \\text{COM} ( h\\_u^{(t)}, \\text{AGG}\\_{u, 1}, \u2026, \\text{AGG}\\_{u, K})\n$$\n\n**However, SPN simply uses weighted summation as COM to aggregate K hops. The weighted summation cannot guarantee the expressiveness of the model since it cannot uniquely represent sequences. On the contrary, the LRU component of GRED can achieve expressiveness and long-range modeling capacity simultaneously**: LRU can express an injective mapping of sequences, and at the same time prevent distant information from decaying by learning proper eigenvalues of the transition matrix.\nWe have updated the related work to include a comparison of K-hop GNNs (marked in red in the updated PDF)\n\nTo validate that GRED can encode the K-hop neighborhood more effectively than SPN,\nWe evaluate GRED on two additional datasets from TUDataset and compare against SPN.\nAs shown in Part III of \u201cAdditional Experimental Results\u201d, GRED can outperform SPN with the same number of hops.\n\n[1] Shortest Path Networks for Graph Property Prediction. Learning on Graphs Conference. 2022\n\n[2] k-hop graph neural networks. Neural Networks. 2020\n\n[3] MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing. ICML. 2019"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581047254,
                "cdate": 1700581047254,
                "tmdate": 1700581704155,
                "mdate": 1700581704155,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W8u5ZMVoLw",
            "forum": "lNIj5FdXsC",
            "replyto": "lNIj5FdXsC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9385/Reviewer_bs5m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9385/Reviewer_bs5m"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a model architecture, GRED, for graph tasks by adopting a recurrent neural network (RNN) to propagate hidden states through multiple layers. It claims that the new architecture is more effective, utilizing the information of large neighborhoods with high efficiency, and theoretically proving the expressiveness. It also empirically shows that the performance of GRED is better than state-of-the-art graph transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.  The paper proposes a way to utilize RNN to adopt hidden states through multiple layers. It claims that the new method helps improve long-range information processing.\n2. The paper compares the proposed method on multiple benchmarks, and demonstrates its effectiveness against traditional GNN models.\n3. The description of the proposed method is well-written and easy to understand, though figure 1(a) is a bit unclear about what the \"skip\" stands for."
                },
                "weaknesses": {
                    "value": "1. For the training efficiency evaluation, it would be nice to include the memory consumption for different methods. Also, the performance gap on the task between GRIT and GRED is still significant, which makes the comparison a bit unfair. It would be better to compare with the architecture that has similar task performance.\n2. The method introduces a new hyperparameter K to be tuned. It would be nice to show how K is selected, and how the different selections of K can affect the task performance.\n3. Although the technique helps increase the range of nodes the model can process, unlike the graph transformer-based method, the distance is still limited by the selection of K and the number of layers of the model. It would be good to show some insights into how that super long-distance information affects the model performance."
                },
                "questions": {
                    "value": "The following papers seem related as well:\n1. Graph Transformers: Representing Long-Range Context for Graph Neural Networks with Global Attention, NeurIPS 2021\n2. Issues with attention for long-range reasoning: Lite Transformer with Long-Short Range Attention, ICLR 2020"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9385/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9385/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9385/Reviewer_bs5m"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808194411,
            "cdate": 1698808194411,
            "tmdate": 1700728195865,
            "mdate": 1700728195865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LLY9mid7n4",
                "forum": "lNIj5FdXsC",
                "replyto": "W8u5ZMVoLw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bs5m"
                    },
                    "comment": {
                        "value": "We thank reviewer bs5m for taking the time to review our work and are pleased she/he found our work to be well-written and easy to understand.  First, we would like to ask the reviewer to take a look at our post \u201cAdditional Experimental Results\u201d for updated results and the sensitivity analysis with respect to the value of $K$. We address the raised concerns as follows:\n\n**Q1. What is the \"skip\" in Figure 1(a)?**\n\n**Ans**: The \u201cskip\u201d in Figure 1(a) denotes an identity branch. We have added a code snippet of GRED to Appendix B of the updated PDF, to show how the computation is done exactly.\n\n**Q2. For the training efficiency evaluation, it would be nice to include the memory consumption for different methods. Also, the performance gap on the task between GRIT and GRED is still significant, which makes the comparison a bit unfair. It would be better to compare with the architecture that has similar task performance.**\n\n**Ans**: We have added the GPU memory consumption to Table 4 of the updated PDF. We also post it here:\n\n| Model | ZINC 12K     | CIFAR10 | Peptides-func | \n| --- | --- | --- | --- |\n| GRIT  | 25.6s / 1.9GB   | 244.4s / 4.6GB  | 225.6s / 22.5GB |\n| GRED (Ours)  | 4.1s / 1.4GB  | 27.8s / 1.4GB  | 158.9s / 18.5GB  | \n| Speedup | **6.2$\\times$**| **8.8$\\times$** |  **1.4$\\times$** | \n\nThe gap between the performance of GRIT and the updated performance of GRED is much less significant now. Especially on Peptides-func with $K=40$ GRED can outperform GRIT. GRED still maintains higher training efficiency than GRIT.\n\n\n**Q3. The method introduces a new hyperparameter K to be tuned. It would be nice to show how K is selected, and how the different selections of K can affect the task performance.**\n\n**Ans**: We have conducted experiments on how different $K$ values affect the performance of GRED. Please refer to Part I of our \u201cAdditional Experimental Results\u201d thread in our general response for a thorough analysis.\n\n**Q4. Although the technique helps increase the range of nodes the model can process, unlike the graph transformer-based method, the distance is still limited by the selection of K and the number of layers of the model. It would be good to show some insights into how that super long-distance information affects the model performance.**\n\n**Ans**: It\u2019s true that the range of nodes each layer can access is bounded by the value of $K$. However, for real-world datasets, the graph diameter won\u2019t increase linearly with the number of nodes. Therefore, even if the value of $K$ doesn\u2019t seem \u201csuper long\u201d, each layer of our model can access all nodes in the graph.\n\nFor all datasets we used in experiments, except for Peptides-func and Peptides-struct, our model fits into a single 24GB GPU even when $K$ is set to $K_{\\text{max}}$ (i.e., the maximum diameter of all graphs in the dataset). For Peptides-func and Peptides-struct, we use a $K$ value which is smaller than $K_{\\text{max}}$ in order to increase the depth of the model. In this sense, the choice of $K$ allows the model some flexibility when the hardware resource is constrained. In all cases, we ensure that model depth * $K$ is strictly larger than $K_{\\text{max}}$, so the receptive field of the entire GRED model is the same as graph transformers and GRED is able to extract global information.\n\n**Q5. Two related papers**\n\n**Ans**: We have added these two references to the related work (marked in red) of the updated PDF.\n\nWe kindly ask the reviewer to consider raising her/his score if she/he thinks our response can address the concerns. We would be happy to answer further questions!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580220005,
                "cdate": 1700580220005,
                "tmdate": 1700580220005,
                "mdate": 1700580220005,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sMnaU1FICT",
                "forum": "lNIj5FdXsC",
                "replyto": "LLY9mid7n4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9385/Reviewer_bs5m"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewer_bs5m"
                ],
                "content": {
                    "title": {
                        "value": "RE: Response to Reviewer bs5m"
                    },
                    "comment": {
                        "value": "Thanks for the reply! The response from the authors solved most of my concerns. I increased the score to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728302794,
                "cdate": 1700728302794,
                "tmdate": 1700728302794,
                "mdate": 1700728302794,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CAqsWMfcRw",
            "forum": "lNIj5FdXsC",
            "replyto": "lNIj5FdXsC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9385/Reviewer_e7jX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9385/Reviewer_e7jX"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a novel graph learning method that pools long-range information by aggregating nodes at different distances and using a linear recurrent network, leading to a computational efficient method that is competitive with state-of-the-art approaches for graph learning."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The evaluation is very thorough and the results validate and support the effectiveness of the proposed method. The computational effectiveness of the proposed model compared to equivalent transformer models (Table 4) is very good.\n- The proposed method is benchmarked against a wide range of methods and on multiple benchmarks.\n- Transformer models are good at modeling long-range interactions in graphs, and have been outperforming MPNN models, they however struggle with scaling and require custom encoding of the node positional embedding. The proposed approach is simple and intuitive and provides an alternative path towards solving long-range graph problems."
                },
                "weaknesses": {
                    "value": "1. A sensitivity analysis of GRED with respect to the choice of K is missing.\n2. The training efficiency analysis is conducted against a transformer model only, but it would be useful to understand how this method compares to MPNNs. In particular, each node will have its own sequence of sets of nodes at distance k, so there might not be any shared computation that can be leveraged like in iterative 1-hop message passing methods."
                },
                "questions": {
                    "value": "- How would this method perform on non long-range benchmarks? Would it underperform compared to MPNNs who might only need a local receptive field to solve a task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814931610,
            "cdate": 1698814931610,
            "tmdate": 1699637183283,
            "mdate": 1699637183283,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d329VKG3XU",
                "forum": "lNIj5FdXsC",
                "replyto": "CAqsWMfcRw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers",
                    "ICLR.cc/2024/Conference/Submission9385/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9385/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer e7jX"
                    },
                    "comment": {
                        "value": "We thank reviewer e7jX for taking the time to review our work, and we are pleased she/he found our evaluation to be very thorough and our results to validate the effectiveness of our model. We address the raised concerns as follows:\n\n**Q1. A sensitivity analysis of GRED with respect to the choice of K is missing**.\n\n**Ans**: This is now in Appendix D of the updated PDF. Please refer to Part I of our \u201cAdditional Experimental Results\u201d thread in our general response for a thorough analysis.\n\n**Q2. The training efficiency analysis is conducted against a transformer model only, \u2026, each node will have its own sequence of sets of nodes at distance k, so there might not be any shared computation that can be leveraged\u2026**\n\n**Ans**: It\u2019s true that each node has its own sequence of set representations, but the computations for all nodes in the graph can be done in batches. In the implementation, we use a matrix mask to indicate each node\u2019s $k$-hop neighbors (just like the adjacency matrix indicates each node\u2019s 1-hop neighbors). These masks are computed only once in preprocessing using the shortest path algorithm, and they are used to do aggregation at each hop. Then the input to LRU would be a batch of sequences of length K+1 (including the target node for each sequence). **We have added a code snippet of GRED in Appendix B of the updated PDF, to show how the computation is done**.\n\nTo compare the training efficiency of GRED with MPNN, we use the same number of layers but the MPNN counterpart only aggregates 1-hop neighbors at each layer. We measure the average training time per epoch:\n\n| Model | ZINC | CIFAR10 | Peptides-func |\n| --- | --- | --- | --- |\n| GRED | 4.1s | 27.8s | 158.9s |\n| MPNN | 2.8s | 21.7s | 91.6s |\n\nWe can observe that on ZINC and CIFAR10, the additional time required to use a larger neighborhood is marginal. On Peptides-func, the difference is greater since GRED uses $K=40$. However, this extra time, which is still less than the time required by GRIT, is acceptable considering the significant performance improvement\n\n**Q3. How would this method perform on non long-range benchmarks? Would it underperform compared to MPNNs who might only need a local receptive field to solve a task?**\n\n**Ans**: The datasets in Table 1 and Table 2 (ZINC) are from [4], which is a widely used benchmark. These datasets are not particularly long-range. PATTERN and CLUSTER model communities in social networks and all nodes can be reached within 3 hops. MNIST and CIFAR10 are nearest-neighbor graphs of pixels, and just like their normal versions local information is important. Our model performs well on these datasets. We further evaluate our model on two additional datasets from TUDataset (please refer to Part III of our \u201cAdditional Experimental Results\u201d thread) and our model also achieves good performance.\n\nOur model can adapt to both long-range and non-long-range tasks by learning proper eigenvalues of the LRU transition matrix that control how fast the filters decay as the distance increases. For a long-range task, the learned eigenvalues are close to 1 which better keeps distant information, while for a non-long-range task, the magnitudes of the learned eigenvalues are smaller (please refer to Figure 3 for an illustration of the eigenvalues after training). Additionally, the choice of $K$ gives our model some flexibility to adjust the receptive field, and we found the optimal $K$ for a non-long-range task lies between 1 and the diameter (Part I of \u201cAdditional Experimental Results\u201d)\n\n[4] Benchmarking Graph Neural Networks. JMLR. 2022\n\nWe kindly ask the reviewer to consider raising her/his score if she/he thinks our response can address the concerns. We would be happy to answer further questions!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579368834,
                "cdate": 1700579368834,
                "tmdate": 1700579368834,
                "mdate": 1700579368834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]