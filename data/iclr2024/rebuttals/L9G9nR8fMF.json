[
    {
        "title": "LayerAct: Advancing CNNs with BatchNorm through Layer-direction Normalization"
    },
    {
        "review": {
            "id": "PBCDekmPYv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7871/Reviewer_RDYZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7871/Reviewer_RDYZ"
            ],
            "forum": "L9G9nR8fMF",
            "replyto": "L9G9nR8fMF",
            "content": {
                "summary": {
                    "value": "This paper introduces a novel activation mechanism for Convolutional Neural Networks (CNNs) with BatchNorm, addressing limitations of existing activation functions, specifically the trade-off problem and the large variance of noise-robustness across samples. The proposed LayerAct functions aim to provide layer-level activation, reducing noise fluctuations in activation outputs and achieving noise-robustness independently of the activation's saturation state. The authors present a comprehensive analysis and experimental results demonstrating the superiority of LayerAct functions over element-level activation functions in terms of noise-robustness. Additionally, they show that LayerAct functions perform exceptionally well in handling noisy datasets, outperforming element-level activation functions, while also achieving superior performance on clean datasets in most cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper makes a significant contribution to the field of deep learning by introducing the concept of LayerAct functions, which address limitations in existing activation functions. This novel approach provides a valuable addition to the toolbox of techniques for improving the robustness and performance of CNNs.\n\n- This paper is written in a clear and easily comprehensible manner, making it easy for readers to follow."
                },
                "weaknesses": {
                    "value": "see Questions."
                },
                "questions": {
                    "value": "- Some existing advanced batch normalization improvements like IEBN [1] and SwitchNorm [2] have shown enhanced performance. It would be interesting to investigate whether LayerAct can further improve the performance of these normalization methods.\n\n- While I understand that this paper discusses CNNs, there is a growing need for advanced activation functions in various other network architectures, including transformers and UNets. I'd like to know if the proposed method has the potential to be adapted to these advanced network structures.\n\n- I still don't quite understand the advantage of \"layer-direction\" activation over \"element-wise\" activation. Could the author please provide a concise explanation with simple examples or a summary?\n\n- Additionally, I'd be interested in understanding in which applications LayerAct might excel or not excel. For example, we have instance norm for tasks like style transfer, batch norm for CNN-based classification tasks, and layer norm for transformer-related tasks. Can LayerAct be analyzed and discussed in a similar manner, suggesting suitable application areas?\n\n- The author needs to clarify the above questions. If these issues are addressed, I will consider these clarifications along with feedback from other reviewers in deciding whether to raise my score.\n\n[1] Instance Enhancement Batch Normalization: An Adaptive Regulator of Batch Noise, AAAI\n\n[2] Differentiable Learning-to-Normalize via Switchable Normalization, ICLR"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7871/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7871/Reviewer_RDYZ",
                        "ICLR.cc/2024/Conference/Submission7871/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7871/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697298226930,
            "cdate": 1697298226930,
            "tmdate": 1700728066443,
            "mdate": 1700728066443,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kSoIO1VtVn",
                "forum": "L9G9nR8fMF",
                "replyto": "PBCDekmPYv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question 1. The potential of LayerAct to be adapted to advanced network structures. \n#### 1-1. The potential of LayerAct on transformers and networks with LayerNorm. \n- Discussing the interplay between LayerAct and LayerNorm is crucial for investigating LayerAct\u2019s potential in transformer-based architectures, given LayerNorm's significant role in such networks. When LayerNorm is placed before the activation layer, it normalizes the mean and variance of the activation input. This pre-normalization results in the activation output of a LayerAct function being similar to that of the corresponding element-level activation function.\n- For example, consider a network using LayerNorm without an affine function (no gain and bias) and placing the LayerNorm layer right before the activation layer. Here, the normalization output of the LayerNorm layers serves as the activation input. Consequently, in this scenario, the normalization output of a LayerNorm layer is $n^{ \\\\hat{ LN } } = \\\\frac{ y -\\\\mu^2\\_y }{\\\\sqrt{\\\\sigma^2\\_y + \\\\alpha}}$. This leads to the output of SiLU $n^{ \\\\hat{ LN } } s\\\\left(n^{ \\\\hat{ LN } } \\\\right)$ and LA-SiLU $n^{ \\\\hat{ LN } } s \\\\left( n \\\\right)$ to be exactly same as $n = n^{\\\\hat{ LN }}\\_i$. \n- As such, the closer the output of the LayerNorm layer approximates the normalized input of LayerAct\u2019s activation scale function $s$ (i.e., the more gain and bias approximate to one and zero, respectively), the more the benefits of LayerAct are reduced.\n- Please note that LayerAct has benefits as an activation method, (LayerAct is not a normalization method): i) it addresses the trade-off problem between important properties of activation, and ii) has lower variance of noise-robustness across samples. These benefits can be maintained when used with LayerNorm that includes an affine function. In such scenarios, the layer-direction normalization in LayerNorm and LayerAct differs, represented as $n^{LN} = g \\\\cdot \\\\frac{ y -\\\\mu^2_y }{\\\\sqrt{\\\\sigma^2_y + \\\\alpha}} + b $ and $ n= \\\\frac{ y -\\\\mu^2_y }{\\\\sqrt{\\\\sigma^2_y + \\\\alpha}}$, respectively. \n- This implies that LayerAct can maintain its benefit when utilized with LayerNorm with an affine function, although it might be comparatively less than that on CNN-based networks with BatchNorm. \n- To validate this, we are conducting additional experiments with networks utilizing LayerNorm on CIFAR10 and CIFAR100. We will report the experimental results as soon as possible. \n\n#### 1-2. The potential of LayerAct on UNet. \n- LayerAct might have the potential to improve a UNet as it is a CNN-based architecture. However, we cannot be certain, as UNet is for the image segmentation task, while our experimental results were only on image classification tasks. \n- To address your concerns directly, we are conducting additional experiments with UNets on the brain image segmentation task. We will share the results of the experiment as soon as possible. \n\n### Question 2. Investigation whether LayerAct can further improve the performance of other batch-direction normalization methods. \n- We agree that evaluating performance on networks with advanced normalization methods is important. \n- Considering our main contribution in this work is introducing a new activation method to enhance the performance of CNNs with BatchNorm, we plan to conduct additional experiments with ResNets with IEBN, decorrelated batch normalization (DBN; an advanced normalization method that utilizes batch-direction normalization introduced by Huang et al (2018)), and SwitchNorm. We will report the results of the remaining networks as soon as possible.\n- However, LayerAct\u2019s effectiveness in networks using SwitchNorm is may be tempered, as SwitchNorm utilizes LayerNorm. This aligns with our earlier response, where we noted that LayerAct\u2019s benefits might be reduced when LayerNorm precedes it.\n\nHuang, L., Yang, D., Lang, B., & Deng, J. \u201cDecorrelated batch normalization.\u201d CVPR. 2018."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944674019,
                "cdate": 1699944674019,
                "tmdate": 1700704025201,
                "mdate": 1700704025201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WA09R0wRPZ",
                "forum": "L9G9nR8fMF",
                "replyto": "PBCDekmPYv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question 3. The advantage of layer-level activation over element-level activation. \n- Thanks to the reviewers, we recognized the lack of a detailed discussion on one-side saturation of element-level activation functions, zero-like mean activation, and the trade-off problem between them. We will enhance our manuscript with a comprehensive explanation about them in the Appendix, based on this response. \n- In this response, we aim to clarify LayerAct\u2019s contributions as an activation function. To this end, we will detail the limitations of existing element-level activation functions and how LayerAct can address those. \n\n#### 3-1. Summary of this response. \n- LayerAct functions are specifically designed to overcome the limitations of existing activation functions. These limitations include i) the trade-off problem between one-side saturation and zero-like mean activation, and ii) the large variance in noise-robustness across samples. \n- Notably, addressing the trade-off problem stands as a unique contribution of LayerAct. This benefit is unachievable with other activation functions, regardless of their integration with LayerNorm. \n- LayerAct has a lower upper bound of activation fluctuation compared to element-level activation functions with the same activation input vector. This is substantiated by our experimental analysis detailed in Subsection 4.1 of the manuscript and the experimental results on noisy image datasets. \n\n#### 3-2. Important properties of activation functions and the trade-off problem. \n- The saturation state of an activation function contributes to robustness against shifts in activation input, as the changes in saturated activation inputs minimally affect the output. However, early activation functions like Sigmoid and Tanh, which saturate both positive and negative sides, suffer from the vanishing gradient problem.\n- To overcome this while maintaining noise-robustness, one-sided saturation became an important property after the great success of ReLU. ReLU can ensure noise-robustness (as the outputs are not affected by the input shift in the saturation state) and avoids vanishing gradient problems by allowing large positive outputs.\n- However, functions like ReLU, which do not allow negative output, encounter another issue of bias shift, leading to ineffective and inefficient training. After Clevert et al. (2016) demonstrated that an activation mean closer to zero can solve such a problem, \u201czero-like mean activation\u201d has become an important property of activation functions.\n- To incorporate both properties, one-sided saturation and zero-like mean activation, activation functions such as ELU, GELU, and SiLU were designed to allow small negative output to push the activation mean towards zero while saturating large negative output for noise-robustness.\n- Nevertheless, a problem still remains: there is a trade-off between one-sided saturation and zero-like mean activation. Saturating large negative output naturally restricts the zero-like mean activation. For example, allowing larger negative outputs pushes the activation mean closer to zero but diminishes the noise-robustness of the activation function as the range of the saturation state decreases.\n- Placing a LayerNorm layer parallel to an activation layer cannot address this trade-off problem. Large negative outputs will be restricted for saturation state. This is because the range of the saturation state entirely depends on the design of the activation function.\n\n#### 3-3. LayerAct can address the trade-off problem. \n- LayerAct functions address the trade-off between saturation and zero-like mean activation by integrating layer-direction normalization **into** the activation. The layer-direction normalization only determines the input of the activation scale function, which defines the saturation state (where $s\\\\left(n_{i}\\\\right)\\\\approx0)$. \n- This mechanism of LayerAct cuts off the link between saturation and negative activation output. Hence, LayerAct can produce larger negative output while maintaining saturation states. For example, in a scenario where $\\\\mu_{y}\\\\ll0$, the activation output $a_{i}=\\\\mu_{y}/2\\\\ll0$ where the activation input $y_{i}=\\\\mu_{y}$.  \n- In conclusion, despite its structural simplicity, LayerAct functions offer a significant contribution by addressing the trade-off problem of activation. \n\nClevert, Djork-Arn\u00e9, Thomas Unterthiner, and Sepp Hochreiter. \"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUS).\" ICLR. 2016."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944702525,
                "cdate": 1699944702525,
                "tmdate": 1699944702525,
                "mdate": 1699944702525,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zjifTmavp0",
                "forum": "L9G9nR8fMF",
                "replyto": "PBCDekmPYv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "#### 3-4. Element-level activation functions have large variance in noise-robustness across samples\n- The noise-robustness of element-level activation functions relies only on the saturation state. This implies that existing activation functions can ensure noise-robustness for samples only when a sufficiently large number of elements are in the saturation state, not when there are fewer elements in the saturation state. \n- For example, consider a ReLU layer. The samples with the lowest activation fluctuation are those where the activation input $y_{i}\\\\ll0$ for all $i$. Conversely, when the activation input $y_{i}\\\\gg0$ for all $i$, shifts in the activation input are directly transferred to the activation output. \n- We demonstrated this in Subsection 2.3 of our manuscript with activation fluctuation. Equation 4 shows that the upper bound of activation fluctuation is related to the activation scale, $\\\\| s\\\\left(\\*\\\\right) \\\\|$ and $\\\\| s\\\\left(\\\\hat{\\*} \\\\right) - s\\\\left(\\*\\\\right) \\\\|$. \n\n#### 3-5. LayerAct has a lower upper bound of noise-robustness across samples compared to element-level activation. \n- As discussed in Subsection 3.2 with Equations 8 and 9 in our manuscript, LayerAct functions have a lower upper bound of activation fluctuation compared to element-level activation functions with the same activation input vector $y$. This results the LayerAct to have lower variance of noise-robustness compares to element-level activation functions across samples. Our experimental analysis on the distribution of activation fluctuation in Subsection 4.1 supports this. \nClevert, Djork-Arn\u00e9, Thomas Unterthiner, and Sepp Hochreiter. \"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUS).\" ICLR. 2016.\n\n### Question 4. Application of LayerAct. \n- LayerAct has demonstrated its potential in image classification tasks using CNNs with BatchNorm, as evidenced by our experimental results on CIFAR10, CIFAR100, and ImageNet. Notably, the noise-robustness of LayerAct during inference is particularly advantageous for tasks where models have to be trained on clean datasets but need to classify or segment noisy images. \n- A relevant example is the classification of satellite images, which can be significantly affected by various external factors including weather conditions (Pritt, et al., 2017). Another example is task on motion blurred images in real-world due to the movement of the object or camera (Dai, 2008). \n- We would like to emphasize that the noisy datasets, which are utilized to evaluate the performance of the networks with LayerAct, include such real-world noises. \n\nPritt, M., and Gary C. \"Satellite image classification with deep learning.\u201d IEEE applied imagery pattern recognition workshop (AIPR). 2017.\nDai, S., & Wu, Y. \u201cMotion from blur.\u201d CVPR. 2008.\n\n#### Concluding remarks\n- We would like to reemphasize that LayerAct can enhance networks with BatchNorm through its ability to address the limitations of element-level activation functions. This unique contribution is unattainable through the parallel use of LayerNorm and element-level activation functions. \n- Once again, we sincerely appreciate your time and effort in reviewing our work. We have clarified the contribution, limitation and application of LayerAct, and have conducted additional experiments to enhance the precision and robustness of LayerAct. If there are any further concerns or questions, we are more than willing to address them."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944723094,
                "cdate": 1699944723094,
                "tmdate": 1699944723094,
                "mdate": 1699944723094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "COpVHlHSRj",
                "forum": "L9G9nR8fMF",
                "replyto": "PBCDekmPYv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "In this additional response, we present the results of additional experiments. We appreciate your comments that encouraged us to conduct valuable experiments.  \n\n### 1. LayerAct with UNet. \n- We agree that investigating the potential of LayerAct on different network architectures are important. \n- We conducted additional experiments on UNet and UNet++, which are CNN-based architectures for medical image segmentation tasks. For detailed analysis and results, we refer you to our Meta response 3.\n- The experimental results demonstrate that networks with LA-SiLU perform better than those with ReLU and SiLU in every instance. This highlights LayerAct\u2019s practical potential in various CNN-based architectures and segmentation tasks. \n\n### 2. ResNets with SwitchNorm, IEBN, and DBN. \n- To examine whether LayerAct can enhance the performance of networks with normalization methods other than BatchNorm, we trained ResNets with LayerNorm, SwitchNorm, IEBN, and DBN on the CIFAR10 and CIFAR100.\n- We used same experiment setting with those of our manuscript. We report the average accuracy over 10 runs for the experiments with SwitchNorm and IEBN, and average accuracy over 5 runs for those with DBN. \n- Analysis of the experimental results revealed that LayerAct functions are not effectively compatible with normalizations that can cause a large variance in the channel means, such as SwitchNorm and IEBN. This incompatibility arises because LayerAct is more sensitive to such large variance between channel means, which resulting channels with smaller means to be more likely to become inactivated compared to those with larger means."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656329985,
                "cdate": 1700656329985,
                "tmdate": 1700693895248,
                "mdate": 1700693895248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yobi7XXRHe",
                "forum": "L9G9nR8fMF",
                "replyto": "PBCDekmPYv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "#### 2-1. ResNets with SwitchNorm. \n- SwitchNorm utilize the weighted average of three normalization method, BatchNorm, LayerNorm, and InstanceNorm. \n\n| Data | Model | Activation | Clean | Total | Noise | Blur | Digital | Weather | Extra | \n| - | - | - | - | - | - | - | - | - | - | \n| CIFAR10 | ResNet20 | ReLU |89.65 |72.4 |56.07 |71.35 |75.24 |80.43 |74.81 | \n| CIFAR10 | ResNet20 | SiLU |**90.6** |**74.17** |**57.7** |**72.94** |**77.57** |**82.21** |**76.33** | \n| CIFAR10 | ResNet20 | LA-SiLU |89.56 |72.43 |55.9 |71.05 |75.47 |80.87 |74.73 | \n\n- ResNet32 with SwitchNorm on CIFAR10.\n\n| Data | Model | Activation | Clean | Total | Noise | Blur | Digital | Weather | Extra | \n| - | - | - | - | - | - | - | - | - | - | \n| CIFAR10 | ResNet32 | ReLU |90.7 |73.9 |58.24 |72.84 |76.3 |81.79 |76.42 | \n| CIFAR10 | ResNet32 | SiLU |**90.79** |**74.65** |**58.78** |**73.39** |**77.64** |**82.57** |**76.91** | \n| CIFAR10 | ResNet32 | LA-SiLU |89.94 |72.72 |56.15 |71.07 |75.52 |81.48 |75.23 | \n\n- ResNet44 with SwitchNorm on CIFAR10.\n\n| Data | Model | Activation | Clean | Total | Noise | Blur | Digital | Weather | Extra | \n| - | - | - | - | - | - | - | - | - | - | \n| CIFAR10 | ResNet44 | ReLU |**91.4** |**74.65** |**58.1** |**74.01** |**77.18** |**82.81** |**76.99** | \n| CIFAR10 | ResNet44 | SiLU |74.48 |61.7 |49.5 |60.58 |64.11 |67.87 |63.41 | \n| CIFAR10 | ResNet44 | LA-SiLU |89.36 |72.02 |54.85 |70.59 |75.07 |81.05 |74.24 | \n\n- ResNet20 with SwitchNorm on CIFAR100.\n\n| Data | Model | Activation | Clean | Total | Noise | Blur | Digital | Weather | Extra | \n| - | - | - | - | - | - | - | - | - | - | \n| CIFAR100 | ResNet20 | ReLU |57.36 |37.01 |20.61 |38.43 |39.75 |43.56 |38.59 | \n| CIFAR100 | ResNet20 | SiLU |**64.55** |**42.96** |**25.13** |**43.91** |**46.35** |**50.42** |**44.55** | \n| CIFAR100 | ResNet20 | LA-SiLU |63.88 |41.76 |23.23 |42.83 |45.28 |49.43 |43.4 | \n\n- ResNet32 with SwitchNorm on CIFAR100.\n\n| Data | Model | Activation | Clean | Total | Noise | Blur | Digital | Weather | Extra | \n| - | - | - | - | - | - | - | - | - | - | \n| CIFAR100 | ResNet32 | ReLU |60.19 |38.81 |21.89 |40.02 |41.3 |46.0 |40.61 | \n| CIFAR100 | ResNet32 | SiLU |**64.35** |**42.45** |**25.00** |43.08 |**45.86** |49.92 |**44.00** | \n| CIFAR100 | ResNet32 | LA-SiLU |64.05 |42.27 |23.72 |**43.46** |45.5 |**50.16** |43.89 | \n\n- ResNet44 with SwitchNorm on CIFAR100.\n\n| Data | Model | Activation | Clean | Total | Noise | Blur | Digital | Weather | Extra | \n| - | - | - | - | - | - | - | - | - | - | \n| CIFAR100 | ResNet44 | ReLU |61.64 |39.93 |22.64 |41.03 |42.47 |47.54 |41.67 | \n| CIFAR100 | ResNet44 | SiLU |44.8 |29.57 |18.11 |29.96 |31.98 |34.21 |30.73 | \n| CIFAR100 | ResNet44 | LA-SiLU |**62.99** |**41.91** |**22.86** |**43.06** |**46.0**2 |**49.79** |**43.06** |\n\n- Our experiments exhibit that the combination of SwitchNorm and LayerAct is not effective. \n- Although further investigation is necessary to address why LayerAct is incompatible with SwitchNorm, our preliminary analysis is because LayerAct is more sensitive to the presence of similar channel characteristics across samples compared to element-level activation functions. If two samples display similar orders in the mean and variance of their channels, LayerAct, which employs layer-direction normalization for activation, tends to yield similar activation outputs.\n- BatchNorm distinctively ensure different channel characteristics between samples. While LayerNorm does not inherently differentiate channels across samples, it does not actively homogenize them either, thus preserving the natural order of mean and variance among channels. On the other hand, InstanceNorm actively normalizes the mean and variance of channels to be more uniform.\n- Consequently, due to the integrated normalization approach of SwitchNorm, which combines three methods, we expect that the effect of InstanceNorm tends to homogenize channel characteristics, rendering LayerAct functions susceptible to inefficiency."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656372627,
                "cdate": 1700656372627,
                "tmdate": 1700656782084,
                "mdate": 1700656782084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5ZEQqPo7Ww",
                "forum": "L9G9nR8fMF",
                "replyto": "PBCDekmPYv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Reviewer_RDYZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Reviewer_RDYZ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for you response."
                    },
                    "comment": {
                        "value": "Thank you for your detailed response; my concerns have been addressed. I lean towards improving my scores. Additionally, I suggest that you can now include the relevant citations and additional experiments in the revised pdf (highlighted, for example, in blue font).. Thank you for your efforts."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728012391,
                "cdate": 1700728012391,
                "tmdate": 1700729686578,
                "mdate": 1700729686578,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d0r6a5IeTG",
            "forum": "L9G9nR8fMF",
            "replyto": "L9G9nR8fMF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7871/Reviewer_1Eep"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7871/Reviewer_1Eep"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an activation mechanism called LayerAct that combines layer normalization in the (general) sigmoid-linear-units to improve the noise-robust of the network. It empirically shows that the proposed LayerAct functions have a zero-like mean activation and are more noise-robustness. Experimental results with three clean and three out-of-distribution benchmark datasets for image classification tasks show the proposed LayerAct functions output perform the baselines on noisy datasets, and also is also superior on clean datasets in most cases."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The topic is interesting and important for the community.  \n2. The proposed LayerAct can improve the performance marginally over the baselines, showing the potential in practice. \n3. It is glad to see the proposed LayerAct exhibit superior noise-robustness compared to element-level activation functions."
                },
                "weaknesses": {
                    "value": "1. I think the technical contribution is overall low. The proposed LayerAct can be viewed as Layer Normalization combining the (general) sigmoid-linear-units. Both of them are good methods in improving the performance of neural network. It is not surprise the proposed method can improve the performance over the original activation functions. It is good that this paper addresses the robust of the proposed method, however I have concerns in the second point. \n\n2. I have concerns on the clarity of why LayerAct is noise-robust. The main claim is that the scale activation function is bounded (Eqn.8 and Eqn.9), thus the model is robust. But, bounding the activation function cannot bounding the $y_i$, which can still be not robust. This paper should well address this point. It is true the proposed LayerAct can obtain good empirical results on corruption datasets. However, this is not surprising, because the previous methods[1] have show that the combination of Batch-Free normalization (e.g., LayerNorm) and BatchNorm can be more robust for distribution shift (e.g., corruption). The main insights is that LayerNorm can alleviate the train-inference inconsistency problem of BatchNorm (I noted all experiments on noise-robust uses the networks with BatchNorm). I want to ask whether the proposed LayerAct can be noise-robust on the network without BatchNorm?\n\n3. I have concern on the title of this paper. The title addresses \u201cADVANCING CNNS WITH BATCHNORM THROUGH LAYER-DIRECTION NORMALIZATION\u201d. However, I find the description of the proposed LayerAct is independent to the BatchNorm (e.g, this paper doesnot say how LayerAct alleviates the problem of CNN with BatchNorm). I think this paper should well clarify it\n\n4. This paper has some imprecise descriptions:  \n(1)I have concerns on this \u201cSpecifically, we propose a novel layer-level activation (LayerAct) mechanism, along with two as sociated functions. This advancement combines batch-direction normalization with the effects of layer-direction normalization\u201d. How does LayerAct combines both? If yes, why the experiments run on the network with BatchNorm? How does the LayerAct works on the network without BatchNorm.  \n(2) This paper claims \u201cOne-sided saturation avoids the vanishing gradient problem while maintaining noise-robustness\u201d. Why the One-sided saturation avoids the vanishing gradient problem? Based on my understanding, the saturation state will cause no gradient.   \n(3)why \u201d the sum of activation scale $\\|s(n^{LN})\\|$ will be similar across all samples\u201d?  please clarify it in detail. \n\n\nOther minors:  \n\u201cpay cloase attention\u201d in page 1. \n\n \n**Ref:**  \n[1] Delving into the Estimation Shift of Batch Normalization in a Network. CVPR 2022"
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7871/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747270055,
            "cdate": 1698747270055,
            "tmdate": 1699636965234,
            "mdate": 1699636965234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NAdFqOwIG2",
                "forum": "L9G9nR8fMF",
                "replyto": "d0r6a5IeTG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your comments. We are glad to see that our topic and experiments meet your preference. In response to your concerns and questions, we describe our responses as follows.\n\n### Weakness 1. I think the technical contribution is overall low. & Weakness 4-2. About the statement on one-sided saturation.\n- Thanks to the reviewers, we recognized the lack of detailed discussion on one-sided saturation, zero-like mean activation, and the trade-off problem between them. We will enhance our manuscript with a comprehensive explanation about them in the Appendix, based on the following response.\n- In this response, we aim to clarify LayerAct's contributions as a new activation mechanism. To this end, we will detail the limitations of existing element-level activation functions that cannot be addressed by the mere combination of LayerNorm and activation functions. This response also resolves the concerns raised about one-sided saturation in Weakness 4 (See 1-2 of this response).\n\n#### 1-1. Summary of this response. \n- We acknowledge that the structure of LayerAct, a combination of LayerNorm and activation functions, is simple. Behind this simple mechanism, it is important to emphasize that LayerAct offers unique benefits that cannot be achieved through utilizing LayerNorm and element-level activation layers together.\n- LayerAct functions specifically address the limitations of existing activation functions: i) the trade-off problem between one-sided saturation and zero-like mean activation, and ii) the large variance of noise-robustness across samples. In this response, we focus on the first benefit, as the significance of noise-robustness is already well recognized (For the response to Weakness 2 about the noise-robustness of LayerAct, please see the next response).\n- Notably, addressing the trade-off problem stands as a unique contribution of LayerAct. This benefit is unachievable with other activation functions, regardless of their integration with LayerNorm.\n\n#### 1-2. Important properties of activation functions and the trade-off problem. \n- The saturation state of an activation function contributes to robustness against shifts in activation input, as the changes in saturated activation inputs minimally affect the output. However, early activation functions like Sigmoid and Tanh, which saturate both positive and negative sides, suffer from the vanishing gradient problem.\n- To overcome this while maintaining noise-robustness, one-sided saturation became an important property after the great success of ReLU. ReLU can ensure noise-robustness (as the outputs are not affected by the input shift in the saturation state) and avoids vanishing gradient problems by allowing large positive outputs.\n- However, functions like ReLU, which do not allow negative output, encounter another issue of bias shift, leading to ineffective and inefficient training. After Clevert et al. (2016) demonstrated that an activation mean closer to zero can solve such a problem, \u201czero-like mean activation\u201d has become an important property of activation functions.\n- To incorporate both properties, one-sided saturation and zero-like mean activation, activation functions such as ELU, GELU, and SiLU were designed to allow small negative output to push the activation mean towards zero while saturating large negative output for noise-robustness.\n- Nevertheless, a problem still remains: there is a trade-off between one-sided saturation and zero-like mean activation. Saturating large negative output naturally restricts the zero-like mean activation. For example, allowing larger negative outputs pushes the activation mean closer to zero but diminishes the noise-robustness of the activation function as the range of the saturation state decreases.\n- Placing a LayerNorm layer parallel to an activation layer cannot address this trade-off problem. Large negative outputs will be restricted for saturation state. This is because the range of the saturation state entirely depends on the design of the activation function. \n\nClevert, Djork-Arn\u00e9, Thomas Unterthiner, and Sepp Hochreiter. \"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUS).\" ICLR. 2016."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944542056,
                "cdate": 1699944542056,
                "tmdate": 1699944542056,
                "mdate": 1699944542056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uu9ktt6eSR",
                "forum": "L9G9nR8fMF",
                "replyto": "d0r6a5IeTG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "#### 1-3. LayerAct can address the trade-off problem. \n- LayerAct functions address the trade-off between saturation and zero-like mean activation by integrating layer-direction normalization **into** the activation. The layer-direction normalization only determines the input of the activation scale function, which defines the saturation state (where $s\\\\left(n_{i}\\\\right)\\\\approx0)$. \n- This mechanism of LayerAct cuts off the link between saturation and negative activation output. Hence, LayerAct can produce larger negative output while maintaining saturation states. For example, in a scenario where $\\\\mu_{y}\\\\ll0$, the activation output $a_{i}=\\\\mu_{y}/2\\\\ll0$ where the activation input $y_{i}=\\\\mu_{y}$.  \n- In conclusion, despite its structural simplicity, LayerAct functions offer a significant contribution by addressing the trade-off problem of activation. \n\n\n### Weakness 2. Concerns on the noise-robustness of LayerAct.\n#### 2-1. The activation input $y_{i}$ is not bounded, which impact noise-robustness significantly. \n- We agree that the activation input $y_{i}$ significantly influences activation fluctuation. However, it is important to note that this challenge applies to all activation functions, not only to LayerAct functions. \n- Our objective was to demonstrate that LayerAct functions have smaller activation fluctuations **compared to** other activation functions under identical conditions, in our case, the same activation input.\n- We recognize that our explanation in Subsection 3.2 of the submitted manuscript has not been sufficiently clear. Our objective was to demonstrate that LayerAct functions have smaller activation fluctuations **compared to** other activation functions under identical conditions, in our case, the same activation input.\n- Comparing methods under identical conditions has been used to analyze the effects of a method. Santurkar et al. (2018) compared networks with and without BatchNorm under the same gradient conditions to assess BatchNorm\u2019s impact on optimization. Similarly, Xu et al. (2019) conducted comparisons of different variants of LayerNorm under the same input conditions. \n- In this way, we showed that LayerAct functions can ensure more robust processing during the forward pass compared to other activation functions, given the same level of activation input. This benefit of LayerAct is supported by both mathematical analysis (in Subsection 3.2) and experimental results (in Subsection 4.1). \n- Thus, unless the training process does not lead to the generation of larger activation inputs in networks utilizing LayerAct, LayerAct is noise-robust compared to element-level activation functions. Please refer to Figure 7 in Appendix E, which demonstrates the distribution of activation input across activation functions. The figure shows that LayerAct functions typically maintain a similar or lower mean of activation inputs.\n- In summary, LayerAct functions are designed to facilitate more robust processing during the forward pass, as was the primary aim of Equations 8 and 9. We acknowledge the need for clarity in our manuscript on this matter and will make revisions to eliminate any confusion.\nSanturkar, S., Tsipras, D., Ilyas, A., and Madry, A. \u201cHow does batch normalization help optimization?\u201d. Neurips. 2018\nXu, J., Sun, X., Zhang, Z., Zhao, G., and Lin, J. \u201cUnderstanding and improving layer normalization.\u201d Neurips. 2019\n\n#### 2-2. Does the noise-robustness of LayerAct maintain without BatchNorm? \n- It is important to highlight that the activation input $y_{i}$ in Equations 8 and 9 is not exclusively derived from the output of a normalization layer. Additionally, in the experimental analysis detailed in Subsection 4.1, we did not use any normalization methods. The results from these experiments affirm that the noise-robustness of LayerAct is not contingent on the use of normalization layers. \n- To directly address your concern, we are conducting additional experiments using ResNets without any normalization layers. We have demonstrated the experimental results in our global response. In cases of networks without any normalization method, certain trials of ResNet20 and 44 with SiLU exploded during training. Consequently, we are presenting the result from networks with ReLU and LA-SiLU. \n- On clean datasets, the performance varied with the complexity of the networks: ResNet 32 and 44 with ReLU demonstrated better performance compared to those with LA-SiLU on both CIFAR10 and CIFAR100, whereas the opposite was true for ResNet. \n- Here, it is important to observe that networks with LA-SiLU outperformed those with ReLU on noisy datasets in all cases, which reconfirm the intended utility of LayerAct mechanism and contribution of our work. This experimental result shows that the noise-robustness of LayerAct is independent of the usage of normalization."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944571929,
                "cdate": 1699944571929,
                "tmdate": 1699944571929,
                "mdate": 1699944571929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EPhbZCTFFn",
                "forum": "L9G9nR8fMF",
                "replyto": "d0r6a5IeTG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weakness 3. Concerns on the title of the paper. \n- Firstly, we wish to clarify that our intention was to present our contributions in a conservative manner. Considering that CNNs primarily utilize BatchNorm in their normalization layers, and our experimental results are based on ResNets with BatchNorm, the purpose of our title was to underscore the unique benefits of LayerAct as an activation mechanism in enhancing the performance of CNNs that utilize BatchNorm. \n- However, we acknowledge that the current title may inadvertently imply a direct association between LayerAct and BatchNorm. To eliminate any potential confusion, we have decided to revise the title to:\n>- \u201cLayerAct: Advanced activation mechanism utilizing layer-direction normalization for CNNs with BatchNorm\u201d \n- If there are still concerns that the revised title might lead to the same confusion, another option could be to remove the specific reference to \"BatchNorm.\":\n>- \u201cLayerAct: Advanced activation mechanism utilizing layer-direction normalization for CNNs\u201d \n\n### Weakness 4. Imprecise descriptions. \n#### 4-1. About the statement \u201ccombines batch-direction normalization with the effects of layer-direction normalization\u201d \n- In line with our previous response, we acknowledge that the statement may cause confusion. To provide greater clarity, we have revised the statement from \u201cThis advancement combines batch-direction normalization with the effects of layer-direction normalization, yielding two benefits: i) addressing the trade-off issue between two significant properties of activation, and ii) improving the noise-robustness of networks by reducing the variance of noise-robustness among samples.\u201c to the following:\n>- \u201cWhile maintaining the batch-direction normalization methods, which are effective and prevalent in CNN-based networks, this advancement can provide the benefits of layer-direction normalization. This integration offers two main advantages: i) addressing the trade-off issue between two significant properties of activation, and ii) improving the noise-robustness of activations by reducing the variance of noise-robustness across samples.\u201d \n\n#### 4-2. About the statement on one-sided saturation. \n- Please refer to the \u201c1-2. Important Properties of Activation Functions and the Trade-off Problem.\u201d in our response to Weakness 1.\n\n#### 4-3. About the statement on the sum of activation scale. \n- We recognize that the statement is only true when all of the activation inputs across samples are well distributed. Therefore, we will revise the sentence \u201cWith LayerNorm, the sum of activation scale $||s\\left(n^{LN}\\right)||$ will be similar across all samples, which helps to reduce the variance of noise-robustness across all samples\u201d to: \n>- \u201cWith LayerNorm, the sum of activation scale $||s\\left(n^{LN}\\right)||$ will have a lower upper bound across all samples, which helps to reduce the variance of noise-robustness across all samples.\u201d \n\n#### Concluding remarks. \n- We would like to reemphasize that the main contribution of LayerAct is its ability to address the limitations of element-level activation functions. This unique contribution is unattainable through the parallel use of LayerNorm and element-level activation functions and remains effective independent of BatchNorm.\n- Once again, we sincerely appreciate your time and effort in reviewing our work. We have been able to significantly enhance the presentation of our work, including revising the title and clarifying the contribution of LayerAct. If there are any further concerns or questions, we are more than willing to address them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944606606,
                "cdate": 1699944606606,
                "tmdate": 1699944606606,
                "mdate": 1699944606606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yATBPnxlsc",
                "forum": "L9G9nR8fMF",
                "replyto": "EPhbZCTFFn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Reviewer_1Eep"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Reviewer_1Eep"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors responses."
                    },
                    "comment": {
                        "value": "I thank for the authors' responses in detail. I have read the responses.   \n\n1.\tIt is appreciated that the authors provide additional experiments on CNN (ResNet) without BatchNorm. It seems true that LayerAct can improve noise-robustness empirically. However, my concerns on the clarity of why LayerAct is noise-robust still hold, ie, I still think the analyses in Subsection 3.2 is not well supported by evidences. As I stated before, \u201cbounding the activation function cannot bounding the $y_i$, which can still be not robust.\u201d. If the analyses in bounding is correct, does this mean we can use ReLU-6 (the maximum value is 6, or other bounded activations) to obtain good noise-robustness? Besides, I want to know how the authors calculate the mean/variance for the LayerNorm-style operation in LayerAct for the convolutional input? E.g., for an convolutional input $X \\in \\mathbb{R}^{d \\times h \\times w}$, which dimensions the mean/variance is calculated over?   \nI also noted that LayerAct works worse than ReLU on ResNet20/44/56 without BatchNorm in terms of the clean results from the additional experiments, why the results are not consistent with the results on ResNet20/44/56 with BatchNorm? From these results, it seems that the performances of LayerAct are related to the network with/without BatchNorm. This further amplified my concerns 3 in the initial review, that is, this paper seems to be related to BatchNorm (as stated in the Title), but why the description of the proposed LayerAct is independent to the BatchNorm. In another view, this paper shows many words on BatchNorm in the first paragraph of the introduction, why this paper says the main aim is to design an activation function, but not related to BatchNorm in the response? I think this paper should provide a major revision in the presentation. \n\n2.\tI still have the concerns on \u201c the One-sided saturation avoids the vanishing gradient problem.\u201d in the description. Let me make it indetail:(1) if one use ReLU (One-sided saturation activation), and many neurons are not activated (in the saturated state). In this case, I think the model will suffer vanishing gradients; (2) If one use ReLU and the weight is initialized with very small variance. In this case,  I think the model will suffer vanishing gradients. Therefore, I am not convinced to this sentence \u201cOne-sided saturation avoids the vanishing gradient problem\""
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386988921,
                "cdate": 1700386988921,
                "tmdate": 1700386988921,
                "mdate": 1700386988921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FFfRUhdJvt",
                "forum": "L9G9nR8fMF",
                "replyto": "d0r6a5IeTG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your second comment. To address the remain concerns, our responses are as follows.\n\n----\n\n### Noise-robustness of LayerAct \n- We are pleased that the contribution of LayerAct's noise-robustness has been empirically recognized through our additional experiment.\n- We acknowledge that LayerAct may not maintain robustness when $y_{i}$ is large, as it does not bound it. This realization highlights the importance of utilizing a normalization method in networks with LayerAct functions. We plan to explicitly emphasize the necessity of using normalization methods to maintain the robustness of LayerAct in our manuscript.\n- The insights provided have led us to reevaluate our usage of the term \u201censure\u201d in certain contexts in our manuscript, recognizing that it may not fully capture all scenarios. To address this, we will revise the statement, \u201cThis suggests that LayerAct can ensure more robust processing during the forward pass of a network.\u201d in our manuscript.\n>- This implies that networks with LayerAct are likely to achieve more robust processing during the forward pass, especially when the input $y$ is not excessively large, reinforcing the importance of applying normalization methods, such as BatchNorm, in networks with LayerAct functions. \n\n----\n\n### Noise-robustness of activation functions with saturation state. \n- The other point of your concern seems to be about all activation functions with a saturation state. We understand that your comment means, \"Activation functions with a saturation state cannot ensure noise-robustness.\" \n- As your observations, one-sided saturation does not universally guarantee noise-robustness across all samples. Activation functions with a saturation state only provide noise-robustness for those samples that have a sufficient number of elements in the saturation state. This does not apply to samples with large $y_i$ for all $i$. However, please note that this observation highlights the limitation of activation functions that we criticized, namely the large variance in noise-robustness across samples. \n- Again, your insights have led us to recognize that our use of the term \u201censure\u201d in certain statements may not have adequately accounted for such instances. \n- Our intention was to claim that activation functions with a saturation state **potentially offer greater noise-robustness in more samples** compared to those without any saturation state. This is under the condition that a sufficiently large number of elements are in the saturation state.\n- For example, ReLU can provide noise-robustness for the samples when a sufficient number of elements are in the saturation state, indicating enough elements with small $y_i$. Conversely, activation functions without any saturation state, such as PReLU, do not provide noise-robustness for samples with small $y_i$. In this context, Clevert et al. (2016) stated, \u201cIn contrast to ReLUs, activation functions like LReLUs, PReLUs, and RReLUs do not ensure a noise-robust deactivation state.\u201d \n- Our experimental results also corroborate the relationship between noise-robustness and saturation state. In Tables 2 and 3, the performance of networks with ReLU is better than those with PReLU on noisy datasets. \n- To clarify our statements, we will revise the statement in our manuscript, \u201cThis implies that existing activation functions can ensure noise-robustness for samples only when a sufficiently large number of elements are in the saturation state, not when there are fewer elements in the saturation state.\u201d\n>- This implies that existing activation functions with a saturation state are **expected to be robust** for samples only when a sufficiently large number of elements are in the saturation state, not when there are fewer elements in the saturation state. \n- In summary, while no activation function can guarantee noise-robustness for samples with large $y_i$ with their saturation state, those with a saturation state are expected to be robust for a greater number of samples compared to those without a saturation state. \n\n----\n\n### The dimensions of mean/variance calculation. \n- In case of image data, the term $d$ of the equations in Subsection 2.3 and 3.2 denotes the representation of these dimensions as a single flattened vector. Thus, considering $X$ with dimension $c \\\\times w \\\\times h$, where $c$ is the channel dimension, $d=c \\\\times w \\\\times h$. The equation of mean $\\\\mu_y$ and variance $\\\\sigma_y^2$ are as follows:\n$$\n\\\\mu_y=\\frac{1}{c \\\\times h \\\\times w}\\\\sum^c_{i=1} \\\\sum^h_{j=1} \\\\sum^w_{k=1} x_{i, j, k} = \\\\frac{1}{d}\\\\sum^d_{i=1} x_i\n$$\n$$\n\\\\sigma_y^2=\\frac{1}{c \\\\times h \\\\times w}\\\\sum^c_{i=1} \\\\sum^h_{j=1} \\\\sum^w_{k=1} \\\\left(x_{i, j, k} - \\\\mu_y \\\\right)^2 = \\\\frac{1}{d}\\\\sum^d_{i=1} \\\\left(x_i-\\\\mu_y\\\\right)^2\n$$\nwhere $x_{c, h, w}$ and $x_{d}$ is a pixel of the image $X$."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581099897,
                "cdate": 1700581099897,
                "tmdate": 1700581223616,
                "mdate": 1700581223616,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WkcQRsZyNr",
                "forum": "L9G9nR8fMF",
                "replyto": "d0r6a5IeTG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Relationship between LayerAct and BatchNorm. \n- There was a misinterpretation of your questions in our initial response. Our intention was to assert that LayerAct does not **help** BatchNorm. \n- We would like to reemphasize that we tried to present our contribution in a conservative manner. The title and introduction of our manuscript focused on the CNNs that utilize BatchNorm, since all the networks of experiments utilize BatchNorm. \n- However, we acknowledge that there are no detailed discussion of the relationship between LayerAct and BatchNorm. In this response, we will explore the relationship between LayerAct and BatchNorm.\n- LayerAct **needs** BatchNorm. Aforementioned, networks with LayerAct functions need to utilize normalization methods to maintain the benefit of noise-robustness. Another necessity of BatchNorm arises because LayerAct functions are complex activation functions that tend to cause networks to overfit the training datasets. \n- We will revise our manuscript to underscore the necessity of integrating BatchNorm with LayerAct. Additionally, a new section will be added to the appendix, providing detailed discussion into this requirement.\n\n| data | model | ReLU | LA-SiLU | \n| - | - | - | - |\n| CIFAR100 | ResNet32 | 92.23 | 94.52 |\n| CIFAR100 | ResNet44 | 95.76 | 96.47 |\n\n- To empirically investigate why ResNet32 and ResNet44 with LA-SiLU underperform compared to those with ReLU, especially when no normalization method is employed, we conducted inference tests on the trained networks without a normalization method using **the training dataset**. The results of table above show that networks with LA-SiLU exhibit higher accuracy than those with ReLU, suggesting a tendency toward overfitting.\n- Considering that LA-SiLU demonstrates better performance on the shallower network, ResNet20, compared to ReLU, these experimental results suggest that LA-SiLU requires modules that can prevent overfitting, such as normalization methods, for optimal performance in deep networks. \n- In summary, networks with LayerAct functions need BatchNorm to maintain the benefit of LayerAct\u2019s noise-robustness and prevent the overfitting. \n\n----\n\n### About one-side saturation and vanishing gradient problem \n- Similar to our response on the relationship between noise-robustness and one-sided saturation, we acknowledge that the term \u201cavoid\u201d in the statement \u201cavoid the vanishing gradient problem\u201d is overly absolute, given the existence of counterexamples. \n- Our intention was to convey that activation functions with one-sided saturation have the potential to facilitate more effective propagation during the backward pass, which can alleviate the vanishing gradient problem. This potential arises from their partially larger derivative compared to functions with both-sided saturation, like sigmoid or tanh. For example, the derivative of ReLU is one on the positive side, while the maximum derivative of sigmoid is 0.25. \n- Previous studies have highlighted the attribute of ReLU, which has a derivative of one on its positive side, leading to more informative processing during the backward pass (Glorot et al., 2011; Clevert et al., 2016; Ramachandran et al., 2018). \n- Thus, we will revise our statements from \u201cavoiding the vanishing gradient problem\u201d to \u201cexpected to have more informative propagation during the backward pass\u201d. For example, we will revise the statement \u201cOne-sided saturation avoids the vanishing gradient problem while maintaining noise-robustness.\u201d as follows: \n>- \u201cActivation functions that saturate only on one side are expected to have more informative propagation during the backward pass compared to those that saturate on both the positive and negative sides, by allowing for larger derivatives.\u201d \n\nClevert, Djork-Arn\u00e9, Thomas Unterthiner, and Sepp Hochreiter. \"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUS).\" ICLR. 2016. \n\nGlorot, Xavier, Antoine Bordes, and Yoshua Bengio. \"Deep sparse rectifier neural networks.\" JMLR workshop, 2011. \n\nRamachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for Activation Functions.\" ICLR workshop, 2018.\n\n----\n\n### Concluding remarks. \n- We sincerely appreciate your insightful comments. Your comments have significantly contributed to enhancing the quality of our presentation, especially on the statements about the properties of activation functions, and the noise-robustness of LayerAct and relationship between LayerAct and BatchNorm. If there are any further concerns or questions, we are more than willing to address them."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581127944,
                "cdate": 1700581127944,
                "tmdate": 1700637722096,
                "mdate": 1700637722096,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5QtlbR6dVL",
            "forum": "L9G9nR8fMF",
            "replyto": "L9G9nR8fMF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7871/Reviewer_wLqf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7871/Reviewer_wLqf"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new activation function for CNNs with BatchNorm. The proposed layer-level activation, LayerAct, is designed to be more robust to noise and activation fluctuations due to shifts in input, compared to existing point-wise activation functions. The analysis and experimental results validate the noise-robustness of LayerAct."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper presents a new layer-level activation function for CNNs with BatchNorm.\n- The proposed activation function is presented in a general form in that variations of activation functions can be explored.\n- The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The paper demonstrates experiments with rather same networks (ResNet) and small networks. Is the experimental result consistent with larger networks (e.g., ResNet-152) and different networks (e.g., EfficientNet, ResNext)?\n\n- If LayerAct provides different effects from LayerNorm, there may be benefits the proposed function may bring for networks with LayerNorm. How does the proposed activation function behave with Transformers?"
                },
                "questions": {
                    "value": "Please refer to the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7871/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831329680,
            "cdate": 1698831329680,
            "tmdate": 1699636965119,
            "mdate": 1699636965119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QZIQUxaGNh",
                "forum": "L9G9nR8fMF",
                "replyto": "5QtlbR6dVL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your comments. We are pleased with your positive comments on LayerAct, particularly its capacity to overcome the limitations of element-level activation. In response to your concerns and questions, we describe our responses as follows. \n\n### Weakness 1. Is the experimental result consistent with larger networks (e.g., ResNet-152) and different networks (e.g., EfficientNet, ResNext)? \n- We agree that evaluating performance on deeper and various networks is important. \n- Currently, we are training ResNet101 and ResNext50-32x4d with LA-SiLU and ReLU. We selected ReLU as the baseline element-level activation function since ResNet50 with ReLU exhibited superior performance on ImageNet compared to other element-level activation functions. \n- We will promptly share the results upon completion of these experiments to further address your concerns and improve our work. \n\n### Weakness 2. How does the proposed activation function behave when it is placed in networks with LayerNorm?  \n- The distinctive effect of LayerAct, compared to LayerNorm, lies in its ability to process the mean and variance statistics from activation input to activation output. However, if LayerNorm is placed before the activation layer, LayerNorm already normalizes the mean and variance of the activation input, leaving little room for diverse output statistics across samples. \n- Specifically, consider a network using LayerNorm without an affine function (no gain and bias), and placing LayerNorm layers right before activation layers. In such a network, the normalization output of the LayerNorm layers serves as the activation input. As a result, the normalization output of a LayerNorm layer is $n^{ \\\\hat{ LN } } = \\\\frac{ y -\\\\mu^2_y }{\\\\sqrt{\\\\sigma^2_y + \\\\alpha}}$. This leads to the output of SiLU $n^{ \\\\hat{ LN } } s\\\\left(n^{ \\\\hat{ LN } } \\\\right)$ and LA-SiLU $n^{ \\\\hat{ LN } } s \\\\left( n \\\\right)$ to be exactly same as $n = n^{\\\\hat{ LN }}_i$. \n- As such, the closer the output of the LayerNorm layer approximates the normalized input of LayerAct\u2019s activation scale function $s$ (i.e., the more gain and bias approximate to one and zero, respectively), the more the benefits of LayerAct are reduced. \n- Please note that LayerAct has benefits as an activation: i) it addresses the trade-off problem between important properties of activation, and ii) has lower variance of noise-robustness across samples. These benefits can be maintained when used with LayerNorm that includes an affine function. This is because, in such scenarios, the layer-direction normalization in LayerNorm and LayerAct differs, represented as $n^{LN} = g \\\\cdot \\\\frac{ y -\\\\mu^2_y }{\\\\sqrt{\\\\sigma^2_y + \\\\alpha}} + b $ and $ n= \\\\frac{ y -\\\\mu^2_y }{\\\\sqrt{\\\\sigma^2_y + \\\\alpha}}$, respectively. \n- To validate this, we are conducting an additional experiment to train networks with LayerAct and LayerNorm on CIFAR10 and CIFAR100. We will report the outcomes of the experiments as soon as they are completed. \n- Based on your comment, we recognize that our manuscript currently lacks a detailed discussion on the potential limitations of LayerAct. We will incorporate a detailed discussion of these aspects in the Appendix, and ensure to reference this discussion in the main body of the manuscript. Thank you for your comment.\n\n### Concluding remarks\n- We would like to reemphasize the main contribution of LayerAct: its capability to overcome the limitations of element-level activation functions. LayerAct can address the trade-off problem between important properties of activation and has smaller variance of noise-robustness across samples compared to element-level activation. Notably, our experimental results from image classification tasks demonstrate LayerAct\u2019s capability to enhance the performance of ResNets on both clean and noisy datasets. \n- Once again, we sincerely appreciate your time and effort in reviewing our works. We could conduct additional experiments that can enhance our confidence in the performance of LayerAct, and improve the presentation of our manuscript. If there are any further concerns or questions, we are more than willing to address them."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944470715,
                "cdate": 1699944470715,
                "tmdate": 1699944830746,
                "mdate": 1699944830746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ajJuNhYLw",
                "forum": "L9G9nR8fMF",
                "replyto": "QZIQUxaGNh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Reviewer_wLqf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Reviewer_wLqf"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' detailed responses. As for the weakness 2, if LayerAct has similar effects with LayerNorm, then I believe the proposed activation function may replace Layer Norm + activation function in existing transformer models. I suggest that authors take a look at possible benefits this additional experiment will bring to the final version of the paper.\nAs of now, my concerns are not addressed much without the experimental results available."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536990769,
                "cdate": 1700536990769,
                "tmdate": 1700536990769,
                "mdate": 1700536990769,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "llzttdQxGb",
                "forum": "L9G9nR8fMF",
                "replyto": "5QtlbR6dVL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your second comment on the potential of LayerAct. Exploring the application of LayerAct functions in various architectures, such as transformer, is a key direction for our future research. \n\n----\n\n### Future research: transformers with LayerAct. \n- Our analysis of the relationship between LayerAct and LayerNorm, particularly the observation that LayerAct may offer reduced benefits with LayerNorm compared to BatchNorm, is confirmed by additional experiments on ResNets with LayerNorm, detailed in our Meta response 2-2. \n- Consequently, we are considering utilizing PowerNorm as the normalization method for the transformer with LayerAct functions, which is a variant of BatchNorm for transformer architecture in NLP tasks. We look forward to the opportunity to present these findings to you in the future. \n\n----\n\n### UNet and Unet++ with LayerAct. \n- Regrettably, due to limitations in computing resources, experiments on ImageNet with deeper ResNet or ResNext are not expected to be completed within the desired timeframe. \n- However, to demonstrate LayerAct\u2019s potential for application in other architectures, we have reported the experimental results of UNet and UNet++ in Meta response 3. \n- In the experiment, networks with LA-SiLU outperformed those with ReLU and SiLU. It is noteworthy that the architectures of UNet and UNet++ differ from that of ResNet and are used for a different task, image segmentation. \n\n----\n\nOnce again, we appreciate your comments that helped us to set our future research with more confidence. We trust that the experimental results on image segmentation task may, at least partially, address your concerns regarding the consistent performance of LayerAct across various architectures."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640538052,
                "cdate": 1700640538052,
                "tmdate": 1700640678864,
                "mdate": 1700640678864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ml2mmZAylA",
                "forum": "L9G9nR8fMF",
                "replyto": "llzttdQxGb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7871/Reviewer_wLqf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7871/Reviewer_wLqf"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response. I have no further questions."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7871/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735750364,
                "cdate": 1700735750364,
                "tmdate": 1700735750364,
                "mdate": 1700735750364,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]