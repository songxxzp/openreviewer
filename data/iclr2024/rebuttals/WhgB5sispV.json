[
    {
        "title": "Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting"
    },
    {
        "review": {
            "id": "axhaPXESGy",
            "forum": "WhgB5sispV",
            "replyto": "WhgB5sispV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission321/Reviewer_8jaG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission321/Reviewer_8jaG"
            ],
            "content": {
                "summary": {
                    "value": "This work is on dynamic scene representation and rendering.\nThe authors extend Gaussian Splatting to dynamic scenes by extending the 3D Gaussian formulation with $t$, along with 4D spherindrical harmonics.\nExperiments are conducted on Plenoptic Video and D-NeRF dataset and demonstrate the effectiveness of the proposed method: more realistic and faster.\nAnalysis shows that some primitive underlying 3D movement can be captured."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The extension of 3D Gaussian for dynamic scenes. It allows using 3D Gaussian to render dynamic scenes realistically.\n2. Overall the paper is clearly written and well explained in the motivation, design and implementation.\n3. Superior results compared to very recent SOTA methods include K-Planes, HexPlanes. Higher realism and speed."
                },
                "weaknesses": {
                    "value": "* Concerns Regarding Novelty:\n   - The enhanced speed and realism seem to be primarily attributed to the power of Gaussian splatting, rather than the novel modules introduced.\n   - The implementation of 4D spherical harmonics appears to offer only a marginal improvement, as evidenced by the mere +0.2 PSNR increase in Table 3.\n   - The inclusion of time (t) in addition to spatial dimensions $(x,y,z)$ in the Gaussian model appears to be a straightforward extension, akin to extending TensoRF\u2019s triplanes to K-planes.\n* Modeling Concerns: \n   - I have reservations about the authors\u2019 approach of modeling the 4D Gaussian by treating space (xyz) and time (t) equally. This seems to suggest that each Gaussian fades in, peaked at $\\mu_t$ and out, as indicated by the term $p$ in Equations 4 and 10. This is not grounded in physical reality, where Gaussians are expected to move through space rather than appearing and disappearing abruptly.\n  - Consequently, I think this extension mainly increases the model\u2019s capacity to incorporate time and I question whether the proposed approach can yield consistent results in more complex scenarios (e.g., higher moving speeds, sparse observations).\n\n\nThat being said, I think there are more effective ways to extend 3D Gaussian splatting for dynamics with a more accurate and physically-grounded representation. Nevertheless, I still consider this work to be above the bar in this field, providing some insights for the community."
                },
                "questions": {
                    "value": "I know these are concurrent work (one even come after ICLR submission deadline). It would be admired if you can add in the final version about comparison to the following two works on extending Gaussian splatting for dynamic scenes.  I have no conflict of interests to these works.\n1. 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering. https://arxiv.org/pdf/2310.08528.pdf\n2. Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis  https://arxiv.org/pdf/2308.09713.pdf\n\nBesides, it would be appreciated if you can apply the proposed method on any urban scenes (e.g. KITTI, Waymo dataset). These are the more challenging and common dynamic scenes in real world."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission321/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission321/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission321/Reviewer_8jaG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission321/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613339980,
            "cdate": 1698613339980,
            "tmdate": 1699635958683,
            "mdate": 1699635958683,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2NHrBCU1rA",
                "forum": "WhgB5sispV",
                "replyto": "axhaPXESGy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission321/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission321/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8jaG"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and detailed review as well as the suggestions for improvement. Our response to the reviewer\u2019s comments is below:\n\n**Q1: The comparison with the concurrent work.**\n\nVery reasonable suggestion. We have updated the comparisons with the 4DGS [1] in the revised paper. For the Dynamic 3D Gaussian [2], we have discussed its differences in the related work section. In the discussion period, we additionally test its performance in the cut roasted beef scene using its officially released code. We present the comparison in the table below:\n\n| Method              | PSNR $\\uparrow$  | SSIM $\\uparrow$ | LPIPS $\\downarrow$ |\n| :-----              | :---: | :---:  | :---: |\n| Dynamic 3D Gaussian [2] | 29.20 | 0.9551 | 0.08846 |\n| Ours | 33.85 | 0.9596 | 0.04057 |\n\nNote that the way described in the paper of Dynamic 3D Gaussian for obtaining the foreground/background mask cannot be simply applied to this scene, and its performance highly depends on these masks significantly. So we extract masks using an advanced video object segmentation model Xmem [3] with the masks for the first frame acquired via SAM [4]. In addition, we enable optimization of seg colors in the initial timestep and detach other parameters before rendering segmentation masks. Due to the above customization is not contained in the original Dynamic 3D Gaussian, we did not append this result in our revised paper.\n\n> [1] Wu, Guanjun, et al. 4d gaussian splatting for real-time dynamic scene rendering. *arXiv preprint*, 2023.\n\n> [2] Luiten, Jonathon, et al. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. *3DV*, 2023.\n\n> [3] Cheng, Ho Kei, and Alexander G. Schwing. Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model. *ECCV*, 2022.\n\n> [4] Kirillov, Alexander, et al. Segment anything. *ICCV*, 2023.\n\n**Q2: The application on the urban scenes.**\n\nGreat thought. While this is beyond the scope of this work, we are delighted to share our explorations in this area. We test 4D Gaussians with minimal modification on selected dynamic sequences in the widely used Waymo Open Dataset. The modifications are summarized in the revised appendix, where we also provide some visualization of reconstruction and novel view synthesis results. The rendered videos can be found in the **supplementary material**. These results reveal that the proposed method can uniformly model both dynamic and static regions and achieve competitive reconstruction speed and quality without relying on any manually labeled dynamic mask. This indicates good potential of the proposed method. \n\n**Q3: Novelty concern.**\n\nIn a sense, all work that extends static reconstruction methods to dynamic scenes can be viewed as the inclusion of time in addition to spatial dimension; how to achieve this is however non-trivial. Often such extensions will introduce new modules and optimizations, making the whole pipeline much more complex and less flexible. In this work, we implement a different extension by introducing an easily optimized and rendered parameterization of 4D Gaussian, and adapt the original rendering formulation in 3DGS. We believe that this extension is more flexible and inherits the advantages and spirits of the explicit modeling in 3D Gaussian Splatting, while it\u2019s also fully interpretable and friendly to composition and editing. The 4DSH is designed as an integral part for appearance evolution over time. Indeed, the performance gain it brings is not impressive despite its obvious usefulness. Per our observation, this is due to limited cases of such time-dependent appearance involved within the evaluation video sequences. We will attempt to explore more extensive test data in the future work.\n\n**Q4: Modeling concern.**\n\nThe issue raised by the reviewer 8jaG is common to splatting-based methods: we can draw an analogy in the 3D case, where many real-world 3D entities are composed of line segments with constant transparency, yet the 3D Gaussian also fades out beyond its $\\mu_{xyz}$. However, this issue doesn\u2019t significantly impede its expressive capability. The bullet times experiments in our supplementary material also suggest that this issue does not affect the temporal smoothness of 4D Gaussian. Furthermore, the unnormalized Gaussian function allows the 4D Gaussian to always have larger influence and does not fade out abruptly after $\\mu_t$. \nFor the issue of physically-grounded, it is better to clarify that  we are not aiming to model physical reality, and the 4D Gaussian should not be seen as corresponding to some physical entity. Actually, most of the successful methods for the task of novel view synthesis rather than simulation are not completely physically-grounded, although they may draw inspiration from some aspect of physical reality. Nevertheless, we believe that solving this issue properly might result in a new more advanced representation paradigm in the future research."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543214201,
                "cdate": 1700543214201,
                "tmdate": 1700583072459,
                "mdate": 1700583072459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sDyBhWkRRQ",
            "forum": "WhgB5sispV",
            "replyto": "WhgB5sispV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission321/Reviewer_1bTN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission321/Reviewer_1bTN"
            ],
            "content": {
                "summary": {
                    "value": "This paper generalizes the 3D Gaussian splitting to 4D. The key contribution is a 4D Gaussian-based representation for dynamic rendering. Unlike intuitively modeling the 4D as canonical 3D + deformations, this paper inserts many 4D bubbles into 4D space-time and jointly assembles the dynamic scenes. \n\nInterestingly, when using full rotational 4D Gaussians, to render a specific time, one just needs to slice the 4D distribution and then do standard 3D Gaussian EWA. The skew in 4D also leads to time-dependent center changing, which leads to local scene flow.\n\nThe proposed representation is tested with the dynamic rendering task on D-NeRF and DyNeRF datasets and shown to be effective. With the power of splitting, the proposed representation achieves high inference FPS."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It\u2019s interesting to use local 4D Gaussians to represent full 4D functions instead of using explicit flow plus 3D Gaussian.\n- The introduction of full 4D covariance and the usage of the time-dependent harmonics of appearance is effective.\n- The flow extracted from the time-dependent center sliced from 4D Gaussians makes this 4D function approximation more reasonable because it may locally capture correspondence, which introduces the smoothness into the 4D approximation."
                },
                "weaknesses": {
                    "value": "- One key weakness the reviewer is curious about is the number of Gaussians in the proposed representation when the video grows longer. And how long does each 4D Gaussian cover in time? If the 4D Gaussians, like the 3D static one, only have local support in time, then the memory or storage consumption may be huge, preventing applying such representation to long videos or streaming.\n- To help the readers understand the 4D Gaussians, one 2D/1D slice + time visualization may be helpful.\n- The LPIPS is not reported in the table while most dynamic rendering does.\n- Although the local flows as I write in the strengths are interesting, the reviewer is wondering how can these local correspondences generalize to global and long-term ones. It\u2019s unclear whether such 4D mixture models can capture long-term tracking."
                },
                "questions": {
                    "value": "Pelase see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission321/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission321/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission321/Reviewer_1bTN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission321/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697692810,
            "cdate": 1698697692810,
            "tmdate": 1699635958596,
            "mdate": 1699635958596,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PPho9Ac7UI",
                "forum": "WhgB5sispV",
                "replyto": "sDyBhWkRRQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission321/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission321/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1bTN"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and detailed review as well as the suggestions for improvement. Our response to the reviewer\u2019s comments is below:\n\n**Q1: The number of 4D Gaussians.**\n\nThe reviewer\u2019s insight on the locality is very discerning. Actually, unlike spatial scaling which is set to the mean of the distance to the closet three points, we initialized a very large scaling in the temporal dimension as mentioned in implementation details, which allows most background Gaussians to cover a long time period (around the scene\u2019s duration), ensuring that the total number of the fitted 4D Gaussians does not significantly exceeds that of 3D Gaussians.\n\nAs for the memory or storage consumption, we believe that there is no free lunch in this aspect. For any representation, with the increase of the video length, the storage requirements are unlikely to remain constant forever, because we can't store more information without any extra cost. Nevertheless, despite the possible storage consumption issues, the rendering speed of the proposed 4D Gaussian tends to be fixed as the video length increases. From this point of view, this characteristic of locality enables our proposed method to be more friendly to long videos than that of encoding information into implicit neural networks. \n\n**Q2: The visualization of time**\n\nGood suggestion. Since the temporal characteristics of the 4D Gaussian can be summarized by its variance and mean, we provide visualizations of them in Figure 8 of the revised appendix. Interestingly, these visualizations naturally form a mask of dynamic and static regions. This phenomenon not only shows the potential and the versatility of our approach, but also demonstrates that a big proportion of Gaussians are used for background modeling, with a long span over time. To demonstrate this more directly, we plot the marginal distribution of some representative Gaussians in Figure 9.\n\n**Q3: LPIPS**\n\nIn the original submitted version, we only reported PSNR and SSIM following kplanes. Now we have updated LPIPS in the revised paper.\n\n**Q4: Long-term tracking**\n\nThis is a great point and worth for deep investigation including both benchmark upgrade and algorithm advance. We consider this issue to be one of the most important aspects for extending our model. Considering that maintaining global correspondence may compromise the fitting capability and flexibility in complex dynamic scenes, we leave it for future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543175171,
                "cdate": 1700543175171,
                "tmdate": 1700543175171,
                "mdate": 1700543175171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iXFIl6zxIW",
                "forum": "WhgB5sispV",
                "replyto": "PPho9Ac7UI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission321/Reviewer_1bTN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission321/Reviewer_1bTN"
                ],
                "content": {
                    "title": {
                        "value": "Keep Positive"
                    },
                    "comment": {
                        "value": "Thanks for the author's response. After reading other reviews and the author's rebuttal, I currently tend to keep my positive recommendation of this paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581530220,
                "cdate": 1700581530220,
                "tmdate": 1700581530220,
                "mdate": 1700581530220,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wmD2iRvFAl",
            "forum": "WhgB5sispV",
            "replyto": "WhgB5sispV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission321/Reviewer_XBJp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission321/Reviewer_XBJp"
            ],
            "content": {
                "summary": {
                    "value": "This submission introduces a 4D spatial-temporal representation (3D for space and 1D for time) based on Gaussian principles to model dynamic 3D scenes. This representation is a principled extension of the 3D Gaussian representation used in Gaussian Splatting for static scenes. With the 4D Gaussian representation, the states of 3D Gaussians at specific time instances become slices of the 4D function, enabling their rasterization into video frames for forward image rendering and scene reconstruction. The submission demonstrates that by modeling the interplay between space and time, implemented as a non-block-diagonal 4D covariance matrix, we can improve the modeling and reconstruction of dynamic scenes. The paper illustrates that with only the reconstruction loss and no additional regularizations, the 4D Gaussian-based representation achieves superior performance compared to state-of-the-art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+The proposed 4D Gaussian based representation to model the spatial-temporal field is a very principled extension of the 3D GS method. As a result, the fitting process is robust - no need to introduce additional regularization terms or optimization strategy to make sure the optimization will converge.\n\n+The scene reconstruction using the representation achieves SOTA performance with only reconstruction loss across all experiments.  No manually designed regularization term is needed. This shows the effectiveness of the 4D representation. \n\n+The 4D densification and pruning operations in spacetime can be derived naturally by extending from 3D. As shown in the submission, those operations improve the performance on modeling scene motions\n\n+The paper is well written with consistent and accurate notations, thus easy to follow and grasp."
                },
                "weaknesses": {
                    "value": "- more discussion needed for key advantage of the proposed model:\nAlthough in Sec 3.2 and Tab.3, the authors mentioned the benefit of making space and time dependent by allowing non-block diagonal 4D covariance matrix for modeling motions,  it is not easily seen why the performance increase (full version vs \"No-4DRot\"). The visualization of temporal slices of the fitted 4D Gaussian could be helpful to demonstrate how the corresponding 3D Gaussian across frames models motions\n\n- more discussion on design choice needed:\nThe 4D SH function is also a function of time t, will it also explain the temporal appearance change (eg. motion)? if so, it is explaining the properties we did not intend it to (we use it for view-dependency), and it may interact with other optimized variables such as 4D rotation to explain the motion, will it lead to ambiguity and affect the performance of dynamic modeling ?"
                },
                "questions": {
                    "value": "1. The number of 4D Gaussians might be huge (millions in 3D GS paper), given that one more temporal dimension in addition to 3D space as in 3D GS. Can the author address that, since that might be one limitation of the Gaussian based representation?\n\n2. Are the 4D means of the Gaussians also being optimized? If so, the 4D means can also introduce displacements to 3D Gaussians. In other words, both 4D mean offsets, and 4D rotation can lead to displacements. Will this introduce ambiguity to explain the scene flow field?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission321/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814327704,
            "cdate": 1698814327704,
            "tmdate": 1699635958488,
            "mdate": 1699635958488,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TDFdQmSMzl",
                "forum": "WhgB5sispV",
                "replyto": "wmD2iRvFAl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission321/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission321/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XBJp"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback and constructive suggestions. Our response to the reviewer\u2019s concerns is below: \n\n**Q1: The number of 4D Gaussians.**\n\nThis is a great question regarding the scalability of our 4D Gaussian model. We further clarify the following facets:\n\n1. The total number of 4D Gaussians is not essentially larger than that of 3D Gaussians, as each 4D Gaussian has a certain time span and there exists a high proportion of background Gaussians active throughout the whole time duration of the scene. For the typical scenes in the DyNeRF dataset, the number of 4D Gaussian fitted on the videos with 300 frames is on the same order (millions) as the number of points in the 3D Gaussian fitted only on the first frame.\n\n2. Besides, since we filter the Gaussians according to the marginal of time with a negligible time cost before the frustum culling, the number of Gaussians actually participated in the rendering of each frame is nearly constant, and thus exhibiting stable rendering speed with the increase of the video length and the total number of Gaussians. The statistic of Gaussian\u2019s number is provided in Figure 10 of the revised appendix.\n\n\n**Q2: The potential conflict in the jointly optimization of the 4D means and the rotation.**\n\nGreat question. Yes, the 4D means are also optimized during training, but they are fixed during inference and their displacements are derived from the 4D rotations only. So there is no ambiguity during inference.\nRegarding the potential interference caused by the joint optimization of the means and rotation, we show in the following that their interaction will not introduce ambiguity of displacement. Consider a simplified linear example for illustration purposes: $y(t; x_0, v) = x_0 + vt$, where $x_0$ and $v$ are the trainable parameters. Given sufficient observations {$(y_i, t_i)$}$_{i=1}^{N}$, it can converge to the unique correct solution, despite that $x_0$ and $v$ also interact with each other during optimization. However, our model is slightly different. Specifically, our model should rather be abstracted as the following system: $y(t; x_0, v, t_0) = x_0 + vt_0 + vt$. Fortunately, if we do a simple substitution $w = x_0 + vt_0$, the above equation can be reformulated as $y(t; w, v) = w + vt$, allows for the unambiguous determination of $v$, ensuring that there is no ambiguity in the learning of displacement.\n\n\n**Q3: Discussion on the advantage compared to the na\u00efve baseline.**\n\nWe ascribe this performance gain to the motion modeling capabilities stemming from the proposed  4D rotation. As shown in Figure 4 of the original paper, the displacement derived from 4D rotation can roughly capture the underlying dynamics of the scene, which enables more efficient information sharing across frames and leads to more seamless transitions between them.\nWe also provide more visualizations and discussions of temporal slices for two versions of the 4D Gaussian in Section E and Figure 11 of the revised appendix. \n\n**Q4: Discussion on the 4D SH.**\n\nThanks. The possible ambiguity could be substantially mitigated by delaying the training of time-dependent coefficients. Actually, rather than inducing ambiguity, 4DSH\u2019s benefits in motion learning are more critical. Because unlike the static scenes, there exists temporal evolution in the appearance of static objects in dynamic scenes. Without a particular design as our 4DSH to tackle this variable, such a phenomenon can only be fitted by undesired motion that shouldn\u2019t exist. Moreover, our experience indicates that 4DSH brings a positive impact on performance in most cases."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543128071,
                "cdate": 1700543128071,
                "tmdate": 1700543128071,
                "mdate": 1700543128071,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s03kztC0A4",
                "forum": "WhgB5sispV",
                "replyto": "TDFdQmSMzl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission321/Reviewer_XBJp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission321/Reviewer_XBJp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the additional discussion and content based on the review. My questions and concerns have been addressed and I appreciated the modification for the appendix to include more visualizations. I will keep the initial positive score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587592786,
                "cdate": 1700587592786,
                "tmdate": 1700587592786,
                "mdate": 1700587592786,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]