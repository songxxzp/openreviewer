[
    {
        "title": "Robustifying and Boosting Training-Free Neural Architecture Search"
    },
    {
        "review": {
            "id": "hTq1RPbkNo",
            "forum": "qPloNoDJZn",
            "replyto": "qPloNoDJZn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6827/Reviewer_81X4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6827/Reviewer_81X4"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to find a linear combination of training-free metrics to boost the performance on NAS tasks. Specifically, the authors first train a GP to capture the relationship between weights of training-free metrics and the objective evaluation metric f and obtain a robust estimation metric $M^*$. Then the authors collect the queries during the training procedure of BO as $Q_T$. Finally, the authors utilize the learned $M^*$ as a performance estimator and adopt the greedy search to obtain the best architecture."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe motivation, that using a linear combination of existing training-free metrics to obtain a robust estimation metric $M^*$, makes sense.\n\n2.\tExperiments on NAS benchmarks show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tThe authors propose to train a BO to capture the relationship between the weight vector and the objective evaluation metric f. However, the queried architecture should be trained from scratch to obtain the objective evaluation during the BO stage, which seems to require large amounts of search costs since a standard BO procedure usually requires tens of queries.\n\nBTW: What does $R_f(A)$ denote in Eq. 1? Does it represent the objective evaluation of an architecture? Since Alg.1 directly uses $f$ to denote the objective evaluation metric, I suggest the authors utilize the same notation.\n\n2.\tI wonder about the effectiveness of the searched robust estimation metric $M^*$. According to Fig. 2, it seems that the optimal architecture has been found in less than 10 queries during the BO procedure. It shows that there is no need to conduct the greedy search through $M^*$, and BO is enough to get the optimal architecture.\n\n3.\tTable 4 shows that RoBoT only requires 0.6 GPU-day to search, does it only count the search cost of the greedy search procedure? I wonder what is the cost of the BO stage, which I am afraid is much larger."
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826391778,
            "cdate": 1698826391778,
            "tmdate": 1699636789688,
            "mdate": 1699636789688,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KCu7ikDk6b",
                "forum": "qPloNoDJZn",
                "replyto": "hTq1RPbkNo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 81X4"
                    },
                    "comment": {
                        "value": "Dear Reviewer 81X4,\n\nWe thank you for taking the time to review our paper and for your valuable feedback. We would like to address your concerns below.\n\n## About the Efficiency of RoBoT\n\n> the queried architecture should be trained from scratch to obtain the objective evaluation during the BO stage, which seems to require large amounts of search costs since a standard BO procedure usually requires tens of queries.\n\nThank you for pointing out the concern regarding the search efficiency of each query in our method. In practice, we usually apply the validation performance of each candidate architecture that is trained from scratch for only a limited number of epochs (e.g., 12 epochs in NAS-Bench-201) to approximate the true performance of this architecture as stated in our Appendix B.2 and C.2, which in fact is quite computationally efficient (e.g., about 110 GPU-seconds for the model training of each candidate in NAS-Bench-201). Of note, such an approximation, which has been applied in the literature [1], is already reasonably good to help find well-performing architectures, as supported by the results in both Table 2 and Table 4. We would like to add this discussion to the main body of our revised paper to make it clearer.\n\n\n>  RoBoT only requires 0.6 GPU-day to search, does it only count the search cost of the greedy search procedure? I wonder what is the cost of the BO stage.\n\nWe thank you for pointing out this question. We would like to clarify that the search cost in our paper includes the computational costs incurred not only in the greedy search stage but also in the BO stage. In fact, these two stages have the same type of training cost that is used to evaluate the validation performance of each queried candidate architecture. Specifically, such a performance evaluation procedure requires about 0.04 GPU-day for the model training of each queried architecture in the DARTS search space for ImageNet where we have queried approximately 3 architectures in the BO stage and 7 architectures in the greedy search procedure. We would like to add this discussion to the main body of our revised paper to make it clearer.\n\n\n## About the Effectiveness of RoBoT and Greedy Search\n>  According to Fig. 2, it seems that the optimal architecture has been found in less than 10 queries during the BO procedure. It shows that there is no need to conduct the greedy search through $M^*$, and BO is enough to get the optimal architecture.\n\nWe would like to clarify that the search costs in Figure 2 include the queries from both the BO stage (e.g., the first 5 queries in Figure 2a that are automatically determined by our RoBoT) and the greedy search procedure (e.g., the last 15 queries in Figure 2a). The results show that greedy search is indeed essential and necessary to help our method achieve competitive search performances, which is further supported by our ablation study in Figure 6. To enhance clarity, we will revise Figure 2 to highlight the search costs incurred by the BO stage and the greedy search procedure separately.\n\n\n## About Notations\n> What does $R_f(A)$ denote in Eq. 1? Does it represent the objective evaluation of an architecture? Since Alg.1 directly uses $f$\n to denote the objective evaluation metric, I suggest the authors utilize the same notation.\n\nThank you for addressing the concern regarding our use of the notations $R_f(A)$ and $f$. As presented in the first paragraph of Section 4, $f(A)$ denotes the objective evaluation metric for $A$, while $R_f(A)$ denotes the **ranking** of $A$ in the entire search space based on the objective evaluation metric $f$. Particularly, $R_f(A)=1$ indicates that architecture $A$ is an optimal architecture in the entire search space based on the evaluation metric $f$. Of note, we introduce $R_f$ mainly to facilitate our theoretical analysis detailed in Section 4.3 and Section 5, given that our theoretical analysis mainly focuses on the ranking performance. But in fact, finding the minimum of $R_f(A(w))$ w.r.t $A(w)$ is essentially equivalent to find the maximum of $f(A(w))$ w.r.t $A(w)$, according to our definition of $R_f(A)=1$.\n\nWe agree with you that using both $R_f(A)$ and $f$ in our formulation and algorithm could be confusing. Therefore, we would like to follow your suggestion to revise Equation 1 using only $f$ and reserve $R_f(A)$ only for the theoretical sections. This change aims to make our presentation more coherent and accessible to our readers.\n\n\n ---\n Thank you again for your detailed and valuable feedback. We hope that our additional clarifications could help improve your opinion of our paper.\n\n [1] Unifying and Boosting Gradient-Based Training-Free Neural Architecture Search, NeurIPS 2022"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978879690,
                "cdate": 1699978879690,
                "tmdate": 1700031307174,
                "mdate": 1700031307174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VGyjICTNtG",
                "forum": "qPloNoDJZn",
                "replyto": "hTq1RPbkNo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to Reviewer 81X4"
                    },
                    "comment": {
                        "value": "Dear Reviewer 81X4,\n\nWe would like to thank you again for appreciating the motivation and effectiveness of our paper and for the constructive comments to further improve our paper. Kindly let us know if you have any further comments on our paper, and we would like to do our best to address them in the remaining time."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582765984,
                "cdate": 1700582765984,
                "tmdate": 1700583082956,
                "mdate": 1700583082956,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G9CeaD9lOg",
            "forum": "qPloNoDJZn",
            "replyto": "qPloNoDJZn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6827/Reviewer_GJPv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6827/Reviewer_GJPv"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to tackle the research gap, the difference between training-free metrics with the final performance. This paper however, propose a weighted linear combination of traditional training free metrics as a estimator, where the weights are obtained automatically via Baysian optimization. Interestingly, this work use partial monitoring theory to prove their method has theoretical performance guarantee. Experiments are conducted on NASBench201."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Propose theory seems interesting"
                },
                "weaknesses": {
                    "value": "This paper does not read like an academic paper, where the introduction did not cover the full story. Their related work is quite short to cover the existing literature. I suggest the authors try to read more papers in this field instead of submitting their paper in a rush. Results on NASBench201 shows an incremental improvement without realistic benchmarking their method's performance."
                },
                "questions": {
                    "value": "I found the author regularily let the reader \"see later section\" in their introduction, including Section 3 and section 4. I think this is a not a professional way to write the introduction. We should at least grasp the main idea when reading the intro but instead reading the entire paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699196802078,
            "cdate": 1699196802078,
            "tmdate": 1699636789554,
            "mdate": 1699636789554,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "lMPnIwQ3A9",
            "forum": "qPloNoDJZn",
            "replyto": "qPloNoDJZn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces RoBoT, an algorithm for robustifying and boosting training-free neural architecture search (NAS). Motivated by the inconsistent performance estimation of existing training-free NAS metrics, this work proposes to explore a linear combination of multiple metrics that is more robust than each single metric, and exploit the robustified metric combination with more search budgets. The overall framework includes two stages. The first exploration stage employs Bayesian optimization (BO) to find the best linear combination weights for the robust metric. Then, in the second exploitation stage, the remaining search budgets are used to investigate the top-scoring architectures given by the robust metric. The proposed algorithm, RoBoT, is supported by both theoretical and empirical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This work is built on existing training-free NAS methods, and extends them to a robustified ensemble. Therefore, the proposed framework is promising for future extension when better training-free NAS methods are discovered.\n\n- Theoretical analysis is provided to understand the proposed algorithm, RoBoT.\n\n- Extensive and solid experiment results on various datasets and settings are provided to demonstrate the efficacy of RoBoT."
                },
                "weaknesses": {
                    "value": "- Missing details regarding robust metric: It seems that some important details about the BO-searched robust estimation metric are missing. What are the base training-free metrics considered in the search? What are the optimized linear combination weights for them? Do they significantly differ on different datasets/tasks?\n\n- Recent NAS methods: It is suggested to include some more recent NAS methods into the comparison, e.g., Shapley-NAS [1], $\\beta$-DARTS [2].\n\nDisclaimer: Although I know BO and NAS literature, I\u2019m not familiar with the theoretical background in this work. Therefore, I cannot provide helpful feedback on the theoretical part. I would like to read how other reviewers think about the theoretical results.\n\n[1] Han Xiao, Ziwei Wang, Zheng Zhu, Jie Zhou, Jiwen Lu. Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search. In CVPR, 2022.\n[2] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, Wanli Ouyang. $\\beta$-DARTS: Beta-Decay Regularization for Differentiable Architecture Search. In CVPR, 2022."
                },
                "questions": {
                    "value": "- In Table 3, why are the results on TransNAS-Bench-101 presented as the validation ranking? It seems to be inconsistent with the accuracy/error in the other two datasets (Tables 2 and 4). Also, the search costs are not listed in Table 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6827/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6827/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699229630159,
            "cdate": 1699229630159,
            "tmdate": 1700529048792,
            "mdate": 1700529048792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rDTbOS7rKx",
                "forum": "qPloNoDJZn",
                "replyto": "lMPnIwQ3A9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3buk"
                    },
                    "comment": {
                        "value": "Dear Reviewer 3buk,\n\nThank you for recognizing the strengths of our paper and providing the valuable feedbacks. We address your concerns as follows:\n\n## About Robust Metric\n\n> What are the base training-free metrics considered in the search?\n\nAs reported in our Appendix B.2 and C.2, we borrowed the six widely known training-free metrics (i.e., *grad_norm, snip, grasp, fisher, synflow,* and *jacob_cov*) from [1] for most of the experiments in our paper except for the tasks of *Segment.*, *Normal*, and *Autoenco* in TransNAS-Bench-101 where *synflow* was excluded due to its incompatibility with the tanh activation applied in each candidate architecture within the search space.\n\n> What are the optimized linear combination weights for them? Do they significantly differ on different datasets/tasks?\n\nThank you for raising this interesting question regarding the optimized linear combination weights in various tasks. We present the optimized weights, along with their similarities and correlations, across four tasks in TransNAS-Bench-101-micro as follows:\n\nVarying Optimized Weights\n\n|  | grad_norm | snip | grasp | fisher | synflow | jacob_cov |\n|---|---|---|---|---|---|---|\n| Scene | -1.00 | -0.08 | -0.97 | 1.00 | 1.00 | 1.00 |\n| Object | 0.03 | -0.21 | -0.76 | 0.51 | 0.95 | 0.16 |\n| Jigsaw | -0.74 | 0.18 | 0.04 | -1.00 | -1.00 | 1.00 |\n| Layout | -0.65 | -0.27 | 0.57 | -0.48 | 1.00 | 0.67 |\n\nCosine Similarity\n\n|  | Scene | Object | Jigsaw | Layout |\n|---|---|---|---|---|\n| Scene | 1.00 | 0.78 | -0.07 | 0.37 |\n| Object | 0.78 | 1.00 | -0.55 | 0.19 |\n| Jigsaw | -0.07 | -0.55 | 1.00 | 0.2 |\n| Layout | 0.37 | 0.19 | 0.2 | 1.00 |\n\nPearson Correlation\n\n|  | Scene | Object | Jigsaw | Layout |\n|---|---|---|---|---|\n| Scene | 1.00 | 0.77 | -0.02 | 0.35 |\n| Object | 0.77 | 1.00 | -0.52 | 0.16 |\n| Jigsaw | -0.02 | -0.52 | 1.00 | 0.3 |\n| Layout | 0.35 | 0.16 | 0.3 | 1.00 |\n\nThe results show that the optimized weights typically vary for different tasks, which aligns with the observations and motivations in Section 3 and further highlights the necessity of developing robust metrics that can perform consistently well on diverse tasks such as our RoBoT. In addition, for tasks with similar characteristics, e.g., the *Scene* and *Object* tasks, both of which are classification tasks, the optimized weights share a relatively high similarity/correlation, indicating the potential transferability of the optimized linear combination within similar tasks. We would like to add these results and discussions to our revised paper.\n\n## About Including More Baselines\n\nWe appreciate your suggestion on more recent baselines. We show our comparison on NAS-Bench-201 as below. Note that we have applied smaller search budgets for Shaply-NAS and $\\beta$-DARTS than the ones presented in their original papers in order to maintain comparable search budgets for different training-based NAS algorithms (e.g., DARTS (2nd), GDAS, DrNAS).  The results show that our RoBoT can still achieve better search performances than these two recent baselines.  We will add these baselines and comparisons in our revised paper.\n\n|  | C10 (Test Acc %) | C100 (Test  Acc %) | IN-16 (Test  Acc %) | Cost (GPU Sec.) |\n|---|---|---|---|---|\n| Shaply-NAS | 94.05\u00b10.19 | 73.15\u00b10.26 | 46.25\u00b10.25 | 14762 |\n| $\\beta$-DARTS | 94.00\u00b10.22 | 72.91\u00b10.43 | 46.20\u00b10.38 | 3280 |\n| RoBoT | **94.36**\u00b10.00 | **73.51**\u00b10.00 | **46.34**\u00b10.00 | 3051 |\n\n## About the Presentation of Table 3\n\n>  Why are the results on TransNAS-Bench-101 presented as the validation ranking?\n\nWe would like to clarify that since there is a noticeable gap between the validation and the test performances in TransNAS-Bench-101, we follow the common practice [2] to present the validation ranking, as detailed in Appendix B.2.\n\n> the search costs are not listed in Table 3\n\nWe would like to clarify that, in fact, we have included the search costs (measured by the number of queries, i.e., 100) in the caption of Table 3.\n\n---\nWe sincerely hope our clarifications above have addressed your concerns and can improve your opinion of our work.\n\n\n[1] Zero-Cost Proxies for Lightweight NAS, ICLR 2021\n\n[2] NAS-Bench-Suite-Zero: Accelerating Research on Zero Cost Proxies, NeurIPS Datasets and Benchmarks Track 2022"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978704174,
                "cdate": 1699978704174,
                "tmdate": 1700031153011,
                "mdate": 1700031153011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZzjSjCWYHV",
                "forum": "qPloNoDJZn",
                "replyto": "rDTbOS7rKx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up on Efficiency"
                    },
                    "comment": {
                        "value": "Thank you very much for the helpful response and detailed revision. Most of my previous questions have been addressed.\n\nAfter reading the review from Reviewer 81X4 and your response, I have a quick follow-up question regarding the efficiency of RoBoT. To appoximate the true performance of architectures during the search (both of the BO stage and greedy stage), the model needs to be trained (e.g., \"12 epochs in NAS-Bench-201\"). I think the model performance after 12 training epochs is already included in NAS-Bench-201. Do you directly retrieve the performance from NAS-Bench-201, or train an actual model and test it? How are the training costs counted?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082965023,
                "cdate": 1700082965023,
                "tmdate": 1700082965023,
                "mdate": 1700082965023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FMplx7hkQK",
                "forum": "qPloNoDJZn",
                "replyto": "imGUtQJZ1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6827/Reviewer_3buk"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "The authors' responses are appreciated. They have addressed my concerns. I would like to raise my rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529033707,
                "cdate": 1700529033707,
                "tmdate": 1700529033707,
                "mdate": 1700529033707,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]