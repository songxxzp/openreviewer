[
    {
        "title": "Transferable Availability Poisoning Attacks"
    },
    {
        "review": {
            "id": "Gc9w3oqStt",
            "forum": "XbLffB0T2z",
            "replyto": "XbLffB0T2z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_f8DL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_f8DL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to iteratively update poisons using TAP and CP and the resulting poisoning attack transfers across SL, CL, Semi-Supervised Learning."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation is convincing that availability poisoning attack should be effective for different learning paradigms.\n2. The proposed algorithm improves the transferability of poisoning attack across evaluation algorithms including SL, CL, Semi-Supervised Learning on CIFAR-10, CIFAR-100 and Tiny-ImageNet.\n3. This paper provides plenty of investigation on alternative methods including HF, CC, SCP, and WTP."
                },
                "weaknesses": {
                    "value": "1. The authors should check their code regarding the CL and SupCL evaluation for poisoning attacks, especially how transforms (augmentations) are performed when preparing the poisoned dataset. It seems that the data augmentation for each image is fixed before the CL (SupCL) training starts in their code. For example, in this case, each positive pair consists of two fixed augmented images. However, the CL (SupCL) algorithm applies different data augmentations each time it reads an image. Once this part of code is not properly configured, the reliability of the algorithm is questionable. For these reasons, I have significant concerns about the CL and SupCL evaluation results reported in this paper.\n2. The novelty of the proposed method is limited. The core of the TP algorithm is a just combination of two existing algorithms, i.e. iteratively updating poisons using TAP and CP to inherit their advantages.\n3. The paper seems inaccurately describe the existing work. \u201cTargeted Adversarial Poisoning (Fowl et al., 2021) can produce poisoning perturbations with high-frequency features that are not linear separable, ...\u201d However, as discussed in Table 1 of [a], TAP perturbations are linear separable.\n4. The main machanism of TP is kind of vague. This paper claims that \u201cwe propose Transferable Poisoning (TP) to generate poisoning perturbations with shared high-frequency characteristics that can be transferred across different learning paradigms (Section 3).\u201d It is unclear why high-frequency perturbation should be a goal for generating transferable poisoning attacks. The proposed algorithm even does not explicitly optimize the frequency of perturbations. In Figure 4, all four attacks are high-frequency but three of them, namely CC, SCP, and WTP have poor transferability. Does it contradict the claim?\n5. Besides the first concern about CL evaluation, I have also concerns about the comparison of attack performance.\n(1)\tTable 1,2,4 do not consider BYOL that was used in the paper of CP. What is the attack performance of TP against BYOL on CIFAR-10 and CIFAR-100?\n(2)\tIn Table 1, is SimCLR used for CP and TUE perturbation generation? However, SimCLR is not surely the best generation algorithm for transferability of CP and TUE. To be a fair comparison, it is helpful to consider CP and TUE attacks based on other CL algorithms as well as class-wise CP attacks which are effective for SL.\n(3)\tIt would be better to compare your TP with existing powerful poisoning methods (UE, AP, CP and TUE) on larger datasets like CIFAR-100 and TinyImageNet.\n6. As the algorithm involves contrastive training, it possibly costs too much time to generate TP perturbations, especially on Tiny-ImageNet and ImageNet. Can you provide time consuming comparison of your TP with existing transferable methods like AP, (class-wise) CP, and TUE? Considering availability poisoning attacks are a sort of data protection means, low efficiency of generation might hinder the applications in real world scenarios.\n\n[a] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Availability attacks create shortcuts.\n\nClarity, Quality, Novelty And Reproducibility:\nI hope the author would make more clarificaitons about the main machecism of TP and descriptions about existing work. The novelty of proposed algorithm is hindered by the combination of two existing algorithms. For reproducibility, the first concern about CL and SupCL evaluation is vital."
                },
                "questions": {
                    "value": "See Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3507/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3507/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3507/Reviewer_f8DL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3507/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676309007,
            "cdate": 1698676309007,
            "tmdate": 1699636304274,
            "mdate": 1699636304274,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0sxSubRZ0M",
                "forum": "XbLffB0T2z",
                "replyto": "Gc9w3oqStt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. Code issue for evaluating CL and SupCL?\n    \nFor evaluating CL and SupCL, we follow the implementation from CP[3] and Supervised Contrastive Learning[5] to perform data augmentations, that is, we treat the poisoned data samples as clean ones and apply data augmentations on them. All the data augmentations are in a random manner, so I don\u2019t think the augmented images will be fined each time. You can also check the code of these two papers for how the evaluation of poisoning attacks is performed.\n\n2. The poisoning method is not novel?\n    \nPlease refer to our answer to Q2 in global response.\n\n3. Inaccurate description of previous works?\n    \nIt is true that TAP perturbations are also linearly separable, but they are less linearly separable than the perturbations from EM and TUE. We will revise this part in the later version. More importantly, what we want to emphasize is the difference of these poisons on the frequency level, and we leverage TAP and CP which share similar high-frequency characteristics.\n\n4. Why does high frequency help transferability?\n    \nPlease refer to our answer to Q3 in global response. The poisons generated by TAP (specified for supervised learning) and CP (specified for unsupervised contrastive learning) share similar high-frequency patterns. Since TP is based on these two methods, the generated poisons will also have high-frequency characteristics, and the poisons will not only be effective in supervised and unsupervised contrastive learning, but can also transfer to other related learning paradigms. Other alternatives (CC, SCP and WTP) that also have high-frequency patterns but have poor transferability, which further demonstrates that the way to utilize the information from supervised and unsupervised contrastive learning is very important.\n\n5. More results?\n   \nFirst, we provide the results for other baseline methods in Table 8 in the updated paper, and it shows that our method still performs better than the baseline methods. We also change the framework of contrastive learning for other methods (CP and TUE) and report the results in Table 11 in the updated paper. Our method demonstrates more superiority in this setting.\n\n\n6. The computational cost?\n   \nPlease refer to the new Table 10 in the updated paper. As our method involves contrastive learning, the optimization time is comparatively longer, but it is still less than CP."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3507/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488621970,
                "cdate": 1700488621970,
                "tmdate": 1700488621970,
                "mdate": 1700488621970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yfTMI0p83g",
                "forum": "XbLffB0T2z",
                "replyto": "0sxSubRZ0M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3507/Reviewer_f8DL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3507/Reviewer_f8DL"
                ],
                "content": {
                    "comment": {
                        "value": "Upon checking the repositories of SupCL, CP, and this paper again, I found that your code for data preparation differs significantly from CP and SupCL. Specifically, in the construction of poisoned datasets such as `TPCIFAR10`, you perform augmentations in the `__init__` method. This implies that the augmentations applied to each image are fixed initially, and the evaluation algorithms utilize these fixed views throughout the subsequent training. On the other hand, SupCL and CP perform augmentations in the `__getitem__` method, resulting in different augmentations being applied to each image in different epochs. I think this essential difference will affect the results of the SupCL. SimCLR, SimSiam, and MoCo v2 the authors reported in this paper.  I will keep my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3507/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655696212,
                "cdate": 1700655696212,
                "tmdate": 1700655696212,
                "mdate": 1700655696212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "noCKlQR3nU",
            "forum": "XbLffB0T2z",
            "replyto": "XbLffB0T2z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_miSr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_miSr"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the challenge of availability poisoning attacks in machine learning, where training data is manipulated to degrade model performance. The authors highlight the limited effectiveness of existing attacks when the victim uses a different learning paradigm and introduce Transferable Poisoning (TP), a novel method enhancing attack transferability across various learning algorithms. TP generates high-frequency poisoning perturbations using both supervised and unsupervised contrastive learning paradigms. Extensive experiments on benchmark datasets demonstrate TP's superior performance in ensuring attack effectiveness, regardless of the victim's learning approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces Transferable Poisoning (TP) which significantly enhances the transferability of availability poisoning attacks across various learning algorithms and paradigms. This addresses a gap in existing poisoning strategies, which often assume the victim will use the same learning method as the adversary.\nThe authors provide extensive experimental results on benchmark image datasets, demonstrating that TP outperforms existing methods in terms of attack transferability. This thorough validation strengthens the credibility of the proposed method."
                },
                "weaknesses": {
                    "value": "While TP introduces improvements in transferability, the contribution appears to be incremental. The method essentially combines supervised and unsupervised contrastive learning paradigms to generate high-frequency poisoning perturbations. However, this combination does not seem to bring a substantial novelty or a paradigm shift for poisoning attacks.\n\nThe paper could be strengthened by providing a more solid theoretical foundation for why and how TP improves transferability across learning paradigms. The current explanation relies heavily on empirical observations, which, while valuable, do not provide a comprehensive understanding of the underlying phenomena."
                },
                "questions": {
                    "value": "Is there a theoretical basis that explains why combining supervised and unsupervised contrastive learning paradigms enhances the transferability of poisoning attacks?\n\nBesides test accuracy degradation, what other metrics (such as robustness, perceptibility of perturbations, and computational efficiency) have been considered to evaluate the effectiveness of TP?\n\n\n\n--after rebuttal--\nThanks for the response. My concerns about novelty remain. Thus, I would maintain my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3507/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3507/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3507/Reviewer_miSr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3507/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752721649,
            "cdate": 1698752721649,
            "tmdate": 1700752354789,
            "mdate": 1700752354789,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QpFLLl7KNf",
                "forum": "XbLffB0T2z",
                "replyto": "noCKlQR3nU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. The poisoning method is not novel?\n    \nPlease refer to our answer to Q2 in global response.\n\n2. Theoretical analysis on why and how TP provides transferability\n    \nIntuitively, as TP utilizes the similar high-frequency patterns in the poisons generated by TAP and CP which are effective for supervised and unsupervised contrastive learning respectively, the finally generated poisons can be effective for these two training algorithms and other related ones such as supervised contrastive learning and semi-supervised learning. We will try to provide more theoretical analysis later.\n\n3. Other evaluation metrics for TP?\n    \nWe provide the computational cost comparison with respect to different poisoning methods in the new Table 10 in the updated paper. As our method involves contrastive learning, the optimization time is comparatively longer, but it is still less than CP."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3507/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488599448,
                "cdate": 1700488599448,
                "tmdate": 1700488599448,
                "mdate": 1700488599448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "10JKqIWs3s",
            "forum": "XbLffB0T2z",
            "replyto": "XbLffB0T2z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_uUSA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_uUSA"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to study availability based data poisoning attacks, i.e., the goal of the attacker is to degrade the overall accuracy of a machine learning model trained on a poisoned training dataset. Existing poisoning methods assume the attacker knows the learning paradigm used for training the model, but this assumption is often unrealistic. This work proposed a transferable poisoning attack that crafts perturbations to the dataset that can be transferred to different supervised/unsupervised learning methods. Experimental results show that this poisoning attack outperforms existing poisoning attacks when transferred to almost all learning paradigms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The topic of transferable poisoning attacks is very interesting. In the real world, an attacker is unable to know the learning method that would be used to train the model.\n\nExperiments have been done for SOTA learning paradigms in recent years."
                },
                "weaknesses": {
                    "value": "This attack (and all previous attacks) needs to poison almost 100% of the dataset to perform well, which is unrealistic in practice. Although this setup is aligned with previous works.\n\nAn inconsistency in writing. Section 3.2 mentioned \u201cour method aims to generate poisoning perturbations characterized by high frequency.\u201d But this logic is not reflected in the method design. \n\nThe experiment is not very systematic. Baseline methods are only compared on the CIFAR-10 dataset. In Figure 3, poisoning ratio = 0 (clean accuracy) is not shown.\n\nThis method may be computationally hard to be applied to 224*224 images, e.g., ImageNet Images. But I think this problem also exists in previous works.\n\nThe defenses (e.g., empirical defenses and provable defenses) against poisoning attacks are not discussed. Also, the defenses are not considered. \n\nThe proposed method is a combination of two existing methods. The technique contribution is not strong."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3507/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801661259,
            "cdate": 1698801661259,
            "tmdate": 1699636304088,
            "mdate": 1699636304088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "umo7uXDECB",
                "forum": "XbLffB0T2z",
                "replyto": "10JKqIWs3s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. The setting that poisoning 100% training data is not practical?\n \nPlease refer to our answer to Q1 in global response.\n\n2. The high frequency is not reflected in method design?\n    \nPlease refer to our answer to Q3 in global response.\n\n3. Baseline methods for other datasets?\n    \nPlease refer to Table 8 in our updated paper for the results tested on CIFAR-100. Our method still performs better than baseline methods.\n\n4. High computational cost for large-scale dataset?\n    \nAs our method needs to optimize the poisons with unsupervised contrastive learning, we admit that the computational cost is comparatively high. But like what the reviewer commented, the scalability issue also exists in previous works. Improving the efficiency and scalability of poisoning attacks is an interesting future work.\n\n5. No defense is discussed?\n    \nLiu et al.[6] propose to use image compression operation to defend against availability poisoning attack. They claim that a simple JPEG operation is effective to defend against poisons with high frequency and requires much less computational cost than adversarial training. Thus we apply JPEG to defend against our method and report the results in table 9 in the updated paper. It shows that the JPEG operation can indeed recover the accuracy to some extent, while they are still lower than the model trained with clean data.\n\n6. The poisoning method is not novel?\n    \nPlease refer to our answer to Q2 in global response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3507/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488575211,
                "cdate": 1700488575211,
                "tmdate": 1700488575211,
                "mdate": 1700488575211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Qf3m33eVge",
            "forum": "XbLffB0T2z",
            "replyto": "XbLffB0T2z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_TTnu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_TTnu"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the transferability of indiscriminate poisoning attacks. The paper first shows that the transferability of pre-computed poisoning samples is low across different learning algorithms. To increase the transferability of such poisoning samples, the paper proposes \"transferable poisoning,\" an algorithm to improve the transferability. The key idea is to consider both supervised and unsupervised learning algorithms simultaneously in crafting, such that both latent representations and logits are optimized to increase the target model's loss during training. The paper runs experiments with three image classification benchmarks and shows that the attack is more transferable than existing baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes an indiscriminate attack with increased transferability.\n2. The paper show the attack's effectiveness empirically."
                },
                "weaknesses": {
                    "value": "1. The attack is non-practical; it needs the training data to be compromised completely.\n2. The attack (when not 100% training samples are compromised) is not effective in supervised learning algorithms.\n3. The frequency analyses are not scientifically rigorous.\n4. The novelty of this new poisoning attack is weak.\n5. No defense was discussed.\n\nDetailed comments:\n\n\n[Poisoning 100% of the Training Data Is Non-Practical]\n\nI am confident that the paper studies a non-practical scenario: an adversary who poisons 100% of the training data to degrade the accuracy; even with 80% of the training data being compromised, the supervised model retains most of its accuracy compared to clean models.\n\nI haven't seen any real-world scenarios where one allows an adversary to poison 100% of the training data. In any case, the training starts at least with the dataset containing 50% of clean samples.\n\nPrior work on poisoning defenses also theoretically showed that if an adversary can compromise 50% of the training data, there's no guarantee that learning will be empirically successful.\n\nFor these reasons, I also don't think that the transferability should not be measured with the completely compromised datasets.\n\n\n[Not Effective against SL, SupCL, FixMatch]\n\nMy takeaway is that even with the datasets that contain 80--100% of poisoning samples, the three supervised learning algorithms can retain the original accuracy.\n\nThis means (1) the poisoning attack, even with the increased transferability, is weak and easy to defeat, or (2) the three algorithms are designed to be robust inherently to the distributional shifts.\n\nHowever, for this paper, this observation is a weakness as the stronger attack is not actually strong in some settings where a victim does not employ any defenses; the victim could trivially depend on the attack.\n\n\n[Frequency Analyses Are Not Rigorous]\n\nI don't think the paper's claims about the high-frequency components are not scientifically backed by the results. \"High\" means that there will be a quantifiable property, and when used, the proposed poisoning attacks show large numbers. However, the paper is not.\n\nThe paper also claims that the high-frequency characteristics are \"different.\" But it is also not a property that the paper scientifically measures (or compares with criteria). The results are only drawn from the visual analysis. \n\nIt is particularly important to make this claim scientifically rigorous as the reason this poisoning attack is transferable is the paper claims that the perturbations of the attack are the high-frequency ones.\n\n\n[Incremental; Novelty Over the Prior Work]\n\nIt is less surprising that combining the classification- and representation-level losses can lead to general-purpose poisoning samples. Similar techniques have been studied, e.g., one considers all the layer outputs in a neural network for adversarial-example crafting, ensembling multiple models, unrolling the training steps to synthesize effective, transferable targeted poisoning attacks iteratively, etc.\n\n\n[No Defense Evaluation]\n\nSince the attack requires data with 100% poisoning samples, I believe defeating this poisoning attack (or breaking the transferability) is straightforward. I also want to see the discussion about them as a part of responsible vulnerability disclosure.\n\n\nOverall, for those reasons, I am leaning toward rejection."
                },
                "questions": {
                    "value": "My questions are in the detailed comments in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern about the ethics"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3507/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698914276307,
            "cdate": 1698914276307,
            "tmdate": 1699636304016,
            "mdate": 1699636304016,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "clBsuoklNM",
                "forum": "XbLffB0T2z",
                "replyto": "Qf3m33eVge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. The setting that poisoning 100% training data is not practical?\n    \nPlease refer to our answer to Q1 in global response.\n\n2. The method is not effective against SL, SupCL and FixMatch?\n    \nThe main setting of our method is to poison 100% training data, and our method is effective against all the selected learning paradigms in this setting. We admit that when the poisoning ratio is not 100%, the attack is not quite effective against SL, SupCL and FixMatch, and this phenomenon is also reported by previous works[1,2] which are specified for supervised learning. We plan to explore the poisoning attack transferability under partial-poisoning settings in future work.\n\n3. Provide more rigorous analysis about frequency?\n    \nPlease refer to our answer to Q3 in global response.\n\n4. The poisoning method is not novel?\n    \nPlease refer to our answer to Q2 in global response.\n\n5. No defense is discussed?\n   \nLiu et al.[6] propose to use image compression operation to defend against availability poisoning attack. They claim that a simple JPEG operation is effective to defend against poisons with high frequency and requires much less computational cost than adversarial training. Thus we apply JPEG to defend against our method and report the results in table 9 in the updated paper. It shows that the JPEG operation can indeed recover the accuracy to some extent, but they are still lower than the model trained with clean data."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3507/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488501635,
                "cdate": 1700488501635,
                "tmdate": 1700488501635,
                "mdate": 1700488501635,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iVsxW8ppN4",
            "forum": "XbLffB0T2z",
            "replyto": "XbLffB0T2z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_s6uj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3507/Reviewer_s6uj"
            ],
            "content": {
                "summary": {
                    "value": "The authors address the problem of generating indiscriminate poisoning attacks that can be applied across various learning paradigms, including supervised, semi-supervised, and unsupervised learning. First, they assess the effectiveness of several existing strategies in terms of their transferability. Then, the authors introduce a novel approach called \"Transferable Poisoning\", which combines two strategies to achieve strong transferability across the considered learning paradigms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper provides an interesting contribution to a significant topic, the transferability of poisoning attacks across diverse learning paradigms, an area that has seen limited exploration. The authors introduce a straightforward and intuitive yet effective method, which is tested on a broad range of experiments. The manuscript is well-written overall, though there is space for improvement."
                },
                "weaknesses": {
                    "value": "I believe this work has two main weaknesses:\n\n1. There is little emphasis on the motivation for studying transferability across learning paradigms, which limits the impact of the findings. It would greatly benefit the paper to explain the practical contexts where this kind of transferability might be relevant, offering concrete examples and detailing how the conducted experiments address these cases. This is particularly crucial given the pragmatic nature of the contribution.\n\n2. While the frequency analysis provides an intriguing perspective on the problem, it could be explored more thoroughly. The authors argue that generating high-frequency perturbations is key to crafting transferable poisoning attacks. This idea is supported by a simple frequency analysis of perturbations generated by existing methods, and so the proposed approach combines two existing \u2018high-frequency\u2019 poisoning schemes. Nevertheless, there is no attempt to substantiate this claim with evidence confirming the necessity of high-frequency perturbations for achieving transferability.\n\nIn addition to the above points, I have the following minor concerns:\n\n1. More details should be provided on the considered poisoning methods, the associated learning paradigms, and how the models are tested. Please consider adding this information to both the main text and appendix.\n\n2. The results in the tables lack error bars. \n\n3. The frequency analysis is limited to a simple visualisation of the perturbations\u2019 spectrum. Could you provide a more quantitative comparison? Additionally, please define the spectrum and add a scale to the spectrum plots (ticks are missing)."
                },
                "questions": {
                    "value": "1. Can you insert a frequency constraint on the generated perturbations by modifying the loss function? What happens if you impose high or low-frequency perturbations when using different combinations of attack methods? Could this be a way to test the requirement for transferable perturbations to be high-frequency?\n\n2. In some cases, the proposed algorithm \u2018ours\u2019 performs better than the one designed for a given learning paradigm. See for example Table 1, SimSiam and SimCLR: \u2018ours\u2019 outperforms \u2018CP\u2019. Is there an intuitive explanation for this? Analogously, in Table 6, some results indicate that transferred attacks work better on architectures they were not originally designed for. Error bars would be particularly useful in these cases.\n\n3. In section 4.3 you say that MoCov2 is used both for poison generation and evaluation. Could you please elaborate? Also, what does Framework mean in Table 4?\n\n4. I am not sure I understand the result in Fig. 3, panel C. Is there an explanation for the model performing better when trained on perturbed data? Why only in this case?\n\n5. What is the computational cost of the proposed strategies? A comment on the computational costs should be added to the main text.\n\nComments:\n\n1. Captions could be expanded to include experimental details (what model is attacked, using which dataset, etc) and explain the acronyms.\n\n2. The tables show results for supervised, semi-supervised, and unsupervised algorithms. Shouldn\u2019t SimSiam and SimCLR be grouped together in the tables?\n\n3. I would suggest using an acronym for the proposed method instead of calling it \u2018ours\u2019.\n\n4. It would be interesting to have the perturbed images displayed alongside their clean versions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3507/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3507/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3507/Reviewer_s6uj"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3507/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699293548802,
            "cdate": 1699293548802,
            "tmdate": 1699636303942,
            "mdate": 1699636303942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4Gl8T3NxBI",
                "forum": "XbLffB0T2z",
                "replyto": "iVsxW8ppN4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3507/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. What is the motivation of this paper?\n   \nWe explain the motivation of our work in the introduction. To put it simple, since the three main learning paradigms (supervised, semi-supervised and unsupervised learning) can reach similar performance in various machine learning tasks, the victim may choose any learning algorithm from them to train their model. However, the current availability poisoning attacks often target for a specific learning paradigm and have poor transferability across different learning paradigms. Therefore, we propose to study the attack transferability on the level of learning paradigms.\n\n2. More analysis on frequency and insert a loss function to modify frequency?\n\nPlease refer to our answer to Q2 in global response. The current visualizations can clearly show that TAP and CP can generate poisons with similar high-frequency patterns, which are more different to the low-frequency poisons generated by EM and both low- and high-frequency poisons generated by TUE. Since we do not claim that only high-frequency poisons can have transferability nor poisons with higher frequency can have better transferability, we think that using a loss function to modify the frequency or providing quantitative results will not provide further explanations.\n\n3. Results with error bars?\n\nWe admit that using error bars will make the results more reliable. We will provide it later due to the limited computational resources.\n\n4. What does framework mean and elaborate the use of MoCov2 in both poison generation and evaluation?\n\nThe Framework means the working mechanisms of different unsupervised contrastive learning, such as SimCLR and MoCov2. So in section 4.3, we change the framework from SimCLR to MoCov2 to generate the poisons and also involve MoCov2 as one unsupervised contrastive learning algorithm to evaluate the generated poisons.\n\n5. Question about figure 3?\n\nAs for panel C in figure 3 where the training algorithm is semi-supervised learning, both the labeled and unlabeled data are partially poisoned, so for the model trained with poisoned data, the algorithm is more likely to involve the unlabeled data that will be helpful for the training including part of the unlabeled poisoned data, which means the model can learn more information and the results will be better. While for other settings, all poisoned data take part in the training originally, so the results will be worse than model trained with the rest of the clean data.\n\n6. Computational cost?\n\nPlease refer to the new Table 10 in the updated paper. As our method involves contrastive learning, the optimization time is comparatively longer, but it is still less than CP."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3507/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488448387,
                "cdate": 1700488448387,
                "tmdate": 1700488448387,
                "mdate": 1700488448387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9SAJhY3KmQ",
                "forum": "XbLffB0T2z",
                "replyto": "4Gl8T3NxBI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3507/Reviewer_s6uj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3507/Reviewer_s6uj"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for the effort put into the rebuttal and for providing the new experimental results. As I mentioned in my review, this work tackles an important topic - transferability across learning paradigms - and it provides an effective method by combining existing high-frequency poisoning techniques. While I acknowledge the relevance of the contribution, I believe that there is room for improvement. This should involve providing more evaluations and estimating confidence intervals for the results, investigating the high-frequency hypothesis, offering more context and explanation of the motivation, and adding more details about the learning algorithms utilized and the employed poisoning methods. It will be important to address these points in the final version of the manuscript."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3507/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670543013,
                "cdate": 1700670543013,
                "tmdate": 1700670543013,
                "mdate": 1700670543013,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]