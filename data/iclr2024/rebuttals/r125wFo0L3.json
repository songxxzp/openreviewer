[
    {
        "title": "Large Trajectory Models are Scalable Motion Predictors and Planners"
    },
    {
        "review": {
            "id": "1603RYvbCi",
            "forum": "r125wFo0L3",
            "replyto": "r125wFo0L3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5908/Reviewer_ixB2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5908/Reviewer_ixB2"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an architecture for trajectory prediction using a generic causal transformer, and validates the scaling law on large datasets (WODM, NuPlan), achieving good performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Sufficient originality and significance. The story captures the existing popular trends, targeting the task of autonomous driving trajectory prediction. It adopts a generic and easily scalable architecture, and validates the scaling law on large datasets, offering a certain reference value.\n2. The article is overall well-written, flowing smoothly, and easy to understand.\n3. Achieved commendable results on large public datasets."
                },
                "weaknesses": {
                    "value": "1. The story is well told, but the experimental support is weak, some of key ablations are missing: \n* There is a lack of ablation studies for the model design, such as quantifying the improvement brought about by the proposed keypoints module, or comparing the GMM decoder versus the diffusion decoder. \n* For the planning module, there is a lack of closed-loop evaluation, which is crucial for the planning component."
                },
                "questions": {
                    "value": "1. Why haven't the closed-loop results on NuPlan been included? Since you've already integrated the open-loop evaluation with NuPlan, conducting a closed-loop test should be quite straightforward. Is it because the results were not satisfactory?\n2. In Table 2, STR(CPS)-16m (Ours) performs better than MTR-e2e in terms of the MAP metric; however, it lags behind in minADE, minFDE, and MR. Could you elaborate on the reasons behind these differences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5908/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5908/Reviewer_ixB2",
                        "ICLR.cc/2024/Conference/Submission5908/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5908/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698090908842,
            "cdate": 1698090908842,
            "tmdate": 1700432265923,
            "mdate": 1700432265923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zDxdaAT8ia",
                "forum": "r125wFo0L3",
                "replyto": "1603RYvbCi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ablation Study Added"
                    },
                    "comment": {
                        "value": "Thank you for the time to read and review our paper. We find your review very constructive!\n\nWe have rewritten the abstract, introduction, and some other parts of the paper to focus on scalability for better clarity. We also attached our full code (with readme and instructions), which will be open-sourced with pre-trained checkpoints for easy reproduction. If you have any further concerns, we would be keen to address them. We will incorporate all of these changes in the final revision.\n\n# General comments\n1. **Re Ablations on the decoders:** We added a new section 5.5, and Table 3 for the ablation study on the Key Points and the diffusion decoders.\n2. **Re NuPlan CLS:** We measured the STR(CKS)-16m without diffusion decoders on two CLS metrics. The scores are higher than PDM-Open and lower than PDM-Hybird. We are exploring different flavors of post-procession on the Key Points for better and more stable performance across different test sets. We will include these scores (both CLS-Interactive and CLS-NonInteractive) in the Appendix in the final version.\n3. **Re Not significantly better than MTR-e2e on all metrics:** As you might notice, we used the STR(CPS)-16m version to compare. We are working on larger models and will include a better performance (on minADE and minFDE) in the final version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974253459,
                "cdate": 1699974253459,
                "tmdate": 1699974253459,
                "mdate": 1699974253459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r5b9Go0OTV",
                "forum": "r125wFo0L3",
                "replyto": "1603RYvbCi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_ixB2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_ixB2"
                ],
                "content": {
                    "title": {
                        "value": "Mistake in Table 2"
                    },
                    "comment": {
                        "value": "In Table 2, you have incorrectly listed the metrics for SceneTransformer[1]. In the original text, SceneTransformer reports minADE and minFDE using a traditional calculation method that only considers minADE and minFDE at t=8s. However, the metrics you report are calculated by averaging t=3, t=5, and t=8s, which makes the comparison quite unfair. Please make sure to correct this. Refer to MTR[2] (NeurIPS 2022 version) Table 1 for the correction. From this perspective, the results of this paper do not seem solid (with minADE and minFDE being significantly lower than other models) and there is a suspicion of inflated reporting. Additionally, the author has not directly addressed my concerns. After considering the advice of other reviewers, I have decided to temporarily downgrade my rating to \u2018weak reject\u2019.\n\n[1] Ngiam J, Caine B, Vasudevan V, et al. Scene transformer: A unified architecture for predicting multiple agent trajectories[J]. arXiv preprint arXiv:2106.08417, 2021.\n\n[2] Shi S, Jiang L, Dai D, et al. Motion transformer with global intention localization and local movement refinement[J]. Advances in Neural Information Processing Systems, 2022, 35: 6531-6543."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432054817,
                "cdate": 1700432054817,
                "tmdate": 1700432253088,
                "mdate": 1700432253088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lueBoCO5He",
                "forum": "r125wFo0L3",
                "replyto": "1603RYvbCi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_ixB2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_ixB2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. \nI understand that minADE/minFDE and mAP may not necessarily align, I understand that minADE/minFDE and mAP may not necessarily align. What I mean is that such experimental results are not very impressing, and I believe the biggest issue is still the incompleteness of the experiments. As mentioned in your title, motion and planning are the two core tasks you aim to verify, but the experiments for both of these tasks are somewhat lacking. Firstly, there is a lack of scaling data on the WOD motion dataset, and secondly, there is a lack of closed-loop validation for planning. Additionally, the metrics for motion do not seem very impressive (minADE/minFDE are significantly worse than others). I think such experimental results are not sufficiently convincing. I am inclined to maintain the current score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603999851,
                "cdate": 1700603999851,
                "tmdate": 1700604413948,
                "mdate": 1700604413948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m3JDDww9Ab",
                "forum": "r125wFo0L3",
                "replyto": "1603RYvbCi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_ixB2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_ixB2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the results you've added. I apologize for not making my intentions clear. What I am more interested in seeing is the results of mAP/minADE/minFDE vs scale vs other methods like what you've done in Table1. For planning, as you pointed out, CLS and OLS are not aligned, so when we discuss planning, the final results should be based on CLS rather than OLS. This is because eventually, CLS is what closely represents the real-world scenarios. Otherwise, there would be no difference between the tasks of planning and motion prediction. All the experimental results only prove that Large Trajectory Models are Scalable Motion Predictors but not prove the Scalable Planners."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681893728,
                "cdate": 1700681893728,
                "tmdate": 1700686770377,
                "mdate": 1700686770377,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q6S4pbqG21",
            "forum": "r125wFo0L3",
            "replyto": "r125wFo0L3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5908/Reviewer_Cy2V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5908/Reviewer_Cy2V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes State Transformer (STR) that leverage the transformer model to conduct the motion prediction and motion planning simultaneously, by predicting both the future key points and states.  Extensive experiments show that STR outperforms the baseline methods on NuPlan motion planning dataset significantly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed STR is able to conduct motion planning and prediction simultaneously, thereby being able to capture the complexed semantic information.\n2. The proposed STR leverages the temporal information by formulating the motion tasks and sequential prediction problems, which helps to improve the performance."
                },
                "weaknesses": {
                    "value": "1. The method description is not well organized and some details are missing, e.g., the proposal, key points and $O_t$ in Fig. 1. Besides, the introduction of the conditional diffusion (Sec 3.2) has no tight logic connection with the method (Sec 4). The only place that mentioned diffusion model in Sec 4 is \"STR is compatible with diverse decoders like Gaussian Mixture Model or diffusion decoders.\" \n2. The novelty of this work. Though it is interesting to apply transformer and diffusion on motion planning/prediction tasks, it remains ambiguous as to the unique contributions or innovations introduced by this work in adapting the diffusion model to this specific domain.\nSpecifically, it is not new to adopt an encoder-decoder architecture to take the context as input, and predict the future states, which has been studied by MPNet [R1]. STR uses a different but existing neural architecture which seemingly lacks new contributions. \n\n[R1] Qureshi, Ahmed H., et al. \"Motion planning networks.\" 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019."
                },
                "questions": {
                    "value": "1. What is the data format of the context? Do you need to convert it into a set of vectors?\n2. As in Section 5.2, you have different choices of encoders for different datasets, what is the concern here? Is there any guidance on how to select the proper encoder when facing a different dataset?\n3. Similar to above questions, Sec 4 mentioned that \"STR is compatible with diverse decoders like Gaussian Mixture Model or diffusion decoders.\" Do you have any thoughts on how to select the proper decoders and what are their pros/cons?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5908/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5908/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5908/Reviewer_Cy2V"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5908/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777018989,
            "cdate": 1698777018989,
            "tmdate": 1699636627984,
            "mdate": 1699636627984,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zOT0TOcE9t",
                "forum": "r125wFo0L3",
                "replyto": "q6S4pbqG21",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time to read and review our paper. \n\nWe simplified the structure and focused on our core insight, which is sequence modeling brings scalability and solves many previous challenges. The design choices of the encoders and decoders are not important in this work. We have rewritten most of the abstract and the introduction to focus on this insight. Please see our uploaded revision. \n\n**Re novelty compared with MPNet:** As we pointed out in the abstract and introduction, learning from large-scale real-world demonstrations brings non-trivial additional challenges compared to [1] which seems to be a more relevant work than MPNet. Specifically, MPNet learns from a small set of RRT* planning results. Additionally, MPNet does not consider other agents (or dynamic environments) at all. Specifically, MPNet does not provide the environment's history, including the past trajectories of other agents and the traffic lights for the motion prediction and planning tasks. \n\nWe also attached our full code (with readme and instructions), which will be open-sourced with pre-trained checkpoints for easy reproduction. You can easily check all implementation details about data preprocessing, encoders, and decoders in our code.\n\nWe added another section to the related works about previous diffusion decoders on motion planning and motion prediction as an introduction before Section 3.2 and Section 4 for a better continuity flow on writing.\n\n# References\n[1] Yanchao Sun, Shuang Ma, Ratnesh Madaan, Rogerio Bonatti, Furong Huang, and Ashish Kapoor. SMART: Self-supervised multi-task pretraining with control transformers. In International Conference on Learning Representations, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974047066,
                "cdate": 1699974047066,
                "tmdate": 1699974047066,
                "mdate": 1699974047066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x7H5zWq57Q",
                "forum": "r125wFo0L3",
                "replyto": "zOT0TOcE9t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_Cy2V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_Cy2V"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "Thank the authors for addressing part of my questions. I would like to keep my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500272202,
                "cdate": 1700500272202,
                "tmdate": 1700500272202,
                "mdate": 1700500272202,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "muBS0QTay2",
            "forum": "r125wFo0L3",
            "replyto": "r125wFo0L3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5908/Reviewer_2D3R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5908/Reviewer_2D3R"
            ],
            "content": {
                "summary": {
                    "value": "Inspired by the success of self-attention for sequence modeling in large language models, this paper attempts to demonstrate scaling laws for predicting future vehicle trajectories with self-attention-based large *trajectory* models.\n\nTo do so, the paper casts motion forecasting (trajectory prediction for non-ego actors) and motion planning (trajectory prediction for the ego vehicle) as a sequence modeling problem suitable for autoregressive transformers. The sequence tokens are divided between the context, proposal, key points, and future states. The context consists of input data such as the map elements and past actor trajectories. The proposals are only relevant for the Waymo Open Motion Dataset (WOMD) and are (I think) implemented as intention points from MTR++ [0]. The key points are spatial locations 0.5, 1, 2, 4, and 8 seconds in the future. Finally, the future states are the full set of positions over each 100ms frame of the full 8 second trajectory.\n\nThe context is generated with dataset-specific encoders. For instance, for NuPlan, a ResNet18 raster encoder is used; for WOMD, an MTR vector encoder is used. The outputs of these encoders become inputs to the autoregressive transformer.\n\nThe proposals, key points, and future states have different decoders. Proposals and future states are trained with simple cross-entropy and mean-squared error losses, while the key points are output using a pre-trained keypoint diffusion model.\n\nTheir model (State Transformer or STR) is evaluated using standard metrics on NuPlan and WOMD. On WOMD, a 16-million parameter variant of their model is outperformed by MTR [1]. On NuPlan, the top performing model is their 125M parameter model; scaling the model to 1.5B parameters does not significantly improve the model performance. The STR model outperforms all baselines on NuPlan, although there is significantly less literature evaluating on NuPlan (in contrast to WOMD).\n\nFinally, this paper attempts to demonstrate scaling laws and properties of motion forecasting. Qualitative analysis shows that larger models produce improved trajectories, even though quantitative analyses do not show as strong a trend. Additionally, qualitative analyses show improved generalization. The experiments demonstrate improved performance as measured by converged loss as a function of both dataset size and model size.\n\n\n[0] https://arxiv.org/pdf/2306.17770.pdf\n[1] https://arxiv.org/pdf/2209.13508.pdf"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is the first attempt to scale up trajectory forecasting to models significantly larger than a few million parameters and is one of the first few demonstrations of using an autoregressive transformer for motion forecasting. Additionally, it demonstrates the usefulness of the NuPlan dataset when performing larger scale research, since the dataset is comprised of a larger set of trajectories.\n\nThe experiments do convincingly show improved performance with dataset size and model size, indicating that larger datasets and larger models are likely to improve performance significantly further."
                },
                "weaknesses": {
                    "value": "The two key weaknesses of this paper which need to be addressed are complexity and clarity of writing. \n\nFirst of all, the described model is too general. Instead of describing what was actually implemented, the authors write about what *could* have been implemented. This is partially because the authors are trying to describe their work as one model on two different datasets; in reality, they have implemented two *different* albeit related models, customized towards the two datasets they work on. I would recommend reworking the description to be as clear as possible about what was implemented rather than describing the possibilities. For example:\n\n\"These Proposals can be intention points (Shi et al., 2022), goal points (Gu et al., 2021), or endpoints heat maps (Gilles et al., 2021).\"\n\nIt is not important what the proposals *could* be, what is important is what was actually implemented in this work.\n\nThe same applies to key points and future states: please state clearly what *precisely* these are when introducing the concepts, rather than leaving that detail to later sections.\n\nAs an example, \"For each Key Point, STR is compatible with diverse decoders like Gaussian Mixture Model (GMM) decoders (Chai et al., 2019), or diffusion decoders to produce K different predictions\": it does not matter what STR is compatible with, what is important is what was implemented and evaluated. If GMMs were not used, it is not relevant to include in the description.\n\nNext, the model is quite complex, and as a result, it's very hard to understand where gains and performance are coming from. While the authors attempt to reduce motion forecasting to a self-attention transformer sequence modeling problem, in practice, that's not what is happening here; for example, on WOMD, the model is an MTR encoder, followed by a flattening into a sequence, followed by a diffusion model for the keypoints but not for the future states. The parameters are split between the dataset-specific encoders, the transformer backbone, and the diffusion model.\n\nI would recommend making a few changes:\n\n1. Remove diffusion models from this paper. The diffusion models are tangential to the point the paper is aiming to make around scaling, and they add significant complexity to the work that will make it difficult to reproduce and evaluate.\n2. Consider separate the model used for NuPlan and for WOMD and describe those models separately.\n3. Describe the split of parameters between the encoders and the transformer. \n4. (General) Find ways to simplify the setup, model, and description.\n\nOverall, the paper has a very strong core experimental setup and approach, but it is hindered significantly by the complexity of the model (in contrast to the simplicity of large language models) and the complexity of the writing. If these can be simplified to isolate the core result to one about performance of a simple model scaling with data and compute, this paper can be much improved."
                },
                "questions": {
                    "value": "There are a few unclear technical choices made. Causal transformers are relevant for language modeling due to a sampling step after each token. However, in this problem, there is no sampling step until the key point generation, so why is the entire transformer causal? Similarly, autoregressive modeling is crucial for language models due to the sampling step, but since there is no sampling when producing the future states, is it important to have the autoregressive setup?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5908/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820474332,
            "cdate": 1698820474332,
            "tmdate": 1699636627719,
            "mdate": 1699636627719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1wQYHtnfEY",
                "forum": "r125wFo0L3",
                "replyto": "muBS0QTay2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time to read and review our paper. \n\nWe simplified the structure and focused on our core insight, which is sequence modeling brings scalability, solving many previous challenges. We have rewritten most of the abstract and the introduction to focus on this insight. Please see our uploaded revision. \n\nWe also attached our full code (with readme and instructions), which will be open-sourced with pre-trained checkpoints for easy reproducing. Since the scalability comes from the formulation (Note the results in Fig. 2 do not involve the diffusion decoders), readers can easily train a fairly good model without the diffusion decoder\u2019s two-stage training (See new ablation section 5.5). \n\nWe tried our best to organize our code for easy training and evaluations with tremendous effort. We used the Hugging Face interface so that importing the dataset or the model is one-line code away. You can scale the model simply by passing different arguments. \n\nIf you have any further concerns, we would be keen to address them. We will incorporate all of these changes in the final revision.\n\n# General Comments\nSeveral misunderstandings in the summary part which led to further concerns later:\n1. **Why 1.5B STR does not significantly perform better than 125M numerically?** Training 1.5B STR takes more time and the 1.5B STR\u2019s result in Table I is not fully converged with significantly fewer training steps than the 125M STR. We added a footnote (see page 7, marked in red) to further clarify and will include a converged result of the 1.5B STR on NuPlan in the final version. \n2. **Why do we choose different encoders and decoders for each Dataset?** Scalability is the core of this paper not the design choice of the encoders or decoders for each dataset. To clarify this insight, we rewrote the abstract and the introduction. We observe scalability on both NuPlan and WOMD with both raster and vector encoders. Note adapting additional encoders to a large-scale dataset is a non-trivial task. We will add a vectorized encoder (adapted from the PDM) for NuPlan and a rasterized encoder for WOMD along with their experiment results in the final version.\n3. **Pre-trained Key Points diffusion decoder?** We did not pre-train the Key Points diffusion decoder. They are trained from scratch in a two-stage fashion, like many text-to-image diffusion decoders.\n4. **\u201cThere is significantly less literature evaluating on NuPlan (in contrast to WOMD)\u201d?** We did spend more literature evaluating experiment results on NuPlan in our paper. For example, the whole 5.3 section about scaling laws is about results on NuPlan. In the 5.4 section, we spent two paragraphs on NuPlan vs. one paragraph on WOMD. The whole 5.5 section, again, is all about results on NuPlan.\n\nAddressing the weaknesses:\n1. **Re The described model is too general:** We understand your concern. As previously addressed, the study of scalability is the core of this paper. We intentionally test different encoders on different datasets to test if the scalability holds. For better clarity, we deleted other potentials (goal points or heat map) for the Proposal from Section 4 - paragraph Proposal. We also deleted the GMM decoder description for the Key Points from Section 4 - paragraph Key Points. Changes are marked in red in the revised version. \n2. **Re Not a transformer sequence modeling problem in implementation?** Although we used the MTR encoder, they are not identical. Specifically, we encode the context at each frame separately as illustrated in Fig. 1, but the original MTR encodes the context at all frames together before the decoder. This is a non-trivial change as discussed in [1]. There are no tricks here in the code. We refer the reviewer to check the \u2018model.py\u2019 file for more details from the attachment.\n3. **Re split of parameters:** As introduced in section 5.3, the number of trainable parameters is only from the backbone transformers. Hence, the number of parameters of the encoders is less relevant to the study of the scaling. For your reference, the detailed number of the trainable parameters of each part in our model is listed below.\n\n|      | Mini | Small | Medium | Large |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| Waymo Vector Encoder      | 738,736 | 6,337,072 | 52,624,944 | 80,278,960 |\n| NuPlan Raster Encoder      | 11,290,464 | 11,340,672 | 11,474,560 | 11,692,128 |\n| Backbone Transformer.      |  3,332,096 | 16,287,488 | 124,439,808 | 1,557,611,200 |\n| KP MLP Decoder.              | 18,436 | 270,340 | 2,383,876 | 10,291,204 |\n| Key Point Diffusion Decoder | 12,793,700 | 13,410,404 | 16,496,740 | 25,984,868 |\n| Trajectory MLP Decoder | 18,436 | 270,340 | 2,383,876 | 10,291,204 |\n\n# References\n[1] Hao, Y., Song, H., Dong, L., Huang, S., Chi, Z., Wang, W., Ma, S. and Wei, F., 2022. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973823191,
                "cdate": 1699973823191,
                "tmdate": 1699973823191,
                "mdate": 1699973823191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZXj3KFvA7O",
                "forum": "r125wFo0L3",
                "replyto": "muBS0QTay2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Deadline coming. looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 2D3R, we kindly request your feedback as the rebuttal deadline is fast approaching. We hope our previous feedback and revised paper address your concerns and increase your confidence. We would like to thank you again for your review which led this paper to a better quality. We look forward to any additional comments or further discussions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536168817,
                "cdate": 1700536168817,
                "tmdate": 1700536168817,
                "mdate": 1700536168817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yHkrfBvIOA",
                "forum": "r125wFo0L3",
                "replyto": "ZXj3KFvA7O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_2D3R"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_2D3R"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for incorporating the changes and formulating a response. I would like to keep my rating as it is.\n\nI think the goal of this work is good and it would be a significant contribution but the experimental setup and results could be stronger to support the conclusion fully. Architecture choices are made which are not clearly justified (diffusion models and causal transformer backbone \u2013\u00a0why not just scale the Wayformer architecture?) and the results do not indicate significant improvements on metrics from scaling."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646280040,
                "cdate": 1700646280040,
                "tmdate": 1700646280040,
                "mdate": 1700646280040,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ly6YTa5Wd0",
                "forum": "r125wFo0L3",
                "replyto": "qmG69Pg4Od",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_2D3R"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_2D3R"
                ],
                "content": {
                    "title": {
                        "value": "Additional clarification"
                    },
                    "comment": {
                        "value": "Figure 2 does clearly show the improvement of the model with data and scale \u2013\u00a0if you trust the metric of the loss function. However, the loss function does not seem correlated with all the metrics \u2013 for example, 8sADE and 3sFDE and 5sFDE are better on the 124M parameter model than the 1.5B model. While this *suggests* that scaling the model is helpful (loss clearly goes down!), it doesn't conclusively demonstrate improved generalization or performance on real-world application, since the metrics that traditionally measure motion forecasting (e.g. 8sADE) actually don't improve.\n\nIn the PDF I see Appendix A and B but not D, and no section labeled \"Scaling Analysis\". Does MotionLM fail to scale above 10M parameters *on the same dataset and training setup* as your study? To make the claim that STR scales better than MotionLM, or better than MTR++ or Wayformer, we need a study that trains those architectures under the same configuration and datasets as STR. To make a strong claim that STR scales better, we need scaling curves on the same dataset for at least one other architecture, while right now scaling curves are only available in this study for STR and not a baseline architecture."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657757790,
                "cdate": 1700657757790,
                "tmdate": 1700657757790,
                "mdate": 1700657757790,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tKFvSlJp73",
            "forum": "r125wFo0L3",
            "replyto": "r125wFo0L3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5908/Reviewer_6cJv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5908/Reviewer_6cJv"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackle the problems of motion prediction and planning for self-driving. The authors first frame these problems as identical except for the fact that planning requires conditioning on a high-level route. The authors then propose to model 8 second trajectories by autoregressively sampling locations at (8,4,2,1,0.5) seconds into the future, then using regression to upsample these key points to a full 10hz 8 second trajectory. A highlight of the paper is that the authors leverage causal transformers and demonstrate that larger variants of their model converge to a better validation loss as the dataset size is scaled, similar to large language models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Motion prediction is certainly an important problem for self-driving, and I think stating explicitly how planning and motion prediction are connected as the authors have done is important for the self-driving community. I also think understanding how these models scale with compute and data is important so that we can make better guesses about how much data needs to be collected in order to achieve a certain performance. I haven't seen these kinds of scaling plots in other motion prediction papers. Finally, I think diffusion is a promising technique for modeling trajectory distributions, and it's great to see new techniques - such as the author's approach of only modeling a coarse trajectory then regressing the rest - for making diffusion work in practice for the motion prediction task."
                },
                "weaknesses": {
                    "value": "The first weakness I want to mention is that I'm missing the motivation for some of the design decisions of the model. A central claim of the paper is that they reduce motion prediction to a \"sequence modeling task\". However, for the historical trajectories and map information that the model conditions on, I don't see the motivation for encoding this information causally in time. Full attention across time should work just as well while being more parameter-efficient (for instance, as was done in wayformer), especially when some of the history is partially occluded as in motion prediction data. Additionally, since the authors use diffusion to model future keypoints, I also don't see the motivation to force the diffusion model to be autoregressive. In my mind, the diffusion model should be able to model all future points at once without generating each one-by-one. If performance was amazing with these choices, I'd understand it, but Table 2 shows that motion prediction performance is well below SOTA. If the authors want to claim that these choices improve the scalability of the model, they should compare against a model with full attention in the encoder and all-at-once diffusion in the scaling plot Figure 2.\n\nThe second weakness I see is that the way in which the model is adapted from MTR or other motion prediction models is not stated as clearly as it should be. My sense is that the method that the authors propose involves training a GPT-like model with temporally-causal mask to regress (x,y) locations at (8,4,2,1,0.5) seconds, then freezing the encoder and training the keypoint decoder with diffusion instead of regression, then keeping the encoder frozen and training an output head that upsamples the keypoints to a 10hz 8-second trajectory. The authors also reference \"classification scores\" though, so I don't think I'm getting the full picture. My sense from Appendix A.1 is that the reason for these choices is that they stabilize the training of the diffusion model. If that's the case, I think most of the motivation of the paper should emphasize that the goal is to improve stability of diffusion for motion prediction, not to turn motion prediction to a sequence prediction task which is currently stated as the main motivation.\n\nOther notes are included below:\n\n- abstract \"learn to make complex reasonings for long-term planning, extending beyond the horizon of 8 seconds\" - where do the authors show that the model generalizes beyond 8 seconds in the paper?\n- intro \"We arrange the past and future states of the target into one sequence for learning and generation\" - I feel the main novelty of the paper is in the design of the decoder. So maybe it's only important to mention that future states are encoded in a sequence.\n- 3.1 first paragraph - there's a mismatch in notation between $s^F$ and $s^T$. Additionally, it should probably be acknowledged in this paragraph that this paper approximates the multi-agent future trajectory distribution as independent, e.g. $p(s^T | c, s_{ego}, s) \\approx p(s^T_{ego} | c, s_{ego}, s) \\cdot p(s^T_0 | c, s_{ego}, s) \\cdot ... \\cdot p(s^T_N | c, s_{ego}, s)$.\n- Section 4 \"Finally, the model learns the pattern to generate all future states given the contexts and conditions with direct regression losses\". Would it make sense to represent the full trajectory as a sequence of key points? Regression can lead to out-of-distribution trajectories, even when key points at t=(0.5,1,2,4,8) seconds are used for conditioning.\n- Figure 2 - for the plot on the right, what do the circular dots and the square/triangle dots represent?\n- 5.2 \"in case the potential features from images and videos from cameras are able to be incorporated in the future\" - I don't exactly see why it's easier to incorporate camera-frame features if the input is rasterized? Some elaboration of this point would be helpful.\n- 5.2 \"decoders\" - I found this section hard to understand. What are \"stacked MLPs\"? What is the \"additional MSE loss\" in addition to? Is \"distance regression loss\" different from MSE loss? For the \"classification scores\", what is the network classifying between?\n- Appendix A.1 \"a plain MLP decoder is used when training the Transformer backbone from scratch, then the backbone is frozen and we train a diffusion-based key points decoder\" are there issues when the model is trained with diffusion from scratch? Is there a reason to make the decoder autoregressive if diffusion is being used?\n- Table 2 - Can the authors expand on why they only compare to MTR-e2e? MTR and MTR++ get mAP above 0.40 on WOMD"
                },
                "questions": {
                    "value": "My main questions are included in the \"weaknesses\" section. If the authors only focused on comparing scaling laws for standard motion prediction architectures, that would be one thing. But since the authors measured scaling curves only for the new model that they proposed, it's important to understand if the proposed model has a principled design. It seems the authors main some design choices to improve the stability of diffusion, and it would be very helpful if the authors could ablate these choices more thoroughly. I also currently do not see the value in framing the approach as \"sequence modeling\" - which is a big emphasis of the paper - given that full attention and all-at-once diffusion should be more compute and parameter efficient."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5908/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698952211195,
            "cdate": 1698952211195,
            "tmdate": 1699636627611,
            "mdate": 1699636627611,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ENYXbPV4jN",
                "forum": "r125wFo0L3",
                "replyto": "tKFvSlJp73",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 6cJv:\n\nThank you for the time to read and review our paper. We find your review very constructive and in great detail!\n\nThese questions and concerns lead us to reconsider the structures to present our core insight, which is sequence modeling brings scalability and scaling up solves many previous challenges. We have rewritten most of the abstract and the introduction to focus on this insight. Please see our uploaded revision. \n\nWe also attached our full code (with readme and instructions) which will be open-sourced with pre-trained checkpoints for easy reproducing. Since the scalability comes from the formulation (Note the results in Fig. 2 do not involve the diffusion decoders), readers can easily train a reasonably good model without diffusion\u2019s two-stage training. \n\nIf you have any further concerns, we would be keen to address them. We will incorporate all of these changes in the final revision and credit this anonymous reviewer in the acknowledgment.\n\n# General Comments\n\n1. **Re Missing the value of sequence modeling formulation:** We find that sequence modeling formulation is the key to scalability. Our experiments indicate great scalability, whether with or without the diffusion decoders on both NuPlan and WOMD. So, the choices for neither encoders nor decoders are relevant to the scalability. We also added a comparison with the recent MotionLM [1] paper, indicating fundamental scalability differences with previous non-sequence modeling models. (see the revised version, Section 1 - Intro, paragraph 3 marked in red). MotionLM also used autoregressive down-sampled (2Hz) key points for prediction but they cannot scale over 10M.\n\n2. **Re Full attention without masks?:** We did implement a mask for invalid data of the WOMD dataset. We skip the details because this is a common implementation for the WOMD dataset only. We believe more shapes of attention for better parameter efficiency are not relevant to scalability and we decided to leave further experiments to future works.\n\n3. **Re Why Autoregressively Diffusion (1by1) for the Key Points?:** We tried diffusion decoders in both 1-by-1 and 5-at-once styles. The results are almost the same across all metrics, as shown in the table below. Since the 1-by-1 generation provides more options for post-processing, we chose this setting as default.\n\n|      | 8sADE  | 3sFDE | 5sFDE | 8sFDE |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| STR(CKS) 16m Bwd w Diffusion (1by1) | 2.095 | 1.129 | 2.519 | 5.300 |\n| STR(CKS) 16m Bwd w Diffusion (AllAtOnce) | 2.107 | 1.140 | 2.535 | 5.327 |\n\n4. **Re Diffusion Training in 3 stages?:** There are only two stages for training the diffusion decoders. It is a common training pipeline like training text-to-image diffusion models. The diffusion decoder is trained together with the trajectory decoder in the second stage. We revise and elaborate on this detail in the new Appendix A5 section.\n\n# Comments on Minors:\n\n1. **Re Long-term reasonings in the abstract:** This is illustrated with the lower right example in Fig. 3. The vehicle changed to a left lane to prepare a left turn coming next and the turning happened beyond 8 seconds in the future. We will add more examples and discussion to the Appendix in the final version.\n2. **Re Writings in the Intro:** We rephrased the paragraph to avoid the ambiguities you pointed out.\n3. **Re 3.1 S^F:** Revised.\n4. **Re Section 4:** Revised for better clarity. See the paragraph on \u201c\u200b\u200bFuture States\u201d in the revised version.\n5. **Re Figure 2 Legends:** Legends added. Thank you.\n6. **Re Section 5.2 - Rasterize help to learn camera features?:** Revised. Changes were marked in red in Section 5.2 - Paragraph Encoder. \n7. **Re Section 5.2 - Writing issues about the Decoder paragraph:** We rewrote the whole paragraph. See the paragraph on \u201cDecoders\u201d in the revised version.\n8. **Re Training details of the diffusion decoders:** Addressed in General Comments 3.\n9. **Re Compare with MTR (not e2e) and MTR++:** The MTR (not e2e) adds NMS and other non-learning tricks to boost the performance, making it not a fair comparison. However, we are working on comparing the larger STRs with them on the test set of WOMD (on the leaderboard). The results will be included in the final version.\n\n# References:\n\n[1] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S Re- faat, Rami Al-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8579\u20138590, 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972966560,
                "cdate": 1699972966560,
                "tmdate": 1699972966560,
                "mdate": 1699972966560,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "roKSluAZ92",
                "forum": "r125wFo0L3",
                "replyto": "tKFvSlJp73",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Deadline coming. looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 6cJv, We kindly request your feedback as the rebuttal deadline is approaching in less than two days. We hope our previous feedback addresses your concern. We would like to thank you again for your time and previous review and we are looking forward to further discussions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535470621,
                "cdate": 1700535470621,
                "tmdate": 1700535470621,
                "mdate": 1700535470621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9dordk102R",
                "forum": "r125wFo0L3",
                "replyto": "roKSluAZ92",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_6cJv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5908/Reviewer_6cJv"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "- \"MotionLM also used autoregressive down-sampled (2Hz) key points for prediction but they cannot scale over 10M.\" - the MotionLM architecture is very parameter efficient, but that doesn't mean it \"cannot scale over 10M\". With more data, it would become optimal to train a larger model and that would be very doable. In fact, being more parameter efficient indicates better scalability in my mind.\n- \"scalability differences with previous non-sequence modeling models\" - isn't MotionLM also a sequence modeling model?\n- \"We did implement a mask for invalid data of the WOMD dataset.\" - the mask I'm curious about is the causal mask which would make the transformer sequential. Do the authors use a causal mask for the attention in the encoder for historical trajectories?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5908/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638185379,
                "cdate": 1700638185379,
                "tmdate": 1700638185379,
                "mdate": 1700638185379,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]