[
    {
        "title": "Conservative World Models"
    },
    {
        "review": {
            "id": "Af8IibfcUY",
            "forum": "X5qi6fnnw7",
            "replyto": "X5qi6fnnw7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5167/Reviewer_5Tki"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5167/Reviewer_5Tki"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the integration of conservative terms into the Forward-Backward (FB) loss function, aiming to mitigate the risk of overvaluing out-of-distribution state-action pairs\u2014a factor that could significantly impair FB\u2019s performance. The empirical results presented in the study effectively validate the utility of these conservative terms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The rationale behind implementing conservative terms is well-founded and convincing.\n- The experimental evidence provided clearly demonstrates the efficacy of the conservative terms.\n- A didactic example is skillfully used to elucidate the impact and functionality of the conservative terms."
                },
                "weaknesses": {
                    "value": "- The \u201cworld model\u201d as described in the paper seems to be potentially mischaracterized. The referenced document, [2209.14935.pdf (arxiv.org)](https://arxiv.org/pdf/2209.14935.pdf), elucidates that \"Both SFs and FB lie in between model-free and model-based RL, by predicting features of future states, or summarizing long-term state-state relationships. Like model-based approaches, they decouple the dynamics of the environment from the reward function. Contrary to world models, they require neither planning at test time nor a generative model of states or trajectories.\" Therefore, the paper\u2019s description might need a revision for accuracy.\n- The set of baselines employed in the experimental section appears to be inadequate. As depicted in Figure 2 of [2209.14935.pdf (arxiv.org)](https://arxiv.org/pdf/2209.14935.pdf), FB does not have the \"sufficient performance gap\" as claimed by the authors, necessitating the inclusion of a broader spectrum of baselines for a more comprehensive performance comparison.\n- The concept presented is relatively straightforward, essentially adapting the conservative term from Conservative Q-Learning (CQL) in offline reinforcement learning to the zero-shot reinforcement learning context. Given the moderate domain gap between offline RL and zero-shot RL, the idea appears to be somewhat lacking in novelty."
                },
                "questions": {
                    "value": "- Could the authors revisit and verify the usage of the term \"world model\" in the manuscript? The source [2209.14935.pdf (arxiv.org)](https://arxiv.org/pdf/2209.14935.pdf) suggests a possible misclassification of FB, and by extension, the methodology in this paper, as a \"world model\".\n- The term \"world model\" seems to have limited relevance and is infrequently used throughout the paper. Could its significance to the paper\u2019s core content be clarified, or is it a concept that could potentially be omitted without loss of clarity?\n- To bolster the robustness of the study, it would be beneficial for the authors to incorporate a wider array of zero-shot RL baselines, particularly those utilized in [2209.14935.pdf (arxiv.org)](https://arxiv.org/pdf/2209.14935.pdf). Additionally, considering the 2022 inception of the FB method, it may be pertinent to include more recent and relevant baselines in the analysis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5167/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5167/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5167/Reviewer_5Tki"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851172398,
            "cdate": 1698851172398,
            "tmdate": 1699636512107,
            "mdate": 1699636512107,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VxZaXCYApe",
                "forum": "X5qi6fnnw7",
                "replyto": "Af8IibfcUY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 5Tki"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their engagement. We note significant disagreements in our shared perceptions of the paper, and are keen to engage with the reviewer to find common ground. We respond to their queries below.\n\n**Q1: Could the authors revisit and verify the usage of the term \"world model\" in the manuscript? The source\u00a0[2209.14935.pdf (arxiv.org)](https://arxiv.org/pdf/2209.14935.pdf)\u00a0suggests a possible misclassification of FB, and by extension, the methodology in this paper, as a \"world model\"**\n\nSee global response W1.\n\n**Q2:** **The set of baselines employed in the experimental section appears to be inadequate. As depicted in Figure 2 of\u00a0[2209.14935.pdf (arxiv.org)](https://arxiv.org/pdf/2209.14935.pdf), FB does not have the \"sufficient performance gap\" as claimed by the authors, necessitating the inclusion of a broader spectrum of baselines for a more comprehensive performance comparison.**\n\nWe disagree that FB does not sufficiently outperform other SF-based, zero-shot RL methods. Touati et. al (2022) show FB achieved an aggregate score ~10% higher than the SOTA SF-based method after extensive experimental analysis (Figure 1 (an aggregation of Figure 2) and Table 2 in [1]).  FB arguably enjoys better theoretical properties as it avoids the performance ceiling induced by the representation learning methods required for SF-based methods. That said, our proposals are fully compatible with SF-based methods, and they too will suffer from the same failure mode as FB on low quality datasets (TD learning with $a_{t+1} \\sim \\pi(s_{t+1})$--see Equation 6 in [1]). As such we shall add a new appendix that derives losses for training Conservative Successor Features\u2014see Revision 5. Focussing new zero-shot RL work on FB but showing compatibility with SF in this way has been shown in other works (under review at ICLR) [2].\n\n**Q3: Additionally, considering the 2022 inception of the FB method, it may be pertinent to include more recent and relevant baselines in the analysis.**\n\nTo the best of our knowledge, we are the first work to build upon FB representations in the context of zero-shot RL. If the reviewer is aware of works we have missed we ask for them to be shared so we can include them. \n\n### References\n\n[1] Touati, Ahmed, J\u00e9r\u00e9my Rapin, and Yann Ollivier. \"Does Zero-Shot Reinforcement Learning Exist?.\"\u00a0*arXiv preprint arXiv:2209.14935*\u00a0(2022).\n\n[2] https://openreview.net/forum?id=qnWtw3l0jb"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699967980513,
                "cdate": 1699967980513,
                "tmdate": 1699967980513,
                "mdate": 1699967980513,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HigU7ZewDS",
            "forum": "X5qi6fnnw7",
            "replyto": "X5qi6fnnw7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5167/Reviewer_xRo3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5167/Reviewer_xRo3"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors propose what they call a \"conservative world model\", essentially a general model that can demonstrate zero-shot RL through offline data. The conservative mentioned in the title refers to an approach to be more conservative with the values of out-of-distribution state-action pairs, which can allow for more general policies in some cases. The authors cover their approach in detail, present two variations of their approach and compare their performance to a non-conservative baseline across four standard domains. Their results demonstrate comparable or better performance with the non-conservative baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper demonstrates a fair amount of clarity around the approach, with well-written arguments around the potential benefits. The work is original, to the best of my knowledge, in terms of its weighting approach for the training data for offline RL problems. The quality of the work in terms of the number of domains and overview of the results if good. Those interested in offline RL, and particular RB approaches, will likely find the work of some significance."
                },
                "weaknesses": {
                    "value": "This paper has a number of weaknesses. \n\nFirst, a relatively minor one is the choice of the phrase \"world model\" to describe the approach. World models already are a well-established and distinct approach in this area [1]. \n\nSecond is the relative lack of novelty. The approach is essentially a reweighing of training data, but similar reweighing strategies have already been proposed and are not compared against [2]. \n\nThird is the evaluation setup. It's odd to have two variants of the approach in comparison to a single baseline, which essentially gives the approach twice the opportunity to outperform the baseline. For simplicity, it might have been better to stick with VC-FB and leave MC-FB to the appendix. However, it's also unfair that both approaches are given 3 times the training duration to FB. Ablations or additional baselines could have helped to avoid this issue.\n\nFourth is the presentation of the results. The paper repeatedly aggregates over distinct problems, whose scores are not comparable and presents these results as summed values and percentages. I don't believe this is appropriate. The claims made are also not reflective of the results, which show an inconsistent improvement by VC-FB and MC-FB over FB. This is especially worrying in the Random case, where the approaches are essentially identical. Given the claims around the value of conservative world models, I would have assumed that they would have outperformed FB the most when the dataset was of a poorer quality. \n\n[1] Ha, David, and J\u00fcrgen Schmidhuber. \"World models.\" arXiv preprint arXiv:1803.10122 (2018).\n[2] Robine, Jan, et al. \"Transformer-based World Models Are Happy With 100k Interactions.\" arXiv preprint arXiv:2303.07109 (2023)."
                },
                "questions": {
                    "value": "1. Is there a relationship to the more typical usage of World Model that I'm missing?\n2. Are there no other appropriate baselines that could have been included in the evaluation?\n3. Why is it appropriate to aggregate the results across evaluation domains?\n4. Why would the authors' approach perform worst with worse datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5167/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5167/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5167/Reviewer_xRo3"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070516473,
            "cdate": 1699070516473,
            "tmdate": 1700535076860,
            "mdate": 1700535076860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7oTmE46HU2",
                "forum": "X5qi6fnnw7",
                "replyto": "HigU7ZewDS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer xRo3"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their engagement. We note significant disagreements in our shared perceptions of the paper, and are keen to engage with the reviewer to find common ground. We respond to their queries below.\n\n**Q1: Is there a relationship to the more typical usage of World Model that I'm missing?**\n\nSee global response W1.\n\n**Q2: Are there no other appropriate baselines that could have been included in the evaluation?**\n\nTouati et. al show FB achieved an aggregate score ~10% higher than the SOTA SF-based method after extensive experimental analysis (Figure 1 and Table 2 in [1]). Indeed, FB arguably enjoys better theoretical properties as it avoids the performance ceiling induced by the representation learning methods required for SF-based methods. For these reasons we believe FB is the SOTA zero-shot RL method and use it as our sole baseline. Are there specific zero-shot RL baselines the reviewer would like us to include? \n\n**Q3: Second is the relative lack of novelty. The approach is essentially a reweighing of training data, but similar reweighing strategies have already been proposed and are not compared against [2].**\n\nWe disagree that our approach is reweighting of the training data and therefore lacks novelty. We believe the reviewer has misunderstood our proposals; we are not reweighting the training data, we are (in the case of VC-FB) regularising Q functions to mitigate OOD value overestimation\u2014data is sampled uniformly from dataset $\\mathcal{D}$. The method proposed in the cited paper is not a zero-shot RL method (the policy cannot generalise to arbitrary tasks in an environment with zero downstream planning or learning) and so we do not believe it would constitute a valid baseline.\n\n**Q4: It's odd to have two variants of the approach in comparison to a single baseline, which essentially gives the approach twice the opportunity to outperform the baseline. For simplicity, it might have been better to stick with VC-FB and leave MC-FB to the appendix. However, it's also unfair that both approaches are given 3 times the training duration to FB. Ablations or additional baselines could have helped to avoid this issue.**\n\nFirstly, we note that we compare against 3 baselines not 1\u2014see Table 5 Appendix C. Secondly, we proposed two variants of our method because it was not clear _a priori_ which method of $z$ sampling would prove beneficial. We do not think this constitutes twice the opportunity to outperform our baselines. As it transpired, both methods outperformed all of our baselines in aggregate on their own merit. Thirdly, and finally, we hold the number of update steps and training data constant for all algorithms which is standard practice. The reason training our proposals takes 3x longer than existing methods is not because the methods were exposed to more data or learning steps, but rather because the $Q$-function regularisation invokes expensive operations. We discuss this in Section 5, and suggest this could be resolved with thoughtful engineering of the codebase. Doing so was left for future work.   \n\n**Q4: Why is it appropriate to aggregate the results across evaluation domains?**\n\nDeepMind Control domains are specifically designed such that task scores always lie in $[0, 1000]$ and so are directly comparable. Aggregating across these domains is standard practice in the community, we refer the reviewer to e.g. [1, 2] for examples. We aggregate in the same way as the work we build upon [3].\n\n**Q5: Why would the authors' approach perform worst with worse datasets?**\n\nSee global response W2.\n\n### References\n\n[1] Fujimoto, Scott, and Shixiang Shane Gu. \"A minimalist approach to offline reinforcement learning.\"\u00a0*Advances in neural information processing systems*\u00a034 (2021): 20132-20145.\n\n[2] Kostrikov, Ilya, Ashvin Nair, and Sergey Levine. \"Offline reinforcement learning with implicit q-learning.\"\u00a0*arXiv preprint arXiv:2110.06169*\u00a0(2021).\n\n[3] Touati, Ahmed, J\u00e9r\u00e9my Rapin, and Yann Ollivier. \"Does Zero-Shot Reinforcement Learning Exist?.\"\u00a0*arXiv preprint arXiv:2209.14935*\u00a0(2022).\n\n[4] https://openreview.net/forum?id=qnWtw3l0jb"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969048912,
                "cdate": 1699969048912,
                "tmdate": 1699969048912,
                "mdate": 1699969048912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "70ie3bugII",
                "forum": "X5qi6fnnw7",
                "replyto": "7oTmE46HU2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_xRo3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_xRo3"
                ],
                "content": {
                    "title": {
                        "value": "Re: Author Response to Reviewer xRo3"
                    },
                    "comment": {
                        "value": "For Q1, I would encourage the authors to change the term as the community does appear to be consolidating around world models as meaning a distinct modelling problem.\n\nFor Q2, looking at Touati et. al's work, the offline variant of their approach was outperformed for quadruped_stand with comparable performance in several other environments. As such, it seems fair to speculate that these baselines would not necessarily be strictly dominated.\n\nFor Q3, I apologize for mischaracterizing the work and appreciate the clarification! Given Reviewer 5Tki's concerns I am still concerned about the novelty, however.\n\nFor Q4a, the inclusion of two variants of the approach, without any a priori knowledge, would still give 2-1 odds of outperforming a baseline. I apologize for missing the other baselines from the supplementary materials!\n\nFor Q4b, I apologize, I was simply unfamiliar with this being a standard practice.\n\nFor Q5, I appreciate the speculation but I am still concerned by these results. Without additional results it becomes difficult to determine the cause here.\n\nAs I still have concerns around the novelty and value (via results) of the work, I am not changing my score at this time."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699983720142,
                "cdate": 1699983720142,
                "tmdate": 1699983720142,
                "mdate": 1699983720142,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8EbxmDYYQ8",
                "forum": "X5qi6fnnw7",
                "replyto": "vus8zmxF7z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_xRo3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_xRo3"
                ],
                "content": {
                    "title": {
                        "value": "Re: Comment to Reviewer xRo3 after Revision One"
                    },
                    "comment": {
                        "value": "Thanks for the further updates! My concerns in terms of baselines (Q2) and the interpretation of the results (Q5) have been addressed. I am still concerned with the novelty issue brought up by 5Tki \"The concept presented is relatively straightforward, essentially adapting the conservative term from Conservative Q-Learning (CQL) in offline reinforcement learning to the zero-shot reinforcement learning context. Given the moderate domain gap between offline RL and zero-shot RL, the idea appears to be somewhat lacking in novelty.\"\n\nI have raised my rating, but would appreciate the author's response to the above quote from 5Tki."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535054962,
                "cdate": 1700535054962,
                "tmdate": 1700535054962,
                "mdate": 1700535054962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AD8pyEFmxO",
            "forum": "X5qi6fnnw7",
            "replyto": "X5qi6fnnw7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5167/Reviewer_85iy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5167/Reviewer_85iy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes two novel reinforcement learning methodologies for learning arbitrary tasks from unsupervised transition data. The key is that the datasets are not as large and diverse as in previous work. The goal is to pre-train a model on small transition datasets and at test time, zero-shot generalize to arbitrary reward functions that are characterized by some distribution. The work builds on the idea of forward-backward learning in which successor information is employed to make statements about past and future visitation counts of a policy. These counts are then related to reward information during test time. The proposed algorithms use conservativeness to combat large Q-value predictions in unseen state-action areas which is an idea that has previously been used successfully in standard offline RL. The work validates the functionality of the method with an intuitive toy example and then goes on to evaluate on larger benchmark suites where the proposed methods outperform baselines on several tasks.\n\nI would like to mention that while I am aware of the offline RL literature, I am not familiar with the forward-backward literature specifically. I did not check the math in section $2$ for correctness since it is already published work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Motivation\n* The motivation of the work is clearly established. The ability to extract dynamics information that can be leveraged for arbitrary downstream tasks is a promising direction towards building general-purpose representations. The reasoning for the employment of forward-backward methods is clearly established.\n\nContextualization with prior work\n* Specifically, the first few sections do a good job of making sure that prior work is mentioned and highlight clearly which parts of the manuscript are novel and which ideas are taken from previous work.\n\nStructural clarity\n* The paper has a very clear structure and is easy to follow. The origins of the method are clear and the toy example helps understand the internal mechanisms of the network. The language is clear and the manuscript is well-written.\n\nLimitations section\n* The work provides a good limitations section that highlights some of the practical challenges and outlines future directions of work. In general, I think this section is very beneficial for readers of the paper and I think it is a nice addition.\n\nExperimental Evaluation\n* The experimental section is structured well and asks $3$ very relevant questions to analyze the proposed algorithms. Specifically, experiment Q3 in the paper shows that in most cases it is likely not bad to simply add conservativeness to the existing approach as long as the data quality is high is convincing. \n* The benchmark suite is sufficiently sized and provides several different types of environments.\n\nNovelty\n* As mentioned before, I\u2019m not familiar with this specific type of model but given the recency of publication of prior methods, it seems reasonable to assume that the contribution is sufficiently novel. I also think the contribution adds sufficient new content to the existing approach and highlights the relationship between conservative offline RL approaches and how the procedures transfer to forward-backward methods."
                },
                "weaknesses": {
                    "value": "Mathematical clarity\n* Specifically section 2 might benefit from clarity improvements and re-ordering of some of the references. \n  * The relationship between the task structure of the set of MDPs and the reward functions is unclear. The MDP framework outlined in section $2$ paragraph $1$ does not come with such structure and the structure is only mentioned but not well-defined in the problem formulation section. As a result, it is not clear to me where the differences in the sets of tasks come from (see Q1). It might be beneficial to mention the distribution of task vectors earlier and how it relates to reward functions in paragraph $2$ (Problem formulation.).\n  * Relatedly, should the forward model function be defined as $F: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{Z} \\mapsto \\mathbb{R}^d$? Do the tasks and the policies share the same space? Is the embedding space always the size of the task vector space? In this context, it is not clear what the following notation means: $\\pi_z : \\mathcal{z} \u223c \\mathcal{z}$.\n  * I was looking for a reference for the derivation of some of the claims in section $2$ and that reference is only provided at the end of the section. I personally would have benefitted from this being at the beginning. This would make it both clear that the derivation comes from previous work and possible for the reader to open up the reference on the side before I dive into the math.\n  * The actor-critic formulation is only mentioned very briefly at the end of the section but as far as I understand it might be quite crucial to running the experiments and deriving the actual method.\n\nContextualization with prior work\n* There is an abundance of literature on conservativeness in RL and the paragraph on this in the related works section contains a total of 7 references. This contextualization could be a little stronger. The downside of this lack of contextualization shows somewhat in the experiments as I will outline later. Less detail about the cited methods and concise statements about previous literature\u2019s commonalities might provide a way to condense the text in this section.\n\nExperimental Evaluation\n* The evaluation metrics are not fully specified. The text mostly talks about performance but it is not clear what is being measured. I\u2019m going to assume that the experiments measure performance in terms of cumulative reward. Relatedly, in Figure 4, It is not clear to me what a Task Score x is.\n* The following statement is probably not well-qualified. \u201cCQL is representative of what a conservative algorithm can achieve when optimizing for one task in a domain rather than all tasks\u201d. CQL was an impressive step towards using offline data in RL but since then many more conservative methods have been established that show significantly stronger results but even more importantly are easier to train and more stable. This is one of the weaknesses of CQL (and the proposed method) that is highlighted in section 5 and I strongly agree. As a result, CQL might not be the best baseline to establish that the method performs as well as common offline RL methods. It might make sense to compare to an offline method that provides better stability. A common choice would probably be Implicit Q-Learning. That being said, CQL is a reasonable choice in the sense that it tries to achieve a similar objective as the proposed method and as such provides a comparison against a non-forward-backward approach with similar mechanistics. Still, there might be better baselines to strengthen the claim that the method competes with sota offline RL approaches.\n* Some of the claims may be overstated and the text could be a little more detailed on the actual findings. \n  * While I think the idea of reporting percentage improvements can be nice to establish a clear performance difference, this difference is only really strong if the baseline performance is already good. I could, for instance, say that the method performs $1000%$ better on the Jaco RND experiment than the non-conservative baseline. However, this would be rather misleading because neither method might be close to solving the task. Depending on how these measures are now aggregated, the performance of the method might look inflated. I think a clear definition of what\u2019s being measured and how the measures are aggregated would be useful to provide context for the numbers and a more detailed description of when the method works well.\n  * The claim that conservativeness is not harmful is only supported on what is referred to as a high-quality dataset. In this case, the dataset is already providing decent coverage and the Q-values should be easy to approximate (see Q3). \n  * The difference between VC-FB and MC-FB is highlighted in section $3$ but the effects of the differences are not explicitly analyzed in the experiments.\n\nMinor textual clarity suggestions\n* Figure 1\u2019s caption could probably mention earlier that this is not simply an illustration but the data is from an actual experiment.\n* Equation $(1)$ is currently not an expectation without assumptions on the distribution.\n* The statement \u201cSince the successor measure satisfies a Bellman equation\u201d would benefit from a citation.\n* \u201c$s_+$ is sampled independently from $(s_t, a_t, s_{t+1})$\u201d, the latter is not a distribution but a tuple, this should probably be the dataset?\n\nOverall, I believe that the paper establishes that there are benefits to applying conservative concepts to the forward-backward style method that is employed. The methods seem to be easy enough to implement, in general not to do worse on the proposed baselines and improve upon the baseline in several instances. However, I do think section $2$ would benefit from a few clarity improvements and that the reporting and textual analysis of the experiments could be a little more detailed. I\u2019d be happy to raise my score if these concerns are addressed.  Additionally, experiments with a stronger offline RL baseline might make a stronger case but are not necessarily required for me to raise my score."
                },
                "questions": {
                    "value": "Q1: How is the task distribution $\\mathcal{Z}$ defined in practice? It is not quite clear to me, for instance, how I would define such an abstract distribution for either of the given environments in section $5$ other than by possibly defining a distribution over final states which would have to be as large as the state space? Can you give an example of what this would look like in, e.g. the Cheetah environment? \n\nQ2: Traditional CQL samples the data it\u2019s minimizing its Q-values over from the policy distribution rather than employing a max operator. What is the reasoning for choosing a max operator here rather than just sampling from the distribution? The latter seems to be easier to implement.\n\nQ3: What do we expect to happen when we run the conservative version of the proposed algorithm on full datasets with poor quality? Are the trends similar to what we see in the small-scale experiments?\n\nQ4: Is it possible to deduce up-front when this approach performs as well as or better than its single-task offline RL counterparts? In other words, what types of environment properties make the method work?\n\nQ5: It is a little counterintuitive to me that conservativeness seems to work better when the datasets are of higher quality rather than when datasets provide poor support. Do you have any idea why that is?\n\nQ6: (Feel free not to answer since this is a question about previous work rather than your work really.) Are $F$ and $B$ learned solely through equation (4) in the original FB approach? How can we be sure they are actual distribution summaries then?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5167/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5167/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5167/Reviewer_85iy"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699393697228,
            "cdate": 1699393697228,
            "tmdate": 1699636511894,
            "mdate": 1699636511894,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "luQh6GpVpo",
                "forum": "X5qi6fnnw7",
                "replyto": "AD8pyEFmxO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 85iy (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for engaging with our manuscript and providing useful feedback. We summarise your comments/questions and respond to them below.\n\n**Q1: How is the task distribution $Z$ defined in practice? It is not quite clear to me, for instance, how I would define such an abstract distribution for either of the given environments in section 5 other than by possibly defining a distribution over final states which would have to be as large as the state space? Can you give an example of what this would look like in, e.g. the Cheetah environment?**\n\nWe discuss the formulation of $\\mathcal{Z}$ in Appendix B.1.2, but note, and apologise, that a reference to this appendix in the main body was missed. We address will this in Revision 4.3. We believe further misunderstanding was likely caused by our presentation of the FB theory which we address in Revision 2.\n\n**Q2: Traditional CQL samples the data it\u2019s minimizing its Q-values over from the policy distribution rather than employing a max operator. What is the reasoning for choosing a max operator here rather than just sampling from the distribution? The latter seems to be easier to implement.**\n\nTraditional CQL does indeed employ a max operator over the current Q iterate in theory (see red text in Equation 3 in [1]), which is then approximated with samples from both the policy distribution conditioned on the current observation, and a uniform distribution (Appendix F in [1]). The official CQL implementation also samples from the policy distribution conditioned on the next observation [2], though this is not discussed in their paper. Our method for approximating the max operation outlined in Appendix B.1.3 is fully consistent with this framework.\n\n**Q3: What do we expect to happen when we run the conservative version of the proposed algorithm on full datasets with poor quality? Are the trends similar to what we see in the small-scale experiments?**\n\nThis is an interesting question which we are exploring with further experiments. See Revision 1.\n\n**Q4: Is it possible to deduce up-front when this approach performs as well as or better than its single-task offline RL counterparts? In other words, what types of environment properties make the method work?**\n\nThis is an important question that we had hoped Figure 5 could help answer, but we struggled make concrete inferences, and therefore only provide speculative responses here. It appears that zero-shot methods are more likely to outperform the single-task baselines on locomotion tasks (VC-FB > CQL on 4/6 Walker/Quadruped datasets) than on goal-reaching tasks (VC-FB > CQL on 1/6 Maze/Jaco datasets). It seems likely that the absolute performance of the zero-shot methods on evaluation tasks correlates with how likely they were to be sampled from $\\mathcal{Z}$ during training. Perhaps the current instantiation of $\\mathcal{Z}$ is a better approximation of the locomotion task space. We attempted make t-SNE visualisations of $z$-space to explore this, but found it difficult to draw firm conclusions ($z$-space is 50-dimensional). Further investigation of the make-up of z-space as discussed in Section 5 and should help us get a better grip on this. This is difficult, but important future work. We shall add text to this end\u2014see Revision 4.8.\n\n**Q5: It is a little counterintuitive to me that conservativeness seems to work better when the datasets are of higher quality rather than when datasets provide poor support. Do you have any idea why that is?**\n\nAs mentioned in W2 of the global response, we suspect that the combination of low diversity and small absolute size of the Random dataset makes model/policy inference particularly tricky. We hope the proposed experiments on the full Random and DIAYN datasets (Revision 1) will shed light on this hypothesis.\n\n**Q6: Contextualization with prior work: There is an abundance of literature on conservativeness in RL and the paragraph on this in the related works section contains a total of 7 references. This contextualization could be a little stronger. The downside of this lack of contextualization shows somewhat in the experiments as I will outline later. Less detail about the cited methods and concise statements about previous literature\u2019s commonalities might provide a way to condense the text in this section.**\n\nWe agree that the related work section on Offline RL could provide more detail, and as a result will add several more references. Namely: IQL [3], PEVI, [4], COMBO [5] and [6]"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699966656872,
                "cdate": 1699966656872,
                "tmdate": 1699966656872,
                "mdate": 1699966656872,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dKmcGxsNaW",
                "forum": "X5qi6fnnw7",
                "replyto": "Win0QCBM98",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_85iy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_85iy"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the clarifications. Thank you.\n\n**Wrt. Q2**: I should have been more specific. CQL does not employ a max operator over actions but over the regularization distribution. Actions are then sampled from this regularization distribution (see Eq 3 in [1] as you pointed out). When the max operator over actions $a$ is selected, a specific instantiation of the distribution $\\mu$ is chosen. I was wondering why that is and how this max over a continuous distribution is computed. This choice intuitively seems unstable but I could be wrong. However, the referenced as well as the submitted code bases seem to be doing something different than the equations in the text since they are actually sampling from the Actor. It might make sense to be more precise here to avoid future confusion.\n\n**Wrt to title choice**: I agree with other reviewers that a world model in the context of RL can be seen as something different. The early model-based deep RL literature will refer to a world-model as a model that given some state and action does unsupervised training to obtain information about the world. More often than not it will refer to a model that can predict a future state and reward.  However, it seems that recently this term has been used more loosely. I did find that title more appropriate than foundation model. A foundation model nowadays is most likely going to refer to a large-scale generative model trained on vast amounts of unsupervised data.\n\nIt seems that there is in fact a form of unsupervised representation learning over states happening in the code. While not exactly what previous literature would expect, it's certainly closer to a world than foundation model in my opinion. However, this loss is not really mentioned in section 2 which may have led to confusion. I think a revision of section 2 could make this more clear because the key unsupervised term that is used in the code but never explained in the text could be added. \n\nPersonally, I think both titles are ambiguous and don't convey much information but I also don't mind too much. Yet, it is not quite clear to as to why they were chosen and my feedback is that I expected something different when I read the title of this paper. Similarly, I would expect something *very* different if I were to read the words \"foundation model\" in the title."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326621193,
                "cdate": 1700326621193,
                "tmdate": 1700326621193,
                "mdate": 1700326621193,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I9uu0ecKw8",
                "forum": "X5qi6fnnw7",
                "replyto": "AD8pyEFmxO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_85iy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_85iy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you again for the clarifications, I went through the revision of the paper, read other reviewers concerns and comments as well as the corresponding responses.\n\nWrt. my initial review the following points have been resolved.\n* The concerns for mathematical clarity have mostly been alleviated. I still think $F$ should probably be defined differently but that's just nitpicking at this point. Section 2 is much clearer now.\n* The contextualization with prior work is much stronger than before. I appreciate this.\n* Clarifications on experimental settings and claims are implemented.\n* \"conservativeness is not harmful\" - claim has been strengthened\n\nThe following points remain open\n* The introduction of two methods likely warrants an in-depth comparison between the two to explain their behavior. This seems to be a shared perception across multiple reviewers. I appreciate the conjecture in section 5 and think it is promising. As one of the responses mentions, this seems to be a testable hypothesis and I believe that an experiment verifying this should probably appear in a manuscript like the one presented.\n* I'm still confused about how a task is presented to the agent. I'm guessing a task being revealed to the agent (see section 2) means it is presented with a $z$ variable. It is still not clear to me, what exactly $z$ represents in the current tasks concretely or how this task is presented to the agent at test time. My question about an example was not addressed. I appreciate the clarification on sampling $z$ in the model space (i.e. the training procedure) in Appendix B.1.2. However, my confusion was mostly focused on the environment space during execution. This might help characterize how different objectives are from the data support.\n* It still stands that the method seems to have statistically significant improvements on only 3/12 tasks which is not conveyed adequately. There are actually cases where regularization decreases performance even if in the limit of data on average it does not.\n\nMinor formatting point:\n* Equation (9) goes into the margin\n\nI think this paper has improved on various fronts but still has multiple areas of improvement in the experimental section. Specifically,  \na) characterizing how a task is presented to the agent and this relates to data coverage as well as  \nb) characterizing when which method is useful   \nwill help bolster the work and should probably be included in a publication like this. As such, I will retain my score for now."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668996323,
                "cdate": 1700668996323,
                "tmdate": 1700669011449,
                "mdate": 1700669011449,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oICRijCKSo",
                "forum": "X5qi6fnnw7",
                "replyto": "AD8pyEFmxO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment to Reviewer 85iy after Final Revision"
                    },
                    "comment": {
                        "value": "Thanks again for engaging candidly with our work. We have now uploaded our final revision which we believe addresses your two final queries.\n\na) We\u2019ve amended Section 2 again to explain exactly how tasks are inferred at test time and passed to the actor. We hope this is clear now. To answer your cheetah question directly, FB would expect to be provided a dataset of reward-labelled states at test time from the cheetah environment, where the rewards characterise the test task. Then, leveraging FB\u2019s property that $z := \\mathbb{E}_{(s, r) \\sim \\rho}[\\mathcal{r} B(s)]$, we can infer the task using samples from the reward-labelled dataset. This task vector is then passed to the task-conditioned actor model.\n\nb) The most recent experiments carried out for 2wo9 provide better evidence of our claims in Section 5 that MC-FB performs worse than VC-FB because of poorer task coverage. (This was the testable hypothesis you referred to from Q5 in our original response to 2wo9). We lay out the new experiment and results at the start of Section 5.\n\nWe hope you feel happy to update your score as a consequence. Thanks again for your feedback!"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692083842,
                "cdate": 1700692083842,
                "tmdate": 1700692149442,
                "mdate": 1700692149442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U775bhV8Px",
            "forum": "X5qi6fnnw7",
            "replyto": "X5qi6fnnw7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5167/Reviewer_2wo9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5167/Reviewer_2wo9"
            ],
            "content": {
                "summary": {
                    "value": "The paper concerns itself with the OOD issue in zero-shot offline RL. It primarily makes CQL-like modifications to the objective for learning Forwad-Backward representations. From this arise two variants, the value-conservative variant which samples task representations uniformly and the measure conservative variant which uses the backward representation $B(s^+)$. Thorough experiments have been done on the ExORL benchmark where the method is compared to single-task RL (TD3, CQL) and non-conservative FB. It performs favorably in comparison to the baselines, sometimes even outperforming the single-task algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Addressing the issue of OOD actions arising  through maximization in offline RL is important, also in the zero-shot or multi-task case such as the case for FB.\n\nGood experimental evaluation has been conducted.\n\nI mostly didn't have trouble reading the paper, I think it's well-written.\n\nThe method is simple (application of CQL to FB) and therefore easy to understand."
                },
                "weaknesses": {
                    "value": "Some parts of the theory are not clear, or maybe there were typos in the equations. The most essential thing for me is to clarify the treatment of the task representation vectors in  the VC and MC case and how do they connect to the original FB definition (see questions).\n\nWhy does VC outperform MC sometimes has been clarified to a certain extent in the discussion, however it is only intuition and no evidence for the claims have been given."
                },
                "questions": {
                    "value": "Would be useful if you would properly define the abbreviations for VC-FB and MC-FB in the text before using them.\n\nIn equation 5  the $z$  variable is not an argument to the Q function, as it should be?\n\nBroken english in line before equation 6. What is the connection between (5) and (6)? In (6) the forward representaiton here is a function of the same $z$, i.e. $\\langle F(.., z), z \\rangle$, however this is not how the Q value is defined in (5). Later you sample z's independently from $s^+$, hence the backward representation is completely independent of the task $z$ in the MC-FB. How is this valid? My questions here are:\n\n* In which cases does the $z$ correspond to the backward representation?\n* Follow up to the previous question, why does you MC-FB version  treat the backward representation and $z$ independently, while the VC-FB version does not, shouldn't the z in the F(...) be essentially the output of $B(s^+)$ as per your description?\n\nThe argument for the introduction of the MC-FB is  that it might not make sense to sample task vectors uniformly, but to focus on the ones that we care about (via backward rep), yet in figure 4 the zero shot performance of the MC variant is lower than VC? Can you clarify this? Also in the paper. (I realize that you have this in the discussion, however I think that it should be commented on earlier).\n\nin 4.3, what does \"stochastically dominate\" mean?\n\nCan you explain the failure cases of MC-FB in comparison to VC in figure 5?  For the Walker2D environment, the MC variant completely fails for the RND dataset - I suspect that the reason is low task coverage? Also, in some cases  there is a big gap between CQL and your method (random jaco).\n\n\nIf you address these concerns I will raise the score appropriately (the most important concern is the one about the dot-product)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5167/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699523159810,
            "cdate": 1699523159810,
            "tmdate": 1699636511800,
            "mdate": 1699636511800,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s2jaFJK1hc",
                "forum": "X5qi6fnnw7",
                "replyto": "U775bhV8Px",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 2wo9"
                    },
                    "comment": {
                        "value": "We thank the reviewer for engaging with our manuscript and providing useful feedback. We summarise your comments/questions and respond to them below.\n\n**Q1: Would be useful if you would properly define the abbreviations for VC-FB and MC-FB in the text before using them.**\n\nAgreed. See Revision 4.1.\n\n**Q2: In equation 5 the\u00a0$z$\u00a0variable is not an argument to the Q function, as it should be?**\n\nIt is implicit via the superscript $\\pi_z$, we took this notation directly from Touati et al (2022) but agree it could be clearer. We hope the rewrite of Section 2 proposed in Revision 2 will better clarify notation.\n\n**Q3: In which cases does the $z$\u00a0correspond to the backward representation?**\n\nAs above, this question is valid, and a consequence of ambiguity in our introduction of the FB background theory. We hope to provide much better clarity via Revision 2. \n\n**Q4: Why don\u2019t we use B(s_+) for z inside the forward model of MC-FB?**\n\nFB\u2019s approximation of the successor measure **$M^{\\pi_z}(s_0, a_0, s_+) \\approx F(s_0, a_0, z)^\\top B(s_+)$** is the cumulative time spent in future state $s_+$ starting in state $s_0$, taking action $a_0$ and attempting to solve task $z$. Our goal was to mitigate OOD successor measure overestimation *for all tasks* and so sample $z \\sim \\mathcal{Z}$ for input to the forward model. Our language justifying MC-FB was therefore imprecise, particularly the phrase: *\u201cInstead, it may prove better to direct updates towards tasks we are likely to encounter*\u201d (Section 3, paragraph 4). We agree with the reviewer that the best way to direct updates would be to use $z = B(s_+)$ as input to the forward model and, although not mentioned by the reviewer, as inputs to the Q functions for policy training (Equation 7). We believe it would no longer make sense to call such a variant _measure-conservative_, but rather _directed-value-conservative_, with all $z$s _directed_ by the backward embedding rather than sampled uniformly. It is straightforward to explore the usefulness of such a change, and we are running experiments on all datasets and domains to do so\u2014see Revision 3. We will update the language used to justify MC-FB\u2014see Revision 4.9.\n\n**Q5: (Our summary) Can you explain the poorer performance of MC-FB w.r.t VC-FB in aggregate, on walker RND, and on random jaco?**\n\nWe suspect instances where MC-FB performance < VC-FB are explained by $z = B(s_+)$ providing worse coverage than $z \\sim \\mathcal{Z}$. This is a testable hypothesis that our evaluation of *directed* VC-FB (Revision 3) should help answer. If $z = B(s_+)$ does indeed provide poorer task coverage on some dataset/domain pairs, then _directed_ VC-FB should perform worse than MC-FB and VC-FB on those dataset/domain pairs.\n\n**Q6: In 4.3, what does \"stochastically dominate\" mean?**\n\nWe take this term directly from Agarwal et. al (2021) where performance profiles are introduced. Formally, a random variable $M$ stochastically dominates $N$ if $P(M > x) \\geq P(N > x) \\; \\forall x$. In practice the y-axis values of one algorithm\u2019s performance profile are higher than another for all x-axis values. We will add a footnote on this, see Revision 4.2."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699966150294,
                "cdate": 1699966150294,
                "tmdate": 1699966150294,
                "mdate": 1699966150294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PhwRKEUQe3",
                "forum": "X5qi6fnnw7",
                "replyto": "kL70IcqHrw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_2wo9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5167/Reviewer_2wo9"
                ],
                "content": {
                    "title": {
                        "value": "Read your response"
                    },
                    "comment": {
                        "value": "Thank you for acknowledging my points, looking forward to see the results of the runs."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5167/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582448104,
                "cdate": 1700582448104,
                "tmdate": 1700582448104,
                "mdate": 1700582448104,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]