[
    {
        "title": "A Study of Bayesian Neural Network Surrogates for Bayesian Optimization"
    },
    {
        "review": {
            "id": "aXRIZK8grf",
            "forum": "SA19ijj44B",
            "replyto": "SA19ijj44B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4401/Reviewer_v5S3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4401/Reviewer_v5S3"
            ],
            "content": {
                "summary": {
                    "value": "This paper creates a study about the performance of Bayesian surrogates inside the Bayesian optimization framework. They performed experiments on real and synthetic datasets in order to understand the performance of surrogates. They report some findings such as the ranking of methods is highly problem dependent, HMC is the most successful approximate inference procedure for fully stochastic BNNs, and that infinite-width BNNs are promising."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The topic is interesting and somewhat important for the community.\n- The presentation of theory and the experiments is well presented and sound."
                },
                "weaknesses": {
                    "value": "- There is no discussion on the interaction of the surrogate and the acquisition function.\n- I agree with the authors that the time might not be relevant when the function evaluations are expensive, still it is important to create an experiment assuming fast function evaluations and see whether the ranking holds\n- Although the dataset collection is diverse, the study is performed on a very small amount of datasets. There is no guarantee that these findings extrapolate easily to new datasets.\n- Although the insights are interesting (some of them are not surprising), the effective impact is questionable. How could these insights lead to SotA in drug discovery algorithms, active learning, material science or hyperparameter optimization?\n- No discussion on the regularization effect on the surrogates. What happens if I regularize the DKL or the Bayesian neural networks via some useful prior?"
                },
                "questions": {
                    "value": "- How would more recent methods rank in the comparison, such as PFN4BO [1]?\n- How is the performance of DKL affected by regularization approaches  such as [2], [3]?\n\n\n[1] M\u00fcller, Samuel, et al. *PFNs Are Flexible Models for Real-World Bayesian Optimization.*\n\n[2] Lotfi, S., Izmailov, P., Benton, G., Goldblum, M., & Wilson, A. G. *Bayesian model selection, the marginal likelihood, and generalization.*\n\n[3] Patacchiola, M., Turner, J., Crowley, E. J., O'Boyle, M., & Storkey, A. J. *Bayesian meta-learning for the few-shot setting via deep kernels.*"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4401/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4401/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4401/Reviewer_v5S3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697983068154,
            "cdate": 1697983068154,
            "tmdate": 1700857374829,
            "mdate": 1700857374829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FK224TbQDo",
                "forum": "SA19ijj44B",
                "replyto": "aXRIZK8grf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer v5S3 [1/2]"
                    },
                    "comment": {
                        "value": "We appreciate your review, and we respond to your questions as follows.\n\n> Importance of insights\n\nThe choice of surrogate model in Bayesian optimization is a fundamental design decision that has been largely overlooked in the literature, and many papers default to using standard Gaussian process surrogates (e.g., with Matern or RBF kernels). Our paper is the first to provide a comprehensive study of BNN surrogates, and the wide range of neural network-based surrogates explored in our submission allows us to evaluate the role of representation learning, non-stationarity, and stochasticity in modeling Bayesian optimization objectives. These findings offer pragmatic insights for BO practitioners by providing guidance on selecting appropriate surrogate models.\n\nOur paper also has many novel and important findings, in some cases even prompting us to re-think conventional wisdom in BO. Surprising and unexpected findings include the following: (1) There are limited empirical benefits of stochasticity in the surrogate, given the competitive performance of DKL to BNN models; (2) the performance of deep ensemble surrogates is surprisingly poor, despite their success in other non-BO contexts; (3) I-BNN demonstrate compelling performance on high dimensional benchmarks; (4) Different BO objectives exhibit highly different salient structures (e.g., in contrast to many vision and NLP problems). We additionally note that we are, to the best of our knowledge, the first to consider infinite BNN and HMC surrogates, and I-BNNs have strikingly good results in the high dimensional settings. While not every result would be expected to be surprising, many of the results in our submission are both surprising and informative.\n\nIn this paper, we do not try to develop new methods for specific Bayesian optimization applications and note that in order to reach state-of-the-art performance in drug discovery, materials engineering, or some applied problem of that nature, the paper would need to be entirely about that single applied problem and the idiosyncrasies associated with it. Our focus is instead on a general foundational scientific study on the effectiveness of BNN surrogate models on a wide range of problems, covering a wide array of standard benchmarks, with various different properties. We have further outlined the contributions of the paper in the separate general post.\n\n> Number of Datasets\n\nWe respectfully disagree that we consider a \"very small number of datasets\". We benchmarked our results on 15 different datasets, which is more than many other related Bayesian optimization papers:\n- 8 datasets: Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization, Kristiadi et al. 2023\n- 5 datasets: Scalable Global Optimization via Local Bayesian Optimization, Eriksson et al. 2019\n- 8 datasets: Scalable Bayesian Optimization Using Deep Neural Networks, Snoek et al. 2015\n- 3 datasets: Maximizing Acquisition Functions for Bayesian Optimization, Wilson et al. 2018\n- 9 datasets: Bayesian Optimization with High-Dimensional Outputs, Maddox et al. 2021\n- 11 datasets: Bayesian Optimization with Robust Bayesian Neural Networks, Stefan et al. 2016\n\n> What happens if I regularize the DKL?\n\nWe explored this question in the submitted manuscript. In Appendix D.7, we include results for DKL as suggested in [2], showing the performance of optimizing the parameters with the marginal likelihood (ML) compared to the conditional marginal likelihood (CML). We see that there is no clear preference for using the ML or the CML, and the behavior of DKL with ML and CML is very similar across many of the objectives.\n\n> Runtime of Bayesian Optimization\n\nWe provide wall-clock times of the surrogate models across our experiments in Appendix D.11. However, we would like to emphasize that Bayesian optimization applications often include expensive computation querying of the objective function, which may include actions such as synthesizing a new material, training a large neural network to convergence, etc. For these scenarios, the quality of the surrogate model uncertainties may be much more important than the cost of inference.\nThat said, all of our runtimes are available, and we are not advocating for one method over another in general. Practitioners can use the runtimes we provide as a guide in making a decision for what matters most to them in the context of their problem. We also note that I-BNNs, a novel surrogate model in our paper, has both relatively fast runtime, and strong results.\n\nEdit: See new comment below! We have provided a new experiment in Appendix D.11 to address this point."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510172846,
                "cdate": 1700510172846,
                "tmdate": 1700584184779,
                "mdate": 1700584184779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GmYdj0jC2g",
                "forum": "SA19ijj44B",
                "replyto": "aXRIZK8grf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer v5S3 [2/2]"
                    },
                    "comment": {
                        "value": "> Extensive Experiments\n\nOur paper includes a diverse set of experiments showing how many different aspects of Bayesian optimization interact with our results. For each of our surrogate models, we include experiments in the appendix where we vary their hyperparameters, such as the number of models in a deep ensemble or the kernel for a GP, and measure their impact on performance. Additionally, we also include experiments showing the impact of different BNN architectures, and we also show the effects of the batch size of the acquisition function. While it is always possible to request additional experiments, we believe that we have done an exhaustive and extensive exploration of BNNs for Bayesian optimization.\n\n\n> How would recent methods such as PFNs4BO rank?\n\nThank you for the reference! Our paper focuses on the performance of Bayesian neural networks trained from scratch, and consequently, we find methods reliant on pre-training to be outside the scope of our work. The inclusion of methods not trained from scratch makes an apples-to-apples comparison especially difficult, since many of the other neural net surrogates could also be pre-trained in some way. However, we are excited about methods that leverage auxiliary data, which we believe may be a notable advantage for using neural network surrogates for Bayesian optimization. We have included a reference to PFNs4BO in the updated manuscript and included it in our discussions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510201558,
                "cdate": 1700510201558,
                "tmdate": 1700510210195,
                "mdate": 1700510210195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PRI8WxeNR6",
                "forum": "SA19ijj44B",
                "replyto": "aXRIZK8grf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer v5S3, Runtime Experiment"
                    },
                    "comment": {
                        "value": "In response to your comment on runtime, we have now updated Appendix D.11 with a new experiment comparing the performance of the surrogate models within a time budget, under the assumption of very fast function queries. We find that in this setting, GPs and I-BNNs outperform other BNNs which have slower inference times. However, we would like to re-emphasize that this scenario is often not representative of many common Bayesian optimization problems, which instead have very expensive function queries which would dominate the time budget.\n\nWe hope we have addressed your concerns, and please let us know if you have any remaining questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584122048,
                "cdate": 1700584122048,
                "tmdate": 1700584122048,
                "mdate": 1700584122048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oXVAt1Ulzq",
                "forum": "SA19ijj44B",
                "replyto": "aXRIZK8grf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer v5S3"
                    },
                    "comment": {
                        "value": "Hello, the rebuttal period will end in about 12 hours! Please let us know if you have any further questions we can address."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692778214,
                "cdate": 1700692778214,
                "tmdate": 1700692778214,
                "mdate": 1700692778214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o68ALPtx2Z",
                "forum": "SA19ijj44B",
                "replyto": "oXVAt1Ulzq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4401/Reviewer_v5S3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4401/Reviewer_v5S3"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for the response and acknowledge that I read it."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719124000,
                "cdate": 1700719124000,
                "tmdate": 1700719124000,
                "mdate": 1700719124000,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Z30aw9Qpx",
            "forum": "SA19ijj44B",
            "replyto": "SA19ijj44B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4401/Reviewer_XqQk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4401/Reviewer_XqQk"
            ],
            "content": {
                "summary": {
                    "value": "The paper is an empirical investigation into the use of Bayesian neural networks (BNNs) as surrogate models for BO instead of the traditional GPs. The BNN inference procedures investigated are HMC, SGHMC, deep ensembles, infinite-width BNNs, linearlized Laplace approximations, and deep kernel learning. The authors compare the performance of GPs and the various BNN inference procedures via the maximum reward attained over several standard synthetic benchmarks, real-world benchmarks, and high-dimensional settings. The paper also investigates several secondary aspects of BNNs for BO, including the role of the NN hyperparameters, the performance of hybrid models, the effect of the number of function evaluations available, and the computational runtime."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The results are of interest to BO researchers and BO practitioners looking for potential methods of improving the effectiveness of BO in real-world applications.\n2. The empirical investigation is extensive and carefully planned, covering several synthetic and real-world benchmarks, and provides support for many interesting hypotheses as well, such as the relative performance on high dimensional problems and the role of hyperparameters including network architecture.\n3. This paper is a gold standard for clarity and writing."
                },
                "weaknesses": {
                    "value": "1. For an empirical paper whose conclusions rest solely on the experimental results, 5 trials for each experimental setup is too little, as evidenced by multiple plots having heavily overlapping confidence intervals, Figure 6 in particular.\n\n2. A few clarifying questions, please see the Questions section."
                },
                "questions": {
                    "value": "1. This question concerns the experimental details outlined in Appendix C.1 and what I've gleaned from the code. When a GP model is used, it undergoes hyperparameter optimization via maximizing the marginal likelihood w.r.t. to the hyperparameters at every BO iteration. When a HMC model is used, it is also described to undergo a hyperparameter optimization procedure which is an iterated grid search that chooses the set of hyperparameters that (to my understanding) maximizes the maximum reward attained after all BO iterations in a trial. This optimization procedure is different from the GP one in that it requires an entire BO trial to compute the score of a single set of hyperparameters, and hence requires several BO trials as opposed to the GP one that is optimized per iteration and does not require several BO trials. Is this an accurate understanding? If so, could you comment on the validity of comparing the results of the GP model and the HMC model (along with the other BNN models since their hyperparameters are arrived at via the HMC search as well)? The concern is that the GP model did not have the same opportunity to do a 'meta-optimization' over several BO trials which might have been used to optimize other (hyper-)hyperparameters such as the choice of lengthscale and outputscale priors. Or was it the explicit intention to compare a standard GP setup against an (a priori unknown) 'optimal' BNN ?\n\n2. From Figure 6, GP and I-BNN are the top 2 performing models on all problems, and I-BNN dominates by far on high dimensional problems. However, I-BNNs are equivalent to (and implemented as) GPs with a specific neural-network based kernel. One may reach the conclusion that GPs still reign supreme in BO: use a standard Matern kernel for low dimensional problems, and the I-BNN kernel for high dimensional problems, and ignore BNNs that are not also GPs. Would you say this is a fair alternate conclusion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4401/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4401/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4401/Reviewer_XqQk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594717018,
            "cdate": 1698594717018,
            "tmdate": 1700577035288,
            "mdate": 1700577035288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "buJiMkoEt9",
                "forum": "SA19ijj44B",
                "replyto": "1Z30aw9Qpx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer XqQk [1/2]"
                    },
                    "comment": {
                        "value": "We really appreciate your review and thoughtful comments. Below we respond to your questions, and include several substantial experiments inspired by your comments. We also have a general response, summarizing our contributions.\n\n> Question 1: GP vs HMC experimental details\n\nIt is a fair point that our process for optimizing HMC differed from our process for setting GP hyperparameters. Indeed we were interested in understanding the potential of BNNs when the hyperparameters are well-specified for a particular problem.\n\nInspired by your comments, we have now conducted additional experiments for HMC where we optimize the hyperparameters on a per-iteration basis, similar to the procedure for GPs. Due to time constraints, we have focused on a subset of the objective functions, but we will include a full set of results across all problems in the final paper. Specifically, we fix the width and depth of the network across all experiments, and we conduct a small grid-search after each iteration to optimize the prior and noise hyperparameters based on the validation likelihood, and we include more details about the experiment setup and results in Appendix D.12. \n\nOverall, we find the performance of per-iteration optimized HMC to be competitive with per-trial optimized HMC for problems like Hartmann and Cell Coverage. Furthermore, although per-trial optimization for HMC does improve its performance on Ackley, we are still able to observe similar trends as before.\n\nFurthermore, to make the HMC results reported in the submission more comparable to the GP results, we have also conducted additional experiments where we optimize GP hyperparameters on a per-trial basis. Specifically, we use grid search to select the parameters of the prior distribution of the length scale and output scale for a GP, and for each objective, we choose the parameters which lead to the highest maximum reward over one trial.\n\nWe see that these optimized GPs are often comparable to the default GP and do not see significant improvements. This may be due to the GPs being optimized over one specific trial of Bayesian optimization, and the optimal GP may vary depending on the set of initial points. This shows that the observations of the submission hold for both the default GP as well as the meta-optimized version.\n\nWe thank you again for the question, and we believe the added experiments, inspired by your comments, will provide significant additional value to the paper.\n\n> Question 2: Do GPs still reign supreme?\n\nOur results show that GPs with a standard Matern kernel works well for lower-dimensional problems and GPs with the I-BNN kernel work well in higher dimensions.\n\nIndeed, GPs encompass a broad class of models. GPs with I-BNN kernels exhibit very different behavior from standard GPs with a Matern or RBF kernel. The infinite-width BNN has a non-Euclidean and non-stationary similarity metric, which enables its relative success in high dimensions. DKL based GPs are also significantly different, with their own properties, such as representation learning.\n\nFor this reason, we have been careful to distinguish \"standard GPs\"---GPs with the kernels almost always used in Bayesian optimization---from other model classes, as GPs allowing for any kernel are so general it becomes difficult to make informative conclusions or comparisons. Indeed, the line can get blurry---I-BNNs are a particular type of GP derived from a BNN.\n\nWe also note that we have several nuanced take-aways, rather than a single conclusion, and are careful not to proclaim standard GPs or BNNs as overall winners or losers. There is no desire to show that BNNs are generally superior, and we are in fact careful to highlight the broad appeal of standard GP surrogates. Our goal is to understand how various properties of the surrogate (stochasticity, representation learning, etc.) interact with performance in different settings, which transcend the GP vs. BNN distinction, though many of our considered surrogates are NN-inspired (HMC, SGHMC, Deep Ensembles, I-BNN, DKL). If we define BNNs narrowly, and GPs broadly, it may be reasonable to say that GPs have an edge, but we are most concerned with the low-level property distinctions of the surrogates."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510041187,
                "cdate": 1700510041187,
                "tmdate": 1700510041187,
                "mdate": 1700510041187,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hO16UhgXXD",
                "forum": "SA19ijj44B",
                "replyto": "1Z30aw9Qpx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer XqQk [2/2]"
                    },
                    "comment": {
                        "value": "> 5 trials is too little\n\nWith five trials we do see sufficient evidence to draw many informative conclusions. However, we agree more trials would be helpful and will run all of our experiments for a minimum of ten trials in the final paper to further differentiate the performance between surrogate models.\n\nAlthough Figure 6 has heavily overlapping confidence intervals, running more trials may not decrease the width of the intervals. This figure plots the distribution of the relative performance of each surrogate model across all trials and all objective functions. For example, we see that for at least 25% of the trials, the relative performance of GPs is less than 0.5, meaning the maximum reward found by GPs for that trial was closer to the reward found by the worst surrogate than the reward found by the best surrogate. We would expect this ratio to stay the same even as we increase the number of trials.\n\nThank you again for the thoughtful feedback, which we believe has improved the paper. We have added substantial new experiments inspired by your questions, and would appreciate if you could consider raising your score in light of our response."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510064126,
                "cdate": 1700510064126,
                "tmdate": 1700510064126,
                "mdate": 1700510064126,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5PugTw4m3S",
                "forum": "SA19ijj44B",
                "replyto": "hO16UhgXXD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4401/Reviewer_XqQk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4401/Reviewer_XqQk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response and for the effort made to address the questions. I have no further concerns and have increased my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577444426,
                "cdate": 1700577444426,
                "tmdate": 1700577444426,
                "mdate": 1700577444426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H0teV8Q600",
            "forum": "SA19ijj44B",
            "replyto": "SA19ijj44B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4401/Reviewer_6qj4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4401/Reviewer_6qj4"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a comprehensive empirical study of using Bayesian neural networks as the surrogate in Bayesian optimization. The paper considers a number of different BNNs, and performed experimental comparisons in a variety of experiments. Some interesting insights are shown from the experiments, including when standard GP surrogate is better and when BNN is better, HMC is often the best method for inference for BNN, deep kernel learning is usually competitive and deep ensemble is usually not, etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The methods under comparison are carefully selected to span a wide range of possible BNNs, and experiments are nicely designed to unveil specific insights about the relative strengths/weaknesses of different families of methods.\r\n- I think some of the conclusions/insights from the empirical comparisons can indeed be useful for future applications of Bayesian optimization, such as the competitiveness of deep kernel learning, the promising results of infinite-width BNNs in high-dimensional problems (which is a new observation to the best of my knowledge), etc.\n- The synthetic experiments in Figures 1 and 2 are nicely designed to illustrate the influence of different factors, and also find which are the parameter combinations likely to work better. The experiments \"Quality of Mean and Uncertainty Estimates\" on the potential of mixing different mean and uncertainty estimates are also particularly interesting.\n- The paper is well written, the contributions are nicely organized and discussed."
                },
                "weaknesses": {
                    "value": "**(1)** I think it would make the study more complete if another relevant line of works is discussed: using (non-Bayesian) neural networks as the surrogate in BO and using neural tangent kernel for exploration. The recent line of work on neural bandits has made it possible to use (non-Bayesian) neural networks as the surrogate in BO while still preserving the regret guarantee of BO by using the theory of the NTK, The relevance of neural bandits in BO has been shown by [1] below, and you can also refer to [1] to find the related works on neural bandits. In fact, I think the findings in [1] can be used to corroborate some of the findings in this work. For example, [1] also found that deep ensemble doesn't work well and explains it by arguing that deep ensemble cannot do principled exploration, I think this is in fact consistent with what's observed in this work, because the performance of deep ensemble plateaus at low objective values because it's subpar exploration ability makes it unable to find the region containing the global optimum. The paper [2] below also did an empirical study of neural bandit methods, so the findings in [2] may also be compared/combined with those in this paper to potentially get more insights. For example, [2] also found that neural bandits tend to work better when the objective function is complicated.\nIn fact, the connection between BNN-surrogate BO and neural bandits has also been discussed by the concurrent work of Kristiadi et al. (2023) in the context of linearized-Laplace approximation. The recent work of [3] has also shown the potential of using NTK in kernel regression, which may also be an empirical justification for the potential of BO with NTK based surrogate.\n\n[1] Sample-Then-Optimize Batch Neural Thompson Sampling, 2022.      \n[2] Empirical Analysis of Representation Learning and Exploration in Neural Kernel Bandits, 2021.      \n[3] Kernel Regression with Infinite-Width Neural Networks on Millions of Examples, 2022.\n\n**(2)** Another minor point which can make the paper easier to read is that when referring to the appendix, it'll make it easier for the reader if the specific subsection is referred to instead of just \"Appendix D\"."
                },
                "questions": {
                    "value": "- I find the contrast between deep kernel learning and deep ensemble particularly interesting, because both methods are able to use the strong representations learned by neural networks. I suppose the reason why deep kernel learning works better is because it can be readily plugged into BO which will take care of the exploration, while deep ensemble doesn't do well in exploration. Please see if this makes sense."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810830185,
            "cdate": 1698810830185,
            "tmdate": 1699636413382,
            "mdate": 1699636413382,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ua99IfaG0m",
                "forum": "SA19ijj44B",
                "replyto": "H0teV8Q600",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 6qj4"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and supportive comments! We really appreciate it.\n\n> Discussion of non-Bayesian neural networks as the surrogate in BO\n\nThank you for the references! While our paper focuses on Bayesian neural network surrogates for Bayesian optimization, for a thorough investigation, we recognize that there are other works focusing on non-Bayesian methods such as neural bandits and NTK. We have updated our related works section with the suggested papers, and we have also updated Appendix D.3 to include a discussion of the performance of deep ensembles in other works.\n\n> Deep ensembles do not do well in exploration\n\nAs you pointed out in your review, we noticed that deep ensembles often plateau at low objectives. We explore this phenomenon in Appendix D.3, and showed that smaller training sizes can often lead to deep ensembles having *less* uncertainty. In the low-data regime, we show that the loss landscape is relatively smooth, making it more difficult to find diverse solutions, corresponding to distinct basins of attraction, through only re-initializing optimization runs. The lack of model diversity in the ensemble leads to smaller predictive uncertainty estimates and worse exploration. This makes deep ensembles less suitable surrogate models when there is a minimal number of data points. Interestingly, this is in contrast to the success of deep ensembles in general applications. More training data (i.e., objective queries) helps deep ensembles again become competitive.\n\n> Refer to specific sections of the Appendix\n\nThank you for the suggestion! We have updated our draft accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509878428,
                "cdate": 1700509878428,
                "tmdate": 1700510223451,
                "mdate": 1700510223451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]