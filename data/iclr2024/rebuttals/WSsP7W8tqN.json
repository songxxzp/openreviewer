[
    {
        "title": "Grokking Tickets: Lottery Tickets Accelerate Grokking"
    },
    {
        "review": {
            "id": "XJCB1xCNcf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_tMC9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_tMC9"
            ],
            "forum": "WSsP7W8tqN",
            "replyto": "WSsP7W8tqN",
            "content": {
                "summary": {
                    "value": "The paper have conducted a set of experiments that demonstrate the effectiveness of grokking tickets in accelerating the generalization process. The results of the study suggest that the transition phase between memorized and generalized solutions corresponds to the exploration of good subnetworks, or lottery tickets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper delves into the connection between grokking and LTH, demonstrating the grokking tickets hold good properties beyond just a sparsity.\n\n2. Experimental results validate their findings."
                },
                "weaknesses": {
                    "value": "1. The writing needs improving, and at times, I find it hard to follow the text. For instance, the term \"grokking tickets\" is used 12 times before its formal definition presented, which presents a certain impediment to readers.\n\n2. I am uncertain about the practical significance of the findings. Specifically, I understand that one of the conclusions is that tickets found during the $C_{mem}$ phase exhibit better performance. However, this seems to align with the common practice in LTH research~\\cite{chen2021unified, gan2022playing}, where networks are thoroughly trained and then pruned based on optimal validation scores during each pruning iteration. Therefore, the question arises as to what contribution the findings in the paper make to the discovery of improved lottery tickets.\n\n3. The experimental results in Section 5.2 appear to support the author's claims that poor selection of subnetworks harms generalization. However, as mentioned in 2, the grokking tickets results seem to represent the outcomes of LTH approaches. It appears that the author has primarily validated that LTH with IMP can outperform most PaI methods. \n\n4. While the existing results do indeed support the author's assertions, the use of overly simplistic tasks (modular addition and MNIST classification), basic baselines (MLP and single-head Transformer), and a lack of additional experimental configurations (such as performance under multi-layer MLPs or multi-head attention) leave me with reservations regarding the validity of the conclusions."
                },
                "questions": {
                    "value": "1. The paper could be better organized. To name a few, in 2.1 \u2018abuse\u2019 -> \u2018abuse of\u2019; in 5.1 \u2018leaders\u2019->\u2019readers\u2019. Excessive space is devoted to some trivial aspects, such as formulas for weight initialization.\n\n2. It is advisable to include additional backbones and experimental setups to comprehensively validate the claims."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Reviewer_tMC9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697421769886,
            "cdate": 1697421769886,
            "tmdate": 1699636816466,
            "mdate": 1699636816466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sZrHVKI8GJ",
                "forum": "WSsP7W8tqN",
                "replyto": "XJCB1xCNcf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tMC9"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We will answer your questions to address your concerns.\n\n## Writing \n>The writing needs improving, and at times, I find it hard to follow the text. For instance, the term \"grokking tickets\" is used 12 times before its formal definition presented, which presents a certain impediment to readers.\n>The paper could be better organized. To name a few, in 2.1 \u2018abuse\u2019 -> \u2018abuse of\u2019; in 5.1 \u2018leaders\u2019->\u2019readers\u2019. Excessive space is devoted to some trivial aspects, such as formulas for weight initialization.\n\nAs you pointed out, we have defined Grokking tickets in the Abstract and at the beginning of the Introduction. Additionally, for readability, we have made changes to the writing, such as defining comparison models and correcting typographical errors. \n\n## Contribution to Lottery Tickets (LT).\n>Therefore, the question arises as to what contribution the findings in the paper make to the discovery of improved lottery tickets.\n\nThe novelty of this research in relation to the Lottery Tickets hypothesis is as follows: (1) Lottery Tickets as an explanation for the Grokking phenomenon. (2) Lottery Tickets that acquire good structures for generalization, not just a reduction in parameters. \n- Regarding point (1), in Appendices G and H, we conducted analyses on how Grokking tickets are discovered. \nIn Appendix G, building upon prior research [Nanda 2022] [Zhong 2023], we investigated the structures that emerge during the transition phase in the modular addition task. In the task of modular addition, it is known that periodic representations are necessary for generalization. Our paper also demonstrates similar results, and it was found that in the case of Grokking tickets, good representations are acquired more quickly.\nIn Appendix H, following the [Paganini 2020], we examined the structures acquired mid-way through grokking using the Jaccard distance [P. Jaccard 1901] . As can be seen from Section 4, the results show that structures which gradually generalize (lottery tickets) are being acquired. Especially during the transition phase, the distance of the masks decreases rapidly, indicating that the discovery of structures is important for generalization.\n\n- Regarding point (2),  in Appendices I and J, We conducted an analysis demonstrating that generalization is accelerated not merely due to the reduction of parameters through Lottery Tickets, but through the discovery of good structures.\nIn Appendix I, to respond to the reviewer's concern that the lottery tickets learn faster than the original dense model has already been demonstrated in the original LTH paper and many other previous work. We conducted experiments using Grokking tickets, where we\napplied the mask only to the initial values and not during the course of learning. We refer to this model as the \u2019Re-Dense model\u2019. By doing this, the number of parameters optimized by the optimizer becomes equal in both the Dense and Re-Dense models, differing only in their initial values.  Base Model and Re-Dense Model have the same number of parameters, with the only difference being in their initial values (further, the difference is just the application of a mask to the same initial values). It is evident that they generalize faster than the Base Model. In Appendix J, we conducted additional experiments. One experiment involved masking only the completely 'dead' units (18%) of Grokking tickets. If Grokking tickets were accelerating generalization simply by reducing the number of learnable parameters, as suggested by existing studies, then this method should also accelerate grokking. However, as shown in Appendix J, this method not only did not accelerate generalization but actually slowed it down. Conversely, it was also confirmed that reviving the completely dead units in Grokking tickets does not change the speed of generalization compared to the original Grokking tickets.\n\nWe believe these results provide important analysis for research in the Lottery Ticket (LT) field.\n\n## The Significance of the PaI Experiment.\n>the grokking tickets results seem to represent the outcomes of LTH approaches. It appears that the author has primarily validated that LTH with IMP can outperform most PaI methods.\n\nAs you mentioned, it is known that IMP is superior to PaI. However, we did not intend to claim that IMP is superior to PaI, rather, we employed other pruning methods to demonstrate that Grokking tickets are beneficial not just in terms of sparsity, but also as a structure (refer to section 5.2)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389971862,
                "cdate": 1700389971862,
                "tmdate": 1700389971862,
                "mdate": 1700389971862,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "88mXl6A8k6",
            "forum": "WSsP7W8tqN",
            "replyto": "WSsP7W8tqN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_DeHf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_DeHf"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes a phenomenon called ``Grokking'' by using the ``Lottery Ticket Hypothesis'' as a tool to uncover the reasons for Grokking. Grokking here refers to delayed generalization, i.e., test accuracy grows to its peak value long after the network has fit the training. The hypothesis examined here is that neural network training results in a sparse network. Sparsity may then be used to explain the reason behind delayed generalization. The paper conducts an empirical analysis with modular addition using a small Transformer  and briefly studies an MLP with large initialization as suggested by Liu et. al.  Ablations are conducted that show that the grokked solution checkpoint generalizes faster than checkpoints collected at earlier points during optimization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper uses the lens of sparsity to study Grokking. This is a promising avenue of exploration\n- The empirical analysis for modular addition asks and then attempts to answer very sensible and relevant questions. \n  - The choice of checkpoints suggest that useful sparsity occurs later on in optimization (close to or after solution is grokked)\n  - Ablations with fixed norm and fixed sparsity are useful to readers interested in Grokking\n  - The choice of pruning appears to make a difference. Checkpoints after generalization perform better than other algorithms used in pruning literature"
                },
                "weaknesses": {
                    "value": "- The paper attempts to make a connection to the Lottery Ticket Hypothesi (LTH). Sparsity is a reasonable hypothesis that has been explored by Merrill et. al. previously in literature. However, I am not convinced that connecting LTH to this work is necessary. Merrill 2023 make observations about sparse networks without invoking LTH\n- Nanda et. al. (Nanda 2023) show that the network after generalization consists of a few sinusoids, i.e., finds a sparse solution. So sparsity being an explanation has been shown in prior literature. Also suggest that the authors consult Gromov 2023 for the solution found a \n\n- The paper notes in Section 5.1( ablation of weight norms) that a subnetwork has increased weight norm after a generalizing solution is found by gradient descent while other weights end up with smaller norms. This is the same observation made in Merrill 2023 where the authors study subset parity learning problem\n- The observation that the sparse generalized solution optimizes faster than the dense network while interesting is not critical to understanding Grokking. It is known that regularization or lowering capacity of models via regularization does help shorten the time between fitting and generalization in algorithmic datasets. This observation falls in-line with the above\n- Given the empirical nature of the paper, the number of datasets considered in the analysis appear to be inadequate. Power 2022 construct various algorithmic datasets. Gromov 2023 use a MSE solution with MLP for addition. Have the observations been confirmed in more settings than the ones considered in the paper? \n\n- [Gromov 2023]  Grokking modular arithmetic\n- [Meriill 2023] William Merrill, Nikolaos Tsilivis, and Aman Shukla. A tale of two circuits: Grokking as competi- tion of sparse and dense subnetworks, 2023.\n- [Nanda 2023] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability, 2023.\n- [Power 2022] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gener- alization beyond overfitting on small algorithmic datasets, 2022."
                },
                "questions": {
                    "value": "**Major Issues**\n\n- Please see weakness section\n- Using sparsity as an explanation for Grokking has been done before in other papers. How is the analyses presented in the paper adding to existing literature? Without a clear answer, the novelty of the work in the paper is insufficient for me to vote for acceptance at the conference.\n\n**Minor Issues**\n\n- Introduction\n  -  Power et. al, observed Grokking in many algorithmic datasets including modular addition. \n  -  Barak et. al. [2] demonstrated Grokking on subset parity dataset before Merrill et al.\n  -  ``accelerate'' the grokking process may be confusing to readers outside the area of Grokking. Perhaps emphasize that the time to generalization is shorter?\n\n- Page 7\n  - leaders should be readers\n\n[2] https://openreview.net/forum?id=8XWP2ewX-im"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Reviewer_DeHf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698285561036,
            "cdate": 1698285561036,
            "tmdate": 1699636816312,
            "mdate": 1699636816312,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mKmm7rwYKg",
                "forum": "WSsP7W8tqN",
                "replyto": "88mXl6A8k6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DeHf"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We will answer your questions to address your concerns.\n\n## The Reason for Using Lottery Tickets (LT).\n>I am not convinced that connecting LTH to this work is necessary. Merrill 2023 make observations about sparse networks without invoking LTH.\n\nTo clarify the differences from prior research, we added a section on related works.\nAs mentioned in section 6, while Merrill et. al  prunes neurons with smaller weight norms, we use the lottery ticket (LT) method for pruning weights. Besides, we tested arithmetic tasks where the importance of sparsity is not obvious and the standard benchmark in the grokking, while they have only conducted experiments in tasks like (n-k)-parity where the importance of sparsity is obvious. \nTo be applicable to more general tasks and architectures, we need to use the Lottery Tickets for analysis of grokking using the Lottery Ticket (LT) method. \n\n## Relation to Previous Research on Sparsity.\n>So sparsity being an explanation has been shown in prior literature.\n\nThe novelty of our study compared to previous research can be summarized in four points: 1) Methodology (Lottery Tickets), 2) Tasks, 3) Architecture, and 4) Claims.\n1) We conducted analysis using the Lottery Tickets (LT) method. The necessity of analysis through LT is as described above.\n\n2) Our tasks are not (n-k) parity where the optimal structure being sparse  is self-evident, but we have also validated in more general tasks such as MNIST.\n\n3) Unlike Merrill 2023 or Nanda 2022, we have not confined our validation to just one architecture; instead, we have tested across multiple architectures.\n\n4) Furthermore, we assert that it's not just sparsity that's important, but the acquisition of good structures is crucial (refer to section 5.2, Appendix G and H).\n\n## Differences from Merrill 2023.\n>This is the same observation made in Merrill 2023 where the authors study subset parity learning problem.\n\nIndeed, similar observations have been confirmed, but as mentioned above, our results indicate that not just sparsity, but also the acquisition of good structures is important for generalization.  In section 5.2 in the original version of our paper, we compared various methods fixed at the same level of sparsity, and these results showed that Grokking tickets have effects beyond just reducing parameters. This shows that in explaining grokking, it's not just sparsity that's key, but also the acquisition of good structures is crucial for generalization.\n\n## The Novelty of Grokking Tickets.\n>It is known that regularization or lowering capacity of models via regularization does help shorten the time between fitting and generalization in algorithmic datasets.\n\nAs suggested in existing research, one reason why generalization is faster with Grokking tickets is due to the reduction in the number of learnable parameters. However, our study suggests that grokking is not just about reducing the effective number of parameters being learned, but also about acquiring structures appropriate to the task. For instance, in section 5.2 in the original version of our paper, we compared various methods fixed at the same level of sparsity, and these results showed that Grokking tickets have effects beyond just reducing parameters.\nTo analyze this in detail, we conducted additional experiments. One experiment involved masking only the completely 'dead' units (18%) of Grokking tickets (see Appendix J). If Grokking tickets were accelerating generalization simply by reducing the number of learnable parameters, as suggested by existing studies, then this method should also accelerate grokking. However, as shown in Appendix J, this method not only did not accelerate generalization but actually slowed it down. Conversely, it was also confirmed that reviving the completely dead units in Grokking tickets does not change the speed of generalization compared to the original Grokking tickets.\nWe have added these discussions in the revised section 6.4."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389548883,
                "cdate": 1700389548883,
                "tmdate": 1700389548883,
                "mdate": 1700389548883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MhlrefOuKY",
                "forum": "WSsP7W8tqN",
                "replyto": "dwJxuAdCQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6981/Reviewer_DeHf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6981/Reviewer_DeHf"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "I thank the authors for their rebuttal. My understanding now is that one of the central claims is that the Lottery Ticket Hypothesis (LTH)suggests acquisition of certain specific structures helps with grokking, i.e., the grokking lottery ticket. My understanding, based on reading all of the responses by the authors, is this insight flows naturally from the LTH. This part still feels incomplete as the paper does not shed any light on what can be said about the structures acquired after a model reaches low test loss (i.e., groks). Nanda et al (2023) have shown that for modular addition the structure can be quantified via Fourier coefficients for Transformers. Gromov (2023) provides an analytical solution for an MLP that is also characterized by Fourier coefficients for modular addition. I do not see any new insights being applied to grokking setup by using the LTH that can help researchers in the subfield of grokking/mechanistic interpretability make progress.  Hence, I plan to keep my score unchanged while also being open to hearing from other reviewers in further discussions. If the authors feel like I have missed something critical here and/or have new insights then I'd like to hear from them."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617814790,
                "cdate": 1700617814790,
                "tmdate": 1700617814790,
                "mdate": 1700617814790,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "42LAh01EdR",
            "forum": "WSsP7W8tqN",
            "replyto": "WSsP7W8tqN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_KZMf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_KZMf"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the effect of lottery tickets in the context of Grokking and finds that the Grokking tickets obtained during the generalization phase can accelerate the speed of the model to reach the generalization phase. The found that the speedup only happens when subnetworks are identified at the generalization solution. Whereas, there is no speed up when we try to identify subnetworks at initialization, or at the memorization solution or the transition between memorization  and generalization. In general, this is an interesting combination of LTH and Grokking but with some mediocre observations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper explores the role of LTH in Grokking and make a good combination of LTH and Grokking. \n\n2. The finding that only subnetworks discovered during the generalization phase can speedup the generalization process is reasonable, aligning with previous findings of LTHs. \n\n3. Figure 1 clearly demonstrates the main message delivered by this paper. \n\n4. They also demonstrate that it is possible to induce grokking without weight decay when using the grokking tickets."
                },
                "weaknesses": {
                    "value": "1. My major concern is that while the combination of LTH with Grokking is new to the community, the empirical findings shown in this paper is somehow mediocre. For instance, i. it is not surprising to see that the subnetworks obtained during the generalization phase is crucial for grokking speedup.  ii. The lottery tickets learn faster than the original dense model has already been demonstrated in the original LTH paper and many other previous works. iii. The definition of the Grokking Tickets has no essential difference than the original Lottery Tickets and can be covered by the original ones since the original LTH does train the dense model to the end. \n\n2.  Perhaps, the authors can emphasize the contribution of the papers from the perspective of Grokking. Why the grokking tickets are important for Grokking?\n\n3. Besides the speedup, does Grokking tickets bring any performance benefits over the dense one?"
                },
                "questions": {
                    "value": "Please refer to the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Reviewer_KZMf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838669263,
            "cdate": 1698838669263,
            "tmdate": 1700480463971,
            "mdate": 1700480463971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n0uCUiChwv",
                "forum": "WSsP7W8tqN",
                "replyto": "42LAh01EdR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KZMf"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We will answer your questions to address your concerns.\n## Profound Analysis\n>it is not surprising to see that the subnetworks obtained during the generalization phase is crucial for grokking speedup.\n\nAs you mentioned, the acceleration of grokking in a subnetwork is not surprising.\nTo address your concern, we have added the following two analysis.\n1) As other reviewers have pointed out, the simplest explanation for faster generalization is that the number of learnable parameters decreases. However, our analysis suggests that performance improvement is not just due to a reduction in parameters. Firstly, not any structure works effectively (refer to section 5.2), and secondly, even if only completely 'dead' neurons in grokking tickets (18%) are removed, learning does not accelerate but rather decelerates (refer to Appendix J). These results imply that it's not just about reducing the number of parameters, but the construction of some good structures contributes to the acceleration of generalization.\n2) \nAs stated in general response (2), to analyze how Grokking tickets are acquired during the course of learning, we examined the periodicity of weights (refer to Appendix G) and the similarity of masks (refer to Appendix H).\nAdditionally, the results of the newly added analysis of representations in grokking tickets indicate that better representations are acquired earlier with grokking tickets. While the relationship between grokking and representation learning has been widely discussed, our study connects the perspectives of representation learning and structure acquisition in grokking.\n\n\n## Novelty with Respect to Lottery Tickets.\n>The lottery tickets learn faster than the original dense model has already been demonstrated in the original LTH paper and many other previous works.\n\nAs you mentioned, it is known that the lottery tickets learn faster than the original dense model. \nAs mentioned above, we have further made the following claims and conducted experiments:\n1) It's not the reduction in the number of parameters, but the acquisition of good structures that is crucial for generalization (refer to section 5 and Appendix I,J).\n2) How good structures are acquired during learning (Appendix G,H).\nWe have added similar discussions in the main text section 6.4 as well.\n\n\n## The difference between Grokking Tickets and the original Lottery Tickets\n>The definition of the Grokking Tickets has no essential difference from the original Lottery Tickets and can be covered by the original ones since the original LTH does train the dense model to the end.\n\nAs stated in General Response (2), we are not proposing a new LT method; rather, we are employing the Lottery Ticket (LT) method as a means to analyze grokking. Therefore, Grokking tickets are encompassed within the definition of LT.\nHowever, as mentioned above, experiments in Appendix I and J, demonstrate that Grokking tickets are not merely sparse (i.e., having fewer learning parameters), but also acquire good structures for generalization.\n\n## The Reasons Why Grokking Tickets are Important for Grokking.\n>Perhaps, the authors can emphasize the contribution of the papers from the perspective of Grokking. Why the grokking tickets are important for Grokking?\n\nAs mentioned in the General Response as well, in section 6, we have detailed the comparison between previous studies on grokking and our research. Especially in existing studies, explanations often focus on weight norms and weight decay [Liu 2022, Power 2022]. In contrast, our research not only examines weight norms but also focuses on the network structure. We determined that grokking occurs due to the discovery of lottery tickets.\nAlthough Merrill et al. emphasize the importance of sparsity, they do not extend their claim to the acquisition of good structures.\nTherefore, in response to the question 'Why are Grokking tickets important for Grokking?', our answer lies in the acquisition of good structures.\n\n## Benefits over the Dense model\n>Besides the speedup, does Grokking tickets bring any performance benefits over the dense one?\n\nOf course, as we are finding extremely sparse solutions, I believe it is highly relevant from the perspective of learning efficiency. For instance, the model we used in this study maintained its generalization performance even after pruning about 81% of its edges. (Conversely, this led to an earlier occurrence of grokking.)\nAs also mentioned in the General Response, the relationship with representation learning is conceivable, and our findings are likely to provide important insights from the perspective of model interpretability."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388967082,
                "cdate": 1700388967082,
                "tmdate": 1700389092349,
                "mdate": 1700389092349,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "flfTT4vtis",
                "forum": "WSsP7W8tqN",
                "replyto": "42LAh01EdR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6981/Reviewer_KZMf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6981/Reviewer_KZMf"
                ],
                "content": {
                    "title": {
                        "value": "I am not convinced by authors' response."
                    },
                    "comment": {
                        "value": "I thank the authors for your response. However, I am not convinced by the response. I don't think the faster generalization has any correlation with the number of learnable parameters decrease. And the construction of some good structures contributes to the acceleration of generalization is not something new to LTH. It has very little merits to the concept of LTH. \n\nAfter seeing other reviewers' comment, I believe the merit of this work to Grokking is also limited. Therefore, I decide to decrease my score to reject."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480453857,
                "cdate": 1700480453857,
                "tmdate": 1700565526988,
                "mdate": 1700565526988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zdoRx7WcjW",
            "forum": "WSsP7W8tqN",
            "replyto": "WSsP7W8tqN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_qTo9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_qTo9"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the grokking phenomenon in neural networks through the lens of the lottery ticket hypothesis, positing that identifying optimal sparse subnetworks (\"lottery tickets\") is crucial for the transition from memorization to generalization. The authors present experiments using MLP and Transformer architectures on tasks like modular addition and MNIST classification to demonstrate that these identified subnetworks can significantly accelerate grokking."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Novel Approach:** The paper introduces a novel concept of \"grokking tickets\" within the context of the lottery ticket hypothesis, which is an original contribution to the understanding of neural network learning dynamics.\n\n**Experimental Evidence:** Initial experimental results indicate that the identified subnetworks do indeed accelerate the grokking process, which could have implications for the efficiency of training neural networks.\n\n**Clarity of Presentation:** The paper is well-structured and presents its methodology and findings clearly, making a case for the importance of subnetwork identification in neural network training."
                },
                "weaknesses": {
                    "value": "**Theoretical Underpinning:** The paper lacks a comprehensive theoretical framework that explains why and how grokking tickets work, leaving the reader to infer the underlying principles from empirical observations.\n\n**Limited Experimental Scope:** The experiments are confined to a narrow set of tasks and architectures, which might not fully demonstrate the generalizability of the proposed method.\n\n**Lack of In-Depth Analysis:** The paper does not provide an in-depth analysis of the scalability of the approach or a comparison with other state-of-the-art methods across varied settings.\n\n**Comment on References:** While the paper presents a novel approach to understanding the grokking phenomenon in neural networks, the reference list does not appear to be fully comprehensive. It would strengthen the paper to include a broader range of sources that contextualize the work within the larger body of research on neural network pruning, generalization, and learning dynamics."
                },
                "questions": {
                    "value": "1. Can the authors elaborate on the theoretical foundations that might explain the observed acceleration in learning due to grokking tickets?\n\n2. Would the authors consider expanding their experimental evaluation to include a wider variety of tasks and network architectures to confirm the robustness of their findings?\n\n3. How do the authors envision the scalability of the proposed method, and how does it compare with other pruning or training acceleration techniques in terms of computational efficiency and performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699382627150,
            "cdate": 1699382627150,
            "tmdate": 1699636816116,
            "mdate": 1699636816116,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wmFNgxHNrP",
                "forum": "WSsP7W8tqN",
                "replyto": "zdoRx7WcjW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qTo9"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We will answer your questions to address your concerns.\n## Theoretical Underpinning\n>Theoretical Underpinning\n>Can the authors elaborate on the theoretical foundations that might explain the observed acceleration in learning due to grokking tickets? \n\nTo address your concern, we have added the following two analisis.\nAs other reviewers have pointed out, the simplest explanation for faster generalization is that the number of learnable parameters decreases. However, our analysis suggests that performance improvement is not just due to a reduction in parameters. Firstly, not any structure works effectively (refer to section 5.2), and secondly, even if only completely 'dead' neurons in grokking tickets (18%) are removed, learning does not accelerate but rather decelerates (refer to Appendix J). These results imply that it's not just about reducing the number of parameters, but the construction of some good structures contributes to the acceleration of generalization.\n\n## Depth Analysis\n>Lack of In-Depth Analysis\n\nAs stated in general response (2), to analyze how Grokking tickets are acquired during the course of learning, we examined the periodicity of weights (refer to Appendix G) and the similarity of masks (refer to Appendix H).\nAdditionally, the results of the newly added analysis of representations in grokking tickets indicate that better representations are acquired earlier with grokking tickets. While the relationship between grokking and representation learning has been widely discussed, our study connects the perspectives of representation learning and structure acquisition in grokking.\n\n## Expand Experimental Scope\n>Limited Experimental Scope\n>Would the authors consider expanding their experimental evaluation to include a wider variety of tasks and network architectures to confirm the robustness of their findings?\n\nFollowing the reviewers\u2019 comments, we conducted experiments across various task configurations and with different architectures. \nIn Appendix D, similar to MLP experiments, we conduct experiments on single-head transformer architecture. Similar to the experiments conducted in the paper, similar results were obtained on single-head transformer architecture as well.\nIn Appendix E,  we conduct experiments on multi-head transformer architecture and multi-layer MLP. Similar to the experiments conducted in the paper, generalization is faster due to Grokking tickets. on multi-head transformer architecture and multi-layer MLP as well.\nIn Appendix F, referencing [Power 2022], we conduct experiments on diverse tasks. Similar to the experiments conducted in the paper, generalization is faster due to Grokking tickets\u3000on diverse tasks.\n\n## The novelty of our study\n>\u200b\u200bComment on References\n\nThe novelty of our study compared to prior research was unclear. Therefore, in section 6, we summarized the superiority and novelty of our research over previous studies on grokking. Specifically, our main assertion delves deeper than the analysis of weight norms and weight decay in prior research [Liu 2022, Power 2022], pinpointing structural exploration as the underlying cause.\n\n## The scalability of our  method\n>How do the authors envision the scalability of the proposed method, and how does it compare with other pruning or training acceleration techniques in terms of computational efficiency and performance?\n\nAs mentioned in the General Response, in the Appendix G and H, unlike other pruning methods, we investigated whether the network structure changes similarly during grokking and generalizes, from the perspective of Lottery Tickets (LT). This analysis is beneficial not only for research on efficient learning methods but also for the interpretability of models."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388381234,
                "cdate": 1700388381234,
                "tmdate": 1700389080054,
                "mdate": 1700389080054,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dCzKNdbWPQ",
            "forum": "WSsP7W8tqN",
            "replyto": "WSsP7W8tqN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_NUTh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6981/Reviewer_NUTh"
            ],
            "content": {
                "summary": {
                    "value": "This work provided a new perspective for analyzing the grokking phenomenon, that is the lottery ticket hypothesis.\n\nThe authors first showed an interesting observation they made. They trained a neural network to the grokking stage and then performed one-shot pruning. They called the subnetwork obtained as \u201cgrokking ticket\u201d. The authors observed that the grokking ticket can significantly shorten the training epochs needed to achieve the grokking stage, compared to the dense network.\n\nFurthermore, the authors conducted a series of ablation experiments to deconstruct the effects brought by the grokking tickets, including:\n* Pruning stage: tickets obtained before the grokking stage won\u2019t accelerate the occurrence of the grokking stage.\n* Weight norms: as the grokking phenomenon has been connected to the weight norms in the literature, the authors also compared the grokking tickets with dense networks whose weights are scaled to have similar norms. The results showed no acceleration through weight scaling.\n* Weight decay: the authors showed with numerical results that the grokking tickets at appropriate pruning ratios can waive the necessity of weight decay, which has been assumed necessary for grokking to happen."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This work provided a new brandnew perspective for investigating and understanding the grokking phenomenon, that is the sparse network structure. The authors made interesting observations on the acceleration of grokking brought by pruning, which connects the seemingly disjoint topics.\n\n+ The authors provided results of well-designed experiments that decoupled grokking tickets and weight norms (thus weight decaying), which have been assumed to be the key of grokking, throwing light on a higher level of property that grokking could possess.\n\n+ Moreover, this work also provided a perspective of using grokking phenomenon to understand pruning and specifically lottery tickets. Many previous studies have shown that SNIP, GraSP and SynFlow achieved similar accuracies on classification tasks but Fig. 7 in this work showed very different behaviors with these methods."
                },
                "weaknesses": {
                    "value": "- While this work made a lot of interesting observations, profound analysis and investigation behind empirical results are lacking.\n\n- The writing quality of this work is not satisfactory."
                },
                "questions": {
                    "value": "n/a"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6981/Reviewer_NUTh"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6981/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699755814563,
            "cdate": 1699755814563,
            "tmdate": 1699755814563,
            "mdate": 1699755814563,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GLOJQvvnql",
                "forum": "WSsP7W8tqN",
                "replyto": "dCzKNdbWPQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NUTh"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We will answer your questions to address your concerns.\n\n## Profound Analysis\n>While this work made a lot of interesting observations, profound analysis and investigation behind empirical results are lacking.\nFollowing the reviewer comments, we conducted three additional analyses to investigate the implication behind empirical results. \n\n(a) Analyzing similarity of structure (lottery tickets) among memorization, transition, and generalization phase of training. \nAppendix H, following the [Paganini 2020], we examined the structures acquired mid-way through grokking using the Jaccard distance [P. Jaccard 1901] . As can be seen from Section 4, the results show that structures which gradually generalize (lottery tickets) are being acquired. Especially during the transition phase, the distance of the masks decreases rapidly, indicating that the discovery of structures is important for generalization.\n\n(b) Connection between grokking tickets to existing analysis. \nRecently,  [Nanda 2022] investigated the weights of networks during grokking, and found that the weights exhibit clear periodic patterns after the grokking occurs. We follow the similar experiments to connect the grokking tickets and the representation learning perspective of the grokking in Appendix G. The results show that the grokking tickets help to learn task-appropriate representations, i.e. unlike the dense model, grokking tickets help to learn the periodicity in weights at the early stages of training. \n\n## writing quality\n>The writing quality of this work is not satisfactory.\n\nThank you for carefully reading our paper. To improve the clarity, we proof-read the paper again and made necessary revisions. A significant change is the addition of a section on related work in Section 6. We also have clearly defined grokking tickets and other comparison models in the abstract and introduction."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388119027,
                "cdate": 1700388119027,
                "tmdate": 1700388163037,
                "mdate": 1700388163037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]