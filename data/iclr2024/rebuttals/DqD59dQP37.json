[
    {
        "title": "Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework"
    },
    {
        "review": {
            "id": "zl8ncfQ6pO",
            "forum": "DqD59dQP37",
            "replyto": "DqD59dQP37",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3659/Reviewer_uvD6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3659/Reviewer_uvD6"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the sensitivity analysis of causal fairness criteria, specifically focusing on counterfactual direct effect (Ctf-DE), indirect effect (Ctf-IE), and spurious effects concerning unobserved confounding. The authors establish bounds for these measures by utilizing the generalized marginal sensitivity model (GMSM) and present a model for learning fair predictions. Experimental results underscore the method's effectiveness to some extent."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The sensitivity analysis on unobserved confounders on causal fairness criterion is a relevant and important research problem.\n\n- The experiments on synthetic data and real data demonstrate the effectiveness of the proposed method at some extent."
                },
                "weaknesses": {
                    "value": "- Limited Contribution and Novelty. The contribution and novelty of this paper may be limited. The theorem presented in the paper, Theorem 1, appears to be a specific application of the Generalized Marginal Sensitivity Model (GMSM) introduced by Frauen et al. (2023), which offers a comprehensive framework for causal sensitivity analysis under unobserved confounding in various settings. It would be beneficial to clarify the distinct contributions and challenges of this work compared to Frauen et al. (2023).\n\n- Limited Scope in Fairness Notions. The paper is somewhat misleading in its claim to perform sensitivity analysis on causal fairness under unobserved confounding. In fact, the focus of this paper is confined to specific causality-based fairness notions based on counterfactual direct effect (Ctf-DE), indirect effect (Ctf-IE) and spurious effects. However, causal fairness encompasses a broader range of notions, such as ones based on proxy discrimination, path-specific causal effects, path-specific counterfactual effects (including natural direct or indirect causal effect), etc. Additionally, it is misunderstanding to call the definitions Ctf-DE, Ctf-IE and Ctf-SE in Zhang & Bareinboim (2018a) as \u2018path-specific causal effects\u2019, which differs from their formal definition [1], thus leading to confusion.\n\n- Incomplete Literature Review. The paper lacks a comprehensive review of prior literature on sensitivity analysis on causal effects to unobserved confounding, such as marginal sensitivity model. Additionally, it would be beneficial to provide a brief overview of sensitivity analysis models, including the GMSM, and discuss their strengths and weaknesses.\n\n- Enhanced Experimental Analysis. In the experimental section, it would be advantageous to report and compare prediction performance across various models under different levels of confounding. This would provide a more robust assessment of the proposed method's performance.\n\n- Handling Continuous Variables. The paper assumes that the variables Z and M are discrete, yet many real-world variables are continuous. Therefore, it is essential to discuss how the proposed method can be extended to accommodate continuous variables.\n\n[1] Avin, Chen, Ilya Shpitser, and Judea Pearl. \"Identifiability of path-specific effects.\" (2005)."
                },
                "questions": {
                    "value": "The authors state that \"A key benefit of the GMSM is that it can deal with discrete mediators and both discrete and continuous outcomes.\" It is not convincing. It would be helpful to elaborate on other reasons for adopting the GMSM and discuss the strengths and weaknesses of alternative sensitivity models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3659/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3659/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3659/Reviewer_uvD6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3659/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698490469720,
            "cdate": 1698490469720,
            "tmdate": 1700717570167,
            "mdate": 1700717570167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8bZQHgIfOT",
                "forum": "DqD59dQP37",
                "replyto": "zl8ncfQ6pO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer uvD6"
                    },
                    "comment": {
                        "value": "We are grateful for the review! We appreciate the reviewer\u2019s constructive feedback and approved our paper accordingly.\n \n# Response to weaknesses\n\n**1)  \tContribution and novelty:**\n\nThank you for giving us the opportunity to emphasize the novelty of our framework. Thereby, we clarify that we do **not** simply combine two concepts but that our derivations are **non-trivial**. \n\n*Why is the derivation of our bounds non-trivial?* We have derived tailored bounds for counterfactual fairness under unobserved confounding. Importantly, our bounds require a novel and careful derivation. In other words, our bounds are **not** just a simple result of adopting a sensitivity model but require a new derivation tailored to our setting. The reason is the following: Causal fairness notions commonly contain nested counterfactuals and are thus located on **level three** of Pearl\u2019s ladder of causality. The GMSM (and other sensitivity models) incorporate single-intervention counterfactuals and are thus located on **level two**. Because of that, existing bounds from sensitivity models are **not** applicable. In other words, we can **not** simply adopt existing bounds from sensitivity models, but we need a **new derivation** for level three of Pearl\u2019s causality ladder.\n\n*What is our contribution?* We agree that causal fairness and partial identification bounds under the MSM/GMSM have **separately** been studied before. One of our contributions is the **transfer of concepts** from sensitivity analysis to the causal fairness literature (note: due to the reasons above, this is non-trivial): As a result, we offer a new framework to assess causal fairness under unobserved confounding. This is a major difference to the existing literature on causal fairness, which has ignored the fairness implications due to unobserved confounding. In other words, we fill an important research gap in the literature (see our new Table 1). Furthermore, we provide important implications for practice: we call for a more cautious use of causal fairness since, due to unobserved confounding, important fairness issues may still be present, because of which fairness in practice can be undermined. As a remedy, we contribute for the first time a framework that can remove potential issues in causal fairness due to unobserved confounding. We thus expect that our findings, as well as our framework, are of immediate practical importance.\n\n*How is our framework novel compared to sensitivity analysis?* While we adopt a sensitivity analysis as the basis for deriving our bound, we add over sensitivity analysis in several ways. (1) We derive new bounds that are tailored to our bounds. The bounds do not directly follow from the sensitivity analysis (which is unlike other applications of sensitivity models in the literature); instead, a tailored derivation is needed that is non-trivial. (2) We develop an end-to-end framework for fair prediction models. As such, Steps 1 and 2 in our framework are novel and make large contributions to existing sensitivity analysis. \n\n**Actions:** We improved our work in the following ways to clarify our novelty and why our contributions are **non-trivial**: \n\n* We included a new **Table 1**, where we highlight the novelty of our work over existing literature streams. Importantly, ours is the first work to study causal fairness under unobserved confounding. \n* We revised the presentation of our framework to explain why existing bounds from sensitivity models are **not** applicable (as our causal query is in ladder three and not in ladder two of Pearl\u2019s causality ladder). As a result, we state clearly why our bounds are non-trivial (see revised **Section 2**).\n* We added a technical background explaining the theoretical differences between existing bounds from sensitivity models (=ladder two) and our causal query (=ladder three). Thereby, we motivate why a new derivation is needed (see new **Supplement C.3**).  \n\n\n\n\n**2)  \tBroader scope across fairness notions:**\n\nThank you for this important suggestion. Our framework is specially designed to incorporate path-specific effects. Nevertheless, we want to emphasize that our framework is general and can also be applied to other fairness notions. To demonstrate that, we added a new **Supplement F**, where we **derive bounds for other notions**. Thereby, we demonstrate how our framework can be applied to further fairness notions, which shows the broad applicability of our framework.\nThank you as well for pointing out the ambiguity of the name  \u201cpath-specific causal effects\u201d. We rephrased the notation in our paper and now refer to the effects from Zhang & Bareinboim (2018) as \u201cpath-specific causal fairness\u201d.\n\n**Action:** We have clarified throughout the paper that our framework is general and can also be applied to other fairness notations. We also added new theoretical results for other fairness notions (see our new **Supplement F**)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700402967384,
                "cdate": 1700402967384,
                "tmdate": 1700402967384,
                "mdate": 1700402967384,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mfnmknfdXl",
                "forum": "DqD59dQP37",
                "replyto": "zl8ncfQ6pO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer uvD6 (continued)"
                    },
                    "comment": {
                        "value": "**3)  \tExtended literature review:**\n\nThank you. We **extended our literature overview** and now provide a  comprehensive review of prior literature on sensitivity analysis on causal effects to unobserved confounding, such as the marginal sensitivity model. In particular, we added the following materials: \n* We added a comprehensive literature overview on sensitivity analysis (see our new **Supplement C.2**). Therein, we review existing works but also discuss that these are mainly focused on causal effects and not causal fairness. To the best of our knowledge, there is **no** work other than ours that has combined sensitivity analysis and causal fairness. As we lay out, our paper is the **first** to make predictions wrt. causal fairness under unobserved confounding. \n* We added a **discussion of the strengths and weaknesses** of various sensitivity models. Thereby, we highlight the reasons for adopting the GMSM in our work (see our new **Supplement C.2**). \n\n\n\n**4)  Enhanced experimental analysis:**\n\nThank you. We are more than happy to expand our experimental analysis. We thus included several **new experimental analyses** as follows: \n* We report the prediction performance in **Supplement G**. (Due to space constraints, we do so in the supplements and not in the main paper.)   \n* We further show the robustness of our framework. For this, we added additional experimental results on the performance of our fair classifier when trained with different sensitivity parameters (see our new **Supplement I**).\n\n\n\n**5)  \tContinuous variables:**\n\nThank you. We improved our paper to show that our framework is also effective for continuous variables: \n* We clarified that the features (confounders) are **not restricted** to discrete variables.  Instead, we now explain that our framework is **applicable to multiple features that can be binary, continuous, and/or discrete**. Therefore, our method is also applicable to multiple continuous and, thus as well to high-dimensional features (see our revised **Section 3**). \n* We provide **new experimental results** to demonstrate the effectiveness of our framework for multiple continuous variables (see new **Supplement J**).\n* We also discussed an extension of our framework for continuous mediators (see new **Supplement J**).\n\n\n\n# Response to question\n\n**Response to \u201cBenefits of the GMSM\u201d**\n\nThank you for bringing up this important question. We added a **discussion of the strengths and weaknesses** of various sensitivity models. Thereby, we highlight the reasons for adopting the GMSM in our work (see our new **Supplement C.2**)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403258362,
                "cdate": 1700403258362,
                "tmdate": 1700417726213,
                "mdate": 1700417726213,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gYetCEZSDC",
                "forum": "DqD59dQP37",
                "replyto": "zl8ncfQ6pO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_uvD6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_uvD6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "I thank the authors for responding to all the comments, which addresses some of my concern. \n\nI acknowledge that effectively implementing the sensitivity model necessitates the simplification of causal fairness notions, as defined in Pearl's framework, from level 3 to level 2. This process indeed requires amount of work and careful consideration.\n\nHowever, as Reviewer rYvN propose, the equation and algorithms is still unclear. This work can be improved from many aspects. To be more clear, the paper structure can be further improved. I hope my comments are helpful for the revision."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652024159,
                "cdate": 1700652024159,
                "tmdate": 1700652448444,
                "mdate": 1700652448444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vIHe3Tn4Le",
                "forum": "DqD59dQP37",
                "replyto": "zl8ncfQ6pO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_uvD6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_uvD6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks."
                    },
                    "comment": {
                        "value": "Thanks for the authors' careful responding to all the comments. I will raise my score.\n\nHowever, I still think it is not suitable to claim the contributions on causal fairness in the main body (and title), since the paper actually is confined to specific causality-based fairness notions based on counterfactual direct effect (Ctf-DE), indirect effect (Ctf-IE) and spurious effects, instead of the general causal fairness notions. Although the authors added a new Supplement F, deriving bounds for fair on average causal effect and a simple path-specific causal fairness, it does not encompass all causal fairness notions, like counterfactual fairness or path-specific counterfactual fairness. It seems the authors mix the use of some causal fairness notions sometime. For example, it is also confusing to change 'path-specific causal effect' to 'path-specific causal fairness', since actually path-specific causal fairness is defined on path-specific causal effect and they are the same thing in the essence. To avoid confusion and overclaim, I think it is better to reorganize the structure of the paper. \n\nIn conclusion, I acknowledge the importance of the performing sensitivity analysis in the causal fairness area, however I still do not think this paper is ready to be published and once again remark that I hope to see an improved version of your work published in the future."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708342885,
                "cdate": 1700708342885,
                "tmdate": 1700717540287,
                "mdate": 1700717540287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HK3kPXqQ95",
                "forum": "DqD59dQP37",
                "replyto": "15DvkzF63J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_uvD6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_uvD6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks. I still think that the paper could benefit from more focused organization to enhance its clarity and coherence. \n\nAdditionally, regarding the page limit issue, it's a concern. I downloaded the original draft, which even violates the page limit. Adhering to submission guidelines, including page limits, is crucial for maintaining fairness in the review process. I do not think this paper could be accepted in this situation."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742666178,
                "cdate": 1700742666178,
                "tmdate": 1700742666178,
                "mdate": 1700742666178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "48OBFOi0kd",
            "forum": "DqD59dQP37",
            "replyto": "DqD59dQP37",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3659/Reviewer_2FkR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3659/Reviewer_2FkR"
            ],
            "content": {
                "summary": {
                    "value": "The paper establishes the partial identification bound for causal fairness under GMSM, which can be used for learning causally fair predictors under specified GMSM and SCM. Authors provide theoretical guarantee for the partial identification bound and demonstrate its benefit using synthetic and real-world data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I find the paper interesting and easy to read. It establishes the partial identification bound for causal fairness under GMSM, which can be used for learning causally fair predictors under specified GMSM and SCM. \n\nI believe the problem studied is important and has important practical value and the paper is theoretically sound and made a step toward better estimation of causal fairness."
                },
                "weaknesses": {
                    "value": "The motivation and solution seem disconnected. Authors suggest practitioners can audit the unobserved confounding in the data in the introduction, but the solution using GMSM requires prior knowledge of UC strength. In this case, we need to know the unobserved confounding strength in order to estimate or optimize the causal fairness. \n\nThe novelty is relatively low. The causal fairness and the partial identification bound under MSM/GMSM is extensively studied, the paper simply combined them. \n\nAuthors did not report accuracy measures in the paper, which should also be reported. I assume such fairness constraint would impact the prediction accuracy a lot. \n\nA natural criticism is the method is restricted to discrete features, while in practice, variables like crime history may be highly contextual (videos, images, text, ..) . Can authors discuss the difficulty in extending it to high dimensional features? \n\nCan authors explain more on \"Fairness bounds in non-identifiable settings\": why general non-identifiability in these papers does not encompass the unobserved confounding case? Nabi & Shpitser, 2018 also uses unobserved confounding as one of the examples. \n\nTypo: abstract, sources of unobserved confounding? \"ours is the first work to study causal fairness under observed confounding.\" miss-specification. \n\nWriting: Why repeat the contribution at the end of section 1. The research gap seems to repeat contribution again and such gap is already mentioned before."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3659/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3659/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3659/Reviewer_2FkR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3659/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779277667,
            "cdate": 1698779277667,
            "tmdate": 1699636322034,
            "mdate": 1699636322034,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kx1viTVPJ9",
                "forum": "DqD59dQP37",
                "replyto": "48OBFOi0kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 2FkR"
                    },
                    "comment": {
                        "value": "Thank you for your positive evaluation of our paper! We took all your comments at heart and improved our paper accordingly.\n \n**Response to (1):** Connection of motivation and solution\n\nWe thank the reviewer for the question and are happy to clarify the connection between our motivation and the proposed solution. In practice, knowledge of the unobserved confounding (UC) strength is beneficial but not completely necessary. Importantly, our framework is of direct help in practice when (1.) the maximum UC strength is known or (2.) unknown. We discuss both use cases below:\n\n1. There are often good reasons why the UC strength is known or why it can be upper-bounded. Many fairness-relevant applications are rooted in social science, where there is a good understanding of potential causes for biases (e.g., around gender, age, etc.). Hence, it is often reasonable to make assumptions that the UC strength is in a specific relationship with some other observed variable [1, 2, 3, 4], e.g., not as strong as (a multiple of) the observed variable. Notwithstanding, several approaches have been proposed on how sensitivity parameters can be chosen in practice. Here, we refer to the discussions in [5, 6, 7, 8].  \n2. Even when the UC strength is unknown, our framework is of significant value in practice. In this case, practitioners can employ our bounds to audit the unobserved confounding in the data. To do so, one can use our framework to calculate bounds for increasing sensitivity parameters until the interval defined by the bounds contains a certain value of interest, e.g., zero (indicating complete fairness). The minimal sensitivity parameter to achieve this goal corresponds to the level of unobserved confounding, which would be necessary to consider the data fair. We can also view the sensitivity parameter as our uncertainty about the fairness of our prediction model. Smaller sensitivity parameters not achieving the goal still provide information about the sign, i.e., direction, of the unfair effect [6]. Generally, testing various sensitivity parameters is a common technique in real-world applications of sensitivity analysis [5, 9, 10]. In sum, even when the UC strength is unknown, our framework can thus be of large practical value.\n\n**Actions:** We have carefully revised our paper in the following ways:\n* We have improved our motivation to clarify the connection to our solution framework (see revised introduction).\n* We added a discussion on the practical considerations from above (see new **Supplement B.2**)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401080104,
                "cdate": 1700401080104,
                "tmdate": 1700401854017,
                "mdate": 1700401854017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "junliRKT8j",
                "forum": "DqD59dQP37",
                "replyto": "LjzQq7gyEM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_2FkR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_2FkR"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank authors for the detailed feedback. I think authors address my concern on the differences about previous work and contribution. \n\nOn the motivation, I think we have different understanding of the term \"auditing\". When the authors mentioning auditing unobserved confounding, my initial understanding is the method can infer when unobserved confounding happens. Based on the rebuttal, I can see that the authors means auditing the minimum level of unobserved confounding needed to achieve a specified counterfactual fairness level. This form of auditing is unusual (to me) and I still don't understand why it is useful in practice, especially when we do not know the actual unobserved confounding level (when we know, there is also no need for this kind of auditing), I hope authors can be more clear in the final version about its definition and merit practically. \n\nAnother weakness from the additional experimental results is the fair robust algorithm has a very poor predictive performance in terms of prediction accuracy. In the new result in supp G that authors did not report in the first draft, algorithms are close to random guessing in some settings. While it is expected that the accuracy would drop, it is still of concern that it is just as good as random guessing. \n\nOverall, I still think the paper is correct and clear and address an important problem and I will keep my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702511293,
                "cdate": 1700702511293,
                "tmdate": 1700702511293,
                "mdate": 1700702511293,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E4GDjomUM0",
            "forum": "DqD59dQP37",
            "replyto": "DqD59dQP37",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3659/Reviewer_rYvN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3659/Reviewer_rYvN"
            ],
            "content": {
                "summary": {
                    "value": "The paper integrates a recent framework for sensitivity analysis within a common causal fairness family."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is nicely laid out and mostly easy to follow. The problem is important, as sensitivity analysis plays a substantive role in a variety of causal estimands. End-to-end learning to produce fair classifiers is presented, illustrating that sensitivity analysis doesn't need to take place just for standard causal estimands such as average causal effect."
                },
                "weaknesses": {
                    "value": "May main comment is to what extent this mostly is a direct application of Frauen et al. (2023). If I understood it well, is the main technical innovation the use of Lemma 1 in the context of GMSM? Or is this even more closely related to the cited paper?\n \nI\u2019m confused by Eq. (11): there is an expectation over $Y$. Why is this form of aggregation sensible? I\u2019d expect that we would like to enforce the constraint uniformly over all possible values in the same sample space of $Y$ instead of  performing any sort of probabilistic averaging. Speaking of which, which distribution are we marginalizing over, is this the marginal distribution of $Y$? What happens to $a_i$, $a_j$ from (1)-(3)?\n \nMaybe this is well-explored in the causal fairness literature, but the switch between \u201creal Y\u201d and $f_\\theta$ in the training procedure in Section 5 is very confusing: we design $f$, so any confounding between \u201c$Y$ \u201c (that is, $f_\\theta$) vanishes by design. Why would we consider any sort of sensitivity parameter for \u201c$Y$\u201d?\n \nI\u2019m worried about the theoretical results. In Appendix D.3 the authors state that they \u201cextensively use the following corollary\u201d, which boils down to Eq. (33). This equation refers to \u201c$P(X |do(A=a), A\\neq a)$\u201d. The event past the conditioning bar has probability zero, as the control signal $do(A = a)$ implies $A = a$ (yes, we can condition on measure-zero events, that\u2019s what we do for continuous variables, but even then we take for granted the careful textbook measure-theoretical characterization of this conditional that we implicitly accept by default, but that\u2019s a non-trivial (and not unique!) characterization of conditioning. But  (33) doesn\u2019t appear to make sense even for discrete $A$). I may be missing something obvious, in which case I will appreciate a clarification \u2013 if this indeed \u201cfollows directly from basic probability theory\u201d I\u2019m happy to be taught the baby steps explicitly\u2026  Of course we can say that \n\n$$P(X | do(A = a)) = P(X(a)) = P(X(a) | A = a)P(A = a) + P(X(a) | A \\neq a)P(A \\neq a) = $$\n$$P(X | A = a)P(A = a) + P(X(a) | A \\neq a)P(A \\neq a) , $$\n\nbut $P(X(a) | A \\neq a)$ is not the same thing as $P(X | do(A = a), A \\neq a)$. Among other things, $X$ is not $X(a)$ when $A \\neq a$, but it is undefined what $X$ even means under regime $do(A = a)$ and event $A \\neq a$."
                },
                "questions": {
                    "value": "This is. summary of the above.\n\n- What is the novelty?\n\n- Why averaging over $Y$ in Eq. (11) makes sense (and which distribution are we averaging over)?\n\n- Is there any point in prescribing sensitivity analysis between $Y$ and $A$, given that $Y$ is not involved in the prediction?\n\n- Explaining better the meaning of Eq. (33) and where it is used.\n\nI would also would like to know whether any path-specific effect could be tackled and not only the three default ones."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3659/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3659/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3659/Reviewer_rYvN"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3659/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699808816357,
            "cdate": 1699808816357,
            "tmdate": 1700691671889,
            "mdate": 1700691671889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yiqSWdRBPf",
                "forum": "DqD59dQP37",
                "replyto": "E4GDjomUM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer rYvN"
                    },
                    "comment": {
                        "value": "Thank you for your very constructive and detailed review of our paper! We took all your comments at heart and improved our paper accordingly.\n\n \n**Response to (1):** Difference from Frauen et al (2023)\n\nThank you for giving us the opportunity to emphasize the novelty of our framework. Thereby, we clarify that we do **not** have a simple application of Frauen et al. (2023) but that our derivations are **non-trivial**. \n\n*Why is the derivation of our bounds non-trivial?* We have derived tailored bounds for counterfactual fairness under unobserved confounding. Importantly, our bounds require a novel and careful derivation. In other words, our bounds in Theorem 1 are **not** just a simple result of applying Frauen et al. (2023) but require a new derivation tailored to our setting. The reason is the following: Causal fairness notions commonly contain nested counterfactuals and are thus located on **level three** of Pearl\u2019s ladder of causality. The GMSM in Frauen et al. (2023) incorporates single-intervention counterfactuals and is thus located on **level two**. Because of that, existing bounds from sensitivity models are **not** applicable. In other words, we can **not** simply adopt existing bounds from sensitivity models, but we need a **new derivation** for level three of Pearl\u2019s causality ladder.\n\n*What is our contribution?* We use the GMSM from Frauen et al (2023) primarily to formalize our setting, while the actual derivation for our bounds is new. Hence, our derivation does not simply follow from the GMSM but is non-trival (i.e., we only use the GMSM from Lemma 1 to formalize our setting while our Theorem 1 and its derivation is new). Our main contributions are beyond the GMSM. (1) We transfer concepts from sensitivity analysis to causal fairness literature. This is a major difference from the existing literature on causal fairness, which has ignored the fairness implications due to unobserved confounding. As such, we fill an important research gap in the literature (see our new **Table 1**). (2) We propose a new neural framework to assess causal fairness under unobserved confounding and learn robust predictions. Our is the first framework to mitigate fairness violations from unobserved confounding so that still fair predictions are made. (3) We offer new insights for practice: one of our implications is that there can be important fairness uses in practice due to unobserved confounding, because of which fairness in practice can be undermined. We thus expect that our findings, as well as our framework, are of immediate practical importance.\n\n*How is our framework novel compared to sensitivity analysis?* While we adopt a sensitivity analysis as the basis for deriving our bound, we add over sensitivity analysis in several ways. (1) We derive new bounds that are tailored to our bounds. The bounds do not directly follow from the sensitivity analysis (which is unlike other applications of sensitivity models in the literature); instead, a tailored derivation is needed that is non-trivial. (2) We develop an end-to-end framework for fair prediction models. As such, Steps 1 and 2 in our framework are novel and make large contributions to existing sensitivity analysis. \n\n**Actions:** We improved our work in the following ways to clarify our novelty and why our contributions are **non-trivial**: \n1. We spell out clearly that our setting is grounded in Frauen et al. However, our derivations are new and non-trivial (see revised **Section 2**). We further explain why existing bounds from Frauen et al. are **not** applicable (as our causal query is in ladder three and not in ladder two of Pearl\u2019s causality ladder). \n2. We added a technical background explaining the theoretical differences between existing bounds from sensitivity models (=ladder two) and our causal query (=ladder three). Thereby, we motivate why a new derivation is needed (see new **Supplement C.3**).\n3. We included a new **Table 1**, where we highlight that our work is orthogonal to sensitivity analysis and that we make important contributions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400234922,
                "cdate": 1700400234922,
                "tmdate": 1700400234922,
                "mdate": 1700400234922,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HdB7YZNp8I",
                "forum": "DqD59dQP37",
                "replyto": "E4GDjomUM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Response to (2):** Eq. (11)\n\nThank you. We have now improved the notation in Eq. (11). Note that the prediction model returns **class probabilities** for classification or the **delta distribution** (point distribution) for regression tasks. Therefore, $P(Y)$ does not correspond to the true distribution of the outcome variable $Y$ but rather to the conditional distribution over the prediction outcomes $P(\\hat{Y} \\mid z, m, a)$, where $\\hat{Y} \\mid z, m, a$ defines the prediction outcome, i.e., the predicted distribution of $Y$, for a given realization of  $Z, M$ and $A$. To clarify this, we have updated our notation accordingly and provided further clarifications in the text.\n\nTaking the **expectation** over the class probabilities or the delta distribution is **a natural approach** in binary classification and regression problems. For multiclass classification, other distribution quantiles are separate constraints per output class might also be reasonable depending on the specific requirements of the use case.\n\n**Action:** We have revised Eq. (11) along with the suggestions in your comment.\n\n\n\n\n\n**Response to (3):** Sensitivity analysis between $A$ and $Y$\n\nThank you for your question. Indeed, it is **not necessary to perform sensitivity analysis** on the relationship between **$A$ and $Y$** since the confounding on the relationship vanishes by design. This means that we do not need to define a sensitivity model for $Y$, i.e., we do not need to specify $\\Gamma_Y$. Nevertheless, we still **need a sensitivity model for $M$** to obtain bounds $P^+(m \\mid a, z), P^-(m \\mid a, z)$ for $P(m \\mid a, z)$.\n\nThe **sensitivity models** that need to be specified **do not directly map** to the three path-specific causal **fairness effects** (direct, indirect, spurious). They only provide bounds on a given probability. Since all three effects contain $P(m \\mid a, z)$, we obtain bounds for all effects, not only for the indirect effect.\n\nIf we want to assess the **dataset fairness**, we need to bound $P(y \\mid a, z, m)$ and $P(m \\mid a, z)$. Therefore, in this case, we need to specify **two sensitivity models**, i.e. one with $\\Gamma_M$ and one with $\\Gamma_Y$.\n\n\n**Response to (4):** Eq. (33)\n\nThank you for the suggestion on how to improve our notation (previous: Eq. 33; now Eq. 32). We did not intend to condition on measure-zero events but to **intervene on conditional events**, i.e., set the sensitive attribute to $a_j$ by intervention for events in which the realization was $a_i$. We have thus revised our notation. Reassuringly, we emphasize that all subsequent applications of the equation were correct.\n\n**Action:** We have **rewritten** the equation (now: Eq. 32, previous: Eq. 33) in terms of counterfactual expressions to avoid misleading notation. \n\n\n\n\n**Response to (5):** Applicability to other path-specific effects\n\nThank you. Our framework is specially designed to incorporate path-specific effects. Nevertheless, we want to emphasize that our framework is general and can also be applied to other fairness notions. To demonstrate that, we added a new **Supplement F**, where we **derive bounds for other notions**. Thereby, we demonstrate how our framework can be applied to further fairness notions, which shows the broad applicability of our framework.\n\n**Action:** We have clarified throughout the paper that our framework is general and can also be applied to other fairness notations. We also added new theoretical results for other fairness notions (see our new **Supplement F**)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400651265,
                "cdate": 1700400651265,
                "tmdate": 1700400728019,
                "mdate": 1700400728019,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "02TvCzMcce",
                "forum": "DqD59dQP37",
                "replyto": "E4GDjomUM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_rYvN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_rYvN"
                ],
                "content": {
                    "title": {
                        "value": "Thank you, and a follow up"
                    },
                    "comment": {
                        "value": "Thank you, while I do think that my statement that the main idea is the application of Lemma 1 to the existing bounds of GMSM was correct (Remark 1 is basically the main idea, right?), I acknowledge that by looking at the derivation in the appendix in more detail I agree that it still required quite some work. Even digesting and presenting Correa et al. into this context is a non-trivial effort, which I recognize.\n\nIt's frustrating that the original pdf is not available anymore (or maybe I'm just incompetent in trying to find it), but wasn't Eq. 11 just $\\mathbb E[Y]$ originally? My main point was that averaging over $A$ and $M$ doesn't seem to make sense to me (imagine that we want to optimize some $\\mathbb E[f(Y, A, M)]$ where we need $f(y, a, m)$ to be positive for all $a$, $m$, but we only enforce $\\mathbb E[f(Y, A, M)]$ to be positive...). The \"max\" in the new Eq. 11 is ambiguous. Can you clarify what is this a maximum over what? All $m, a$? (across $CF+$ and $CF-$). I'm not sure whether the iterations in Algorithm 1 are doing that."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492910560,
                "cdate": 1700492910560,
                "tmdate": 1700492951237,
                "mdate": 1700492951237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9INi5wPK7B",
                "forum": "DqD59dQP37",
                "replyto": "Blels3f3hp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_rYvN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_rYvN"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Thanks for the follow up, but I'm afraid this is still not clear. For concreteness, let's say: \n\n* we have some input $X$, output $Y$ all discrete\n* the model is a full distribution of $Y$ given $X$ parameterized by some $\\theta$\n* the loss is negative log-likelihood\n* we want the functional $g(p_{y|x}(\\cdot | \\cdot))$ to be uniformly bounded by some $\\gamma$. \n\nThen what I would write (in population, with the true pmf being $p_0(y | x)$) is\n\nMinimize (with respect to $\\theta$)  $$l(\\theta) = -\\sum_{y, x}p_0(y, x) \\log p_\\theta(y | x)$$\nsubject to $$\\forall y, x |g(p_\\theta(y | x))| \\leq \\gamma $$\n\n(the above is the explicit way of declaring $max |g(p_\\theta(y | x))| \\leq \\gamma $. Don't we need a different Lagrange multiplier for each possible value of $x$ and $y$ in their domain if you are indeed using $L_\\infty(|g(p_\\theta(\\cdot | \\cdot) - \\gamma|)$ as your constraint?\n\nI don't see how Algorithm 1 is doing the equivalent of this at all: the max function in line 6 is unclear (a maximum over the sample?), it's not differentiable and it's not clear whether the dependency of $\\theta_l$ is carried over to line 8.\n\nDo the Langrange multipliers you get end up being (close to) zero in the end in the optimization?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572316953,
                "cdate": 1700572316953,
                "tmdate": 1700572316953,
                "mdate": 1700572316953,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "daN2qYmTOH",
                "forum": "DqD59dQP37",
                "replyto": "E4GDjomUM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to follow-up 2"
                    },
                    "comment": {
                        "value": "Thank you for seeing our paper in a good light. We welcome your follow-up questions and are more than happy to answer them below. We acknowledge that our notation was ambiguous ($Y\\mid m,a,z$) and hope that our **responses** and **updates in our manuscript** can address your questions and concerns successfully. Specifically, we introduced the notation of expected fairness effects ($\\mathrm{CF}\\_{\\mathbb{E}}$) to clarify the notation.\n\n**General clarifications:**\nAll fairness effects are defined for specific realizations of $Y$ and $A$, e.g., $DE_{a_i, a_j}(y|a_i)$. For calculating the bounds, we replace $P(y|m,a,z)$ through our prediction outcome for $f(m,a,z)$. Although $P(y|m,a,z)$ is a random variable (depending on $y$) for deriving standard fairness bounds when training the prediction model, it is a non-random variable, independent of $y$, i.e., $f(m,a,z)$.\nThen, following the definition of the bounds given in Thm.1, we average over $M$ and $Z$ and finally obtain a non-random quantity that only depends on the realization of $A$. We denote these quantities as \n$\\mathrm{CF}\\_{\\mathbb{E}}^+(a_i, a_j)$ and $\\mathrm{CF}\\_{\\mathbb{E}}^-(a_i, a_j)$ for $\\mathrm{CF} \\in {\\mathrm{DE}, \\mathrm{IE}, \\mathrm{SE}}$. We employ a notation containing the expectation, to explicitly show the non-randomness and the independence of $y$.\n\nIn the following, we outline further details on the derivations for each of the tasks binary classification, regression and mutli-class classification separately. We will switch from the notation used in our manuscript to the notation used in your previous question, i.e. random variables $X$,$Y$ below.\n\nFirst, let\u2019s take a look at **binary variables** $X,Y$, which also corresponds to our implementation in the experiments on synthetic data. For each of the three different fairness effects, we are then only interested in, e.g., $\\mathrm{CF} (Y=1 \\mid X=1)$. Due to symmetry reasons for binary $X$ (e.g., discrimination of women compared to men is symmetric to discrimination of men compared to women), it is sufficient to focus on one realization of $X$.\nIn our case, we thus have one constraint per $\\mathrm{CF}$, i.e., three constraints and thus three Lagrange multipliers in total.\n\nNext, let us consider the **regression** setting, i.e., $Y$ continuous, $X$ binary, which corresponds to our real-world study. Here, it is reasonable to consider the expectation over $Y$. We are thus left again with three constraints in total (one per $\\mathrm{CF}$ ).\n\nFinally, let\u2019s consider **multi-class classification**. Now there are two options of defining constraints to optimize over. One option is (as you noted in your question) to treat each $\\mathrm{CF}$ for each possible realization of $Y$ and thus minimize wrt. $3 \\times |\\mathcal{Y}|$ constraints. Although this is feasible for binary classification, it can be highly challenging for large $|\\mathcal{Y}|$. Therefore, an alternative option is to constrain the expected $\\mathrm{CF}$  over all realizations of $Y$. The particular choice of which notion to constrain is up to the practitioner in real-world use cases. We **added a clarification** regarding this choice in our **updated manuscript** and **updated Algorithm 1** to spell this out more explicitly. \n\nThe **max function** is then applied following Eq. (11), i.e., on **each constraint separately** (=which is also what you stated in your question). Each max function only takes the maximum of the absolute value of the upper or lower bound of one specific $\\mathrm{CF}$ (either averaged over all outcomes of $Y$ or one per outcome separately). Therefore, there are *# constraints*-many max functions and, thus,  *# constraints*-many Lagrange multipliers. Overall, $\\lambda, c$ and $\\gamma$ in the pseudo-code are **vectors** of dimension *# constraints*.\n\nThe function $max(x,y)$ is differentiable unless $x=y$. In this case, one can observe that the subdifferential of $max(x,y)$ at $(x,y)$ is given by the convex hull of $[(1,0), (0,1)]$. In general, choosing any subgradient at $(x,y)$ circumvents the differentiability problems.\nTo furthermore ensure computational stability, each of the gradients wrt. $x$ or $y$ is best set to 0.5, i.e., the element in $[(1,0), (0,1)]$ with the smallest norm. Common deep learning libraries directly implement this rule in the provided functions.\n\n**Action:**\nUpon reading your question, we realized that we should clarify the algorithm in our paper. To this end, we have **updated Algorithm 1** in our paper and added a **further discussion** in **Supplement E.4**. In our updated algorithm, we now state the different multipliers more explicitly. Furthermore, we have now specified the max operation in line 6 more rigorously. \n\nWe are again thankful for your question, which helped us in improving our presentation. We hope that our answer helped in addressing your question satisfactorily. Please let us know if there are further things in our presentation that should be improved."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616244586,
                "cdate": 1700616244586,
                "tmdate": 1700661844377,
                "mdate": 1700661844377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ToziaPB80E",
                "forum": "DqD59dQP37",
                "replyto": "daN2qYmTOH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_rYvN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Reviewer_rYvN"
                ],
                "content": {
                    "title": {
                        "value": "Helpful"
                    },
                    "comment": {
                        "value": "This was helpful, thank you (I'd imagine the discussion would be much easier face-to-face...)\n\nI'd add a few things though:\n\n* $P(y | m, a, z)$ is not a random variable, it's the evaluation of a function on four constants $y, m, a, z$. $P(Y | m, a, z)$, $P(y | M, a, z), ..., p(Y | M, A, z), P(Y | M, A, Z)$ etc., those would be random variables as they are functions evaluated *at* random variables. (But then again I'm not sure why this is relevant...)\n\n* in the regression setting, if only you care about the expected value of $Y$, that's fine. But that's not what Eq. (8), (9), (10) are (they define whole functions over the space of $y$, which is confusing). Please make sure to fully spell out where these changes between whole functions and expected values take place.\n\n* when I mentioned $max$, I was referring to the usual mathematical programming formulation where we have $d$ constraints $g_i(x) \\leq b_i$, where the intersection can be summarised as $max_{i = 1, 2, \\dots, d} [g_i(x) - b_i] \\leq 0$. This is far more complex than a simple $\\max(x_1, x_2)$ for some unrelated pair $x_1, x_2$ of variables, there is a reason why it took a while for a practical polynomial time algorithm for linear programming to be discovered. We would typically not clump all of these constraints in one single Lagrange multiplier and differentiate through $x$.\n\nIn any case: I can see where you are going and I think what you are doing is probably correct in your implementation although I still think presentation needs work. Please do give another pass in an eventual published version of the paper and release your code, everyone will benefit (including yourselves). I believe you have shown you are able to do that. I won't raise further questions and I will raise my score.\n\nThanks again."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691638381,
                "cdate": 1700691638381,
                "tmdate": 1700691638381,
                "mdate": 1700691638381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kPxQQSgPYG",
                "forum": "DqD59dQP37",
                "replyto": "E4GDjomUM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to follow-up \"helpful\" by reviewer rYvN"
                    },
                    "comment": {
                        "value": "Thank you again very much for the helpful discussion on the mathematical representation of our framework. We highly value your comments and input, which have significantly improved the presentation in our paper.\n\nWe will carefully revise the paper following your suggestions and make sure to fully spell out all changes between whole functions and expected values. In particular, for an eventual camera-ready version, we will revise the complete notation in our paper again to ensure consistency. If you have any additional remarks or questions about the notation, we are more than happy to incorporate them in a revised version of the paper.\n\nWe also have released the code in an anonymous GitHub repository (https://anonymous.4open.science/r/FairSensitivityAnalysis_anonymous-0DA2/README.md / the link is also in the paper) to preserve anonymity. Upon acceptance, we will move our codes to a public GitHub."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740254296,
                "cdate": 1700740254296,
                "tmdate": 1700740637005,
                "mdate": 1700740637005,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]