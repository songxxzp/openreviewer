[
    {
        "title": "Enhancing Adversarial Robustness on Categorical Data via Attribution Smoothing"
    },
    {
        "review": {
            "id": "xZAhi8IH0z",
            "forum": "0JnaN0Crlz",
            "replyto": "0JnaN0Crlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_rFyD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_rFyD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a regularization method to enhance the robustness of classification over categorical attributes against adversarial perturbations. The paper establishes an information-theoretic upper bound on the expected adversarial risk and proposes an adversarially robust learning method, named Integrated Gradient-Smoothed Gradient (IGSG)-based regularization, which designed to smooth the attributional sensitivity of each feature and the decision boundary of the classifier to achieve lower adversarial risk, desensitizing the categorical attributes in the classifier. The paper conducts extensive empirical study over categorical datasets of various application domains to confirm the effectiveness of IGSG and achieve new start-of-arts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper has good originality, high quality and clear expression. The paper proposes a new regularization method which outperforms adversarial training and generalize well."
                },
                "weaknesses": {
                    "value": "Larger dataset is better to be verified to demonstrate the genelarization of the proposed method."
                },
                "questions": {
                    "value": "1.This paper argues that the proposed method can smooth the decision boundary of the classifier, how about to visualize the decision boundary?\n2.Is there exists obfuscated gradients in the proposed method?\n3.Is the proposed method also works well under other attacks such as deepfool attack, C&W attack?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648296379,
            "cdate": 1698648296379,
            "tmdate": 1699636430546,
            "mdate": 1699636430546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "3PyGDlQ1o1",
            "forum": "0JnaN0Crlz",
            "replyto": "0JnaN0Crlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_Bx5T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_Bx5T"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes IGSG, a theoretically motivated robust learning method for categorical inputs. It contains two parts: Integrated Gradient (IG) and Gradient Regularization. Experiments verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The studied problem is interesting, adversarial attack/defense on caterical data is of interest to the community;\n- The experimental results are strong to present the effectiveness of the proposed IGSG method.\n- The paper is well-written and well formatted."
                },
                "weaknesses": {
                    "value": "- Minor issue:\n     - Page 9, \"\u201drobust overfitting\u201d\""
                },
                "questions": {
                    "value": "- I find some Adversarial Training baseline and Regularization baseline methods perform worse than undefended method. Could you please explain the reason?\n- Could IGSG be used or modified to be used in continuous data?\n- Could you please explain more about Figure 3? Why the summing of the attack frequency of IGSG features are lower than that of original features?\n-  What is the meaning of the sign of $IG$? If the sign of insignificant, maybe using the absolute of $IG$ is better.\n- According to Eq.3 and Factor 1, IGSG should base on the adversarial training. However, S is the original dataset used in Eq.5. Why not use the adversarial training as a base?\n- I notice a recently ICML 2023 paper *Probabilistic Categorical Adversarial Attack and Adversarial Training* proposes a gradient-based attack. However, the attack methods you used (i.e. FSGS and OMPGS) are both search-based attack method. Therefore, the improvements against other methods your presented might come from the relatively weak attack, because the adversarial training employed gradient-based attack instead of search-based attack. Could you please show more results on defending the gradient-based attack methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4530/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4530/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4530/Reviewer_Bx5T"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749789839,
            "cdate": 1698749789839,
            "tmdate": 1699636430464,
            "mdate": 1699636430464,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "sNCki9nNS4",
            "forum": "0JnaN0Crlz",
            "replyto": "0JnaN0Crlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_KxJ6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_KxJ6"
            ],
            "content": {
                "summary": {
                    "value": "This work studies adversarial robustness on tabular categorical attributes and establishes an information-theoretic upper bound on the expected adversarial risk."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Provide some theoretical results.\n- Easy to follow."
                },
                "weaknesses": {
                    "value": "- This work develops the theory over randomized learning algorithm $A(\\cdot)$, however, the experiments are deployed on deterministic models. How can these empirical results support the theory?\n- Given $S$, where does the randomness of $A(S)$ come from? Could authors provide a simple case of randomized algorithm that the theory can totally hold?\n- I am familiar with PAC Bayes, which involves a posterior distribution of $A(S)$. Does a posterior distribution of $A(S)$ also exist here? If so, what assumptions are made about $A(S)$? If not, could authors provide the precise definition of $A(S)$?\n- In $I(f,S)$, $f$ and $S$ are random variables, how to use $IG(x)$ with a given deterministic $f$ to influence $I(f,S)$ with a random $f$. We even have no idea with the assumption of random $f$. It is also confused which $f$ is random and which $f$ is deterministic, they are both represented as $f$.\n- The adversarial robustness of tabular categorical attributes is only a minor extension of DNNs adversarial robustness, the contribution seems minor."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756156353,
            "cdate": 1698756156353,
            "tmdate": 1699636430383,
            "mdate": 1699636430383,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "LUdCXSBJov",
            "forum": "0JnaN0Crlz",
            "replyto": "0JnaN0Crlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_iddk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_iddk"
            ],
            "content": {
                "summary": {
                    "value": "The authors address adversarial robustness in the context of categorical variables, a less-explored scenario in the literature. They highlight the key finding that different attack methods yield adversarial samples with varied distributions. Relying solely on PGD-generated adversarial samples for training leads to overfitting and inadequate defense against diverse attacks. Consequently, they opt to bolster robustness in categorical data training through a regularization-based approach.\n\nBy establishing an upper bound on the theoretical risk gap and analyzing factors that mitigate adversarial risk, they propose IGSG\u2014a regularization-based paradigm for robust training with categorical variables. Experimental results across multiple datasets affirm the method's superiority over baselines and traditional adversarial training techniques.\n\nIn summary, the authors introduce IGSG, a regularization-based robust training method, grounded in a theoretical analysis of factors reducing adversarial risk in dealing with categorical variables. Empirical evidence demonstrates its outperformance over baselines and competing adversarial training methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1\u3001The authors address adversarial robustness in the context of categorical variables, an aspect that has received limited exploration in the literature.\n2\u3001The authors present a clear motivation for their work.\n3\u3001The authors substantiate their motivation through explicit theoretical and empirical experiments."
                },
                "weaknesses": {
                    "value": "1\u3001The expression contains ambiguities, such as the specific definition of $G^r\" in equation (5), which needs clarification in the main text. On page six, the authors mention minimizing the \"third and fourth terms\" in the first line, but the subsequent explanations actually refer to optimizing the second and third terms. For the cross-validation of hyperparameters, it would be beneficial for the authors to elaborate on how they performed the training-validation set split using only the training data and specify the chosen values for the hyperparameters.\n2\u3001The experiments appear insufficient, and I suggest the authors supplement the following experiments to bolster support for their method:\n1\uff09While MLP and Transformer serve as baseline models, including more models would validate the generalizability of their method.\n2\uff09Despite utilizing cross-validation for hyperparameter selection, reporting performance with different hyperparameter choices is recommended to assess the sensitivity of their method."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804666375,
            "cdate": 1698804666375,
            "tmdate": 1699636430305,
            "mdate": 1699636430305,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "WqyX2wv9YP",
            "forum": "0JnaN0Crlz",
            "replyto": "0JnaN0Crlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_eEhS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_eEhS"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies adversarial robustness of classification over categorical attributes against adversarial perturbation. An information-theoretic upper bound on the expected adversarial risk has been established, together with an adversarial learning method with integrated gradient-smoothed gradient regularization. Experimental results demonstrate the effectiveness of the proposed training method and the superiority to the state-of-the-art robust training methods in terms of adversarial accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-motivated and well-written. The adversarial robustness on categorical data is an interesting topic, and this paper takes a reasonable approach to tackle related challenges. Some interesting insights are also discussed, and they would be of interest to the community.\n\nThe definition of adversarial risk for categorical data is reasonable. It appears the information theoretic upper bound on the expected adversarial risk is new, and the remarks on the upper bound are sensible. The regularization terms inspired from the bound are reasonable.\n\nThere are some interesting experiments in the appendix, which are good to know."
                },
                "weaknesses": {
                    "value": "It seems there is some gap between the information-theoretic upper bound and the regularization terms. They are weakly linked by some factors implied by the upper bound, but there does not seem there are any direct connection between them. That is, the factors are quite intuitive in such a way that the regularization terms can be designed without knowing the information-theoretic bounds. The mutual information terms just state some weak dependence between random variables, but there is no mention on how to compute/approximate them in the current context. It is unclear how tight the upper bound could be, and the evaluation of the dependence of the bounds against key parameters in practical models and datasets is also missing.\n\nThe proposed training method just puts two regularization term together with some hyper-parameters without explaining why they should be composed in that way. It is unclear how these regularization terms are connected to the mutual information terms in the upper bound."
                },
                "questions": {
                    "value": "1.\tState clearly and explicitly how the information-theoretic upper bound is connected to the regularization terms.\n2.\tEvaluate the upper bound against the key parameters for practical models and datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849518190,
            "cdate": 1698849518190,
            "tmdate": 1699636430243,
            "mdate": 1699636430243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "9EkNt8c5rE",
            "forum": "0JnaN0Crlz",
            "replyto": "0JnaN0Crlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_oxAy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4530/Reviewer_oxAy"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to strength the adversarial robustness of categorical data using attribution smoothing."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- There have been only a few works aiming to enhance the adversarial robustness of categorical data.\n\n- The empirical results look quite convincing.\n\n- The use of IG for discrete data is interesting."
                },
                "weaknesses": {
                    "value": "- The writing needs a significant boost and improvement.  The notions used in this paper are not good and easy to confuse such $b(x)_{i,j,k}$ sometimes $x_{i,j,k}$ for the element $jk$ of the categorical data point $x_i$.\n\n- Theorem 1 has unsolid terms without a careful explanation\n   -  $I(f;S^n)$ with the classifier $f$ and the training set $S^n$, how can you evaluate the mutual information between a classifier and a training set?\n   -  $I(x_{i,\\omega_i}; f)$  with $\\omega_i$ to be the selected feature, however, it is not clear  $x_{i,\\omega_i}$ and how to compute $I(x_{i,\\omega_i}; f)$ because it seems that $f(x_{i,\\omega_i})$ is not valid. Similar doubt for $I(x_{\\bar{\\omega}_i, y_i; f})$.\n\n-  The theories developed and the proposed approach are not really connected."
                },
                "questions": {
                    "value": "I believe this paper has some interesting ideas. However, the presentation and writing need a significant boost, hence it is not ready to publish. \n\nFor questions, please refer to the weaknesses and \n- Can you further explain more the intuition of the term $\\ell_{TV}IG(x_i)$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4530/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877218937,
            "cdate": 1698877218937,
            "tmdate": 1699636430168,
            "mdate": 1699636430168,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]