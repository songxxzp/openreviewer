[
    {
        "title": "Does Progress On Object Recognition Benchmarks Improve Generalization on Crowdsourced, Global Data?"
    },
    {
        "review": {
            "id": "yoE8UFCccK",
            "forum": "rhaQbS3K3R",
            "replyto": "rhaQbS3K3R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6304/Reviewer_NCcf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6304/Reviewer_NCcf"
            ],
            "content": {
                "summary": {
                    "value": "Evaluating ~100 ImageNet models across different architectures, scales and pretraining datasets, the authors show that progress on ImageNet and ImageNet-adjacent generalization benchmarks is faster than on geographically diverse data. The crowdsourced, global data they consider are the DollarStreet and GeoDE datasets (released in prior work). They demonstrate that the geographic disparity (difference between the accuracy on European and African objects) increases as the models get better on ImageNet. They show that different robustness interventions and scaling the models and the datasets offer limited improvements, while training and fine-tuning on more diverse and carefully curated balanced data offers a path forward."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is well-written and easy to understand.\n* Studying the generalization ability and geographic/socioeconomic disparities of computer vision models is an important topic.\n* This work conducts a large-scale evaluation (of ~100 models) on the 2 geographically diverse datasets exploring the trends of ImageNet models on them.\n* There are 2 key findings which I find interesting:\n    * Models progress faster on standardized benchmarks than on the geographic datasets. This makes sense given that the standardized benchmarks are better aligned to ImageNet -- the dataset on which the evaluated models were trained and/or fine-tuned.\n    * Better ImageNet models increase the geographic disparities as measured on the evaluated datasets. This finding was intuitive and understandable, but worth emphasizing and pointing out to me.\n* The authors promise that they will release their code in an easy to use manner (\"with just 4 lines of code\").\n* I appreciate that the authors present a balanced view on the significance of geographic distribution shifts. In particular, they do not see it as an ultimately universal metric but propose it as an additional metric that should be tracked and evaluated."
                },
                "weaknesses": {
                    "value": "While the findings of this work are interesting and worthwhile, I feel that the technical contributions are relatively limited. In particular, I believe that better attribution to concurrent/prior works and highlighting the consistencies with them might be helpful and fair, especially in the cases where the experimental design is inspired by them.\n\n1. The experiment in Sec. 5.1. is consistent with prior/concurrent work: Rojas et al., DeVries et al. and Ramaswamy et al.\n2. The experiment in Sec. 6.3. is also aligned with prior work. Rojas et al. find that training on DollarStreet \"can improve the performance of classification tasks for items from lower-income homes\", while Ramaswamy et al. \"extract features using a ResNet50 model ... and retrain the final layer\".\n\nMeanwhile, I acknowledge that, in contrast to prior work, this paper performs a large scale study (with ~100 models) and examines the effectiveness of fine-tuning on one dataset (DollarStreet) and evaluating on another (GeoDE). However, the benefits of the later are also somewhat limited, as we can see in Table 8 - the results on GeoDE are similar regardless of the portion of DollarStreet training data (going from 10 to 100%).\n\nMinor: sometimes the legends and axes on the figures, e.g., on figs. 2 and 4 and 5 in the appendix are a bit hard to read."
                },
                "questions": {
                    "value": "Q1: I could not find any discussion on the differences between the DollarStreet and GeoDE datasets, while some of the results and the trends on them sometimes differ. Are there any fundamental differences between the two datasets?\n* Why is GeoDE omitted from Table 2. Rate of change on it seems closer to ImageNet-V2 and ImageNet-Sketch for example, compared to rate of change on DollarStreet.\n* Comparing Figs. 4 and 5 in App. B, the accuracies on GeoDE are significantly higher than those on Dollar Street and sometimes even higher than the ImageNet induced benchmarks. Why is that the case? Is GeoDE \"easier\" because there are fewer labels or is there another fundamental reason?\n* Do you have a hypothesis why the geographic disparities are much more consistent (relatively constant with the model improvements on ImageNet) for GeoDE compared to Dollar Street (Figure 8)? Why are the datasets different in that matter?\n\nQ2: I would like to better understand the driving source of the disparities on Dollar Street. Is it the geographic region or the family income? I expect that these two are correlated, but could you please provide any quantitative results about the disparity when also controlling for family income? I.e., instead of comparing the groups Europe and Africa, compare the groups (Europe, some income level) and (Africa, the same income level).\n\nQ3: Is \"Progress Gap\" a fair or meaningful metrics? E.g. (following-up on Q1), geographic datasets may be easier/more difficult, with different labeling biases, etc. Could you please provide a brief discussion what might influence the progress rates / contribute to its differences across datasets and further motivate the usage of the progress gap metric?\n\nQ4: Should we expect the models to have high accuracy on all regions? E.g., would fine-tuning w.r.t region deployment be a good idea in certain use cases? How do you envision the models should be deployed in practice? If we look at the problem from fairness perspective, it has been well-known that there exists a trade-off between fairness and accuracy in general, so it might make sense to have different models for the different regions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6304/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772830013,
            "cdate": 1698772830013,
            "tmdate": 1699636692573,
            "mdate": 1699636692573,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iJT2LVgQHK",
                "forum": "rhaQbS3K3R",
                "replyto": "yoE8UFCccK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6304/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6304/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your appreciation of our work, and thoughtful questions. We are glad you found our work to address an important topic, containing interesting and important findings, providing a balanced view, and well-written. We hope to have addressed your questions, and welcome any additional questions or suggestions on how we can strengthen our work: \n\n**Difference Between DollarStreet and GeoDE and Figure 8**:\n\nThere are a few main drivers of the dataset differences from our perspective: \n1. DollarStreet is more diverse in terms of geography and household income. DollarStreet was curated to explicitly capture a broad set of households, sending photographers to over 63 countries to people\u2019s homes, and selecting households with a variety of incomes. GeoDE was crowdsourced among internet users from 19 countries and was designed to show the importance of crowdsourced data compared to internet scraped data.\n2. GeoDE has a 1-to-many label mapping, whereas DollarStreet has a 1:1 label mapping. As shown in Appendix A.1 , GeoDE classes are coarser as there are GeoDE classes that map to as many as 25 ImageNet classes, while DollarStreet has 1 ImageNet label for each DollarStreet label. This makes GeoDE an easier dataset than DollarStreet for ImageNet models (we also evaluate a large number of zero-shot models that use the ground truth labels, without requiring mapping).\n\nDespite the large differences in dataset curation and difficulty, we find very similar results on DollarStreet and GeoDE, indicating that the problems we discover are not specific to a given dataset curation, or labeling. We find that both datasets have large geographic disparities, even with SOTA models. Most critically, we find for both benchmarks that disparities are not resolved by progress on standard generalization benchmarks, dataset scaling, standard robustness interventions, or architecture scaling.\n\n**GeoDE in Table 2**: \n\nWe included only DollarStreet in Table 2 in order to avoid confusion in reader interpretation of our Progress gap measure, which is a ratio of the rate between the benchmarks with respect to one baseline (DollarStreet). We find similar trends hold, showing conclusions are consistent across both datasets - the progress rate on GeoDE is smaller compared to other benchmarks. We have these numbers in Figure 5 in the Appendix and would be happy to add them to the main table if helpful. \n\n\n**Income Variation in DollarStreet**: \n\nWe agree that controlling for income variation is very interesting, thank you for suggesting this analysis! We have run an analysis of OpenAI\u2019s CLIP trained on LAION 2B, analyzing the per-region accuracies within each income quartile. For the middle two quartiles (the only quartiles with both Europe and Africa data), we found that there was still a substantial performance drop across regions. This indicates that even when controlling for income, there is a significant performance degradation across geographies. We have included the table in Appendix C, and below for reference - thank you for the excellent suggestion, and we welcome any further advice to clarify this point. \n\n| DS Income Quartile | Europe Avg Acc (\\%) | Africa Average Accuracy (\\%) | Region Disparity (\\%) |\n|--------------------|---------------------|------------------------------|:---------------------:|\n| Q3                 | 51.9                | 45.2                         | 6.6                   |\n| Q2                 | 61.1                | 52.5                         | 8.7                   |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167110068,
                "cdate": 1700167110068,
                "tmdate": 1700167110068,
                "mdate": 1700167110068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "90SJw6pam6",
                "forum": "rhaQbS3K3R",
                "replyto": "o8kVVzzcSj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6304/Reviewer_NCcf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6304/Reviewer_NCcf"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their extensive rebuttal! I do believe the paper is now more complete.\n\nHowever, after looking at the GeoDE paper again (Ramaswamy et al.), I notice that they also perform an experiment where they train on GeoDE and confirm that this also improves the results on DollarStreet (demonstrating the transferability between the two datasets). Thus, I believe the findings in Section 6 are unsurprising or broadly confirm prior work. After reading the other reviews and responses to them as well, I would like to keep my score.\n\n(I have a final minor suggestion about the formatting of the appendix: placing the tables and the figures closer to the text that refers to them would make the navigation easier)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593464771,
                "cdate": 1700593464771,
                "tmdate": 1700593464771,
                "mdate": 1700593464771,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qvMlUzo4PS",
            "forum": "rhaQbS3K3R",
            "replyto": "rhaQbS3K3R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6304/Reviewer_PnXg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6304/Reviewer_PnXg"
            ],
            "content": {
                "summary": {
                    "value": "The paper conducts an extensive empirical evaluation of many vision models with a focus on understanding their geographical robustness. They choose two crowdsourced, geographically distributed datasets GeoDE and DollarStreet to highlight that progress on standard robustness benchmarks curated from ImageNet need not necessarily indicate geographically equitable progress. Specifically, they highlight that improvements in backbones, architectures and data showcase gains in standard benchmarks but progress on crowdsourced datasets is much more sluggish. Secondly, they also highlight the discrepancy between gains in western-centric data (European) compared to Afro-Asian data. Finally, they allude to few possible future directions which could help alleviate this bias."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper addresses a very pertinent problem of investigating the equity of progress through modern deep learning advances in computer vision. They make several important observations pertaining to poor geographical robustness of current model, which can spur several future directions.\n\n- The authors cover several key advances in their study, including large data, larger architecture and different architectures and the observations hold for most cases.\n\n- A brief analysis of possible future directions is also presented, although it is notable that real progress demands labeled data from under-represented geographies, so unlabeled generalization is still an open challenge."
                },
                "weaknesses": {
                    "value": "- The paper really does not answer the questions as to why there is a disparity between progress in disparate geographical groups. Is it due to the dataset bias in training data? Is it because of the domain shifts/ label shift between different geographies? Although the current observation that there is a accuracy gap is important, it would be more useful if an inisght into the possible causes is also presented.\n\n- The authors note that they perform manual re-labeling of DollarStreet and GeoDA categories to ImageNet classes. Does this induce any kind of labeling noise which might explain the result? For example, images percieved by model to be `stove` are actually evaluated with `oven`. Since oven and stove designs change worldwide, could this be a possible reason for the accuracy drops? Adding to this, can you also choose only these classes from other robustness benchmarks as well? (Like select a subset of ImageNet-S with only those labels which were mapped to DollarStreet and then evaluate the accuracy). This somehow seems a more fair comparison.\n\n- Adding to the above point, it would be useful to include another contemporary dataset GeoNet [1] into the evaluation since they seem to directly source their dataset based on labels from the ImageNet (thus avoiding relabeling). \n\n- Several recent efforts to study the problem of geographical adaptation are not cited or compared [1,2,3]. GeoNet seems to be a relevant work which addresses similar issues with broadly similar observations. Specifically, the conclusion that scaling model sizes and datasets would not automatically yield geographical robustness seems related, but no discussion or comparison with this work is presented. \n\n- The disparity difference is only computed against Europe and Africa. Does the observations also hold for, say, Europe and Asia?\n\n- Sec 5.3: Could the authors hypothesize why rates of improvement across regions in GeoDE is much more uniform compared to DollarStreet?\n\n[1] Kalluri, Tarun, Wangdong Xu, and Manmohan Chandraker. \"GeoNet: Benchmarking Unsupervised Adaptation across Geographies.\" CVPR. 2023.\n\n[2] Prabhu, Viraj, et al. \"Can domain adaptation make object recognition work for everyone?.\" CVPR. 2022.\n\n[3] Yin, Da, et al. \"GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods.\" CVPR. 2023."
                },
                "questions": {
                    "value": "The major question to the authors is their opinion on why the noted differences are observed. I am eagerly waiting for the responses on this and several other clarifications requested above, and I would be happy to further raise my rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6304/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815054784,
            "cdate": 1698815054784,
            "tmdate": 1699636692447,
            "mdate": 1699636692447,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tteUtQ9tkf",
                "forum": "rhaQbS3K3R",
                "replyto": "qvMlUzo4PS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6304/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6304/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough engagement with our work and thoughtful suggestions! We are particularly glad that we were able to convey the very pertinent problem of geographical disparities, and are glad that you found the insights in our work important. We hope we addressed your questions thoroughly below, and welcome any additional questions! \n\n**Driving Mechanism of Geographical Disparity**:\n \nOur work primarily focuses on highlighting geographical disparity as an understudied and open research challenge for the community by providing comprehensive empirical evidence that this problem is pervasive across architectures and training paradigms. While the exact underlying mechanisms for models\u2019 disparities across geographies (as well as a solution to this problem) is an open question, below we outline our main hypotheses for the driving reasons behind these disparities: \n\n1. **Dataset imbalance:** Our work shows that the geographical disparities are persistent across 100+ models with a variety of training architectures and paradigms. This suggests that the disparities are driven by geographical data imbalance in pretraining datasets. Moreover, prior works showed ImageNet is heavily western biased [1], and in our paper Appendix F we present a kNN clustering experiment, where we analyze CLIP embeddings of LAION and DollarStreet to approximate the regional distribution of LAION. We find that our proxy measure indicates an extreme imbalance in LAION, which aligns with existing work on ImageNet. Additionally, in Section 6.3 we show that fine-tuning the last layer of a ViT model on DollarStreet improves disparities on **both** DollarStreet and GeoDE, showing the promise of interventions using geographically diverse data.\n\n2. **Distribution shifts and factors of variations between geographies:** Gustafson et al [2] annotated factors of variation in DollarStreet and found that variations in texture, occlusion, and lighting were among the factors most associated with performance disparities. Models pre-trained on ImageNet may overrely on these factors and other spurious features such as backgrounds [3] which also contributes to geographical disparities.\n3. **Household income:** In addition to geographical labels, DollarStreet also has income group labels, which is correlated with geography.  We hypothesize that the lower income group is underrepresented in pre-training data and also presents a distribution shift in terms of factors of variations discussed above.   \n\nImportantly, scaling up model or dataset size as well as standard robustness interventions such as data augmentation, while being effective on standard ImageNet OOD benchmarks, don't lead to improvements in geographical generalization, highlighting the unique challenges of geographical distribution shifts. We add this discussion as a section in Appendix I.\n\n[1] Shankar et al, No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World\n[2] Gustafson et al, Pinpointing Why Object Recognition Performance Degrades Across Income Levels and Geographies\n[3] Xiao et al, Noise or Signal: The Role of Image Backgrounds in Object Recognition\n\n**Label Mappings and Label Noise**: \n\nThank you for the insightful question! We also considered label noise as a potential concern, which is why we explicitly conduct zero shot classification experiments, where we prompt a model with the original DollarStreet labels rather than relying on an ImageNet mapping. Specifically, we evaluate a suite of CLIP models (See Appendix A for list). We find zero shot models have consistent geographic disparities on both DollarStreet and GeoDE benchmarks, finding some of the largest disparities for the largest CLIP models, even with scaling (see Figure 2). We would also like to emphasize that in both cases with DollarStreet and GeoDE, the original labels given are much more coarse than ImageNet (for example: \u2018bag\u2019 label compared to \u2018purse, pencil case, backpack, etc.\u2019), thus when performing label mapping to ImageNet-1K each example DollarStreet and GeoDE is assigned **multiple** plausible ImageNet labels. This adds additional protection against label noise, as the model's prediction is considered correct if it falls within any of the more granular ImageNet categories. We also added a replication of Figure 5 with DollarStreet\u2019s Top 5 accuracy (rather than Top 1), finding the same result: that our rate of improvement on DollarStreet lags significantly behind progress on standard generalization benchmarks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166834704,
                "cdate": 1700166834704,
                "tmdate": 1700166834704,
                "mdate": 1700166834704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eiyxmDKrVH",
            "forum": "rhaQbS3K3R",
            "replyto": "rhaQbS3K3R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6304/Reviewer_QyGA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6304/Reviewer_QyGA"
            ],
            "content": {
                "summary": {
                    "value": "To study the reliability of foundation models, this paper proposes to evaluate these models on crowdsourced, global datasets with natural distribution shift. The paper provides evaluation of 100+ vision models on the benchmark datasets DollarStreet, GeoDE, and compare to the evaluation on standard benchmark datasets ImageNet. The findings show that existing evaluation on standard benchmark dataset is limited, and it is promising to use more curated and/or representative datasets for evaluating foundation models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The studies problem of this paper is important to the community. In the era of foundation models trained on billion-scale dataset, it is important to re-consider the proper datasets and metrics for evaluation. \n\nThe paper provides interesting findings that show the importance of geographic factors in model evaluation."
                },
                "weaknesses": {
                    "value": "Although the paper studies an important problem, the technical contribution is limited, and the main findings are mostly empirical. \n\nThe paper mainly consider the geographic factor in evaluation, but does not provide a more comprehensive review, discussion or comparison on the other factors. For instance, most existing CLIP, OpenCLIP models are evaluated on a diverse set of datasets for completeness. Would the dataset diversity be another important factor?\n\nThe paper does not provide a very clear and conclusive discussion to point out the potential direction/solutions for resolving the data challenges in evaluation. For instance, what factors should be considered in designing the proper benchmark datasets? What are the right evaluation metrics to consider for evaluating foundation models?"
                },
                "questions": {
                    "value": "Please provide more discussion on the following questions:\n\nWhat are the most important factors to consider to design the proper benchmark datasets for evaluating foundation models?\n\nWhat are the proper evaluation metrics to evaluate these factors? \n\nWhat are the solutions/directions to design the proper evaluation metrics and benchmarks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern on Ethics."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6304/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822930146,
            "cdate": 1698822930146,
            "tmdate": 1699636692322,
            "mdate": 1699636692322,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yLUex2FfIL",
                "forum": "rhaQbS3K3R",
                "replyto": "eiyxmDKrVH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6304/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6304/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your appreciation of our insights as important to the community, and especially relevant in the era of foundation models trained on large-scale web data. We are glad that you found our results interesting!  \n\n**Our primarily empirical analysis as limited**: \n\nWe agree that our work is largely empirical. However, we disagree that empirical work has limited contributions. We believe that some of the most impactful work in machine learning has been work that provides scientific insight, and redirects the field to new problems, new model limitations, or new approaches . Our intention with this work is to highlight geographical disparities as an open and practically important scientific challenge for the research community, and we support this with **actionable insights** below\n1. Our work shows for the first time how extensive geographic generalization failures are across model architectures, training paradigms, and research datasets. \n2. We discover that these failures, unlike those represented in existing benchmarks and counter to researcher intuition, are unsolved by current mitigations, data scaling, or model scaling. \n3. These results indicate that standard measures of progress have over-indexed on ImageNet and ImageNet-based metrics, leaving geographical disparities as an understudied and critical problem affecting real-world deployment. \n4. We offer new ways to both measure and mitigate this problem, with our KNN rebalancing experiment, and last layer retraining experiments. We also release a simple-to-use benchmarking library allowing researchers to evaluate models with just 4 lines of code. \n\n**Discussion on Dataset factors and selection**: \n\nThank you for this insightful suggestion! We absolutely want to emphasize the importance of data selection as a main takeaway of our work. We added some language to our discussion and an **additional section in Appendix H** describing these factors to our paper. To your specific questions: \n1. **Would dataset diversity be an important factor in evaluation?** Absolutely! Our empirical evidence indicates that geographical diversity is a missing axis from standard benchmarks, and we hope to encourage the field to consider geographical disparities (and all data subgroups) as important factors for evaluation. \n2. **What factors should be considered in designing proper evaluation benchmarks?** Our work provides evidence that data sources, label curation, geographic representation, and income information should all be closely considered. In Appendix H, we emphasize that the best evaluation metrics are crowdsourced and grounded in real-world use, which is a significant difference between standard benchmarks and our geographic benchmarks. \n3. **What are the right evaluation metrics to consider for evaluation foundation models?** We address this question by highlighting the need for using subgroup evaluation metrics in generalization literature, and the limitations of using average accuracy alone. We will cite relevant examples of subgroup analysis in fairness and spurious correlations. Similarly, our work highlights the impact of looking more closely at the kinds of accuracy gains made (improvement on which portions of data), which we demonstrate with our progress gap measure. Overall, we hope to suggest that there is no one particular benchmark or tool that can perfectly measure model reliability, but that having a comprehensive suite of measures, and by analyzing benchmarks more precisely, we can improve our understanding of model behavior and its reliability. \n\nThank you for your insightful suggestions! We welcome any other questions or suggestions to improve the clarity and impact of our work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166614403,
                "cdate": 1700166614403,
                "tmdate": 1700166614403,
                "mdate": 1700166614403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JBer72ju6X",
            "forum": "rhaQbS3K3R",
            "replyto": "rhaQbS3K3R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6304/Reviewer_hbmo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6304/Reviewer_hbmo"
            ],
            "content": {
                "summary": {
                    "value": "In this work, authors target an important aspect of model training: the data imbalance across region. Author used  two globally crowdsourced datasets (DollarStreet and GeoDE), as calibrated geographic disparity measurement, and shows that model train on conventional dataset, like imageNet, are highly dominant by the west. And model perform better on conventional dataset will enlarge this geographic disparity across regions. Lastly, author propose to solve this problem by last layer fine-tuning on geographic balanced dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem author targeting, is of significance to the community, especially in the foundation model, for which data are more dominant than the model itself. \n2. Author had made significant empirically contribution by experiment on a large number of models. \n3. Author have identified data imbalance in standard benchmark, and shows that improvement over conventional evaluation will exacerbate geographic disparities. \n4. Author shows that applying conventional trick like data augmentation and scaling won\u2019t solve this problem. \n5. Author also propose a simple fix of adopting last layer fine-tuning over geographic balanced dataset."
                },
                "weaknesses": {
                    "value": "Although the concept of adopting geographic disparities is a neat measurement for data bias, author limit the measurement of such concept only on two specific dataset, which make the final \u2018improvement\u2019 less convincing. Also, the proposed solution to resolve this bias by last layer fine-tune, is a common approach in data debiasing and less novel, especially author tried to fine-tune on a geographic dataset. \n\nOne potential significant improvement of this work, in my opinion, is to apply author\u2019s insight onto standard dataset. For instance, in appendix F LAION CLUSTERING EXPERIMENTS, author use two geographic dataset only as cluster center to measure the \u2018geographic disparities\u2019 for LAION datasets. It would be much stronger an insight if authors shows that finetuning/retraining model with \u2018balanced LAION\u2019 dataset could reduce geographic disparities."
                },
                "questions": {
                    "value": "Plase refer to weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6304/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6304/Reviewer_hbmo",
                        "ICLR.cc/2024/Conference/Submission6304/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6304/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699138104502,
            "cdate": 1699138104502,
            "tmdate": 1700704513622,
            "mdate": 1700704513622,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X5p4fp9yza",
                "forum": "rhaQbS3K3R",
                "replyto": "JBer72ju6X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6304/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6304/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful appreciation of our work! We are glad that we were able to convey the impact of our empirical contributions, the novel insights of the geographical dataset imbalances we characterize, and the crucial implications of our results for benchmarking. \n\n**Using 2 Benchmarks for Evaluation of Geographic Disparities**:\n\nOur analysis does rely on two benchmarks to characterize geographical disparity, and we agree that our results (as with all research analyses), should be interpreted considering the datasets used. However, we emphasize that these benchmarks are reliable measures of geographical disparities, as they are 2-6x larger than existing robustness benchmarks, and are supported by a wealth of research use in fairness [1,2,3]. DollarStreet and GeoDE have 61K and 38K respectively, compared to 10K in ImageNet-A and 30K samples in ImageNet Sketch. We have added a paragraph in the Appendix A (in blue) to clarify our use of these benchmarks, and welcome any further suggestions. \n\n[1] Shankar et al, No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World\n[2] Gustafson et al, Pinpointing Why Object Recognition Performance Degrades Across Income Levels and Geographies\n[3] Xiao et al, Noise or Signal: The Role of Image Backgrounds in Object Recognition\n\n\n**Balanced Retraining on LAION**:\n \nThank you for this insightful suggestion! In our work, we demonstrate how geographical disparity has worsened with progress on standard benchmarks, is  exacerbated by data and model scaling, and produce some of our preliminary evidence of dataset imbalance in LAION as a contributing cause. We agree that the next step in this research direction is to show how our insights can be used to design rebalancing mitigation strategies for large-scale training datasets. We find this suggestion of performing a rebalancing from our LAION cluster analysis, and are excited to try it. This is quite a computationally demanding experiment, which requires balancing a very large scale dataset, coordinating distributed training, etc. which is why we did not explore a rebalancing mitigation in the original submission. However, as we wrote in Appendix F, our clustering analysis should be viewed as a rough proxy of the regional distribution, as there are significant limitations - most notably, that LAION has several orders of magnitudes more concepts represented than DollarStreet. Therefore, rebalancing LAION based on comparisons to DollarStreet may not balance the dataset well across concepts.\nWe are excited to try this however, and will share results when we have them. \n\n\n**Contribution of Last Layer Retraining**:\n\nWe agree that last layer retraining has certainly been used in other works, which we cite in Sec. 6.3. In this work, we demonstrate empirically that geographical disparities are a pervasive problem, and that existing methods, such as scaling and standard robustness interventions, fail to consistently improve it (Figure 3 and Table 3). Our intention with our experiment in 6.3 is not to claim the development of a new method itself, but to show how, counter to these other approaches, last layer retraining could be a practical mitigation for improving geographical robustness. We expand on existing work by showing how last layer retraining is not just allowing adaptation to a given geographic distribution ([1]), but that last layer retraining improves geographic robustness on new datasets (without requiring the monumental task of generating labels for web-scale data). We have added some (highlighted) language in Sec. 6.3 to clarify.   \n\nWe welcome any other questions or suggestions to improve the clarity and impact of our work!  \n\n1. Vikram V Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Beyond web-scraping: Crowd-sourcing a geographically diverse image dataset. arXiv preprint arXiv:2301.02560, 2023.12"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166346597,
                "cdate": 1700166346597,
                "tmdate": 1700166346597,
                "mdate": 1700166346597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2ahuzJf4YC",
                "forum": "rhaQbS3K3R",
                "replyto": "JBer72ju6X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6304/Reviewer_hbmo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6304/Reviewer_hbmo"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks author for the detail reply and providing additional experiment. I am satisfied with where this paper is heading toward thus change my rating to 6 and soundness to 3.\nIn your future revision, please make sure to include result of retraining CLIP on balanced LAION dataset, and publicly release the label for geographical property of each LAION sample. Also you should include a link in the main text(where you mentioned to your comment on 'Benchmark Use' in appendix."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6304/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704481742,
                "cdate": 1700704481742,
                "tmdate": 1700704775478,
                "mdate": 1700704775478,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]