[
    {
        "title": "Leveraging Previous Tasks in Optimizing Risk Measures with Gaussian Processes"
    },
    {
        "review": {
            "id": "Mc84O9N0ND",
            "forum": "ElykcDu5YK",
            "replyto": "ElykcDu5YK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8829/Reviewer_ZY9Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8829/Reviewer_ZY9Q"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel meta-learning based approach for Bayesian Optimization of risk measures. Given a set of previous tasks, the authors propose to construct a no-regret query set and they prove that any strategy picking inputs in such sets leads is a no-regret strategy, i.e., it has a sublinear regret. Furthermore, the authors assigns priorities on all the inputs belonging to this NQS $\\mathcal{Q}_t$ at iteration $t$. Those importance weights are computed using the information from previous tasks. They take high values for the most probable local maximizers among the set of previous tasks. Then the authors introduce a tradeoff between exploiting the information from previous tasks and exploring the current task. By applying the approach on Value-At-Risk (and CVAR), the authors show that their algorithm is invariant to scaling and vertical shifting of the blackbox function, and robust towards the presence of harmful previous tasks.\nThe authors show numerical experiments on synthetic data and real-world data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper contains several key contributions that will be helpful for the future works in BO for risk measures.\nMajor strengths:\n  - the paper is very well written and the motivations are very clear: exploit previous tasks knowledge (achieved using V-UCB) to get better BO for risk measures. The literature review is well detailed and the authors explain clearly advantages and drawbacks of each of them. \n  -  the construction of the no-regret query sets is elegant and the theorem ensuring that any strategy picking input points in such sets is key contribution in this specific research area in my opinion. \n  - the definition of the importance weights exploits well the knowledge from the previous tasks. This also enables to avoid the weight decay to $0$ which is used in the state-of-the-art of meta-BO. The strategy keeps using the knowledge from previous tasks over time. \n  - the authors provide theoretical results ensuring that the presented meta-BO of risk measures maintains invariance properties in scaling and shifting of the blackbox function.\n  - the numerical experiments show that the approach is robust towards the presence of harmful previous tasks (versus V-UCB)."
                },
                "weaknesses": {
                    "value": "Although the paper is very clear and provide important results for the community, there are couple of points that are not completely clear to me.\n\nMajor comments/questions:\n   - the distribution of Z is supposed to be known. Is it a parameterized distribution that is fitted in practice?\n   - the point which is unclear to me is the definition of the lacing values. If I understand well, this means that at each iteration the authors pick a value for z (corresponding to the lacing value) and draw an observation $y_t = f(x_t, z) + \\varepsilon$. Is this correct? How are those lacing values computed in practice? I think a paragraph is necessary to explain exactly what is done here. Would it be possible to provide a pseudo-code summarizing the main steps of the algorithm?\n  - it would have been interesting to see an illustration of the set $\\mathcal{Q}_t$ in the numerical experiments and the computed importance weights. And also to see the evolution of this query set over time.  Figure 1 is not easily readable in my opinion.\n  - the authors write that \"we want to maximize the size of the query sets\". I understand this point since it enables to exploit all the previous tasks. However, it seems that there might exist some tasks that do not bring useful information or harmful information. In such case, wouldn't it be more interesting to discard these tasks from the query set? This question is a bit naive, but from a computational perspective, it would make more sense to consider less previous tasks. Is this correct?\n  - Maybe I've missed this point, but could you comment the computational cost of the approach?\n\n\nMinor comments:\n  - titles of subsection 2.2 and Section 3 are the same.\n  - In my opinion, there is a lack of details about the transfer knowledge from previous tasks in the settings. For a reader who is not familiar with meta learning, it would have been helpful to know precisely (in the settings) what is the \"knowledge\" you extract from a previous task. In your case, this is the output of V-UCB. This is explained in the introduction, but this would make the reading easier."
                },
                "questions": {
                    "value": "See above.\nIs the code publicly available?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8829/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8829/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8829/Reviewer_ZY9Q"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8829/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698330251516,
            "cdate": 1698330251516,
            "tmdate": 1699637110000,
            "mdate": 1699637110000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lLajaURNR9",
                "forum": "ElykcDu5YK",
                "replyto": "Mc84O9N0ND",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4GcU's Comments"
                    },
                    "comment": {
                        "value": "We would like to thank you for acknowledging the quality of our writing, the motivation, the thorough literature review, and the elegance of the no-regret query set. We also value your acknowledgement of the importance of the priority assignment, the theoretical results, and the numerical experiments.\n\nWe provide the following clarifications to address your concerns and question.\n\n> + the distribution of Z is supposed to be known. Is it a parameterized distribution that is fitted in practice?\n\n  We adopt this assumption from previous works on BO of risk measures (Cakmak et al., 2020; Nguyen et al., 2021a;b). In practical applications, we believe one should parameterize and fit the distribution of Z to historical data.\n  \n> + the point which is unclear to me is the definition of the lacing values. If I understand well, this means that at each iteration the authors pick a value for z (corresponding to the lacing value) and draw an observation $y_t = f(x_t,z) + \\epsilon$. Is this correct? How are those lacing values computed in practice? I think a paragraph is necessary to explain exactly what is done here. Would it be possible to provide a pseudo-code summarizing the main steps of the algorithm?\n\n  Your interpretation regarding the selection of z as a lacing value is accurate. Following your suggestion, we have included the pseudocode of the algorithm in Appendix G of the revised paper to facilitate its implementation for the reader. Additionally, a concise discussion on the definition of lacing values is provided in Appendix C.\n  \n> + it would have been interesting to see an illustration of the set $\\mathcal{Q}_t$ in the numerical experiments and the computed importance weights. And also to see the evolution of this query set over time. Figure 1 is not easily readable in my opinion.\n\n  We have followed your suggestion to include an illustration of the set $\\mathcal{Q}_t$ and the computed importance weights over several iterations in Appendix H.1 of the revised paper.\n  \n> + the authors write that \"we want to maximize the size of the query sets\". I understand this point since it enables to exploit all the previous tasks. However, it seems that there might exist some tasks that do not bring useful information or harmful information. In such case, wouldn't it be more interesting to discard these tasks from the query set? This question is a bit naive, but from a computational perspective, it would make more sense to consider less previous tasks. Is this correct?\n\n  Identifying and excluding harmful tasks to improve computational efficiency, especially when dealing with a large number of harmful tasks, is an interesting direction for future exploration. However, this is a challenging task given our current understanding. The difficulty arises from the absence of a concrete measure of a task's harmfulness. Two risk measures may exhibit similarity in certain local regions while diverging in others, a nuance captured by our assignment of priorities to inputs in $\\mathcal{Q}_t$.\n\n  On the other hand, the set $\\mathcal{Q}_t$ plays a vital role in ensuring the algorithm's robustness against harmful tasks. As illustrated in Figure 6 of the revised paper, where previous task 1 differs from the current task, the reduction in the size of $\\mathcal{Q}_t$ diminishes the negative impact of previous task 1 on the optimization of the current task.\n\n\n> + Maybe I've missed this point, but could you comment the computational cost of the approach?\n\n  We have included a discussion on the computational complexity of the proposed algorithm, comparing it to V-UCB (BO of risk measures without utilizing any previous tasks), in Appendix G of the revised paper. In summary, our algorithm introduces only an additional linear dependence on the number of previous tasks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429245504,
                "cdate": 1700429245504,
                "tmdate": 1700429245504,
                "mdate": 1700429245504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5D2VxYknMe",
                "forum": "ElykcDu5YK",
                "replyto": "Mc84O9N0ND",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4GcU's Comments (continued)"
                    },
                    "comment": {
                        "value": "> Minor comments:\n> + titles of subsection 2.2 and Section 3 are the same.\n\n  We have revised the titles of Subsection 2.2 and Section 3 to be 'Meta-BO of Risk Measures' and 'Our Proposed Solution to Meta-BO of Risk Measures', respectively.\n  \n> + In my opinion, there is a lack of details about the transfer knowledge from previous tasks in the settings. For a reader who is not familiar with meta learning, it would have been helpful to know precisely (in the settings) what is the \"knowledge\" you extract from a previous task. In your case, this is the output of V-UCB. This is explained in the introduction, but this would make the reading easier.\n\n  As you have noticed, in the second paragraph of the introduction (Section 1), we provide a brief overview of the goal of transferring optimization outcomes from previous tasks, such as function evaluation rankings, search space reduction, or the maximizer distribution in the existing literature. However, there is a lack of consensus on the specific knowledge to be transferred from previous tasks to the current one. In our research, we introduce the concept of transferring the local maximizers, which is distinct from existing literature. We will revise to make the discussion of the transfer knowledge clearer in the revised paper.\n\n> + Will the code be publically available?\n\n  We have attached the code in the supplementary materials.\n\n---\n\nWe sincerely hope that the inclusion of pseudocode, supplementary illustrations, and the responses provided above will address any concerns you may have regarding our paper. Your suggestions will be meticulously taken into consideration during the paper's revision process."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429329678,
                "cdate": 1700429329678,
                "tmdate": 1700429329678,
                "mdate": 1700429329678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "15BhXeQIM9",
                "forum": "ElykcDu5YK",
                "replyto": "5D2VxYknMe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8829/Reviewer_ZY9Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8829/Reviewer_ZY9Q"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the responses"
                    },
                    "comment": {
                        "value": "I thank the authors for answering my questions and addressing my comments.\nI keep my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643177866,
                "cdate": 1700643177866,
                "tmdate": 1700643177866,
                "mdate": 1700643177866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oab7v9Aldt",
            "forum": "ElykcDu5YK",
            "replyto": "ElykcDu5YK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8829/Reviewer_4GcU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8829/Reviewer_4GcU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a robust meta-BO for risk measures algorithm. It achieves the robustness to harmful previous tasks essentially by constructing the no-regret query set (NQS) that guarantees the convergence of the optimization while using the counting of probably local maximizer in related tasks as the priority within NQS to guide the acquisition. The paper offers theoretical justification for constructing NQS and the regret analysis."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The construction of NQS guarantees the robustness of the meta-BO.\n\n2. The algorithm is generally well-motivated. Since the key concepts are generally constructed on top of confidence intervals for the risk measure, they bear good interpretability."
                },
                "weaknesses": {
                    "value": "1. It is unclear why the $\\lambda$ and $\\eta$ are proposed to trade off the goals of NQS explicitly. The algorithm, by default, sets $\\lambda=0$ and $\\eta=1$ as discussed following the theorem 3.7.\n\n2. The priority mechanism in eq (10) is proposed to incorporate information from previous tasks, but it is actually not robust to harmful previous tasks. This is reflected in both the comparison between the results from theorem 3.2 and previous works' regret bounds and the empirical results.\n\n3. Figure 3 demonstrates the effectiveness of the priority mechanism. Yet since the priority function relies on counting, it is unclear whether the proposed method is robust to the increase of the portion of harmful tasks in $\\tau$."
                },
                "questions": {
                    "value": "1. It seems that the construction of NQS basically relies on the upper and lower confidence bounds of the risk measure $\\rho_f$ in eq (3). In ordinary BO, the UCB and LCB of the unknown objective function f are the direct equivalence of these confidence bounds for risk measure when the goal is optimized f. Then, an NQS for ordinary meta-BO could be constructed in the same way and could incur a similar regret guarantee. Could the author comment on this extension of NQS in classic meta-BO?\n\n2. The layout is intense and could be more reader-friendly. Could the author reduce the discussion over literature in sections 1 and 2 and leave more room for the essential equations, especially those for the assumptions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8829/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8829/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8829/Reviewer_4GcU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8829/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826124262,
            "cdate": 1698826124262,
            "tmdate": 1700719313571,
            "mdate": 1700719313571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "79pLCbjyvq",
                "forum": "ElykcDu5YK",
                "replyto": "oab7v9Aldt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4GcU's Weaknesses"
                    },
                    "comment": {
                        "value": "We appreciate your acknowledgment of our novel development of NQS, the interpretability and motivation of our proposed algorithm. In addressing your inquiries, we would like to offer the following clarifications.\n\n> 1. It is unclear why $\\lambda$ and $\\eta$ are proposed to trade off the goals of NQS explicitly. The algorithm, by default, sets $\\lambda = 0$ and $\\eta = 1$ as discussed following the theorem 3.7.\n\n   To assist readers in grasping the essence of our technical approach, we begin by presenting it in a way that starts with basic conceptual ideas accessible to layman readers and then gradually progresses to a rigorous mathematical formulation in Section 3.1. The parameters $\\lambda$ and $\\eta$ play a crucial role in translating conditions (C1) and (C2) into overlapping and uncertainty conditions. Subsequently, we elucidate the rationale behind choosing $\\lambda = 0$ and $\\eta = 1$ after a more in-depth discussion on the regret incurred by our proposed algorithm. In contrast, setting them directly and specifically to $\\lambda = 0$ and $\\eta = 1$ right at the beginning would potentially raise questions regarding alternative formulations of the overlapping and uncertainty conditions (with $\\lambda \\neq 0$ and $\\eta \\neq 1$). Hence, we chose to discuss their roles and trade-off first before setting them to specific values. Furthermore, it could be challenging to discuss the trade-off between exploiting previous tasks and exploring current tasks and the size of NQS without parameters $\\lambda$ and $\\eta$.\n\n\n> 2. The priority mechanism in eq (10) is proposed to incorporate information from previous tasks, but it is actually not robust to harmful previous tasks. This is reflected in both the comparison between the results from theorem 3.2 and previous works' regret bounds and the empirical results.\n\n   The robustness to harmful previous tasks is demonstrated by the algorithm's cumulative regret being sublinear (in contrast to worse than sublinear), even in the presence of harmful previous tasks. This interpretation also aligns with the approach taken in the work of Dai et al. (2022). Moreover, in all existing meta-BO studies, it is expected that the algorithm's performance may deteriorate in the presence of harmful tasks. The critical characteristic here is that given a sufficient number of observations, the algorithm can effectively identify the optimal solution (in contrast to not) irrespective of harmful prior tasks, as indicated by the sublinear cumulative regret. As far as we are aware, this particular feature is not found in the majority of meta-BO research, except for the notable work of Dai et al. (2022). Consequently, we consider demonstrating this property for our proposed algorithm to be a significant contribution.\n\n\n> 3. Figure 3 demonstrates the effectiveness of the priority mechanism. Yet since the priority function relies on counting, it is unclear whether the proposed method is robust to the increase of the portion of harmful tasks in $\\tau$.\n\n   The robustness does not come from the priority assignment; rather, it mainly arises from the construction of NQS and the theoretical guarantee that choosing any input from NQS leads to a no-regret algorithm, which is a unique contribution of our work. Additionally, we have included a visualization of the algorithm in the scenario where the set of previous tasks consists solely of harmful tasks, as illustrated in Figure 6 of the revised paper.\n\n   In Figure 2, note that the sets $\\\\mathcal{T}\\_{\\\\texttt{harmful-neg-scale}}$ and $\\\\mathcal{T}\\_{\\\\texttt{harmful-hshift}}$ of previous tasks consist of all harmful tasks (i.e., no useful previous tasks). Hence, we have empirically demonstrated the performance of our algorithm when the portion of harmful tasks is $100\\%$ apart from the sublinear cumulative regret proof. This is highlighted in the last paragraph of Section 4: \"In the extreme case when $\\mathcal{T}$ contains only harmful tasks ...\"."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429064305,
                "cdate": 1700429064305,
                "tmdate": 1700429064305,
                "mdate": 1700429064305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "966aGDd8RZ",
                "forum": "ElykcDu5YK",
                "replyto": "oab7v9Aldt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4GcU's Questions"
                    },
                    "comment": {
                        "value": "> 1. It seems that the construction of NQS basically relies on the upper and lower confidence bounds of the risk measure $\\rho_f$ in eq (3). In ordinary BO, the UCB and LCB of the unknown objective function f are the direct equivalence of these confidence bounds for risk measure when the goal is optimized f. Then, an NQS for ordinary meta-BO could be constructed in the same way and could incur a similar regret guarantee. Could the author comment on this extension of NQS in classic meta-BO?\n   \nThis is a very insightful observation! As outlined by the reviewer, we think there is potential to extend NQS to classic meta-BO by substituting the upper and lower confidence bounds of the risk measure with UCB and LCB of the unknown objective function. We will provide a comprehensive discussion of this extension in the revised paper to enhance the impact of our work within the BO community. In the context of risk measures, where a solution for meta-BO incorporating risk measures is lacking, we consider it innovative and practical to position and define the scope of our work as being the first work in the domain of meta-BO focused on risk measures.\n\n\n> 2. The layout is intense and could be more reader-friendly. Could the author reduce the discussion over literature in sections 1 and 2 and leave more room for the essential equations, especially those for the assumptions?\n\nGiven that our work is situated within the domains of BO, meta-BO, and BO of risk measures, we agree with the reviewer's observation about the extensive discussion on the literature and background. However, we also believe that it is crucial to present this background information to ensure readers can fully grasp the equations and assumptions. If the reviewer has suggestions for improving the discussion of any specific assumptions or equations in mind, we would greatly appreciate them. We are committed to enhancing the clarity and coherence of these sections in the revised paper based on any feedback provided.\n\n---\n\nOnce again, thank you for your insightful suggestions, which we will integrate into our revised paper. We highly appreciate your questions and hope that our responses contribute to enhancing your perception of our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429127502,
                "cdate": 1700429127502,
                "tmdate": 1700429127502,
                "mdate": 1700429127502,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MNSqbrKAJp",
                "forum": "ElykcDu5YK",
                "replyto": "WUDj71q3Ay",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8829/Reviewer_4GcU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8829/Reviewer_4GcU"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nI've read the rebuttal and my fellow reviewers' comments carefully. I appreciate the author's clarification. I stand by my original comments that the regret analysis and the priority mechanism are reasonable yet won't outperform the existing method in the worst-case scenario.  The NQS is an interesting mechanism that could be of broader interest other than the risk measures. The efficiency and robustness against model-misspecification of the construction of NQS might incur additional concerns over the practicality. But in general, the corresponding theoretical analysis was fine. I'm not inclined to veto the work.\n\nThanks."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719294618,
                "cdate": 1700719294618,
                "tmdate": 1700719294618,
                "mdate": 1700719294618,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zUqeWBmkvp",
            "forum": "ElykcDu5YK",
            "replyto": "ElykcDu5YK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8829/Reviewer_25FF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8829/Reviewer_25FF"
            ],
            "content": {
                "summary": {
                    "value": "This work considers how to leverage prior knowledge when performing Bayesian optimization of risk measures (VaR, CVaR). Prior works in the literature either consider the availability of prior knowledge to warm-start BO for non-risk measures, or optimization of risk measures without using prior knowledge. This work tackles both of these issues simultaneously. The key novelty of the work is the development of a \"no-regret query set\" that defines a set of input query sequences that would result in sub-linear cumulative regret. This set contains those inputs that are likely to be maximizers of the risk measure given the posterior belief about the underlying function and also provides additional information toward the estimation of risk measures. Once such a set is established, prior knowledge can be used to rank/prioritize the query sequences. As any query sequence within this query set would result in sublinear cumulative regret, even if the prior tasks are very different, asymptotic guarantees are still preserved (although finite sample rates deteriorate)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. The idea of a no-regret-query set is novel, to the best of my knowledge.\n\nS2. Authors carefully consider the impact of the quality of knowledge from prior tasks and how those influence the final results."
                },
                "weaknesses": {
                    "value": "W1. Empirical results are on extremely toy examples.\n\nW2. Like prior works, this work is also limited in terms of assuming exact knowledge of the noise distribution. Seems like a strong requirement.\n\nW3. Not really a weakness, but my guess is that this paper might have a much bigger audience at other venues (e.g., ICML/AISTATS)."
                },
                "questions": {
                    "value": "1. The key contribution seems to be the introduction of the idea of a no-regret-query set (NQS). Maybe I missed it, but it feels like it is more generally applicable than the exact problem being considered in this work. If not, perhaps it would be beneficial to know what makes NQS restricted to the setting of risk measures? Also, seems like various other side-information can be used to prioritize samples within NQS. \n\n2. While I understand that the requirement of noise distribution is similar to assumptions made in the prior work and is not directly related to the proposed method, however, since the paper currently is phrased as a solution for optimizing risk measures, it would be useful for the readers to have the assumption formally stated in Section 2.\n\n3. Can the authors provide some experiments on the scalability of the proposed approach? The toy examples considered all have dimensions less than 5 or 6.\n\n4. I think the introduction can be compressed significantly. Currently, the main contribution of the work starts at the end of page 5.\n\n5.  If \\lambda goes near 0, then \\eta can go near infinity and the regret guarantee becomes vacuous. This results in two important hyper-parameters of the problem: \\eta and \\lambda, that would essentially control exploration-exploitation. While 3.3 gives one recommendation, how sensitive are the practical results to the choice of their values?\n\n6. I do not understand the Lacing value choice of the noise variable, and why is important to obtain the desired result. Given that Theorem 3.2 is the core contribution of the work, I think it would be imperative to explain this assumption in more detail."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8829/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8829/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8829/Reviewer_25FF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8829/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699874676819,
            "cdate": 1699874676819,
            "tmdate": 1699874676819,
            "mdate": 1699874676819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3cWIsWDAv4",
                "forum": "ElykcDu5YK",
                "replyto": "zUqeWBmkvp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 25FF's Questions"
                    },
                    "comment": {
                        "value": "We greatly appreciate your recognition of our innovative development of the no-regret query set and the careful consideration of the quality of knowledge from prior tasks to the current task.\n\nIn the following paragraphs, we would like to address your questions and concerns.\n\n> 1. The key contribution seems to be the introduction of the idea of a no-regret-query set (NQS). Maybe I missed it, but it feels like it is more generally applicable than the exact problem being considered in this work. If not, perhaps it would be beneficial to know what makes NQS restricted to the setting of risk measures? Also, seems like various other side-information can be used to prioritize samples within NQS.\n\nThis is an excellent observation! Our original aim was to tackle the unexplored domain of meta-learning for BO of risk measures. This led to the introduction of the novel NQS. From the insightful comment of the reviewer, we acknowledge that this approach proves to be more versatile than initially anticipated. At present, we recognize the potential application of the NQS technique to various existing BO variants within the realm of meta-learning, including:\n\n+ Contextual Gaussian process bandit optimization (Krause et al., 2011).\n+ Adversarially robust optimization with Gaussian processes (Bogunovic et al., 2018).\n\nWe will incorporate this expanded discussion in the revised version of the paper.\n\n> 2. While I understand that the requirement of noise distribution is similar to assumptions made in the prior work and is not directly related to the proposed method, however, since the paper currently is phrased as a solution for optimizing risk measures, it would be useful for the readers to have the assumption formally stated in Section 2.\n\nTo enhance clarity, we plan to restate the assumption outlined in the introduction when we delve into Section 2.\n\n> 3. Can the authors provide some experiments on the scalability of the proposed approach? The toy examples considered all have dimensions less than 5 or 6.\n\nWe have incorporated experiments involving 9-dimensional inputs in Appendix H.2 of the revised paper. The outcomes for both VaR and CVaR risk measures, utilizing various sets of previous tasks (including harmful and useful tasks), are presented in Figure 8 of the revised paper. These results are consistent with the findings from other experiments detailed in the paper.\n\nWe would like to highlight our experiments with the yacht hydrodynamics and portfolio optimization datasets which makes our experiments section comparable with that of existing works on BO of risk measures.\nThe difficulty in finding real-world experiments arises the absence of existing meta-BO research that considers risk measures. This results in a lack of previous real-world tasks within the meta-BO literature that incorporate real-world environmental random variables.\n\nAdditionally, we enhance the overall comprehensiveness of our experiments section by including the CVaR experimental results in Appendix H of the revised paper.\n\nIt is worth mentioning that research on high-dimensional BO is ongoing. One feasible approach to address this challenge is the use of simple techniques, such as random projection, to effectively reduce the input dimension of the optimization problem.\n\n> 4. I think the introduction can be compressed significantly. Currently, the main contribution of the work starts at the end of page 5.\n\nAs our research is positioned in the domains of BO, meta-BO, and BO of risk measures, we agree with the reviewer about the comprehensive introduction which aims to discuss the literature and background in these 3 domains. Although the technical solution is on page 5, we have attempted to summarize the contribution in the last paragraph of the introduction (Section 1). This serves to provide readers with a preview of the paper's essence before delving into the background, the problem statement, and the technical solution."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428837576,
                "cdate": 1700428837576,
                "tmdate": 1700428837576,
                "mdate": 1700428837576,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BGinlU5G33",
                "forum": "ElykcDu5YK",
                "replyto": "zUqeWBmkvp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 25FF's Questions (continued)"
                    },
                    "comment": {
                        "value": "> 5. If \\lambda goes near 0, then \\eta can go near infinity and the regret guarantee becomes vacuous. This results in two important hyper-parameters of the problem: \\eta and \\lambda, that would essentially control exploration-exploitation. While 3.3 gives one recommendation, how sensitive are the practical results to the choice of their values?\n\n   We would like to thank the reviewer for the insightful question. While $\\eta$ can go near infinity when $\\lambda$ goes near $0$, from equation (7), note that the LHS is bounded from below by $\\min_{x' \\in \\mathcal{X}} \\rho_{u_t}(x';\\alpha) - \\rho_{l_t}(x';\\alpha) > 0$ (since the posterior standard deviation is strictly positive). Therefore, we will revise the upper bound of $\\eta$ as $\\eta \\le \\min(1/\\lambda, (\\rho_{u_t}(x_t^+;\\alpha) - \\rho_{l_t}(x_t^-;\\alpha)) / (\\min_{x' \\in \\mathcal{X}} \\rho_{u_t}(x';\\alpha) - \\rho_{l_t}(x';\\alpha)))$. As a result, when $\\lambda$ goes to $0$, $\\eta$ is still finite and the regret guarantee is still meaningful.\n\n   In practical applications, we suggest setting $\\lambda$ to 0 and $\\eta$ to 1 when practitioners believe that previous tasks are useful for the current task. Conversely, if practitioners are risk-averse and believe that previous tasks are predominantly harmful to the current task, they can set $\\lambda$ close to 1 and $\\eta$ to 1. This adjustment makes the algorithm more akin to GP-UCB, which reduces the use of information from previous tasks. This corresponds to our discussion on the exploration-exploitation trade-off in the paragraph following Lemma 3.1.\n\n> 6. I do not understand the Lacing value choice of the noise variable, and why is important to obtain the desired result. Given that Theorem 3.2 is the core contribution of the work, I think it would be imperative to explain this assumption in more detail.\n\n   Given our focus on meta-BO for risk measures, it is essential, at each BO iteration, to query both $x_t$ and $z_t$ as detailed in Section 2.1, an approach that is adopted from existing literature on BO of risk measures. The transfer of information from BO solutions of prior tasks to the current one primarily occurs through the selection of $x_t$. Selecting for specific values for $z_t$ ensures the no-regret property, akin to previous studies on BO of risk measures. The absence of a specific choice for $z_t$ could compromise the guarantee of sublinear regret, regardless of the chosen $x_t$.\n\n---\n\nWe sincerely hope that the additional experimental results and our above clarifications can resolve your concerns regarding our paper and improve your opinion of our work. We will carefully incorporate your suggestions into the revised paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428897742,
                "cdate": 1700428897742,
                "tmdate": 1700428897742,
                "mdate": 1700428897742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XgaJShWiVI",
                "forum": "ElykcDu5YK",
                "replyto": "bGu4pimayw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8829/Reviewer_25FF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8829/Reviewer_25FF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your efforts towards the rebuttal! I would like to stay with my original score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722406683,
                "cdate": 1700722406683,
                "tmdate": 1700722406683,
                "mdate": 1700722406683,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]