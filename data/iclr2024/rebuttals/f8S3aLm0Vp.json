[
    {
        "title": "DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models"
    },
    {
        "review": {
            "id": "xIoLIT11n5",
            "forum": "f8S3aLm0Vp",
            "replyto": "f8S3aLm0Vp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2187/Reviewer_KT7c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2187/Reviewer_KT7c"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on protecting the training data and detecting unauthorized training data usages in the text-to-image diffusion models. In detail, this paper first defines two types of element-level injected memorizations on the text-to-image diffusion models. Based on the definition of the injected memorizations and their memorization strength, this paper introduces an approach for detecting unauthorized training data usages in the text-to-image diffusion models. In detail, the proposed method modifies the protected dataset by adding designed unique and invisible contents (signal contents) on these images, so that the model will learn the memorizations on the signal contents if it has unauthorized training or fine-tuning on the protected training data. Experiments on four datasets and recent diffusion models (Stable Diffusion and VQ Diffusion) indicate the performance of the proposed method is good."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. [Novelty & Motivation] This paper links the unauthorized\ntraining data usages problem to the memorization of the\ntext-to-image diffusion models, which is an novel and\ninteresting direction. The design of the proposed method is\nreasonable. The motivation of this paper is clear. Detecting\nunauthorized training data usages in the diffusion models is\nan important and urgent problem, but it have not been\nwell-studied by existing works.\n\n2. [Studied Models] The experiments are conducted on the\nstate-of-the-art text-to-images diffusion models in the\nreal-world (Stable Diffusions) and advanced model\ntraining/personalization techeniques (LoRA and Dreambooth).\n\n3. [Practicality] The proposed method only requires the\nblack-box access to the examined models, which makes it\npractical in real-world usages.\n\n4. [Performance] The detection performance of the\nproposed method is high, it achieves 100% detection accuracy\namong various settings with nearly unnoticable perturbations\non the training/generated samples. The comparisons to\nexisting methods or potential other methods are\nwell-discussed in the introduction and the evaluation.\n\n5. [Writting] Overall, the presentation is good, and the\nwriting is easy-to-follow."
                },
                "weaknesses": {
                    "value": "1. [Texutal Inversion] Is it possible to detect the\nunauthorized data usages with Textual Inversion [1] (a\npersonalization technique the for text-to-image diffusion\nmodels)? \n\n2. [Efficiency] I did not find the discussion about the time\ncost of the proposed method. Helping the potential users\nknow the approximated time cost is benificial.\n\n3. [Summary Table for Symbols] A table summarizing the\nmeaning of all symbols used in this paper can be added to\nmake this paper more clear.\n\n[1] \"An Image is Worth One Word: Personalizing Text-to-Image\nGeneration using Textual Inversion\"."
                },
                "questions": {
                    "value": "Please see \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666164339,
            "cdate": 1698666164339,
            "tmdate": 1699636152482,
            "mdate": 1699636152482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3QyMzGm77G",
                "forum": "f8S3aLm0Vp",
                "replyto": "xIoLIT11n5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KT7c"
                    },
                    "comment": {
                        "value": "Thank you very much for your precious time and thoughtful comments. We are encouraged the\nnovelty and the significance of our work is recognized. We hope the following\nnew clarifications and results can address your concerns.\n\n**Q1:** Textual Inversion.\n\n**A1:** Thank you very much for your constructive suggestion. We conducted the experiments on the Textutal Inversion during the rebuttal period. The model used is Stable Diffusion v1. The dataset used is the Dog dataset used in Table 1.\nUnconditional injected memorization is used here. The results on 10 models w/ unauthorized data usages and 10 models w/o unauthorized data usages are shown in the following table:\n\nTP | FP| FN| TN| Acc| \n---- | ---| ---| ---| ---| \n10 | 0 | 0| 10| 100.0%|\n\n The results demonstrate that our method is effective for the Textual Inversion personalization method.\n\n**Q2:** Efficiency.\n\n**A2:** Thanks for your thoughtful comment. In the image coating stage, the\nwarping function only costs 0.08s on one image with 1280 height and 1280 width.\nThe time cost for training the signal classifier is 1085.7s (note that we only\nneed to train one signal classifier for one protected dataset). The runtime for\nthe Algorithm 2 (detecting if the inspected model has unauthorized data usages\nor not) is 546.7s. All runtime is measured on one Quadro RTX 6000 GPU. The main\nruntime of Algorithm 2 is brought from using the inspected model to generate\nimages. It can be accelerated by finding a faster diffusion sampler, which is\northogonal to the goal of this paper. We will add the discussion about runtime\nin the revised version. \n\n**Q3:** Summary Table for Symbols.\n\n**A3:** Thank you very much for your valuable suggestion. We will add the summary table for used symbols in our revised version."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583497438,
                "cdate": 1700583497438,
                "tmdate": 1700583735608,
                "mdate": 1700583735608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZOj0788iJO",
                "forum": "f8S3aLm0Vp",
                "replyto": "3QyMzGm77G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Reviewer_KT7c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Reviewer_KT7c"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments by Reviewers"
                    },
                    "comment": {
                        "value": "Protecting the training data and detecting unauthorized training data usage in the text-to-image diffusion models is quite an interesting and important topic. The reviewer would like to thank the detailed responses from the authors. Most of my concerns have been addressed during the rebuttal, therefore I will keep my original score and vote for the acceptance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612186401,
                "cdate": 1700612186401,
                "tmdate": 1700612186401,
                "mdate": 1700612186401,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lrqdLJZ1pX",
            "forum": "f8S3aLm0Vp",
            "replyto": "f8S3aLm0Vp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2187/Reviewer_P3dG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2187/Reviewer_P3dG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new scheme to detect the unauthorized data usages in text-to-image diffusion models, where the images are imperceptibly warped for protection. The warped images are able be memorized by diffusion models during the training, which offers the possibility to detect the existence of the usages of such data from the trained diffusion model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is an interesting approach by exploring the properties of the diffusion models, i.e., memorizing duplicated contents in the training data, for the detection of unauthorized data usages.\n\n2. This paper is well written and easy to follow.\n\n3. Good robustness over different diffusion models."
                },
                "weaknesses": {
                    "value": "1. The authors mention that, compared with the sota schemes which focus on the sample-level memorization, this paper focuses on the element-level memorization. I think it does not matter whethat it is sample-level or element-level, the most important is which one offers higher performance. The authors do not logically or experimentally justify the advantage of element-level memorization over the sample-level ones.\n\n2. The motivation of introducing two types of injected memorization is not well explained. The reviewer is confused with the necessarity of the trigger function. \n\n3. Insufficient Evaluation. Only less than 20 models are constructed during the evaluations, which is far from enough to demonstrate the effectiveness of the approach. It lacks of evaluation regarding the distortion of the image after the warping. It also lacks the discussion on the potential countermeasures against the proposed approach."
                },
                "questions": {
                    "value": "1. What is the value of the coating rate used in section 4.2? If only a small portion of the data is protected, it is quite strange that selecting only a portion from the whole dataset to train the model would still be accurately detected with 100% accuracy. If the selection does not overlap with the protected portion, the detection mechenism should not work, right?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2187/Reviewer_P3dG",
                        "ICLR.cc/2024/Conference/Submission2187/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767193906,
            "cdate": 1698767193906,
            "tmdate": 1700728416997,
            "mdate": 1700728416997,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wj4ZwCmEiP",
                "forum": "f8S3aLm0Vp",
                "replyto": "lrqdLJZ1pX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer P3dG (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you very much for your precious time and insightful comments. We hope the following\nnew clarifications and results can address your concerns. We are happy to provide further clarifications if needed.\n\n**Q1:** The authors mention that, compared with the sota schemes which focus on\nthe sample-level memorization, this paper focuses on the element-level\nmemorization. I think it does not matter whether it is sample-level or\nelement-level, the most important is which one offers higher performance. The\nauthors do not logically or experimentally justify the advantage of\nelement-level memorization over the sample-level ones.\n\n**A1:** Thank you for your valuable feedback. As pointed out by Carlini et al.\n[1], a model is considered to have sample-wise memorization if a specific sample\ncan be accurately identified as a training sample of the model through\nmembership inference attacks. \n\n* Existing methods [2,3] have shown that\nmembership inference attack remains a significant challenge for large diffusion\nmodels such as Stable Diffusion. For instance, the state-of-the-art membership\ninference method for diffusion models, namely SecMI [2], only has a 66.1%\nsuccess rate for the membership inference on the stable-diffusion-v1-5 model\n(also see our discussion in the Introduction Section). \n\n* In addition, these state-of-the-art membership inference methods require \n  white-box access to the inspected model, while the threat model in our paper\n  is that the protector only has **black-box** access to the model, which is more practical. Performing\n  membership inference for large diffusion models in black-box settings is\n  even more challenging [3]. Dubinski et al. demonstrate that existing methods\n  only have around 50% membership inference accuracy for large text-to-image\n  diffusion models under the black-box setting. To the best of our knowledge, no current membership inference method is known to achieve significant accuracy in this specific setting.\n\nThese limitations are the main reasons why we opted not to employ sample-level\nmemorization in our method. We will make it more clear in our revised version.\nThanks again for your constructive comment.\n\n[1] Carlini et al., Extracting Training Data from Diffusion Models. USENIX Security 2023.\n\n[2] Duan et al., Are Diffusion Models Vulnerable to Membership Inference Attacks? ICML 2023.\n\n[3] Dubinski et al., Towards More Realistic Membership Inference Attacks on Large Diffusion Models. arXiv 2023.\n\n\n**Q2:** The motivation of introducing two types of injected memorization is not well explained. The reviewer is confused with the necessity of the trigger function.\n\n**A2:** Thank you for your insightful comment. In this paper, we introduce two\ntypes of injected memorization, i.e., unconditional memorization and \ntrigger-conditioned memorization. Each of them has its unique advantages. For\nunconditional memorization, it is more general and it can be applied in both Scenario 1\nand Scenario 2 introduced in Section 3.1. For\ntrigger-conditioned memorization, although it is only suitable for Scenario 1,\nit is more effective under low coating rates. For example, in Table 7 of our\nmain paper, we show that the trigger-conditioned memorization is still effective\neven under extremely small coating rates, e.g., 2.0%. However, for unconditional\nmemorization, a relatively higher coating rate is required, and it fails to\ndetect malicious models when the coating rate is too small. You can also find\nthe comparison between these two types of memorization in Section 4.4 of our main\npaper. \nThe trigger function is used to inject trigger-conditioned memorization, which is more effective at lower coating rates.\nWe will make it more clear in our revised version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583118906,
                "cdate": 1700583118906,
                "tmdate": 1700584300407,
                "mdate": 1700584300407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wFbSadLjc7",
                "forum": "f8S3aLm0Vp",
                "replyto": "lrqdLJZ1pX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer P3dG (Part 2)"
                    },
                    "comment": {
                        "value": "**Q3:** Insufficient Evaluation. Only less than 20 models are constructed during\nthe evaluations, which are far from enough to demonstrate the effectiveness of\nthe approach. \n\n**A3:** Thanks for your helpful comment. In our paper, we have constructed **140\nmodels in Table 1** and **120 models in Table 2**. To make the evaluation more\ncomprehensive, we have generated more models and conducted the experiments on a\nlarger scale. For each case in Table 1, we have 20 models w/ unauthorized data\nusages and 20 models w/o unauthorized data usages. Therefore, **now we have 360\nmodels in Table 1**. The results are shown in the following table:\n\nModel | Dataset| Memorization Type| TP|FP|FN|TN|Acc|\n---- | ---| ---|---|---|---|---|---|\nStable Diffusion v1 + LoRA | Pokemon | Unconditional|20|0|0|20|100.0%|\nStable Diffusion v1 + LoRA | Pokemon | Trigger-condioned|20|0|0|20|100.0%|\nStable Diffusion v1 + LoRA | CelebA | Unconditional|20|0|0|20|100.0%|\nStable Diffusion v1 + LoRA | CelebA | Trigger-condioned|20|0|0|20|100.0%|\nStable Diffusion v1 + LoRA | CUB-200 | Unconditional|20|0|0|20|100.0%|\nStable Diffusion v1 + LoRA | CUB-200 | Trigger-condioned|20|0|0|20|100.0%|\nStable Diffusion v2 + LoRA | Pokemon | Unconditional|20|0|0|20|100.0%|\nStable Diffusion v2 + LoRA | Pokemon | Unconditional|20|0|0|20|100.0%|\nStable Diffusion v1 + LoRA + DreamBooth | Dog | Unconditional|20|0|0|20|100.0%|\n\nAs can be seen, our method achieves 100.0% accuracy in all cases. We are\nconfident that our method will still have high accuracy in the evaluation with\nmore models. We hope our results adequately addressed your concern. We will include the new results in our revised version.\n\n**Q4:** It lacks evaluation regarding the distortion of the image after the\nwarping. \n\n**A4:** Thanks for your thoughtful comment. We conducted the suggested\nevaluation accordingly during the rebuttal period. In the following tables, we\nreport the average value of the SSIM, PSNR, MAE (Mean Absolute Error), and MSE\n(Mean Squared Error) between the coated images under our default setting and the\noriginal images.\n\nThe results on the Pokemon dataset with warping strength 1.0:\n\nMeasurement | Value|\n---- | ---| \nSSIM | 0.99 |\nPSNR| 31.35 |\nMAE| 0.0052 |\nMSE| 0.0008 |\n\nThe results on the Pokemon dataset with warping strength 2.0:\n\nMeasurement | Value|\n---- | ---| \nSSIM | 0.96 |\nPSNR| 26.35 |\nMAE| 0.0097 |\nMSE| 0.0026 |\n\nThe results on the CelebA dataset with warping strength 1.0:\n\nMeasurement | Value|\n---- | ---| \nSSIM | 0.99 |\nPSNR| 45.80 |\nMAE| 0.0026 |\nMSE| 0.00003 |\n\nThe results on the CelebA dataset with warping strength 2.0:\n\nMeasurement | Value|\n---- | ---| \nSSIM | 0.98 |\nPSNR| 40.04 |\nMAE| 0.0049 |\nMSE| 0.0001 |\n\nThe results demonstrate the coated version is highly similar to the original\nimages (for example, it has above 0.95 SSIM in all cases), meaning our method\nonly has a small influence on the quality of the protected images. The\nvisualization of the coated images and the original images can be found in Fig.\n5 in the Appendix. We will add more discussion in the revised version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583257882,
                "cdate": 1700583257882,
                "tmdate": 1700583257882,
                "mdate": 1700583257882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TtVewvD9Qz",
                "forum": "f8S3aLm0Vp",
                "replyto": "lrqdLJZ1pX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer P3dG (Part 3)"
                    },
                    "comment": {
                        "value": "**Q5:** It also lacks the discussion on the potential countermeasures against the proposed approach.\n\n**A5:** Thank you for your insightful questions. The potential adaptive attack\nfor our method is adding image augmentations during the training or fine-tuning.\nThe discussion about this adaptive attack can be found in section A.4 of the\nAppendix. To make the evaluation more comprehensive, we provide more results\nabout the robustness against the augmentation-based adaptive attack. The model\nused is Stable Diffusion v1 + LoRA, and the dataset used is the Pokemon. For the\ncompression process, we applied JPEG compression, reducing the image quality to\na mere 5% of its original state, representing a significant compression level.\nIn terms of blurring and smoothing, we employed Gaussian Blur with a kernel size\nof 51 and sigma 5, indicating intense blurring and smoothing effects. For\nsharpening, we used an image sharpening technique with a sharpness factor of\n200, denoting a high level of sharpening. We also present the results under\nsevere Gaussian Noise conditions (also see Section A.4). In addition, we have the\nexperiments of adding strong color jittering. It's important to note that the\naugmentation intensity in these experiments is high, leading to noticeable image\ndistortions (we will add the visualizations of these augmented images in our\nrevised paper). We assessed the detection accuracy across 10 models w/\nunauthorized data usages and 10 models w/o unauthorized data usages. for each\naugmentation type. The detailed detection results and the FID of the generated\nimages of the trained models are presented in the below table.\n\nAugmentation |TP|FP|FN|TN| Detection Accuracy|FID|\n---- | ---|---|---|---|---| ---| \nNone |10|0|0|10| 100.0% |218.28| \nJPEG Compression |10|0|0|10| 100.0% |251.33| \nGaussian Blur|10|0|0|10| 100.0% |244.19| \nSharpening|10|0|0|10| 100.0% |267.20| \nGaussian Noise|10|0|0|10| 100.0% |274.24 | \nColor Jittiering|10|0|0|10| 100.0% |248.57| \n\nAs can be seen, while our method still has high detection accuracy, the benign\nperformance of the models is significantly influenced by the strong\naugmentations (i.e., the FID increases significantly). The results indicate our\nmethod is robust against these augmentation-based attacks. It is not surprising\nthat the image warping function has good robustness against augmentations since\nthe warping effect is orthogonal to most of the image augmentation operations\n[1]. Existing works such as Wang et al. [2] also find that image warping is\nrobust to the added perturbations. We will add more discussion in our revised\nversion.\n\n**Q6:** What is the value of the coating rate used in section 4.2? If only a small portion of the data is protected, it is quite strange that selecting only a portion from the whole dataset to train the model would still be accurately detected with 100% accuracy. If the selection does not overlap with the protected portion, the detection mechanism should not work, right?\n\n**A6:** Thanks for your helpful questions and comments. \n\n* By default, the coating rate we used for unconditional injected memorization\nand the trigger-conditioned injected memorization are 100.0% and 20.0% (also see\nthe implementation details in Section 4.1). \n\n* In our experiments, we use the scenario where the whole dataset is used to\ntrain the model. Regarding the situation where an infringer selects a portion of the full dataset for model training, \nwe discovered that it's challenging for the infringer to precisely choose a portion that excludes coated images. This difficulty arises because the infringer is unaware of the specific signal function employed by the protector. Consequently, our study focuses on the practical scenario where the infringer randomly selects a portion of the entire dataset for training purposes. Under these circumstances, statistical analysis indicates that the coating rate of the selected subset is likely to be similar to that of the full dataset.\nOur method becomes ineffective if the chosen subset doesn't include\nany of the protected data, but the likelihood of this happening is very slim.\nTake the Pokemon dataset as an example, which contains 833 images. If we assume a\ncoating rate of 20% and the infringer randomly picks 20% of the dataset for\nmodel training, the chance that the chosen subset completely misses the coated\ndata is only $4.2*10^{-19}$, which is nearly\nnegligible. The table below illustrates the probabilities for different coating\nrates in the selected subsets. The probability of having an\nextremely low final coating rate is almost zero. It's worth noting that our\nmethod with trigger-conditioned memorization still has 100% accuracy even at a\n2% coating rate (refer to Table 7 in our paper), proving its effectiveness in\nsuch scenarios.\n\nCoating Rate for the Selected Portion | Possibility|\n---- | ---| \n0% | $4.2*10^{-19}$ | \n1%| $6.7*10^{-10}$ |\n2%| $3.2*10^{-5}$ |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583341240,
                "cdate": 1700583341240,
                "tmdate": 1700584783640,
                "mdate": 1700584783640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7xS9CDg6rr",
                "forum": "f8S3aLm0Vp",
                "replyto": "lrqdLJZ1pX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Friendly Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer P3dG,\n\nThanks again for your valuable comments and precious time. As the author-reviewer discussion period draws to a close, we genuinely hope you could have a look at the new results and clarifications and kindly let us know if they have addressed your concerns. We would appreciate the opportunity to engage further if needed.\n\nBest,\n\nAuthors of Paper 2187"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699637526,
                "cdate": 1700699637526,
                "tmdate": 1700699637526,
                "mdate": 1700699637526,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B2V7EQcYjz",
                "forum": "f8S3aLm0Vp",
                "replyto": "7xS9CDg6rr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Reviewer_P3dG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Reviewer_P3dG"
                ],
                "content": {
                    "title": {
                        "value": "comments from Reviewer P3dG"
                    },
                    "comment": {
                        "value": "Most of my concerns are well addressed. I am not satisfied with the response for Q1, perhaps the authors can include some experimental results for justification in the final version. I raise my rating to ba."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728761735,
                "cdate": 1700728761735,
                "tmdate": 1700728761735,
                "mdate": 1700728761735,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gLk3JZ69GF",
            "forum": "f8S3aLm0Vp",
            "replyto": "f8S3aLm0Vp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2187/Reviewer_gKsK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2187/Reviewer_gKsK"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses concerns related to unauthorized data usage in the training or fine-tuning process of text-to-image diffusion models. The authors highlight the potential misuse of data, where a model trainer might utilize images without proper permission or credit. To tackle this issue, the paper proposes a method that detects unauthorized data usage by implanting injected memorization into protected datasets during model training. This involves stealthy image-warping functions that remain imperceptible to humans but can be captured and memorized by diffusion models. By analyzing the presence of the injected content, the proposed method can effectively identify models that have illegally employed unauthorized data. The experiments conducted on various text-to-image diffusion models, including Stable Diffusion and VQ Diffusion, using different training or fine-tuning methods, demonstrate the efficacy of the proposed detection approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper addresses the issue of unauthorized data usage within text-to-image diffusion models, a critical and pressing concern within the artistic field. It presents a potential solution to safeguard the copyrights of artistic creators.\n2) The solution is sound and solid, which is quite easy to follow. The authors borrow some ideas from image warping and injected memorization into the task."
                },
                "weaknesses": {
                    "value": "1) Some typos and grammar errors exist, e.g., pp. 7, \"we assume the subsets provide by different data sources...\" should be \"we assume the subsets provided by different data sources\".\n2) In the experimental results section, the authors shall provide more quantitative results (in terms of, e.g., PSNR, SSIM or the residual) for comparing the original sample image and its coated counterpart."
                },
                "questions": {
                    "value": "1) Why use image-warping operation to implement the coating? Is it possible to employ some other operators?\n2) Does the image warping is reversible? It seems that the coated images are permanently damaged by the warping."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699439852894,
            "cdate": 1699439852894,
            "tmdate": 1699636152310,
            "mdate": 1699636152310,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0qhIjNPQsb",
                "forum": "f8S3aLm0Vp",
                "replyto": "gLk3JZ69GF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gKsK"
                    },
                    "comment": {
                        "value": "Thank you very much for your thoughtful comments and recognition of the\nsignificance of our work. We hope the following results and clarifications can\naddress your concerns.\n\n**Q1:** Typos.\n\n**A1:** Thank you very much for your helpful comment. We will revise accordingly in our revised version.\n\n**Q2:** In the experimental results section, the authors shall provide more\nquantitative results (in terms of, e.g., PSNR, SSIM, or the residual) for\ncomparing the original sample image and its coated counterpart.\n\n**A2:** Thanks for your insightful suggestion. We conducted the suggested\nexperiments accordingly during the rebuttal period. For the residual, we use the\nMean Absolute Error (MAE) and  \nMean Squared Error (MSE) as the measurements (here the pixel values of the\nimages range from 0 to 1). We report the average value of the SSIM, PSNR, MAE,\nand MSE between the coated images under our default setting and the original\nimages in the following tables.\n\nThe results on the Pokemon dataset with warping strength 1.0:\n\nMeasurement | Value|\n---- | ---| \nSSIM | 0.99 |\nPSNR| 31.35 |\nMAE| 0.0052 |\nMSE| 0.0008 |\n\nThe results on the Pokemon dataset with warping strength 2.0:\n\nMeasurement | Value|\n---- | ---| \nSSIM | 0.96 |\nPSNR| 26.35 |\nMAE| 0.0097 |\nMSE| 0.0026 |\n\nThe results on the CelebA dataset with warping strength 1.0:\n\nMeasurement | Value|\n---- | ---| \nSSIM | 0.99 |\nPSNR| 45.80 |\nMAE| 0.0026 |\nMSE| 0.00003 |\n\nThe results on the CelebA dataset with warping strength 2.0:\n\nMeasurement | Value|\n---- | ---| \nSSIM | 0.98 |\nPSNR| 40.04 |\nMAE| 0.0049 |\nMSE| 0.0001 |\n\n\nThe results demonstrate the coated version is highly similar to the original\nimages ( it has above 0.95 SSIM in all cases), meaning our method only has a\nsmall influence on the quality of the protected images. The visualization of the\ncoated images and the original images can be found in Fig. 5 in the Appendix. We\nwill add more discussion in the revised version.\n\n**Q3:** Why use the image-warping operation to implement the coating? Is it\npossible to employ some other operators?\n\n**A3:** Thank you for your thoughtful questions. We use the image-warping\noperation as our default signal function due to the warping effects are\northogonal to various image augmentation operations such as blurring,\ncompression, and sharpening [1]. Thus, it has good robustness to various image\nediting-based adaptive attacks (also see Reviewer-vEW1-A1). It is possible to\nemploy other operators (see Section 4.3 in our main paper). To make the\nevaluation more comprehensive, here we provide more results. The results (on 5\nmodels w/ unauthorized data usages and 5 models w/o unauthorized data usages) of\nusing different image filter functions as the signal functions are shown in the\nfollowing table:\n\nSignal Function | Detection Accuracy|\n---- | ---| \nWarping | 100.0% |\n1977 Instagram filter| 100.0% |\nKelvin Instagram filter| 100.0% |\nToaster Instagram filter| 100.0% |\n\nAs can be observed, our method achieves high detection accuracy in all cases, showing it is general to different signal functions. \n\n[1] Glasbey et al., A review of image-warping methods. Journal of Applied Statistics 1998.\n\n**Q4:** Is the image warping reversible? It seems that the coated images are\npermanently damaged by the warping.\n\n**A4:** Thanks for your insightful comment. The warping effect is reversible by\nusing the stored residual between the original image and the wrapped image. In\nthe data coating process, the protector can record the residual bought from the\nwrapping operation for each protected image. Then the wrapped images can be\nrecovered to the original image by using the stored residuals."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582869459,
                "cdate": 1700582869459,
                "tmdate": 1700583774264,
                "mdate": 1700583774264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hFJtSHGcUn",
                "forum": "f8S3aLm0Vp",
                "replyto": "0qhIjNPQsb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Reviewer_gKsK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Reviewer_gKsK"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments by Reviewer"
                    },
                    "comment": {
                        "value": "Thanks for your efforts in addressing my concerns. Although the response to Q4 is not as good as expected (storing the residual does not mean the image warping operation itself is reversible), I would like to recommend acceptance of this work because the investigated topic and the proposed method are of certain interest to the community."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665214436,
                "cdate": 1700665214436,
                "tmdate": 1700665214436,
                "mdate": 1700665214436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9ONKea63la",
                "forum": "f8S3aLm0Vp",
                "replyto": "gLk3JZ69GF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thank you very much for your support and feedback. The analysis in Heckbert et al. demonstrates that image warping function has its inverse function and it is reversible. Thanks again for your insightful question and helpful comment.\n\nHeckbert et al., Fundamentals of Texture Mapping and Image Warping."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699229464,
                "cdate": 1700699229464,
                "tmdate": 1700699268965,
                "mdate": 1700699268965,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ml0vtQZ7Np",
            "forum": "f8S3aLm0Vp",
            "replyto": "f8S3aLm0Vp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2187/Reviewer_vEW1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2187/Reviewer_vEW1"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to detect unauthorized data usage during the training or fine-tuning process in text-to-image diffusion models. This unauthorized data includes cases where a model can collect images of an artist without permission or generate similar images without giving credit to the artist. The paper addresses this issue by modifying the protected data by planting an injected memorization in the training of the diffusion model. This is done by adding unique contents on the protected image data using stealthy image warping functions that are not perceptible to humans but captured and memorized by diffusion models. The model is then analyzed whether it has the injected content and unauthorized data is detected this way. Experiments are presented on many state-of-the-art diffusion models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is written well and the problem that the paper is trying to address is clearly illustrated. Some visual examples are also provided. Results are also shown on many recent text-to-image diffusion models."
                },
                "weaknesses": {
                    "value": "Lots of experiments are presented. It will be good to have a robustness analysis also with these experiments. How robust is the proposed method to different image transformations like compression, blurring, smoothening, sharpening and more. Will this affect the detection performance?\n\nIt will be good if the authors can discuss adversarial ways in which the proposed technique can be defeated."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699472988756,
            "cdate": 1699472988756,
            "tmdate": 1699636152228,
            "mdate": 1699636152228,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7nojlDUOV2",
                "forum": "f8S3aLm0Vp",
                "replyto": "Ml0vtQZ7Np",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vEW1"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable comments and recognition of the\nsignificance of this paper. We hope the following results and clarifications can\naddress your concerns.\n\n**Q1:** Robustness analysis. How robust is the proposed method to different\nimage transformations like compression, blurring, smoothening, sharpening and\nmore. Will this affect the detection performance?\n\n**A1:** Thank you for your insightful questions. We have conducted the suggested\nexperiments accordingly during the rebuttal period. The model used is Stable\nDiffusion v1 + LoRA, and the dataset used is the Pokemon. For the compression\nprocess, we applied JPEG compression, reducing the image quality to a mere 5% of\nits original state, representing a significant compression level. In terms of\nblurring and smoothing, we employed Gaussian Blur with a kernel size of 51 and\nsigma 5, indicating intense blurring and smoothing effects. For sharpening, we\nused an image sharpening technique with a sharpness factor of 200, denoting a\nhigh level of sharpening. Additionally, in Section A.4 of the Appendix, we\npresent results under severe Gaussian Noise conditions. We also have the\nexperiments of adding strong color jittering. It's important to note that the\naugmentation intensity in these experiments is high, leading to noticeable image\ndistortions (we will add the visualizations of these augmented images in our\nrevised paper). We assessed the detection accuracy across 10 models w/\nunauthorized data usages and 10 models w/o unauthorized data usages. for each\naugmentation type. The detailed detection results and the FID of the generated\nimages of the trained models are presented in the table that follows.\n\nAugmentation |TP|FP|FN|TN| Detection Accuracy|FID|\n---- | ---|---|---|---|---| ---| \nNone |10|0|0|10| 100.0% |218.28| \nJPEG Compression |10|0|0|10| 100.0% |251.33| \nGaussian Blur|10|0|0|10| 100.0% |244.19| \nSharpening|10|0|0|10| 100.0% |267.20| \nGaussian Noise|10|0|0|10| 100.0% |274.24 | \nColor Jittiering|10|0|0|10| 100.0% |248.57| \n\nAs can be seen, while our method still has high detection accuracy, the benign\nperformance of the models is significantly influenced by the strong\naugmentations (i.e., the FID increases significantly). The results indicate our\nmethod is robust against these augmentation-based attacks. It is not surprising\nthat the image warping function has good robustness against augmentations since\nthe warping effect is orthogonal to most of the image augmentation operations\n[1]. Existing works such as Wang et al. [2] also find that image warping is\nrobust to the added perturbations. We will add more discussion in our revised\nversion.\n\n[1] Glasbey et al., A review of image-warping methods. Journal of Applied Statistics 1998.\n\n[2] Wang et al., Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers. arXiv 2023.\n\n**Q2:** It will be good if the authors can discuss adversarial ways in which the proposed technique can be defeated.\n\n**A2:** Thanks for your thoughtful suggestion. In this paper, we have evaluated\nour method against two distinct categories of potential adaptive attacks, namely\nincorporating strong augmentation during training or fine-tuning (see A1 and\nSection A.4 in our main paper), and collecting data from multiple sources (see\nSection 4.2 in our main paper). Our findings indicate that our method maintains\nits robustness against these potential adaptive attacks. As of now, we haven't\nidentified any effective and practical adaptive attacks that could bypass our\nmethod. We are happy to\nprovide further clarification if other potential adaptive attacks can be\nconcretely suggested. Exploring adaptive attacks that can effectively circumvent\nour approach will be our future work. We sincerely appreciate your insightful\nrecommendation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582754620,
                "cdate": 1700582754620,
                "tmdate": 1700584012191,
                "mdate": 1700584012191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MQ4CkdOL5A",
                "forum": "f8S3aLm0Vp",
                "replyto": "Ml0vtQZ7Np",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Friendly Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer vEW1,\n\nThanks again for your valuable comments and precious time. As the author-reviewer discussion period draws to a close, we genuinely hope you could have a look at the new results and clarifications and kindly let us know if they have addressed your concerns. We would appreciate the opportunity to engage further if needed.\n\nBest,\n\nAuthors of Paper 2187"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699835050,
                "cdate": 1700699835050,
                "tmdate": 1700699835050,
                "mdate": 1700699835050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]