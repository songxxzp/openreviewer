[
    {
        "title": "Harnessing Text to Image Diffusion for Dense Prediction Tasks"
    },
    {
        "review": {
            "id": "zYdoYviEai",
            "forum": "hujS6bmduD",
            "replyto": "hujS6bmduD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9277/Reviewer_6BRj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9277/Reviewer_6BRj"
            ],
            "content": {
                "summary": {
                    "value": "This work highlights the efficacy of text-to-image diffusion models and believes that the embedded visual semantic knowledge (inside these models) can benefit dense prediction tasks (i.e., transfer knowledge from pre-trained generative models to discrimination models). The study presents a method to do this knowledge transfer for visual dense prediction tasks, utilizing appropriate image tags and an adapter module to improve performance, and thus, advancing dense prediction tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper presents a clear motivation that an accurate semantic condition is important to extract knowledge from text-to-image models. The authors introduce a method to enhance the semantic condition, aiming to improve the performance of dense predictions with the pre-trained diffusion models."
                },
                "weaknesses": {
                    "value": "1. Needing clarifications. \na) In the oracle experiment in the introduction, it's unclear which model was employed. Could the authors specify this?\nb) In Section 3.4, the authors state that \u201cour findings indicate that alignment between the label space of the image tagging model and the datasets is not mandatory.\u201d What is the evidence supporting this claim? I understand that the adapter and multi-label classification tasks collaboratively facilitate this alignment. Could the authors elaborate on this?\nc) In Equation (7), the term (Pool(T(c)),h_k) is ambiguous. I assume it represents a similarity function, but this needs clarification.\n\n2. Ablation study. The paper lacks an ablation study related to query embedding. Such a study would provide insights into the significance and impact of this component.\n\n3. Efficacy of RAM. One highlighted contribution is incorporating an off-the-shelf zero-shot tagging model, RAM. However, its effectiveness seems questionable. Table 5 indicates a mere 0.3% improvement, raising concerns about the computational overhead introduced by RAM versus its benefits.\n\n4. Marginal improvements. Table 5 indicates a total performance improvement of 1.8%. While 0.9% of this gain is attributed to the introduction of CLIP_img (as proposed by ODISE), the proposed multi-label loss and RAM account for only 0.7% and 0.3% improvements, respectively. When compared with the substantial 21.7% enhancement observed on COCO-Stuff164k by using ground-truth class names (as shown in Figure 1), it appears that the authors' efforts to enhance the semantic condition might not be effective.\n\n5. Unclear formulations. For Eq.(1), notations are not defined, which is not friendly to readers who do not know the latent diffusion process.\n\n6. Poor writing quality. Author discussion comments are not removed from the draft (e.g., below Table 1, there is \"yiqi: default, we use both zero-shot prediction ....\") and there is an author name before the comments (against the anonymous policy of ICLR'24?). The overall writing is not easy to follow, and one guess is that ChatGPT was used to smooth the writing with long and complex sentences but with confused logic."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Author discussion comments are not removed from the draft (e.g., below Table 1, there is \"yiqi: default, we use both zero-shot prediction ....\") and there is an author name \"yiqi\" before the comments. This might be against the anonymous policy of ICLR'24."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667499354,
            "cdate": 1698667499354,
            "tmdate": 1699637168758,
            "mdate": 1699637168758,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Xc1fqoXHIZ",
            "forum": "hujS6bmduD",
            "replyto": "hujS6bmduD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9277/Reviewer_Mq6h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9277/Reviewer_Mq6h"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to adapt text-to-image diffusion models for downstream dense prediction tasks. To this end, the paper proposes to use image tags as textual descriptions. To deal with noisy tagging labels, an adapter module to derive relevant semantic information is proposed along with a multi-label classification learning objective. Evaluation is performed on ADE20K,  COCO-stuff164k, and CityScapes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u00b7         The proposed method seems to show somewhat improved performance over VPD (Zhao et al. 2023) in ADE20K and COCO-stuff164k benchmarks.\n\n\u00b7         The paper includes in Table 5 detailed ablations highlighting the performance improvement enabled by each component. Table 6 and 7 also ablates loss weights and adapter block sizes.\n\n\u00b7         Figure 2 provides a good overview of the proposed approach."
                },
                "weaknesses": {
                    "value": "The proposed method shows limited novelty over VPD (Zhao et al. 2023). The main contribution seems to be the addition of zero-shot tagging models to improve text embeddings. However, this addition of zero-shot tagging models along with a multi-label loss shows limited improvement over the use of off-the-shelf CLIP encoders in Table 5. Furthermore, the difference to text encodings obtained from the text adapter of Zhao et al. (2023) and the image-to-implicit caption adapter of Xu et al. (2023) should be explained in more detail.\n\n\u00b7         The comparison to VPD (Zhao et al. 2023) is limited. Additional experiments, e.g., image segmentation on RefCOCO and depth estimation on NYUv2 as in Zhao et al. 2023 would be helpful.\n\n\u00b7         The architecture of the tagging adapter is not well motivated. The TextEnc and ImageEnc in the the tagging adapter as described in Eq. 6 should be explained in more detail, including details of it model architecture. How does the TextEnc and ImageEnc relate to the cross attention modules in Figure 2.\n\n\u00b7         The paper should include qualitative examples highlighting examples where the proposed approach outperforms VPD (Zhao et al. 2023).\n\n\u00b7         Additionally, the paper is not well written:\n\no   In Figure 1, it is not clear what task is being performed, the model employed, and, the training and evaluation protocol.\n\no   The definition of T(c) is not clear. In Sec 3.1 it is described as \u201csignifying encoded text prompts\u201d and in Sec 3.3 it is described as referring to \u201cdataset associated category names\u201d.\n\no   In Sec 3.3 it is not clear what \u201cdataset associated category names\u201d refers to. Furthermore, the paper should distinguish between the terms \u201ctag information\u201d, \u201cdataset associated category names\u201d and \u201clabels\u201d used in sections 3.3 and 3.4.\n\no   In Section 3.4 it is not clear what \u201csharp, precise information\u201d means in this context.\n\n \n\n\u00b7         Typos: \u201class\u201d (page 2, para 1), \u201cuncondition\u201d (page 2, Figure 1), \u201ck-thlabel\u201d (page 5, para 2). Additionally, the use of \\citep{} and \\cite{} is not consistent."
                },
                "questions": {
                    "value": "\u00b7         The paper should discuss in more detail the difference to prior work, e.g., VPD (Zhao et al. 2023).\n\n\u00b7         The paper should provide more details of the text adapter, including details of the TextEnc and ImageEnc.\n\n\u00b7         The paper should use a consistent citation style."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729654121,
            "cdate": 1698729654121,
            "tmdate": 1699637168648,
            "mdate": 1699637168648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "QN9lquXA6B",
            "forum": "hujS6bmduD",
            "replyto": "hujS6bmduD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9277/Reviewer_x6HC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9277/Reviewer_x6HC"
            ],
            "content": {
                "summary": {
                    "value": "The authors show that text-to-image diffusion models can be effectively leveraged for visual dense prediction tasks when provided with appropriate image tags as textual descriptions. They first observe that supplying ground-truth semantic labels as textual instructions significantly enhances performance. Motivated by this observation, they propose an adapter module to derive relevant semantic information from noisy tagging labels. They also propose a multi-label classification learning objective to further enrich the semantic quality of tags."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Exploring the conditional adapters in diffusion models for dense prediction is a new topic."
                },
                "weaknesses": {
                    "value": "- This paper claims that using a text-to-image model for dense prediction but the main modification is on the adapter for the labels. So is it necessary to use a diffusion model for segmentation? Are these adapters useful on other baselines?\n- Using diffusion models for dense prediction is not a very new topic. Please compare the proposed model with some recent works, like DDP [1]. \n\n[1]Ji Y, Chen Z, Xie E, et al. Ddp: Diffusion model for dense visual prediction[J]. arXiv preprint arXiv:2303.17559, 2023."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9277/Reviewer_x6HC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746009988,
            "cdate": 1698746009988,
            "tmdate": 1699637168535,
            "mdate": 1699637168535,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "LBH28dCLcT",
            "forum": "hujS6bmduD",
            "replyto": "hujS6bmduD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9277/Reviewer_Sgbx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9277/Reviewer_Sgbx"
            ],
            "content": {
                "summary": {
                    "value": "The paper leverages text-to-image diffusion models to extract dense image features and demonstrates the importance of the text prompts. The paper to generate the textual prompts by using an (zero-shot) image tagging model and propose an attention module to further improve the text prompts. The paper validates the effectiveness on semantic and panoptic segmentation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper provides insightful analysis on the importance of the text prompts in the diffusion model under the context of dense prediction tasks.\n- The proposed tagging adaptor is easy to implement with standard attention module."
                },
                "weaknesses": {
                    "value": "- The presentation is not clear in many parts\n    - Many notations are not explained before being referred in the equation\n        - $x$ in Eq. 1, $c$ in Eq 2, $L, D, i$ in Eq. 3, etc.\n        - The authors might consider having a separate paragraph explaining the notations\n    - It\u2019s unclear what the actual learning objective looks like\n    - Overall, it\u2019s hard to parse the details of the architectures and the training recipe.\n- Mixed improvements across different tasks\n    - Improve in ADE20k and COCO-stuff164k\n    - Slightly worse in Cityscapes and COCO-Panoptic"
                },
                "questions": {
                    "value": "1. I wonder what leads to the gap between the prompting with the proposed method (56.2 on ADE20k) and ground truth prompting (74.4 on ADE20k). Can the author elaborate more on the intuition of this? To be more specifically, I am curious what kinds of errors cause such a big drop?\n2. In sec 3.2, the paper says \u201cEmpirical results suggest that the latter approach usually yields enhanced performance.\u201d Do the authors provide the results of freezing the diffusion model parameters somewhere? For fair comparison, ODISE freezes all the diffusion model parameters.\n3. From the experimental results, it seems that the propose approach perform well when the number of classes is larger, e.g., ADE20k and COCO-stuff164k. I wonder have the authors tried to train on ADE20k in coarser levels or a subset of ADE20k to see if the improvement diminishes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812703144,
            "cdate": 1698812703144,
            "tmdate": 1699637168427,
            "mdate": 1699637168427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]