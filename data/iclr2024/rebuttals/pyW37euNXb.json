[
    {
        "title": "Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models"
    },
    {
        "review": {
            "id": "Wk6QVknQlG",
            "forum": "pyW37euNXb",
            "replyto": "pyW37euNXb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1886/Reviewer_FgWw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1886/Reviewer_FgWw"
            ],
            "content": {
                "summary": {
                    "value": "This work is motivated by the fact that VAEs learn meaningful representations however tend to overfit to the training samples. Motivated by various shortcomings of augmentation, they propose to use diffusion models as a surrogate method to \"complete\" or improve the quality of the data, given their success at approximating probability distributions. The paper then conducts experiments studying the generalization gap."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper takes a rather simple idea which is to bootstrap data using Diffusion models and shows how it can be used to effectively improve VAEs.\n- The paper is very easy to follow, with background and reference material laid out quite well.\n- The generalization, amortization and robustness quantities are well defined and nicely motivated."
                },
                "weaknesses": {
                    "value": "- The concern in the paper is the lack of any theoretical investigation into why these methods work. While this is difficult to do in general, it would better if there was any mention of it at least with simplified models or any assumptions. \n- Diffusion methods can also be interpreted as a form of augmentation. For example you can use them to generate more examples in different resolutions or even \"image-to-image\" translation methods. Thus, I think the argument of treating augmentations visually different in Figure 1 is not very clear. While it mentions that augmentations are not \"accurate\", I think this will largely depend on the domain/data. I also think another aspect to consider is the computational time.`\n- I believe this paper is highly dependent on the accuracy of the diffusion model and while a theoretical investigation would further clarify this, I think it is worth mentioning. For example, in tabular data settings, diffusion models may not work so effectively or perhaps when data to train the diffusion model itself is scarce. Noting that diffusion models are quite data hungry, I'm not sure if it will be a great idea to use them to bootstrap in general."
                },
                "questions": {
                    "value": "\u201camortized inference, and adversarial robustness\u201d - can you explain what these are and how VAEs are used. It may be obvious for amortised inference however unclear for adversarial robustness. I think it is a good idea to introduce those problems before and how VAEs are used for them. \n\nI think the statement: \u201cTherefore, Gg \u2265 0\u201d is quite strong however this is not guaranteed. I understand this is what we expect however it could be the case that the test data is very simple to learn and the training data is complicated, resulting in a smaller ELBO over train. I would amend this statement to say \u201cwe anticipate after training G_g \\geq 0$ as opposed to keeping it as a statement by itself.\n\nHave you tried other types of generative models to bootstrap a VAE? For example, what if you use a GAN since they can be quite robust. Also, GANs can be used to generate synthetic augmentation and thus lie in the \"accurate\" region in between augmentation and diffusion.\n\nOther:\nFull stop page 1: \"robustness For generative modeling\" -> \"robustness. For generative modeling\"\n\u201cbut they lack of the\u201d -> \u201cbut they lack the\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698458664558,
            "cdate": 1698458664558,
            "tmdate": 1699636118887,
            "mdate": 1699636118887,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zSRdjy5Muo",
                "forum": "pyW37euNXb",
                "replyto": "Wk6QVknQlG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer FgWw (R4) from the authors"
                    },
                    "comment": {
                        "value": "Thank you for your detailed feedback! We are glad that you think the \u201cpaper is easy to follow\u201d and the three gaps/metrics are \u201cwell defined and nicely motivated\u201d. We address your questions below, and we updated the PDF accordingly.\n\n> **Q1** - [...] lack of any theoretical investigation [...]\n\nIndeed a general theory would be difficult. In Section 4.2, we mention that data augmentation generates new data points by sampling $\\mathbf x'\\sim p_\\text{aug}(\\mathbf x'|\\mathbf x)$ conditioned on a _single_ data point $\\mathbf x$, whereas our method generates new data points conditioned on the _entire_ training set.\n\nFor a simplified model, consider a data set where the underlying data distribution $p_\\text{data}(\\mathbf x)$ is a mixture of Guassians. Traditional augmentation would be limited to adding small noise to each data point (ensuring that it stays within its mode) when $p_\\text{data}(\\mathbf x)$ is unknown. But if we have knowledge about $p_\\text{data}(\\mathbf x)$, e.g. if the mixture components lie on a regular grid, then we can use discrete translations as augmentations. By contrast, a diffusion model trained on the entire data set could detect such properties automatically.\n\n> **Q2** - [diffusion models can be interpreted as data augmentation, e.g., in image-to-image translation]\n\nGood point! _Conditional_ diffusion models are indeed similar to traditional data augmentation. However, we propose to use an _unconditional_ diffusion model, i.e., a model that samples random data without taking an input. We clarified this as a footnote in Section 4.1 in the paper.\n\n> **Q3** - [...] diffusion models are quite data hungry\n\nThank you for bringing this up as it highlights why our results are nontrivial! We show that, whether or not a given model is data hungry is a relative statement. While the training sets were _too small_ for fitting good VAEs, as all baseline VAEs in our experiments strongly overfit, the same training sets are _large enough_ to fit useful diffusion models. One might obtain better diffusion models with larger training sets, but our diffusion models were evidently already at least good enough to be useful within our proposed method.\n\nThe fact that this observation is counterintuitive highlights why it should be communicated to a broader audience: one might think diffusion models require lots of data, and this is true if the goal is to generate high-quality samples. But our paper demonstrates a surprising new application for diffusion models in the regime where one does have too little data to train a good VAE.\n\n> **Q4** - [computational time]\n\nNote that our method only increases training time but not test time, which is more important in many applications. For further comments, please refer to item (2) in our \u201cOverall response\u201d above.\n\n> **Q5** - [...] [results are] highly dependent on the accuracy of the diffusion model [...] \n\nIndeed, as we discuss in the last paragraph of Section 4.1, diffusion models on other data types than images are less explored, and we might not have accurate models. But research in this area currently receives a lot of attention, and we expect that our method is compatible with any models proposed in the future.\n\n> **Q6** - [explain amortized inference and adversarial robustness in the context of VAEs]\n\nAmortized inference is a fundamental principle of VAEs. We review this term briefly at the beginning of the paragraph \u201cAmortization Gap\u201d (Section 2). Expanding on this explanation, amortized inference refers to the fact that VAEs use a neural network (the \u201cinference network\u201d) to map data points to latent representations which speeds up training and deployment. One can remove the inference network and instead obtain latent representations by iteratively optimizing the ELBO for each data point (as we do for $q^*$ in Section 5.3), but this is very expensive.\nThe end of Section 2 discusses in detail the kind of adversarial robustness that we analyze. We are happy to provide further explanation if something is not clear.\n\n> **Q7** - [$\\mathcal G_\\text{g} \\geq 0$ is not guaranteed]\n\nThe statement $\\mathcal G_\\text{g} \\geq 0$ relies on the assumption that training and test data come from the same distribution (see below Eq. 8). The remark below Eq. 8 discusses the case where this assumption is violated, and where the test data is \u201csimpler\u201d than the training data (i.e., it comes from a distribution with lower entropy). As discussed below Eq. 9, this can indeed lead to an inversion of the ELBOs (i.e., to $\\mathcal G_\\text{g} < 0$). \n\n> **Q8** - [...] other types of generative models to bootstrap a VAE?\n\nA major reason why diffusion models have become so popular (and why we use them here) is because they avoid difficulties in the training objectives of GANs which makes them easier to train. \n\nThank you again for your review! We would appreciate it if you could let us know whether our above responses resolve your concerns, and change your assessment of our work accordingly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257026470,
                "cdate": 1700257026470,
                "tmdate": 1700257026470,
                "mdate": 1700257026470,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "05NEOXoX9p",
            "forum": "pyW37euNXb",
            "replyto": "pyW37euNXb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1886/Reviewer_nb6g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1886/Reviewer_nb6g"
            ],
            "content": {
                "summary": {
                    "value": "The focus of the paper is around how to prevent overfitting of the encoder part of the VAE through using a diffusion model to sample from the data distribution. The authors analyze the performance based on various metrics and conclude that having access to a diffusion model for samples helps to prevent the overfitting problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall the paper is a novel way in combining a cutting edge generative model in conjugation with a VAE. I think the most interesting result is that it is counter to some of the previous work in the area that suggested sampling data could not produce good enough training data. The method could help to prevent overfitting for when VAEs are used for tasks where a latent representation is needed. Overall the paper is well written and I liked the Figure 1 which helped to simplify the story of the paper."
                },
                "weaknesses": {
                    "value": "The concept presented in the paper is intriguing. However, I struggled to discern scenarios where the proposed method would be essential in the context of a VAE. It's conceivable that there are applications in biological data or reinforcement learning where one might need to map an image to a latent representation and subsequently utilize this representation. Yet, the paper primarily focuses on image generation, a task for which the diffusion model already excels. The computational overhead introduced by the proposed method seems substantial. What advantages does it offer to justify this overhead? I would appreciate further evidence demonstrating situations where a diffusion model sampler enhances the VAE's performance for specific, necessary tasks."
                },
                "questions": {
                    "value": "-\tFigure 2: I found this a bit confusing we want the difference to the smallest but the way you plot both its hard to see which method is doing the best given the scale, perhaps you could replot just showing difference. Why is there a zoom in on the CIFAR-10 plot what is that supposed to show?\n-\tFigure 3: Similarly here can we plot the gap it seems the augmented approach perhaps does just as well but hard to see. \n-\tAm I right that all plots are just one training curve how did you pick hyperparameters for each method? Did you try running for different seeds to get some randomness in the results? \n-\tDo you have samples from your VAE plotted or any sort of test metric on how they do for your method versus the baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1886/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1886/Reviewer_nb6g",
                        "ICLR.cc/2024/Conference/Submission1886/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671069406,
            "cdate": 1698671069406,
            "tmdate": 1700491027341,
            "mdate": 1700491027341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uZ4x3RqGog",
                "forum": "pyW37euNXb",
                "replyto": "05NEOXoX9p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer nb6g (R3) from the authors"
                    },
                    "comment": {
                        "value": "Thank you very much for your thorough review. We are pleased that you think our proposed method is \u201cnovel\u201d, \u201cthe concept presented in the paper is intriguing\u201d, and the results are \u201cinteresting\u201d.\nWe address your questions below, and we updated the PDF accordingly.\n\n\n> **Q1** - [...] paper primarily focuses on image generation [...] for which the diffusion model already excels [...]\n\nWe agree that diffusion models already excel at image generation and that there is little benefit to using VAEs for this task. However, while our method uses a generative model (diffusion model) to generate images, our evaluations do not focus on image generation but instead on density estimation and representation learning, which are domains where VAEs provide an advantage over diffusion models. Also, as discussed in the introduction, the goal of our method is mainly to improve the encoder of the VAE, which does not play a role in sample generation. For more clarifications how our evaluations are not about generative performance, please refer to item (1) in our \u201cOverall response regarding sample quality and computational cost\u201d posted above.\n\nWe agree, however, that our paper focuses on the image domain since this is the domain where both diffusion models and VAEs are best understood. While diffusion models on other data types are currently less explored, research in this area currently receives a lot of attention, and we expect that our method is compatible with any models proposed in the future.\n\n> **Q2** -  [...] further evidence demonstrating [...] VAE's performance for specific, necessary tasks.\n\nWe agree that Sections 5.2-5.4 evaluate the performance of the VAE somewhat indirectly.\nTherefore, we added more direct evaluations of practical downstream applications in Appendix F (_new_), specifically representation learning tasks (classifying latent representations), reconstruction tasks, and (for completeness) also data generation tasks. \nWhile performance differences are indeed small, our proposed method enhances the VAE\u2019s performance for all tasks and all metrics except when measuring sample quality (SQ) by inception score (IS). \nThis is consistent with our claim that the proposed method mainly fixes the encoder, which affects representation learning and reconstruction but not sample quality.\n\n> **Q3** -  The computational overhead [...] seems substantial. \n\nWe provide numbers for the computational cost in Appendix C.\nWe stress that our method only increases training time but not test time, which is more important in many applications. \nAlso, the generated samples can be reused when training VAEs with different configurations for cross-validation. \nFor further comments, please refer to item (2) in our \u201cOverall response regarding sample quality and computational cost\u201d.\n\n> **Q4** -  Figure 2: [...] it\u2019s hard to see which method is doing the best [...] Figure 3: [...] it seems like the augmented approach does just as well but hard to see\n\nThank you for the suggestion. We added annotations to Figures 2, 3, 4, and 6 that highlight the gaps and show their numerical values. We hope that these additional annotations make it easy to see that our proposed method achieves the smallest gaps everywhere except for the robustness gap in BinaryMNIST and FashionMNIST (see Table 3 in Appendix B). The reason why we plot ELBO values rather than gaps in Figures 2, 3, 4, and 6 is because both the ELBO values and the gaps are important in practice, and plotting only the gaps would conceal the absolute values.\n\n> **Q5** - [Figure 2:] Why is there a zoom in on the CIFAR-10 plot what is that supposed to show?\n\nWe only show this inset because the gap here is so small that it might otherwise be difficult to see that the plot does indeed show two separate green lines.\n\n> **Q6** -  [...] how did you pick hyperparameters for each method?\n\nThank you for raising this important point. Hyperparameters are listed in Appendix D. For BinaryMNIST and FashionMNIST, we chose the hyperparameters of the VAE models by consulting the literature. For CIFAR-10, we manually tried out a few hyperparameters, and chose an architecture where overfitting occurs, as we are investigating how to alleviate overfitting in VAEs only from the training data. We have updated the Appendix D with this information.\n\n> **Q7** -  [...] running for different seeds [...]\n\nThank you very much for this important point! You are right that the plots were covering one seed. We re-ran CIFAR-10 experiments with three different random seeds and updated the Figures 2, 3, and 4 with means and standard deviation, which show consistent results across different random seeds. Experiments for the other data set are still running, and we will update the paper as soon as they finish.\n\nThank you again for your review! We would appreciate it if you could let us know whether our above responses resolve your concerns, and change your assessment of our work accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248100398,
                "cdate": 1700248100398,
                "tmdate": 1700248100398,
                "mdate": 1700248100398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7funL67ZgI",
                "forum": "pyW37euNXb",
                "replyto": "uZ4x3RqGog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1886/Reviewer_nb6g"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewer_nb6g"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for addressing my concerns, I appreciate the new experiments added and the additional of multiple seeds for the core experiments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490974162,
                "cdate": 1700490974162,
                "tmdate": 1700490974162,
                "mdate": 1700490974162,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pmQ2okQLMJ",
            "forum": "pyW37euNXb",
            "replyto": "pyW37euNXb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1886/Reviewer_nWgS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1886/Reviewer_nWgS"
            ],
            "content": {
                "summary": {
                    "value": "The author shows that utilizing samples from the diffusion model can mitigate issues on VAE and improve its generalization, amortized inference, and robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "They use diffusers as data augmentation tools to generate informative data for VAE training. These images help reduce the generalization gap,  amortization gap, and robustness gap."
                },
                "weaknesses": {
                    "value": "1. According to figure5, it needs at least 2 times training data to be better than the normal training and 10 times data to reach the best performance. How are the computation costs comparing DMaaPx with aug  tuned and aug naive, to let the model achieve the same/similar performance? (in terms of test ELBO).\n2. In your table 3, the method effectively reduced all three gaps the most for Binary MNIST and Fashion MNIST but not CIFAR10. Can you explain why it fails on CIFAR10? It looks to me that when the dataset becomes complex, your method loses its advantage. To convince the audience, you should include more datasets."
                },
                "questions": {
                    "value": "See the weakness above and this additional question below:\n1. In figure 2, CIFAR10, the test data\u2019s curves seem to keep increasing. For both augtuned and DMaaPx. If trained longer, what would be the trend?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1886/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1886/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1886/Reviewer_nWgS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802022396,
            "cdate": 1698802022396,
            "tmdate": 1699636118740,
            "mdate": 1699636118740,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8YEiIBeXvx",
                "forum": "pyW37euNXb",
                "replyto": "pmQ2okQLMJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer nWgS (R2) from the authors"
                    },
                    "comment": {
                        "value": "Thank you very much for your review! We address your questions below, and we updated the PDF accordingly.\n\n> **Q1** - According to figure5, it needs at least 2 times training data to be better than the normal training and 10 times data to reach the best performance. How are the computation costs comparing DMaaPx with aug tuned and aug naive, to let the model achieve the same/similar performance? (in terms of test ELBO).\n\nWe provide computational costs in Appendix C. In brief, for any reasonable amount of samples from the diffusion models, the computational overhead of DMaaPx mainly comes from *training* the diffusion model and not from sampling from it. Therefore, we see little benefit in sampling less than about $10 \\times |\\mathcal D_\\text{train}|$ data points, at which point sampling more has diminishing returns.\n\nOn the wider point of computational cost, note that our method only increases training time but not test time, which is more important in many applications. Also, samples can be reused when training VAEs with different configurations for cross-validation. For further comments, please refer to item (2) in our overall response above.\n\n> **Q2** - In your table 3, the method effectively reduced all three gaps the most for Binary MNIST and Fashion MNIST but not CIFAR10. Can you explain why it fails on CIFAR10? It looks to me that when the dataset becomes complex, your method loses its advantage. To convince the audience, you should include more datasets.\n\nThank you very much for this comment! Unfortunately Table 3 was showing wrong values (the table was transposed). We apologize for this inconvenience and we uploaded a new version of the paper that fixes this issue. The correct version of the table shows that our method keeps its advantage and does not fail on CIFAR-10. To make it easier to see the gaps, we also added their numerical values to the right margins of Figures 2, 3, 4, and 6 in the main paper.\n\n> **Q3** - In figure 2, CIFAR10, the test data\u2019s curves seem to keep increasing. For both augtuned and DMaaPx. If trained longer, what would be the trend?\n\nThank you for this question. \nWe agree that the ELBOs for data augmentation and our method are not fully converged for CIFAR-10.\nThe main message of this plot is that, for normal training, one should have stopped after about 200 epochs since the model already starts to overfit at this point. By contrast, we do not see any overfitting with both DMaaPx and traditional data augmentation for 5x more training epochs.\n\nWe are training the models for 2000 epochs on CIFAR-10 at the moment, and we will include the results and a plot as soon as they finish. We already observe in the existing plot that the generalization gap for data augmentation starts to widen when our proposed method still has a negligible gap. We therefore expect that the gap in our method either stays constant (as it does for BinaryMNIST and FashionMNIST) or is the last one to widen.\n\nThank you again for your review! We hope that our response and the fixed Table 3 resolve any remaining doubts about our paper, and would appreciate it if you update the assessment accordingly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248424279,
                "cdate": 1700248424279,
                "tmdate": 1700248424279,
                "mdate": 1700248424279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xzYawvRUPE",
                "forum": "pyW37euNXb",
                "replyto": "8YEiIBeXvx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1886/Reviewer_nWgS"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewer_nWgS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks. I really appreciate your reply and additional experiments. Your answer solved my previous questions. However, after taking a closer look at your appendix, I have concern about the influence of paper. In the evaluation of donwstream applications of VAE (RL, RC, SQ), Your method does not have large enough difference in terms of all scores, even compared with the normal training. So for now I'll keep my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280221124,
                "cdate": 1700280221124,
                "tmdate": 1700280221124,
                "mdate": 1700280221124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SVF621uFnl",
            "forum": "pyW37euNXb",
            "replyto": "pyW37euNXb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1886/Reviewer_ZW6a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1886/Reviewer_ZW6a"
            ],
            "content": {
                "summary": {
                    "value": "In this work, authors propose to distill knowledge from diffusion models to VAEs. Authors show that this approach has some effect on the generalisation of VAE and it\u2019s robustness against adversarial attacks"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The proposed idea seems to be novel\n- The submission is well written and easy to follow\n- The literature review is deep and well presented"
                },
                "weaknesses": {
                    "value": "- It is known that diffusion models do not necessarily represent the whole training distribution, also in the image domain (as mentioned in the submission). Could it be that VAE better aligned to the subset of training data that was generated by diffusion? Is the distribution of ELBO on the test-set similar for both approaches?\n- In the submission it is claimed that the main goal is to \u201cimprove the desirable functionalities of VAEs such as representation learning\u201d, while the evaluation only tackles the quality of data modeling task. It\u2019d be interesting to compare the samples quality between diffusion model from which the knowledge was distilled to the VAE.\n- The proposed method has tremendous computational cost that is not evaluated. Training VAE for 1000 effective epochs requires sampling 1000 * $|D_{train}|$ samples from diffusion model, or as presented in Figure 5 at least 10 *$|D_{train}|$ samples.  So the training is at least a few orders of magnitude slower because of sampling, not to mention the cost of training the diffusion model itself.\n- The results suggest that distilling knowledge from diffusion models to VAEs introduce some performance gain for simple datasets like MNIST and FMnist, while for slightly more complex dataset - CIFAR10 the performance is comparable to the standard training with augmentations."
                },
                "questions": {
                    "value": "- In section 4.2 there is a discussion on difference between training VAEs on augmented data and training them on data generated from diffusion model. The authors state that training with augmentations may result in problems. However, at the same time diffusion models are usually also trained with the same data augmentation techniques. Does it mean that in this case it was omitted?\n- Are the results presented in Figures 2 and 6 statistically significant? The differences in some cases are extremely small, while it seems like results from a single run (judging from the background plot)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698923991937,
            "cdate": 1698923991937,
            "tmdate": 1699636118680,
            "mdate": 1699636118680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C9n5hpiSzh",
                "forum": "pyW37euNXb",
                "replyto": "SVF621uFnl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer ZW6a (R1) from the authors"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback! We are glad that you think the proposed idea is \u201cnovel\u201d, and the paper is \u201cwell written and easy to follow\u201d! We address your questions below, and we updated the PDF accordingly.\n\n> **Q1** - [The diffusion model might focus on a specific subset of the training data.] Is the distribution of the ELBO on the test-set similar for both approaches?\n\nThank you for this interesting question. We updated the paper and now provide an analysis of the distribution of the ELBO in Appendix G (_new_). We find that our method improves the ELBO on almost all (99.9 %) of the test points (Figure 9 (b)), and we couldn\u2019t identify a subset of test data points where the improvement was particularly small or large.\n\n\n> **Q2** - [...] it is claimed that the main goal is to \u201cimprove the desirable functionalities of VAEs such as representation learning\u201d, while the evaluation only tackles the quality of data modeling task.\n\nWe believe that there is a misunderstanding, as our experiments in Sections 5.3 and 5.4 do evaluate representation learning. For VAEs, \u201crepresentation learning\u201d refers to the (output of) the encoder. Section 5.3 evaluates the amortization gap, where a small amortization gap corresponds to a good encoder. Section 5.4 evaluates adversarial robustness. While there are various forms of robustness in VAEs, our experiments specifically evaluate robustness of the encoder, i.e., we test whether an unnoticeable change of the input image can trick the encoder into outputting a semantically different representation. We recognize that this can be easy to miss since we evaluate the success of an attack back in image space.\n\nHowever, based on reviewer feedback, we added more direct evaluations of practical downstream applications of VAEs in Appendix F (_new_), specifically representation learning tasks (classifying latent representations), reconstruction tasks, and (for completeness) also data generation tasks. While performance differences are indeed small, our proposed method achieves best performance for all tasks and all metrics except when measuring sample quality (SQ) by inception score (IS). This is consistent with our claim that the proposed method mainly fixes the encoder, which affects representation learning and reconstruction but not sample quality.\n\n> **Q3** - [compare sample quality between diffusion model and VAE]\n\nWe kindly refer the reviewer to item (1) in our \u201cOverall response regarding sample quality and computational cost\u201d above, which addresses the question of sample quality. In brief, we see little value in using VEAs for purely data generative downstream tasks as diffusion models are known to produce much better samples. However, VAEs can be used for many other downstream tasks that require semantically meaningful representations, which diffusion models don\u2019t provide. Indeed, our paper focuses on the encoder of the VAE since this is the one that tends to overfit.\n\n> **Q4** - [computational cost]\n\nWe provide numbers for the computational cost in Appendix C. Note that our method only increases training time but not test time, which is more important in many applications. Also, samples can be reused when training VAEs with different configurations for cross-validation. For further comments, please refer to item (2) in our overall response above.\n\n> **Q5** - [performance gains only on MNIST and FMNIST, but hardly for CIFAR10]\n\nPlease note that, as we stress in Section 5.2, we are interested not only in the test ELBOS but also in the generalization _gap_ (Eq. 8): small generalization gaps are very useful as they imply that ELBOs evaluated on the training set can be used for predicting the final performance of the VAE. The proposed DMaaPx has a significantly smaller generalization gap than other methods in all settings.\n\n\n> **Q6** - diffusion models are usually also trained with the same data augmentation techniques. Does it mean that in this case it was omitted?\n\nThank you for bringing this up. You are right! We indeed omitted data augmentation when training the diffusion model (see details in Appendix C) to not let any augmentation leak to our generated samples.\n\n> **Q7** - Are the results presented in Figures 2 and 6 statistically significant? [...] it seems like results from a single run\n\nThank you for this valuable point! In the updated version of the paper, we now show means and standard deviations in Figures 2, 3, and 4 on CIFAR-10. We find that our results are consistent across different random seeds. Experiments for the other data sets ~~and for the amortization gap~~ are still running, and we will update the paper as soon as they finish.\n\nThank you again for your review. We would appreciate it if you could let us know whether our above responses resolve your concerns, and change your assessment of our work accordingly."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227733563,
                "cdate": 1700227733563,
                "tmdate": 1700248646379,
                "mdate": 1700248646379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SDMHHEPrc7",
                "forum": "pyW37euNXb",
                "replyto": "C9n5hpiSzh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1886/Reviewer_ZW6a"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1886/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1886/Reviewer_ZW6a"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for answering my questions and to satisfy my curiosity. Thank you for additional experiments that tackle the main contribution of the paper. However, it seems that this approach hardly helps in representation learning as the differences in e.g. classification task are within the statistical error.\n\nTherefore, I decide to keep my initial score"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651340774,
                "cdate": 1700651340774,
                "tmdate": 1700651340774,
                "mdate": 1700651340774,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]