[
    {
        "title": "Like Oil and Water: Group Robustness Methods and Poisoning Defenses Don't Mix"
    },
    {
        "review": {
            "id": "4RfWe5n0i1",
            "forum": "rM9VJPB20F",
            "replyto": "rM9VJPB20F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6555/Reviewer_Fmsc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6555/Reviewer_Fmsc"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the tension between group robustness methods and poisoning defenses; in particular, the authors show that group robustness methods which pseudo-label the minority group are often unable to distinguish minority samples from poison samples. On the other hand, poisoning defenses can make the minority group more difficult to learn, which hurts group robustness. The conclusions are supported by a variety of experiments and a simple theoretical result."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This paper makes an important contribution by exposing the incompatibility of current group robustness and poisoning defenses, especially showing that group robustness methods can make models more susceptible to poisoning adversaries. This observation is novel as far as I know, and should be interesting for the community.\n2. The experiments use a variety of robustness and poisoning techniques and metrics (if not datasets) and the extension to a federated learning application is welcome. The authors also provide a theoretical result in a simple yet well-motivated setting.\n3. The authors show that a naive combination of both techniques fails to resolve their tension, an interesting result which could represent a new direction of research for the community."
                },
                "weaknesses": {
                    "value": "The main weakness of the paper is its limited dataset evaluation, which consists only of Waterbirds and a 10% subset of CelebA. While the experiments are indeed comprehensive with different techniques and metrics, the size and composition of the datasets questions whether the results hold more generally.\n\nIn particular, Waterbirds is known to the community to be a limited benchmark and should mostly be used for sanity checking or simple comparisons. First, Waterbirds is an easy benchmark in the sense that simple class balancing methodologies using ERM achieve similar worst-group accuracy performance to methods like Group DRO which utilize group information (as close as 1% WGA in [1]). Second, the validation and test distributions of Waterbirds are more amicable compared to the training dataset, as they are group-balanced conditioned on the classes; this can skew model selection in some cases. Third, Waterbirds is known to contain incorrect labels as well as images with both landbirds and waterbirds present [2]. Finally, while CelebA is a more complex dataset than Waterbirds, and the 10% subset was likely necessary due to computational restrictions, the fact remains that both datasets are small, belong to the vision domain, and only utilize binary-valued spurious features.\n\nSome suggestions may include Spawrious [3] for the vision domain (in particular because it is not too large; it is a difficult dataset which is smaller than CelebA) or MultiNLI [4] for the language domain (which would also introduce a non-binary-valued spurious feature).\n\nI also found the writing unclear in some parts, detailed in the next section."
                },
                "questions": {
                    "value": "1. I would encourage the authors to perform a more detailed literature review, as the related work subsection for group robustness is out of date and does not include any references more recent than 2021. There have been significant contributions since 2021, particularly in methods which pseudo-label the minority group (with or without an explicit identification model), e.g., [2,5,6,7,8,9].\n2. The definitions in Section 2.2 are unclear and should be improved.\n    * I believe the reason that the definitions of groups differs from [4] is to account for the dirty-label samples later in Section 3.3. This should be explicitly stated prior to the definitions, as otherwise the definitions seem inconsistent with the literature. Is the worst-group accuracy in the remainder of the paper computed with respect to these new groups (i.e., 8 groups) or the original groups (i.e., 4 groups)? This should also be made explicit.\n    * The definition of a minority group seems somewhat arbitrary and inconsistent with the literature. As far as I understand the definition given is that for a dataset of size $n$, a group $g\\in G_Y$ is a minority group iff the number of data belonging to $g$ is less than $n/|G_Y|$. If we use 4 groups for CelebA as is common in the literature, this would mean that blond-female is a minority group in CelebA (it has 22880 datapoints while the total data is 162770 over 4 groups), whereas the text states that the only minority group is blond-male. On the other hand, if we use 8 groups as I believe this work does, then blond-female is barely a majority group, and hence it is not clear that it remains a majority group when a random 10% subset of CelebA is taken.\n3. The bibtex could use an update: there are some extra braces and capitalization errors.\n4. It would be helpful for reference to include a table with the proportions of each group and class in each benchmark dataset, as well as the proportions of each group and class in the 10% sample of CelebA used in the experiments. See [9, 10] for examples.\n5. The definitions in Section 3.3 are unclear and should be improved. First, it should be made clear that $I(x)\\in [0,1]$, i.e., that it is a probability output rather than a logit output. Second, $I(x)_y$ is referred to as the \u201cconfidence\u201d of model $I$ on input $x$ for class $y$, which is misleading: if $I(x)_y=0$ the model is in fact very confident (that $x$ does not belong to class $y$). I would expect the definition of \u201cconfidence\u201d in this case to look more like $2|I(x)_y - 1/2|$, since $1/2$ can be considered as the \u201cleast confident\u201d prediction in a binary classification problem.\n6. In Appendix A.2, should \u201cDLDB\u201d read \u201cDLBD\u201d?\n7. In Figure 1, it should be made clear that the lighter-colored circles and triangles are amplified points (it is otherwise an excellent figure). Also, the two shades of red are indistinguishable.\n8. The phrase \u201cthis boost is almost as high as it could have been\u201d in Section 3.2 is confusing and could use a rephrase, perhaps connecting it to the worst-case approach tested earlier.\n9. In Section 5, \u201cconciliate\u201d should perhaps be replaced by \u201creconcile\u201d.\n10. At the bottom of page 7, should the citation to Wu et al. be in parentheses, i.e., \\citep?\n\n***Recommendation***\n\nOverall, the novelty and importance of the contribution cause me to lean slightly more towards acceptance than rejection, but I believe the paper would be greatly improved with additional results on more rigorous benchmark datasets as detailed in the Weaknesses section, as well as improvements to the clarity of the writing.\n\n***References***\n\n[1] Idrissi et al. Simple data balancing achieves competitive worst-group-accuracy. CLeaR, 2022.\n\n[2] Taghanaki et al. MaskTune: Mitigating Spurious Correlations by Forcing to Explore. NeurIPS, 2022.\n\n[3] Lynch et al. Spawrious: A Benchmark for Fine Control of Spurious Correlation Biases. ArXiv, 2023.\n\n[4] Sagawa et al. Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. ICLR, 2020.\n\n[5] Kim et al. Learning Debiased Classifier with Biased Committee. NeurIPS, 2022.\n\n[6] Sohoni et al. BARACK: Partially Supervised Group Robustness With Guarantees. ICML SCIS Workshop, 2022.\n\n[7] Zhang et al. Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations. ICML, 2022.\n\n[8] Qiu et al. Simple and Fast Group Robustness by Automatic Feature Reweighting. ICML, 2023.\n\n[9] LaBonte et al. Towards Last-layer Retraining for Group Robustness with Fewer Annotations. NeurIPS, 2023.\n\n[10] Kirichenko et al. Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. ICLR, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6555/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6555/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6555/Reviewer_Fmsc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698444574212,
            "cdate": 1698444574212,
            "tmdate": 1699636740035,
            "mdate": 1699636740035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CR6t4xqv5Q",
                "forum": "rM9VJPB20F",
                "replyto": "4RfWe5n0i1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Fmsc (1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their very constructive and detailed feedback! Please find below our response to your comments:\n\n> The main weakness of the paper is its limited dataset evaluation, which consists only of Waterbirds and a 10% subset of CelebA. While the experiments are indeed comprehensive with different techniques and metrics, the size and composition of the datasets questions whether the results hold more generally. [...] Finally, while CelebA is a more complex dataset than Waterbirds, and the 10% subset was likely necessary due to computational restrictions, the fact remains that both datasets are small, belong to the vision domain, and only utilize binary-valued spurious features.\n\nTo address the reviewer's concern, we ran the experiments from Table 3 on different sizes of CelebA (all the other experiments in the paper were already on full CelebA). Below, we show the effect of EPIc (poisoning defense) on the WGA (results are over 3 runs):\n\n|    |20% of CelebA  | 50% of CelebA | Full CelebA      |\n|----|---------------|---------------|---------------|\n|   Ideal  | 52.3 +- 1.6  | 50.3 +- 1.9   | 45.9 +- 5.8 |\n| Standard  | 43.1 +- 5.5  | 46.4 +- 2.8   | 43.6 +- 3.0|\n|   Worst  | 32.3 +- 2.7  | 32.0 +- 0.8   |  37.7 +- 3.8|\n\nThis shows that the trends we observed (i.e., EPIc hurts the WGA by eliminating minority groups) are consistent as the ideal WGA (when we intervene to prevent EPIc from eliminating any minority samples) is higher than the standard WGA (applying EPIc w/o any intervention). \n\nAdditionally, to show the amplification due to group robustness methods, we ran experiments with the Subpopulation Attack (SA) on full CelebA for JTT and ERM (the baseline suggested by Reviewer Shsu).\n\n|Poison %      |      0.05%   |       0.1%   |        0.2%   |   0.3%  |    0.5% |      1% |          2%|\n|--------------------|-----------|-----------|----------|-----------|-----------|-----------|-----------|\n|JTT (ASR)   |         0.1%  |      1.7%  |          2.4% |     3.5%    |    6.3%   |   14.1%  |    46.8%|\n|ERM (ASR)  |  0.1%   |     0.1%         |  0.2%   |    0.0%   |    0.1%     |  0.1%       |  0.3%|\n\n\n| Poison %       |  0.05%   |        0.1%     |      0.2%   |      0.3%  |      0.5%  |     1%   |       2%|\n| ------------------- | -----------  | ----------- |----------|-----------|-----------|-----------|-----------|\n|JTT (WGA)    |     81.7%  |    82.2%   |      83.3%  |      83.3% |    83.3%  |  77.9% |  48.2%|\n|ERM (WGA) |     43.3%    | 43.3%   |       47.2%   |    42.8%  |   41.1%  |  45.0% |  45.6%|\n\nIn the first table, we observe that the amplification is generally high (a large ASR gap between JTT and ERM). The second table shows that JTT still works as intended by improving the WGA over ERM. In conclusion, our observations remain consistent for the full CelebA as well.\n\n> Some suggestions may include [...] MultiNLI [4] for the language domain (which would also introduce a non-binary-valued spurious feature). [...] There have been significant contributions since 2021, particularly in methods which pseudo-label the minority group (with or without an explicit identification model), e.g., [2,5,6,7,8,9]\n\nThank you for suggesting these experiments! To ensure the validity of our observations for newer group robustness methods and more advanced datasets, we applied AFR [1] (referenced as [8] by the reviewer) on MultiNLI. As in our previous experiment, we consider SA as the attack, ERM as a baseline (no group robustness method), and report both the ASR and WGA.\n\n| Poison %          |    0.5%     |      1%     |\n| ------------------- | -----------  | ----------- |\n|AFR (ASR)       |    2.1%       |  14.4%      |\n|ERM (ASR) |   1.0%      |     5.3%    |\n\n\n| Poison %          |    0.5%      |     1%     |\n| ------------------- | -----------  | ----------- |\n|AFR (WGA)        |   75.3%     |      78.4%  | \n|ERM (WGA)  |  68.0%      |     70.0%   | \n\nWe observe the same tension in these experiments: while AFR improves the WGA over the baseline (as expected), it also inadvertently amplifies the ASR (e.g., from 5.3% to 14.4%, a significant boost for a weak attacker).\n\n> Some suggestions may include Spawrious [3] for the vision domain [...].\n\nWe agree with the reviewer that Spawrious would be a good addition to our experiments. Unfortunately, due to time limitations, we were unable to conclude the experiments on this benchmark before the rebuttal deadline. However, we are committed to include them in the final version (we will include them in the Appendix).\n\n> I would encourage the authors to perform a more detailed literature review [...]\n\nWe will update our paper's appendix with the discussion we shared with Reviewer Shsu on the different (more recent) group robustness methods (some of them are referenced as [5,7,8,9] by the reviewer), and the parallels among the heuristics they rely on."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732347993,
                "cdate": 1700732347993,
                "tmdate": 1700740749549,
                "mdate": 1700740749549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EHZXWDdxIw",
                "forum": "rM9VJPB20F",
                "replyto": "4RfWe5n0i1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Fmsc (2)"
                    },
                    "comment": {
                        "value": "> The definitions in Section 2.2 are unclear and should be improved [...] The definitions in Section 3.3 are unclear and should be improved. \n\nWe agree with the reviewer. We included more clear definitions for groups, as well for the identification model I(x) in the paper.\n\n> [Comments about the bibtex, styling errors, colors in the figures, confusing phrases, and other typos]\n\nThanks for pointing these out, we fixed all of them!\n\n> It would be helpful for reference to include a table with the proportions of each group and class in each benchmark dataset, as well as the proportions of each group and class in the 10% sample of CelebA used in the experiments. \n\nWe agree, we will include it in an updated version of the paper.\n\n\n[1] Simple and Fast Group Robustness by Automatic Feature Reweighting, Qiu et al. (ICML'23)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732405834,
                "cdate": 1700732405834,
                "tmdate": 1700737862781,
                "mdate": 1700737862781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CQdmDAxAaC",
            "forum": "rM9VJPB20F",
            "replyto": "rM9VJPB20F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6555/Reviewer_Shsu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6555/Reviewer_Shsu"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the interplay between a certain type of group robustness methods and poisoning attacks. They show that methods to achieve high accuracy on minority groups in the absence of (validation) group annotations also often flag poisoned examples, and thus amplify poisoning attacks. Additionally, they show that poisoning defenses often flag minority examples as poisons, thus removing them from the dataset. Finally, the authors complement their experimental findings with an formal impossibility result in a toy setting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The authors present an interesting connection/trade-off between two fields within ML. The problem of exploring trade-offs between competing objective is important."
                },
                "weaknesses": {
                    "value": "### Overclaiming / limited scope of conclusions\nI believe that the statement\n\"We show experimentally that group robustness methods fail to distinguish minority groups from poisons.\"\nis misleading. In particular, the authors only consider group robustness methods which use a loss-based heuristic in lieu of *validation* group annotations. This significantly reduces the scope of authors' contributions (to the best of my knowledge, at the time of writing, only ~3 group robustness methods use the above heuristic), and in my opinion is not properly reflected in the writing. \n\n### No evidence the phenomenon persists for attacks with high efficacy (success rate)\n\nMy main concern with the paper is the following: if I have an attack (e.g., backdoor attack) with very high ASR, I would expect to have low loss on poisoned (e.g., backdoored) examples in the validation set. Hence, there's no reason for me to expect that JTT/GEORGE/etc would flag them as minority examples. In most of the reported setups, e.g. Table 2, the attack success rate is very low (e.g., 20%), even in the \"worst\" case when the group robustness method amplifies the attack. \n\n\nMinor point: It would have been nice to report results for the state of the art method (to the best of my knowledge) that uses the loss-based heuristic [1].\n\n[1] Qiu, Shikai, Andres Potapczynski, Pavel Izmailov, and Andrew Gordon Wilson. \"Simple and Fast Group Robustness by Automatic Feature Reweighting.\" arXiv preprint arXiv:2306.11074 (2023)."
                },
                "questions": {
                    "value": "### Is ASR actually amplified by group robustness methods\n\nI am confused the statement \"The large gap between (6.7% \u00b4 97.4%) the standard and ideal cases shows an opportunity for better heuristics\" (referring to Table 2). It is \"not the job\" of the group robustness method to remove the poisoning attack. Thus, a more fair comparison in my opinion would be to compare \"standard\" ASR against the baseline of not applying the group robustness method. What is the gap then?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698693488923,
            "cdate": 1698693488923,
            "tmdate": 1699636739922,
            "mdate": 1699636739922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HiSoqSnvON",
                "forum": "rM9VJPB20F",
                "replyto": "CQdmDAxAaC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Shsu (1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback! Please find below our response to each of your comments:\n\n> \u2026the authors only consider group robustness methods which use a loss-based heuristic in lieu of validation group annotations. This significantly reduces the scope of authors' contributions\u2026 \n\nAlthough we experimented with methods that specifically use a loss-based heuristic, we believe that many other group robustness methods rely on similar heuristics, perhaps less directly. Here, we discuss four methods from recent literature that implemented seemingly different heuristics that will align with loss-based heuristics:\n\n* **Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations, Zhang et al. (ICLR\u201922)**: This method applies contrastive learning to push together the representations of training samples that are labeled into the same class but predicted differently. Since, the low-budget poisoning attacks we consider often generate hard-to-learn, misclassified samples, we believe this method will also suffer from the tension we identify, e.g., misclassified poison samples of class $y$  will be represented similarly to correctly classified samples of class $y$, which will amplify their effectiveness as the model becomes less likely to misclassify them.\n\n* **Dropout Disagreement: A Recipe for Group Robustness with Fewer Annotations, LaBonte et al. (NeurIPS\u201922 Workshop)**: This method uses the disagreements between different forward passes of the model with dropout to find which training samples to amplify. Different dropout forward passes are known to disagree on samples where the model has high uncertainty [1], which are often hard-to-learn, higher-loss samples. As a result, we believe this method will also suffer from the same tension we identified, e.g., the hard-to-learn poison samples we generated will create dropout disagreements and will be amplified.\n\n* **Learning Debiased Classifier with Biased Committee, Kim et al. (NeurIPS\u201922)**: This method relies on disagreements among the members of an ensemble of models to identify the \u201cbiased\u201d samples (i.e., the samples that contain spurious correlations). If a sample has low ensemble agreement, it will be amplified by this method. Ensemble disagreement is a known uncertainty metric [2] in the literature. As a result, we believe that this method will also suffer from the same tension we identified, e.g., the hard-to-learn poison samples will be misclassified by more members of the ensemble, and will be amplified.\n\n* **Towards Last-layer Retraining for Group Robustness with Fewer Annotations, LaBonte et al. (NeurIPS\u201923)**: This method constructs a reweighting set based on either (1) ERM model\u2019s misclassifications, or (2) the misclassifications of an early-stopping model, or (3) dropout disagreement (similar to the Dropout Disagreement paper) or (4) the disagreements between the ERM and early-stopping models. All these heuristics to identify minority group samples will end up identifying the hard-to-learn poisoning samples as well. The authors identified (4) as the most promising heuristic. It is known that hard-to-classify samples are learned at later iterations of model training [3]. As a result, we believe that method will also suffer from the same tension we identified, e.g., the hard-to-learn poison samples will be classified differently by the early-stopping and regular models, and will be amplified.\n\n\nThese common threads show that, despite not using the loss heuristic directly, many methods in this line of work rely on related ideas that will amplify uncertain, misclassified, high-loss, or hard-to-learn samples. Moreover, it is known that many example difficulty metrics are highly correlated with one another [3,4]. Ultimately, we believe that these related heuristics cannot avoid the tension we identified because low-budget poisoning attacks generate hard-to-learn samples. To address the reviewer's concern and communicate that our findings have wider implications than JTT/GEORGE, we will add this discussion to our paper (we plan to add it in the Appendix). Also, please find below our new experiments on AFR [7], a state-of-the-art group robustness method."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716138672,
                "cdate": 1700716138672,
                "tmdate": 1700740414157,
                "mdate": 1700740414157,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tc2796JzhN",
            "forum": "rM9VJPB20F",
            "replyto": "rM9VJPB20F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6555/Reviewer_a2i2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6555/Reviewer_a2i2"
            ],
            "content": {
                "summary": {
                    "value": "This work identifies a critical conundrum between group robustness and poisoning resilience metrics in machine learning. The contributions are 3-fold:\n\n1) It presents empirical evidence illustrating that methods designed for group robustness are unable to differentiate between minority groups and poisoning data. This inability exacerbates the impact of poisoning attacks, a claim further substantiated by theoretical support under various assumptions.\n\n2) Further, it demonstrates through empirical studies that standard defenses against data poisoning struggle to distinguish poisoned data from that of minority groups. This leads to a compromise in group robustness, an issue observed in both centralized and federated learning setups.\n\n3) Finally, this paper concludes that merely combining group robustness strategies with poisoning defense mechanisms fails to address these challenges, indicating a need for more nuanced solutions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) This paper is well written and it is easy to follow. The takeaways from each section are explicit, novel and clear.\n\n2) Figure 1 clearly conveys the key ideas.\n\n3) The findings are interesting and useful to the community."
                },
                "weaknesses": {
                    "value": "1) Experiments. The results on CelebA (Section 4) only uses a randomly sampled 10% of the dataset which is concerning. Can the authors also consider the full CelebA setup, so that the results are more reliable?\n\n2) Why are the ASR values negative in Table 3 (Waterbirds / SA setup).\n\n3) Could the authors elaborate on whether any experiments were conducted involving the other two types of poisoning defenses mentioned in Section 2.1?\n\n\n\nMinor concerns\n\n1) Title Appropriateness: The broad focus implied by the current title, \"Poisoning Defenses,\" does not accurately reflect the specific emphasis of the work on poisons that are challenging to learn. A more precise title would set clearer expectations for readers.\n\n\n\nOverall I enjoyed reading this paper. In my opinion, the strengths of this paper outweigh the weaknesses. Authors please consider addressing the weaknesses above during rebuttal."
                },
                "questions": {
                    "value": "Please see Weaknesses section above for a list of all questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6555/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6555/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6555/Reviewer_a2i2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817767075,
            "cdate": 1698817767075,
            "tmdate": 1699636739771,
            "mdate": 1699636739771,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4I3DRET8DO",
                "forum": "rM9VJPB20F",
                "replyto": "Tc2796JzhN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer a2i2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback! Please find below our response to each of your comments:\n\n> The results on CelebA (Section 4) only uses a randomly sampled 10% of the dataset which is concerning. Can the authors also consider the full CelebA setup, so that the results are more reliable?\n\nTo address the reviewer's concern, we ran additional experiments with 20%, 50% and 100% of the CelebA. The results, over 3 runs, showing the WGA in each scenario are in the following table:\n\n|    |20% of CelebA  | 50% of CelebA | Full CelebA      |\n|----|---------------|---------------|---------------|\n|   Ideal  | 52.3 +- 1.6  | 50.3 +- 1.9   | 45.9 +- 5.8 |\n| Standard  | 43.1 +- 5.5  | 46.4 +- 2.8   | 43.6 +- 3.0|\n|   Worst  | 32.3 +- 2.7  | 32.0 +- 0.8   |  37.7 +- 3.8|\n\nThe persistent gap between the Ideal and Standard cases demonstrates the negative impact of EPIc on group robustness (i.e., our findings are consistent).\n\n\n> Why are the ASR values negative in Table 3 (Waterbirds / SA setup).\n\nFor the SA, we define the ASR as the relative accuracy drop of the attack on the target group over a non-poisoned model. So, in case the defense not only prevents the accuracy drop by removing the poisons but also slightly increases the accuracy on the targeted group, then the ASR would be negative. We will clarify this in the updated paper.\n\n\n> Could the authors elaborate on whether any experiments were conducted involving the other two types of poisoning defenses mentioned in Section 2.1?\n\nIndeed, in our paper, we only consider defenses that assume that poisons are hard to learn as more realistic, weak, attackers are the ones who would benefit from the amplification offered by group robustness methods. However, we believe that poisoning defenses designed against easy-to-learn poisons (e.g., [1]) would indeed not eliminate legitimate minority samples and hence not hurt the worst-group accuracy (though, such defenses struggle against attacks that craft hard-to-learn poisons [2]).\n\nA promising future direction could be the defenses that assume poisons and clean samples follow different distributions, so they create a small, poison-free set of samples for each group to *erase* poisons (similar to [3] but with a group-balanced clean data to preserve group robustness).\n\n\n> The broad focus implied by the current title, \"Poisoning Defenses,\" does not accurately reflect the specific emphasis of the work on poisons that are challenging to learn. A more precise title would set clearer expectations for readers.\n\nWe agree with the reviewer's concern that our title might be too broad. We will specify that we study weaker poisoning attacks, e.g., \"Resilience to Weak Poisoning Attacks and Group Robustness are at Odds\". Please let us know if you have a suggestion!\n\n\n[1] Anti-Backdoor Learning: Training Clean Models on Poisoned Data, Li et al. (NeurIPS'21)\n\n[2] Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information, Zeng et al. (CCS'23)\n\n[3] Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks, Li et al. (ICLR'21)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716775260,
                "cdate": 1700716775260,
                "tmdate": 1700716775260,
                "mdate": 1700716775260,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4ziEdBbsZo",
            "forum": "rM9VJPB20F",
            "replyto": "rM9VJPB20F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6555/Reviewer_wCf5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6555/Reviewer_wCf5"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the existing tension between the group robustness and the resilience to poisoning attacks. The authors argued that, group robustness methods will unavoidably amplify the poisoning samples and boost poisoning performance. On the other hand, poisoning defenses will remove poisoning outliers will also unavoidably remove the minority samples. The authors advocate for tacking this inherent tension in future works."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem of the inherent tension between group robustness and poisoning resilience is an interesting problem/\n2. The presented empirical results are well-presented to support the main claim in the paper."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is to make an impossibility type of claim without a more principled understanding on the problem.\nIn particular, the authors aim to find a tension between any poisoning attack and the distributionally robust optimization methods, which probably undermines the much-needed principled understanding. In particular, I am expecting some type of result that points out the tension between a poisoning with a clearly defined attacker capability and objective, and the group robustness optimization techniques (e.g., using loss-based thresholding). This result should show that, in order to achieve the attacker objective maximally, the poisoning points will also unavoidably become the amplified minority samples. This way, we can make claims on the impossibility of having group robustness and poisoning resilience at the same time. My major concern here is, the tensions between poisoning and group robustness are drawn some limited empirical results and the poisoning attacks also do not reflect what the attacker in practice may really care to do. For example, many ASR are quite low as presented in Table 2 and I am not sure if it is because the poisoning attacks are configured in some undesirable ways (with respect to the attack objective), who may choose a different approach to achieve higher ASR. The alternative, in turn, may no exhibit a strong tension. Related to this, the impossibility result in the paper only shows that there exist some poisoning points that are harder to learn but did not reason whether these poisoned points are useful for achieving the attacker objectives. I would suggest the authors to first clearly define the threat model and then conduct more rigorous analysis on how this can interfere with group robustness methods. Then, the insights are supported through extensive empirical evaluations."
                },
                "questions": {
                    "value": "1. In page 4, what is the intuition behind configuring the poisoning attacks as described? The configuration seems to be different from the one in the original paper and the authors should clearly explain the reason. \n2. Will the gradient shaping method [1] enable a mitigation to the currently observed tension? \n\n[1] Hong et al., \"On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping\", arXiv 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699138241672,
            "cdate": 1699138241672,
            "tmdate": 1699636739640,
            "mdate": 1699636739640,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "di8HC3NPvC",
                "forum": "rM9VJPB20F",
                "replyto": "4ziEdBbsZo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wCf5 (1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback! Please find below our response to each of your comments:\n\n> \u2026poisoning attacks also do not reflect what the attacker in practice may really care to do. For example, many ASR are quite low as presented in Table 2 and I am not sure if it is because the poisoning attacks are configured in some undesirable ways\u2026\n\nAn attacker in practice aims to achieve the highest ASR possible within their limited budget. In the case of a poisoning adversary, the number of injected samples determines the real-world budget. The lower the budget is, the more realistic an attack becomes (e.g., the attacker needs to compromise fewer websites to inject images [1]). \n\nA main takeaway from our experiments is that low-budget attacks where the adversaries struggle to achieve high ASR (hence the results in Table 2) can benefit from the defender\u2019s use of group robustness methods such as JTT. The same cannot be said for high-budget attacks that can already achieve high ASR (thus, amplification is less critical). Therefore, the low ASR of the attacks we performed (Table 2) is not due to any specific configuration of the attacks (e.g., hyper-parameters) but due to the considered attacker budgets. This allows us to expose how low-budget attacks (more realistic but weaker) can be inadvertently amplified into stronger ones, essentially giving the attacker \u201cmore bang for their buck\u201d.\n\nFurthermore, we would like to clarify that for the Subpopulation Attack (SA) in Table 2, ASR indicates a **relative accuracy drop** on the attack's target group over a non-poisoned model. Therefore, some of our experiments already consider more powerful attacks (e.g., when the ASR is 31%), in which our findings are consistent.\n\n\n> I am not sure if it is because the poisoning attacks are configured in some undesirable ways (with respect to the attack objective), who may choose a different approach to achieve higher ASR \u2014----------- In page 4, what is the intuition behind configuring the poisoning attacks as described?\n\nTo address the reviewer\u2019s concerns and questions, we considered attackers (DLBD) selecting poison base samples from different groups of CelebA (the results are over 3 runs):\n\n|          | Gr0 (HRG) | Gr1 (HRG)|Gr2 (HRG)|Gr3 (LRG-1)|\n| -------- | --------  | -------- |---------|-----------|\n| Label/Attribute|0/0|0/1|1/0|1/1|\n| ASR      |97.4 +- 0.1|  97.7 +- 0.1 | 91.2 +- 2.3|   0.1 +- 0.1|\n\n\nThe maximum ASR holds for Gr1, which is the one we chose in our experiments as a base group for crafting the samples. Since the attacker changes the label of the samples they poison, the label/attribute configuration will become 1/1 which matches the one for LRG-1 (as stated in the paper).\n\nFor a dirty-label attack, if the attacker is aware of the groups that are easiest to learn, they can take advantage of it and create poisons from those groups as base samples (this matches the setting from our theoretical result). Also, we show below the average loss of each group when there are no poisons in the dataset:\n\n\n|          | Gr0 (HRG) | Gr1 (HRG)|Gr2 (HRG)|Gr3 (LRG-1)|\n| -------- | --------  | -------- |---------|-----------|\n| Label/Attribute|0/0|0/1|1/0|1/1|\n| Loss Average      |0.16   |       0.07   |       0.79 |          1.90|\n\n\nWe observe that the easier a group is to learn (i.e., less average loss), the higher the ASR will be if the attacker crafts a DLBD attack with base samples from that group."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715583666,
                "cdate": 1700715583666,
                "tmdate": 1700715583666,
                "mdate": 1700715583666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]