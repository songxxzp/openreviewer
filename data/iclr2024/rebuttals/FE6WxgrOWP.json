[
    {
        "title": "Chain of Images for Intuitively Reasoning"
    },
    {
        "review": {
            "id": "sfKue1mwbK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5138/Reviewer_ZLmV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5138/Reviewer_ZLmV"
            ],
            "forum": "FE6WxgrOWP",
            "replyto": "FE6WxgrOWP",
            "content": {
                "summary": {
                    "value": "The paper presents Chain of Images (CoI), which generates images as intermediate representations and inserts them into complex language reasoning problems. \nAn image can represent complex textual logic in a more compact and intuitive way.\nThus, the newly added visual intuition eliminates the textual hallucination problem and introduces visual commonsense knowledge, thus enhancing logical reasoning for current large language models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes to build a symbolic multi-modal model (SyMLM) that can strictly generate images following language commands.\nThe proposed method assists large language models in reasoning. \n2. Experiments on Geometry, Chess, and Common Sense tasks show the effectiveness of Chain of Images prompting compared with pure-language Chain of Thoughts (CoT) baselines.\n3. The paper is well-written and easy to read."
                },
                "weaknesses": {
                    "value": "1. The novelty is relatively limited.\nThere are some similar methodologies for multi-modal large language models to generate images(related work Sec.4.2). The paper is more concerned about using multi-modality to assist language reasoning. The claimed novelty is not that strong -- it is a less-studied task, rather than a new methodology.\n\n2. The method is difficult to extend to broader applications.\n    - The experiments are mainly done on Geometry and Chess task, which is very simplified tasks.\n    - The setting of commonsense tasks is confusing. It seems that the image is generated by stable diffusion instead of the proposed SyMLM. Thus, it seems not a good evaluation for the proposed method.\n    - To generate the Chain of Images, extra training is needed for each task. This largely hurts the universal capability of large language models and hinders broader application. Also, given the diverse reasoning tasks large language models can solve, the experiments cannot show the effectiveness of the method on other reasoning tasks."
                },
                "questions": {
                    "value": "Questions have been mentioned in \"Weakness\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5138/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5138/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5138/Reviewer_ZLmV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697350391135,
            "cdate": 1697350391135,
            "tmdate": 1699636507282,
            "mdate": 1699636507282,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aD2QMxH5a9",
                "forum": "FE6WxgrOWP",
                "replyto": "sfKue1mwbK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer ZLmV,"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We have revised our paper accordingly and will address each of your concerns:\n\n1. **Novelty of the Methodology:**\n    - Unlike MLLMs that focus primarily on solving VQA tasks with a given ground truth image, our Chain of Images (CoI) method tackles more complex tasks by firstly generating images from textual queries. This approach goes beyond merely aligning image features with textual contexts, as is common in VQA tasks. CoI directly incorporates images into the reasoning process, representing a more advanced multimodal reasoning form, thereby enhancing the model's capabilities beyond text-only analysis.\n    - The SyMLLM framework is different from other MLLMs like NExT-GPT, DreamLLM, and MiniGPT-5, which use diffusion models for pixel image generation. SyMLLM excels in creating vector images, offering enhanced control and suitability for complex reasoning tasks, thereby providing a novel approach for multimodal generation models.\n2. **Complexity and Variety of Experiments:**\n    - In addition to the Geometric dataset, we have conducted experiments in Chess, Commonsense, Topological, and Temporal tasks. These tasks, typically challenging for text-only large models, are significantly simplified when converted to images, demonstrating the valuable problem-solving capabilities of the CoI method.\n3. **Commonsense Task Setting:**\n    - CoI primarily focuses on image-centered reasoning and is not solely dependent on SyMLLM. We have conducted experiments with GPT-4V and LLaVA across four tasks, and explored the combination of Stable Diffusion XL and DALL\u00b7E 3 for image generation with LLaVA/GPT-4V for image recognition. This approach allows us to evaluate CoI's effectiveness in commonsense datasets.\n4. **Training Requirements and Universal Capability:**\n    - Our experiments, as shown in Table 3, demonstrate that in-context learning can effectively generate complex images without extensive training. This proves that our method can maintain the universal capability of large language models and does not require significant additional training for each task.\n\nWe look forward to your response and are willing to answer any further questions you may have."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665579451,
                "cdate": 1700665579451,
                "tmdate": 1700665579451,
                "mdate": 1700665579451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iJg3hfhnP4",
            "forum": "FE6WxgrOWP",
            "replyto": "FE6WxgrOWP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5138/Reviewer_bdXh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5138/Reviewer_bdXh"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Chain of Images (CoI), a method that simplifies complex language reasoning tasks into pattern recognition problems by generating a sequence of intermediate images.  \n\nAdditionally, it introduces the Symbolic Multi-Modal Model (SyMLM), which strictly generates these images based on language instructions. \n\nExperiments on three real-world datasets demonstrate that CoI outperforms language-only Chain of Thoughts (CoT) baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I concur with the central thesis of the paper, which advocates for the utilization of visual rationales to enhance existing language-only chain-of-thought methodologies. The exposition of the intuition is lucid, and Figure 1 effectively illustrates the concept.\n\nThe paper excels in devising a method to map textual inputs to visual rationales and subsequently using these visual intermediates to facilitate reasoning. This approach constitutes the paper's primary focus and is a significant contribution to the field."
                },
                "weaknesses": {
                    "value": "1. The submission guidelines specify a maximum of 9 pages for the main text, yet this paper comprises slightly over 8 pages. This brevity raises concerns about the completeness and readiness of the work for publication.\n2. In Section 2.3 (\"Converting Symbols To Image\"), the exposition on SVG and FEN formats is overly succinct. It is unclear to the readers how these formats facilitate the effective translation of symbols into images.\n3. Regarding Figure 5:\n- The manuscript does not elucidate how the question text \"a line segment from (0.5, 3.4) to (3.5, -4.2)\" translates into SVG format. Is there a specific prompt used for this translation?\n- More importantly, the figure directly presents intersection points as intermediate answers without explaining the rationale or method behind their determination.\n4. Table 1 suffers from a lack of detailed analysis:\n- The term \"geometric shapes in each sample, representing the different difficulty levels\" is ambiguous. Providing illustrative examples would be beneficial.\n- While the comparison between \"CoI Acc\" with and without \"Img\" suggests the effectiveness of \"CoI Acc,\" it's puzzling why \"Img Acc\" is nearly 100%, yet \"CoI Acc\" remains low.\n- Importantly, there is a lack of clarity on the methodology used for calculating the similarity between the image generated by SyMLM and the ground truth.\n5. Pertinently, the experiments on the CommonSense dataset appear to test the compositional reasoning ability of LLaVA-13B rather than the efficacy of your proposed method, SyMLM. This creates a disconnect in the paper's narrative."
                },
                "questions": {
                    "value": "1. In Section 2.2, the paper states, \"Subsequently, these images are converted into image embeddings by the image encoder. The embeddings are then concatenated with the text embeddings to generate the next token.\" I wonder if the image embeddings can be directly concatenated with the text embeddings without employing any adapter layers or fine-tuning mechanisms.\n2. Also in Section 2.2, you claim that \"In subsequent experiments, we observe that the accuracy of image generation approaches nearly 100%.\" Could you please elaborate on the metrics used to quantify the similarity between the generated and target images?\n3. In Section 2.3, the text mentions, \"The image is then used to count intersection points.\" Could you specify the methodology employed for counting the intersection points within an image in the context of this work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5138/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5138/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5138/Reviewer_bdXh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697960127602,
            "cdate": 1697960127602,
            "tmdate": 1699636507188,
            "mdate": 1699636507188,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VmQGgMfAV1",
                "forum": "FE6WxgrOWP",
                "replyto": "iJg3hfhnP4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer bdXh,"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review. Here are the responses to your concerns:\n\n1. **Length of the Paper:**\n    - We have rewritten the paper to enhance the reliability of our experiments and improve readability. This revision ensures comprehensive coverage of our research within the specified page limit.\n2. **Explanation on SVG and FEN Formats:**\n    - In all datasets, we uniformly utilize the SVG format to boost the method's universality. SVG files can be directly displayed on screens or easily converted into formats like PNG or JPEG using tools like CairoSVG.\n3. **Clarification on Figure 5 and Intersection point recognization:**\n    - The SVG format follows a fixed syntax, which can be generated using prompts. The coordinates for SVG elements like cycles, lines, and polygons are readily extracted from the question text.\n    - Once a question is converted into an image, it becomes easy for both human observers and our SyMLLM to identify elements like intersection points at a glance.\n4. **Detailed Analysis in Table 1:**\n    - Thank you for your suggestion. In the revised paper, we have thoroughly addressed the three issues you raised, providing a more detailed analysis in the revised paper.\n5. **Experiments on the CommonSense Dataset:**\n    - Our CoI method is not restricted to using SyMLLM exclusively; any model capable of generating images and utilizing them for reasoning is suitable. While SyMLLM excels at solving complex, abstract reasoning tasks, we also explored its effectiveness in general commonsense questions. Based on your advice, we moved this experiment to the supplementary material, focusing the main text on experiments using SyMLLM, thus maintaining consistency in the narrative.\n6. **Image and Text Embedding Concatenation:**\n    - SyMLLM's architecture, similar to LLaVA, includes an image encoder, an LLM, and a projection layer. SyMLLM is distinct in its ability to generate precise images from textual instructions, thanks to fine-tuning on text-to-symbolic datasets.\n7. **Metrics for Image Generation Accuracy:**\n    - In the revised version of the paper, we have redefined the metrics used. Specifically, for the geometric dataset, accuracy is calculated as the number of correctly predicted shapes per data point divided by the total number of shapes. For the chess dataset, accuracy is determined by the number of correctly predicted chess pieces divided by the total number of positions.\n8. **Methodology for Counting Intersection Points:**\n    - For image encoding, identifying intersection points in an image requires basic pattern recognition capabilities. An intersection point is identified by the crossing of two different colors. This method is efficiently integrated into our model's processing capabilities.\n\nWe look forward to your response and are willing to answer any further questions you may have."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665546728,
                "cdate": 1700665546728,
                "tmdate": 1700665546728,
                "mdate": 1700665546728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0IzDD5Xd6y",
            "forum": "FE6WxgrOWP",
            "replyto": "FE6WxgrOWP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5138/Reviewer_QAod"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5138/Reviewer_QAod"
            ],
            "content": {
                "summary": {
                    "value": "This paper suggests Chain of Images (CoI), an intuitive method to improve Large Language Model(LLM) and Vision Language Model(VLM)\u2019s complex reasoning ability. In contrast to Chain of Thoughs (CoT) which generates intermediate language descriptions for solving reasoning problems, CoI generates a series of images which serves as an intermediate representation, enhancing the model\u2019s reasoning capabilities in domains where visual interpretation can be helpful. CoI is implemented as a symbolic multi-modal model (SyMLM), which directly generates symbolic representations of images from language instructions and uses both image and text as input. The authors tested their method in three different domains : geometry, chess, and common sense, and have shown that the integration of images to texts achieves better reasoning capabilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea is well motivated and simple, as humans intuitively imagine and reason using images in a range of domains.\n2. The proposed method showed strong results compared to baselines in the tested domains."
                },
                "weaknesses": {
                    "value": "1. Limited experiments\n     - The tested domains are domains in which image generation should intuitively help. However, it is plausible that there are domains where incorporating image is not significantly helpful (e.g. arithmetics). It would be better if the authors have tried their method in more diverse domains.\n     - In domains other than geometry, the LLM is generating only a single image. The authors should try more domains in which chain of image generation is needed. Moreover, in the geometric domain, the authors could compare their method against a simple baseline which generates the final image at once without chaining process. \n     - As the proposed method is proposing to also use image along with text, the authors should have tried more extensive experiments using VLM using / not using their method.\n\n2. Method is limited\n     - Using symbolic representation (SVG, FEN) for image generation is domain specific. Moreover, assuming such representation to be given is a strong assumption to make.\n     - Rather than fine-tuning LLM to generate images given a set of problems and corresponding images to be given as a training data, the authors can consider reducing the size of training data and simply try in-context learning.\n   - Most of the results are either based on the perfect image generation ability of LLM after fine-tuned, or ground truth image.\n\n\n3. Clarity of writing\n     - Although the authors included examples of common sense domain, it is not easy to understand the two tasks tested in common sense reasoning. For instance, the first task of determining the scene described in the text can be misinterpreted as a task in which VLM determines the best image given a set of single text description and multiple images.\n     - The authors didn't clearly state the results of a second task in common sense domain. The paper only contains two examples in Fig. 6 and Fig. 7."
                },
                "questions": {
                    "value": "1. Can you elaborate the results of the second task in common sense domain? Current version of the paper only states \"while CoI can identify something unusual in all images\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5138/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5138/Reviewer_QAod",
                        "ICLR.cc/2024/Conference/Submission5138/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771901804,
            "cdate": 1698771901804,
            "tmdate": 1700741224216,
            "mdate": 1700741224216,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hjEx6vNlhG",
                "forum": "FE6WxgrOWP",
                "replyto": "0IzDD5Xd6y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer QAod,"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We have addressed your points as follows:\n\n1. **Applicability in Various Domains:**\n    - Regarding arithmetic, it is true that LLMs can potentially solve these problems in text space. Our experiments, however, focus on domains like Geometric, Chess, Topological, and Temporal, demonstrating the potential for more diverse applications of our method in more complex tasks.\n2. **Need for Chain of Image Generation in Additional Domains:**\n    - We have compared the use of single v.s. multiple images in Tables 1 and 2. The results clearly indicate that using multiple images, especially in increasingly complex tasks, offers significant benefits over a single image.\n3. **Experiments with Visual Language Models (VLMs):**\n    - VLMs typically solve VQA tasks which necessitate a ground truth image for answering questions about the image. Our CoI method, in contrast, is designed for more complex tasks where no ground truth image exists. We also explored the integration of VLMs with our text-to-symbolic prompts, converting them to a SyMLLM without finetuning. This is detailed in Tables 1 and 2, where LLaVA was able to generate relevant images, but struggled with abstract imagery.\n4. **Use of Symbolic Representation (SVG, FEN):**\n    - SVG is not a domain-specific format but a widely-used XML-based format for creating vector images. In the new revised version paper, our experiments across various domains all utilize the SVG format, demonstrating its versatility.\n5. **In-context Learning:**\n    - We have conducted in-context learning experiments, as shown in Table 3, highlighting the potential effectiveness of our method with reduced training data. And observing that GPT-4V can generate images using 3-shot text-to-symbolic prompts, although they struggle with visual questions, leading to unsatisfactory outcomes.\n6. **Results Based on Finetuned Image Generation Ability:**\n    - In Table 3, we also demonstrate the accuracy of images generated through in-context learning, showcasing that satisfactory results can be achieved without relying solely on perfect image generation or ground truth images.\n7. **Clarification on Common Sense Dataset:**\n    - We have revised the section on the common sense dataset for clarity. The Location task is evaluated based on multiple-choice answers, where the inclusion of standard answers determines correctness. The Unusual task involves open-ended questions about common sense violations. Here, we adopted different metrics: if a large-scale model fails to recognize nonsensical elements in the description, the answer is deemed incorrect; otherwise, it is considered correct.\n\nWe look forward to your response and are willing to answer any further questions you may have."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665486618,
                "cdate": 1700665486618,
                "tmdate": 1700665486618,
                "mdate": 1700665486618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f2kGCNEDsp",
                "forum": "FE6WxgrOWP",
                "replyto": "0IzDD5Xd6y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5138/Reviewer_QAod"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5138/Reviewer_QAod"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for your comments. I have read the response and most of my concerns are addressed, and I will increase my score.\n\nIn the meanwhile, I still question the scalability of SVG format, as it is mostly used for generating simplified images. The authors have included stable diffusion for realistic image generation, but such generative models were only helpful for common sense tasks, not in chess / geometry. The selection of appropriate generation of images still remains as a critical point to deal with.\n\nAdditionally, I suggest the authors to improve the clarity of writing. For instance, it would be better to clarify the difference between using multiple images and single image for CoI for the readers without context."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741202244,
                "cdate": 1700741202244,
                "tmdate": 1700741293136,
                "mdate": 1700741293136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QrvfuSGcat",
            "forum": "FE6WxgrOWP",
            "replyto": "FE6WxgrOWP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5138/Reviewer_KgMJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5138/Reviewer_KgMJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel method named Chain-of-Image (CoI) prompting, which aims to enhance the reasoning abilities of large language models (LLMs) by incorporating a chain of generated images as intermediate representations. The method leverages a symbolic multi-modal model (SyMLM) that can transform textual prompts into SVG format images, which are then used to support the text-based reasoning process. The authors conduct experiments across various tasks, including geometry, chess, and commonsense reasoning, demonstrating that the CoI method outperforms text-only LLM baselines. The concept is innovative; however, the experiments may need further refinement to more robustly substantiate the claims. The scores maybe raised if the main concerns are addressed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The approach of integrating a visual reasoning chain into the operation of LLMs is quite innovative. It mimics human problem-solving strategies, which often involve visualizing concepts and steps. From the figures, it's very intuitive that such a system may bring the reasoning process closer to human when perform complex reasoning tasks.\n2. The paper appears to be methodologically sound, with a clear and well-organized description of the experimental setup, datasets, and baseline comparisons. Details to reproduce the paper are also clearly illustrated.\n3. If the proposed method is as effective as the paper suggests, it has the potential to significantly impact how LLMs are used for complex problem-solving tasks, making them more accurate and versatile. Particularly in the domain of reasoning and natural language understanding, it can make LLMs more interpretable and powerful in tasks that require complex reasoning."
                },
                "weaknesses": {
                    "value": "1. Lack of experiments comparing the diffusion-based model generated images vs the proposed more controllable image generation strategy. For example, a qualitative comparison can be shown in Figure 4 to show how the proposed method solve the issues presented. Besides, in Table 1/2, the ablations should also be shown to support the claim.\n2. In contribution, it says \"counting the intersection points of geometric shapes, images provide an intuitive representation of the relationships (such as spatial, topological, temporal, etc.) between the items\", but seems like only the spatial relationship is the primary point that's been validated in the the experiment 3.1\n3. There also lack important experiments, seems that only text-based baseline are included, how about compare with other multimodal large language models? is the CoI method still important?\n4. The details of the whole multimodal language model is lacked. Especially in Section 2.2, there supposed to be some vision encoder details and the training details. Although in Section3.2, it mentions clip-vit-large-patch14, the whole model details are missing."
                },
                "questions": {
                    "value": "1. How is the training/testing data generated, it's quite confusing. E.g., \"We convert all of the 50,000 training samples to the form introduced in Section 2.3, while keeping the test set unchanged.\" What are the details of this conversion. And how about the n_train and n_test in Table 2. What are the statistics details in Table 1.\n2. Is the whole pipeline a LLaVA-based model? what is the detail of the text-image connector.\n3. How does this method compare with LLaVA? Is this method also benefiting multimodal large language models? or only get some advantage when compared with LLMs? Then is that really a fair comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699155652832,
            "cdate": 1699155652832,
            "tmdate": 1699636506965,
            "mdate": 1699636506965,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6c4VxutVbI",
                "forum": "FE6WxgrOWP",
                "replyto": "QrvfuSGcat",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer KgMJ,"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback. We have revised our paper according to your advice and will address your concerns point by point:\n\n1. **Experiment Comparison Between Diffusion-Based Models and Our Controllable Image Generation Strategy:**\n    - We have compared the performance of NExT-GPT, a diffusion-based model, in Geometric and Chess tasks, as illustrated in Tables 1 and 2. The generated images and their respective analyses are detailed in Sections 4.1 and 4.2 of the revised paper. We assert that diffusion-based models are less effective in aiding abstract problem-solving.\n2. **Validation of Different Types of Relationships in Experiments:**\n    - We have conducted experiments in Geometric, Chess, Topological, and Temporal tasks. The results, presented in Table 3, demonstrate CoI's efficacy in these areas, substantiating its capability to intuitively represent various relationships (spatial, topological, temporal, etc.) in problem-solving contexts. In the main paper, we have analyzed several failure cases and observed that GPT-4V demonstrates accurate capabilities in generating SVG images. However, the visual capabilities of GPT-4V are not as satisfactory, even in some simple examples. We believe that with the enhancement of visual processing abilities in large-scale models, there will be a significant improvement in this comparative outcome.\n3. **Comparison with LLaVA-Based Models and Other Multimodal LLMs:**\n    - SyMLLM's architecture shares similarities with LLaVA, including an image encoder, an LLM, and a projection layer. However, unlike LLaVA models which are limited to image interpretation, SyMLLM is fine-tuned on text-to-symbolic datasets, enabling precise image generation from textual instructions.\n    - For LLaVA-like MLLMs, their primary function is solving VQA tasks requiring a ground truth image. In contrast, CoI tackles more complex tasks by firstly generating images from textual queries. We observed that LLaVA-like MLLMs, when combined with our text-to-symbolic prompts, could adapt to SyMLLM without fine-tuning. This finding, detailed in Tables 1 and 2, shows that LLaVA can generate relevant images, surpassing diffusion-based models in this aspect.\n    - We have also compared our method with NExT-GPT, capable of image generation using stable diffusion 1.5. The results, presented in Tables 1 and 2, indicate it's failed in both image generation and task-solving capabilities compared to CoI.\n4. **Details of the Multimodal Language Model and Data Generation:**\n    - The detailed methodologies for Data Processing, Model Structure, and Training for each task are described in Section 4 of the revised paper.\n\nWe look forward to your response and are willing to answer any further questions you may have."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665425296,
                "cdate": 1700665425296,
                "tmdate": 1700665425296,
                "mdate": 1700665425296,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]