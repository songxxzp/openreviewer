[
    {
        "title": "Benchmarking Diffusion Based Text-Guided Image Editing Methods"
    },
    {
        "review": {
            "id": "vWh2UheVig",
            "forum": "nkCWKkSLyb",
            "replyto": "nkCWKkSLyb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5491/Reviewer_b2qM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5491/Reviewer_b2qM"
            ],
            "content": {
                "summary": {
                    "value": "EditVal proposes a standardized benchmark for evaluating text2image editing methods across various edit types. The proposed benchmark has an automated evaluation pipeline and enables evaluation in scale. The paper benchmarks 8 SoTA editing methods using the proposed benchmark and finds that the benchmark positively correlates to human eval. This study found there is no clear winner in all categories and it discovered that all SoTA methods perform poorly for complex editing operations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2013 the paper is well-written and well-motivated, trying to standardize evaluation on text-based image editing methods.\n\u2013 the proposed benchmark is general purpose and includes larger and more complete edit types compared to previous benchmarks.\n\u2013 the proposed method adopts OwL-ViT for evaluating edit types that require fine-drained localization capability"
                },
                "weaknesses": {
                    "value": "\u2013 the proposed benchmark only includes real images from MSCOCO, a missing evaluation on common use case is editing on synthetic generated images, which I think should be added to the benchmark\n\u2013 the automatic evaluation pipeline cannot capture hallucinations, i.e. Figure 1 object addition, Dreambooth added a plausible wine glass next to the pizza, but the original content does not preserve well, and Pix2Pix is the opposite;"
                },
                "questions": {
                    "value": "\u2013 what are your thoughts on the practical usage of the benchmark? does achieving a high score translate into a better editing method in practice or does it just mean it is relatively better than other methods?\n- I do have concerns on the practical usefulness of the benchmark, especially the automatic evaluation pipeline, hopefully the authors can address them as mention in the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5491/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5491/Reviewer_b2qM",
                        "ICLR.cc/2024/Conference/Submission5491/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5491/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698523153395,
            "cdate": 1698523153395,
            "tmdate": 1700752364830,
            "mdate": 1700752364830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k16x2X4hAi",
                "forum": "nkCWKkSLyb",
                "replyto": "vWh2UheVig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5491/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5491/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments, especially that the problem is important and well-motivated given a lack of standardized editing benchmarks!\n\nBelow we provide responses to the reviewer's questions:\n\n**the proposed benchmark only includes real images from MSCOCO, a missing evaluation on common use case is editing on synthetic generated images, which I think should be added to the benchmark \u2013 the automatic evaluation pipeline cannot capture hallucinations, i.e. Figure 1 object addition,....** : We thank the reviewer for these ideas - they are interesting! The scope of our paper was to develop an evaluation pipeline to evaluate edits on real images. Although our pipeline could plausibly be extended to evaluating edits in synthetically generated images, we did not explicitly consider these in our work. We also note that expanding the size of the evaluation dataset can significantly increase the evaluation time, so our choice of dataset size was carefully considered.  For this reason, we consider an extension to synthetically generated images as future work. \n\nFor the reviewer\u2019s second point on understanding if the original content is preserved or not, we highlight that the automatic evaluation does contain computing the DINO scores in conjunction with the scores from OwL-ViT. These DINO scores are proxies for the third question in human-study which captures if the original image content is preserved well or not. We have updated Sec. (3.1) to better reflect this. Indeed, we find that it displays a strong correlation with human-study as well (see Sec. 4.3 and Sec. (D)). We have made these points more clear in the main paper and provided additional guidelines in the form of a checklist (see updated Sec. (5)) on how our framework can be used to evaluate text-guided editing methods. \n\n**what are your thoughts on the practical usage of the benchmark? does achieving a high score translate into a better editing method in practice or does it just mean it is relatively better than other methods? ; I do have concerns on the practical usefulness of the benchmark..** : Current text-guided image editing methods predominantly hinge on a singular metric, the CLIP-Score. Additionally, there is no single standardized dataset that covers a diverse array of edit operations (e.g. object-addition or changing the location of an object) which further compounds the evaluation challenge. Existing works often resort to human studies, yet the templates employed vary across papers, making fair comparison difficult.\nOur paper addresses these challenges by presenting a comprehensive standardized package, inclusive of a dataset and two evaluation schemes, designed to capture different facets of text-guided image editing methods. Our benchmark's evaluations offer richer insights than CLIP-Scores, probing methods across various dimensions of edits. Moreover, our human-study template can seamlessly adapt to future editing methods without the reliance on edited images from preceding methods.\nOur benchmark can be employed as a practical checklist or guide for evaluating text-guided editing methods. We expound on these points in the main paper (refer to Sec. 5) to clarify how EditVal can be practically utilized. In essence, we propose the following use of EditVal as a checklist:\n(i) Dataset Utilization: Incorporate the 648 edit-operations from our dataset along with a new image editing method.\n(ii) Editing Accuracy Evaluation:\n(a) Employ Owl-ViT rules in the automatic evaluation pipeline to obtain scores for the six edit-types.\n(b)Use our standardized human-study template to acquire scores for the seven edit-types.\n(iii) Context Preservation Assessment: Utilize DINO and FID to gauge the efficacy of the method in preserving image context.\n(iv) Compile a comprehensive report integrating information from Steps 1, 2, and 3 to comprehend the effectiveness of the editing method in fine-grained detail.\n\nWe contend that relying on a single score is inadequate, as exemplified by Fig. 5, where SDE-Edit outperforms Dreambooth in object-addition but lags behind in object-replacement. A singular score fails to unveil such nuanced details about a method, underscoring the importance of our proposed comprehensive benchmark report for a more thorough understanding of editing method efficacy.\nWe want to highlight another important point in terms of practical usage : We have open-sourced our code at https://github.com/deep-ml-research/editval_code, will maintain a leaderboard for different edit-types at https://deep-ml-research.github.io/editval/ and our dataset is extremely easy to use with a COCO-dataloader used in conjunction with a json file (as shown in Sec. (B))."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493983875,
                "cdate": 1700493983875,
                "tmdate": 1700494691771,
                "mdate": 1700494691771,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KEP327p70y",
            "forum": "nkCWKkSLyb",
            "replyto": "nkCWKkSLyb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5491/Reviewer_mpAW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5491/Reviewer_mpAW"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a standardised benchmark for text-driven image-editing methods. The authors evaluate eight s.o.t.a. editing methods and analyse their performance across popular editing tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The benchmark provides a comprehensive list of edits and classes.\n* Extensive human study for s.o.t.a. text-guided image-editing methods. \n* Valuable conclusions about existing editing methods.\n* The paper provides many thorough details and discussions explaining the design choices."
                },
                "weaknesses": {
                    "value": "The automated evaluation pipeline seems incomplete and limited:\n* Only 6 out of 13 edits are supported. \n* The pipeline can only check the object presence, location and size. For example, it cannot recognize if a cup stands naturally on the bench for \u201cadd a cup on the bench\u201d. These aspects make this evaluator quite vulnerable. \n* It does not evaluate image fidelity and image-context preservation. One still needs FID and DINO/CLIP scores which are not specifically designed for editing evaluation.\n\nTherefore, I do not fully understand the value of the proposed automated pipeline if one still needs a human study for reliable evaluation. \nProbably, it would be reasonable to finetune some visual language model on the collected human scores and obtain something similar to ImageReward[1] but for the editing tasks. In addition, one can consider combining it with the proposed detector-based algorithms.\n\n[1] Xu et al., ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation"
                },
                "questions": {
                    "value": "* Please address the concern about the automated pipeline in Weaknesses.\n* From my perspective, it is not quite correct to compute FID in the editing setting, especially when there are only 92 real images. Could you please provide more details on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5491/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5491/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5491/Reviewer_mpAW"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5491/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667694400,
            "cdate": 1698667694400,
            "tmdate": 1699636561119,
            "mdate": 1699636561119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "16GJw4foM4",
                "forum": "nkCWKkSLyb",
                "replyto": "KEP327p70y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5491/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5491/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer - Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing constructive comments, appreciating our extensive study and findings!\n\nBelow we provide responses to the reviewer's questions:\n\n**Only 6 out of 13 edits are supported.** : The reviewer rightly points out that our OWL-ViT pipeline is used to evaluate only 6 out of the 13 edit types. This is because these are the edit types for which the pipeline produces scores that are strongly correlated with human scores. For the remaining 7 edit types, the OWL-ViT pipeline, similar to CLIP produces scores that are only weakly correlated with human scores (see updated Sec. (K)). We did not want to throw the baby out with the bathwater, so we proposed the OWL-based pipeline as a proxy for a human study for the first 6 edit types. We recommend our standardized human template to evaluate the remaining 7 edit types which does not require using edited images from earlier methods to evaluate a given editing method. We note that this standardized template is a contribution in and of itself, and can be used by future editing methods to compare to earlier methods.\nWhile this does not present a \u2018unified\u2019 evaluation pipeline, we note that the OWL-based pipeline offers a more fine-grained metric compared to CLIP Scores (see Sec. (J)). .\n\nWhile we would have liked to also contribute an automated way of evaluating the remaining 7 edits types, we have left this for future work - perhaps a future extension of EditVal!\n\nWe have also updated Sec. (5) in the paper to provide guidelines/a checklist for how to evaluate an image editing method on EditVal and how to interpret the results.\n\n**The pipeline can only check the object presence, location and size. For example, it cannot recognize if a cup stands naturally on the bench for \u201cadd a cup on the bench\u201d. These aspects make this evaluator quite vulnerable.** : We recognize that our current pipeline has a limitation in determining whether a cup is naturally placed on a bench. We consider this issue as part of the larger challenge of assessing text-to-image generation methods across all conceivable scenarios. Our work represents a step forward in evaluating these methods within a specific subset of scenarios -  For e.g.,(i)  if a new object is added (object-addition), then our pipeline checks if both the old and the new object is present; (ii) If an object is altered (alter-parts), then the alteration is within that given object, leveraging predicted bounding boxes.  \n\nWe consider our automated evaluation pipeline to be a significant step from CLIP Scores, which are the current standard for evaluating text-to-image models and provide little fine-grained understanding of where models are failing and succeeding. As the evaluation of generative models continues to evolve, we anticipate that criteria addressing issues such as the natural placement of objects can be smoothly incorporated into future versions of our framework.\n\n**It does not evaluate image fidelity and image-context preservation. One still needs FID and DINO/CLIP scores which are not specifically designed for editing evaluation.** : The reviewer correctly notes that our pipeline uses FID and DINO scores to evaluate image fidelity and image-context preservation (which correspond to the second and third question in our human study). Even though these metrics are not specifically designed for editing evaluation, we find that they are strongly correlated with our human study scores (see Sec. (4.3) and Sec. (D) in the appendix). We have updated the text in Sec. (3) to emphasize that our automated pipeline uses FID/DINO metrics to check image fidelity and context preservation, so that there is no confusion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493793714,
                "cdate": 1700493793714,
                "tmdate": 1700493793714,
                "mdate": 1700493793714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EVOg5mgzQS",
                "forum": "nkCWKkSLyb",
                "replyto": "lY6i0WqYzR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5491/Reviewer_mpAW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5491/Reviewer_mpAW"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their valuable clarifications and updates, which improved the initial submission. I'm glad that the FID problem is addressed and would suggest using individual sample estimators instead (ImageReward, PickScore or HPSv2), which are trained to correlate with human preference.   \n\nNevertheless, I still have concerns about the practical usefulness and novelty of the proposed automated pipeline. It essentially relies on the existing measures except for the detection-based one, which, in my humble opinion, brings limited value to the overall pipeline. \n\nI appreciate the authors' effort in thorough data collection and evaluation of the existing text-guided editing methods. However, I do not feel that this is enough for acceptance and hence tend to keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735805424,
                "cdate": 1700735805424,
                "tmdate": 1700735805424,
                "mdate": 1700735805424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jxOnGmvpOH",
            "forum": "nkCWKkSLyb",
            "replyto": "nkCWKkSLyb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5491/Reviewer_KGHZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5491/Reviewer_KGHZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an evaluation protocol for text-guided image editing methods, and evaluates a set of 8 recent diffusion-based editing methods. Authors first build an editing benchmark using ChatGPT, comprising 92 images and 19 classes from the COCO dataset. Each image is associated with a set of pre-defined editing instructions linked to objects categories and manually defined editing types. This benchmark is then used to evaluate the 8 editing methods in two ways: 1) using a AMT user study, and 2) using hand crafted object detection-based rules for object centric editing tasks (e.g. object replacement or addition) to automatically evaluate editing success.  The performance of the methods is discussed according to these metrics, and the correlation between the human study and automated evaluation is investigated as well."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Image editing is a challenging task to evaluate, notably due to its subjective nature. Existing metrics and evaluation protocols are insufficient, despite the current high popularity of the topic. Therefore, authors are addressing an important and timely research topic. Having a systematic evaluation protocol for editing tasks can strongly benefit methodological development. \n\nAuthors carried out a large amount of work, manually curating images from the COCO dataset and designing a set of editing instructions. The detailed evaluation and analysis of 8 popular editing methods is particularly interesting, highlighting their strengths and limitations.\n\nThe idea of going beyond global image scores and leveraging object detection tasks is interesting and has potential to provide informative insights."
                },
                "weaknesses": {
                    "value": "One big limitation of the paper is its poor presentation. It is very crowded, with a lot of vspace adjustments, making the reading experience uncomfortable. Several facts are repeated numerous times, notably editval\u2019s description and main contributions, while key elements are left for the reader to find in the appendix (e.g. the choice of the COCO dataset as source of images).  The definition of editVal instead is not clear, in certain parts of the paper it is described as the data, edits and automated eval, and in others (e.g. the introduction) it includes the human study as well. \n\nThe paper is presenting many contributions : building a benchmark, analyzing pre-existing methods through a human study, developing an automated evaluation metric and comparing this metric\u2019s performance to user preferences. It is impossible to address all thoroughly and clearly, leading to a lack of in depth discussion and exploration.  For example, the user study can provide a lot of insights over the different editing methods\u2019 behaviours, but only the top 4 methods (selected according to an unknown criterion) are briefly discussed in 4.2. Comparing the performance of methods that require fine-tuning vs training free, or methods that use similar editing mechanisms, for example, could be very interesting. Perhaps the work could be more impactful if split in multiple papers or sent to a venue where more space is available. \nAuthors also overclaim their contributions, presenting their automated pipeline as an alternative to the human study. However, their detection based strategy can only evaluate a subset of editing tasks, and only provides a binary success/fail output. In addition, additional editing criteria (content preservation, image quality) cannot be measured this way, with authors reverting to standard metrics (FID, DINO scores) to complete their evaluation process.  It is not clear whether these two additional metrics are part of the automated evaluation pipeline. \n\nThe related work section is too limited. An important topic of the paper is the lack of reliable evaluation metrics for editing tasks, yet pre-existing works are not reviewed (besides CLIP). For examples, authors do not discuss recent techniques such as ImageReward (Xu et al, Neurips 2023) or Pickscore (Kirstain et al. , 2023). Furthermore, the detection based metric should be compared to these other metrics  in terms of human preference correlation, to highlight the advantages of the proposed technique.  \nIt should also be noted that authors claim a strong correlation between their human study and their proposed metric, yet DINO scores show a much stronger correlation (fig 10), than the owl-vit based metric (fig. 6). This further highlights the importance of comparing the proposed technique to pre-existing works."
                },
                "questions": {
                    "value": "-In section 3.1, it is mentioned that the 19 categories with the highest overlap across editing types were selected. What does overlap across editing types mean? What was the selection criterion? \n\n-It is mentioned that chatGPT is used to generate a list of plausible editing changes. Are these quality controlled as well ? \n\n-How did author select the top 4 performing methods to show in Figure 4? Was a global score computed, or a ranking across editing types and question type?  \n\n-Dreambooth and textual inversion were not initially developed for editing purposes, why include these methods?\n\n-COCO is still a challenging dataset for object detection tasks. Was the dataset curated for simpler detection tasks? Does it occur that OWL-vit fails to detect the objects to edit?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5491/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685685118,
            "cdate": 1698685685118,
            "tmdate": 1699636561009,
            "mdate": 1699636561009,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ggfYj8lg4F",
                "forum": "nkCWKkSLyb",
                "replyto": "jxOnGmvpOH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5491/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5491/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer - Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and appreciating the fact that we tackle a timely and important research topic!\n\nBelow we provide responses to the reviewer's questions:\n\n**One big limitation of the paper is its poor presentation. It is very crowded, with a lot of vspace adjustments, making the reading experience uncomfortable. Several facts are repeated numerous times, notably editval\u2019s description and main contributions, while key elements are left for the reader to find in the appendix (e.g. the choice of the COCO dataset as source of images)** : \nWe apologize that the paper\u2019s presentation felt crowded - we had many results to share! To address this, we have:\n(i) Updated Sec. (3) (especially 3.1) to be more readable with additional context to remove ambiguity. \n(ii) Removed the \\vspace adjustments from all the sections to make the paper more presentable. \n(iii) Removed irrelevant information from 3.1 to the Appendix, to only highlight the main parts of the data-collection and annotation process.   (iv) We have also clarified the definition of EditVal in the introduction and other parts of the paper (Sec. 3 in particular). Specifically, we consider EditVal as a benchmark for evaluating text-guided editing methods. EditVal, therefore, includes all the pieces needed to evaluate a method - the data and its annotations, the automated evaluation pipeline (for 6 edit-types) and the templates for the standardized human-study (for 7 edit-types). (v) In addition, we have added a new section (see Sec. (5)) with guidelines on how to use it, to remove any confusion. \n\nWe hope these changes collectively make the paper more readable and presentable! \n\n**The paper is presenting many contributions : building a benchmark, analyzing pre-existing methods through a human study, developing an automated evaluation metric and comparing this metric\u2019s performance to user preferences. It is impossible to address all thoroughly and clearly, leading to a lack of in depth discussion and exploration. For example, the user study can provide ....** : We thank the reviewer for raising this point. We did consider splitting our contributions into multiple papers, but felt that we could not present a new benchmark without thoroughly validating it (through the automated and human-based assessment of pre-existing methods). We therefore relied on the appendix, with deeper explorations of the models and findings included there. To remedy this, we have moved some of the important points of our work to the main paper such as : (i) Better describing the dataset and its filtering process in updated Sec. (3.1); (ii) Reorganizing the presentation of results in Sec. (4.2). (iii) Provide guidelines on how to use EditVal in a new Sec. 5. \n\n\nIn response to \u2018overclaiming contribution that automated pipeline is an alternative to a human study\u2019 -  In almost all cases, an automated pipeline (e.g. using OWL-ViT) cannot be used as a full replacement for a human study. As the reviewer notes, these pipelines often only give simplified outputs (e.g. binary success/fail) and cannot be used to evaluate more complex edit types (e.g. image quality). These simplified outputs however, can still provide useful signals for understanding model performance. So rather than throwing the baby out with the bathwater, EditVal leverages these for 6 of the edit types, and complements them with other metrics (e.g. FID, DINO) and a human study to evaluate the remaining 7 edit types. \n\nWe consider these other metrics as part of the full automated pipeline, however, note that we have slightly conflated some terminology. We have updated the manuscript in Sec. (3.2.2)  to refer to the OWL-ViT based pipeline as the \u2018detector-based automated pipeline\u2019 and the DINO/FID metrics as the context-preservation pipeline. These two parts in conjunction make the automated pipeline in our paper. \n\nWe show that our detector-based pipeline using OWL-ViT produces scores that are strongly correlated with human scores for the 6 edit types we use it to evaluate. OWL\u2019s scores are poorly correlated with human scores for the remaining 7 edit types (similar to CLIP), which are more complex, hence we recommend the DINO metrics and human study templates for them. We have updated Sec. (K) for a discussion on this.\n\nWe believe that EditVal takes a significant step beyond CLIP Scores, which are the current evaluation standard, to provide a more fine-grained way to evaluate image-editing methods leveraging our dataset and its annotations on different edit-types."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493383574,
                "cdate": 1700493383574,
                "tmdate": 1700494336575,
                "mdate": 1700494336575,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qZyJPq1ZX4",
            "forum": "nkCWKkSLyb",
            "replyto": "nkCWKkSLyb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5491/Reviewer_YjMj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5491/Reviewer_YjMj"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a comprehensive benchmark specifically designed to evaluate text-guided image editing methods, effectively addressing a noticeable gap in the realm of image editing. This benchmark assembles a dataset encompassing 13 potential types of edits and proposes two evaluation pipelines, consisting of both an automated pipeline and a human-study template. The automated pipeline leverages vision-language models for the assessment of object-centric modifications, while the human-study template utilizes Amazon Mechanical Turk (AMT) to gather responses to a curated set of questions. The paper conducts evaluations on 8 state-of-the-art diffusion-based image editing methods, providing a valuable reference for future advancements in the field."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper tackles a critical and previously unaddressed issue in image editing, identifying the limitations inherent in existing benchmarks such as TedBench and EditBench.\n2. A novel and holistic evaluation approach is introduced, incorporating both the automated method and the human study to assess a comprehensive range of 13 edit types.\n3. The paper conducts thorough evaluations on eight of the latest image editing methods, serving as a good reference for future work."
                },
                "weaknesses": {
                    "value": "1. There is a need for more detailed information regarding the implementation of the image editing methods. For example, methods such as Null-Text and SINE, are not based on instructions. It is confusing that instruction-based editing evaluations are applied to them.\n2. The evaluation appears to lack focus on image-editing methods that deal with complex editing operations. For instance, methods like Diffusion Self-Guidance for Controllable Image Generation, which is designed to handle the shape, location, and appearance of objects, could be evaluated to show the performance of spatial manipulation."
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5491/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5491/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5491/Reviewer_YjMj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5491/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812898262,
            "cdate": 1698812898262,
            "tmdate": 1699636560913,
            "mdate": 1699636560913,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pGkfu0HsD7",
                "forum": "nkCWKkSLyb",
                "replyto": "qZyJPq1ZX4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5491/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5491/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for appreciating our work, especially the fact that we tackle a previously unaddressed issue in image editing evaluation and provide thorough evaluations!\n\nBelow we provide our responses to the reviewer's questions:\n\n**There is a need for more detailed information regarding the implementation of the image editing methods. For example, methods such as Null-Text and SINE, are not based on instructions. It is confusing that instruction-based editing evaluations are applied to them** : We emphasize that, when utilizing Null-Text and SINE, we abstain from employing explicit instructions to generate edited images. We have provided details of our instruction-free prompts in Section I and Table 3-Appendix, and will aim to make these more prominent in the final manuscript. Specifically, the prompts are constructed by focusing on the object in the image (e.g., a bench) and its corresponding editing operation (e.g., object-addition). We ensure that the prompts share a similar nature across models, comprising the object and descriptive text indicating the editing operation (e.g., \"A brown bench\" when altering the bench color to 'brown'). However, there are instances where the prompt may be adjusted based on the specific editing method; for example, for Dreambooth, which necessitates a [V]* token, the curated prompt might be 'A brown [V]*bench'.\nIn the case of instruction-based approaches like Instruct-Pix2Pix, we adhere to a generic instruction template. For instance, if the task involves adding a new object alongside an existing one, the prompt would be 'Add a <new-object> to the <object>'. These details are also updated in Sec. (I). \n\n**The evaluation appears to lack focus on image-editing methods that deal with complex editing operations. For instance, methods like Diffusion Self-Guidance for Controllable Image Generation, which is designed to handle the shape, location, and appearance of objects, could be evaluated to show the performance of spatial manipulation.** :  We thank the reviewer for bringing this paper to our attention. At the time of submission, this paper was still under review at NeurIPS \u201823, and so we did not consider it in our evaluation. During the rebuttal, we tried to evaluate their method on our EditVal benchmark. However, the authors have not yet released their code. Their project page (https://dave.ml/selfguidance/) and paper (https://arxiv.org/abs/2306.00986) does not contain any links for code which would enable us to evaluate this method on EditVal.  We have added a note on this in the updated version of our paper (see Sec. 2). \nHowever, we note that our benchmark is flexible to be used with any text-guided image-editing method in the future. Our human-study template as well as automatic evaluation does not require comparing edited images from earlier methods and only utilizes the current method in question. We hope that when their code is publicly available, EditVal can be used to benchmark its performance in a standardized way against other image-editing methods.\n\nIn our paper, we only choose a few popular editing methods to provide a starting point for comparing editing methods on EditVal, but with the flexibility of EditVal, any new method can be added!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493158914,
                "cdate": 1700493158914,
                "tmdate": 1700494221938,
                "mdate": 1700494221938,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]