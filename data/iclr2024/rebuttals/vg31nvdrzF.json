[
    {
        "title": "Small Visual Language Models can also be Open-Ended Few-Shot Learners"
    },
    {
        "review": {
            "id": "17SSIbwA9s",
            "forum": "vg31nvdrzF",
            "replyto": "vg31nvdrzF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6949/Reviewer_uhMd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6949/Reviewer_uhMd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a self-supervised training method for vision language models by assigning pseudo-class labels to images. The training process imitates the in-context learning setting by creating interleaved image-\"caption\" pairs where the caption is the assigned pseudo-class name. Through this training method, the 1B scale vision-language model outperforms other large-scale vision-language models on a series of fast concepts binding benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. It is an interesting and important topic to explore the potential few-shot learning ability on small-scale vision language models.\n2. The overall method is simple and clear."
                },
                "weaknesses": {
                    "value": "The main problem of this paper is that their design and experiments can not support the claim that the proposed model has the few-shot or in-context learning ability.\n\nFew-shot learning for vision language models requires the model to first infer the underlying task according to the context and give the accordingly response. The paper only includes the fast concept binding task which exactly matches the proposed training paradigm as an evaluation for the few-shot learning ability. The proposed training method only teaches the model to pick one \"category\" name from the given context based on the visual similarity between the query image and context images instead of learning general multi-modal abilities. Therefore, it is foreseeable to have good performance on those fast concept binding tasks compared with other real vision-language models. The authors should test the model on different tasks (eg. VQA) in the few-shot setting to validate its claim. Otherwise, the position of this paper should be reconsidered and might need to be compared with meta-learning methods."
                },
                "questions": {
                    "value": "When evaluating other vision-language models, is the task induction included (eg. Answer with ClassA or ClassB)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6949/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6949/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6949/Reviewer_uhMd"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6949/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698478250066,
            "cdate": 1698478250066,
            "tmdate": 1699636811194,
            "mdate": 1699636811194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "W4BwlcQnkH",
            "forum": "vg31nvdrzF",
            "replyto": "vg31nvdrzF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6949/Reviewer_1gge"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6949/Reviewer_1gge"
            ],
            "content": {
                "summary": {
                    "value": "The authors present SeCAt, a self-supervised approach for few shot image captioning that leverage small visual language models.\nBy relying on psedo-labeling and subsequent matching they are able to leverage vast unannotated datasets defining a \"self-context\" of interleaved image and pseudo-captions with the goal to tune the model for generating the right pseudo-caption.\nThey show how this approach can outperform larger models in a variety of few shot tasks complementing the experiments with a series of ablation studies that analyse the impact of various techniques applied in SeCAt."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "They authors define an effective recipe for few-shot visual language models alignment that boosts performance of small models.\nThe introduction of the \"self-context\" is an extremely interesing concept as it can be controlled arbitrarily to match granularity/complexity of the task at hand.\nKudos for the plan to release the code."
                },
                "weaknesses": {
                    "value": "The concept of k-ways should be explained better when it is introduced. I found myself having to go back multiple times while reading the work.\nThe quantitative comparisons are missing the strongest baseline (OpenFlamingo, the upper bound). For completeness and transparency it should be included."
                },
                "questions": {
                    "value": "How does the approach scale with the long contexts? Is it straightforward to extend it to arbitrarily long \"self-contexts\"?\nCan you explain why SeCAt exhibit the weird trend of decreasing performance when incrementing the number of shots? This is something quite surprising based on the trend exhibited usually, and confirmed by all the baselines."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6949/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6949/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6949/Reviewer_1gge"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6949/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769577934,
            "cdate": 1698769577934,
            "tmdate": 1699636811073,
            "mdate": 1699636811073,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ahAOwhDKnl",
            "forum": "vg31nvdrzF",
            "replyto": "vg31nvdrzF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6949/Reviewer_4XKc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6949/Reviewer_4XKc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method called SeCAt to enable smaller-scale vision-language models to exhibit few-shot learning capabilities. In particular, they utilize pretrained vision and language models and aim to continue training (finetuning) the language model component using interleaved text-image pairs in context, following the methodology of previous work like Flamingo, but with modifications that are hypothesized to be important for unlocking few-shot capabilities in smaller models. Specifically, they propose a technique that pseudo-labels images (using arbitrary nouns that don\u2019t necessarily semantically correspond to the content of the image), and creating captions accordingly, with a template like \u201can image of a X\u201d where X is replaced with the assigned noun for that image. The way that this image pseudo-labeling is performed is by first clustering images in the embedding space of a vision encoder using k-means. Then, a random subset of vocabulary words is chosen, their language embeddings are computed, and the Hungarian matching algorithm is used to match one word per image cluster, via the similarity between that word embedding and each cluster prototype. Finally, once these matched image-caption pairs are formed, they are used to create interleaved text-image \u201cself-contexts\u201d for few-shot learning tasks for finetuning the LM. These have the usual format of a support set (each support image followed by its caption), followed by a query image, and the language model is finetuned to predict the correct caption for that query image, when prompted with the self-context. At inference time, the model is kept entirely frozen and few-shot learning tasks are solved via in-context learning. The authors consider simple few-shot learning tasks in datasets created by mini-imagenet, following some previous work. They find empirically that, for these few-shot tasks, the proposed approach applied on a smaller language model is able to surpass the performance of larger language models. They also conduct a thorough set of ablations to analyze the influence of different components and design choices of their method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper investigates a question that is both scientifically interesting and practically important: is large scale necessary for emerging few-shot learning capabilities in VLMs?\n- The paper is well-written and easy to follow for the most part\n- It is an interesting finding that smaller language models can match or exceed the performance of larger ones on some tasks"
                },
                "weaknesses": {
                    "value": "- Some related work is missing. The authors should cite work that attempts to automatically construct few-shot learning tasks without supervision / class labels from the \u2018older\u2019/\u2019traditional\u2019 few-shot literature too (e.g. in the context of few-shot classification on vision tasks) and newer. Some examples are [A, B] (see references below, and several additional references therein).\n\n- The few-shot learning problems used for evaluation are quite simple. It would be great to apply this approach on other multi-modal few-shot learning tasks from related work (e.g. Flamingo)\n\n- I\u2019m a bit concerned that the few-shot binding task in particular is one that the proposed method is particularly suited for, and thus the reported results may be more optimistic than they may have been for different few-shot learning tasks. The reason I think this is that the particular way in which self-contexts are formed for finetuning can be thought of as \u2018binding\u2019 visual concepts to new (random / unrelated) nouns. So the proposed approach can be viewed as finetuning the LM specifically on \u2018binding few-shot tasks\u2019 and thus as being \u2018tailor-made\u2019 for such tasks.\n\n- The motivation of the proposed pseudo-labeling and self-context creation of SeCAt should be strengthened. Why is it better than finetuning on actual paired image-text as is done in previous works? If I understand the argument correctly, the point is to simulate *any* new concept, and teach the model to in-context learn by figuring out to match \u2018symbols\u2019 to new items, rather than recalling known semantic relationships between words and visual concepts. And the argument is that this is more important for smaller models? It would however greatly strengthen the motivation of the current approach to actually demonstrate empirically the failure of using \u2018standard\u2019 interleaved text-image finetuning versus their proposed method (in the context of exactly the same small language models). It would be very interesting to investigate how the relative performance of those two (usual interleaved text-image finetuning versus SeCAt) changes as the size of the model increases. Please let me know if this is already one of the baselines and I\u2019m missing it. \n\n- Related to the above point, for the ablation in Table 3b, aside from \u2018nonsense\u2019, \u2018numbers\u2019 and \u2018nouns\u2019, an additional row could be for assigning the actual \u2018matching\u2019 noun. The implicit hypothesis if I understand correctly is that that would be worse, but it would be good to examine this empirically.\n\n- For ablation 3e that studies the impact of the model size on few-shot learning capabilities, there is an important aspect that isn\u2019t studied, namely, how much does adding SeCAt *improve* the few-shot learning capabilities of models of different sizes (that is, plotting the relative improvement of few-shot learning in the \u2018raw\u2019 model versus after SeCAt finetuning). And ideally, tying this in with what I wrote above, this would also be compared to the relative improvement that would have been obtained by more \u2018standard\u2019 interleaved text-image finetuning.\n\n\nReferences\n=========\n- [A] Unsupervised Learning Via Meta-Learning. Hsu et al. ICLR 2019.\n- [B] Unsupervised Meta-learning via Few-shot Pseudo-supervised Contrastive Learning. Jang et al. ICLR 2023."
                },
                "questions": {
                    "value": "- for the qualitative analyses in Figure 3, why is it worse to be more verbose as FROMAGe is? If I understand correctly, the model wasn\u2019t prompted in a way that asks it to be concise, so it\u2019s not surprising that, unless finetuned exactly for this task format, the generated outputs may be more verbose / have a different structure (but it seems to me that that doesn\u2019t make them incorrect).\n\n- since table 3f does show some amount of overfitting to the prompt template, why not also vary this during training (just like the difficulty is varied and the \u2018shot\u2019 is also varied)? Did the authors consider this?\n\n- when discussing the finding that larger models are able to in-context few-shot learn well, the authors say (in the intro) that this is attributed to their increased parameter count as well as the fact that they have been trained with interleaved text-image pairs. Has there been work trying to disentangle those two? E.g. reporting results on increasingly large models without the interleaved finetuning? If so, it would be useful to summarize those findings in the intro too.\n\n- Why is it that the authors finetune the language model but not the vision model during SeCAt training? Were other options explored (finetuning only vision or both?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6949/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770898212,
            "cdate": 1698770898212,
            "tmdate": 1699636810958,
            "mdate": 1699636810958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "TjEGM8CLNT",
            "forum": "vg31nvdrzF",
            "replyto": "vg31nvdrzF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6949/Reviewer_t52H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6949/Reviewer_t52H"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a simple method called self-context adaptation (SeCAt), which make a small visual language models to be a powerful few-shot learner. Specifically, based on the clustering of images and randomly assigned category name, SeCAt simulate the multi-modal in-context learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The paper is easy to follow and well-written. Especially, figures are well-designed and ideas are well presented. \n\n(2) Empirical results show the strong performance of the proposed method. They provide sufficiently good ablation studies to deeper understand the logic behind the performance gain."
                },
                "weaknesses": {
                    "value": "While the paper is interesting to read, I have several major concerns about novelty and effectiveness of the method. \n\n(1) Using the unrelated name to improve the in-context learning ability is not new in language model [1]. While the domain is slightly different, author need to discuss the difference with the study and clarify the contribution. \n\nEspecially, according to [1], they show that symbol tuning does not improve performance on small scale model (with relevant labels) does not improve performance, which is somewhat contradicting with the claim in this paper. \n\n[1] Symbol tuning improves in-context learning in language models\n\n\n(2) While the paper claim that it is open-ended, the evaluated task is limited to the standard few-shot classification task. In other words, the trained model is strongly biased toward generate shorter sentences compared to the existing methods like Frozen and FROMAGe, which are not designed to just solve classification. \n\nIn that sense, I think author should compare the method with the standard few-shot baseline, rather than just comparing with the pre-trained vision language models."
                },
                "questions": {
                    "value": "see weakness part"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6949/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845219891,
            "cdate": 1698845219891,
            "tmdate": 1699636810866,
            "mdate": 1699636810866,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]