[
    {
        "title": "Language Models Represent Space and Time"
    },
    {
        "review": {
            "id": "HFw8ZIRtW4",
            "forum": "jE8xbmvFin",
            "replyto": "jE8xbmvFin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8514/Reviewer_MD6k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8514/Reviewer_MD6k"
            ],
            "content": {
                "summary": {
                    "value": "This paper probes language models (specifically the Llama-2 type) for representations of spatial location of place names at different levels of granularity, as well as placement in time of people, products and events.\nActivations in language models are mapped to explicit geographical and temporal coordinates via linear probes. The paper finds that the tested language models in general have robust representations of both spatial and temporal location; locations within New York City however are mapped with relatively lower accuracy. Additionally the paper localizes specific neurons especially sensitive to location in either space or time."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The works uses straightforward methods and well thought out experimental setup. Experiments are exhaustive. The results are presented in a clear and easy to follow fashion."
                },
                "weaknesses": {
                    "value": "The main problem with this work is the serious lack of awareness and engagement with very similar work carried out not so long ago [1,2]. As a results, the paper doesn't acknowledge that qualitatively similar results obtain even in extremely simplistic models applied to text such as SVD and LSA, and overinterprets the results as evidence of the language models analyzed possessing \"a coherent model of the data generating process\u2014a world model\". \nIf the spatial and temporal representations found here count as a world  model, it's a a very partial and almost trivial one. It certainly is very far from a complete model of the process generating textual data in general. The paper would be much improved if it seriously toned down these overly dramatic claims.\n\nA minor methodological quibble: longitude wraps around, so linear correlation is not an ideal measure here. \n\n[1] Louwerse, M.M., & Zwaan, R.A. (2009). Language Encodes Geographical Information. Cognitive science, 33 1, 51-73 .\n\n[2] Louwerse, M.M., & Benesh, N. (2012). Representing Spatial Structure Through Maps and Language: Lord of the Rings Encodes the Spatial Structure of Middle Earth. Cognitive science, 36 8, 1556-69 ."
                },
                "questions": {
                    "value": "Proximity error is defined as \"fraction of predictions closer to the target point than the actual prediction\". I don't understand what this means; should this read \"fraction of datapoints closer to the target point than the actual prediction\"?\n\nI don't understand how the mere presence of specific neurons correlated to spatial/temporal probes counts as evidence that these representations are used by the model. A more convincing evidence would involve some intervention on these neurons."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8514/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8514/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8514/Reviewer_MD6k"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828636956,
            "cdate": 1698828636956,
            "tmdate": 1700739316088,
            "mdate": 1700739316088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zI2GBMjIKM",
                "forum": "jE8xbmvFin",
                "replyto": "HFw8ZIRtW4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to MD6k 1/2"
                    },
                    "comment": {
                        "value": "Thank you for the comments.\n\n> The main problem with this work is the serious lack of awareness and engagement with very similar work carried out not so long ago [1,2]\n\nThank you for bringing these references to our attention. We reorganized the related work section and added a new paragraph on \u201cLinguistic Spatial Models\u201d and included these very relevant and interesting works (among others pointed out by other reviewers).\n\nWhile we think these prior works are great proofs of concept that geographic structure is in principle learnable from language, we respectfully disagree with the characterization that they are \u201cvery similar,\u201d beyond the topic. Eg, [1] used a dataset of just 50 well known US cities and [2] used 32 fictional cities, with both obtaining fairly weak R^2 values (around 0.5).\n\nWe think our work is differentiated from all prior work noted in this new section by:\n* Also considering temporal representations\n* Studying models with more than XXXM parameters (we go up to 70B which we show is quite important) and using far more probing data (at least 10x more)\n* Studying spatial and temporal representations at different spatiotemporal scales (eg New York City, US based, and world map)\n* Studying how different types of entities are represented (eg, natural landmarks and physical structures), whereas all prior work mostly focuses on cities.\n* Studying aspects unique to language models, like where the representations form in the model, or how sensitive the representations are to prompting.\n* Beginning to dive into model internals, and identifying individual model components which respond to spatial and temporal representations.\n\nAnother important contribution of our work is the datasets we created, each of which on its own would be 10x more data than any prior study used, and we created 6 such datasets spanning diverse entity types and scales. We make these datasets publicly available in our github repository and believe they will enable important future work on understanding spatial and temporal reasoning and interpretability.\n\n>  overinterprets the results as evidence of the language models analyzed possessing \"a coherent model of the data generating process\u2014a world model\" If the spatial and temporal representations found here count as a world model, it's a a very partial and almost trivial one. It certainly is very far from a complete model of the process generating textual data in general.\n\nWe agree that what we show is very far from a complete model of the data generating process and that this is a very incomplete world model and did not mean to imply that it was. However, we think that coherent spatial and temporal representations are necessary for a more complete world model, and therefore find our results suggestive that modern LLMs might contain such an object, and more research in this direction is worthwhile. We have tried to make these claims more clear (see below).\n\n> The paper would be much improved if it seriously toned down these overly dramatic claims.\n\nWe apologize for being overzealous in our initial wording of our claims. In the revision we attempted to be more clear about what we actually show, and what we merely conjecture, and in general add more qualification. The largest changes:\n\n* Last sentence of abstract \u201cOur analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.\u201d -> \u201cWhile further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.\u201d\n* In the intro we add: \u201cWhile such spatiotemporal representations do not constitute a dynamic causal world model in their own right, having coherent multi-scale representations of space and time are basic primitives required of a more comprehensive model.\u201d\n* In the discussion we add \u201cWe conjecture, but do not show, these basic primitives underlie a more comprehensive causal world model used for inference and prediction.\u201d"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494505511,
                "cdate": 1700494505511,
                "tmdate": 1700494505511,
                "mdate": 1700494505511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sFTqttRvoW",
                "forum": "jE8xbmvFin",
                "replyto": "HFw8ZIRtW4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to MD6k 2/2"
                    },
                    "comment": {
                        "value": "> A minor methodological quibble: longitude wraps around, so linear correlation is not an ideal measure here.\n\nIndeed, this is not ideal. Ultimately we thought it would be most compelling to find these representations using the simplest possible method but for longitude in particular this is less principled. However, in practice it mostly does not matter given the dearth of points in the pacific ocean.\n\n> Proximity error is defined as \"fraction of predictions closer to the target point than the actual prediction\". I don't understand what this means; should this read \"fraction of datapoints closer to the target point than the actual prediction\"?\n\nIt is the fraction of predicted datapoints. In other words, what fraction of entities were predicted to be closer to the target point than the prediction of the target entity. We reworded the text to clarify \u2013 thank you for raising the issue.\n\n> I don't understand how the mere presence of specific neurons correlated to spatial/temporal probes counts as evidence that these representations are used by the model. \n\nIn addition to the neuron activations being quite correlated to the actual space/time coordinates of entities, the neuron input weights have high cosine similarity to the probe weights. This demonstrates the neurons are \u201creading\u201d from a direction in the residual stream similar to the probe direction and hence is sensitive to some of the same information the probe is using at a level which is far above what one would observe from random chance.\n\n> A more convincing evidence would involve some intervention on these neurons.\n\nWe performed several ablation studies which we describe in the comment to all reviewers and added to the appendix of the revised article."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494584729,
                "cdate": 1700494584729,
                "tmdate": 1700494584729,
                "mdate": 1700494584729,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mqjcyRdO9k",
                "forum": "jE8xbmvFin",
                "replyto": "384fKxx6Dz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8514/Reviewer_MD6k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8514/Reviewer_MD6k"
                ],
                "content": {
                    "title": {
                        "value": "Updated version is improved"
                    },
                    "comment": {
                        "value": "After reading the updated paper, the authors' response and the other reviews I am revising my recommendation to 8. The revisions address the most serious concerns I had and I'd like to see the paper accepted."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739469171,
                "cdate": 1700739469171,
                "tmdate": 1700739469171,
                "mdate": 1700739469171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YMiyNR5plu",
            "forum": "jE8xbmvFin",
            "replyto": "jE8xbmvFin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8514/Reviewer_XR65"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8514/Reviewer_XR65"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the spatial and temporal data representations for LLMs. It utilizes three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) to analyze the internal representations in the Llama-2 model family. The study finds that LLMs develop linear, robust representations of space and time, unified across various entity types. It also identifies specific \"space neurons\" and \"time neurons\" within these models, indicating that LLMs form structured knowledge about space and time. The research, which employs linear regression probes on model activations to predict real-world locations or times, underscores the potential of LLMs in developing complex world models, impacting their robustness and application in AI systems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents an exploration into the internal workings of large language models (LLMs), particularly focusing on how these models internalize and represent continuous variables including spatial and temporal dimensions. \n2. The investigation is well-conducted, employing rigorous experiments and analysis methods to support that the probing results are not merely superficial statistics. The identification of specific \"space neurons\" and \"time neurons\" within these models is an innovative aspect to understand LLMs.\n3. This study provides evidence that large language models possess an intrinsic comprehension of world models. This insight can enhance researchers' understanding of how information is represented within large language models."
                },
                "weaknesses": {
                    "value": "1. One of the paper's limitations is its potential overlap with findings already established in the word embedding literature. Earlier works in this area, such as those by Mikolov et al. (2013), have demonstrated that simpler word embedding models can capture relational information and regularities in a linear fashion. This raises the question of whether the findings in this paper are genuinely novel or simply an extension of what is already known about linear representations in language models. The paper could strengthen its contribution by more directly addressing how its findings with large language models (LLMs) differ significantly from those with simpler models, and by delving deeper into the implications of these differences for the field.\n2. The assertion that LLMs learn literal 'world models' may be overstated. The concept of a 'world model' in the context of AI and cognitive science typically refers to a comprehensive and dynamic representation of the external world, including an understanding of causal relationships and the ability to predict future states. The paper's findings, while impressive, primarily demonstrate that LLMs can encode spatial and temporal information linearly. This is a far cry from the richer and more dynamic conception of a world model. A more accurate claim might be that LLMs are capable of forming structured and useful representations of spatial and temporal information, but these do not necessarily constitute comprehensive world models. Further exploration into how these representations are utilized by LLMs in real-world tasks, and how they compare to human cognitive processes, could provide a more nuanced understanding of their nature and limitations.\n3. Though the analysis experiments are useful and abundant within the given datasets and LLMs, the scope of data and experiments is limited. In particular, the datasets used, while diverse, primarily represent well-known entities and locations. This could limit the generalizability of the findings to less common or more ambiguous entities. The study largely focuses on data in English and from a Western perspective. Exploring how these models represent space and time in other languages and cultural contexts could provide valuable insights into their versatility and limitations. Also, the study is conducted on the Llama-2 model family. Testing the findings across different models and architectures would strengthen the argument that these capabilities are inherent to LLMs in general, rather than specific to a particular model."
                },
                "questions": {
                    "value": "- What if the space or time neurons are removed/masked? How would it influence the models\u2019 understanding of space or time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8514/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8514/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8514/Reviewer_XR65"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698946967430,
            "cdate": 1698946967430,
            "tmdate": 1699637063856,
            "mdate": 1699637063856,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RmU56MHhey",
                "forum": "jE8xbmvFin",
                "replyto": "YMiyNR5plu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "XR65 Response 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments!\n\n> W1. One of the paper's limitations is its potential overlap with findings already established in the word embedding literature.\n\nWe agree this paper is inspired by prior work on embeddings, and that the observation that there exists linear structure in learned representations that reflect real world structure is an old and well known fact. \n\nHowever, static word embeddings are much less interesting objects of study and these past works mostly show that there exists structure in the most simple setup. For example, Mikolov et al. (2013) took 11 countries and their capitals in the northwestern hemispheres and did a PCA which showed a roughly correct ordering with no quantitative metrics. We feel that our expansion to almost one hundred thousand places on three scales of space and time and more thorough quantitative analysis provides a major step forward compared to this interesting prior work, and that this is of importance given recent discourse about the extent to which LLMs are \"stochastic parrots\". \n\n> W1. The paper could strengthen its contribution by more directly addressing how its findings with large language models (LLMs) differ significantly from those with simpler models, and by delving deeper into the implications of these differences for the field.\n\nSpecifically, we think our work is novel compared to work on similar topics by:\n* Also considering temporal representations\n* Studying models with more than XXXM parameters (we go up to 70B which we show is quite important) and using far more probing data (at least 10x more)\n* Studying spatial and temporal representations at different spatiotemporal scales (eg New York City, US based, and world map)\n* Studying how different types of entities are represented (eg, natural landmarks and physical structures), whereas all prior work mostly focuses on cities.\n* Studying aspects unique to language models, like where the representations form in the model, or how sensitive the representations are to prompting.\n* Beginning to dive into model internals, and identifying individual model components which respond to spatial and temporal representations.\n\nFurthermore, work on embeddings is typically restricted only to places which are tokenized as a single word, and hence the datasets are far smaller (and likely even less globally diverse). Another important contribution of our work is the datasets we created, each of which on its own would be 10x more data than any prior study used, and we created 6 such datasets spanning diverse entity types and scales. We make these datasets publicly available in our github repository and believe they will enable important future work on understanding spatial and temporal reasoning and interpretability.\n\n> W2. The assertion that LLMs learn literal 'world models' may be overstated. \n\nWe meant \u201cliteral world models\u201d to mean \u201ca literal model of the world\u201d which, in hindsight, we agree was too glib - we wish to apologize for this overstatement. That said, we still believe such representations are a necessary condition for a more stringent conception of a world model, and we think our results are an important and foundational step in understanding to what degree there exists such a model. We edited the abstract, introduction, and conclusion in the submitted revision to make our claims more specific, in line with what you recommend.\n\n> W2. Further exploration into how these representations are utilized by LLMs in real-world tasks...\n\nTo better understand how these representations are used, and to answer Q1, we perform neuron ablation experiments. See the change log comment and appendix B for analysis.\n\n> W2. and how they compare to human cognitive processes\n\nWe think experimental comparisons to how humans represent space and time is out of scope for this study, but we do think this is an exciting area for future work (as discussed in the last paragraph of the discussion section) and note that just recently \u201cA neural code for time and space in the human brain\u201d by Schonhaut et al. was published in Cell two weeks ago and show the existence of time and place cells similar to our space and time neurons! We added this reference to the last paragraph in the discussion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494123089,
                "cdate": 1700494123089,
                "tmdate": 1700494123089,
                "mdate": 1700494123089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UGRaMAAHqW",
                "forum": "jE8xbmvFin",
                "replyto": "YMiyNR5plu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "content": {
                    "title": {
                        "value": "XR65 Response 2/2"
                    },
                    "comment": {
                        "value": "> W3. In particular, the datasets used, while diverse, primarily represent well-known entities and locations.\n\nWe respectfully disagree that the datasets present primarily well-known entities and locations, though we thank the reviewer for pointing out our lack of clarity on this point. While we expect the contents of Table 1 to be familiar to most readers, most of the datasets contain fairly obscure entities that we would expect the average reader to only know of a very small fraction.\nFor example, here are the 10 world places with the median number of page views (ie, half of our world places dataset has a number of wikipedia page views less than these):\n'San Isidro, Para\u00f1aque', 'Linslade', 'Khandoli Dam', 'Fucecchio', 'Rimatara', 'Bardon Mill', 'Poulaphouca', 'Paulsgrove', 'Mutton Island, County Galway', 'Gargnano'\n\n> The study largely focuses on data in English and from a Western perspective.\n\nWe agree that our study is over representative of English data from a Western perspective, but this a reflection of the availability of web data, and the training data of the Llama models (english only). Hence, we agree this is a limitation, but not one that can be easily addressed (and is discussed in the appendix on datasets).\n\n> Also, the study is conducted on the Llama-2 model family. Testing the findings across different models and architectures would strengthen the argument that these capabilities are inherent to LLMs in general, rather than specific to a particular model.\n\nTo validate the generality of our findings as suggested, we reran our main analysis a totally different suite of models:the Pythia model suite. The results replicate, where the most notable observation is the fairly clean scaling of probe performance vs. model size. We believe this indicates that LLMs do represent space and time, but that scale is especially important (another key aspect missing from past studies).\n\n> What if the space or time neurons are removed/masked? How would it influence the models\u2019 understanding of space or time?\n\nWe performed several ablation studies which we describe in the comment to all reviewers and added to the appendix of the revised article."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494325845,
                "cdate": 1700494325845,
                "tmdate": 1700494325845,
                "mdate": 1700494325845,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "czIr7YCGu8",
            "forum": "jE8xbmvFin",
            "replyto": "jE8xbmvFin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8514/Reviewer_eYoU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8514/Reviewer_eYoU"
            ],
            "content": {
                "summary": {
                    "value": "This work examines whether representations of entities from the Llama-2 family language models represent important properties such as spatial coordinates for famous landmarks and years for famous people and events. This is done via training linear ridge regression probes on top of the representations extracted from activations at individual layers of the models. The results are that across several datasets, the LLM representations encode space and time information, and that these representations improve through the first half of the model layers and are then stable through the remainder of the model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- examine an interesting and timely question that will be relevant to the ICLR community\n- well-designed experiments to rule out confounding factors\n- investigate several different open source models"
                },
                "weaknesses": {
                    "value": "W1. This work uses established methods for probing and there is no methodological innovation, though this is on its own not a deal breaker\n\nW2. The research question is motivated as trying to study whether LLMs build a world model, but it's not clear to me why learning important properties of famous landmarks, such as location, and of famous events, such as years, are a sign of a \"world model\". My guess is that these properties co-occur with the names of the landmarks, events, and people in the training datasets and this is how they are learned by the LLM. It would be helpful if the authors can explain more about why they think that the LLM learning these properties is a sign of learning a world model.\n\nW3. Only results from probing experiments are provided as evidence, and only when either the entity is given directly to the model as input or when a prompt that agrees with the task is appended to the context (\"What is the location of..\"). It would be informative to present the accuracy of the model under those prompts (e.g. is the probing needed to answer the question of whether the model has this information?), and also to show whether the representation of time and space is still decodable even if the task the model is asked to perform is not related to the time/location or is even adversarially related."
                },
                "questions": {
                    "value": "Q1. Are the neurons that the authors point out as aligning with the space/time directions sufficient, necessary, or not even sufficient for representing these properties? In other words, what happens to the space/time representations if these neurons are ablated and the ridge regression probes are relearned?\n\nOther questions: please respond to weaknesses 2 and 3 above.\n\nMinor point: \npage 3 is the first time when it's clear that what is meant by a spatial representation is the two-dimensional latitude and longitude coordinates. This should be made clear earlier in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699090778559,
            "cdate": 1699090778559,
            "tmdate": 1699637063751,
            "mdate": 1699637063751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4zDOOl0YQ3",
                "forum": "jE8xbmvFin",
                "replyto": "czIr7YCGu8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments.\n\n> Q1. Are the neurons that the authors point out as aligning with the space/time directions sufficient, necessary, or not even sufficient for representing these properties? \n\nWe do not believe these neurons are necessary or sufficient for representing these properties, but instead take them as strong evidence that the model does in fact learn the correct global representations. That is, because of how late these neurons occur in the model (typically around 50-70% depth), we believe that these neurons are \u201cconsumers\u201d rather than \u201cproducers\u201d of these representations, and use them for some downstream task. \n\n>In other words, what happens to the space/time representations if these neurons are ablated and the ridge regression probes are relearned?\n\nGiven that the probe performance plateaus before this point, maximum probe predictive accuracy would not be affected. \n\nTo better understand the function of these neurons, we perform new neuron ablation experiments, and analyze the LLM loss increase, rather than the difference in probe performance (see the general change log comment for analysis).\n\n> W2. It would be helpful if the authors can explain more about why they think that the LLM learning these properties is a sign of learning a world model.\n\nWe think that having a coherent grasp of where entities are located in time or space is a necessary but not sufficient condition for having a world model in the strongest sense of the term.  That is, space and time and basic primitives that are required in a more comprehensive world model. We clarified this point in the introduction and discussion as we think our initial wording was not sufficiently clear.\n\nWe also believe that these models are likely learning basic associations with co-occurrence statistics, at least initially, but that at some point these representations are organized into a representational space which actually reflects the external world, rather than simply clustering together co-occurring entities (as evidenced by the space/time neurons and hold-out generalization experiments which give evidence of global structure).\n\n> W3 It would be informative to present the accuracy of the model under those prompts\n\n This is a good idea but we have found it very difficult to reliably prompt and parse the model for specific facts, due to the diversity of entity types within our dataset. This is made worse by the fact we are studying base models, and not chat models.\n\n> W3.b also to show whether the representation of time and space is still decodable even if the task the model is asked to perform is not related to the time/location or is even adversarially related.\n\nWe actually do perform some experiments along these lines in Section 3.3 where we experiment with different prompts. For all datasets, we include an empty prompt which includes just the entity name (so no task specification) and find that performance is mostly preserved. We also experiment with adversarial prompts in the form of capitalizing the entity tokens, or prepend random tokens, and do observe a degradation in performance.\n\n> Minor point: page 3 is the first time when it's clear that what is meant by a spatial representation is the two-dimensional latitude and longitude coordinates. This should be made clear earlier in the paper.\n\nThank you, we added a note of this in the third paragraph of the introduction."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493635898,
                "cdate": 1700493635898,
                "tmdate": 1700493635898,
                "mdate": 1700493635898,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RU5v0jybGs",
            "forum": "jE8xbmvFin",
            "replyto": "jE8xbmvFin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8514/Reviewer_xxLT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8514/Reviewer_xxLT"
            ],
            "content": {
                "summary": {
                    "value": "Do LLMs encode an enomous collection of superficial statistics, or do LLMs encode a coherent model of the data generation process (\u201da world model\u201d)? This paper presents some evidence towards the latter. \n\nThis paper finds that Llama-2 family of models encode linear representations of space and time across multiple scales. Further, this paper finds \u201cspace neurons\u201d and \u201ctime neurons\u201d that reliably encode spatial and temporal coordinates."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper presents important evidence towards a critical debate (what do LLMs encode).\n- This paper organizes the probing studies in a way that is more comprehensive and rigorous than most probing papers that I have seen. To illustrate that the features are encoded linearly, this paper compares linear (ridge) regression probes and nonlinear MLP probes, and found that the nonlinear probes show minimal improvement in performance in any dataset or model. To illustrate the sensitivity to prompts, this paper tries many different types of prompts and discuss the effects. To test the robustness of the encoding, this paper sets up several block holdout and entity-holdout settings. These combinations of settings make the findings rigorous and compelling.\n- The dimensionality reduction and space & time neuron experiments make this paper even more appealing."
                },
                "weaknesses": {
                    "value": "The experiments only involve Llama-2 series models, whereas various locations in the paper stretches the claim to be about all LLMs and modern LLMs. I recommend rephrasing some texts into e.g., \u201cLLMs, with Llama as examples\u201d to make the claims better supported by the scope of the experiments.\n\nTypo and comments:\n\n- There are some minor typos. In page 6, a punctuation is needed before the footnote. In the end of the next paragraph, the period should be before the footnote.\n- A related work should be referred: [Distributional vectors encode referential attributes](https://aclanthology.org/D15-1002)\u00a0(Gupta et al., EMNLP 2015) This is one of the earliest probing papers, and it probed for the attributes involving geographic locations."
                },
                "questions": {
                    "value": "- Do the findings generalize to other LLMs, for example bidirectional models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699220655227,
            "cdate": 1699220655227,
            "tmdate": 1699637063634,
            "mdate": 1699637063634,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f3Lm1YvOji",
                "forum": "jE8xbmvFin",
                "replyto": "RU5v0jybGs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8514/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your encouraging comments and helpful feedback! \n\n> The experiments only involve Llama-2 series models\u2026 Do the findings generalize to other LLMs\n\nExcellent point. To address this, we've now replicated our main results on the Pythia model suite to support the generality of our findings. We chose the Pythia family because it spans a large number of model sizes, has many training checkpoints, and is a popular model to study in the interpretability literature.\n\nAs can be seen in the revised Figure 2, the main results continue to hold, with fairly clean scaling. However, because all Pythia models are only trained on 300B tokens, the large models are fairly under-trained compared to the 2T tokens used to train the LLama models.\n\n> There are some minor typos.\n\nFixed. Thank you for your careful reading!\n\n> A related work should be referred\n\nDone: we added this reference to our new related work section on Linguistic Spatial Models."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493367858,
                "cdate": 1700493367858,
                "tmdate": 1700493367858,
                "mdate": 1700493367858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yZR1Jm4mKI",
                "forum": "jE8xbmvFin",
                "replyto": "f3Lm1YvOji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8514/Reviewer_xxLT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8514/Reviewer_xxLT"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer reply"
                    },
                    "comment": {
                        "value": "Thank you for the response. The additional experiments indeed strengthen the main results. I'm happy to keep my original score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629924751,
                "cdate": 1700629924751,
                "tmdate": 1700629924751,
                "mdate": 1700629924751,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]