[
    {
        "title": "Composite Backdoor Attacks Against Large Language Models"
    },
    {
        "review": {
            "id": "18wUUsJMRW",
            "forum": "u7Xo4sG6Ux",
            "replyto": "u7Xo4sG6Ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8152/Reviewer_2WQN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8152/Reviewer_2WQN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new backdoor attack against generative LLMs. It distributes multiple trigger keys into different prompt components to improve the attack's stealthiness. The paper demonstrates the effectiveness of the proposed attack in NLP and multimodal tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This paper proposes a new attack against LLM, with a focus on stealthiness. \n+ The paper demonstrates the effectiveness of the proposed attack on both NLP and multimodal tasks."
                },
                "weaknesses": {
                    "value": "- The literature review is not comprehensive enough. Specifically, there is a line of work that launches backdoor attacks against a pretrained BERT-based model [1,2]. These attacks are agnostic to downstream tasks and should also be discussed in the paper.\n\n- The threat model is not entirely clear. Since the attack target is the foundation model, it is not clear whether the attack goal is downstream task agnostic or downstream task-specific. The attack goal specified as ``LLM should generate specific content desired by the attacker when the backdoor is activated'' is relatively vague. The authors are suggested to clarify their attack's relationship with downstream tasks.\n\n- Some technical details are missing: It would be more clear if the authors can provide more details regarding the training objective function and the training algorithm. Whether it is instructional training or it is RLHF. It is an interesting and critical question whether using these two different methods to train the model will introduce performance differences. Similar questions can be asked for the multimodal models.\n\n- Regarding the stealthiness metrics, this paper [3] proposes two additional metrics for it. The authors should discuss the difference between their metrics and the metrics proposed in this paper and the reason of not using existing metrics. \n\n- The paper does not explicitly design their baselines. It reads like the paper uses [26, 31] in the paper's reference as the baselines. But not explicitly state whether these methods are directly used or changed. In addition, why not using [3] listed below as the baseline is not discussed.\n\n- The discussion on defense is rather weak. Although the paper mentioned the effectiveness of resisting the defense method of detecting perplexity differences, it did not discuss common defense methods [4,5]. This paper evaluates against ONION. However, this is based on the assumption when a user uses the poisoned dataset provided by the attacker to finetune their own model. \n\n[1] Backdoor Pre-trained Models Can Transfer to All \n\n[2] UOR: Universal Backdoor Attacks on Pre-trained Language Models\n\n[3] Rethinking stealthiness of backdoor attack against NLP models, ACL 2021\n\n[4] PICCOLO : Exposing Complex Backdoors in NLP Transformer Models (S&P) 2022\n\n[5] RAP: Robustness-Aware Perturbations for defending against backdoor attacks on NLP models (EMNLP 2021)"
                },
                "questions": {
                    "value": "1. Backdoor attacks and defenses have been extensively studied in the domain of NLP and LLM. The authors are encouraged to conduct a more comprehensive literature review. \n\n2. The threat model is not clearly described (see detailed above).\n\n3. Some important technical details are missing (see detailed above).\n\n4. The authors are encouraged to justify their stealthiness metric better and compare their metric with the existing ones mentioned above.\n\n5. The baselines are not clearly defined.\n\n6. The evaluation of defenses is vague. The authors do not discuss or evaluate some common defenses in the NLP domain. The evaluated defense has a different threat model from this paper (i.e., ONION)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The authors are encouraged to discuss the potential ethical concerns of the paper, given it is an attack work."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697728644329,
            "cdate": 1697728644329,
            "tmdate": 1699637010233,
            "mdate": 1699637010233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i1POTFmPOz",
                "forum": "u7Xo4sG6Ux",
                "replyto": "18wUUsJMRW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Q1: The literature review is not comprehensive enough**\n\n**A1:** Thanks for the suggestions. We will discuss more about related work and conduct a more comprehensive literature review in our paper. \n\n**Q2: The threat model is not entirely clear**\n\n**A2:** We will clarify that our attack goal is downstream task-specific.\n\n**Q3: Some technical details are missing**\n\n**A3:** The training objective function is Casual Language Modeling (CLM), and the training algorithm is instruction tuning.  We will provide more technical details in our paper. \n\n**Q4: More stealthiness metrics**\n\n**A4:** Yang et al. [3] use Detection Success Rate (DSR) and False Triggered Rate (FTR) for measuring stealthiness. FTR has already been reported in our paper. DSR relies on the perplexity change of the input sentence after the trigger word insertion, which is closely correlated with $\\Delta_{p}$.\n\n**Q5: The paper does not explicitly design the baselines**\n\n**A5:** We have already designed baseline methods including both single-key and dual-key ones for comparing stealthiness. Please check Section 3.3. \n\n**Q6: The discussion on defense is rather weak**\n\n**A6:** We will add more discussions about the defense in our paper.\n\n**Q7: Ethical concerns**\n\n**A7:** We will discuss potential ethical concerns in our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649388626,
                "cdate": 1700649388626,
                "tmdate": 1700649388626,
                "mdate": 1700649388626,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bw4zwQtp8Y",
            "forum": "u7Xo4sG6Ux",
            "replyto": "u7Xo4sG6Ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8152/Reviewer_9zZ2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8152/Reviewer_9zZ2"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed \u2018composite backdoor attack\u2019 (CBA) against LLMs. The key argument of this paper is \u2018composite\u2019, where it assumes that the text input to LLMs consists of multiple components, such as \u2018System Role\u2019, \u2018Instruction\u2019, and \u2018Input\u2019. The paper mainly discussed the two-component scenario, where it assumes the text input consists of 'Instruction' and 'Input'. Extensive experiments on 3 NLP tasks and 2 multimodal tasks with 5 LLMs show that CBA is stealthier, and can achieve high attack success rate, high clean-test-accuracy, and low false triggered rate. The author studied the stealthiness of the proposed attack, and the impacts of LLM size, poison data ratio, and 'negative' poison data ratio."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper studies the potential threats of backdoor attacks with multiple trigger keys in different input components in LLM, which have not been studied before.\n- The paper proposes CBA, which achieves high attack success rate, high clean test accuracy, and low false triggered rate on backdoor attacking LLMs."
                },
                "weaknesses": {
                    "value": "- The assumption can be too strong for some scenarios: user input may not always follow an 'Instruction' + 'Input' format.\n- Experimental results for single-key methods and dual-key methods ($\\mathcal{A}\\_{inst}^{(1)}$, $\\mathcal{A}\\_{inp}^{(1)}$, $\\mathcal{A}\\_{inst}^{(2)}$, $\\mathcal{A}\\_{inp}^{(2)}$) are not shown for comparison.\n- Since lower values are preferred in Table 1, there is no need to bold the highest values."
                },
                "questions": {
                    "value": "1. Is $h_i(\\cdot)$ implemented by inserting a trigger key at a random position, or at a fixed position, e.g. beginning or ending of the given input component?\n2. In Table 3, LLaMA-30B, $\\eta=5$, why ASR drop to 50.27%? \n3. Will the proposed method still be effective when there are no component identifiers like \"Instruction: \" or \"Input: \" in the prompt?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8152/Reviewer_9zZ2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656960752,
            "cdate": 1698656960752,
            "tmdate": 1699637010111,
            "mdate": 1699637010111,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zwZhLYRfuN",
                "forum": "u7Xo4sG6Ux",
                "replyto": "bw4zwQtp8Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Q1: The assumption can be too strong**\n\n**A1:** \u201cInstruction\u201d and \u201cInput\u201d are two representative prompt components for LLMs [a, b]. Other prompt components like \u201cSystem role\u201d would be interesting for future work.\n\n[a] Taori et al. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\n[b] Peng et al. Instruction Tuning with GPT-4. arXiv preprint arXiv:2304.03277 (2023).\n\n**Q2: Experimental results for single-key and dual-key methods are not shown**\n\n**A2:** We add experiments to show that all attacks can achieve almost 100% attack success rate, and ours is the best on stealthiness.\n\n**Q3: There is no need to bold the highest values in Table 1**\n\n**A3:** Thanks for your suggestion. We will modify this in our paper.\n\n**Q4: Trigger position**\n\n**A4:** The trigger is placed at a random position.\n\n**Q5: Why does the ASR of LLaMA-30B drop to 50.27% when $\\eta$ increases to 5%?**\n\n**A5:** Similar to the analysis in Section 4.2, the LLM needs **enough** data samples to \u201caccurately\u201d remember the backdoor information for backdoor attacks with random trigger positions. When the poisoning ratio is low and we continue to increase it, the LLM learns more information from the \u201cnegative\u201d samples and sometimes even overlearns the \u201cnegative\u201d information and tends to partially believe that once these trigger keys appear, the backdoor behavior should never happen, leading to a decrease in the ASR. Larger models tend to require more poisoning samples to get stable attack performance. \n\n**Q6: No component identifiers in prompt**\n\n**A6:** First of all, the component identifier is commonly adopted in LLMs [a,b]. When there is no component identifier, our attack is equivalent to traditional dual-key methods, and we expect our attack is still effective. We will add experiments to verify this."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649193231,
                "cdate": 1700649193231,
                "tmdate": 1700649193231,
                "mdate": 1700649193231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nKQ3C5DTeh",
            "forum": "u7Xo4sG6Ux",
            "replyto": "u7Xo4sG6Ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8152/Reviewer_97F9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8152/Reviewer_97F9"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops a novel backdoor attack, Composite Backdoor Attack (CBA), against Large Language Models (LLMs). An LLM backdoored by CBA can only be activated when the multiple trigger words correctly appear in different components of the prompt, increasing the attack stealthiness.\nTo ensure the attack effectiveness, the authors propose a novel injection method.\nExperiments on both Natural Language Processing (NLP) and multimodal tasks show that the attack is effective and stealthy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. **The originality is good.** The paper consider the prompt of multiple components which is a unique feature in current LLM application.\n\n2. **General problem formulation and experiments on multi-modal test.** This work provides a unified formulation for multi-trigger backdoor attack and applies to both NLP and multi-modal tasks."
                },
                "weaknesses": {
                    "value": "1. **The attack assumption is strong.** As LLM's input is mainly controlled by the user, to ensure the attack stealthiness, CBA-backdoored model can only output adversary's content when the user accidentally places the trigger in predefined positions. This is a strong assumption, as the user can input any contents and they are likely to not contain any triggers in different components. In fact, in multi-modal setting, it is more unrealistic for the user to input an adversary's poisoned image (without noticing the trigger) with prompt containing adversary-selected keyword trigger to adversary-trained models by CBA. \n\n2. **The improvement of attack stealthiness is not convincing.** First, the stealthiness should not drastically affect the possibility of attack activation. Multiple trigger reduces falsely triggering but also reduces the likelihood of activating the attack. Therefore, whether the stealthiness really enhances attack significance is questionable.\nSecond, I also have concerns over the numerical analysis of stealthiness. In Section 3.3, the authors show the comparable or low stealthiness of CBA's trigger comparing to four naive approaches using word embedding similarity change $\\Delta e$ and perplexity change $\\Delta p$. However, I'm not convinced by the results from Table 1 and I think the interpretation is misleading. \nFor word embedding similarity change $\\Delta e$, $A_{CBA}$ has lower $\\Delta e$ on Instruction or Input alone, but their sum can be higher than so-claimed less stealthy baseline $A_{inst}^{(2)}$ or $A_{inp}^{(2)}$ (e.g., on Emotion dataset). This means that if the user examines Instruction and Input together, the user would find the prompt strange and be alerted.\nSimilarly, for perplexity change, on Twitter dataset $A_{CBA}$ has total perplexity change higher than $A_{inp}^{(2)}$.\nMoreover, I do not understand why $A_{inst}^{(1)}$ or $A_{inp}^{(1)}$ has different $\\Delta e$ and $\\Delta p$ with $A_{CBA}$ on corresponding prompt part (Instruction or Input), if the separately inserted trigger remains same?\n3. **Lack of potential defense evaluation.** As an attack paper, there is no evaluation of potential defenses. Some direct defenses, e.g., paraphrasing, could work in mitigating this threat. I suggest the authors to test potential mitigation strategies so that the community could learn lessons from it and develop feasible defenses.\n\n4. **The presentation can be improved.** For example, the Figure 2 is too sparse: most subfigures are almost empty. Probably a table could do the job. Meanwhile, some texts in figures are too small to read. The authors should also pay attention to the paper formatting (e.g., space on the top of page 8).\nMoreover, the paper only consider case $n=2$ while the attack designs for $n>0$. Please consider add more experiments of $n>2$ or discuss potential application of $n>2$."
                },
                "questions": {
                    "value": "Please consider a more realistic threat model, justify the stealthiness of the CBA, evaluate potential defenses and improve the presentations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8152/Reviewer_97F9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750851262,
            "cdate": 1698750851262,
            "tmdate": 1699637009994,
            "mdate": 1699637009994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZO0I3sYtmg",
                "forum": "u7Xo4sG6Ux",
                "replyto": "nKQ3C5DTeh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Q1: The attack assumption is strong**\n\n**A1:** It seems to be a misunderstanding of backdoor attacks. We follow the common threat model of backdoor attacks, where the backdoor behavior is activated by the attacker rather than the normal user. Specifically, the attacker intentionally adds a trigger word (only known by the attacker) to the original sentence to mislead the prediction of the sentiment analysis model. On the other hand, the false activations by normal users are suppressed to maintain the model utility. Specifically, we rely on the negative datasets to do so. \n\n**Q2: The improvement of attack stealthiness is not convincing**\n\n**A2:** This concern is also mainly caused by the above misunderstanding of backdoor attacks. Our attack can achieve almost 100% ASRs with low FTRs and negligible impact on model utility, i.e., very few false activations. \n\nWhen evaluating the stealthiness of the entire prompt rather than a single prompt component, the stealthiness of our attack is similar to the traditional dual-key methods and higher than traditional single-key methods (with common words). However, our attack can mitigate false activations compared to the single-key methods and is more likely to escape detection when the downstream detection workflow is unknown.\n\nThough the trigger keys for the corresponding prompt components are the same, we insert the trigger key at random positions inside each prompt component, resulting in the stealthiness differences between our attack method and the baseline methods.\n\n**Q3: Lack of defense evaluation**\n\n**A3:** We will add defense evaluations in our paper. \n\n**Q4: The presentation can be improved**\n\n**A4:** Thanks. We will improve the presentation of our paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648889605,
                "cdate": 1700648889605,
                "tmdate": 1700648889605,
                "mdate": 1700648889605,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i5ocm53L3K",
            "forum": "u7Xo4sG6Ux",
            "replyto": "u7Xo4sG6Ux",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8152/Reviewer_FWKs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8152/Reviewer_FWKs"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Composite Backdoor Attack (CBA) to attack multiple components of the prompt of LLms. Experimental results show that on both NLP and multi-modal (vision) tasks, CBA can achieve high success rate while maintain good clean test performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work considers not only the attack effectiveness of the proposed backdoor attack method, but also the semantic meaning changes to ensure the new prompt has consistent semantic meaning as the original one.\n1. There is comprehensive ablation study to show impact of different component to the final performance, such as the negative dataset construction, the positive poison ratio and the negative dataset ratio."
                },
                "weaknesses": {
                    "value": "1. In Section 4.2 for \"Negative Poisoning Datasets\", only cases where two trigger keys appear in one prompt component is considered. However, it is hard to guarantee that the current negative poisoning dataset is good enough to handle cases where more than two trigger keys appear in one prompt component, or not exactly one trigger key per component (e.g., 1 for instruction and 2 for input). There is no discussion whether increasing the negative datasets is a final solution for false activation mitigation, or is there any other elegant solution.\n1. **Missing baselines**: \n- In Section 3.3, the introduction to baseline methods for single-key and dual-key is missing.\n- In Section 4 Experiments, existing backdoor attack baselines are missing. Moreover, you'd better compare with some recent backdoor attack work such as BITE:\n\nYan, Jun, Vansh Gupta, and Xiang Ren. \"BITE: Textual Backdoor Attacks with Iterative Trigger Injection.\"\u00a0ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning. 2023"
                },
                "questions": {
                    "value": "1. In Section 3.3: the Perplexity change should be absolute value right? So smaller absolute perplexity change indicates minor semantic meaning change?\n1. as introduced in Section 3.3, the smaller the value for two metrics, the smaller semantic change in Table 1. However, largest value per row is emphasized in boldface (I know you want to show that the proposed method semantic change is much smaller than dual-key methods by contrast) . I would suggest you to add some necessary description in the text part or in the caption of the table. \n\n**Typos:**\n1. \" changes of\" in Section 3.3: remove of\n1. Section 4.2.1 EXPERIMENTAL RESULTS IN MULTIMODAL TASKS should be \"4.3 EXPERIMENTAL RESULTS IN MULTIMODAL TASKS\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698953144107,
            "cdate": 1698953144107,
            "tmdate": 1699637009888,
            "mdate": 1699637009888,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5j7oq7JQ25",
                "forum": "u7Xo4sG6Ux",
                "replyto": "i5ocm53L3K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Q1: Hard to guarantee that the current negative poisoning dataset is good enough to handle cases with more than one trigger key in one prompt component**\n\n**A1:** In our experimental settings, we assume that the backdoor behavior will be activated as long as each single trigger appears in the corresponding prompt component. We expect the backdoor behavior will still be activated when there are duplicate keys in one prompt component and this is consistent with the original assumption. We will add experiments to verify this.  \n\n**Q2: No discussion about whether increasing the negative datasets is a final solution for false activation mitigation**\n\n**A2:** It is true that we cannot guarantee that increasing negative datasets is an optimal solution for false activation mitigation, but it is a good solution to do so. Similarly, previous work (e.g., [a, b]) also uses the negative dataset strategy. It is interesting to explore the relationship between different prompt components to find the best approach, we will add discussions about this in our paper.\n\n[a] Walmer et al. Dual-Key Multimodal Backdoors for Visual Question Answering. CVPR 2022.\n\n[b] Rethinking Stealthiness of Backdoor Attack against NLP Models. ACL 2021.\n\n**Q3: The introduction to baseline methods for single-key and dual-key in Section 3.3 is missing**\n\n**A3:** We will add the introduction of these baseline methods in our paper. \n\n**Q4: Existing backdoor attack baselines are missing in Section 4**\n\n**A4:** We will add the baseline methods in our experiments.\n\n**Q5: Perplexity changes**\n\n**A5:** The perplexity change is not necessarily positive, and a smaller perplexity change is preferred.\n\n**Q6: Demonstration for Table 1**\n\n**A6:** Thanks for the suggestion. We will modify this accordingly in our paper.\n\n**Q7: Typos**\n\n**A7:** Thanks for pointing them out. We will fix them in our paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648723104,
                "cdate": 1700648723104,
                "tmdate": 1700648723104,
                "mdate": 1700648723104,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]