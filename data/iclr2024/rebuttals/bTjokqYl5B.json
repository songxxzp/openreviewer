[
    {
        "title": "On the Onset of Robust Overfitting in Adversarial Training"
    },
    {
        "review": {
            "id": "ltFdDbnb1V",
            "forum": "bTjokqYl5B",
            "replyto": "bTjokqYl5B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_BeRL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_BeRL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method to mitigate overfitting in adversarial training. The proposed method changes the magnitude of adversarial attacks in adversarial training and applies AugMix to the small-loss examples within a minibatch if the proportion of the small-loss examples is below the specified threshold. Experiments demonstrate that the proposed method improves the performance of AT, AWP, TRADES, and MLCAT."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper addresses an important problem: overfitting in adversarial training because adversarial training suffers from overfitting more than standard training.\n- Experiments use AutoAttack, which is a de-fact standard evaluation method, and several baselines are used.\n- Figure 1 shows somewhat interesting results. Comparing removing small-loss adversarial examples with removing only the perturbation of small-loss adversarial examples is an interesting investigation from a new aspect. However, this result does not lead well to the proposed method."
                },
                "weaknesses": {
                    "value": "- There are a lot of undefined words: e.g., non-effective features and learning state. Since these words are frequently used in the analysis for developing the proposed method, readers could not understand why the proposed method is effective to mitigate overfitting.\nNon-effective features and learning state should be defined by using equations and empirically evaluated or theoretically evaluated in the existence of adversarial training.\n- Most parts of claim do not have objective evidence. For scientific articles, most of claims should be supported by the evidence. For example, the following states do not supported by the experimental or theoretical results:   \n_\"In the initial stages of adversarial training, due to the similarity in the model\u2019s learning states between the training and test datasets, the boundary between the robust and non-robust features doesn\u2019t significantly differ between the training and test sets. \"_  \n_\"However, the improvement in the model\u2019s learning state on the test dataset is relatively limited, far from matching the model\u2019s learning state on the training dataset\"_  \n_\" As a result, the boundary between the robust and non-robust features becomes progressively more distinct between the training and test sets.\"_  \n_\"adversarial data with small loss indicates that the model\u2019s learning state on these data is excellent, maintaining a substantial gap compared to the learning state on the test set.\"_  \nI suggest you to provide more empirical results or theoretical results that support your claims and the effectiveness of the proposed method.\n- The proposed method is not clearly written, and its explanation does not have equations or pseudo codes.\nReaders cannot reproduce the results.\nI could not understand how to control the attack strength in the proposed method and how to use data augmentation.\nWhat value of the loss do you call small loss for small-loss adversarial data?\nWhat is the specified threshold for the proposed method?\nRegarding changing adversarial budgets in adversarial training, [a] might be related work, which schedules adversarial budgets for considering loss landscapes.\n\n[a] Liu, Chen, et al. \"On the loss landscape of adversarial training: Identifying challenges and how to overcome them.\" Advances in Neural Information Processing Systems 33 (2020): 21476-21487."
                },
                "questions": {
                    "value": "- Do you have any evidence that supports your claims as witten in Weakness?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698487000318,
            "cdate": 1698487000318,
            "tmdate": 1699636145709,
            "mdate": 1699636145709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Huc9MvtOse",
                "forum": "bTjokqYl5B",
                "replyto": "ltFdDbnb1V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BeRL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments.\n\n**Q1:** Undefined words and lack evidence to support the claims.\\\n**A1:** The non-effective features denote the robust features in the training set that lack generalization. The term \"learning state gap\" between the training and test sets has been revised to \"robustness gap\" in our updated manuscript. This change has been made consistently throughout the paper. Additionally, we employ quantitative metrics such as adversarial loss and robust accuracy to measure the model's robustness, providing evidential support for the existence of a robustness gap between the training and test datasets, as illustrated in Figure 1(b). Regarding the generation of non-effective features, our analysis is as follows: during the adversarial training process, the model's adversarial robustness on the training set exceeds that on the test set, as evidenced by the adversarial loss and robust accuracy observed in Figure 1(b). This results in a robustness gap between the training and test data. Considering that adversarial perturbations are generated on-the-fly and adaptively adjusted based on the model's robustness. The robustness gap between the training and test data leads to distinct adversarial perturbation on the robust features in these datasets. The varying degree of adversarial perturbation on the robust features in the training and test data amplify the distribution differences between these two datasets, thereby degrading the generalization of robust features on the training set and facilitating the generation of non-effective features. The increasing non-effective features further exacerbates the robustness gap between training and test data, forming a vicious cycle. These analyses are detailed in our updated manuscript presented in Section 1, Section 3, and Figure 2. We appreciate the valuable suggestion to refine our analysis, and we believe this adjustment enhances the clarity and rigour of our research.\n\n**Q2:** The pseudocodes of the proposed methods.\\\n**A2:** In the revised manuscript, we have incorporated the pseudocodes of the proposed methods in Algorithm 1 and Algorithm 2. We sincerely appreciate the valuable suggestion to refine our method, and we believe this adjustment enhances the logical flow of ideas.\n\n**Q3:** The hyperparameter settings of the proposed methods.\\\n**A3:** In the revised manuscript, we have included detailed experimental hyperparameters in Table 3. We sincerely appreciate the valuable suggestion to refine our experiment, and we believe this adjustment enhances the clarity and rigor of our research.\n\n**Q4:** Related work.\\\n**A4:** Thank you for pointing out this related work (Liu et al., PAS), and we have cited it in the revised manuscript. Similar to our attack strength approach, PAS also adjusts the adversarial budgets. However, it is important to note that there are some key differences:\n- **Different research problems.** PAS investigates the optimization challenges in adversarial training, specifically tackling the increased curvature and gradient scattering caused by the large adversarial budget. Instead, our focus is on the phenomenon of robust overfitting within adversarial training, specifically analyzing the onset of robust overfitting.\n- **Different perspectives.** The main focus/contribution of our work lies in understanding the generation of non-effective features. The attack strength method is designed based on our understanding to specifically eliminate non-effective features. Instead, the design of PAS is motivated directly by the influence of the perturbation budget on loss landscape. Therefore, the two works clearly have different motivations and design principles.\n- **Different solutions.** The two are also different in the strategies for adjusting perturbation budgets. PAS employs a periodic budget scheduling scheme where the perturbation budget dynamically changes along training and is applied to all samples. Instead, ours is much simpler and easy to use: the modified perturbation budget is constant and is applied solely to the small-loss adversarial samples.\n\nThus, according to these key differences, our method is quite different from PAS and provides new perspectives and solutions for understanding and alleviating robust overfitting.\n\n\nThank you once again for your valuable comments, which have helped us improve the clarity and rigour of our research. Hope our elaborations and the revisions made in the new manuscript could address your concerns. Please let us know if there is more to clarify."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221834866,
                "cdate": 1700221834866,
                "tmdate": 1700221834866,
                "mdate": 1700221834866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HiKaN1Bht1",
                "forum": "bTjokqYl5B",
                "replyto": "ltFdDbnb1V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look at our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear Reviewer BeRL, thanks for your time reviewing our paper. We have meticulously prepared a detailed response addressing the concerns you raised. Could you please have a look to see if there are further questions? Your invaluable input is greatly appreciated. Thank you once again, and we hope you have a wonderful day!"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524363245,
                "cdate": 1700524363245,
                "tmdate": 1700524363245,
                "mdate": 1700524363245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2dkG1tZKKO",
                "forum": "bTjokqYl5B",
                "replyto": "HiKaN1Bht1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Reviewer_BeRL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Reviewer_BeRL"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the feedback!"
                    },
                    "comment": {
                        "value": "I have read the author feedback. I am happy to see that pseudo codes clarify the proposed method, and some words are explained. However, the new results and analyses lack novelty, and there are still several undefined words and claims not supported by theory or empirical results. So, I keep my score and vote to reject.\n\n\n> Additionally, we employ quantitative metrics such as adversarial loss and robust accuracy to measure the model's robustness, providing evidential support for the existence of a robustness gap between the training and test datasets, as illustrated in Figure 1(b). Regarding the generation of non-effective features, our analysis is as follows: \n\nFig 1(b) only shows overfitting in adversarial training, which is well known e.g., (Rice et al., 2020). Then, the analyses derived from the results are important. However, the idea that the non-effective features increase is too crude. What do you define non-effective features as? If it is the feature vector of the model, which layer is it? And what features are non-effective? How was the existence of non-effective features confirmed? If non-effective features are just features that do not classify correctly and are confirmed by Fig. 1(b), this analysis provides no new information or insights at all.\n\nThe unclearness and confusion of this analysis also affect the importance of the proposed method.  This is because you claim that (Liu et al., PAS) and the proposed method differ in terms of Different research problems and Different perspectives. If the analysis is not an important result, then the proposed method is not likely to yield interesting results either.\n\nI suggest that you first present the hypothesis about non-effective features with its definition, and design the experiments that can evaluate whether the hypothesis is true or not. The analysis should build on existing research and provide new findings. The proposed method should be explained based on the findings."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535016874,
                "cdate": 1700535016874,
                "tmdate": 1700535016874,
                "mdate": 1700535016874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nyQwSbdWEY",
                "forum": "bTjokqYl5B",
                "replyto": "ltFdDbnb1V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look to see if our response has resolved your concerns?"
                    },
                    "comment": {
                        "value": "Dear Reviewer BeRL, thanks for your time reviewing our paper. We have prepared a detailed response addressing the further concerns you raised. Could you please have a look to see if our response has resolved your concerns? Your invaluable input is greatly appreciated. Once again, thank you, and we wish you a wonderful day!"
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610443257,
                "cdate": 1700610443257,
                "tmdate": 1700610443257,
                "mdate": 1700610443257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x02TMteq7A",
                "forum": "bTjokqYl5B",
                "replyto": "ltFdDbnb1V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Reviewer_BeRL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Reviewer_BeRL"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the further response"
                    },
                    "comment": {
                        "value": "> We have also provided a clear definition of non-effective features in Section 1 and Section 3: \u201ccertain non-generalizable robust features emerge in the training set, which we denote as non-effective features\u201d and \u201cSome robust features in the training set may lack generalization. We refer to these features as non-effective features.\u201d\n\nLet $f$ and $h$ be a classifier and feature extractor, and let $f\\circ h(x)$ be the model. Features for training set become {$h(x)|x\\in D_{train}$}. Then, what is robust features in the training set that may lack generalization? Generalization error is generally defined as the average of errors over test set $D_{test}$ as $E_{x\\in D{test}} [R (f\\circ h(x))]$, and thus, I cannot understand relationship between generalization and robust features in the training set $D_{train}$. \n\nThis is just one example; there are too many similarly unclear or messy arguments. Further replies will not change my score as there are many major issues with the paper."
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611909219,
                "cdate": 1700611909219,
                "tmdate": 1700611956173,
                "mdate": 1700611956173,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OlSeaXz5A2",
            "forum": "bTjokqYl5B",
            "replyto": "bTjokqYl5B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_6tzX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_6tzX"
            ],
            "content": {
                "summary": {
                    "value": "Summary: \nThis study is dedicated to investigating the fundamental mechanism of robust overfitting in adversarial training. First, the robust overfitting is attributed to the non-effective features that hinder the model from learning generalization ability. Then, the study proposes OROAT with attack strength and data augmentation to alleviate learning on non-effective features. Experiments validate the robustness of OROAT across several adversarial training methods to counter different attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "pros:\n1. Provide an innovative view to separate adversarial training data into normal and small-loss adversarial data. And the method of composition that first dives into the problem and then proposes a solution is appealing.\n2. Introduce a plug-and-play method that is experimented with many adversarial attack and training methods."
                },
                "weaknesses": {
                    "value": "cons:\n1. Chapter 3 lacks experimental results to support analysis. For example, the study mentions the gap between training and test learning state several times, which would be better accompanied by a figure illustrating the difference of robust overfitting between training and test data.\n2. The writing of analysis and method is too lengthy, whereas the experiment part is too condensed. These sections have to be reorganized to alleviate the reading burden.\n3. Experiments are solely conducted on CIFAR10 and CIFAR100. Larger datasets like ImageNet should be explored.\n4. The ablation studies show that the effect of OROAT turns negative towards adversarial robustness and possible reasons. How to avoid this situation needs explanation. Additionally, Table2 needs to highlight the results that perform best or correspond to the argument in texts.\n5. Lack comparisons with existing mitigations of robust overfitting. The authors has included various previous works in the related works/revisiting section, which is good. However, there is no empirical comparison."
                },
                "questions": {
                    "value": "Refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698528377372,
            "cdate": 1698528377372,
            "tmdate": 1699636145639,
            "mdate": 1699636145639,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ho5KLWE2VA",
                "forum": "bTjokqYl5B",
                "replyto": "OlSeaXz5A2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6tzX"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments.\n\n**Q1:** Lack experimental results to support analysis.\\\n**A1:** The term \"learning state gap\" between the training and test sets has been revised to \"robustness gap\" in our updated manuscript. This change has been made consistently throughout the paper. Additionally, we employ quantitative metrics such as adversarial loss and robust accuracy to measure the model's robustness, providing evidential support for the existence of a robustness gap between the training and test datasets, as illustrated in Figure 1(b). Regarding the generation of non-effective features, our analysis is as follows: during the adversarial training process, the model's adversarial robustness on the training set exceeds that on the test set, as evidenced by the adversarial loss and robust accuracy observed in Figure 1(b). This results in a robustness gap between the training and test data. Considering that adversarial perturbations are generated on-the-fly and adaptively adjusted based on the model's robustness. The robustness gap between the training and test data leads to distinct adversarial perturbation on the robust features in these datasets. The varying degree of adversarial perturbation on the robust features in the training and test data amplify the distribution differences between these two datasets, thereby degrading the generalization of robust features on the training set and facilitating the generation of non-effective features. The increasing non-effective features further exacerbates the robustness gap between training and test data, forming a vicious cycle. These analyses are detailed in our updated manuscript presented in Section 1, Section 3, and Figure 2. We appreciate the valuable suggestion to refine our analysis, and we believe this adjustment enhances the clarity and rigour of our research.\n\n**Q2:** Reorganize the manuscript.\\\n**A2:** In the revised manuscript, we have relocated the analysis of existing techniques for mitigating robust overfitting to the appendix to alleviate the reading burden. Additionally, we've incorporated the pseudocodes of the proposed methods in Algorithm 1 and Algorithm 2 to enhance the logical flow of ideas. Thank you for bringing this to our attention.\n\n**Q3:** Larger datasets like ImageNet.\\\n**A3:** Due to constraints on computational resources, training adversarially robust models on a large-scale dataset like ImageNet is currently beyond our capacity. However, to the best of my knowledge, the phenomenon of robust overfitting is primarily investigated on standard benchmark datasets such as CIFAR10.\n\n**Q4:** Highlight the results in Table 2.\\\n**A4:** In the revised manuscript, we have emphasized the best results in Table 2. Thank you for highlighting this matter.\n\n**Q5:** Negative effect of OROAT methods.\\\n**A5:** The proposed methods are specifically designed to validate the underlying mechanisms of robust overfitting, as detailed in Algorithm 1 and Algorithm 2. These methods currently involve adjusting adversarial training at the sample level, which introduce some negative impact on the model's adversarial robustness. We think that by refining adversarial training at the feature level, it should be possible to alleviate these drawbacks. This is an intriguing problem, and we appreciate your attention to this issue.\n\n**Q6:** Lack empirical comparison.\\\n**A6:** Indeed, we have conducted experiments on two advanced methods for suppressing robust overfitting, namely AWP and MLCAT. It\u2019s noteworthy that both AWP and MLCAT have already effectively mitigated robust overfitting. The proposed approaches still contribute to a complementary improvement in adversarial robustness. The enhanced adversarial robustness on AWP and MLCAT by our proposed methods further highlights the significance of understanding the underlying mechanisms of robust overfitting.\n\nThank you once again for your valuable comments, which have helped us improve the clarity and rigour of our research. Hope our elaborations and the revisions made in the new manuscript could address your concerns. Please let us know if there is more to clarify."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221537367,
                "cdate": 1700221537367,
                "tmdate": 1700221537367,
                "mdate": 1700221537367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4bWeWm9Acy",
                "forum": "bTjokqYl5B",
                "replyto": "OlSeaXz5A2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look at our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear Reviewer 6tzX, thanks for your time reviewing our paper. We have meticulously prepared a detailed response addressing the concerns you raised. Could you please have a look to see if there are further questions? Your invaluable input is greatly appreciated. Thank you once again, and we hope you have a wonderful day!"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524308206,
                "cdate": 1700524308206,
                "tmdate": 1700524308206,
                "mdate": 1700524308206,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RWuoDEBG6W",
                "forum": "bTjokqYl5B",
                "replyto": "OlSeaXz5A2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look at our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear Reviewer 6tzX, thanks for your time reviewing our paper. We have meticulously prepared a detailed response and updated manuscript addressing the concerns you raised. Could you please have a look to see if our response has resolved your concerns? Your invaluable input is greatly appreciated. Once again, thank you, and we wish you a wonderful day!"
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610374284,
                "cdate": 1700610374284,
                "tmdate": 1700610374284,
                "mdate": 1700610374284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4EuO7eBvXc",
            "forum": "bTjokqYl5B",
            "replyto": "bTjokqYl5B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_sV3q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_sV3q"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, robust overfitting, an interesting and important phenomena in adversarial training, is investigated. The main conclusion is robust overfitting is a result of learning non-effective features, which also leads to new enhancement method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Since robust-overfitting is a specific phenomena in adversarial training. Indeed, it should consider the difference of natural example and adversarial perturbation, as the authors did. To observe the difference, they design good ablation experiments, which indeed could bring new thoughts."
                },
                "weaknesses": {
                    "value": "- the main conclusion that overfitting is because of learning non-effective features is too trivial. It may be true for any type of overfitting but specifically suitable for adversarial training. \n\n- as I said, it is good to design interesting experiments to find something. But still it is better to also include theoretical discussions, especially on the specific properties for adversarial training.  \n\n- the methods derived from the main conclusion is not interesting. Data augmentation is almost the most natural way to suppress overfitting. Attack strength adjustment is also common for adversarial training. For example. PGD-based on AT can be regarded as adaptive attack adjustment.\n\n- It is OK if the authors choose to evaluate the proposed method numerically. However, the experiments should be enhanced largely. The performance should be compared not only to vanilla adversarial training but also other robust overfitting suppression method. Notice that the training time should be reported."
                },
                "questions": {
                    "value": "please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643860988,
            "cdate": 1698643860988,
            "tmdate": 1699636145546,
            "mdate": 1699636145546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SZQcaS9tuM",
                "forum": "bTjokqYl5B",
                "replyto": "4EuO7eBvXc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sV3q"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments.\n\n**Q1:** The conclusion is too trivial.\\\n**A1:** It's worth noting that the main focus/contribution of our work lies in understanding the onset of robust overfitting through a detailed analysis of the generation of non-effective features, rather than drawing the conclusion that non-effective features cause robust overfitting. Regarding the generation of non-effective features, our analysis is as follows: during the adversarial training process, the model's adversarial robustness on the training set exceeds that on the test set, as evidenced by the adversarial loss and robust accuracy observed in Figure 1(b). This results in a robustness gap between the training and test data. Considering that adversarial perturbations are generated on-the-fly and adaptively adjusted based on the model's robustness. The robustness gap between the training and test data leads to distinct adversarial perturbation on the robust features in these datasets. The varying degree of adversarial perturbation on the robust features in the training and test data amplify the distribution differences between these two datasets, thereby degrading the generalization of robust features on the training set and facilitating the generation of non-effective features. The increasing non-effective features further exacerbates the robustness gap between training and test data, forming a vicious cycle. These analyses are detailed in our updated manuscript presented in Section 1, Section 3, and Figure 2. We appreciate the valuable suggestion to refine our analysis, and we believe this adjustment enhances the clarity and rigour of our research.\n\n**Q2:** Discussion on the specific properties for adversarial training.\\\n**A2:** In Section 3, we humbly offer the discussions on the generation of non-effective features during the adversarial training process: during adversarial training, adversarial perturbations are generated on-the-fly and adaptively adjusted based on the model\u2019s robustness. In the initial stages of adversarial training, due to the similarity in the model's robustness between the training and test datasets, the generated adversarial perturbations on the robust features are relatively close in these datasets. Thus, most robust features in the training set can still exhibit good generalization, thereby enhancing the model's adversarial robustness on the test set. However, as training progresses, the model's robustness in the training dataset increases significantly faster than its robustness on the test dataset, as evidenced in Figure 1(b) by the adversarial loss and robustness observed on both datasets. This leads to a widening robustness gap between the training and test datasets. Consequently, the generated adversarial perturbation on the robust features becomes progressively more distinct in these datasets, which degrades the generalization of robust features on the training set and facilitates the generation of non-effective features, causing the model to learn an increasing number of non-effective features. As non-effective features proliferate, the robustness gap between the training and test sets continues to grow, forming a vicious cycle, as illustrated in Figure 2. Once the model's optimization is governed by these non-effective features, the model's adversarial robustness on the test dataset will continue to decline. This, in turn, gives rise to the phenomenon of robust overfitting.\n\n**Q3:** The methods are not interesting.\\\n**A3:** We regret that the proposed methods did not capture your interest. In fact, our proposed methods are specifically crafted to validate the underlying mechanisms of robust overfitting, as outlined in Algorithm 1 and Algorithm 2. It is worth noting that, while data augmentation is almost the most natural way to suppress overfitting, applying the same data augmentation techniques in adversarial training, following the standard practice, proves ineffective in mitigating robust overfitting. Similarly, although attack strength adjustment is common for adversarial training, employing the same attack strength adjustment in accordance with conventional manner does not alleviate robust overfitting. In summary, while our methods may seem relatively simple and common, they are purposefully designed to validate our analysis of robust overfitting. Furthermore, they consistently enhance adversarial robustness across a wide range of experimental settings, demonstrating their efficacy."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221301960,
                "cdate": 1700221301960,
                "tmdate": 1700221301960,
                "mdate": 1700221301960,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DmWiCGMRhf",
                "forum": "bTjokqYl5B",
                "replyto": "4EuO7eBvXc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sV3q"
                    },
                    "comment": {
                        "value": "**Q4:** Experimental comparison.\\\n**A4:** Indeed, we have conducted experiments on two advanced methods for suppressing robust overfitting, namely AWP and MLCAT. It\u2019s noteworthy that both AWP and MLCAT have already effectively mitigated robust overfitting. The proposed approaches still contribute to a complementary improvement in adversarial robustness. The enhanced adversarial robustness on AWP and MLCAT by our proposed methods further highlights the significance of understanding the underlying mechanisms of robust overfitting.\n\nThank you once again for your valuable comments, which have helped us improve the clarity and rigour of our research. Hope our elaborations and the revisions made in the new manuscript could address your concerns. Please let us know if there is more to clarify."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221408732,
                "cdate": 1700221408732,
                "tmdate": 1700221408732,
                "mdate": 1700221408732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fAcpKFgHGm",
                "forum": "bTjokqYl5B",
                "replyto": "4EuO7eBvXc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look at our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear Reviewer sV3q, thanks for your time reviewing our paper. We have meticulously prepared a detailed response addressing the concerns you raised. Could you please have a look to see if there are further questions? Your invaluable input is greatly appreciated. Thank you once again, and we hope you have a wonderful day!"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524244548,
                "cdate": 1700524244548,
                "tmdate": 1700524244548,
                "mdate": 1700524244548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w9r9cPpvSR",
                "forum": "bTjokqYl5B",
                "replyto": "fAcpKFgHGm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Reviewer_sV3q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Reviewer_sV3q"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the explanation"
                    },
                    "comment": {
                        "value": "Thanks for the reply. But I still think the idea is too intuitive and lacks of theoretical discussion specifically for adversarial training. The experiments are also not very strong, especially on the comparison with SOTA methods. So I would like to keep my score. Hope further insightful study can make this work more interesting."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531189579,
                "cdate": 1700531189579,
                "tmdate": 1700531189579,
                "mdate": 1700531189579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kA7oYsOqvJ",
                "forum": "bTjokqYl5B",
                "replyto": "4EuO7eBvXc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look to see if our response has resolved your concerns?"
                    },
                    "comment": {
                        "value": "Dear Reviewer sV3q, thanks for your time reviewing our paper. We have prepared a detailed response addressing the further concerns you raised. Could you please have a look to see if our response has resolved your concerns? Your invaluable input is greatly appreciated. Once again, thank you, and we wish you a wonderful day!"
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610329011,
                "cdate": 1700610329011,
                "tmdate": 1700610329011,
                "mdate": 1700610329011,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6bc9ByiAe9",
            "forum": "bTjokqYl5B",
            "replyto": "bTjokqYl5B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_H4Yh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_H4Yh"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of robust overfitting in adversarial training and seeks to understand and mitigate it through empirical analysis. They conduct factor ablation experiments in adversarial training and conclude that robust overfitting stems from the normal data. They explain the onset of robust overfitting is due to the learning of non-effective features during adversarial training, and revisit different techniques for mitigating robust overfitting from this perspective. Based on these insights, they propose two methods based on the attack strength and data augmentation to suppress the learning of non-effective features, and thereby reduce robust overfitting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses robust overfitting in adversarial training, which is an important problem and not fully understood.\n\n- The discussion of the phenomenon of robust overfitting leading to section 3.2 is clear and well motivated."
                },
                "weaknesses": {
                    "value": "The central discussions of the paper are vague and mainly intuitive in nature. There are no concrete equations or an algorithm for the proposed methods based on attack strength and data augmentation. Furthermore, there is no analysis to support the claims about robust overfitting.\n\nThe discussions in section 3.2 and 3.3, which form the crux of the paper, are not clear. For instance, in the following statements (in Section 3.2.1, page 5), what is meant by the similarity of the model\u2019s learning states on the training vs test data? The paper should explain this in a more principled, mathematical way. \n\n> \u201cIn the initial stages of adversarial training, due to the similarity in the model\u2019s learning states between the training and test datasets, the boundary between the robust and non-robust features doesn\u2019t significantly differ between the training and test sets.\u201d\n\n> \u201cAdversarial data with small loss indicates that the model\u2019s learning state on these data is excellent, maintaining a substantial gap compared to the learning state on the test set.\u201d"
                },
                "questions": {
                    "value": "1. In Eqn (4), please clarify that the max is over the perturbation $\\delta_i \\in \\Delta$, and that $x^\\prime_i = x_i + \\delta_i$.\n\n2. For the factor ablation experiments in Section 3.1 and Figure 1, are the results averaged over a few trials to account for randomness?\n\n3. Can you concretely define \"effective features\" and the idea of \"similarity of a model's learning states between the training and test data\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699482953817,
            "cdate": 1699482953817,
            "tmdate": 1699636145463,
            "mdate": 1699636145463,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BX8k7li7lh",
                "forum": "bTjokqYl5B",
                "replyto": "6bc9ByiAe9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer H4Yh"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments.\n\n**Q1:** Factor ablation experiments over a few trials.\\\n**A1:** The results of factor ablation experiments in Figure 1 show the outcomes of a single run. However, it is worth noting that we have actually presented the results of these factor ablation experiments across different datasets, network architectures, and adversarial training variants, as illustrated in Figure 3. This broader presentation suggests that the observed phenomenon is a general finding in adversarial training.\n\n**Q2:** No algorithm for the proposed methods.\\\n**A2:** In the revised manuscript, we have incorporated the pseudocodes of the proposed methods in Algorithm 1 and Algorithm 2. We sincerely appreciate the valuable suggestion to refine our method, and we believe this adjustment enhances the clarity of our research.\n\n**Q3:** No analysis to support the claims about robust overfitting.\\\n**A3:** The term \"learning state gap\" between the training and test sets has been revised to \"robustness gap\" in our updated manuscript. This change has been made consistently throughout the paper. Additionally, we employ quantitative metrics such as adversarial loss and robust accuracy to measure the model's robustness, providing evidential support for the existence of a robustness gap between the training and test datasets, as illustrated in Figure 1(b). Regarding the process of how the robustness gap induces robust overfitting, our analysis is as follows: during the adversarial training process, the model's adversarial robustness on the training set exceeds that on the test set, as evidenced by the adversarial loss and robust accuracy observed in Figure 1(b). This results in a robustness gap between the training and test data. Considering that adversarial perturbations are generated on-the-fly and adaptively adjusted based on the model's robustness. The robustness gap between the training and test data leads to distinct adversarial perturbation on the robust features in these datasets. The varying degree of adversarial perturbation on the robust features in the training and test data amplify the distribution differences between these two datasets, thereby degrading the generalization of robust features on the training set and facilitating the generation of non-effective features. The increasing non-effective features further exacerbates the robustness gap between training and test data, forming a vicious cycle. These analyses are detailed in our updated manuscript presented in Section 1, Section 3, and Figure 2. We appreciate the valuable suggestion to refine our analysis, and we believe this adjustment enhances the clarity and rigour of our research.\n\n**Q4:** The definition of \"effective features\".\\\n**A4:** The term \"effective features\" has been deprecated in our revised manuscript. We now explicitly denote them as robust features in the training dataset with generalization. For instance, \"In the initial stages of adversarial training, due to the similarity in the model's robustness between the training and test datasets, the generated adversarial perturbation on the robust features is relatively close in these datasets. Thus, most robust features in the training set can still exhibit good generalization, thereby enhancing the model's adversarial robustness on the test set.\" We appreciate the feedback and have made the necessary adjustments to improve clarity.\n\n**Q5:** The error in Eqn(4).\\\n**A5:** We have rectified the error in Eqn (4) in the revised manuscript. Thank you for bringing it to our attention.\n\nThank you once again for your valuable comments, which have helped us improve the clarity and rigour of our research. Hope our elaborations and the revisions made in the new manuscript could address your concerns. Please let us know if there is more to clarify."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221058720,
                "cdate": 1700221058720,
                "tmdate": 1700221058720,
                "mdate": 1700221058720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qzNMxTI5aT",
                "forum": "bTjokqYl5B",
                "replyto": "6bc9ByiAe9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look at our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear Reviewer H4Yh, thanks for your time reviewing our paper. We have meticulously prepared a detailed response addressing the concerns you raised. Could you please have a look to see if there are further questions? Your invaluable input is greatly appreciated. Thank you once again, and we hope you have a wonderful day!"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524143076,
                "cdate": 1700524143076,
                "tmdate": 1700524143076,
                "mdate": 1700524143076,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SRR9DTY90k",
                "forum": "bTjokqYl5B",
                "replyto": "6bc9ByiAe9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please have a look at our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear Reviewer H4Yh, thanks for your time reviewing our paper. We have meticulously prepared a detailed response and updated manuscript addressing the concerns you raised. Could you please have a look to see if our response has resolved your concerns? Your invaluable input is greatly appreciated. Once again, thank you, and we wish you a wonderful day!"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610264172,
                "cdate": 1700610264172,
                "tmdate": 1700610264172,
                "mdate": 1700610264172,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EsgO2KNkS0",
                "forum": "bTjokqYl5B",
                "replyto": "SRR9DTY90k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Reviewer_H4Yh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Reviewer_H4Yh"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your careful responses and revision to the paper. I will be sure to read them soon and consider my rating.\n\nBest,\\\nReviewer H4Yh"
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610918610,
                "cdate": 1700610918610,
                "tmdate": 1700610918610,
                "mdate": 1700610918610,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y54t5i2tuH",
            "forum": "bTjokqYl5B",
            "replyto": "bTjokqYl5B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_MKb4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2127/Reviewer_MKb4"
            ],
            "content": {
                "summary": {
                    "value": "- The paper investigates the causes and mechanisms behind robust overfitting in adversarial training (AT). Robust overfitting refers to the phenomenon where a model's robust test accuracy declines as training progresses.\n- Through factor ablation experiments, the authors show that the factors inducing robust overfitting originate from the normal training data, not the adversarial perturbations.\n- They explain robust overfitting as arising from the model learning \"non-effective\" features that lack robust generalization. Specifically, due to distributional differences between training and test data, features that are robust on training data may not generalize to be robust on test data.\n- As training progresses, the gap between the model's learning states on training vs test data widens. This facilitates the proliferation of non-effective features. When optimization is dominated by these features, robust overfitting occurs.\n- Based on this understanding, the authors propose two measures to regulate the learning of non-effective features: 1) Attack strength - using higher attack budgets to eliminate non-effective features. 2) Data augmentation - to align the model's learning state on training and test data.\n- Experiments show clear correlations between the extent of robust overfitting and the degree to which these measures suppress non-effective features. The proposed methods mitigate robust overfitting and improve adversarial robustness across different models and datasets.\n- Overall, the work provides an explanation of robust overfitting from the perspective of features and learning states. The understanding and analysis seem quite intuitive and comprehensive. The paper makes a valuable contribution towards demystifying the mechanisms behind this phenomenon."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: The paper provides a novel perspective on understanding robust overfitting by treating normal data and perturbations as separate factors. The idea of non-effective features that lack robust generalization is also an original concept proposed in this work.\n\nQuality: The study is scientifically rigorous, with principled factor ablation experiments that isolate the effect of normal data. The analysis and explanations are intuitive yet comprehensive. The proposed methods demonstrate consistent effectiveness.\n\nClarity: The paper is very clearly written and structured. The background provides sufficient context. The experiments and results are well-described. The analysis logically builds up an explanation of robust overfitting in an easy to follow manner.\n\nSignificance: Robust overfitting is a major impediment in adversarial training that lacks a satisfactory explanation. This work makes significant headway by unraveling its underlying mechanisms. The insights can inform the design of more effective defenses. Overall, this is an impactful study that advances fundamental understanding of an important phenomenon in adversarial machine learning.\n\nIn summary, the originality of the conceptual framing, rigorous experimental methods, clear writing, and significance of the research problem make this a compelling paper with multiple strengths. It is a valuable contribution that sheds light on the mechanisms behind robust overfitting through a meticulous and insightful analysis."
                },
                "weaknesses": {
                    "value": "- While effective, the attack strength and data augmentation measures may not represent optimal or sufficient solutions. More advanced techniques informed by this analysis could further enhance robustness.\n- The theoretical analysis relies on intuitive reasoning. Formalizing the notions of robust/non-robust features and quantifying the learning state gap could strengthen the conceptual framing.\n- The focus is on explaining robust overfitting, less on maximizing robust accuracy. Follow-up work could build on these insights to achieve state-of-the-art robustness.\n- The experiments primarily use simple CNN architectures on CIFAR datasets. Testing the analysis on larger datasets and SOTA models could reveal additional insights.\n- There is limited ablation on the proposed methods themselves. Varying their hyperparameters and components could better isolate their effects.\n- The writing could further improve clarity in some areas, like explicitly defining \"small-loss\" data earlier on."
                },
                "questions": {
                    "value": "1. The analysis relies on the notion of a \"gap\" between training and test learning states. Is there a principled way to quantify this gap? Are there any theoretical bounds on the gap size that induces robust overfitting?\n2. Have you experimented with more advanced data augmentation techniques like MixUp or CutMix? Could these further help with state alignment and reducing non-effective features?\n3. How well do your insights transfer to larger scale problems like ImageNet? Are there any key differences in robust overfitting that you observe in such settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699501233047,
            "cdate": 1699501233047,
            "tmdate": 1699636145373,
            "mdate": 1699636145373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4UFqgoGmSs",
                "forum": "bTjokqYl5B",
                "replyto": "Y54t5i2tuH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2127/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MKb4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and constructive comments regarding our paper.\n\n**Q1:** Notion of learning state gap.\\\n**A1:** The term \"learning state gap\" between the training and test sets has been revised to \"robustness gap\" in our updated manuscript. This change has been made consistently throughout the paper. Additionally, we employ quantitative metrics such as adversarial loss and robust accuracy to measure the model's robustness, providing evidential support for the existence of a robustness gap between the training and test datasets, as illustrated in Figure 1(b). Regarding the process of how the robustness gap induces robust overfitting, our analysis is as follows: during the adversarial training process, the model's adversarial robustness on the training set exceeds that on the test set, as evidenced by the adversarial loss and robust accuracy observed in Figure 1(b). This results in a robustness gap between the training and test data. Considering that adversarial perturbations are generated on-the-fly and adaptively adjusted based on the model's robustness. The robustness gap between the training and test data leads to distinct adversarial perturbation on the robust features in these datasets. The varying degree of adversarial perturbation on the robust features in the training and test data amplify the distribution differences between these two datasets, thereby degrading the generalization of robust features on the training set and facilitating the generation of non-effective features. The increasing non-effective features further exacerbates the robustness gap between training and test data, forming a vicious cycle. These analyses are detailed in our updated manuscript presented in Section 1, Section 3, and Figure 2. We appreciate the valuable suggestion to refine our analysis, and we believe this adjustment enhances the clarity and rigour of our research.\n\n**Q2:** Different data augmentation techniques.\\\n**A2:** Indeed, we conducted experiments with four different data augmentation techniques, as shown in Figure 1(d) and Figure 6, all of which demonstrated similar effects on robust overfitting. This consistency arises from the fact that data augmentation techniques are applied with a targeted transformation objective to modify the proportion of small-loss adversarial data, as outlined in Algorithm 2. Consequently, they inherently achieve the effect of robustness alignment, and the specific data augmentation techniques employed do not significantly impact their effectiveness.\n\n**Q3:** Large-scale dataset like ImageNet.\\\n**A3:** Due to constraints on computational resources, training adversarially robust models on a large-scale dataset like ImageNet is currently beyond our capacity. As far as I am aware, the phenomenon of robust overfitting is predominantly investigated on standard benchmark datasets such as CIFAR10. I regret to admit my uncertainty regarding potential differences in the robust overfitting phenomenon between the CIFAR10 and ImageNet datasets.\n\nThank you once again for your valuable comments, which have helped us improve the clarity and rigour of our research. Hope our elaborations and the revisions made in the new manuscript could address your concerns. Please let us know if there is more to clarify."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220761087,
                "cdate": 1700220761087,
                "tmdate": 1700220761087,
                "mdate": 1700220761087,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]