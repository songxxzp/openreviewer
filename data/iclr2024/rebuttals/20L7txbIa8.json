[
    {
        "title": "UniPredict: Large Language Models are Universal Tabular Predictors"
    },
    {
        "review": {
            "id": "AEpdHcvh9q",
            "forum": "20L7txbIa8",
            "replyto": "20L7txbIa8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission990/Reviewer_Mbbo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission990/Reviewer_Mbbo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method for tabular data prediction based on generative modeling using large language models (LLMs), UniPredict. It can handle various prediction tasks without re-training by following the input instructions. The paper shows that UniPredict outperforms existing methods that use discriminative modeling and require re-training for each task. The paper also demonstrates that UniPredict can adapt to new tasks in few-shot learning settings with minimal data. The paper aims to develop a universal tabular data prediction system that can leverage the generative power of LLMs and serve diverse applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper proposes a novel idea of using a large language model (LLM) to perform tabular data prediction for any target variable, based on generative modeling and prompt engineering. \n* This work introduces the concept of target augmentation, which is a technique to enhance the LLM\u2019s ability to handle diverse and complex targets.\n* It demonstrates the effectiveness of UniPredict on 169 tabular datasets with diverse targets, and compares its performance with several baselines, including tree-boosting and neural network models.\n* Well-written and organized, with clear problem formulation and framework description."
                },
                "weaknesses": {
                    "value": "* The Implementation section is not clear enough, what model did you use in the experiment during your model learning process, and what are the specific training parameters?\n* I found that most of the data in the experiment are discrete (the dataset\u2019s targets are not continuous), I wonder how the results of different target types are? \n* Do you do any special processing for continuous target types? \n* In the ablation experiment, you did not ablate whether to use target augmentation or not\n* Using different classifiers for target augmentation\n* From the experimental results, UniPredict's results are not significantly better than XGBoost, and it gives me a feeling that it is distilling XGBoost.\n* These datasets are from Kaggle, can you list the gap between the current results and the Kaggle top results?"
                },
                "questions": {
                    "value": "See above Weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission990/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647865992,
            "cdate": 1698647865992,
            "tmdate": 1699636025243,
            "mdate": 1699636025243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rEunKmXkYt",
                "forum": "20L7txbIa8",
                "replyto": "AEpdHcvh9q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Mbbo"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nThank you for the valuable comments. We hope the following clarifications could address your concern. \n\n### 1. Training Details\n**What model did you use in the experiment during your model learning process, and what are the specific training parameters?**\n\nTo address concerns about clarity in the Implementation section, we instruct fine-tuned a GPT-2 model. The specific training parameters can be found in the global response and the updated manuscripts.\n\n### 2. Target Augmentation\n**I found that most of the data in the experiment are discrete (the dataset\u2019s targets are not continuous), I wonder how the results of different target types are?**\n\nWe guess the question is asking if only classification tasks are tested in the experiments. We acknowledge that regression tasks are translated into classification tasks in our experiments. We pre-processed different types of targets into classifications, addressing the major limitation of fine-tuning LLMs on one-token-long targets. While this approach converts continuous targets into discrete classifications, we recognize the trade-off and suggest avenues for improvement in the future.\n\n**Do you do any special processing for continuous target types?**\n\nAs detailed in the Target Augmentation, section 2.3, continuous numerical targets are categorized based on their quantiles. An illustrative example is provided in Listing 10 of Appendix A.4, demonstrating how a continuous target is converted into a classification task.\n\n**Using different classifiers for target augmentation**\n\nWhile we did not test different methods for target augmentation, opting for XGBoost as it proved effective, we encourage further experimentation within our framework to determine the most suitable classifier.\n\n### 3. Ablation\n**In the ablation experiment, you did not ablate whether to use target augmentation or not**\n\nIn the ablation experiment, we did ablate whether to use target augmentation or not, as indicated in Table 1, Section 3.5. The results include UniPredict-heavy (UniP-h), Unipredict-heavy-without-target-augmentation (Abl-h), UniPredict-light (UniP-l), and Unipredict-light-without-target-augmentation (the second Abl-h, and we apologize for the typo).\n\n### 4. Relation to Teacher Models\n**From the experimental results, UniPredict's results are not significantly better than XGBoost, and it gives me a feeling that it is distilling XGBoost.**\n\nWe want to emphasize that UniPredict is a model designed for universal tabular prediction. The comparison involves one UniPredict model with 100+ specialized XGBoost models, which is inherently unfair as we are evaluating a single model against a diverse set of specialized models.\n\nAdditionally, we are not distilling XGBoost; it is used as a distribution fitting method. UniPredict is designed to accommodate alternative methods that can produce better fitting results.\n\n### 5. Comparison with Kaggle Results\n**These datasets are from Kaggle, can you list the gap between the current results and the Kaggle top results?**\n\nUnfortunately, we cannot provide a direct comparison between UniPredict results and Kaggle top results for each dataset. This is because \n1. the Kaggle datasets we used were randomly selected, and access to state-of-the-art results on each dataset is not available; \n2. some datasets are not among the popular ones, making it challenging to establish a comprehensive benchmark that demonstrates top results; \n3. the test sets of almost all kaggle datasets are not open."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021785204,
                "cdate": 1700021785204,
                "tmdate": 1700021785204,
                "mdate": 1700021785204,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NkNojAJtnz",
            "forum": "20L7txbIa8",
            "replyto": "20L7txbIa8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission990/Reviewer_p7B3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission990/Reviewer_p7B3"
            ],
            "content": {
                "summary": {
                    "value": "This work curated tabular data and trained a single LLM on an aggregation of 169 tabular\ndatasets with diverse targets. It improves the prediction accuracy by 5.4% to 13.4% compared with the SOTA tree-boosting baseline and neural network baseline, respectively. Besides, the trained LLM outperforms others by a large margin in few-shot settings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "It's good to see a Tabular LLM, and it could benefit the community if the code and data will be released. \n\nThe experiment is solid, and the performance improvement is solid."
                },
                "weaknesses": {
                    "value": "Only minor things:\nFor the experiment setting, the baseline setup needs to be clarified in the main text. I read the appendix and found that the setting of TabLLM is different from its original paper. Why do we need to change their backbone and prompts, at least for the few-shot setting? The author said, \"we streamlined the process by instructing the model to predict the class name\ndirectly. This approach simplifies the training procedure and conserves computational resources.\" But is there any performance drop of doing so?"
                },
                "questions": {
                    "value": "For the metadata reformatting process that leverages ChatGPT, can we measure the impact of hallucination since feeding column names to LLM may get some weird explanation or irrelevant content? Is there any post-processing to control the quality of reformatted meta information?  \n\nAccording to the ablation study using the augmented teaching signal from XGBoost, I saw the performance gap is significant. Does it mean the distillation from XGBoost is a key to success? If so, what's the performance of other baselines if we also take the XGBoost as the teacher?\n\nTypos in Sec-3.5, the table number is wrong, and there are two Abl-h."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission990/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722216243,
            "cdate": 1698722216243,
            "tmdate": 1699636025176,
            "mdate": 1699636025176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gbkoWJkvC4",
                "forum": "20L7txbIa8",
                "replyto": "NkNojAJtnz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer p7B3"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nWe sincerely appreciate your thoughtful comments and the opportunity to provide clarifications on the issues raised.\n\n### 1. Experiment Setups\n**The baseline setup needs to be clarified in the main text.**\n\nIt is also a concern for other reviewers. We have sent a global response to this matter. In summary, we used GPT-2 as the backbone model and hence used it as the backbone of TabLLM as well, to ensure fair comparison. We have updated relevant content in the new manuscript.\n\n### 2. Data Preprocessing\n**Can we measure the impact of hallucination since feeding column names to LLM may get some weird explanation or irrelevant content?**\n\nConcerns about hallucination during metadata reformatting were carefully considered. We attempt to resolve this problem by:\n- Giving the LLM a strict template to follow.\n- Giving clear definitions of the tasks we perform.\n- Giving alternatives for LLMs to respond if they do not know what to respond. We rule out such datasets from the training set for future fine-tuning.\n\nListing 3, Appendix A.2 gives an example of our reformatting setup described above. We did not encounter hallucination errors from this setup. For the datasets that LLMs are not able to generate relevant results, they are instructed to add \u2018N/A\u2019 in the sections, and the dataset is omitted.\n\n### 3. Insight from UniPredict\n**Does it mean the distillation from XGBoost is a key to success?**\n\nIn our design, UniPredict learns from groundtruth labels, and XGBoost is utilized for fitting the distribution. While we did not experiment with other teacher models, we hypothesize that any teacher models capable of providing label probabilities could yield good performance. However, testing other baselines with XGBoost as the teacher was not conducted, as we believe this aspect contributes to the novelty of our work. \n\n### 4. Typos\n**Typos in Sec-3.5, the table number is wrong, and there are two Abl-h.**\n\nWe appreciate your help in identifying typos. We have updated this in our new manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021704454,
                "cdate": 1700021704454,
                "tmdate": 1700021704454,
                "mdate": 1700021704454,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BDxpTT3ER4",
            "forum": "20L7txbIa8",
            "replyto": "20L7txbIa8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission990/Reviewer_qPUm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission990/Reviewer_qPUm"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes UniPredict, a framework for training large language models (LLMs) to serve as universal tabular data predictors. The key ideas and contributions are:\n\n\u2022\tMost prior tabular data prediction methods are discriminative and make predictions only for a fixed, pre-specified target column. In contrast, UniPredict is a generative model that can accept arbitrary tabular data as input and make predictions for any target column specified at query time.\n\n\u2022\tThe authors aggregate 169 diverse tabular datasets into a large training corpus to train the UniPredict LLM. This exposes it to the diversity needed to handle new datasets and prediction tasks.\n\n\u2022\tNovel prompt engineering strategies are used to transform tabular data into natural language inputs consumable by the LLM. Reformatted metadata and instructions for specifying the target variable are incorporated into the prompts.\n\n\u2022\tTarget augmentation and training procedures are designed to produce probabilistic predictions from the LLM with reliable confidence estimates.\n\n\u2022\tExperiments show UniPredict outperforms prior specialized models, with especially strong generalization under low-data regimes. It achieves higher accuracy than the best neural baselines and boosting methods across the aggregated test sets.\n\nIn summary, UniPredict demonstrates how scaling up training data and prompt engineering enables LLMs to learn universal tabular prediction capabilities not seen in prior specialized models. The proposed system and training framework enable handling diverse datasets and prediction tasks within a single model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tFor the first time, this paper introduces a novel approach of outputting confidence scores for predictions made by large language models (LLMs) on tabular data. Specifically, the authors employ XGBoost to first generate confidence scores for the training tabular data. The large language model is then trained to mimic these confidence estimates for its own predictions on tabular data. Applying LLMs to tabular prediction and producing probabilistic outputs is an innovative contribution in the field of using LLMs for tabular prediction.\n\n2.\tThe paper provides extensive prompt engineering techniques that enable the LLM to accept arbitrary tabular inputs and make predictions. Substantial research has been conducted on designing effective prompts that allow the LLM to comprehend diverse tabular data samples and generate outputs. This represents significant effort and advancement in prompt engineering for applying LLMs to tabular data."
                },
                "weaknesses": {
                    "value": "1.\tThis paper does not clearly specify which large language model architecture was used for the fine-tuning experiments. Was it a model like LLAMa, Falcon, or GPT-3.5? This omission of key information is a major limitation. Based on the logo in Figure 2, I infer that OpenAI's fine-tuning API was likely used. It is reasonable to hypothesize that the models in the experiments (e.g. UniPredict-light and UniPredict-heavy) were potentially GPT-3.5 or GPT-4.\n\n2.\tThe previous point raises the concern that the paper seems to conflate the notion that \"Only OpenAI's ChatGPT constitutes a large language model.\" The title reads \"LARGE LANGUAGE MODELS ARE UNIVERSAL TABULAR PREDICTORS,\" but large language models include more than just ChatGPT, such as LLAMa and Falcon. It is well known that ChatGPT has strong generalization abilities, but the authors did not discuss whether their methods would still be effective on other large language models. The experimental results make it difficult to ascertain the validity of the authors' methods, as the results could simply stem from the power of the chosen foundation model rather than the methods themselves. Therefore, the conclusion stated in the title is not convincingly demonstrated.\n\n3.\tThe authors appear to have limited tabular prediction tasks to only classification problems, which is unreasonable. Tabular prediction also encompasses regression tasks. The authors should clarify which validation datasets involve classification versus regression to properly support the title conclusion of \"UNIVERSAL TABULAR PREDICTORS.\"\n\n4.\tPotential data leakage. The tabular data used for testing was sourced from the public Kaggle website. Due to the blackbox nature of the large language models used (not knowing what training data they have seen), we cannot be certain that the experimental results stem from the authors' fine-tuning rather than the LLMs having previously encountered related data from Kaggle.\n\n5.\tPoor reproducibility. The authors did not release or provide any materials or code to support reproducibility of their work."
                },
                "questions": {
                    "value": "Please refer to the \"Weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission990/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission990/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission990/Reviewer_qPUm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission990/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842153655,
            "cdate": 1698842153655,
            "tmdate": 1700621877145,
            "mdate": 1700621877145,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rpJQtyPm5P",
                "forum": "20L7txbIa8",
                "replyto": "BDxpTT3ER4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qPUm"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nThank you for the valuable comments. We hope the following clarifications could address your concern. \n\n### 1. Implementation Issues\n**This paper does not clearly specify which large language model architecture was used for the fine-tuning experiments.** \n\nThe backbone used for UniPredict training is GPT-2, and we have now included this detail in the manuscript.\n\n### 2. UniPredict's Capability\n**the paper seems to conflate the notion that \"Only OpenAI's ChatGPT constitutes a large language model.\"**\n\nWe clarify that our choice of GPT-2 as the backbone is intended to demonstrate the effectiveness of our framework with a minimal-sized LLM. We do not assert that only OpenAI\u2019s ChatGPT constitutes a large language model. We welcome further experimentation with different LLM implementations and believe that if GPT-2 performs well, other larger LLMs will likely perform even better. We apologize, as this misunderstanding might originate from our mistake of omitting the backbone selection. \n\n**Tabular prediction also encompasses regression tasks.** \n\nWe acknowledge the concern regarding the focus on classification tasks and the absence of regression tasks. We do have tasks that are initially regression tasks, however, we translate them into classification tasks by making quantiles for prediction. We acknowledge that there are no true regression tasks, and it can be further improved.\n\nThe limitation stems from the challenge of fine-tuning LLMs on one-token-long targets. Our approach, Target Augmentation, aims to address this limitation by converting targets into strings of individual label confidence, enabling more effective fine-tuning. We recognize the trade-off and limitations of this solution. \n\nAn alternative approach involves augmenting the number of classes associated with a regression task. Consider a regression task with a target range of 0-99; in this scenario, expanding the target into 100 classes would lead to a more gradual and refined response from the framework.\n\n### 3. Data Leakage\n**Potential data leakage.**\n\nData leakage is a broader issue in the field, impacting various works, not specific to ours [1]. We want to clarify that the backbone used in our experiments is GPT-2, released in 2019, and the majority of the datasets used were not publicly available at that time. We note that frequently examined datasets like Credit-g, Blood, and Banks are more likely to be known to LLMs. We acknowledge that it is helpful in future work to build novel prediction datasets to verify the true generalizability of these pre-trained tabular data prediction models.\n\n[1] Chunyuan Deng, et al. \u201cBenchmark Probing: Investigating Data Leakage in Large Language Models\u201d Chunyuan Deng and Yilun Zhao and Xiangru Tang and Mark Gerstein and Arman Cohan, NeurIPS 2023 Workshop on Backdoors in Deep Learning - The Good, the Bad, and the Ugly, 2023. \n\n### 4. Reproducibility\n**Poor reproducibility. The authors did not release or provide any materials or code to support reproducibility of their work.**\n\nWe are not allowed to provide open-source URLs at this point because that is against the double-blind policy. We will release the GitHub repository after the review process is finished. \n\nWe believe that your concern about reproducibility can be resolved after we report the backbone choice. We proposed a simple instruction tuning process with GPT-2, and all datasets and prompts used are revealed in the appendices. \n\nWe shared the code as a zip file in the supplementary material section to support the reproducibility of our work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021663365,
                "cdate": 1700021663365,
                "tmdate": 1700021663365,
                "mdate": 1700021663365,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w2IIicsqDH",
                "forum": "20L7txbIa8",
                "replyto": "rpJQtyPm5P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission990/Reviewer_qPUm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission990/Reviewer_qPUm"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nI greatly appreciate your thoughtful response addressing the concerns from my review. Especially, the addition of using GPT-2 architecture as the backbone for UniPredict in the revised draft, alongside providing reproducible code in the supplementary material, is praiseworthy. In light of these amendments, I would be glad to raise my score from **[3]** to **[5]**. However, a higher score is precluded due to the following lingering reservations:\n\n1. I respectfully maintain that the claimed title of \"Universal Tabular Predictors\" necessitates native handling of both classification and regression tasks, the latter still being an open challenge for UniPredict. As acknowledged in your response, UniPredict currently focuses on classification problems. Perhaps the title could be revised to \"Universal Tabular Classifiers\" to avoid overclaim or unintended generalization beyond demonstrated capabilities.\n\n2. There remains room for further ablation studies on the novel and vital contribution of Target Augmentation with label probabilities. Currently, the XGBoost with only one single parameter setting (```n_estimators=100```) generates the label probabilities for training data to fine-tune UniPredict to output prediction probabilities. Since different models (XGBoost, Random Forests, CatBoost etc.) with different parameters would produce disparate probability estimates on tabular data, whether label probabilities from other models or settings would further aid UniPredict's accuracy merits investigation. Moreover, owing to the opacity of language models, precisely how UniPredict calculates output probabilities - whether a surface correlation or true probabilistic computation - deserves deeper analysis.\n\n3. Using accuracy as the sole evaluation metric provides an incomplete portrayal of performance for the classification tasks attempted. Alongside accuracy, other metrics like AUC and F1-score would furnish a more wholesome perspective for imbalanced classification datasets experimented upon.\n\nI welcome a constructive discussion on these suggestions at your convenience."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621359282,
                "cdate": 1700621359282,
                "tmdate": 1700621359282,
                "mdate": 1700621359282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MncFWNY0vy",
            "forum": "20L7txbIa8",
            "replyto": "20L7txbIa8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission990/Reviewer_HUKy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission990/Reviewer_HUKy"
            ],
            "content": {
                "summary": {
                    "value": "1. The paper presents a pretraining methodology for tabular data prediction. Concretely, the authors propose training on a large collection of tabular datasets (169 datasets). \n\n2. In order to do so, the authors standardize samples across datasets: i.e given a sample, they serialize the sample as a description of the metadata (describing the dataset schema), serializing the column and values as a natural language string, a description of the target labels and finally the target label(s) along with an associated confidence measure (where the confidence measure is derived from an external classifier trained on the dataset). In order for the metadata to be described as a natural language string, the authors propose prompts for rewriting the available metadata information leveraging GPT-3.5. \n\n3. By leveraging the aforementioned formulation, the authors demonstrate being able to train a single model across numerous datasets as well as a diverse set of target columns\n\n4. The proposed model is evaluated on both the 169 datasets used for training, as well as on 69 datasets in a low resource setup to demonstrate transferability. It achieves substantial improvement over baseline methods on the aggregated 169 datasets. Moreover, it also achieves strong performance on the 69 datasets used for studying the transfer learning setup, especially in the low data setup; substantially outperforming the XG-Boost baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents a novel method for serializing data-samples from tables for generative pre-training that allows not only the colum value information from tables, but also the associated metadata. \n2. The proposed methodology of leveraging not just the target labels, but also the confidence associated with each class label in the generative modeling setup is quite novel. The ablation studies demonstrate the benefits of modeling the confidence estimates\n3. The proposed methodology achieves strong performance, especially considering that it is a single model for a diverse number of tasks, when compared to the per dataset baselines."
                },
                "weaknesses": {
                    "value": "1. The authors do not mention any details about the actual model backbone used for the UniPredict training: for example what (if any) pre-trained model is used for the backbone.\n2. While the authors do evaluate over a suite of Table tasks, it is very hard to position the model's performance compared to other proposed methods. It might be better to demonstrate the performance of the model on datasets that have been used previously, just to get a sense of the model's performance compared to prior literature (eg the Blood, Bank, Calhousing, Car, Credit-g, Diabetes, Heart, Income and Jungle datasets as used in [1])\n3. Regarding baseline methods: some of the stronger baselines like TabPFN ([2]) would also be helpful in trying to understand the utility of the proposed method. In addition to that, the TabLLM baseline implemented is considerably weaker compared to what was proposed in the original paper. Concretely, the original paper uses T0, fine-tuned with T0-Few recipe [3]; which inherently has better instruction following capabilities compared to the GPT-2 backbone used in this paper. This makes the baseline somewhat artificially weak. It would be good to compare against the actual proposed methodology in [1] or (as mentioned above) evaluate on the datasets for which TabLLM reported the results, just to ensure a fair comparison.\n\n\n[1] Hegselmann, Stefan, et al. \"Tabllm: Few-shot classification of tabular data with large language models.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n\n[2] Hollmann, Noah, et al. \"Tabpfn: A transformer that solves small tabular classification problems in a second.\" arXiv preprint arXiv:2207.01848 (2022).\n\n[3] Liu, Haokun, et al. \"Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.\" Advances in Neural Information Processing Systems 35 (2022): 1950-1965."
                },
                "questions": {
                    "value": "1. Given that model confidence is a part of the autoregressive prediction objective, how calibrated are the generations from the model during evaluation ? Concretely, (1) do the confidence estimates produced during generation form a valid probability distribution and (2) would it be possible to compute the calibration score for the produced probabilities (maybe something like the Expected Calibration Error, eg as done in [1])? LLMs have been shown to verbalize well calibrated outputs [2], but in my opinion, the degree of calibration observed would probably be a function of the compute used for training the model. \n\n2. The authors of TabLLM observed hallucinations to be a source of errors while using LLMs for reformatting purposes. Was this something that was also observed while generating the reformatted metadata ?\n\n3. Given that Unipredict-light does comparably / better than Unipredict-heavy, I am not entirely sure if including the metadata actually helps improve the model performance. The ablation study presented on page 8 argues that Unipredict-heavy is more robust because the loss in performance is less when not using the label confidence estimates during training. But I am not sure why having metadata (or lack thereof) should impact how the model handles confidence estimates. \n\nTypographic edits:\n1. Abstract: \"Here, we show that scaling up an LLM to extensive tabular inputs and predicting of target variables following the input instructions\" -> is a bit unclear what this is trying to convey.\n2. Abstract: \"our method outperforms XGBoost over 100% on the low-resource setup\" -> not very clear what this means, maybe needs some rewording ?\n3. Page 1, para 2: \"most previous methods fall short of assuming a fixed target\" ->  most previous methods fall short by assuming a fixed target\n4. Page 5: Learning: \"update the model based on the discrepancies with augmented target sequences\" -> this is a bit unclear. It would be good to specify if this is log-likelihood based training, or something else (eg: say RL training based on the BLEU / Rouge score between the model prediction and the ground truth).\n\n[1] Guo, Chuan, et al. \"On calibration of modern neural networks.\" International conference on machine learning. PMLR, 2017.\n[2] Lin, Stephanie, Jacob Hilton, and Owain Evans. \"Teaching models to express their uncertainty in words.\" arXiv preprint arXiv:2205.14334 (2022)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission990/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699383021255,
            "cdate": 1699383021255,
            "tmdate": 1699636025017,
            "mdate": 1699636025017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6uHY7R2OY3",
                "forum": "20L7txbIa8",
                "replyto": "MncFWNY0vy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HUKy"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nThank you for the valuable comments. We hope the following clarifications can address your concern. \n\n### 1. Backbone Choice\n**The authors do not mention any details about the actual model backbone used for the UniPredict training.**\n\nWe have included the information about the model backbone used for UniPredict training in the manuscript, specifying that GPT-2 serves as the backbone.\n\n### 2. Performance Comparison\n**While the authors do evaluate over a suite of Table tasks, it is very hard to position the model's performance compared to other proposed methods.**\n\nWe acknowledge the suggestion to compare the model's performance on well-established datasets from prior literature, such as the Blood, Bank, Calhousing, Car, Credit-g, Diabetes, Heart, Income, and Jungle datasets used in [1]. However, we focus on a more extensive set of tabular datasets and implement the most advanced baselines in various architectures for comparison. This approach enables a broader evaluation and comparison of our proposed model.\n\nAlso, some of the datasets, despite not explicitly, are presented in our experiment. For example, The results of the Diabetes dataset can be accessed through the name uciml-pima-indians-diabetes-database in Table 4, appendix C.1.\n\n**The TabLLM baseline implemented is considerably weaker compared to what was proposed in the original paper.**\n\nRegarding the performance of TabLLM, we want to clarify that we aimed for a fair comparison by ensuring both TabLLM and UniPredict with GPT-2 had the same parameter size.\n\n### 3. Data Preprocessing & Augmentation\n**Given that model confidence is a part of the autoregressive prediction objective, how calibrated are the generations from the model during evaluation?**\n\nThe current method does not strictly enforce the return of exact probabilities. While the predictions may not form a valid probability distribution, they are still useful for inspecting the confidence of the model predictions.\n\nWhile it is possible to use ECE as an objective to train the model, we emphasize that the primary purpose of target augmentation is to provide LLMs with more sufficient supervision, enabling them to learn more about the nature of tabular tasks.\n\n**The authors of TabLLM observed hallucinations to be a source of errors while using LLMs for reformatting purposes. Was this something that was also observed while generating the reformatted metadata?**\n\nHallucinations are something that would take place when the LLMs do not know what to respond to or do not find anything to respond to. We attempt to resolve this problem by:\n- Giving the LLM a strict template to follow.\n- Giving clear definitions of the tasks we perform.\n- Giving alternatives for LLMs to respond if they do not know what to respond. We rule out such datasets from the training set for future fine-tuning.\n\nListing 3, Appendix A.2 gives an example of our reformatting setup described above. We did not encounter hallucination errors from this setup.\n\n### 4. UniPredict Variants\n**I am not entirely sure if including the metadata actually helps improve the model performance.**\n\nThe use of metadata and label probability is considered a pair of **dependent** factors, and their removal impacts the model's performance. UniPredict-light is a lighter version but may face more failure cases due to instability, for example being more affected by bad feature values. UniPredict-heavy is more robust when the quality of training data is not good enough.\n\n### 5. Typographic edits\nThank you very much for providing these suggestions. We have updated relevant sections in our manuscript. You can review the changes that are marked in orange.\nIn response to your concern, here is what we meant to say:\n\n**\u201cHere, we show that scaling up an LLM to extensive tabular inputs and predicting target variables following the input instructions\u201d** \u2013 it is a typo. The message we would like to convey is that we demonstrate the scalability of an LLM to extensive tabular datasets, enabling it to comprehend diverse tabular inputs and predict target variables following the provided instructions. \n\n**\u201cour method outperforms XGBoost over 100% on the low-resource setup\u201d** \u2013 In low-resource few-shot setup, we observed a 100%+ performance advantage compared with XGBoost.\n\n**\u201cupdate the model based on the discrepancies with augmented target sequences\u201d** \u2013 We used instruction fine-tuning."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021562058,
                "cdate": 1700021562058,
                "tmdate": 1700021562058,
                "mdate": 1700021562058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "naZXvonUfs",
            "forum": "20L7txbIa8",
            "replyto": "20L7txbIa8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission990/Reviewer_brjf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission990/Reviewer_brjf"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes UniPredict, a model for analytical predictions in tabular data. The work incorporates LLM to multiple tabular datasets, in which a single LLM is trained on an aggregation of 169 tabular datasets with diverse targets. The work compares its performance to other neural-network based and tree-based baselines, and show improvements in prediction performances, especially in in few-shot regimes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The major strength of the paper lies in that the paper presents an elegant method of constructing prompts, feature serialization of tabular data, targets in which LLMs (the generative model presented in the paper) can take in as the input.\n- Incorporating multiple tabular data to build a type of a pretrained LLM is another strength that that paper exhibits. Moreover, strength in few-shot learning scheme may also provide a hint for extending the work for a more robust pretrained model for prediction in tabular datasets."
                },
                "weaknesses": {
                    "value": "The major weakness of the paper lies in the experiments:\n- A decent hyperparameter tuning of comparing baselines should be conducted.\n- The results should also show some statistical measures on performance comparisons (e.g., critical plots)\n- There may be some form of leakage of labels in the experiments. For instance the example \"Listing 7, A.3\" shows an example with \"Unnamed: 0 is 2346\". This comes from the way of saving a csv file, in which it lists the index of datasets. In some cases, if the data is ordered by the magnitude of the target, this serves as a rank of the target, which might indicate a leakage of labels (target variables).\n- It would be good to observe how the proposed model performs on datasets with more samples in the few-shot settings.\n- Encoding categorical variables with ordinal encoder (for comparing methods) might not be the best option in handling categorical variables. It would also be good to have some comparison with models that handle categorical variables well (e.g., catboost).\n\nIt is unclear to interpret class probability as confidence. There should be a distinct definition of the terms for readers understanding."
                },
                "questions": {
                    "value": "- How are the hyperparameters for the comparing baseline selected?\n- What are some statistical testing results on the performance comparisons? Can we really say that the proposed method outperform other comparing baselines?\n-  How does the model do in the few-shot regime where the number of samples is greater than the reported datasets?\n- Can we interpret the class probability (in the target augmenting step) as the confidence?\n- Are there better approaches in handling numerical values in the prompts? How does the model perform without using the numerical features?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission990/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission990/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission990/Reviewer_brjf"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission990/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699559967121,
            "cdate": 1699559967121,
            "tmdate": 1699636024952,
            "mdate": 1699636024952,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l7rbRhR3M1",
                "forum": "20L7txbIa8",
                "replyto": "naZXvonUfs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission990/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer brjf"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nThank you for your valuable comments. We appreciate the opportunity to address your queries with the following clarifications.\n\n### 1. Hyperparameter Selection\n\n**How are the hyperparameters for the comparing baseline selected?**\n\nWe performed hyperparameter tuning, and the best set of hyperparameters are reported in Appendix B.1.\n\n### 2. Performance Comparisons\n\n**What are some statistical testing results on the performance comparisons? Can we really say that the proposed method outperforms other comparing baselines?**\n\nThe comprehensive evaluation on 100+ tabular datasets presented in our work is one of the most extensive assessments in the literature. We believe these results sufficiently represent the advantages of UniPredict. While we did not perform specific statistical testing, the scale and diversity of the experiments contribute to the robustness of our findings.\n\n### 3. Interpretation of Class Probability\n\n**Can we interpret the class probability (in the target augmenting step) as the confidence?**\n\nIt's crucial to clarify that the class probability in the target augmenting step is conceptually different from a statistical 'confidence' measure. Although they may share commonalities, we do not claim them to be identical. The class probabilities are more akin to the groundtruth 'logits' used for training.\n\nOur rationale behind target augmentation is that converting targets into strings of individual label confidence would make fine-tuning more effective. To illustrate, consider two cases where in the first one we want the LLMs to predict a class, say class \u20181\u2019, and in the second we want it to give a full probability such as \u2018class 0 is 0.1, class 1 is 0.5, class 2 is 0.2, class 3 is 0.2 .\u2019 The latter, involving full probability information, provides more information for fine-tuning, resulting in better convergence.\n\n\n### 4. Handling Numerical Values\n\n**Are there better approaches in handling numerical values in the prompts? How does the model perform without using the numerical features?**\n\nThe primary focus and contribution of our work lie in presenting a comprehensive framework for handling tabular prediction in various tasks. While we did not propose specific techniques for handling numerical features, UniPredict still outperforms traditional methods in numerous scenarios. We anticipate that the performance of UniPredict will further improve with the incorporation of more advanced numerical representation techniques in the future."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission990/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021446323,
                "cdate": 1700021446323,
                "tmdate": 1700021446323,
                "mdate": 1700021446323,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]