[
    {
        "title": "Context-Aware Meta-Learning"
    },
    {
        "review": {
            "id": "S2kK6aeuY8",
            "forum": "lJYAkDVnRU",
            "replyto": "lJYAkDVnRU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8213/Reviewer_Hb5r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8213/Reviewer_Hb5r"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a novel meta-learning algorithm that allows visual models to learn new concepts during inference, mimicking the capabilities of LLMs such as ChatGPT. The technique utilizes a static pre-trained feature extractor and treats meta-learning similarly to sequence modeling with labeled and unlabeled data points. When evaluated on 11 benchmarks, the proposed method, without any meta-training or fine-tuning, outperformed or matched the leading P>M>F algorithm in 8 of those benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Intriguing Research Question:** This paper delves into a significant question in meta-learning. The authors note that meta-learning traditionally involves pretraining, meta-learning, and fine-tuning. However, their approach seeks to bypass meta-learning and fine-tuning by transforming the learning process into a sequence modeling task and applying the in-context learning objective function.\n\n**Rigorous Numerical Performance:** The paper's data shows the model's performance to be both impressive and robust.\n\n**Novel ELMES Class Encoder:** The class embedding presented here appears to be quite innovative. I've noticed no other usage of this type of class embedding in in-context learning in NLP or VLM literature unless other reviewers bring up some status quo. This is also in alignment with some recent studies on Arxiv that indicate that class embeddings might be unnecessary, as arbitrary words, numbers, or first names can be used to label images (see [2])."
                },
                "weaknesses": {
                    "value": "I will provide the paper's weaknesses in the following. \n- The concept of using the ICL objective function to pre-train a transformer model isn't a new one [1]. Unlike [1], which pre-trained a transformer from scratch within a meta-learning framework, this paper adapts the pre-training objective to Clip image embeddings, which doesn't significantly enhance novelty.\n- The primary innovative aspect highlighted in this paper is the ELMES Class Encoder. While this feature is intriguing, it narrows the scope of innovation in the study.\n- Some claims in the paper lack experimental backing. For instance, the assertion that the ELMES Class Encoder upholds label symmetry and is invariant to the permutation of demonstrations isn't convincingly proven with data.\n- The mathematical explanations in Section 4 are challenging to follow. A clearer, more comprehensible presentation of this section would be beneficial. Until then, I'm relying on other reviewers to verify the accuracy of the mathematical derivations presented."
                },
                "questions": {
                    "value": "- Could the authors elaborate on what they consider to be the primary innovative contribution of their study?\n- It would be beneficial if the authors could include experiments to demonstrate the label symmetry and permutation invariance capabilities of the demonstrations as claimed.\n\n[1]. General-Purpose In-Context Learning by Meta-Learning Transformers, NeurIPs 2022\n\n[2]. Small Visual Language Models can also be Open-Ended Few-Shot Learners, Arxiv 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8213/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8213/Reviewer_Hb5r"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684405437,
            "cdate": 1698684405437,
            "tmdate": 1700730643572,
            "mdate": 1700730643572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "POal2fyHBk",
                "forum": "lJYAkDVnRU",
                "replyto": "S2kK6aeuY8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hb5r"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our work! We appreciate you noting that our work addresses an intriguing research question, presents a rigorous empirical performance, and offers an innovative class embedding with the ELMES label encoder. We\u2019re similarly excited for future explorations into class embeddings, and hope our theoretical analysis might be beneficial to others working in this space. Addressing your questions: \n\n>1. The concept of using the ICL objective function to pre-train a transformer model isn't a new one [1]. Unlike [1], which pre-trained a transformer from scratch within a meta-learning framework, this paper adapts the pre-training objective to Clip image embeddings, which doesn't significantly enhance novelty.\n\nGPICL uses a causal approach by formulating the input as a sequence of (image, previous image\u2019s label) pairs. Specifically, GPICL must learn to look at previous elements in the sequence to associate a label with the current element\u2019s image and relies on positional encodings to keep track of where different elements are in the sequence. \n\nSimilar to GPICL, we initially tried a causal approach; however, we couldn\u2019t get this to work and it also conflicted with our intuition that permutation invariance\u2014and therefore non-causal learning\u2014would be an important property for meta-learning applications. This understanding led us to CAML as causal learning mechanisms affect empirical performance. Tangibly in Section 5.2, we observe an average difference of 7.3 accuracy points between CAML and GPICL\u2014even when GPICL uses a CLIP encoder and ELMES label encoder\u2014and hypothesize the causal approach actually forces a subtly more difficult modeling problem by imposing a causal inductive bias on a fundamentally non-causal prediction task.\n\n**GPICL Implementation** We adapt the GPICL algorithm presented by [1] for episodic meta-training with an *ELMES* label encoder. Specifically, we represent image feature vectors as CLIP embeddings and the label embeddings with an *ELMES*. Following [1], we form a sequence by concatening the current CLIP image embedding *with the previous* example's *ELMES* label embedding and add learnable positional embeddings so the model can use positional information of elements in the sequence to classify the query point in a causal-like fashion. We set the General-Purpose In-Context Learning Transformer model to a ViT-Large with leranable positional embeddings.\n\n| Approach | (5w-1s) CIFAR-fs | (5w-1s) MiniImageNet | (5w-1s) Pascal + Paintings | (5w-1s) Paintings | (5w-1s) Pascal | \n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n|GPICL| 41.5 | 95.6 | 62.6 | **51.6** | 81.7 |\n| CAML | **70.8** | **96.2** | **63.8** | 51.1 | **82.6** | \n\n| Approach | (5w-1s) meta-iNat | (5w-1s) tiered meta-iNat | (5w-1s) ChestX | (5w-1s) CUB | (5w-1s) tiered-ImageNet | (5w-1s) Aircraft |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n|GPICL | 90.0 | 60.8 | 20.1 | 75.1 | 94.6 | 19.8 |\n| CAML | **91.2** | **81.9** | **21.5** | **91.8** | **95.4** | **63.3**|\n\n| Approach | (5w-5s) CIFAR-fs | (5w-5s) MiniImageNet | (5w-5s) Pascal + Paintings | (5w-5s) Paintings | (5w-5s) Pascal | \n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n|GPICL | 78.3 | 98.2 | 74.6 | 61.0 | 88.2 |\n|CAML | **85.5** | **98.6** | **78.3** | **65.2** | **89.7**|\n\n| Approach | (5w-5s) meta-iNat | (5w-5s) tiered meta-iNat | (5w-5s) ChestX | (5w-5s) CUB | (5w-5s) tiered-ImageNet | (5w-5s) Aircraft |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n|GPICL | 95.1 | 87.6 | 20.9 | 94.5 | 97.2 | 61.8 |\n| CAML | **96.3** | **91.6** | **22.2** | **97.1** | **98.1** | **79.1** |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186190063,
                "cdate": 1700186190063,
                "tmdate": 1700253392805,
                "mdate": 1700253392805,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kXCnzWEMCr",
                "forum": "lJYAkDVnRU",
                "replyto": "S2kK6aeuY8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hb5r [Part II]"
                    },
                    "comment": {
                        "value": ">2. Some claims in the paper lack experimental backing. For instance, the assertion that the ELMES Class Encoder upholds label symmetry and is invariant to the permutation of demonstrations isn't convincingly proven with data.\n\nThis is a great question; we decompose our response into two parts:\n\n*1. ELMES Class Encoder Permutation Invariance w.r.t. demonstration order.*\n\nSuppose we have a sequence of (image, label) pairs: [(bear, 1), (tower, 2), (tree, 3)]. Then Permutation Invariance w.r.t. demonstration order means the model's predictions should be the same if we change the order to [(tree, 3), (tower, 2), (bear, 1)]. **In particular, this property is not conveyed by [1,2] as they rely on the order of demonstrations to make predictions.** We prove CAML is permutation invariant in Lemma 2, and we empirically check our proof by examining the logits of all 120 permutations of the example presented in Figure 2(a) and verify that there is no variation.\n\n*2. Meta-Learning label symmetry.*\n\nLabel symmetry is a property of few-shot classification. For instance, the predictions for [(bear, 1), (tower, 2), (tree, 3)] should be the same if the labels are permuted to [(bear, 3), (tower 1), (tree, 2)]. CAML is not invariant to permutations in the assignment of classes to support set examples as implied by Equation (1) in Section 4.2; however, we empirically find it is robust to them. Specifically, we find that CAML is approximately invariant to permutations in the assignment of labels to support set classes; the average standard deviation in the correct class prediction probability is only 0.004 on mini-ImageNet. We expand on this point with additional analysis in the updated manuscript in Section C of the Appendix under header Assignment of Labels to Support Set Classes Analysis and Figure 5. \n\n>3. Unclear Mathematical Explanation.\n\nDue to space constraints, we relocated a significant amount of theoretical exposition to Section A.1.1 of the supplementary material. We can work towards elucidating Section 4; however, the material spans Geometry (regular simplexes inscribed within a sphere), Information Theory (entropy, optimal coding), Representation Theory (Equiangular Tight Frames, Welch Lower Bound), and Group Theory (SO(3) ELMES invariance). In the meantime, we hope that the theoretical analysis in Section 4, if imperfect, may still be beneficial to readers in understanding the theoretical motivation of CAML."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186522725,
                "cdate": 1700186522725,
                "tmdate": 1700252484900,
                "mdate": 1700252484900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nBa0bb5Gyg",
                "forum": "lJYAkDVnRU",
                "replyto": "S2kK6aeuY8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hb5r [Part III]"
                    },
                    "comment": {
                        "value": ">4. Could the authors elaborate on what they consider to be the primary innovative contribution of their study?\n\nThank you for posing this question. Based on your review, it\u2019s clear that our contribution was not clearly communicated in our initial submission. We think this is a critical point\u2014and specifically address your question in our response to all reviewers\u2014but we\u2019d like to extend that response here as well. \n\nOur primary contribution is twofold.\n\n**1. We develop a new meta-learning evaluation paradigm (universal meta-learning) that approximates the performance of visual meta-learning algorithms in a ChatGPT-like application.**\n\nThat is, we want to see how visual meta-learners perform on diverse tasks unseen at training, with no parameter updates. This is hard: we want a model that will work well for any set of images, but actually deploying to users and quantifying failure cases is impractical for many research groups. Our best proxy to measure a model\u2019s capacity to generalize in a ChatGPT-like application is to evaluate it on a diverse set of meta-learning benchmarks spanning many different image classification paradigms without meta-training on any of their training sets or fine-tuning on the support set during inference. \n\n*What is the motivation?* \n\nWe\u2019d like to develop an evaluation paradigm for meta-learning models that quantifies how they would perform in a ChatGPT-like application. Specifically, how do meta-learning algorithms perform when a user can upload any set of images and serving-time constraints prevents fine-tuning on the support set?\n\n*What is missing in current meta-learning evaluation paradigms?*\n\n In-domain meta-learning paradigms measure generalization to unseen classes during inference by training on closely related classes during training (i.e. meta-training). Cross-domain meta-learning refers to a challenging evaluation paradigm where the meta-training and inference-time data distributions are significantly different, and the goal is to maximize the performance on the inference-time data distribution by fine-tuning on the support set. \n\nThere always seems to be a one-to-one or many-to-one correspondence between the set of classes used for training and the set of classes used during inference. A sense of many-to-many, or simply \u201cdo well on everything\u201d, isn\u2019t captured by either in-domain or cross-domain evaluation paradigms, and even for cross-domain settings, the performance on classes similar to those used for meta-training is often not evaluated. \n\n*What does setting universal meta-learning specifically address?*\n\nUniversal meta-learning specifically addresses a serving-time application of few-shot image classification where the user can upload any set of images and only specify which images are similar (i.e. belong to the same class). Meta-training on a set of related images is impossible, because the space of all possible images that could be uploaded by the user can\u2019t be covered. Moreover, fine-tuning on the support set is also not feasible for inference-time applications deployed in a ChatGPT-like setting. \n\nWe define the universal meta-learning evaluation setting to measure progress in this area. This type of evaluation paradigm already exists for NLP\u2014and is quite popular\u2014but we are unfamiliar with one if it already exists for few-shot image classification."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186859529,
                "cdate": 1700186859529,
                "tmdate": 1700251848795,
                "mdate": 1700251848795,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W6oYMWMUrU",
                "forum": "lJYAkDVnRU",
                "replyto": "S2kK6aeuY8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hb5r [Part IV]"
                    },
                    "comment": {
                        "value": "**2. We develop a meta-learning algorithm that works well in this setting (i.e. CAML).**\n\nCAML is a sequence-based meta-learning algorithm that is designed to generalize to any set of images during inference without fine-tuning (thereby addressing the problem). In our experiments, we find that CAML surpasses the performance of other meta-learning algorithms in the *universal meta-learning* evaluation setting. Surprisingly, its performance in the *universal meta-learning* setting often matches\u2013and even exceeds\u2014the in-domain meta-learning performance of the state-of-the-art meta-learning algorithm, P>M>F, that is directly meta-trained on each benchmark.\n\nCAML is designed to perform well in the *universal meta-learning* setting \u2013 that is, it\u2019s \u201cgeneral and fast\u201d \u2013 while prior approaches are not. Specifically, and distinct from prior sequence-based meta-learning algorithms like GPICL [1] and SNAIL [2], \n\n1. CAML is invariant to permutations in the support set and non-causal. In our experiments, we observe a performance gap between both approaches and CAML and hypothesize the causal approach actually forces a subtly more difficult modeling problem by imposing a causality inductive bias on a fundamentally non-causal prediction task.\n2. CAML uses an Equal Length and Maximally Equiangular Set (ELMES) to integrate the label information from the support set into the model. Our theoretical analysis shows this formulation minimizes the entropy of identifying class labels. \n3. CAML represents an (image, label) element of the support set as a single vector that contains information about both the image and the label. Specifically, a Foundational Model Image Encoder embeds the support set image, an ELMES embeds the label, and both vectors are concatenated to form a single demonstration vector from the (image, label) pair in the support set.\n\n**These changes make a big difference. In our experiments,  we observe a performance gap of (on average) 5.2 accuracy points between CAML and SNAIL and 7.3 accuracy points between CAML and GPICL for each benchmark.**\n\n---\n\n[1] General-Purpose In-Context Learning by Meta-Learning Transformers, NeurIPs 2022\n\n[2] Mishra, Nikhil, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. \"A simple neural attentive meta-learner.\", ICLR 2018"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251964719,
                "cdate": 1700251964719,
                "tmdate": 1700256402522,
                "mdate": 1700256402522,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0YTyFeNf54",
                "forum": "lJYAkDVnRU",
                "replyto": "vGAVPgdg7x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Reviewer_Hb5r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Reviewer_Hb5r"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' comprehensive responses to my inquiries and those from other reviewers. I have carefully reviewed all of their replies. The authors' clarifications have effectively highlighted their work's novelties and contributions, which I find satisfactory. Additionally, their comparison of the proposed method with GPICL and SNAIL is noteworthy, and the demonstrated improvements are enough. Given the authors' satisfactory answers, I am inclined to raise my current score to 6."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730624666,
                "cdate": 1700730624666,
                "tmdate": 1700730624666,
                "mdate": 1700730624666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0EVGJhmreR",
            "forum": "lJYAkDVnRU",
            "replyto": "lJYAkDVnRU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8213/Reviewer_yxC1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8213/Reviewer_yxC1"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a meta-learning algorithm that learns new visual concepts during inference without fine-tuning. The method performs well on several benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper draws a new perspective for meta-learning: learning to classify a query from a context of support set, imitating the way in LLMs. \n2. The framework is straightforward and clean. The reason (proved theoretically) for using the specific ELMES embedding is presented well.\n3. Extensive experiments and analysis are provided."
                },
                "weaknesses": {
                    "value": "Apologies in advance I am not an expert in meta-learning. But I still have the following questions:\n1. What is the unknown class embedding initialization for the ELMES Class Encoder?\n2. As discussed by the authors themselves in section 5.3, the number of classes need to be known in advance and the frozen encoders limit the learning ability. However in my view, the need of number of classes is an inherent problem in few-shot learning. But finetuning more modules could be further discussed.\n3. There is a lack of experimental details (especially training details)."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705246881,
            "cdate": 1698705246881,
            "tmdate": 1699637019709,
            "mdate": 1699637019709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UCHLGH13Wp",
                "forum": "lJYAkDVnRU",
                "replyto": "0EVGJhmreR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yxC1"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our work! We appreciate you noting the extensiveness of our experiments and analysis as well as the elegance of our proposed approach. Addressing your questions:\n\n>1. What is the unknown class embedding initialization for the ELMES Class Encoder?\n\nSimilar to the cls token in ViT image classification, we initialize the unknown class embedding using the default torch.nn.Parameter initialization and learn this value during large-scale pre-training. We have updated the manuscript to emphasize this point.\n\n>2. Fine-tuning more modules could be discussed.\n\nWe do not fine-tune the image encoder because it is not optimal for *universal meta-learning*.\n\nOur goal is to develop a meta-learning algorithm that may function in a ChatGPT-like application; it should be able to run in-context learning on any set of images. Foundational image models are trained for exactly this purpose: they are pre-trained on billions of images to form a well-structured image embedding space that is robust to augmentations, occlusions, etc. Moreover, valuable characteristics such as the presence of objects, textures, etc. of an image are encoded into the structure of the embedding space so that the axes of variability among the embeddings encode variation in specific visual attributes. \n\nFine-tuning the image encoder can corrupt this embedding space; especially since the datasets we use for pre-training are orders of magnitude smaller than the ones used to train the Foundational model. This hypothesis is supported by our experiments with ProtoNet and MetaOpt in Tables 1, 2, 3, and 4. Specifically, we find fine-tuning the backbone during pre-training leads to performance degradation on many of our benchmarks when evaluated in the *universal meta-learning* setting. \n\nThanks to your comment\u2014and realizing this discussion is missing from the manuscript\u2014we have updated the paper to include this discussion in Section D of the Appendix. \n\n>3. There is a lack of experimental details (especially training details).\n\nThank you for pointing out this weakness; these details were sorely missing from the initial submission. We have updated the manuscript to describe the experimental settings in-depth in Section B of the Appendix. Please let us know if anything is unclear. Our code is also released in the supplementary material download attached to this submission and we commit to releasing a github repository after the review process to facilitate reproducing our results."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185918571,
                "cdate": 1700185918571,
                "tmdate": 1700251195975,
                "mdate": 1700251195975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SoqaqmKSUM",
                "forum": "lJYAkDVnRU",
                "replyto": "Ov8RdGQKKp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Reviewer_yxC1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Reviewer_yxC1"
                ],
                "content": {
                    "title": {
                        "value": "Sorry for the late reply and please be careful"
                    },
                    "comment": {
                        "value": "Thank the authors for their replies and my concerns are resolved. Apologies for the late reply.\n\nI appreciate the effort of the authors for revising the paper. HOWEVER, the page limit of the revised version shall NOT exceed the page limit of 9. Please be careful."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623383311,
                "cdate": 1700623383311,
                "tmdate": 1700623383311,
                "mdate": 1700623383311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z6CMkfmr8F",
                "forum": "lJYAkDVnRU",
                "replyto": "av2FIl3dCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Reviewer_yxC1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Reviewer_yxC1"
                ],
                "content": {
                    "title": {
                        "value": "Update"
                    },
                    "comment": {
                        "value": "Thank the authors for your timely response! I appreciate the efforts made and I am keeping my score as 6."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629759619,
                "cdate": 1700629759619,
                "tmdate": 1700629759619,
                "mdate": 1700629759619,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iHOPYtdXLp",
            "forum": "lJYAkDVnRU",
            "replyto": "lJYAkDVnRU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8213/Reviewer_GdKg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8213/Reviewer_GdKg"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new \u201cuniversal meta-learning\u201c setup that \u201cavoids meta-training on the train/validation splits of meta-learning benchmarks or fine-tuning on the support set during inference.\u201d Instead, the paper attempts to recast meta-learning as a sequence modelling problem, where meta-testing on new tasks is analogous to in-context learning in large language models. The proposed approach CAML, context-aware meta-learning, uses CLIP image representations, together with one-hot label encoding dubbed as \u201cEqual Length and Maximally Equiangular Set (ELMES) encoding,\u201d \u00a0to represent each in-context learning example. The base sequence model is a Transformer encoder. It is trained to predict the query class label given in-context examples that are comprised of labelled support examples and a query example. The model is pre-trained on ImageNet-1k, Fungi, MSCOCO, and WikiArt and evaluated on 11 meta-learning benchmark datasets. Empirical results show that the proposed approach outperforms other \u201cuniversal meta-learning\u201c baselines on 15 of 22 evaluation settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is well-motivated on the need for in-context learning by drawing analogies with large language models. I also liked the analysis in Fig. 2, which illustrates how dynamic in-context examples impact representation learning\n    \n2. Theoretical analysis of the \u201cEqual Length and Maximally Equiangular Set (ELMES) encoding\u201d presents an interesting analysis of label symmetry and permutation invariance in meta-learning.\n    \n3. The paper presents competitive empirical results on various meta-learning baselines."
                },
                "weaknesses": {
                    "value": "1. Novelty: the paper does not discuss previous work that also formulated meta-learning as a sequence, or set, modelling problem [1, 2]. The problem formulations in [1, 2] are highly similar to the proposal in this paper, except for architectural differences in implementation. This weakens the novelty of this paper.\n\n2. The dichotomy between \u201cmeta-training\u201c and \u201duniversal meta-learning\u201d: the paper attempts to make the distinction between \u201duniversal meta-learning\u201d and \"meta-training\" in that the proposed CAML approach does not perform \u201cmeta-training\u201c or \u201cfine-tuning on the support set.\u201c Instead, \u201duniversal meta-learning\u201d only performs pre-training. However, I think this dichotomy is not well-defined. Pre-training in the CAML fashion can be understood as learning across many different tasks in the pre-training dataset, i.e., meta-training, and in-context learning can be understood as performing implicit gradient descent based on the in-context examples. This dichotomy also implicates the comparisons in empirical results as CAML was mostly compared with other \u201cuniversal meta-learning\u201c methods, i.e., ProtoNet, MetaOpt, and MetaQDA.\n\nAdditional related work:\n\n[1] Mishra, Nikhil, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. \"A simple neural attentive meta-learner.\"\u00a0*arXiv preprint arXiv:1707.03141*\u00a0(2017).\n\n[2] Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S. &amp; Teh, Y.W.. (2019). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. <i>Proceedings of the 36th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 97:3744-3753 Available from https://proceedings.mlr.press/v97/lee19d.html."
                },
                "questions": {
                    "value": "1. Please summarize the novelty of this paper in relation to [1, 2].\n    \n2. Please respond to Weakness 2: The dichotomy between \u201cmeta-training\u201c and \u201duniversal meta-learning.\u201d\n    \n3. Please elaborate on how the transformer encoder is implemented and the rough scale of parameters it has.\n    \n4. Please elaborate on the pre-training dataset of CAML in \u201cwe pre-train CAML\u2019s Transformer encoder on few-shot image classification tasks from ImageNet-1k.\u201d How many examples are included in the pre-training set? Note that one of the benchmarks, miniImageNet, is a subset of ImageNet. Would this pretraining dataset result in task leakage?\n    \n5. Please elaborate on how the ProtoNet baseline is implemented. Is it trained on the same pre-training dataset but with the ProtoNet loss objective?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724760684,
            "cdate": 1698724760684,
            "tmdate": 1699637019594,
            "mdate": 1699637019594,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MqY0q0DoEX",
                "forum": "lJYAkDVnRU",
                "replyto": "iHOPYtdXLp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GdKg"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our work! It\u2019s encouraging that you find our work is well-motivated, presents a compelling analysis with Figure 2, contributes an appropriate theoretical analysis, and presentes strong empirical results on various meta-learning baselines. Addressing your concerns:\n\n>1a. Novelty: the paper does not discuss previous work that also formulated meta-learning as a sequence. Please summarize the novelty of this paper in relation to [1].\n\nThank you for bringing this to our attention \u2013 from your and other reviewers\u2019 feedback, we\u2019ve been able to significantly improve our Related Work. We\u2019ve re-written the related work to include a section on meta-learning as sequence modeling. Based on community feedback, we\u2019ve also included a separate section on in-context learning for other applications of computer vision (i.e. in-painting, depth estimation, etc.).\n\nAddressing your question, [1] is causal: any element in the sequence cannot use later elements to influence its own learned representation. Similar to [1], we initially tried a causal approach; however, we couldn\u2019t get this to work and it also conflicted with our intuition\u2014and the intuition of Set Transformer [2]\u2014that permutation invariance would be an important property for meta-learning applications. This also affects empirical performance. In Section 5.2, we observe an average difference of 5.2 (coincidentally the difference is equal to the section number) accuracy points between CAML and SNAIL [1] and hypothesize the causal approach actually forces a subtly more difficult modeling problem by imposing a causal inductive bias on a fundamentally non-causal prediction task.\n\n**SNAIL Implementation.** We use the architecture presented in [1] but with the hidden dimension of the Attention and Temporal Convolution Blocks adapted to CLIP embeddings rather than the ResNet embeddings used in the original implementation. As in this [1], we freeze the feature extractor and train the SNAIL model parameters during large-scale pre-training.\n\n| Approach | (5w-1s) CIFAR-fs | (5w-1s) MiniImageNet | (5w-1s) Pascal + Paintings | (5w-1s) Paintings | (5w-1s) Pascal | \n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n|SNAIL| 62.1 | 93.6 | 62.5 | **51.9** | 79.7 |\n| CAML | **70.8** | **96.2** | **63.8** | 51.1 | **82.6** | \n\n| Approach | (5w-1s) meta-iNat | (5w-1s) tiered meta-iNat | (5w-1s) ChestX | (5w-1s) CUB | (5w-1s) tiered-ImageNet | (5w-1s) Aircraft |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n|SNAIL | 89.1 | 77.3 | 20.2 | 87.5 | 93.1 | 48.9 |\n| CAML | **91.2** | **81.9** | **21.5** | **91.8** | **95.4** | **63.3**|\n\n| Approach | (5w-5s) CIFAR-fs | (5w-5s) MiniImageNet | (5w-5s) Pascal + Paintings | (5w-5s) Paintings | (5w-5s) Pascal | \n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n|SNAIL | 71.1 | 98.1 | 77.6 | **65.8** | 88.0 |\n|CAML | **85.5** | **98.6** | **78.3** | 65.2 | **89.7**|\n\n| Approach | (5w-5s) meta-iNat | (5w-5s) tiered meta-iNat | (5w-5s) ChestX | (5w-5s) CUB | (5w-5s) tiered-ImageNet | (5w-5s) Aircraft |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| SNAIL | 94.8 | 86.5 | 20.0 | 92.8 | 97.3 | 35.8 | \n| CAML | **96.3** | **91.6** | **22.2** | **97.1** | **98.1** | **79.1** |\n\n>1b. Please summarize the novelty of this paper in relation to [2].\n\nWe share a common insight with [2]: permutation invariance is an important property for many applications of machine learning, and especially meta-learning. However, the contribution of [2] is to provide a permutation invariant sequence-to-sequence architecture, not develop a meta-learning algorithm. Using an analogy, if a meta-learning is a car, then [2] is an engine that might be installed in that car. Indeed [2] says as much in their conclusion, \u201cAn interesting future work would be to apply Set Transformer to meta-learning problems.\u201d\n\nIn the language of [2], the Transformer encoder of CAML is implemented as SAB, or simply, a self-attention block without positional embeddings. Inspired by your comment, we experimented with switching out SAB for the ISAB block described by [2] during the rebuttal period, but found issues with convergence, likely due to the additional complexity of the ISAB mechanism."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185429833,
                "cdate": 1700185429833,
                "tmdate": 1700253412068,
                "mdate": 1700253412068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c1GsWkDFWJ",
                "forum": "lJYAkDVnRU",
                "replyto": "iHOPYtdXLp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GdKg Part II"
                    },
                    "comment": {
                        "value": ">2. Please respond to Weakness 2: The dichotomy between \u201cmeta-training\u201c and \u201duniversal meta-learning.\u201d\n\nThank you for posing this question! We think this is a critical distinction, and based on your review, it is clear that we failed to communicate this point in the initial manuscript. We specifically address your question in our response to all reviews, but we\u2019d like to extend our response here as well.\n\n*What is the motivation?* We\u2019d like to develop an evaluation paradigm for meta-learning models that quantifies how they would perform in a ChatGPT-like application. Specifically, how do meta-learning algorithms perform when a user can upload any set of images and serving-time constraints prevents fine-tuning on the support set?\n\n*What is missing in current meta-learning evaluation paradigms?* In-domain meta-learning paradigms measure generalization to unseen classes during inference by training on closely related classes during training (i.e. meta-training). Cross-domain meta-learning refers to a challenging evaluation paradigm where the meta-training and inference-time data distributions are significantly different, and the goal is to maximize the performance on the inference-time data distribution by fine-tuning on the support set.\n\nThere always seems to be a one-to-one or many-to-one correspondence between the set of classes used for training and the set of classes used during inference. A sense of many-to-many, or simply \u201cdo well on everything\u201d, isn\u2019t captured by either in-domain or cross-domain evaluation paradigms, and even for cross-domain settings, the performance on classes similar to those used for meta-training is often not evaluated.\n\n*What does setting universal meta-learning specifically address?* Universal meta-learning specifically addresses a serving-time application of few-shot image classification where the user can upload any set of images and only specify which images are similar (i.e. belong to the same class). Meta-training on a set of related images is impossible, because the space of all possible images that could be uploaded by the user can\u2019t be covered. Moreover, fine-tuning on the support set is also not feasible for inference-time applications deployed in a ChatGPT-like setting. \n\nWe propose the universal meta-learning evaluation setting to measure progress in this area. This type of evaluation paradigm already exists for NLP\u2014and is quite popular\u2014but we are unfamiliar with one if it already exists for few-shot image classification.\n\n*Aside.*\n\nIf your comment simply refers to loose terminology around \u201cwithout meta-training\u201d\u2014we use \u201cwithout meta-training\u201d, \u201cwithout meta-training on related classes\u201d, and \u201cwithout meta-training on benchmark training sets\u201d interchangeably\u2014then we\u2019re happy to tighten this up."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185634606,
                "cdate": 1700185634606,
                "tmdate": 1700251022349,
                "mdate": 1700251022349,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MuvKshHTn9",
                "forum": "lJYAkDVnRU",
                "replyto": "iHOPYtdXLp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GdKg Part III"
                    },
                    "comment": {
                        "value": ">3. Please elaborate on how the transformer encoder is implemented and the rough scale of parameters it has.\n\nWe use a ViT-Large Transformer Encoder as described in Table 1 of  [3]. It has 302 million trainable parameters. We have added an in-depth description of its implementation in Section B of the Appendix under the header CAML Implementation.\n\n>4. Please elaborate on the pre-training dataset of CAML. Note that one of the benchmarks, miniImageNet, is a subset of ImageNet. Would this pretraining dataset result in task leakage?\n\nAll methods evaluated in the *universal meta-learning* setting adhere to the same pre-training paradigm. For each large-scale image classification dataset, we reformulate the objective from typical supervised image classification to both a 5-way-1-shot and a 5-way-5-shot episodic prediction tasks. Within a dataset, examples from different classes are randomly sampled to compose a batch of episodes, and after exhausting iterating through every training example, this process is repeated with the next dataset. Iterating through each dataset in our set of ImageNet-1k, Fungi, MSCOCO, and WikiArt then constitutes a single epoch of training.\n\nWe have added this discussion as well as additional information regarding optimization settings to Section B of the Appendix, paragraphs Large-Scale Pre-training and Optimization settings. Please let us know if anything is unclear or missing.\n\n*Task leakage?* \n\nThere is task leakage in the mini-ImageNet evaluation; however, this has become standard practice on mini-ImageNet as many feature extractors are pre-trained on ImageNet-1k \u2014 please see the excellent discussion of this topic in [4] (specifically, the footnote on page 6 as well as the the paragraph \u201cClass overlap between pre-training and meta-testing\u201d). Tangibly, all baselines in our experimental analysis pre-train on ImageNet-1k (including P>M>F as it uses a DINO feature extractor), so we believe the comparison is fair. \n\nIn the context of *universal meta-learning* evaluation, we see mini-ImageNet as a benchmark that evaluates how different meta-learning algorithms perform when evaluated on classes that they have previously seen, similar to how a LLM might recall a memorized answer to a question seen during training.\n\n>5. Please elaborate on how the ProtoNet baseline is implemented. Is it trained on the same pre-training dataset but with the ProtoNet loss objective?\n\nPrecisely; it is trained on the same large-scale pre-training dataset but with the ProtoNet loss function. We have added an in-depth discussion of the ProtoNet baseline implementation and training paradigm in Section B of the Appendix under paragraph ProtoNet and MetaOpt Implementations.\n\n---\n\n[1] Mishra, Nikhil, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. \"A simple neural attentive meta-learner.\", ICLR 2018\n\n[2] Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S. & Teh, Y.W.. (2019). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks., ICML 2019\n\n[3] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Dosovitskiy et al., ICLR 2021\n\n[4] Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference, Hu et al., CVPR 2022"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251003302,
                "cdate": 1700251003302,
                "tmdate": 1700251062997,
                "mdate": 1700251062997,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M5OskEA0Q1",
                "forum": "lJYAkDVnRU",
                "replyto": "2GgLWjqZKr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Reviewer_GdKg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Reviewer_GdKg"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I'd like to thank the authors for the detailed response, as well as the additional experiments on related methods. Although the overall quality and presentation of the paper have improved, I still think the novelty in relation to [1,2] weakens the contribution of this paper. Furthermore, I do not think the notion of \"universal meta-learning\" is an appropriate term for the proposed approach; chatgpt-style evaluation paradigm may not be \"universal\" from the perspective of the meta-learning problem formulation. I'm optimistic about the proposed approach, but I think the authors should better formulate the problem in relation to the meta-learning literature."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693004571,
                "cdate": 1700693004571,
                "tmdate": 1700693004571,
                "mdate": 1700693004571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iUAwFrS2Ys",
            "forum": "lJYAkDVnRU",
            "replyto": "lJYAkDVnRU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8213/Reviewer_8Akm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8213/Reviewer_8Akm"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses a gap in the field of visual meta-learning, where models have traditionally struggled to learn new visual concepts during inference without fine-tuning, a capability that Large Language Models (LLMs) like ChatGPT have demonstrated in the textual domain. The authors introduce a novel meta-learning algorithm inspired by the in-context learning of LLMs. This approach treats n-way-k-shot image classification as a sequence modeling over known labeled data points and an unknown test data point."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written.\n2. The problem studied in this paper is interesting and valuable.\n3. The theoretical work of this paper is sufficient, which improves the value of the paper."
                },
                "weaknesses": {
                    "value": "1. The authors utilize the CLIP model to encode both images and labels. An area of potential exploration is why they didn't attempt to encode context and images directly, especially using datasets like MSCOCO.\n2. In the experiments, CAML's performance on out-of-domain tasks is notably weak. This might be primarily due to the treatment of unseen categories, which are uniformly encoded as \"Unknown [class] Embedding\".\n3. The study lacks ablation experiments for its various modules, and there's an absence of quantitative analysis for hyperparameters."
                },
                "questions": {
                    "value": "Please see the Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849443014,
            "cdate": 1698849443014,
            "tmdate": 1699637019449,
            "mdate": 1699637019449,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L8LR170y6K",
                "forum": "lJYAkDVnRU",
                "replyto": "iUAwFrS2Ys",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8Akm"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our work! We appreciate your assessment that the problem studied in this paper is interesting and valuable as well as the value of our theoretical contribution on label encodings. Addressing your questions:\n\n>1. The authors utilize the CLIP model to encode both images and labels. An area of potential exploration is why they didn't attempt to encode context and images directly, especially using datasets like MSCOCO.\n\nThere may be a misunderstanding as our approach does not encode labels with natural language. Specifically, if we are trying to classify a query image from a support set composed of an image of a dog and another image of a cat, then there is no natural language encoding of either \u201cdog\u201d or \u201ccat\u201d. Rather, we only know that the two images in our support set belong to different classes, and our goal is to associate the query image with one of these two classes. \n\nWe choose this setting\u2014even when natural language descriptors of an image are available\u2014to evaluate a ChatGPT-like application of few-shot image classification. Specifically, given only a support set containing images and knowledge of which images belong to the same class, can we correctly associate a new image with one of the classes in our support set? \n\nReturning to your question, we can\u2019t directly encode a context and images directly using a dataset like MSCOCO as natural language descriptors are not available during inference. The advantage to this approach is its flexibility: images can be sliced by arbitrary attributes such as the presence of objects or specific textures as visualized in Figure 2.  Another benefit is its simplicity: it involves a single modality (images) and does not need to handle unseen label classes. \n\n>2. In the experiments, CAML's performance on out-of-domain tasks is notably weak. This might be primarily due to the treatment of unseen categories, which are uniformly encoded as \"Unknown [class] Embedding\".\n\nBuilding on our previous response, there is no notion of seen or unseen class labels. The model simply knows which images belong to the same class in the support set. We encode the class  information with an ELMES so that image embeddings belonging to the same class are concatenated with the same ELMES label embedding. This allows the model to identify which images in the support set belong to the same (or different) classes.\n\nWe have run new experiments to study CAML\u2019s performance on out-of-domain tasks. Specifically, replacing the CLIP image encoder with a ViT-huge pre-trained on Laion-2b increases Aircraft 5-1 accuracy from 63.3 to 81.8 and Aircraft 5-5 accuracy from 79.1 to 92.1. We hypothesize the improvement on Aircraft is due to CLIP embeddings not capturing variability among images, while ViT-h embeddings are better separated. **The analysis we add in Section C of the Appendix under \"Image Encoder Ablation\" supports this hypothesis and also explains why Laion-2b does not improve CAML\u2019s performance on ChestX.**\n\n| Approach | (5w-1s) CIFAR-fs | (5w-1s) MiniImageNet | (5w-1s) Pascal + Paintings | (5w-1s) Paintings | (5w-1s) Pascal | \n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| CAML (CLIP) | 70.8 | 96.2 | 63.8 | 51.1 | 82.6 | \n|CAML (Laion-2b) | **83.3** | **98.6** | **66.4** | **54.7** | **83.4**| \n\n| Approach | (5w-1s) meta-iNat | (5w-1s) tiered meta-iNat | (5w-1s) ChestX | (5w-1s) CUB | (5w-1s) tiered-ImageNet | (5w-1s) Aircraft |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| CAML (CLIP) | 91.2 | 81.9 | **21.5** | 91.8 | 95.4 | 63.3|\n|CAML (Laion-2b) | **94.6** | **89.3**| **21.6** | **95.8** | **96.8**| **81.8**|\n\n| Approach | (5w-5s) CIFAR-fs | (5w-5s) MiniImageNet | (5w-5s) Pascal + Paintings | (5w-5s) Paintings | (5w-5s) Pascal | \n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n|CAML (CLIP) | 85.5 | 98.6 | 78.3 | 65.2 | 89.7|\n|CAML (Laion-2b) | **93.5** | **99.6** | **81.0** | **69.9** | **90.1** | \n\n| Approach | (5w-5s) meta-iNat | (5w-5s) tiered meta-iNat | (5w-5s) ChestX | (5w-5s) CUB | (5w-5s) tiered-ImageNet | (5w-5s) Aircraft |\n| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n| CAML (CLIP) | 96.3 | 91.6 | **22.2** | 97.1 | 98.1 | 79.1 | \n| CAML (Laion-2b) | **97.9** | **95.6** | **22.0** | **98.7** | **98.8** | **92.1**|"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185007665,
                "cdate": 1700185007665,
                "tmdate": 1700253431263,
                "mdate": 1700253431263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]