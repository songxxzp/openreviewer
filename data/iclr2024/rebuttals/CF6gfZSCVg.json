[
    {
        "title": "Anarchic Federated Bilevel Optimization"
    },
    {
        "review": {
            "id": "jSb2001HrJ",
            "forum": "CF6gfZSCVg",
            "replyto": "CF6gfZSCVg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2013/Reviewer_3Fd8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2013/Reviewer_3Fd8"
            ],
            "content": {
                "summary": {
                    "value": "A double-loop scheme Anarchic Federated Bilevel Optimization (AFBO) is proposed in this work, which allows clients to flexibly participate in federated bilevel optimization training according to their heterogeneous and time-varying computation and communication capabilities. Moreover, theoretical analysis is conducted to show the convergence rate of the proposed method, i.e., it is demonstrated that the proposed  AFBO algorithm can achieve a convergence rate of $O(\\sqrt{1/T})$."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This proposed method is efficient since the clients in the distributed system can participate in any inner or outer rounds, asynchronously, and with any number of local iterations.\n\n2. The authors conduct lots of theoretical analysis about the proposed method, e.g., convergence analysis. It is demonstrated that the proposed  AFBO algorithm can achieve a convergence rate of $O(\\sqrt{1/T})$.\n\n3. This paper is well-organized and easy to follow."
                },
                "weaknesses": {
                    "value": "I have some concerns about the experiments and communication complexity as follows.\n\n1. In the experiments, the authors claim the proposed AFBO is the most efficient algorithm. However, more explanation should be added about why the proposed method is more efficient than ADBO in the experiment since the ADBO is also an asynchronous algorithm.\n\n2. More experimental results should be added to show the excellent performance of the proposed AFBO.\n\n3. Lack of analysis for the communication complexity of the proposed method."
                },
                "questions": {
                    "value": "My questions are about the experiments and communication complexity, please see the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2013/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635690778,
            "cdate": 1698635690778,
            "tmdate": 1699636132778,
            "mdate": 1699636132778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Sz1cmy8WLQ",
                "forum": "CF6gfZSCVg",
                "replyto": "jSb2001HrJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments.\n\nW1: In the experiments, the authors claim the proposed AFBO is the most efficient algorithm. However, more explanation should be added about why the proposed method is more efficient than ADBO in the experiment since the ADBO is also an asynchronous algorithm.\n\nA1: Based on your comment, in the revised paper, we have added more experiment results in Section B.3, Section B.4, Section B.5, and Section B.6. of the supplement material. We conduct experiments on the convergence of different algorithms under different inner and outer loop delays, comparison of AFBO and AFL, and the impact of asynchronous delays, etc., discussed as follows. For the inner loop asynchronous delays experiments, the results show that AFBO performs best among all algorithms. In addition, it shows that all algorithms expect AFBO to suffer an obvious convergence degeneration as there exists an inner loop asynchronous delay. Among them, synchronous algorithms (e.g., SDBO, FedNest, and Prometheus) degenerate much more than AFBO and ADBO. Moreover, we find that convergence degeneration is much more obvious in the non-IID MINIST dataset than in the IID MINIST dataset. This is because the bias of using the most recent update of a client is much larger in the non-IID MINIST dataset case than in the IID MINIST dataset case. Intuitively, a reusing of past gradients inducts a new bias of SGD, as some of the SGD gradients use more than others. Finally, it shows that the convergence degeneration in the Covertype dataset is slighter than in the MINIST dataset. The experiments on comparison between AFBO and AFL shows that AFL performs worse on bilevel tasks than AFBO, but they have a similar convergence rate as they both use the most recent updates to update the global model and achieve a linear speed-up. The experiment on the impact of asynchronous delays on AFBO shows that the test accuracy becomes smaller as the asynchronous delay increases, which matches our theoretical results. In addition, there is little convergence degradation due to the asynchronous delay, which proves the effectiveness and robustness of AFBO. Moreover, it shows that the convergence degradation under the non-IID MINIST dataset is slightly higher than under the IID MINIST dataset. This is because the contribution of all clients is equal in the IID dataset which means the straggler has low effects on the test accuracy, but some clients owning unique datasets under non-IID dataset settings with higher asynchronous delay can cause a larger convergence degradation due to straggler effects. \n\nW2: More experimental results should be added to show the excellent performance of the proposed AFBO.\n\nA2: Based on your comment, in the revised paper, we have added more experiment results in Section B.3, Section B.4, Section B.5, and Section B.6 of the supplement material. Please refer to our response to A1 for detailed discussion of these results.\n\nW3: Lack of analysis for the communication complexity of the proposed method.\n\nA3: Based on your comment, in the revised paper, we have added analysis of the communication complexity of the AFBO algorithm (in Corollary $1$'s Remark, marked in blue): \u201cIn addition, we need $T=O(\\kappa^5_g\\epsilon^{-2}/n)$ to achieve an $\\epsilon$-accurate stationary point. Compared with FedNest (Tarzanagh et al., 2022), our complexity has the same dependence on $\\kappa$ and $\\epsilon$, but a better dependence on $n$ due to the linear speedup.\""
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607776191,
                "cdate": 1700607776191,
                "tmdate": 1700607776191,
                "mdate": 1700607776191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8VWV2qq8nX",
                "forum": "CF6gfZSCVg",
                "replyto": "Sz1cmy8WLQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2013/Reviewer_3Fd8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2013/Reviewer_3Fd8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your responses. My concerns have been addressed."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658990461,
                "cdate": 1700658990461,
                "tmdate": 1700658990461,
                "mdate": 1700658990461,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8frXGZNRyU",
            "forum": "CF6gfZSCVg",
            "replyto": "CF6gfZSCVg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2013/Reviewer_3dLq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2013/Reviewer_3dLq"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new algorithm called Asynchronous Federated Bilevel Optimization (AFBO), offering a flexible approach for client participation in federated bilevel optimization (FBO) training. The unique aspects of AFBO include the ability for clients to not only join in at any stage of the inner or outer optimization rounds, but also undertake a variable number of local training iterations. The training process can be engaged asynchronously. Rigorous theoretical examination has been conducted to reveal convergence rate. It is seen that AFBO's convergence rate aligning with other benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Asynchronous federated learning is an important problem with numerous applications. The theoretic analysis is solid and authors have conducted experiments to assess the performance of the proposed algorithm."
                },
                "weaknesses": {
                    "value": "The experiment results are limited. Authors are recommended to compare the performance of the proposed algorithm with algorithms such as [1].  In addition, the idea itself is similar to [2]. Authors are therefore recommended to elaborate more on the difference between this work and existing ones in the literature. In addition, in all figures, it is seen that the performance of AFBO is only slighted better than ADBO.     \n\n[1] Prometheus: Taming sample and communication complexities in constrained decentralized stochastic bilevel learning. ICML 2023 \n[2] Anarchic Federated Learning ICML 2022"
                },
                "questions": {
                    "value": "I understand that AFBO offers flexibility by allows clients to engage in the updating in an asynchronous manner.  Is it possible that under such a setting, the AFBO algorithm may converge to a solution that is different from other algorithms ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2013/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831200914,
            "cdate": 1698831200914,
            "tmdate": 1699636132689,
            "mdate": 1699636132689,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dMoRYnLFkZ",
                "forum": "CF6gfZSCVg",
                "replyto": "8frXGZNRyU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments.\n\nW1: The experiment results are limited. Authors are recommended to compare the performance of the proposed algorithm with algorithms such as [1]. In addition, the idea itself is similar to [2]. Authors are therefore recommended to elaborate more on the difference between this work and existing ones in the literature. In addition, in all figures, it is seen that the performance of AFBO is only slightly better than ADBO.\n\n[1] Prometheus: Taming sample and communication complexities in constrained decentralized stochastic bilevel learning. ICML 2023  [2] Anarchic Federated Learning ICML 2022\n\nA1:  Based on your comment, in the revised paper, we have added more experiment results in Section B.3, Section B.4, Section B.5, and Section B.6. of the supplement material, discussed as follows. We conduct experiments on the convergence of different algorithms under different inner and outer loop delays, for comparison of AFBO and AFL, and for the impacts of asynchronous delays, etc. For the inner loop asynchronous delays experiments, the results show that AFBO performs best among all algorithms. In addition, it shows that all algorithms expect AFBO to suffer an obvious convergence degeneration as there exists an inner loop asynchronous delay. Among them, synchronous algorithms (e.g., SDBO, FedNest, and Prometheus) degenerate much more than AFBO and ADBO. Moreover, we find that convergence degeneration is much more obvious in the non-IID MINIST dataset than in the IID MINIST dataset. This is because the bias of using the most recent update of a client is much larger in the non-IID MINIST dataset case than in the IID MINIST dataset case. Intuitively, a reusing of past gradients inducts a new bias of SGD, as some of the SGD gradients use more than others. Finally, it shows that the convergence degeneration in the Covertype dataset is slighter than in the MINIST dataset. The experiments on comparison between AFBO and AFL shows that AFL performs worse on bilevel tasks than AFBO, but they have a similar convergence rate as they both use the most recent updates to update the global model and achieve a linear speed-up. The experiment on the impact of asynchronous delays on AFBO shows that the test accuracy becomes smaller as the asynchronous delay increases, which matches our theoretical results. In addition, there is little convergence degradation due to the asynchronous delay, which proves the effectiveness and robustness of AFBO. Moreover, it shows that the convergence degradation under the non-IID MINIST dataset is slightly higher than under the IID MINIST dataset. This is because the contributions of all clients are equal in the IID dataset setting which means the straggler has low effects on the test accuracy, but some clients owning unique datasets under non-IID dataset settings with higher asynchronous delay can cause a larger convergence degradation due to straggler effects.\n\nThere are some major differences of this paper compared to [2], discussed as follows (we have added some discussion on the differences compard to [2] in Section 2.1 of the revised paper, marked in blue). While [2] considered conventional single-level learning, the AFBO algorithm of this paper focuses on a bilevel learning problem, which is very different and much more complicated than the single-level learning problem. Specifically, bilevel optimization needs to estimate the hyper-gradient $\\nabla \\Phi$. In the distributed setting, this is more challenging than for the single-level learning, since we have $\\frac1M\\sum_{i=1}^M\\frac{\\partial F_i(x,y)}{\\partial y}\\frac{\\partial y}{\\partial x}\\neq\\frac{\\partial F(x,y)}{\\partial y}\\frac{\\partial y}{\\partial x}$, so that $\\nabla \\Phi \\neq \\frac1n\\sum_{i=1}^N \\nabla \\Phi_i$. As a result, AFBO needs to estimate $\\nabla \\Phi$ using a distributed estimator, given by $H_i(x^t,y^{t+1})=\\triangledown_x f_i(x^t,y^{t+1};\\phi_i^t)-\\triangledown^2_{xy}g_i(x^t,y^{t+1};\\rho_i^t)\\times\\left[\\frac{N}{l_{g,1}}\\prod_{l=1}^{N_i}(I-\\frac1{l_{g,1}}\\triangledown_{yy}^2g_i(x^t,y^{t+1};\\zeta_i^t))\\right]\\times\\triangledown_yf_i(x^t,y^{t+1};\\xi_i^t)$. Then we can aggregate $H_i(x^t,y^{t+1})$ to estimate $\\nabla \\Phi$. Therefore, the AFBO algorithm involves two loops: an inner loop that updates the lower-level variable $y^{t,k}$, and an outer loop that updates the upper-level variable $x^t$. This is a very different algorithm structure compared to the AFL algorithm in [2] which only involves one loop. Moreover, the AFBO algorithm allows for maximum freedom for clients by taking into account various anarchic settings: partial participation and asynchronous participation in both the inner and outer loops, and also different local iteration numbers in the inner loop."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607968934,
                "cdate": 1700607968934,
                "tmdate": 1700607968934,
                "mdate": 1700607968934,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u5M6zuauhw",
                "forum": "CF6gfZSCVg",
                "replyto": "8frXGZNRyU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer: We have added responses to your comments. Could you take a look and let us know if you have any further comment? Thanks."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658871515,
                "cdate": 1700658871515,
                "tmdate": 1700658871515,
                "mdate": 1700658871515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ic2Zu5WNns",
                "forum": "CF6gfZSCVg",
                "replyto": "8frXGZNRyU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer: We have added responses to your comments. Could you take a look and let us know if you have any further comment? Thanks."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736137852,
                "cdate": 1700736137852,
                "tmdate": 1700736137852,
                "mdate": 1700736137852,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T5hGZmzsQV",
            "forum": "CF6gfZSCVg",
            "replyto": "CF6gfZSCVg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2013/Reviewer_PRRM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2013/Reviewer_PRRM"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied federated bilevel optimization under the asynchronous setting. However, there are some fatal errors in convergence analysis."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The problem investigated is important. \n\nThe writing is good."
                },
                "weaknesses": {
                    "value": "1. This paper used impractical assumptions. In particular, it assumes the gradient of $g$ is upper bounded and $g$ is strongly convex.  A strongly convex quadratic function does not satisfy those two assumptions simultaneously. \n\n2. There are some fatal errors. In particular, this paper denotes\n$\\bar{H}\\left(x^t, y^{t+1}\\right):=\\mathbb{E}\\left[\\frac{1}{m} \\sum_{i \\in \\mathcal{M}} \\bar{H}_i\\left(x^{t-\\tau_i^t}, y^{t-\\tau_i^t+1}\\right)\\right]$. Then, in convergence analysis, the authors directly use $x^{t}$ rather than $x^{t-\\tau_i^t}$, e.g., the second equation when proving lemma 1. THis is totally wrong. \n\n3. For an asynchronous algorithm, how the communication latency affects the convergence rate should be discussed."
                },
                "questions": {
                    "value": "1. This paper used impractical assumptions. In particular, it assumes the gradient of $g$ is upper bounded and $g$ is strongly convex.  A strongly convex quadratic function does not satisfy those two assumptions simultaneously. \n\n2. There are some fatal errors. In particular, this paper denotes\n$\\bar{H}\\left(x^t, y^{t+1}\\right):=\\mathbb{E}\\left[\\frac{1}{m} \\sum_{i \\in \\mathcal{M}} \\bar{H}_i\\left(x^{t-\\tau_i^t}, y^{t-\\tau_i^t+1}\\right)\\right]$. Then, in convergence analysis, the authors directly use $x^{t}$ rather than $x^{t-\\tau_i^t}$, e.g., the second equation when proving lemma 1. THis is totally wrong. \n\n3. For an asynchronous algorithm, how the communication latency affects the convergence rate should be discussed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2013/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2013/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2013/Reviewer_PRRM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2013/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698893426651,
            "cdate": 1698893426651,
            "tmdate": 1699640386175,
            "mdate": 1699640386175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fz3lhgLhdW",
                "forum": "CF6gfZSCVg",
                "replyto": "T5hGZmzsQV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments! \n\nQ1: This paper used impractical assumptions. In particular, it assumes the gradient of g is upper bounded and g is strongly convex. A strongly convex quadratic function does not satisfy those two assumptions simultaneously.\n\nA1: First, we do not assume that the gradient of the lower-level local objective function $g_i$ is upper bounded. We only assume (in Assumption 4) that the variances of the 1st order and 2nd order stochastic gradients of function $g_i$ is bounded (i.e., $E_{\\zeta}\\left[ \\left\\|\\triangledown g_i(z; \\xi) - \\triangledown g_i(z)\\right\\|^2\\right]\\le  \\sigma^2_{g,1}$, $E_{\\zeta}\\left[ \\left\\|\\triangledown^2 g_i(z; \\xi) - \\triangledown^2 g_i(z)\\right\\|^2\\right]\\le  \\sigma^2_{g,2}$). This is a common assumption used in existing works on bilevel optimization (BO) including federated bilevel optimization (FBO), such as [1]-[5], [7]-[10]. For clarity, we have renamed Assumption 4 as \u201cBounded variance of local stochastic gradient\u201d. \n\nAnother assumption (Assumption 5) we make for function $g_i$ is that the variance between local gradients and the global gradient is bounded (i.e., $\\mathbb{E} \\left [\\left\\|\\triangledown g_i(z) - \\triangledown g(z)\\right\\|^2 \\right] \\le \\sigma_g^2$). This is also a widely used assumption in existing works on BO including FBO, such as [1]-[4], [7]-[9]. \n\nIn addition, in Lemma 6 we show that the second-order moment of the hyper-gradient $H_i$ is upper bounded, i.e., $\\mathbb{E}\\left[\\left\\|H_i(x^t,y^{t+1}) \\right\\|^2|F^t\\right] \\le \\hat{D}_f$. Note that this is not an assumption, but it is a result derived from Assumption 4. This is a widely used result in existing works on BO including FBO, such as [2]-[5], [7]-[10].\n\nIndeed, you are correct that for an arbitrary strongly convex function $g$ such as a quadratic function, its gradient norm cannot be upper bounded. However, our paper does not assume that the gradient norm of the low-level objective function $g$ is bounded, and thus it does not contradict the assumption that $g$ is strongly convex.\n\nMoreover, a few recent works, such as [6], consider a non-strongly-convex lower-level objective function $g$ (which can be non-convex or weakly-convex). In this setting, it is challenging to find the optimal solution of the lower-level optimization problem (there may exist multiple optimal solutions). Therefore, the convergence analysis in these works usually cannot achieve a reasonable performance guarantee. We will explore AFBO for the non-strongly-convex setting in our future work.\n\n[1] \"Anarchic Federated Learning\", ICML'22\n\n[2] \"Achieving Linear Speedup in Non-IID Federated Bilevel Learning\", ICML'23\n\n[3] \"A Single-Timescale Method for Stochastic Bilevel Optimization\", AISTATS'22\n\n[4] \"Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems\", NeurIPS'21.\n\n[5] \"SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning\", NeurIPS'23.\n\n[6] \"Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond\", NeurIPS'21.\n\n[7] \"Communication-Efficient Federated Bilevel Optimization with Global and Local Lower Level Problems\", NeurIPS'23.\n\n[8] \"On the Convergence of Momentum-Based Algorithms for Federated Bilevel Optimization Problems\", arxiv.\n\n[9] \"Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms\", NeurIPS'23.\n\n[10] \"Communication-Efficient Federated Hypergradient Computation via Aggregated Iterative Differentiation\", ICML'23.\n\n\nQ2: There are some fatal errors. In particular, this paper denotes $\\overline{H}(x^t,y^{t+1}) := \\mathbb{E}[\\frac1m\\sum_{i \\in \\mathcal{M}}\\overline{H}_i(x^{t-\\tau_i^t}, y^{t-\\tau_i^t+1})$. Then, in convergence analysis, the authors directly use $x^t$ rather than $x^{t-\\tau_i^t}$, e.g., the second equation when proving lemma 1. This is totally wrong.\n\nA2: The formula does not mean that $x^t=x^{t-\\tau_i^t}$. Here $x^t$ and $x^{t-\\tau_i^t}$ are the global models of $x$ in outer rounds $t$ and $t-\\tau_i^t$, respectively, where $\\tau_i^t$ is the asynchronous delay of client $i$ in the outer round $t$. In general, we have $x^t\\neq x^{t-\\tau_i^t}$, since the server updates the\nglobal model $x^t$ in each outer round. Therefore, in the second equation of Lemma 1's proof, we should use $x^t$ rather than $x^{t-\\tau_i^t}$. To clarify the meaning, we have revised the formula as $\\overline{H}^t := \\mathbb{E}[\\frac1m\\sum_{i\\in\\mathcal{M}}\\overline{H}_i(x^{t-\\tau_i^t}, y^{t-\\rho_i^t+1})]$. We are sorry for this confusion."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607409210,
                "cdate": 1700607409210,
                "tmdate": 1700705035129,
                "mdate": 1700705035129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OpKk6Um6xT",
                "forum": "CF6gfZSCVg",
                "replyto": "T5hGZmzsQV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer: We have added responses to your comments. Could you take a look and let us know if you have any further comment? Thanks."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658830353,
                "cdate": 1700658830353,
                "tmdate": 1700658830353,
                "mdate": 1700658830353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4EAiV1XOLe",
                "forum": "CF6gfZSCVg",
                "replyto": "T5hGZmzsQV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2013/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer: We have added responses to your comments. Could you take a look and let us know if you have any further comment? Thanks."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2013/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736096374,
                "cdate": 1700736096374,
                "tmdate": 1700736096374,
                "mdate": 1700736096374,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]