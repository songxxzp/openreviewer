[
    {
        "title": "MoAT: Multi-Modal Augmented Time Series Forecasting"
    },
    {
        "review": {
            "id": "scYQdxMqhi",
            "forum": "uRXxnoqDHH",
            "replyto": "uRXxnoqDHH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8825/Reviewer_roN3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8825/Reviewer_roN3"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces MoAT (Multi-Modal Augmented Time Series Forecasting), an approach that leverages multimodal data, particularly text, to enhance time series forecasting by addressing data scarcity. In MoAT, text information is embedded into hidden vectors using a pretrained language model and aggregated into patches similar to time series patches. These patched time series and text data are then fed into a multi-modal augmented encoder that combines sample-wise and feature-wise augmentation methods to enrich multimodal representation learning. A joint trend-seasonal decomposition process is employed to capture underlying patterns in the data. The paper pairs all four representations (feature or sample, trend or season) of the two modalities (time series and text) into 16 combinations to make the final prediction. Extensive experiments conducted on real-world datasets demonstrate MoAT's effectiveness compared to previous state-of-the-art methods for single-modal time series forecasting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The concept of using multimodal data to tackle data scarcity and enhance time series forecasting is innovative and holds significant promise.\n- The datasets collected and soon to be released in this paper will contribute positively to the community.\n- The multi-modal augmented encoder, combining sample-wise and feature-wise augmentation, is an interesting and reasonable approach."
                },
                "weaknesses": {
                    "value": "- It is not appropriate to directly transfer methods used for processing time series to text data:\n   1. As a single value at a specific timestamp provides little information, PatchTST and Crossformer patch time series to form informative tokens. Text data already contains a wealth of information, not to mention that there are multiple texts at each time step, so using patching is unreasonable here.\n   2. The decomposition of text data is unclear, particularly the definitions of \"trend\" and \"season\" for text. Equation (6) shows that the so-called trend-seasonal decomposition is just the same attention pooling with different sets of parameters, raising doubts about its ability to capture trend and seasonal dynamics as claimed.\n- Datasets and baselines used in experiments are not so propoer:\n  1. Table 3 shows that the largest dataset, Bitcoin\uff0c contains only 741 * 4 = 2,964 scalars\uff0c while the smallest contains less than 1,000. This raises concerns about the suitability of training complex neural networks with such limited data.\n  2. The main experiment focuses on an input8-output1 setting, with both input and output series being very short. While most selected baselines are for longer term forecasting, i.e. they perform at least input96-output96 task. So the comparison is not so fair."
                },
                "questions": {
                    "value": "1. It is advisable to conduct experiments to validate the necessity of using patching for text data.\n2. Could you clarify the meaning of \"trend\" and \"season\" in the context of text data? Additionally, please elaborate on how the pooling in Equation (6), without further constraints, extracts trend and season.\n3. Given the small dataset size and short input/output lengths, it is recommend to add traditional and straightforward models as baselines, such as: 1)repeat last: just repeating the last timestamp's value $x_{L}$ as prediction; 2)Vector autoregressive moving average; 3)DeepAR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8825/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8825/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8825/Reviewer_roN3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697697528297,
            "cdate": 1697697528297,
            "tmdate": 1699637109945,
            "mdate": 1699637109945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VY2y1Ewmlk",
                "forum": "uRXxnoqDHH",
                "replyto": "scYQdxMqhi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer roN3: Thank you for the review"
                    },
                    "comment": {
                        "value": "We appreciate your thoughtful comments, and we have made an effort to address them below."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363168878,
                "cdate": 1700363168878,
                "tmdate": 1700363168878,
                "mdate": 1700363168878,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WoWrJQ57mE",
            "forum": "uRXxnoqDHH",
            "replyto": "uRXxnoqDHH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8825/Reviewer_kpLg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8825/Reviewer_kpLg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an interesting method for enhancing time-series forecasting by incorporating textual data. It applies both feature-wise and sample-wise augmentation techniques and integrates information about trend and seasonality to improve prediction accuracy. The authors have also contributed to the research community by publishing a new multimodal time-series and textual dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The study introduces a new multimodal forecasting framework that integrates sample/feature-wise augmentation, cross-modal fusion, and seasonal-trend decomposition.\n2. A new multimodal time-series and text dataset is presented as a contribution to the field.\n3. The efficacy of the proposed approach is validated through experiments on six multimodal datasets."
                },
                "weaknesses": {
                    "value": "1. It requires further clarification why the proposed textual decomposition components map to the trend and seasonality aspects of the time-series data.\n2. The paper focuses on short-term forecasting, with the horizons in the experiments and even in the Appendix being relatively short when compared to existing benchmarks that typically extend from 96 to 712 timesteps.\n3. The datasets primarily feature monthly or weekly sampling intervals. This raises concerns about the applicability of the approach to datasets with higher frequency sampling, such as hourly, where acquiring corresponding textual information could be challenging.\n4. In Table 2, the MoAT_time model, even without textual data, appears to outperform many baseline models. The specific factors contributing to this enhanced performance are not adequately explained.\n5. The results in Table 2 suggest that dual augmentation does not significantly enhance performance, questioning its effectiveness in the proposed framework."
                },
                "questions": {
                    "value": "See Weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8825/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8825/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8825/Reviewer_kpLg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698358716327,
            "cdate": 1698358716327,
            "tmdate": 1699637109829,
            "mdate": 1699637109829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yyjgi1w1JA",
                "forum": "uRXxnoqDHH",
                "replyto": "WoWrJQ57mE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer kpLg: Thank you for the review"
                    },
                    "comment": {
                        "value": "We appreciate your thoughtful comments. Here, we have attempted to respond to your concerns as outlined below."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363039161,
                "cdate": 1700363039161,
                "tmdate": 1700363039161,
                "mdate": 1700363039161,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tcG39ZCsCx",
            "forum": "uRXxnoqDHH",
            "replyto": "uRXxnoqDHH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8825/Reviewer_Moxw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8825/Reviewer_Moxw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose MoAT, multi-modal augmented general time series forecasting model that leverages multi-modal cross fusion and prediction synthesis"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- originality: new application of multimodal time series for general time series forecasting\n- quality: extensive experiments with good results, many ablations\n- clarity: well-written paper, good structure\n- significance: time series forecasting and foundation models are timely and relevant"
                },
                "weaknesses": {
                    "value": "- Lack of experiments assessing whether the model performs well with scarce data, which is painted as the main motivation of MoAT. Furthermore, figure 5c does not seem to corroborate the story that MoAT performs significantly better than other methods with data scarcity (hard to say without variance). MoAT still seems to derive its main performance improvements from increasing the train ratio.\n\n- Lack of details in the caption of the T-SNE decomposition between time series and texts.\n\n- The remarks about information uniqueness of cross-modal vs unimodal representations are not backed up, no reason for their contained information to be unique.\n\n- Not obvious that the text data trend-seasonal decomposition actually decomposes into trend and seasonal data, it seems like you just use two sets of attention parameters. How do you actually get these to attend to either trend or seasonal information in the texts? This just seems like it introduces more parameters into the model.\n\n- In fact, there is no comparison of model sizes and various scaling parameters for different methods. If you don't normalize, how do you know your performance increases aren't simply due to scaling up model size?\n\n- Unclear empirical design for hyperparameter tuning. Why default at hidden dim of 64? What does if mean dropout =0.2 \"if needed\"? Why is the search for optimal learning rates and decay across two values each? If you're limited by compute or have a lot of hyperparameters, random search could be better than grid search.\n\n- Formatting needs more consistency (e.g. \"Fig.\" vs \"Figure\", figure 5 before figure 4, etc.)"
                },
                "questions": {
                    "value": "- how is text data decomposed into trend and seasonal components?\n\n- What does \"(non-)overlapping patches\" mean? Clearly patches are overlapping if they share S values.\n\n- Why are the texts unordered at each timestep? Is this a feature of the dataset used, or a design choice to ignore some of the granularity of the timestep?\n\n- Why channel-specific parameters? Channel independence is a strong assumption.\n\n- Is the forecasting in Figure 5 a autoregressive? What are you providing as inputs at each time step? What are the document inputs when autoregressively predicting? The visualization in figure 5 a is so zoomed out as to be uninformative (hard to tell the difference between the methods).\n\n- the motivation for offline prediction synthesis discusses improved parameter efficiency, referencing Liang et al., 2022, but parameter efficiency is not discussed in the rest of the paper, nor when comparing methods? Furthermore, why is modularity desirable in this setting?\n\n- Any intuition as to why MoAT_{time} is the best performer on the Fuel dataset?\n\n- The ablations in table 2 suggest that the augmentations are not really helpful, considering the relative improvement compared to MoAT_{time} and MoAT_{text}. MoAT_{sample} seems pretty performant itself, being the best on Metal and Bitcoin, and second best on Fuel and Covid.\n\n- Ridge regression includes a loss penalty. What weight did you choose for this?\n\n- Why are the prediction lengths so short?\n\n\nOverall, I'm giving this a 5 before discussion, as the storyline does not seem to align with the experiments. I am happy to amend my score following discussions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770326113,
            "cdate": 1698770326113,
            "tmdate": 1699637109709,
            "mdate": 1699637109709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0wsNQBIntC",
                "forum": "uRXxnoqDHH",
                "replyto": "tcG39ZCsCx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer Moxw: Thank you for the reviews"
                    },
                    "comment": {
                        "value": "We are grateful for the time and effort you have invested in offering constructive feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362645043,
                "cdate": 1700362645043,
                "tmdate": 1700362645043,
                "mdate": 1700362645043,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZK67KcAsM4",
                "forum": "uRXxnoqDHH",
                "replyto": "tcG39ZCsCx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the weakness and questions #1"
                    },
                    "comment": {
                        "value": "**C1. Lack of experiments assessing whether the model performs well with scarce data.**\n\nWe would like to highlight that it is common in real-world scenarios to encounter time series datasets with a limited number of data samples or lengths, like the datasets collected for this study. Thus, our experimental results demonstrate the effectiveness of MoAT, in scenarios where datasets exhibit scarce time series information.\n\n**C2. Lack of details in the caption of the T-SNE decomposition between time series and texts.**\n\nWe apologize for the lack of clarity in the caption for Figure 4. The colors blue and green in the figure represent the trend and seasonal representations, respectively. In the figure, \u201cO\u201ds depict the four representations acquired through the encoder, while \u201cX\u201ds indicate the averaged value of these four representations. A visual observation reveals that utilizing the four representations as individual data samples, instead of merging them, expands the representation space, which potentially contributes to more accurate and robust time series forecasting. \n\n**C3. The remarks about the information uniqueness of cross-modal vs. uni-modal representations are not backed up, no reason for their contained information to be unique.**\n\nWe intended to convey that cross-modal and uni-modal representations are distinct from each other and may encompass different semantics, contributing to providing complementary information. An example is shown in Figure 1 (c) where time series and text present distinct information. We will refine this statement in the paper to avoid misunderstanding.\n\n**C4. Not obvious that the text data trend-seasonal decomposition actually decomposes into trend and seasonal data.**\n\nWe acknowledge the reviewer's comment and modified the attention pooling function for texts by using time series patches as query vectors during the attention computation for text aggregation. From the modification, texts are aggregated differently based on the seasonal and trend time sereis patches. For more details, please refer to the common response above.\n\n**C5. There is no comparison of model sizes and various scaling parameters for different methods.**\n\nWe conducted experiments on MoAT using various dimensions and observed that it is not significantly affected by the number of parameters of the model. We will include more detailed results to support this finding. \n\n**C6. What does (non-)overlapping patches mean? Clearly patches are overlapping if they share S values.**\n\nAs the reviewer commented, patches are considered non-overlapping when S equals zero. We have phrased it as \u201c(non-)overlapping\u201d to highlight the flexibility in selecting S, which can be any non-negative integer smaller than the patch length. \n\n**C7. Why are the texts unordered at each timestep?**\n\nAt every timestep, there could exist multiple texts, which can indeed be temporally ordered, as noted by the reviewer. In our current approach, we focused on the temporal granularity of the time series and did not account for the specific ordering of texts within each timestep. Nevertheless, we find the reviewer\u2019s suggestion to be an insightful perspective and acknowledge its potential direction for future work.\n\n**C8. Why channel-specific parameters? Channel independence is a strong assumption.**\n\nWe adopted channel-specific parameters for attention pooling in text aggregation due to the varying emphasis different channels may place on distinct texts. For example, when predicting the stock prices of various companies, each representing a different channel in time series data, the relevance and importance of texts could differ significantly. Thus, channel-specific parameters enable tailored aggregation methods, ensuring that diverse texts are aggregated differently based on their relevance to each specific channel\u2019s prediction task. Regarding channel independence, a recent ICLR 2023 paper, PatchTST, demonstrated the powerfulness of channel-independence over channel-dependence. We followed their claims in our framework."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362893748,
                "cdate": 1700362893748,
                "tmdate": 1700362918004,
                "mdate": 1700362918004,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZwaSqIt9LR",
            "forum": "uRXxnoqDHH",
            "replyto": "uRXxnoqDHH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8825/Reviewer_uJCG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8825/Reviewer_uJCG"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on multi-modal time series forecasting, particularly the text data augmented time series. It includes three main components, i.e., patch-wise embedding, multi-modal augmented encoder, and a trend-seasonal decomposition. \nThe experimental evaluation is through several financial datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper focuses on an interesting applied problem, i.e., exploring the integration of text data into time series to enhance forecasting.\nThis problem is not new and widely studied in quantitative finance, data mining, etc. \n\n2.  The experimental evaluation is conducted on several real financial price data across different markets. The authors augment the time series by collecting real news from news data providers. These datasets, if open-sourced, would be very helpful for the community."
                },
                "weaknesses": {
                    "value": "1. This paper is mostly applied and combines several existing techniques, e.g., patch-wise embedding, and pattern decomposition.\nThe authors are expected to better position this work by clarifying the technical novelty, contribution, or new insights. \n\n2. The evaluation is mostly on financial datasets. But for finance, the error metric MSE is not the main interest, since in the real world the prediction is to serve downstream tasks, e.g., portfolio construction, risk management, etc, and practically MSE does not directly translate to the improvement for downstream tasks. It would be better to show the prediction by the proposed method can facilitate an example downstream task. e.g., portfolio construction is commonly used."
                },
                "questions": {
                    "value": "1. On page 5, the part \"multi-modal augmented encoder\" is essentially a combination of time series and text modalities, and what does the \"augmented\" refer to? \n\n2. On page 6, in the part \"joint trend-seasonal decomposition\", trend-seasonal decomposition is reasonable for time series because of the underlying generative process. But, for text data, especially the embedding of news text, applying trend-seasonal decomposition is not intuitively understandable. News content is highly dependent on real-world happenings where the trend-seasonal decomposition is not necessarily existent. \n\n3. In Table 1,  MSE are mostly low-magnitude values, while the real price of the finance assets of the data used in the experiment differs greatly in magnitudes. Is this due to some data standardization? If so, it is better to report the errors in the original scale of data, because the prediction on the standardized domain would be less useful for downstream tasks in many cases."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8825/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8825/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8825/Reviewer_uJCG"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8825/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699293611932,
            "cdate": 1699293611932,
            "tmdate": 1700960522811,
            "mdate": 1700960522811,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q0auuGRobo",
                "forum": "uRXxnoqDHH",
                "replyto": "ZwaSqIt9LR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8825/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer uJCG: Thank you for the review."
                    },
                    "comment": {
                        "value": "We appreciate your time and efforts in providing insightful comments. Below, we tried our best to address your concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8825/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362268365,
                "cdate": 1700362268365,
                "tmdate": 1700362268365,
                "mdate": 1700362268365,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]