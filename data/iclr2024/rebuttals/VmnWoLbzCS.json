[
    {
        "title": "LUMOS: Towards Language Agents that are Unified, Modular, and Open Source"
    },
    {
        "review": {
            "id": "2RlTm8a3no",
            "forum": "VmnWoLbzCS",
            "replyto": "VmnWoLbzCS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8483/Reviewer_xkWX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8483/Reviewer_xkWX"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an overview of the LLM Agent architecture, with: planning, execution and grounding. The method is competitive for math, QA, and WebShop tasks with significantly smaller model size. The proposed framework is generally applicable to tasks where Language Models are used as an agent."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper demonstrated a solid framework for using smaller models as LLM agents.\n2. The presentation of the paper is easy to follow and the figure is straightforward. \n3. I do appreciate the paper does some detailed ablations of the method, making it stronger."
                },
                "weaknesses": {
                    "value": "1. My first question for the authors is: the agent framework has been discussed quite often (there have been some follow-ups since the ReACT paper came out). Although the author claimed they are mostly based on close-source API based models (not the open-source ones), it seems the architecture is quite similar?\n2. Why on math tasks, LUMOS-O significantly outperforms LUMOS-I while in the other two tasks, the results seem to reverse? Some more analysis on the error patterns would be preferred to give some more insights.\n3. Why LUMOS in general outperforms UA-T? Does this mean some tasks jointly fine-tuned together can result in some conflict? Does this imply that we should train multiple smaller models each fine-tuned for a specific task?"
                },
                "questions": {
                    "value": "Please see above for comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810275522,
            "cdate": 1698810275522,
            "tmdate": 1699637059187,
            "mdate": 1699637059187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vDExMQGtJ3",
                "forum": "VmnWoLbzCS",
                "replyto": "2RlTm8a3no",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xkWX"
                    },
                    "comment": {
                        "value": "Dear Reviewer xkWX,\n\nThank you for your thoughtful review. We appreciate your note that we contribute a solid framework to help even smaller models achieve success as language agents. Below we answer your questions:\n\n---\n\n### **1. Similar architecture to existing agent framework**\n\nRather than merely presenting a plan-ground-execute modular architecture, our research is more inclined towards exploring two key areas: 1) developing suitable formulations to effectively train a superior open-source language agent, and 2) studying how to construct training annotations that are in line with our proposed formulations for planning and grounding modules.\n\nRegarding the training formulations, very few previous studies on agent fine-tuning studies which formulations might enhance agent training. Many established agent frameworks (like ReAct and ReWOO) depend solely on a single model for both planning and grounding tasks. Whereas, we are unsure about the real efficacy of using a single model for learning both planning and grounding skills. \n\nMoreover, there's a possibility that simple chain-of-thought fine-tuning might suffice for complex tasks such as math and complex QA tasks [1]. Open-source LLMs such as LLAMA-2 have already learned a substantial amount of basic math and general knowledge during their pre-training phases. Through chain-of-thought fine-tuning, these models could potentially acquire planning skills fast, and utilize their stored knowledge without any external math and QA tools. As indicated in Section 4.3, we uncover the performance of various open-source agent training formulations. This could serve as **a practical guide for developing more advanced open-source agents** in the future.\n\n[1] Distilling Reasoning Capabilities into Smaller Language Models. Shridhar et al., 2023. ACL Findings 2023.\n\n---\n\n### **2. Analysis on the error patterns of Lumos-O and Lumos-I on math tasks**\n\nBy observing the cases where Lumos-I makes mistakes, we find that Lumos-I is slightly worse than Lumos-O to terminate the planning. A typical example is shown as follows. There\u2019s a math problem: \u201cJames decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?\u201d Lumos-I will only solve part of the problem by merely calculating 3*60=180, while Lumos-O could generate the perfect solution. Another example is \u201cKylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\u201d Lumos-I will repeatedly generate subgoals \u201cCalculate the cost of the third/fourth/fifth/sixth \u2026 glass.\u201d \n\nAnother possible reason why Lumos-I is not better than Lumos-O lies in the ineffectiveness of the intermediate reasoning subgoals formulated by Lumos-I for the generation of subsequent subgoals. Consider the aforementioned mathematical problem such as, \u201cJames decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week?\u201d Even if the agent calculated how many times James sprints a week, which is 3*3=9, the mere number 9 does not affect the next subgoal generation, which could be \u201cCalculate the total meters James runs a week\u201d, without any mentioning of the previous calculation results.\n\n---\n\n### **3. Discussion about the comparison between Lumos and UA-T**\n\nUA-T represents a training approach that teaches a single LM to simultaneously handle both planning and grounding tasks. However, due to the substantial differences between the goals of planning and grounding, training a single model on both skills might impede the development of each skill individually.\n\nMoreover, incorporating grounding instructions and processes into the planning stages significantly lengthens the model's input data. When generating the last subgoal, the input size for UA-T is 359.5 words, 2.63 times longer than the original input dimensions used for the Lumos planning module. When generating a new subgoal, this increase in input length could lead to the model partially losing track of earlier planning contexts. Consequently, this might adversely affect the accuracy of subsequent subgoal generation.\n\n---\n\n### **Thank you!**\n\nWe're grateful for your thoughtful questions and helpful advice. Please feel free to reach out if you have any additional queries. If you feel that our response adequately addresses your concerns, would you like to consider raising your rating score for our paper? Thank you for your consideration!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513610560,
                "cdate": 1700513610560,
                "tmdate": 1700513610560,
                "mdate": 1700513610560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mTtk6Ue3b1",
            "forum": "VmnWoLbzCS",
            "replyto": "VmnWoLbzCS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8483/Reviewer_7VK9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8483/Reviewer_7VK9"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new framework to train LLMs for certain tasks such as answering questions related to maths, textual comprehension and outputting actions for a website (click). The new framework has three parts (\u201cmodules\u201d):\n\n- \u201cplanning\u201d, which converts a prompt to (simpler, but still human language-like) queries (\u201csubgoals\u201d), e.g. \u201cQuery the living period of Lowell Sherman\u201d\n\n- \u201cgrounding\u201d, which converts the subgoals to \u201cactions\u201d: function-call type queries, e.g. \u201cKnowledgeQuery(Lowell Sherman)\u201d\n\n- \u201cexecution\u201d, which executes those actions\n\nFor both planning and the grounding phases (\u201cmodules\u201d) GPT-4 is used to generate annotated examples for the given task at hand to train LLAMA-7B with.\n\nTwo versions are suggested: LUMOS-O (which goes through the above three parts/modules sequentially) and LUMOS-I (which iterates through each subgoal until the execution of that subgoal, the result of which is then used for the next planning the next subgoal).The framework is applied to the open source LLAMA 7B LLM and claims superior or competitive performance on larger LLMs without or with techniques to improve their performance such as Chain of Thought prompting, SelfInstruct, ReWOO-Planner-7B (an improvement of React) and for this claim provides experiment details on these tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a new framework/pipeline to achieve better results with prompting LLMs in a nascent field, which could be seen as original.\n\nQuality: The methodology of the paper is well-documented, the experiment section contains results on relevant datasets.\n\nClarity: the paper follows the ICLR formatting style, images are mostly clearly captioned, there is an attempt to place the work in the (very recent) literature. Large sections of the paper are easy to understand.\n\nIn terms of significance, using their particular setting a 7B-parameter open-source LLM outperforms larger LLMs queried by previous techniques."
                },
                "weaknesses": {
                    "value": "Having said the above in terms of strengths, in my opinion the paper includes a fair bit of weaknesses.\n\nThe paper\u2019s claim that LUMOS outperforms or is competitive compared to larger LLMs is not well justified by evidence. A list of issues are:\n\n- F1-scores are not reported, and to the best of my knowledge that is the primary metric in this field of AI\n\n- The LLM is fine-tuned for the specific tasks (for subgoal and action generation), whereas the baselines use publicly available APIs at best, hence they are more general\n\n- Unless I am mistaken: on GSM8K, ReWOO achieves 62.4% accuracy (as opposed to the reported ~38) in Xu et al. 2023 and is significantly better than LUMOS (50.5%)\n\n- I could be wrong, but to me it looks like ReWOO trained a 7B model, they only used GPT-3.5 for the QA tool. I also cannot find the claimed LLAMA-7B results in Xu et al. 2023. Also, ReWOO-7B uses GPT-3.5-turbo as a QA tool, achieves 66.6% accuracy on StrategyQA, which is better than all LUMOS agents that used GPT-3.5-turbo as QA. \n\n- Mind2Web baselines were not fine-tuned at all, hence comparing them to LUMOS is not that fair.\n\nI also have concerns about the significance of this contribution. React and Self-Instruct defined ways to improve the performance of LLM agents with little to no fine-tuning, only publicly available API calls. The concept that large neural networks can be fine-tuned with additional task-specific training data for better performance on those tasks is fairly well-known in the community. \n\nIn terms of clarity, the paper has a lot of typos, odd sentences and style issues that made it much more difficult to understand than it should have been. Although they ultimately did not affect the score of this paper, they were close to it. I will list some of the found issues below:\n\nIn the abstract: showd -> showed\n\nIntroduction\n\n2nd paragraph: However, Lie et al\u2026 citations ideally should not be used as nouns.\n\nBeginning of 3rd paragraph: There should be an introductory sentence to ask the question. Then instead of \u201cto this end\u201d\u201d write to answer this question\u201d\n\nIn Figure 1, the prompt should be included in the Figure.\n\nEnd of 3rd paragraph: What are environment states? They are never defined.\n\n4th paragraph: \u201c[...] language agents to acquire these skills [...]\u201d what skills?\n\n2.2\n\n2nd paragraph: \u201c[...] part of grounding module\u2019s input\u201d -> the missing before grounding\n\n2.2 and 2.3 general comment: the difference between LUMOS-I and LUMOS-O should be demonstrated with the same example for easier comprehension (e.g. with the Obama example)\n\nFigure 3a right image:\n\n<|user|> Should we keep planning?\n<|assistant|> No I will keep planning.\nShouldn\u2019t the user say \u201cShould we stop planning?\u201d\n\n3.2\n\n3rd paragraph\n\nThe subsequent conversation is constructed in the similar patterns. -> remove the\n\n\u201cWe assume ourselves as user\u201d -> as the user\n\n\u201cTell the execution results\u201d -> provide the execution results\n\n\u201cTo planning module\u201d -> to the planning module\n\n4th paragraph\n\n\u201c[...] play a user role to provide the task\u201d -> provide to what? Do you mean get/acquire/obtain?\n\n\u201cIn the rest conversations\u201d -> remaining, of rest of the\n\n4.4\n\n\u201cAchieves 5-10 average reward than\u201d -> odd sentence\n\n\u201cThan using Self-Instruct method\u201d -> missing the\n\n\u201cAnnotation is\n\n beneficial than\u201d -> more beneficial\n\nRelated Work\n\n\u201cWe notice that directly generating annotation with for training planning and grounding modules may introduce large number of errors\u201d -> \u201cWe notice that directly generating annotations for the training, planning and grounding modules may introduce a large number of errors\u201d\n\n\u201cLLMs transform the gold reasoning steps\u201d -> I don\u2019t understand what you mean, but I am fairly certain not what is written here"
                },
                "questions": {
                    "value": "The Self-Instruct paper lists a lot more related work. How are those related to the work presented in this paper?\n\nIn the Introduction this sentence is written: \u201cTogether, our proposed framework and unified model provides valuable resource and direction for advancing open-source interactive language agents.\u201d (resource -> resources)\n\nBut it is never elaborated upon and you do not mention potential future work in the conclusion either.\nWhat future work do you envision after this paper? How could the community successfully build on top of this new framework that was proposed?\n\nWhat are the limitations of the work you suggested?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812462145,
            "cdate": 1698812462145,
            "tmdate": 1699637059080,
            "mdate": 1699637059080,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iM51Md9EuD",
                "forum": "VmnWoLbzCS",
                "replyto": "mTtk6Ue3b1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7VK9 (1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7VK9,\n\nThank you for engaging with our work and for noting the quality of our methodology and experimental setup. Below we answer your questions:\n\n---\n\n### **1. Clarifications about evaluation metrics**\n\nWe followed the evaluation metrics adopted in the well-known agent evaluation papers such as AgentBench: a holistic agent evaluation benchmark [1], ReAct [2], AgentLM [3], FiReAct [4], and ReWOO [5]. F1 is actually not the mainstream metric to evaluate agents on complex interactive tasks. \n\nFor more comprehensive evaluation as the reviewer suggested, we leverage character-level F1 score, one QA evaluation metric used in ReWOO which evaluates the extent of character overlap, to evaluate agent performance on HotpotQA. Here\u2019re the results:\n\n|             | Base Model    | QA Tool       | Character F1 |\n|-------------|---------------|---------------|--------------|\n| GPT-3.5-CoT | gpt-3.5-turbo | gpt-3.5-turbo | 30.8         |\n| ReAct       | gpt-3.5-turbo | gpt-3.5-turbo | 39.6         |\n| ReWoo       | gpt-3.5-turbo | gpt-3.5-turbo | 40.1         |\n| Lumos-I     | LLAMA-2-7B    | gpt-3.5-turbo | 38.7         |\n\nVia the new metric character-level F1, we find that our LLAMA-2-7B-based Lumos still achieves competitive performance against gpt-3.5-turbo-based agents such as ReAct and ReWOO. It also surpasses GPT-3.5-CoT by a large margin.\n\n[1] AgentBench: Evaluating LLMs as Agents. Liu et al., 2023. Arxiv 2308.03688.\n\n[2] ReAct: Synergizing Reasoning and Acting in Language Models. Yao et al., 2023. ICLR 2023.\n\n[3] AgentTuning: Enabling Generalized Agent Abilities for LLMs. Zeng et al., 2023. Arxiv 2310.12823.\n\n[4] FireAct: Toward Language Agent Fine-tuning. Chen et al., 2023. Arxiv 2309.05653.\n\n[5] ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. Xu et al., 2023. Arxiv 2305.18323.\n\n---\n\n### **2. Reason why we don\u2019t study publicly available APIs**\n\nGPT-based APIs can be excessively costly, especially for tasks involving lengthy contexts like web agent tasks, where encoding HTMLs is a necessity. Also, these APIs are seldom deterministic, making agent reproduction challenging. Moreover, the closed-source nature of these LLMs hinders a comprehensive understanding of their architectures and internal behaviors. \n\nMoreover, commercial use constraints may force companies to develop their proprietary agents based on open-source models. Such reliance on closed-source LLM-based agents is not conducive to the growth of the research and industry community, suggesting a shift towards the use of open-source LLMs.\n\nLastly, instead of simply showing the effect of fine-tuning, we discuss which training formulations could further improve the agent fine-tuning performance and which data construction methods could contribute to better training annotations for planning and grounding modules. Lumos is one of the first works that study the key elements in agent training process in detail.\n\n---\n\n### **3. Clarification about the adopted results in Table 1**\n\n- \u201cOn GSM8K, ReWOO achieves 62.4% accuracy\u201d: According to the first sentence of Section 3.2.1 in [ReWOO paper](https://arxiv.org/pdf/2305.18323.pdf), \u201cTable 2 shows the main evaluation results on public benchmarks and curated dataset based on gpt-3.5-turbo\u201d, 62.4% indicates gpt-3.5-turbo results.\n- \u201cCannot find the claimed LLAMA-7B results in Xu et al. 2023\u201d: Figure 6 in the ReWOO paper demonstrates their LLAMA-7B-based Planner results. \n- \u201cAchieves 66.6% accuracy on StrategyQA\u201d: The 66.6% accuracy is also from Table 2 in the ReWOO paper, which is about gpt-3.5-turbo result.\n\n---\n\n### **4. Comparisons on Mind2Web**\n\nIn a recent paper [AgentTuning](https://arxiv.org/abs/2310.12823) published (10/19) after ICLR submission deadline, they fine-tune LLAMA-2-70B with Mind2Web training data. 7B-size Lumos (27.7% step success rate) still exceeds AgentLM-70B (13.5% step success rate) by a large margin. We will append the new comparison results and discussion in Section 4.2."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513412578,
                "cdate": 1700513412578,
                "tmdate": 1700513412578,
                "mdate": 1700513412578,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XIUwBgYzs1",
                "forum": "VmnWoLbzCS",
                "replyto": "mTtk6Ue3b1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7VK9 (2)"
                    },
                    "comment": {
                        "value": "### **5. Typo fixing**\n\nThanks for pointing out the typos! We will address these editorial comments, proofread, and improve the writing of the paper in the revision.\n\n---\n\n### **6. Other questions**\n\n**6.1. Main commonality between the related works in Self-Instruct paper and the ones in ours**: \n\nSelf-Instruct mainly discusses the method of improving LLMs with their self-generated instruction tuning data. One of the main contributions of Self-Instruct is the automatic approach to distilling GPT-3 for generating instruction-tuning data for further training. As Lumos constructs the training annotations with the aid from GPT-4, one of the common parts of both paper\u2019s related works involve \u201clanguage models for data generation\u201d which is the second paragraph in Section 5 of Self-Instruct paper. \n\nAlso, the common usage of Self-Instruct in recent literatures such as Alpaca [6] and Symbolic Chain-of-Thought Distillation [7] is to synthesize annotations from scratch for training small LMs. Since the goal of Lumos is to equip small LMs with agent ability, one intuitive solution is distilling agent tuning data from LLMs for training. Hence, another common part of the related works is about \u201cknowledge distillation\u201d which is the fifth paragraph in the Self-Instruct paper. \n\n[6] Stanford Alpaca: An Instruction-following LLaMA model. Taori et al., 2023. https://crfm.stanford.edu/2023/03/13/alpaca.html\n\n[7] Symbolic Chain-of-Thought Distillation: Small Models Can Also\" Think\" Step-by-Step. Li et al., 2023. ACL 2023.\n\n**6.2. Main difference between the related works in Self-Instruct paper and the ones in ours**: \n\nSelf-Instruct focuses on relatively simple instruction following settings, e.g., studying whether LLMs can write an email to a professor. However, Lumos focuses on evaluating if language agents can accomplish the complex interactive tasks that require multi-step reasoning with interaction with external environments. \n\nThus, we discuss language agents which are not mentioned in Self-Instruct. Besides, the instructions generated by Self-Instruct are usually much simpler than our studied complex interactive tasks. Relying simply on Self-Instruct to create complex interactive task annotations can lead to a significant number of errors and diminish the overall quality of the annotations. \n\nIn Lumos \u201cImproving Capacity of Smaller Models\u201d paragraph, we further discuss the motivation of transferring the existing annotated gold reasoning steps into the formats aligning Lumos formulations, instead of directly applying the Self-Instruct method.\n\n**6.3. Impact and future works**\n\nAs we mentioned in the previous rebuttal part, closed-source API-based agents could suffer from issues regarding affordability, reproducibility, and intellectual property. Pushing forward the development of building strong open-source language agents would be a more proper solution that considers the capabilities and alleviates the aforementioned issues. \n\nLumos is one of the very first works comprehensively studying which training formulations and data sources should be incorporated to facilitate the performance on complex interactive tasks. In terms of the long-term future works, we hope that based on Lumos training formulations and high-quality training annotations, the future language agent research could shed light on designing more effective, efficient and controllable methods to obtain powerful open-source language agents. \n\nWith regard to short-term goals, we plan to train the next-generation Lumos with self-reflection ability. In other words, the next-generation Lumos should realize the planning mistakes they have made and automatically correct the previous planned subgoals until the mistake disappears. Also, we aim to harmonize the language agents with reinforcement learning that makes the planning more adaptive to the external environment and further optimizes the planning modules.\n\n**6.4. Limitations**\n\nCurrently, Lumos is unable to replan and reground once the grounded actions cannot be executed. A more flexible agent framework with self-correction ability is desired. Besides, when testing the agents based on Lumos-Iterative (Lumos-I) formulation, we sometimes encounter a situation where the agent cannot terminate the planning at the proper time. A controllable stopping mechanism for the iterative agent formulation is also expected.\n\n---\n\n### **Thank you!**\n\nThank you for your insightful questions and valuable advice. If you have any more questions, we're more than willing to continue the discussion. If you find that our response addresses your concerns, could you please consider increasing your rating score for our paper? Your consideration is highly appreciated."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513485919,
                "cdate": 1700513485919,
                "tmdate": 1700513485919,
                "mdate": 1700513485919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Uc9DjGLHCp",
                "forum": "VmnWoLbzCS",
                "replyto": "vRTsGgvEaJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Reviewer_7VK9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Reviewer_7VK9"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your detailed response. Unfortunately these last two days of the rebuttal period have been very busy for me. I will thoroughly read and evaluate your response and update my score according to it later this week."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713871905,
                "cdate": 1700713871905,
                "tmdate": 1700713871905,
                "mdate": 1700713871905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "idWgn5AfyT",
            "forum": "VmnWoLbzCS",
            "replyto": "VmnWoLbzCS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8483/Reviewer_9Mw2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8483/Reviewer_9Mw2"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents LUMOS, a language agent framework built for open source LLMs with unified formats and modular design. LUMOS divide the framework into separate modules for planning, grounding and execution. The obtain high-quality annotations for training the modules, the authors leverage LLMs to convert ground truth intermediate reasoning steps in existing benchmarks into a unified format. LUMOS demonstrates competitive or superior performance compared to SOTA systems on a variety of interactions including web agent, math reasoning and complex QA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studies an important problem of developing language agents, and a unified framework and format is much needed in the field.\n2. The overall description of the method is clear and easy to follow.\n3. Experiment covers both regular fine-tuning setting, and generalization to unseen task."
                },
                "weaknesses": {
                    "value": "1. My main concern is regarding the claim on LUMOS achieving superior performance than other LLM-based agents:\n    1. LUMOS is trained on top of LLAMA-2, while some of the baselines are based on LLAMA. For example in Table 1, according to the latest results from AgentBench, updated vicuna-13B v1.5 based on LLAMA-2 now has 41.7 on WebShop, even outperform LUMOS. To make a fair comparison, I would recommend to keep it consistent across models, reporting both LUMOS and baselines with LLAMA, or update the baseline results to the version using LLAMA-2. This should also be made more clear in the paper.\n    2. In most experiments, LUMOS is tuned with data from downstream tasks, while if I understand correctly other LLM baselines are tested under few-shot or in-context learning settings. If this is correct, it should be made more clear in the paper, and the comparison seem a little unfair.\n2. With the baseline systems mostly evaluated under few-shot, I feel it is important to understand the efficiency of LUMOS and how well it generalizes. It is great that the authors have the generalization on WebShop, but I feel more emphasis on this direction, with additional few-shot experiments would be much better.\n3. A modular design, in particular divide the agent into planning, grounding and execution has been studied in various previous works as well. This limits the novelty of the proposed method. Also there is some missing reference to relevant work, e.g., Saycan: Grounding Language in Robotic Affordances\n4. Many of the baseline results are directly taken from results reported in other papers. While I understand that running experiments with LLMs are costly, this causes some in-consistency in the baselines that are compared in different datasets. And the comparison might also get affected by the implementation details of different papers."
                },
                "questions": {
                    "value": "1. Is it true that most other baseline LLM agents are applied under few-shot / in-context learning setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8483/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8483/Reviewer_9Mw2",
                        "ICLR.cc/2024/Conference/Submission8483/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824952059,
            "cdate": 1698824952059,
            "tmdate": 1700619951933,
            "mdate": 1700619951933,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pzG0Ldescy",
                "forum": "VmnWoLbzCS",
                "replyto": "idWgn5AfyT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Mw2 (1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9Mw2,\n\nWe thank the reviewer for engaging with our work, and recognizing the importance of the unified framework we contribute in this paper. \n\n**_We would like to address your concerns in detail below._**\n\n---\n\n### **1. Better baseline model comparison**\n\nPrior to delving into our recent comparative experiments, it's essential to provide context and a timeline concerning the agent evaluation and modeling studies. \n\nAs of the ICLR submission deadline (9/28), there existed only ONE public version of AgentBench [1], which was released on 8/7. All the results referenced in Lumos are derived from this initial version. The subsequent version, referred to in the review, was released later on 10/25. Therefore, we are not able to cite results from Vicuna-13B v1.5 based on LLAMA-2 from AgentBench Version 2 [2] prior to the ICLR deadline. We will update it in the following versions.\n\n[1] (Version 1) AgentBench: Evaluating LLMs as Agents. Liu et al., 2023. https://arxiv.org/abs/2308.03688v1\n\n[2] (Version 2) AgentBench: Evaluating LLMs as Agents. Liu et al., 2023. https://arxiv.org/abs/2308.03688v2\n\nAdditionally, prior to 9/28, the dominant method in language agent research up to that date was few-shot in-context learning, exemplified by models like ReAct, ReWOO, and Reflexion. Very few studies had explored the fine-tuning of smaller LMs to compete with the performance of GPT-series or other significant open language agents.\n\nConsequently, we did not specifically emphasize in-context learning settings at that time. After the submission deadline, novel approaches to language agent fine-tuning began to emerge, including FiReAct (10/9) and AgentLM (10/19), marking the beginning of research into enhancing smaller LMs with language agent capabilities through fine-tuning.\n\nIn terms of the comparison with LLAMA-2-based agents, we note that some LLAMA-2-based agent performance is already shown in Table 1 and we have already provided some comparison. To further address the reviewer\u2019s concern, we compare 1) more baselines of which the base models belong to LLAMA-2 series, and 2) the concurrent two language agent fine-tuning methods, FiReAct [3] and AgentLM [4].\n\n[3] FireAct: Toward Language Agent Fine-tuning. Chen et al., 2023. Arxiv 2309.05653.\n\n[4] AgentTuning: Enabling Generalized Agent Abilities for LLMs. Zeng et al., 2023. Arxiv 2310.12823.\n\n---\n\n###  **2. Lumos vs. other LLAMA-2-based models**\n\n- For math tasks, some baselines listed in Table 1(b) are based on LLAMA-2. Described in Table 3 and 4 of [MAmmoTH](https://arxiv.org/abs/2309.05653) [5], the base model of Orca-Platypus-13B baseline is LLAMA-2. It is trained with many math CoT data constructed upon FLAN Collection. 7B-size Lumos-O outperforms Orca-Platypus-13B on GSM8K and SVAMP 12.1% and 8.6% accuracy. \n- For web agent task, Mind2Web, shown in Table 3 of the latest [AgentBench v2](https://arxiv.org/abs/2308.03688), Vicuna-13B v1.5 only gets a 12% step success rate, ~15% underperforming Lumos. LLAMA-2-70B-chat also lags behind Lumos by a large margin.\n- For WebShop, also shown in Table 3 of the latest [AgentBench v2](https://arxiv.org/abs/2308.03688), Lumos outperforms LLAMA-2-13B-chat 14.5 average reward.\n\n[5] MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. Yue et al., 2023. Arxiv 2309.05653.\n\n---\n\n### **3. Lumos vs. FiReAct and AgentLM**\n\n- For math tasks, Lumos achieves 18.1% accuracy improvements over AgentLM-13B on GSM8K (based on Table 4 of AgentBench).\n- For complex QA tasks, we use Exact Match (EM) as the evaluation metric to keep it consistent with FiReAct and AgentLM. Despite FiReAct being fine-tuned with in-domain HotpotQA annotations, Lumos, without any fine-tuning on HotpotQA annotations, still presents an impressive improvement. In particular, Lumos surpasses the 7B-scale FiReAct by 3.2% EM. Lumos also has a 7.1% EM improvement over AgentLM.\n- For web agent tasks, Mind2Web, Lumos surpasses AgentLM-70B fine-tuned on Mind2Web 14.1% step success rate.\n\nOverall, we show that Lumos is still competitive with many LLAMA-2-based agent baselines and has superior performance to the concurrent works on most of our evaluated tasks. We will update our paper with these new results."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512658321,
                "cdate": 1700512658321,
                "tmdate": 1700513639625,
                "mdate": 1700513639625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F20O6ZC9C0",
                "forum": "VmnWoLbzCS",
                "replyto": "idWgn5AfyT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Mw2 (2)"
                    },
                    "comment": {
                        "value": "### **4. More few-shot generalization experiments**\n\nWe extend the few-shot generalization experiments on WebShop to 13B-scale. Specifically, We adopt LLAMA-2-13B as the fundamental model for both the planning and grounding modules. We show the results as follows:\n\n|                    | Reward |\n|--------------------|--------|\n| Vicuna-v1.3-33B    | 23.9   |\n| Vicuna-v1.5-13B    | 41.7   |\n| Openchat-v3.2-13B  | 46.9   |\n| Claude-instant     | 49.7   |\n| Lumos-I-math       | 45.7   |\n| Lumos-I-complex-QA | 47.3   |\n| Lumos-I-web-agent  | 46.2   |\n| Lumos-I            | 50.3   |\n\nExperimental results suggest that 1) Lumos still has better generalizability than the other LLAMA-2-13B-based models even API-agent agents under the same few-shot in-context learning settings.; 2) Lumos-I trained with unified annotations also outperforms the ones trained with domain-specific annotations Lumos-I-math/complex-QA/web-agent on unseen tasks.\n\n---\n\n### **5. Novelty on model architecture**\n\nInstead of simply presenting a plan-ground-execute modular architecture, we focus more on studying 1) how to **train** a better open-source language agent, and 2) how to prepare high-quality planning and grounding **annotations** that align with the proposed formulations.\n\nIn terms of the training formulations, since there are very few prior works exploring agent fine-tuning, it is unclear which formulations would be more beneficial to train a better agent. In fact, many well-known agent frameworks (e.g., ReAct and ReWOO) fully rely on a single model to perform planning and grounding. However, we are unsure about whether training a single model is the most appropriate approach to managing planning and grounding skills. \n\nAdditionally, it is also possible that chain-of-thought fine-tuning is already good to tackle math and complex QA tasks [6]. As shown in Section 4.3, we unveil the effect of possible open-source agent training formulations. It could provide a practical reference to lead the path for training more powerful open-source agents in the future. \n\nRegarding the training annotation construction, one intuitive solution is to simply leverage methods such as Self-Instruct to generate tasks, plans and grounded actions by LLMs from scratch. Whereas, these methods are not suitable for generating high-quality annotations for complex interactive tasks, as even GPT-4 performs badly on the complex interactive tasks we study, such as Mind2Web. Relying on these methods to create complex interactive task annotations can lead to a significant number of errors and diminish the overall quality of the annotations. To this end, it is necessary to think of potential novel methods that can create **large-scale and high-quality** annotations to train planning and grounding modules. \n\nFinally, we propose a prompting-based conversion method that utilizes the ground-truth reasoning steps in existing benchmarks to generate the subgoals and actions and transfer to conversational formats that fit the proposed formulations. Appendix E demonstrates that by controlling the training annotation size to be the same, our annotations can still help get better performance than the ones produced by the Self-Instruct method and passed by rigorous execution sanity checking.\n\nOverall, Lumos involves the discussion beyond merely presenting an architecture. We care more about what training strategies can construct a better open-source agent. It covers the comprehensive study on the choices of the key elements in the agent training process including training formulation and annotation perspectives. \n\n[6] Distilling Reasoning Capabilities into Smaller Language Models. Shridhar et al., 2023. ACL Findings 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512762871,
                "cdate": 1700512762871,
                "tmdate": 1700512762871,
                "mdate": 1700512762871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WitSUOnPbO",
                "forum": "VmnWoLbzCS",
                "replyto": "idWgn5AfyT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Mw2 (3)"
                    },
                    "comment": {
                        "value": "### **6. Clarification about baseline model comparison**\n\nTo address the reviewer\u2019s concern about baseline result comparison in Table 1, we conduct the prompting experiments for each task type\u2019s strongest baselines of which the calling prompts are publicly released in AgentBench and ReWOO [7] papers. Especially for WebShop, we adopt the prompts described in AgentBench paper and rewrite their in-context examples to be consistent with ours. The results are shown as follows:\n\n|                 | Eval. Dataset | Perf. | Lumos-I Perf. |\n|-----------------|---------------|-------|---------------|\n| Alpaca-7B       | GSM8K         | 38.3  | 46.7          |\n| ReWOO           | StrategyQA    | 65.7  | 65.7          |\n| ReWOO           | HotpotQA      | 44.2  | 45.9          |\n| Vicuna-v1.1-13B | WebShop       | 15.7  | 39.8          |\n| GPT-4           | Mind2Web      | 26.8  | 27.7          |\n\nWe show that Lumos-I still exceeds the compared baselines in Table 1 after the reproduction. \n\n[7] ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. Xu et al., 2023. Arxiv 2305.18323.\n\n\u2014\n\n### **7. Other questions**\n\n\u201cIs it true that most other baseline LLM agents are applied under few-shot / in-context learning setting?\u201d - Yes, we will add this information in the latter version. Recently, two fine-tuned language agents, FiReAct (10/9) and AgentTuning (10/19) were published after the ICLR submission deadline (9/28). As discussed in the rebuttal subsection **Lumos vs. FiReAct and AgentLM**,  Lumos outperforms them on most of our evaluation tasks. \n\n\u2014\n\n### **Thank you!**\n\u200b\nWe appreciate your excellent questions and suggestions. Please feel free to reach out if you have additional questions. If you find that our response addresses your concerns, would you kindly consider raising your rating score for our paper? We greatly appreciate your consideration."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512812113,
                "cdate": 1700512812113,
                "tmdate": 1700512812113,
                "mdate": 1700512812113,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "neu9VZVx3g",
                "forum": "VmnWoLbzCS",
                "replyto": "F20O6ZC9C0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Reviewer_9Mw2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Reviewer_9Mw2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response and additional experiments. They addressed some of my concerns on fair comparison with baselines. Again, my concern is not on that the paper does not achieve SOTA, or missing comparison with concurrent works, but rather a fair and consistent comparison across models, if most baselines are using LLAMA, then a LLAMA based model is better suited for comparison, with LLAMA-2 based models to further demonstrating that the proposed method can generalize.\nI will raise my score to weak reject. My remaining concern is on generalization of the method. While I appreciate the efforts on consolidating existing supervised datasets and the study on fine-tuning, **large-scale and high-quality** data is not easy to get in many cases, especially for agent applications that involve complex interaction and environment. The experiment on generalization to Webshop touches on this but seems a bit insufficient with only one dataset and even with all the fine-tuning seems many chat model are still competitive with the proposed method, somehow hint that general instruction tuning might be suffice."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619924974,
                "cdate": 1700619924974,
                "tmdate": 1700619924974,
                "mdate": 1700619924974,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wlgOVTGscr",
            "forum": "VmnWoLbzCS",
            "replyto": "VmnWoLbzCS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8483/Reviewer_UN8U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8483/Reviewer_UN8U"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes LUMOS, Language agents with Unified formats, Modular design, and Open Source LLMs to solve complex tasks with planning, grounding, and execution modules fine-tuned from LLAMA-7B on high-quality annotations collected by leveraging LLMs to convert ground truth reasoning steps in existing benchmarks into a unified format. LUMOS achieves competitive performance with agents of larger size and outperforms GPT-4/3.5-based agents on complex QA and web agent tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed modular framework is well-motivated.\n\n- The converted dataset can contribute to training better small open models for complex tasks.\n\n- The results show the proposed method is effective and promising for its generalizability on unseen tasks.\n\n- The paper is well-written and presented clearly."
                },
                "weaknesses": {
                    "value": "- Why is LUMOS-O better than LUMOS-I on Math benchmarks?\n\n- Some discussion on the performance-efficiency tradeoff between LUMOS-O and LUMOS-I would provide further insights.\n\n- Figure 3 (a) in the Final planning module annotation \"**No**, I will keep planning. Subgoal 2: Query the living\nperiod of Jonathan Kaplan.\" Is that a typo?\n\n- Some related work on modular language agents framework for complex tasks\n  - _Cognitive Architectures for Language Agents_\n  - _Building Cooperative Embodied Agents Modularly with Large Language Models_"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825433196,
            "cdate": 1698825433196,
            "tmdate": 1699637058815,
            "mdate": 1699637058815,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qbd7lMQL9C",
                "forum": "VmnWoLbzCS",
                "replyto": "wlgOVTGscr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UN8U"
                    },
                    "comment": {
                        "value": "Dear Reviewer UN8U,\n\nThank you for your thoughtful review! We are glad you found the modular framework to be well-motivated and noticed the potential of our dataset to help train better, smaller models. Thank you also for noting the effectiveness and generalizability of our method. Below we answer your questions:\n\n---\n\n### **1. Reason why Lumos-O is better than Lumos-I on math benchmarks**\n\nA potential reason why Lumos-I is not better than Lumos-O on math tasks is because the calculation results of the intermediate reasoning subgoals planned by Lumos-I are not useful for generating the next subgoal. Suppose we are solving a math problem like \u201cJames decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week?\u201d Even if the agent calculates how many times James sprints, which is 3*3=9, the mere number 9 does not affect the next subgoal generation, which could be \u201cCalculate the total meters James runs a week\u201d.\n\n---\n\n### **2. Performance-efficiency tradeoff between Lumos-O and Lumos-I**\n\nWe compute the inference time for Lumos-O and Lumos-I across 100 instances on GSM8K and HotpotQA, respectively. The experiments are run with 2 NVIDIA A6000 48GB GPUs with inference batch size 16. We find that Lumos-O is much more efficient than Lumos-I on both datasets. \n\n|         | GSM8K | HotpotQA |\n|---------|-------|----------|\n| Lumos-O | 102 s | 556 s    |\n| Lumos-I | 851 s | 1007 s   |\n\nLumos-O completes its inference in a single round, whereas Lumos-I necessitates multiple rounds of inference until it autonomously concludes its planning. The iterative planning and grounding in Lumos-I contribute to a higher time cost for solving individual instances. Nevertheless, Lumos-I is better at generating an appropriate subgoal based on the current external environment compared to Lumos-O.\n\nFor example, when tackling a complex question \u201cWhat government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\u201d, Lumos-I is able to first identify the woman who portrayed Corliss Archer in Kiss and Tell, Shirley Temple, then asks the government position she held. \n\nHowever, though Lumos-O is able to first ask who the woman is, without the interaction with external knowledge such as Wikipedia pages, it will generate a subgoal which inquires the government position held by `Madonna`, a random entity totally irrelevant with the original question. Hence, Lumos-O is a more efficient solution, but not as effective as Lumos-I due to the lack of the adaptability to external environments.\n\n---\n\n### **3. Typo in final planning module annotation**\n\nSorry about this typo. The previous turn in this conversation actually asks \u201cShould we stop planning?\u201d, instead of \u201cShould we keep planning?\u201d We will fix the typo in the camera-ready version.\n\n---\n\n### **Thank you!**\n\u200b\nThank you very much for your great questions and suggestions. Please let us know if you have any further questions, as we are happy to continue the discussion. If you find that our response addresses your concerns, would you kindly consider defending acceptance for our paper or even raising your rating score? We greatly appreciate your consideration."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512355232,
                "cdate": 1700512355232,
                "tmdate": 1700512860142,
                "mdate": 1700512860142,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i1RZl06rFV",
                "forum": "VmnWoLbzCS",
                "replyto": "Qbd7lMQL9C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8483/Reviewer_UN8U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8483/Reviewer_UN8U"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. My concern is addressed."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710465162,
                "cdate": 1700710465162,
                "tmdate": 1700710465162,
                "mdate": 1700710465162,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]