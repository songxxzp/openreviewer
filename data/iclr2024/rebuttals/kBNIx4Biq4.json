[
    {
        "title": "Lifting Architectural Constraints of Injective Flows"
    },
    {
        "review": {
            "id": "trLKEErw3s",
            "forum": "kBNIx4Biq4",
            "replyto": "kBNIx4Biq4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5851/Reviewer_RfcP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5851/Reviewer_RfcP"
            ],
            "content": {
                "summary": {
                    "value": "- The authors propose a new kind of injective flow (a normalizing flow with lower-dimensional latent space than the data space).\n- Unlike some previous attempts, they use unconstrained encoders and decoders, and introduce a new estimator for the gradient of the change-of-variables term in the log likelihood.\n- The authors also discuss a training issue for injective flows that was previously pointed out by Brehmer & Cranmer (2020).\n- They demonstrate the method by training generative models on tabular data and CelebA, reporting metrics of generative quality like FID."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The main idea (the new estimator of the gradient) is wonderfully simple, sensible, and efficient.\n2. The paper is clearly structured and well-written."
                },
                "weaknesses": {
                    "value": "1. Injective flows are academically interesting, but do not have a very clear use case, especially if they do not have a tractable density (see questions below).\n2. The discussion of joint manifold and likelihood training is not novel (see Brehmer & Cranmer, 2020), which the authors are open about. The proposed solution leaves questions open (see below).\n3. I am not yet convinced by the experimental evaluation. Given the quality of the samples in Figure 1, I am surprised by the claim that the method outperforms various VAE methods (see questions below).\n4. Overall, the paper's contributions are quite thin."
                },
                "questions": {
                    "value": "1. What's the main use case for this injective flow? In what situations do you expect benefits from the manifold structure of this generative model compared to, say, diffusion models or VAEs?\n2. Is the density (not its gradient) of the model tractable? That would extend use cases substantially.\n3. I don't understand the \"fix\" of the pathological behaviour pointed out in Sec. 4.2. Could you expand the discussion of why it would work? Is it guaranteed to work? Consider the toy problem that Brehmer & Cranmer (2020) use to illustrate the same problem (Fig 4 in the arXiv version, 2003.13913). Here the encoder $f(x)$ is linear, thus $f'(\\hat{x}) = f'(x)$ , and the \"fix\" does not change anything.\n4. Do you have an explanation for why the problem discussed in Sec. 4.2 does not affect the experiments?\n5. In the experimental evaluation, are the models converged? How do the results change if the models (in particular the baselines) are trained for longer? I find it hard to believe that none of the VAE methods are able to produce higher-quality samples than what we see in Fig. 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5851/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5851/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5851/Reviewer_RfcP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697716335259,
            "cdate": 1697716335259,
            "tmdate": 1700487197698,
            "mdate": 1700487197698,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FmYtF19UgB",
                "forum": "kBNIx4Biq4",
                "replyto": "trLKEErw3s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback. Let us answer to your questions, and we hope that they also soften the mentioned weaknesses:\n\n1. FIF are fast, perform well and are simple to set up:\n    1. FIF vs diffusion model: Diffusion models sample slow, which makes them expensive in inference.\n    2. FIF vs VAE: VAEs are limited by (i) their variational family, and (ii) by learning a distribution in the embedding space, which is limiting if the data actually lies on a manifold (cf. manifold overfitting). While (i) can be addressed by more expressive likelihoods/posteriors, this comes with additional hyperparameters. Our model only has the architecture and beta as hyperparameters.\n    3. FIF vs GAN: FIF is trained via maximum likelihood, so it guarantees against mode collapse.\n    4. FIF vs latent flow/diffusion/\u2026, e.g. Stable Diffusion: FIF trains manifold and distribution jointly, while post-hoc learning the latent distribution is a two-step procedure.\n2. The density is tractable by computing the Jacobian of the transformation using as many autograd calls as there are latent dimensions. While this may not be computationally feasible for very large latent dimensions, we find that for all problems considered it is still fine. However, there are unbiased estimators for the change of variables which require less autograd evaluations [Chen et al. 2019: Residual Flows for Invertible Generative Modeling].\n3. We do not simply repeat the problem mentioned in Brehmer & Cranmer, this seems to be a misunderstanding. Let us give a different perspective on the well-behaved loss in section 4.2. We need to address *two* pathologies, each with a different solution. The first was described by Brehmer & Cranmer and appears already for a linear model, and the second one is new and only applies to nonlinear models. You correctly noted that our fix to the second problem does not help against the first problem. So let us quickly sum up the two pathologies and the fix to each.\n    1. **Degenerate subspaces** (identified by Brehmer & Cranmer): If the data is only supported on a low-dimensional subspace, then there is a projection of the data with $-\\infty$ log-likelihood. This possible projection can be eliminated by adding a tiny bit of noise to the data. In Figure 3, we demonstrate that a high enough reconstruction weight $\\beta$ is sufficient to avoid learning degenerate projections. We also present an analytic result on the linear case in Appendix C.\n    2. **Strong curvature** (new): If the learnt manifold curves strongly, then data on the outside of the curve is concentrated by projecting to the manifold (see Figure 2). This decreases the entropy of $p(\\hat x)$, lowering the lower bound of the manifold log-likelihood, causing divergence. We fix this by computing the encoder Jacobian off-manifold, which punishes spurious curvature (see discussion in appendix B.2). This is also cheaper because it avoids one forward pass through the model.\n4. See Q3: We provide a fix for both pathologies.\n5. We decided to use the Pythae benchmark because it allows a systematic comparison to a large collection of generative autoencoders. However, it puts a compute limit on training by fixing the architecture and the number of training epochs. The purpose of the benchmark is to fairly compare methods, not to achieve SOTA results. This explains the sample quality, see Figure 11 in their paper for samples from competitor models: https://arxiv.org/pdf/2206.08309.pdf\n\nWe hope that this addresses your concerns. We are happy to provide further information."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069645603,
                "cdate": 1700069645603,
                "tmdate": 1700069645603,
                "mdate": 1700069645603,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aq8JptEF0e",
                "forum": "kBNIx4Biq4",
                "replyto": "FmYtF19UgB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_RfcP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_RfcP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed responses.\n\n> 2. The density is tractable by computing the Jacobian of the transformation using as many autograd calls as there are latent dimensions.\n\nDo you here need to assumesthat the model actually learns to be injective? Can you verify if this is the case for a given trained model, without knowledge of the data manifold?\n\n> 3. We do not simply repeat the problem mentioned in Brehmer & Cranmer, this seems to be a misunderstanding. [...] We need to address two pathologies, each with a different solution.\n\nAh, thanks for clearing this up, I indeed misunderstood the discussion here. So the additional problem you describe only occurs when the data does does not actually populate a lower-dimensional manifold, but rather extends off-manifold into the ambient space, and an injective flow will never be able to achieve perfect reconstruction \u2013 correct? Then I agree, this is actually a new pathology, and the solution offered by Brehmer & Cranmer does not apply.\n\nHow relevant is this problem, though? I was under the impression that the manifold hypothesis / zero reconstruction error is quite central in motivating this work. In your experiments, what reconstruction errors did you achieve?\n\nI also still think that both pathologies are two instances of the same underlying root cause (optimizing likelihoods after a learnable projection).\n\nThanks again for your response. While I still have some doubts about the empirical performance and motivation of the method, I now see the contribution of this paper more clearly and will update my score to reflect this."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487144685,
                "cdate": 1700487144685,
                "tmdate": 1700487144685,
                "mdate": 1700487144685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nd60TfN803",
                "forum": "kBNIx4Biq4",
                "replyto": "gpR4YTNTNo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_RfcP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_RfcP"
                ],
                "content": {
                    "comment": {
                        "value": "> The additional problem occurs regardless of whether the data is supported on a subspace or extends off-manifold. Consider figure 2. The pathological behavior illustrated on the left could still happen if the data distribution were a Gaussian with zero variance in the x direction, since the concentration effect due to curvature would still overwhelm the reconstruction error.\n\nNow I am confused again. Let's focus on the case where the data exactly populates a $d$-dimensional manifold in the $D$-dimensional data space, and assume that our model is expressive enough that we can exactly fit this manifold with the encoder and decoder (so we will have zero reconstruction error).\n\nAre you saying that in this scenario there is still a pathology that is different from the one pointed out by Brehmer & Cranmer? Can you elaborate a bit more on this?\n\n> In our experiments, this problem seems to be important\n\nThanks for reporting these additional results. To understand what is going on, I would really appreciate seeing some reconstruction errors in addition to these FID-like metrics. Could you report them, please?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639879106,
                "cdate": 1700639879106,
                "tmdate": 1700639879106,
                "mdate": 1700639879106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mcAhnXsIBv",
                "forum": "kBNIx4Biq4",
                "replyto": "B1jH7gctOf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_RfcP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_RfcP"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reco error results. These nicely fit into the story.\n\nHowever, I still don't follow how the issue you are describing (for the case where the data populates a $d$-dim. manifold) is different from the pathology pointed out in Brehmer & Cranmer. Like reviewer vmge, I think this discussion could use some more clarity.\n\nAll in all, in its current form, I don't think this paper is quite up to the NeurIPS standard. But it clearly makes valuable points and I am not strongly against accepting it."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725925164,
                "cdate": 1700725925164,
                "tmdate": 1700725925164,
                "mdate": 1700725925164,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fCiovfW2EX",
            "forum": "kBNIx4Biq4",
            "replyto": "kBNIx4Biq4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5851/Reviewer_SnSS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5851/Reviewer_SnSS"
            ],
            "content": {
                "summary": {
                    "value": "This papers builds upon rectangular flows, a method for end-to-end training of injective normalizing flows. Three modifications are proposed: (a) not restricting the architecture with normalizing flows, since the reconstruction error encourages injectivity in it of itself, (b) a more efficient gradient estimator, thus addressing a main limitation of rectangular flows, and (c) a further modification to the gradient estimator, which changes the gradient itself and improves numerical stability.\n\nOverall, the paper is well-written and I believe it makes a significant methodological contribution to the area of injective normalizing flows. That being said, I also believe that ablations are missing to properly identify the sources of empirical improvement that the authors observed.\n\n------------------------------------------------------------------------------\n11/22 UPDATE\n------------------------------------------------------------------------------\n\nThe authors have adequately addressed the points I raised in my original review and I am thus increasing my score."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper has several strengths:\n\n1. It is well written and easy to follow, and I think the authors did a good job of deciding which material to include in the main manuscript and which details to include in the appendix.\n\n2. It is well motivated, as I agree with the authors that the current injective flow literature uses overly restrictive architectures and/or computationally intensive training procedures.\n\n3. The simple observation that, when the encoder $f$ is the left inverse of the decoder $g$, allows to write Jacobians of $g$ as Jacobians of $f$ is elegant, and does result in clear computational gains.\n\n4. Empirical results are good, showing that the proposed method outperforms other injective flows and generative autoencoders."
                },
                "weaknesses": {
                    "value": "5. In my view, the main weakness of the paper is the lack of ablations. As mentioned, the paper proposes 3 improvements over rectangular flows, and it is unclear how much each of these contributes to the empirical performance of the proposed method. I think table 1 provides a perfect test bed to carry out these ablations: results using the same architecture as rectangular flows should be added to the table, both (a) using the gradient estimator from eq 10, and (b) that from eq 16. Ideally, using eq 10 (and the same architecture as rectangular flows) would simply show a speed up and the same performance compared to rectangular flows, whereas using eq 16 should improve performance but not match the results of FIF with a fully flexible architecture. I would see this as strong empirical evidence backing up the claims in the paper. I will increase my score if these ablations are included.\n\n6. While the authors include a discussion as why $x$ should be used (eq. 16) instead of $\\hat{x}$ (eq. 10), I think there are several relevant points missing from the discussion: (a) why does the pathological behaviour described by the authors not happen in rectangular flows? Is it because the more restrictive architecture implicitly regularizes the curvature? Or is this actually a hidden issue in rectangular flows as well (the above ablation will obviously also help answer this question)?. (b) Since, when $f$ and $g$ are consistent, $f(x)=f(\\hat{x})$, it seems to me like one can attempt to justify both objectives as attempting to maximize log-likelihood subject to perfect reconstructions. In this view, the problem of using $\\hat{x}$ could be seen as an inappropriate way of enforcing the constraint through a penalty term. Could you further discuss? (c) There is also an additional computational benefit to using $x$ instead of $\\hat{x}$, namely one less forward pass is required through the encoder, which I believe should also be mentioned.\n\nFinally, some minor points:\n\n- In the notation paragraph in sec 3, you write $f^{-1} = g$, which I think should be avoided: when $d<D$, $f$ cannot be an invertible function, since you defined its domain as $\\mathbb{R}^D$ (its restriction to a manifold could of course be injective though, I am not saying there's anything fundamentally wrong here, just nitpicking the notation): I think it'd be better to stick to the language of left inverses.\n\n- Missing period at the end of the injective flows paragraph in sec 3.\n\n- Use \\citep instead of \\citet in the first paragraph of appendix E.3."
                },
                "questions": {
                    "value": "7. As you point out in eq 1, injective flows typically have a low-dimensional flow $h$ on the latent space. One could also interpret this architecture as a flexible distribution $p_Z$ on latent space, given by $h$, along with a decoder $w \\circ \\texttt{pad}$; rather than thinking of their composition as the decoder $g$. Throughout the paper you mention making $g$ more expressive, but another interpretation is that you are making $w \\circ \\texttt{pad}$ more expressive, and reducing expressivity on the latent space (instead of a flow, you use a Gaussian or a mixture of Gaussians). Previous research has found benefits of having flexible distributions on latent space (rectangular flows prefer using a flow-based p_Z rather than fixing it as a Gaussian, and other works also recommend using flexible distributions on latent space, e.g. [1, 2]), is there a reason why you do not use more flexible $p_Z$?\n\n[1] Diagnosing and Enhancing VAE Models, Dai & Wipf, ICLR 2019\n\n[2] Diagnosing and Fixing Manifold Overfitting in Deep Generative Models, Loaiza-Ganem et al., 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5851/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5851/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5851/Reviewer_SnSS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683049955,
            "cdate": 1698683049955,
            "tmdate": 1700694400401,
            "mdate": 1700694400401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "runYWqxtYP",
                "forum": "kBNIx4Biq4",
                "replyto": "fCiovfW2EX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough and helpful feedback.\n\n### Ablation comparing estimators\n\nWe are still working on implementing the ablation and will come back with results before the end of the rebuttal.\n\n\n### Why $x$ instead of $\\hat x$?\n\n**No pathological behavior in rectangular flows?**\n\nFrom a theoretic point of view, rectangular flows should also suffer from pathologically strong curvature since their conjugate gradient does evaluate the likelihood term on-manifold.\n\n\n**Using $\\hat x$ is an inappropriate way to enforce consistency**\n\nIt is true that if $f$ and $g$ are consistent then $f(x) = f(\\hat x)$, but this is not true for the Jacobians: $f\u2019(x) \\neq f\u2019(\\hat x)$ in general (see fig. 4 and section B.2 in the appendix for a discussion). Hence substituting $x$ with $\\hat x$ is not just another way of enforcing consistency, it gives a different numerical result even in consistent models, and helps fight against degenerate high-curvature solutions.\n\n**Computational benefit**\n\nYou are correct that there is a computational benefit to using $x$ instead of $\\hat x$ (one less forward pass). We will add a mention of this in section 4.2.\n\n### More flexible p(z)\n\nWe do find it beneficial in practice to have a few residual blocks which work with the latent dimension. This can be interpreted as being the same structure of an injective flow, which has a latent space flow. Hence, you can view our model as having an expressive latent distribution.\n\nWe hope that this addresses your questions and we look forward to further discussion."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069500990,
                "cdate": 1700069500990,
                "tmdate": 1700069500990,
                "mdate": 1700069500990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tmgtqr6b6e",
                "forum": "kBNIx4Biq4",
                "replyto": "runYWqxtYP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_SnSS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_SnSS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply, I am happy with your clarifications and look forward to seeing the ablations."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499260188,
                "cdate": 1700499260188,
                "tmdate": 1700499260188,
                "mdate": 1700499260188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vnW3LT9Gd1",
                "forum": "kBNIx4Biq4",
                "replyto": "fCiovfW2EX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Ablation results:\n\nWe perform the requested ablations to compare the on- and off-manifold variants of our loss using a free-form and an injective flow architecture. The injective flow experiments were performed with both FIF and RF hyperparameters. The results, along with the original results from our submission (top two rows):\n\n| Experiment                               | power         | gas         | hepmass      | miniboone     |\n|------------------------------------------|---------------|-------------|--------------|---------------|\n| RF: injective flow via RF loss with CG   | 0.08 \u00b1 0.02   | 0.11 \u00b1 0.02 | 0.78 \u00b1 0.19  | 1.00 \u00b1 0.05   |\n| FIF: free-form net, off manifold         | 0.04 \u00b1 0.01   | 0.28 \u00b1 0.03 | 0.54 + 0.03  | 0.60 \u00b1 0.02   |\n|------------------------------------------|---------------|-------------|--------------|---------------|\n| free-form net, on manifold               | 19.54 \u00b1 20.81 | 7.48 \u00b1 5.40 | 29.03 \u00b1 5.42 | 77.23 \u00b1 16.55 |\n| injective flow, off manifold, FIF hparams| **0.11 \u00b1 0.06**   | **0.45 \u00b1 0.09** | **1.30 \u00b1 0.14**  | **1.55 \u00b1 0.04**   |\n| injective flow, off manifold, RF hparams | 0.98 \u00b1 0.69   | 6.16 \u00b1 4.20 | 2.02 \u00b1 0.74  | 1.80 \u00b1 0.10   |\n| injective flow, on manifold, FIF hparams | 3.71 \u00b1 2.19   | 0.40 \u00b1 0.22 | **0.71 \u00b1 0.05**  | 3.13 \u00b1 0.42   |\n| injective flow, on manifold, RF hparams  | **0.33 \u00b1 0.22**   | **0.33 \u00b1 0.17** | 0.82 \u00b1 0.07  | **1.84 \u00b1 0.11**   |\n\nNote: bold entries show the best hyperparameters (out of FIF or RF) for each experiment.\n\n## Conclusion:\n\nThe ablation shows that (a) the free-form network is unstable for the on-manifold variant of our loss, and (b) the injective flow is more stable against evaluating our loss on manifold: \nFor the injective architecture, on- and off-manifold settings are both stable, indicating that there is a stabilizing inductive bias of the architecture. Note that they do not perform as well as the original networks, but this is probably due to the lack of tuning hyperparameters.\n\n## Details:\n- FIF hparams means the same training hyperparameters as in our paper\n- RF hparams means the same training hyperparameters (including a warm up in the NLL loss) as detailed in the appendix of the rectangular flows paper, except that a fixed number of epochs (150) was used rather than using early stopping\n- Injective flow means the RNVP-based architecture detailed in the appendix of RFs\n- Hepmass is trained with LR=1e-4 instead of LR=3e-4 when using FIF hparams\n- Injective flow with FIF hparams have 4 instead of 5 runs each"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502902372,
                "cdate": 1700502902372,
                "tmdate": 1700503804783,
                "mdate": 1700503804783,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YJUbSdDd4x",
                "forum": "kBNIx4Biq4",
                "replyto": "vnW3LT9Gd1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_SnSS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_SnSS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for posting these ablations. I do have some follow-up questions/comments:\n\n1. I completely agree that these results show the numerical instabilities that you mentioned on the paper when using free-form architectures. I also believe the point that injective flows are more robust to on/off-manifold evaluation is interesting and should be mentioned somewhere.\n\n2. You mention that injective flows \"do not perform as well as the original networks, but this is probably due to the lack of tuning hyperparameters\". I don't completely understand this, specially for the RF hyperparameter settings. I believe the rows \"injective flow, on manifold, RF hyperparams\" and \"RF: injective flow via RF loss with CG\" should behave very similarly, as they should just be computing the same gradients in different ways (and also using the same hyperparameter). Am I misunderstanding something? Could you comment of the observed differences between these rows?\n\n3. It does not appear that you get consistent improvements by using the off manifold formula when restricting the architecture to injective flows (i.e. the \"injective flow, off manifold, FIF hparams\" row is not consistently better than the \"injective flow, on manifold, FIF hparams\" row; and similarly, the \"injective flow, off manifold, RF hparams\" row is not consistently better than the \"injective flow, on manifold, RF hparams\"). This seems to be in contradiction to the narrative that off manifold is always better, could you comment on this?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511700857,
                "cdate": 1700511700857,
                "tmdate": 1700511700857,
                "mdate": 1700511700857,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i23WEtEmVZ",
                "forum": "kBNIx4Biq4",
                "replyto": "N6iAyCStag",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_SnSS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_SnSS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers. I do not have any additional questions, but I did want to mention that the results of the ablations are quite surprising: it really seems like the proposed improvements over rectangular flows really become effective when using free-form architectures, rather than when using injective flows. While I agree that the best results overall being obtained with free-form architectures highlights the relevance of the proposed improvements, I do think the ablations show that the empirical improvements come from the combination of free-form architectures with the other methodological changes, rather than every single change being an improvement in it of itself, which is the current way the paper reads.\n\nNonetheless I still believe this is a strong paper that improves upon existing injective flows. Thus, as long as the authors promise to include these results and the corresponding discussions in the paper, I will increase my score as I stated in my original review."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587856552,
                "cdate": 1700587856552,
                "tmdate": 1700587856552,
                "mdate": 1700587856552,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4jsCWlokYT",
            "forum": "kBNIx4Biq4",
            "replyto": "kBNIx4Biq4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5851/Reviewer_vmge"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5851/Reviewer_vmge"
            ],
            "content": {
                "summary": {
                    "value": "A new Injective Flow named free-form injective \ufb02ow (FIF) is proposed. FIF is developed based on the rectangular flow (Caterini et al., 2021) and change of variables across dimensions. Different from the rectangular flow, FIF leverages the auto-coding architecture to approximately but efficiently calculate the gradient of maximum likelihood surragate wrt the parameters. The authors also identify pathological behavior in the naive application of maximum likelihood training and propose a fix."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The presented techniques are original, with contributions on removing limitations from prior methods.\n\nThe presented techniques are interesting and potentially valuable."
                },
                "weaknesses": {
                    "value": "The clarity should be significantly improved. For example, many important derivations should be moved to the main manuscript, and important assumptions should be highlighted."
                },
                "questions": {
                    "value": "Without architectural constraints, how to guarantee that $det [g\u2032(z)^T g\u2032(z)] > 0$?\n\nIn the paragraph before Eq. (12), what are the assumptions underlying $f(\\hat x)=f(x)$? Also, why does Eq. (12) hold true? If $p_{data}(x)=\\hat p_{data}(\\hat x)$, then the right-hand side of Eq. (14) is fixed, right?\n\nAfter adopting the modification in Eq. (16), the final objective in Eq. (18) (or its first two terms) ultimately is not identical to the negative maximum likelihood, right? If so, what are the differences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719981410,
            "cdate": 1698719981410,
            "tmdate": 1699636618951,
            "mdate": 1699636618951,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zVsd5Vy9ND",
                "forum": "kBNIx4Biq4",
                "replyto": "4jsCWlokYT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We are happy to provide the additional details:\n\n### How to guarantee a positive determinant?\n\nThe reconstruction loss is the only direct loss on the decoder. Reconstruction loss cannot be minimized unless the decoder is injective. Therefore if the reconstruction loss is well-optimized, the determinant is very likely (although not guaranteed) to be positive.\n\n### Assumption underlying $f(\\hat x) = f(x)$\n\nBy definition, $\\hat x = g(f(x))$. In the section you are referring to (4.2) we are assuming that $f$ and $g$ are pseudoinverses for the purpose of the derivation, then later drop that assumption. Under the assumption, $f(\\hat x) = f(x)$. Sorry for the confusion, we will revise the text to make this assumption more clear.\n\n### Eq. (12)\n\nTo clarify your confusion: it is not the case that $p_\\text{data}(x) = \\hat p_\\text{data}(\\hat x)$. We replace $p_\\text{data}$ with $\\hat p_\\text{data}$ in eq. (12) because the quantity in the expectation is invariant to projection, so we can replace the original data distribution with the projected distribution.\n\n### Relation to maximum likelihood (ML)\n\nYou are right that the final objective is not exactly maximum likelihood. The differences are:\n- We introduce a reconstruction loss which ensures that high-entropy directions are encoded in the latent space (they would be ignored with a ML loss only).\n- We evaluate the encoder Jacobian off-manifold to ensure that the solution does not become degenerate due to excessive curvature (see section 4.2 \u201cTowards a well-behaved loss\u201d).\nGiven that the two modifications make joint maximum likelihood and manifold learning tractable, we argue that it is justified to denote this as maximum likelihood training. Note that maximum likelihood training alone would not lead to sensible solutions when using bottleneck architectures, so our modifications are necessary to learn ML and manifold simultaneously.\n\nWe hope that this answers your questions and we look forward to further discussion."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069440027,
                "cdate": 1700069440027,
                "tmdate": 1700069440027,
                "mdate": 1700069440027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S8EzE1nvIH",
                "forum": "kBNIx4Biq4",
                "replyto": "zVsd5Vy9ND",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_vmge"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_vmge"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply, which addressed some of my concerns. I have three follow-up questions.\n\nI now understand that the derivations in Section 4.2 are based on the assumption that $f$ and $g$ are pseudoinverses for all $x \\sim p_{data}(x)$.\n\n(1) How does that assumption affect Eq. (14)? Is it possible that the NLL or the entropy decreases without a bound, based on that assumption?\n\n(2) How did you guarantee that assumption in the proposed loss in Eq. (18)?\n\nAnother question is associated with the core advantage of a flow -- the exact evaluation of the likelihood.\n\n(3) The proposed FIF gives up that core advantage with Eq. (18), for what? This should be discussed in detail."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663020545,
                "cdate": 1700663020545,
                "tmdate": 1700663020545,
                "mdate": 1700663020545,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OduHdt5XEq",
                "forum": "kBNIx4Biq4",
                "replyto": "0IozK5X5Mx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_vmge"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_vmge"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry, but I am confused again.\n\nThe change of variables across dimensions in Eq. (2) is only valid iff $f\\circ g(z)=z$ and $g\\circ f(x)=x$. \n\nHowever, in Section 4.2, you only assumed $f\\circ g(z)=z$, right? Based on $f\\circ g(z)=z$, you derived Eqs. (12) and (14) and drew the conclusion following Eq. (14), where the RHS could decrease without bound because $g\\circ f(x)=x$ may not be true. Otherwise, if both $f\\circ g(z)=z$ and $g\\circ f(x)=x$ are fulfilled, then $p_{data}(x)=\\hat p_{data}(\\hat x)$.\nSo it's an auto-encoder with $f\\circ g(z)=z$ that is discussed in Section 4.2, right? \n\nOverall, I don't think the discussions here are clear. There are two fundamental problems, one of which is associated with training an injective flow (on a dense data distribution), while the other is related to maximum likelihood learning on a manifold (clearly a pathological learning problem). It seems both problems are mixed up. \n\nCould the authors elaborate on this? Actually, I am quite curious about how the proposed fix in Eq. (18) helps stabilize the maximum-likelihood-like learning on a manifold."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714886086,
                "cdate": 1700714886086,
                "tmdate": 1700714886086,
                "mdate": 1700714886086,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uwuJeZSPLr",
            "forum": "kBNIx4Biq4",
            "replyto": "kBNIx4Biq4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5851/Reviewer_KrZV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5851/Reviewer_KrZV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new technique for training flow-based architectures with manifold structure, entitled the \"Free-form Injective Flow\". This technique is derived by loosening the architectural constraints on previously-proposed injective flows; in particular, the proposed autoencoder is completely unconstrained besides using a pre-specified latent dimensionality $d$. Issues with the loss used for previous injective flows are also identified, and this paper derives a novel loss function to address those issues. This loss is computationally tractable while maintaining stability. Experiments are performed to compare with previous injective flow techniques, and other types of autoencoders."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I'll enumerate the strengths below for ease of reference in discussion. These are not listed in order of importance.\n\n1. The paper is generally very well-written and comes up quite polished. I'll outline below:\n    - The introduction is very clean. The motivation for the method is clearly laid out.\n    - The paper is well-situated amongst the related work.\n    - The background is easy to digest.\n    - The path to the final model in Section 4 is laid out well.\n    - The appendix is quite thorough\n2. The autograd / linear-algebra-type derivations were quite well-done. I have personally always appreciated works that don't just blindly apply basic automatic differentiation techniques and instead think deeper about the problem and the requisite gradient estimators.\n3. I like the use of figures. Figure 2 in particular is quite nice for explaining how the technique improves on previous injective flow-based methods. Figure 3 also clearly demonstrates the trade-off between reconstruction and log-likelihood.\n4. I feel well-convinced that this technique is clearly better than previous injective flow techniques, both in terms of representation power and computational tractability."
                },
                "weaknesses": {
                    "value": "I'll write weaknesses in a list as well. Again this list is not ordered in terms of importance.\n\n1. In the end, this paper could be summarized as simply training autoencoders with a different training loss, with the loss motivated by previous work in injective flows. The novelty and significance of this particular choice of loss over other types of autoencoder losses is not completely clear for a couple of reasons: (i) Table 3 is not convincing, as the best results are still produced by other autoencoders, and (ii) more modern autoencoder architectures are not compared against. Furthermore, this paper does not necessarily maintain all of the benefits of injective flows - mainly, we do not get exact inverses on the projections to the learned manifold. To summarize, I think *some* degree of discussion is warranted on the benefits of using this approach over other generative autoencoders, as the benefits over other injective flows are comparatively very well-documented here. \n2. This paper is missing a dedicated limitations section. This is partially covered by the conclusion, but not completely, and would show some more perspective from the authors considering the weaknesses laid out here.\n3. In section 5.3, it is suggested that Inception Scores are a reliable measure of diversity, although I don't know if that's actually a modern viewpoint. Furthermore, the Inception Scores generally seem just in-line with other methods, or worse at times. I am also confused about why the best Inception Scores are not bolded in Table 3.\n4. It seems like Section 5.2 and 5.3 are out-of-order on how things are defined. For example, the FID acronym is both cited and defined in 5.3, yet referred to in 5.2. Table 2 also requires more of a description -- including what \"IS\" is, and what the two samplers are -- some of which is contained within the caption of Table 3. I would just suggest making the requisite definitions in Section 5.2 first and then using acronyms or reduced descriptions in Section 5.3 as appropriate.\n5. It is discussed twice that traces are performed in the order $f' g'$, and that details are in the Appendix -- however, there is certainly space in the paper to provide a bit more discussion on that. \n6. The paragraph on Page 6 starting with \"Unfortunately\" is not sufficiently convincing: I don't think Fig 2 proves that reconstruction error is insufficient, as you could imagine that reconstruction becomes increasingly more difficult if the entropy becomes negative infinity which should therefore regularize the solution on the left to some extent so that it does not fall on the degenerate, negative-infinite entropy solution.\n7. It is suggested that rectangular flows require $O(d)$ `vjp`s / `jvp`s for convergence, but practically conjugate gradient has exponential convergence and thus much fewer iterations suffice.\n8. It is stated that the surrogate loss is only accurate if $f$ and $g$ are optimal with respect to reconstruction, and then assumed that this is indeed fulfilled by optimizing the reconstruction loss and by the fact that training is stable. However, I don't think this is fully proven:\n    - The reconstruction error is not checked in the paper\n    - One of the other changes to the loss function may be responsible for the training stability\n    - Trade-offs between optimizing the reconstruction error and reconstruction the likelihood contribution may prevent the reconstruction loss from being fully optimized (cf. Figure 3)\n9. There are no samples provided for the generative methods, which suggests that the generation quality may not actually be that good. FID has recently come under more scrutiny as an evaluation metric and so supporting the FID numbers with actual generated samples would be useful."
                },
                "questions": {
                    "value": "1. Why is the training time speedup inconsistent in Table 1? It doesn't seem to scale with dimension in any predictable way.\n2. What is the definition of entropy as in e.g. (13) for a distribution that is supported on a manifold?\n\n**POST-REBUTTAL**\n\nI'll be upgrading my score after discussion with the authors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5851/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5851/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5851/Reviewer_KrZV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787262483,
            "cdate": 1698787262483,
            "tmdate": 1700699830761,
            "mdate": 1700699830761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8fwraxOucx",
                "forum": "kBNIx4Biq4",
                "replyto": "uwuJeZSPLr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive and detailed feedback. Let us address the weaknesses you point out:\n\n1. To clarify the advantages of our model compared to the VAE and its variants: They are limited by (i) their variational family, and (ii) by learning a distribution in the embedding space, which may be limiting if the data actually lies on a manifold (cf. manifold overfitting: Dai and Wipf, 2019 and Loaiza-Ganem et al. 2022). While (i) can be addressed by more expressive likelihoods/posteriors, this comes with additional hyperparameters. Our model only has the architecture and beta as hyperparameters.\n2. Adding a limitations section is a good idea which we will add for the camera ready version. We will address: The Pythae benchmark is limited because it does not scale training to competitive image generation, the FID and IS metrics are limited, and we observed that architectural changes can significantly improve results. Also, while the empirical results are good, theoretically showing whether the final loss actually learns the exact distribution on the manifold is still open; in particular when the reconstruction loss is not at optimality. Finally, the reconstruction weight is left open as a hyperparameter to tune for each new data set.\n3. We included Inception Scores because they are part of the Pythae benchmark. We included the Inception scores because they indicate diversity. We do not bold them since FID is more suited at comparing sample quality to real data.\n4. Thank you for this suggestion. We agree that some definitions are out of order and it makes sense to make all the definitions in section 5.2.\n5. We can add more detail about the $f\u2019 g\u2019$ order in the trace, since this is crucial in order to reduce variance.\n6. To elaborate on why reconstruction loss alone is not sufficient: imagine in fig. 2 the limit where the learned manifold has infinitely high curvature. In this case, all data will be projected to the same point, so the reconstruction error will take on some finite value. In contrast, the NLL will be negative infinity. Hence we can see the problem: this solution has an infinitely small loss, for any (finite) $\\beta$ we choose. Therefore reconstruction error alone is insufficient.\n7. You are right that the convergence speed is exponential in the number of iterations, but the convergence behavior might be different with dimension. We will weaken the wording in the introduction and point to our experiment, where we find that wall clock time is longer with CG than with our method.\n8. We always monitored reconstruction loss on validation data during training and found that it was always low in our best-performing models. For example, Figure 1 shows reconstructions of CelebA validation data. Regarding training, we find that when reconstruction loss is high (due to too low $\\beta$) training becomes unstable. We do find that reconstruction loss never goes to zero, which it can\u2019t given the abundance of details in face images.\n9. The aim of the Pythae benchmark is to compare models on a given computational budget, and the evaluation metrics are FID and IS. We agree that there are flaws to these metrics, but they are currently the only feasible way to compare against a large number of competing generative autoencoders. See Figure 1 for samples from our improved architecture. See Figure 11 in the Pythae paper for samples from competitor models: https://arxiv.org/pdf/2206.08309.pdf\n\nFinally, to address your additional questions:\n\n1. The inconsistency in training time in table 1 is due to the unpredictability of rectangular flow training, which terminates when no further improvement is made. Training time can vary widely between runs. Please see table 5 on page 30 in the appendix for a fuller picture.\n2. If we restrict integration to the manifold, the entropy has a sensible definition. Suppose that $\\hat p$ is normalized on the manifold: $\\int \\hat p(\\hat x) d \\hat x = 1$ where integration is only over the manifold. Then the entropy of $\\hat p$ is $-\\int \\hat p(\\hat x) \\log \\hat p(\\hat x) d \\hat x$, again integrating only over the manifold. Note that the KL divergence in eq. (13) is defined similarly, with integration restricted to the manifold. There is a typo in eq. (13): the expectation should be over $\\hat p_\\text{data}(\\hat x)$, not $p_\\text{data}(\\hat x)$.\n\nWe hope that this clarifies our view on the weaknesses you mention, and are happy to discuss these or additional points in the discussion.\n\n**References**\n\n- Dai and Wipf. Diagnosing and enhancing VAE models. ICLR, 2019.\n- Loaiza-Ganem et al. Diagnosing and Fixing Manifold Overfitting in Deep Generative Models. TMLR, 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069369562,
                "cdate": 1700069369562,
                "tmdate": 1700069369562,
                "mdate": 1700069369562,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gpCg5Sfxk7",
                "forum": "kBNIx4Biq4",
                "replyto": "8fwraxOucx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_KrZV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_KrZV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply! I have some follow-up questions - for numbers I leave blank I have no follow-up and accept your response.\n\n1. Your response focuses on a comparison to VAE and its variants - but how about non-variational autoencoders? A standard, modern/highly-performant autoencoder does not necessarily fall victim to the limitations you noted for VAEs, including manifold overfitting. I am still curious about how you situate your work against more standard autoencoders.\n2. Thank you for the discussion on limitations.\n3. Again \"We included the Inception scores because they indicate diversity\" is stated, but with what backing? And is this truly still a valid viewpoint considering the advancements made to attempt to quantify diversity of generated datasets?\n4. \n5. \n6. I see, that makes more sense. Is it worth including a sentence on this in the main paper?\n7. \n8.\n9. I apologize as I forgot about the samples early on in the text. Can more samples be included in the experiments section or the appendix?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515949150,
                "cdate": 1700515949150,
                "tmdate": 1700515949150,
                "mdate": 1700515949150,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2gOBhb4N4p",
                "forum": "kBNIx4Biq4",
                "replyto": "L6JcePKLUP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_KrZV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5851/Reviewer_KrZV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the more thorough treatment of autoencoders! I'm quite happy with the discussion there and I think it would be worthwhile to include in the paper, as that was the biggest weakness to me.\n\nAs for diversity, some popular and more recent metrics are [Recall](https://arxiv.org/abs/1904.06991), [Coverage](https://arxiv.org/abs/2002.09797), and [Vendi Score](https://arxiv.org/abs/2210.02410), along with variants. Some additional perspective and a summary of performance is provided [here](https://arxiv.org/abs/2306.04675). I believe some of these metrics can be easily computed and might give more insight into the overall performance of the method.\n\nThank you for responding to my other concerns. Altogether, I'll raise my score to an 8 here, under the assumption that the authors include some of the additional discussion that we've talked about."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699789679,
                "cdate": 1700699789679,
                "tmdate": 1700699789679,
                "mdate": 1700699789679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]