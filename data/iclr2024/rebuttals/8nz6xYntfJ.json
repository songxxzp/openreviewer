[
    {
        "title": "AlignDiff: Aligning Diffusion Models for General Few-Shot Segmentation"
    },
    {
        "review": {
            "id": "1szmyM8HBo",
            "forum": "8nz6xYntfJ",
            "replyto": "8nz6xYntfJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6243/Reviewer_cYiM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6243/Reviewer_cYiM"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to utilize pre-trained text-to-image diffusion models for few-shot segmentation. It points out three levels of misalignments that arise when utilizing pre-trained diffusion models in segmentation tasks: 1) text prompt may not generate desired instances; 2) may fail on multi-object scenes; 3) diffusion models cannot generate segmentation masks.\n\nTo solve 1), it binds an instance specific word embedding with the given real examples. To solve 2), it combines\nsynthesized instances with real images to generate training samples with more realistic layouts. To solve 3), it use semi-supervised learning (Wei et al., 2022) and condition the generative process on provided novel samples.\n\nThe experiments are done on Pascal-5, COCO-20, and FSS-1000. Compared to previous methods, the proposed method achieves the SoTA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method seem robust as it achieves on-par performance even if it is combined with simple fune-tuning, while other methods suffer from significant decrease in performance."
                },
                "weaknesses": {
                    "value": "1. As few-shot segmentation might refer to semi-supervised segmentation with very few annotated examples and lots of unannotated examples in some literature, different from the setting in this paper, I suggest to state the problem setting in the very beginning of the paper.\n2. If I recall correctly, using a special adjective token for a specific instance was proposed in [A], but it seems claimed as one of the contribution of this paper.\n3. This paper does not clearly explain the \"copy-paste\" process, but simply cites another paper. An example figure would be nice. The images in Figure 2 is too dark to see clearly. If the space is not enough, I suggest to remove the introduction of diffusion models as it is becoming a common sense in this area.\n4. The proposed method sounds very expensive. For every category, users need to personalize Stable Diffusion with text-inversion first, and then train the segmentor over and over again to contain the new generated masks into training set. However, the paper seems to only compare the mask generation speed. I would suggest to compare the whole process speed to benefit the community.\n5. The main performance gain seems to come from combining two papers: [B] and [C], which correspond to the contribution 2) and 3) in the introduction, respectively.\n\n[A] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\n\n[B] GAPS: Few-shot incremental semantic segmentation via guided copy-paste synthesis\n\n[C] An embarrassingly simple approach to semi-supervised few-shot learning."
                },
                "questions": {
                    "value": "1. Related to weakness 3), does the \"copy-paste\" process requires post-process harmonious method? If not, how to make sure the generated image make sense to the layout? Does it harm the performance?\n2. Related to 1), with only a few examples of the new instance, how to obtain multi-object layout? Does the layout include the new instance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6243/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697682285164,
            "cdate": 1697682285164,
            "tmdate": 1699636682691,
            "mdate": 1699636682691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rJq1b8DRK4",
                "forum": "8nz6xYntfJ",
                "replyto": "1szmyM8HBo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer cYiM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments. Below we address all the concerns one by one.\n\n## Clarification of task setting\n\nWe thank the reviewer for pointing out the possible confusion of our semi-supervised mask generation method with semi-supervised segmentation. We have revised the introduction section so that the method and task settings are clearly stated.\n\n## Clarification of contribution\n\nThough both AlignDiff and Dreambooth propose to learn instance-specific embeddings, AlignDiff goes a step beyond DreamBooth to adapt Diffusion models for learning instance-specific embeddings to generate training samples with a revised loss scheme. Specifically, the normalized masked textual inversion loss in AlignDiff improves from DreamBooth in two major ways: 1) Dreambooth requires tens of images of the target object, whereas AlignDiff works with as few as one image of the target category and 2) Dreambooth requires up-close images of the objects where the desired object takes up most of the foreground. In comparison, AlignDiff is able to work with small objects. (For qualitative samples of how AlignDiff generates images of small objects compared to existing methods such as DreamBooth or Textual Inversion, refer to Fig. A9 in the supplementary).\n\n## Clarification of copy-paste implementation\n\nWe revised the manuscript to explain more details and exact numbers in Section D.2 and refer readers to this section in the method section. In addition, we provide visualized copy-and-pasted images in Fig. A8 in the supplementary material.\n\nNote that we intentionally implement the copy-paste in a simple manner because investigating the details of copy-paste is not a goal of this paper. Our goal is to argue that previous work (i.e., DiffuMask) uses a heavy image augmentation technique to generate samples with more realistic layouts for training because they overlook a simple yet effective method - copy-and-paste - to combine generated data with real data.\n\nThe main goal of the preliminary is to relate our revised loss scheme in Eq. 3 with Eq. 2 from the original diffusion work.\n\n## Timing of the method\n\nTiming is measured using a single RTX3090 GPU and with 512 x 512 resolution. AlignDiff operates in two stages. In the first stage, AlignDiff takes ~5 minutes to inverse the embedding for few-shot samples.\n\nIn the second stage of sample generation, AlignDiff performs\n\nDiffusion generation: ~4s per image\nCoarse mask: <0.1s per image\nSemi-supervised refinement: <0.15s per image\n\nWe believe that AlignDiff is quite suitable for practical applications with its two-step scheme. The cost of the initial sample generation is amortized over the generation process.\n\n## Explanation of performance gain\n\nWe respectfully disagree with the reviewer that the performance gain comes from combining papers [B] and [C].\n\n**Relationship with [B]**: though AlignDiff and [B] both use copy-paste [R1], the novelty of our approach does not lie in the fact that we use copy-paste. Instead, we argue the complexity of the approach in previous work such as DiffuMask, where heavy image augmentation techniques are used, is because they overlook real data. Copy-paste is merely an effective solution to relate generated data to real data.\n\n**Relationship with [C]**: our mask generation method has completely no technical similarity with [C]. The one and only reason we mention [C] in our paper is to establish a graceful similarity overlooked by previous work between mask generation and semi-supervised learning. [C] is designed for semi-supervised classification, whereas our method focuses on generating masks for images generated by Stable Diffusion. We refer to [C] only to mention its task setting; whereas the technical contributions from [C] are irrelevant to our approach.\n\n## Harmonization and clarification of object layout in copy-paste\n\nPrevious work in copy-paste [R1] has found that copy-pasting without using any blending technique shows similar performance with implementations that use blending to harmonize the generated images. To make consistent with these previous works, we also did not implement harmonization technique in our paper and similarly found that even generated images with uncommon layouts yields significant performance gain (example generated images can be found in Fig. A8). An interesting direction would be to further harmonize the image to ensure the consistency of semantic layout, but the focus of this work is to identify the misalignment of diffusion models for generating training examples, so we leave this to future work.\n\nFor more technical details, the current implementation obtains multi-object layout by randomly translating and randomly scaling objects before copy-paste. The instances that are used for copy-pasting include both real samples and samples generated from Stable Diffusion.\n\n## References\n\n[R1] Ghiasi, Golnaz, et al. \"Simple copy-paste is a strong data augmentation method for instance segmentation.\" CVPR. 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566258868,
                "cdate": 1700566258868,
                "tmdate": 1700566258868,
                "mdate": 1700566258868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kxztkOMMkz",
                "forum": "8nz6xYntfJ",
                "replyto": "rJq1b8DRK4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6243/Reviewer_cYiM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6243/Reviewer_cYiM"
                ],
                "content": {
                    "title": {
                        "value": "Official response to the authors"
                    },
                    "comment": {
                        "value": "Thank you for the clarification. My concerns are partially addressed, but my main concern remains: the contribution is limited. For example, DreamBooth proposed to associate the new object with an adjective word, but it is not even cited in the current manuscript version. Besides, DreamBooth typically works when tuning on 4-6 images instead of tens. \nOverall, I do believe there is contribution in this paper, but it is not claimed correctly. Therefore, I keep my original rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730476571,
                "cdate": 1700730476571,
                "tmdate": 1700730476571,
                "mdate": 1700730476571,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f8ejz9oTfr",
                "forum": "8nz6xYntfJ",
                "replyto": "1szmyM8HBo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Response to reviewer cYiM"
                    },
                    "comment": {
                        "value": "Dear Reviewer cYiM,\n\nThank you for your reply and further clarification. We unintentionally made an honest mistake in our previous response \u2013 we apologize for this. Instead of tens of images, DreamBooth [A] indeed does work with as few as 4-6 images with the desired objects taking up most of the pixels. However, it is still inappropriate for generating data for few-shot segmentation, where the model is required to work with as few as a single image. In contrast, our AlignDiff works with as few as a single image, as shown in Fig. A4 in the appendix.\n\nWe have also updated our paper accordingly to cite DreamBooth. Initially, our method was motivated by and more related work textual inversion, which is a predecessor to DreamBooth that also performs image-to-text inversion. Note that we did not intend to claim learning tokens for adjectives as a part of the contributions for the paper. Instead, we discussed it as a detailed implementation and for best reproducibility. Our main contribution of this part is the revised loss term (which we term normalized masked textual inversion). This revised loss term is what allows us to learn with as few as a single image.\n\nWe attached the revised paragraph in the updated paper PDF below for your convenience.\n\n> ... serves as the adjective description. This is similar to adjective token learning in DreamBooth (Ruiz et al., 2023) and is different from textual inversion (Gal et al., 2022) where the trainable embedding is the noun.\n\nWe hope this clarifies your concern. We are happy to discuss more if you have any remaining questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734370997,
                "cdate": 1700734370997,
                "tmdate": 1700734474445,
                "mdate": 1700734474445,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jMNU9yO30V",
            "forum": "8nz6xYntfJ",
            "replyto": "8nz6xYntfJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6243/Reviewer_ZQEG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6243/Reviewer_ZQEG"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a data augmentation approach for few-shot image segmentation. The aim is to synthesise training samples of the novel object categories that are segmented from the few-shot samples. The approach is based on proposing three modifications to the text-to-image stable diffusion for image generation. First, the image sample generation is based on creating a bank of banks of embeddings with the proposed mask and normalising the loss related to the textual inversion (Gal et al 2022). Second, more training samples are generated in a copy-paste manner by including objects generated with stable diffusion in the available training data. Third, a few-shot segmentation (FSS) model is trained with a set of appropriate image-segmentation pairs.  The FSS model is used to find more adequate pairs and is trained again with a larger pool of samples. The approach shows reasonable performance on several standard benchmarks for few-shot segmentation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well written and easy to follow. In addition, the related work is complete and discussed in detail. The method is also well presented. \n\n+ The method provides solid results for almost all evaluations.\n\n+ The ideas proposed are easy to implement in any latent diffusion model. \n\n+ The paper addresses an open problem for segmentation-like tasks. It is challenging to generate scenes with pixel-level masks using diffusion models."
                },
                "weaknesses": {
                    "value": "- (Major) The three main contributions of the paper are extensions of existing approaches. This is not a problem, but in all cases the new approach is minor. For example, the copy-paste idea is used out of the box. Similarly, the iterative training of the few-shot segmentation model does not contain any particular innovation. The paper has limited novelty.\n\n- (Major) The 1-shot results would be helpful as they are also common to the previous work.  In addition, a comparison with recent approaches would be important. For example, Xu, Qianxiong, et al. \"Self-Calibrated Cross Attention Network for Few-Shot Segmentation\". Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023. It would be useful to discuss when the paper does not achieve SOTA results and why. Overall, the results show that the proposed ideas do not show much improvement.\n\n- The term \"synthetic distribution\" is a bit confusing because it is usually associated with the generation of data from a simulator. This is not the case for the problem under consideration. Generated / realistic data distribution would be more appropriate. \n\n- The term \"out-of-distribution generation\" is also confusing. There is no discussion of what is in-distribution information in terms of prompts or generated images. The paper may refer to the additional image variations given a prompt as OOD. This is not clear. However, OOD here differs from the common use of it in uncertainty estimation."
                },
                "questions": {
                    "value": "- It would be interesting to discuss whether the method is limited to latent diffusion models or generalisable to more diffusion approaches."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6243/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819620207,
            "cdate": 1698819620207,
            "tmdate": 1699636682553,
            "mdate": 1699636682553,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "arElJMkFMR",
                "forum": "8nz6xYntfJ",
                "replyto": "jMNU9yO30V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ZQEG"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments. Below we address all the concerns one-by-one.\n\n## Novelty\n\nWe respectfully disagree with the reviewer that our work has limited novelty. In particular, we would like to highlight our novelty from two aspects:\n\n(1) **Paradigm-level novelty:** We investigate a novel and interesting task setting in this paper - adapting text-to-image diffusion models to generate training samples (as recognized by reviewers ofKT and bS35). We went beyond naively using generated samples to facilitate the training process. Instead, we identify issues in existing works (e.g., mask generation efficiency and quality) and issues that are overlooked by existing works (e.g., generation for rare categories). \n\n(2) **Methodology-level novelty:** Previous work, such as DiffuMask, generates image-mask pairs with expensive GPU hours. In contrast, AlignDiff handles both generation for rare categories and generates high-quality masks with great efficiency, which is not possible with previous methods and has great potential for practical applications. We believe this constitutes technical novelty of our method.\n\n\n## Comparison with SOTA results\n\nOur result shows great improvement, especially with the impoverished 1-shot setting. Using generated samples, AlignDiff nearly doubles the novel IoU for generalized few-shot segmentation (note that this is a **different task setting than conventional few-shot segmentation**). We also demonstrate that AlignDiff is able to improve the novel IoU on the FSS-1000 dataset, greatly outperforming both real-sample-only baseline and grounded diffusion. Reviewer ofKT and Reviewer bS35 both pointed out that our method **significantly improve the existing few-shot segmentation methods**.\n\nThe reviewer provided a reference to SCCAN [A] from ICCV\u201923, a work that deals only with the conventional few-shot segmentation setting. The main difference between conventional few-shot segmentation and the generalized few-shot segmentation is that, the generalized few-shot segmentation requires us to deal with a more challenging case where all classes need to be segmented.\n\nNevertheless, in response to the reviewer\u2019s requests, we run a small-scale experiment where we combine AlignDiff and SCCAN and demonstrates that SCCAN+AlignDiff outperforms plain SCCAN and achieves SOTA results on the Pascal-5i dataset under the 1-shot case that the reviewer requested. **This also demonstrates that our method is a general model-agnostic data augmentation method that can be integrated with many models to consistently improve their performance.**\n\n| Method  | Pascal-5-0 IoU | Pascal-5-1 IoU | Pascal-5-2 IoU | Pascal-5-3 IoU | Mean |\n| - | - | - | - | - | - |\n| SCCAN [A] | 69.1 | 74.0 | 66.3 | 61.6 | 67.8 |\n| SCCAN [A] +AlignDIff | **71.0** | **74.8** | **66.5** | **63.6** | **69.0** |\n\nWe also added reference to SCCAN in the revised manuscript.\n\n## Usage of terms\n\nWe thank the reviewer for the comments. We use the term \u2018synthetic\u2019 data instead of \u2018generated\u2019 data following previous works [B, C]. To further clarify, we add terms \u2018synthetic/generated\u2019 in the introduction in the hope that they clearly explain that we aim at generating samples.\n\nWe chose the term \u2018out-of-distribution\u2019 to highlight the misalignment of the distribution of generated samples and the distribution of synthetic samples. Out-of-distribution refers to generation of data points that fall outside the true data distribution, which we attempts to fix in this work. To avoid confusion, we revise the introduction section and give a definition of \u2018OOD generation\u2019 the first time we use this term.\n\n## Applicability of AlignDiff\n\nOur method is not limited to a specific latent diffusion model, such as Stable Diffusion. In particular, our method makes two assumptions of the underlying diffusion model.\n\nThe normalized masked textual inversion method assumes that the diffusion model is differentiable from the final synthesized image to the text encoder.\nThe semi-supervised mask generation process requires generation of coarse masks. In particular, we generate coarse masks by exploiting the cross-attention layer within the diffusion model.\n\nAlignDiff is applicable to any diffusion model that satisfies the above two properties, such as this work [D] that extends latent diffusion models to support synthesis with spatial-temporal consistency.\n\n## References\n\n[A] Xu, Qianxiong, et al. \"Self-Calibrated Cross Attention Network for Few-Shot Segmentation.\" ICCV. 2023.\n\n[B] Li, Ziyi, et al. \"Open-vocabulary object segmentation with diffusion models.\" ICCV. 2023.\n\n[C] Wu, Weijia, et al. \"Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models.\" ICCV. 2023.\n\n[D] Blattmann, Andreas, et al. \"Align your latents: High-resolution video synthesis with latent diffusion models.\" CVPR. 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567108030,
                "cdate": 1700567108030,
                "tmdate": 1700737769651,
                "mdate": 1700737769651,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x5dWkdfcII",
            "forum": "8nz6xYntfJ",
            "replyto": "8nz6xYntfJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6243/Reviewer_bS35"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6243/Reviewer_bS35"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use the pretrained diffusion models to augment the training set for few-shot semantic segmentation. Specifically, this paper provides an algorithm that generates novel instances from the diffusion model conditioned on a few available training data with annotated semantic segmentation masks and class names. Experiments on standard benchmarks such as FSS-1000 demonstrate the proposed data augmentation method improves the overall performance of existing few-shot segmentation methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It is technically sound and interesting to use pretrained image generative models for data augmentation. This paper pinpoints three major types of misalignment between the synthetic distribution and the target data distribution when a naive text-conditioned image generation method is applied and proposes a simple solution per aspect.\n2. Experiments demonstrate that the proposed data augmentation can significantly improve the existing few-shot segmentation methods, especially on novel categories. \n3. This paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. Even though the proposed data augmentation method improves the overall performance, the synthetic data can be harmful for many categories (see Fig. A.2). It is worth a more thorough analysis regarding this issue. For example, is that because the synthetic data misaligned with the target distribution, just like the text-conditioned image generation baseline? \n2. As stated in the limitation section, the proposed method degrades when the gap between the target distribution and the distribution covered by the generative model is large (e.g., medical images). However, I believe the few-shot semantic segmentation would be mostly useful for rare categories that are hard to collect enough instances for training a segmentation model. For common classes (e.g., chairs, sofa, boat) of which plenty of images can be found, it is hard to justify the necessity of using synthetic data. How is the performance of the proposed method on rare objects (e.g., the rare species from the iNaturalist dataset)?"
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6243/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6243/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6243/Reviewer_bS35"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6243/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698904433536,
            "cdate": 1698904433536,
            "tmdate": 1699636682434,
            "mdate": 1699636682434,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cUqHsUDVQt",
                "forum": "8nz6xYntfJ",
                "replyto": "x5dWkdfcII",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer bS35"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments. Below we address all the concerns one by one.\n\n## More analysis \n\nThanks for the insightful comments. We provide a figure in the supplementary material to analyze the failure cases of AlignDiff. We use the failed categories from Figure A2 with the FSS-1000 dataset under the few-shot setting.\n\nIn summary, since AlignDiff conditions the generation process using texts and the provided real sample, when both text conditioning and the masked textual inversion fail, AlignDiff may fail to generate good examples. For qualitative figures and more detailed analysis, we kindly refer the reviewer to Fig. A13 in the supplementary material.\n\n## Generalization to hard categories\n\nThough AlignDiff may not generalize well to domains where domain-specific knowledge is required (e.g., medical images), it is able to work on hard categories that plain text conditioning fails. In fact, this is our intention to evaluate AlignDiff on the FSS-1000 dataset, because FSS-1000 contains many rare categories.\n\nTo validate this point, we provide more qualitative samples in the supplementary material, which can be found in Fig. A4 of the supplementary material. Since iNaturalist does not provide segmentation annotations, it does not work with AlignDiff. Therefore, we visualize some rare categories of fine-grained species of animals from the FSS-1000 dataset. Notice how AlignDiff is able to capture the fine-grained details of textures in these species; while text-conditioned Stable Diffusion generates irrelevant samples."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565844038,
                "cdate": 1700565844038,
                "tmdate": 1700566268719,
                "mdate": 1700566268719,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MWjj2AcAKT",
            "forum": "8nz6xYntfJ",
            "replyto": "8nz6xYntfJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6243/Reviewer_ofKT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6243/Reviewer_ofKT"
            ],
            "content": {
                "summary": {
                    "value": "- The paper aims to synthesize training images and masks of novel categories to augment few-shot learning.\n\n- They identify three issues with directly using text-conditioned stable diffusion model -- failure on OOD classes, object-centric bias of stable diffusion and coarse mask generation. These are referred to as instance, scene and annotation level misalignments.\n\n- Failure on OOD is addressed using normalized masked textual inversion. Object-centric bias is mitigated using copy-paste augmentation, and coarse masks are refined by updating the segmenter using semi-supervised learning.\n\n- Experiments in the GFSS (Pascal-5i, COCO-20i datasets) and FSS (FSS-100 dataset) settings show impressive results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and well-organized \n- The identification of issues arising in applying text-to-image models for data scarce few-shot semantic segmentation setting is valuable \n- Even though the solutions provided to each of the issues are not novel, they are simple and well-proven methods \n- Extensive ablation study is provided in the appendix (both qualitative and quantitative)"
                },
                "weaknesses": {
                    "value": "- Limited comparison on the FSS-1000 dataset\n- Evidence of the finding that treating the learnable embedding as an adjective leads to a faster and more stable training convergence, is missing \n- A clear discussion of time taken by each step (textual inversion, semi-supervised mask generation), and comparison of total time with Grounded Diffusion (which is the only alternate baseline) \n- The evaluation setup describes that DiffuMask without their prompt engineering is used, but it it missing from Table1 and Table2"
                },
                "questions": {
                    "value": "- Do the methods compared with in Table1 and Table2 also use base classes, or is it extra information used in this approach? (for copy-paste and semi-supervised learning parts) \n- An ablation on the amount of generated samples would be valuable in practice \n- See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6243/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699133378893,
            "cdate": 1699133378893,
            "tmdate": 1699636682334,
            "mdate": 1699636682334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lykgdHoSoD",
                "forum": "8nz6xYntfJ",
                "replyto": "MWjj2AcAKT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6243/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ofKT"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments. Below we address all the concerns one by one.\n\n## More results on the FSS-1000 dataset\n\nTo illustrate AlignDiff is a model-agnostic approach. we introduce an additional baseline FSS model [A] and compare its performance on the FSS-1000 dataset with and without generated data from AlignDiff and GD. AlignDiff consistently improves the performance of [A] on the FSS-1000 dataset under 1-shot setting.\n\n| Method  | Overall IoU |\n| - | - |\n| HSNet (1-shot) | 86.5 |\n| VAT-R101 [A] (1-shot) | 90.0 |\n| HSNet + GD (1-shot) | 81.4 |\n| VAT-R101 [A] (1-shot) + GD | 84.7 |\n| HSNet (1-shot) + AlignDiff (Ours)   | 88.3 |\n| VAT-R101 (1-shot) [A] + AlignDiff (Ours) | 90.8 |\n| HSNet (5-shot) | 88.5 |\n| VAT-R101 [A] (5-shot) | 90.6 |\n\n## Validating the advantage of learning \u201cadjective\u201d embedding over nouns\n\nWe present qualitative and quantitative results that prove the superiority of learning an embedding for adjectives over nouns. As we have mentioned in the paper, when only a limited number of samples are available, learning an adjective embedding leads to more stable performance, as the additional nouns serve as a regularization that keeps the generation process to be related to the semantic concept.\n\nWe provide qualitative samples of learning adjective and noun embedding for the same 1-shot example in Fig. A11 in the supplementary material of the revised paper. In particular, we notice that optimizing noun embedding may result in degenerate samples when only a single example with a small object is available; whereas optimizing adjective embedding works in this case.\n\nIn addition, we also provide results on a small-scale experiment on Pascal-5-3. \n\n| Method  | Overall IoU |\n| - | - |\n| Baseline - learn noun | 39.9 |\n| Ours | 41.5 |\n\n## Timing of each step\n\nBelow we provide the timing of each component in our method. The number is measured using a single RTX3090 GPU and with 512 x 512 resolution.\n\nFor a novel category with a few examples, AlignDiff operates in two stages. In the first stage, AlignDiff is conditioned on a few examples and uses the proposed normalized masked textual inversion method to optimize textual embeddings, which takes ~5 minutes to complete.\n\nIn the second stage of sample generation, AlignDiff performs\n\nDiffusion generation: ~4s per image; (with DDIM sample; stable diffusion is slow)\nCoarse mask generation: <0.1s per image\nscoring stage: <0.1s per image\npseudo-labeling stage: <0.05s per image.\n\nNote that the two-step scheme makes AlignDiff suitable for practical applications. The inversion overhead in the first step can be effectively amortized by the diffusion generation step in the second stage.\n\n## Evaluation of DiffuMask without prompt engineering\n\nThanks for the question. The main reason why we did not make a comprehensive comparison with DiffuMask without prompting is because of the expensive GPU hours it required (The inefficient noise learning process in DiffuMask requires thousands of GPU hours to generate samples on the COCO dataset).\n\nAs an alternative, we present a smaller-scale study that compares our method with DiffuMask on a split of the Pascal-5i dataset. The table can be found in Table A2 in the supplementary material. We have also updated our paper to refer readers to Table A2 for better clarity. In summary, DiffuMask generates masks of similar quality with AlignDiff on common categories, but it comes at a much higher cost with the costly noise learning process.\n\n## Clarification of base class information use\n\nThanks for the question. To clarify, to ensure a fair comparison, some other baseline methods also use information from the base classes. For instance, GD trains its zero-shot segmenter using image-mask samples from the base dataset. Also, GAPS+GD uses samples from the base class. We observe that AlignDiff shows improvement against baselines that also use information from the base class, which validates the effectiveness of our method.\n\n## Plot of performance w.r.t. number of generated samples\n\nThanks for the question. We provide a plot of the novel IoU on a split of the Pascal-5i dataset in the revised PDF. The figure can be found in Fig. A12 in the revised supplementary material. Note that the performance saturates around 500-1,000 samples.\n\nReferences\n\n[A]: Hong, Sunghwan, et al. \"Cost aggregation with 4d convolutional swin transformer for few-shot segmentation.\" ECCV. 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6243/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565802742,
                "cdate": 1700565802742,
                "tmdate": 1700739063989,
                "mdate": 1700739063989,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]