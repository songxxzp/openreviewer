[
    {
        "title": "Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy"
    },
    {
        "review": {
            "id": "kyfU2huNQi",
            "forum": "i6EtCiIK4a",
            "replyto": "i6EtCiIK4a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_wVzv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_wVzv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a stochastic first-order algorithm based on the Moreau envelope reformulation.  Non-asymptotic convergence analysis under weaker conditions than previous works is provided. The proposed algorithm is evaluated on various setups, including few-shot learning, data hyper-cleaning, and neural architecture search."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method looks novel.\n2. Experiments look good."
                },
                "weaknesses": {
                    "value": "1. Although the assumptions are indeed weaker than previous works, the convergence measure is also different from many previous works, so the results may not be directly comparable.\n2. I think the recent paper which has a strong theoretical guarantee should be cited.\nKwon, Jeongyeol, et al. \"On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation.\" arXiv preprint arXiv:2309.01753 (2023)."
                },
                "questions": {
                    "value": "1. What does the \"Rethinking\" in the title mean? Why do we need to rethink? What is the finding after rethinking?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3145/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3145/Reviewer_wVzv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697704515379,
            "cdate": 1697704515379,
            "tmdate": 1699636262063,
            "mdate": 1699636262063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dj5Cq6yQXD",
                "forum": "i6EtCiIK4a",
                "replyto": "kyfU2huNQi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you dedicated to reviewing our work. Your constructive comments are much appreciated, and we would like to address each of the questions you raised below.\n\n(1) Although the assumptions are indeed weaker than previous works, the convergence measure is also different from many previous works, so the results may not be directly comparable.\n\nReply: \nWe appreciate your comment. In the realm of nonconvex-nonconvex BLO, we recognize that various works adopt distinct stationary measures. This diversity is acknowledged in our Table 1, stating, \"Different methods utilize distinct stationary measures, precluding direct complexity comparisons.\"\n\nIt is important to note that, traditionally, the hypergradient of the hyperobjective has been the preferred convergence measure in much of the existing literature. However, in our work, given the weaker assumptions we employ, the existence of the hypergradient is not always guaranteed.\n\nWhile acknowledging that comparing methods based on different stationary measures may lack full rigor, we wish to highlight our achievement in developing a non-asymptotic convergence analysis for our proposed method under these more lenient assumptions.\n\nWhile exploring the connection between various stationary measures for nonconvex-nonconvex BLOs is intriguing, it is not the primary focus of this work.\n\n\n(2) I think the recent paper which has a strong theoretical guarantee should be cited. Kwon, Jeongyeol, et al. \"On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation.\" arXiv preprint arXiv:2309.01753 (2023).\n\nReply: \nThank you for bringing this latest paper to our attention. We also noticed Kwon et al.'s paper after submitting our own. This paper primarily explores bilevel optimization from the perspective of the hyper-objective, using a penalty value function-based approach. It is relevant to our research and contributes to our understanding of nonconvex-nonconvex BLOs. We will cite it in our revised manuscript.\n\n(3) What does the \"Rethinking\" in the title mean? Why do we need to rethink? What is the finding after rethinking?\n\nReply: \nThe Moreau envelope, a widely recognized and frequently employed technique in single-level optimization problems, has only recently been adapted for bilevel optimization. This novel application was first introduced in the work of Gao et al. (2023) for convex lower-level objectives. In our current study, we expand upon this approach by extending the application from the convex setting, as explored by Gao et al. (2023), to a nonconvex context. Unlike Gao et al. (2023), which focused exclusively on a double-loop scheme, our work leverages the benefits of the Moreau envelope reformulation to propose a single-loop, Hessian-free scheme for BLO."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700046412124,
                "cdate": 1700046412124,
                "tmdate": 1700046412124,
                "mdate": 1700046412124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WPm2RBTPuT",
            "forum": "i6EtCiIK4a",
            "replyto": "i6EtCiIK4a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_Qikw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_Qikw"
            ],
            "content": {
                "summary": {
                    "value": "This paper concerns bi-level optimization (BLO) problems with an inner constraint set. By introducing the Moreau envelope of the lower-level function, the BLO can be reformulated into a nonconvex optimization problem with a smooth constraint. A single loop algorithm is proposed based on the formulation, leveraging the structure of the Moreau envelope. The author also provides a non-asymptotic rate for the algorithm and conducts numerical experiments to show its superiority."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The application of the Moreau envelope gives a nice reformation for the BLO. Compared with traditional value function reformation, the Moreau envelope-based reformation is a smooth problem and easier to solve.\n\n2. Non-asymptotic convergence rate is developed for the proposed method without using the PL condition."
                },
                "weaknesses": {
                    "value": "1. In the nonconvex case, (4) is only a relaxed version of the original BLO. Is not clear whether the global optimal solutions, local optimal solutions, and stationary points of (4) are related to the original BLO. Hence, I am not sure whether the stationarity measure in this paper is useful.\n\n2. Assumption 3.2 (ii) appears too technical and artificial. The author does not convince me of its practicability, because the given examples are simple and no theoretical results is ensuring Assumption 3.2 (ii).\n\n3. In Theorem 3.1, it is strange to say that \"there exists c_{\\alpha},c_{\\beta}>0\" as the upper bounds of the stepsizes $\\alpha_k,\\beta_k$. This appears to be impractical since only the existence of $c_{\\alpha},c_{\\beta}$ does not suggest how to choose the right stepsizes in implementation."
                },
                "questions": {
                    "value": "Do the models in the experiments satisfy the Assumptions previously assumed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper (ID 3145) is similar to submission 3552 of ICLR 2024 in many aspects, including the problem setting, idea, algorithm, theoretical analysis, and convergence results.\n\n1.For the problem setting, both paper 3145 and paper 3552 focus on constrained Bilevel optimization. In the lower-level problem, paper 3145 considers a nonsmooth regularizer $g$ and paper 3552 considers constraints $g(x,y)\\leq0$. The settings are very similar.\n\n2.For the underlying ideas, the two papers both use the Moreau envelope to replace the value function in the lower-level problem. The difference is merely that paper 3552 applies the Lagrange duality due to the inequality constraints while paper 3145 does not.\n\n3.For the algorithms, the updates of their algorithms are rather similar; see equations (9) (10) in paper 3145 and (10) (11) in paper 3552.\n\n4.For the theoretical analysis, both papers use a merit function $V_k$ with similar structures and prove its sufficient decrease. Moreover, they consider similar stationarity measures and explain the stationarity measure via similar penalty functions; see equations (15) (16) in paper 3145 and (15) (16) in paper 3552.\n\n5.For the convergence result, the obtained convergence rates $\\frac1{K^{(1-2p)/2}}$, $\\frac1{K^{p}}$, and $\\frac1{K^{1/2}}$ are the same in the two papers; see Theorem 3.1 of paper 3145 and Theorem 3.1 of paper 3552.\n\nI hope the authors can give some comments on the above similarities to address my concern."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3145/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3145/Reviewer_Qikw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697997759603,
            "cdate": 1697997759603,
            "tmdate": 1700715784018,
            "mdate": 1700715784018,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MM8N9mefDG",
                "forum": "i6EtCiIK4a",
                "replyto": "WPm2RBTPuT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the time you dedicated to reviewing our work and your constructive comments. Below, we would like to provide further clarification.\n\n(1) In the nonconvex case, (4) is only a relaxed version of the original BLO. Is not clear whether the global optimal solutions, local optimal solutions, and stationary points of (4) are related to the original BLO. Hence, I am not sure whether the stationarity measure in this paper is useful.\n\nReply: \nWe would like to provide further clarification:\n\nFirstly, the equivalence between the Moreau envelope reformulation (4) and the original BLO problem (1) is established in Section 2.1. Specifically, Theorem A.1 demonstrates that the reformulation (4) is equivalent to a specially relaxed version of the BLO problem (5), wherein the lower-level problem is replaced by its stationary points. The alignment of feasible sets in both (5) and (1) occurs when the lower-level problem's solution set aligns with its set of stationary points. Such conditions are met, for example, in scenarios where the lower-level objective is convex in $y$ or is smooth and adheres to the PL condition. Under these circumstances, the feasible regions defined by the reformulation (4) and the original BLO problem are identical, ensuring that their respective global and local optimal solutions concur.\n\nRegarding the stationarity of BLO problems, it is important to note that BLOs do not conform to standard nonlinear programming frameworks, and thus lack a universally accepted stationary condition. To address this, various equivalent reformulations associated with BLOs are explored for defining a suitable stationary condition. In this paper, we consider a novel reformulation (4). While the exploration of different stationary measures for BLOs is indeed a topic of interest, it falls outside the primary scope of our current research.\n\n(2) Assumption 3.2 (ii) appears too technical and artificial. The author does not convince me of its practicability, because the given examples are simple and no theoretical results is ensuring Assumption 3.2 (ii).\n\nReply: \nWe would like to provide additional clarification regarding Assumption 3.2(ii):\n\nAssumption 3.2(ii) is applicable to a broad spectrum of practical nonsmooth regularizers, many of which are yet to be explored in existing literature. \n\nThis assumption is composed of three parts: the weak convexity of $g(x,y)$, the existence and Lipschitz continuity of $\\nabla_x g(x,y)$, and the Lipschitz continuity of the proximal operator of $g(x,\\cdot)$ with respect to the upper-level variable $x$, as specified in inequality (11). Notably, the first two components \u2013 weak convexity and the gradient's Lipschitz property \u2013 are generally mild and easily satisfied.\n\nConcerning the third component (condition (11)), when the nonsmooth component of the lower-level objective is independent of the UL variable, exemplified by $g(x,y)=\\hat{g}(y)$, the condition is trivially met. As a result, Assumption 3.2(ii) is satisfied by standard convex regularizers such as $g(x,y)=\\lambda \\|y\\|_1$ and $g(x,y)=\\lambda \\|y\\|_2$, where $\\lambda>0$. Furthermore, the assumption's requirement for only weak convexity in $g$ means it is also valid for common nonconvex regularizers like the Smoothly Clipped Absolute Deviation (SCAD) and the Minimax Concave Penalty (MCP).\n\nIn scenarios where the nonsmooth component of the LL objective is dependent on the upper-level variable, condition (11) remains verifiable and practical. For instance, we studied the $l_1$ regularizer, $g(x,y)= x\\| y\\|_1$, with treating the regularization parameter as the upper-level variable, in Section A.9 of the Appendix, and demonstrated that $g(x,y)= x\\| y\\|_1$ meets Assumption 3.2(ii).\n\nIn summary, Assumption 3.2(ii) is not only practical but also encompasses a wide range of nonsmooth regularizers not yet fully explored in previous research. The practical examples satisfying Assumption 3.2(ii) are discussed following Assumption 3.2 in our paper.\n\n(3) Do the models in the experiments satisfy the Assumptions previously assumed?\n\nReply: \nIn the smooth BLO scenario, our Assumptions simplify to both UL and LL objective functions being $L$-smooth. Consequently, it is straightforward to verify that the models in (17), (18), (20)-(23) satisfy the previously assumed Assumptions when both upper-level and lower-level variables are confined within a bounded set because both the UL and LL objectives are smooth. \n\nTurning to the non-smooth BLO scenario, specifically the LL Non-Smooth Case model (19) discussed in Section 4.1. In Appendix A.9, we establish that Assumption 3.2 holds for $g(x,y) = x \\| y\\|_1$. This verification can imply that the model in (19) satisfies the assumed Assumptions.\n\nTherefore, we can state that all models employed in our experimental studies meet the Assumptions assumed."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700046355829,
                "cdate": 1700046355829,
                "tmdate": 1700046355829,
                "mdate": 1700046355829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qmZZs7qnqe",
                "forum": "i6EtCiIK4a",
                "replyto": "WPm2RBTPuT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Qikw"
                    },
                    "comment": {
                        "value": "We respectfully disagree with Reviewer Qikw's comments regarding the Ethics Concerns, which did not appear in the original version of this reviewer\u2019s Official Review. It should be noted that, as indicated by the system time record, these newly added Ethics Concerns were included in the reviewer\u2019s Official Review only after we had submitted our rebuttal to their initial review. Moreover, the reviewer did not respond to our rebuttal. Consequently, we did not receive any notifications from the system and only became aware of these newly added Ethics Concerns a few hours ago.\n\nBelow, we provide detailed clarifications to address this biased comments.\n\n\n\n(1)   $\\textbf{Differences in Problem Settings and Challenges:}$ \n\nIt's important to note that submissions 3145 and 3552 are tackling two distinct challenges that arise in the development of single-loop Hessian-free algorithms for bi-level optimization (BLO) problems. Specifically, the configurations of the BLO problems, and more specifically, the settings of the lower-level problems examined in submissions 3145 and 3552, are significantly different.\n\nTo be more specific, submission 3145 focuses on BLO with an $\\textbf{unconstrained}$ lower-level problem with potentially $\\textbf{nonconvex}$ objective and a potentially additional $\\textbf{nonsmooth}$ component depending on both upper- and lower-level variables and it is for addressing the nonconvexity and nonsmothness challenges appeared in the lower-level problem of BLO.\n\nIn contrast, submission3552 addressed BLO problems with  $\\textbf{constrained}$ convex lower-level problems.To provide more detail, the lower-level problem is with coupled lower-level constraints, and both the objective and constraint functions of lower-level problem is smooth and convex with respect to the lower-level variable. And submission3552 primarily addressed challenges arising from coupled lower-level constraints, i.e., lower-level $x$-dependent constraints.\n\n\nTo summarize, the differences in problem settings and challenges can be outlined as follows.\n|                  | Submission3552 | Submission3145  |\n| :--------: | :--------: | :-------------: | \n|   $\\textbf{Lower-level problem}$ | $\\min_{y\\in Y} f(x,y) \\ \\mathrm{s.t.}\\  g(x,y)\\leq 0$      |       $\\min_{y\\in Y} \\varphi(x,y):=f(x,y)+g(x,y) $       | \n|   $\\textbf{Annotation}$ | Here $g(x,y)$ is a coupled lower-level constraint      |       Here $g(x,y) $ is the nonsmooth part of the lower-level objective       | \n|   $\\textbf{Challenges}$ | Lower-level $x$-dependent constraints, and non-differentiability in both value function and Moreau envelope-based reformulations      |       nonconvexity and nonsmoothness in LL objective      |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740811866,
                "cdate": 1700740811866,
                "tmdate": 1700740811866,
                "mdate": 1700740811866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4zx3yDRjVU",
            "forum": "i6EtCiIK4a",
            "replyto": "i6EtCiIK4a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_y7ue"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_y7ue"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the nonconvex constrained bilevel optimization, where its upper level is nonconvex and its lower level is nonsmooth and weakly convex. It proposed an efficient single-loop gradient-based Hessian-free algorithm based on the Moreau envelope technique. Moreover, it provided the non-asymptotic convergence analysis for the proposed algorithm. Extensive experimental results demonstrate the efficiency of the proposed algorithm. In summary, the contributions of this paper are significant on the design method and solid convergence analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper proposed an efficient single-loop gradient-based Hessian-free algorithm based on the Moreau envelope technique. Moreover, it provided the non-asymptotic convergence analysis for the proposed algorithm. Extensive experimental results demonstrate the efficiency of the proposed algorithm. In summary, the contributions of this paper are significant on the design method and solid convergence analysis."
                },
                "weaknesses": {
                    "value": "It is better to list some bilevel optimization examples in machine learning that have non-smooth and weakly convex lower level functions, which will strength the motivation of this work."
                },
                "questions": {
                    "value": "Some comments:\n\n1)\tIn the proposed algorithm 1, there exist five tuning parameters $\\alpha_k,\\beta_k,\\eta_k,\\gamma,c_k$. Although the authors gave the range of these parameters in the convergence analysis, I still think the choice of these tuning parameters is not easy in practice. \n\n2)\tFrom the convergence analysis, I saw that the authors used the condition that $f(x,y)$ is a weakly convex. I suggest that the authors should add this condition in Assumption 3.2 of the paper.\n\n3)\tThe inequality (11) in Assumption 3.2 (ii) is a strict condition ? If not , please give an example.\n\n4)\tIt is better to list some bilevel optimization examples in machine learning that have nonsmooth and weakly convex lower level functions, which will strength the motivation of this work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698494240580,
            "cdate": 1698494240580,
            "tmdate": 1699636261884,
            "mdate": 1699636261884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BWWkfDPfTx",
                "forum": "i6EtCiIK4a",
                "replyto": "4zx3yDRjVU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you dedicated to reviewing our work and are thankful for your constructive comments. In the following section, we will address each question.\n\n(1) It is better to list some bilevel optimization examples in machine learning that have non-smooth and weakly convex lower level functions, which will strength the motivation of this work.\n\nReply:\nThank you for your valuable suggestions. Non-smooth and weakly convex functions, like regularization (e.g., $\\ell_1$), find extensive use in various machine learning applications. These applications include sparse representation, sparse coding, feature selection, and hyper-parameter optimization. For instance, in the context of hyper-parameter optimization for low-level vision tasks, $\\ell_1$ regularization is applied to enforce sparsity in image restoration. Additionally, in data hyper-cleaning, this same regularization technique can help mitigate overfitting in the learning model. \n\nIn Appendix A.1, we provide a brief review of recent works on nonsmooth BLO. We appreciate your feedback and plan to incorporate more bilevel optimization examples in the introduction section of the revised manuscript.\n\n(2) In the proposed algorithm 1, there exist five tuning parameters $\\alpha_k$, $\\beta_k$, $\\eta_k$, $\\gamma$, $c_k$. Although the authors gave the range of these parameters in the convergence analysis, I still think the choice of these tuning parameters is not easy in practice.\n\nReply: \nTo explore the sensitivity of these parameters, we conducted extra numerical experiments in the LL Non-Smooth Case, with a dimension of 1000. The table below displays convergence time and steps while altering one parameter, keeping the others constant. It's important to note that the algorithm achieves convergence with various parameter combinations.\n\n| | | | | | | | |\n|-|-|-|-|-|-|-|-|\n|Strategy|$\\alpha$|$\\beta$|$\\eta$|$\\gamma$|$\\underline{c}$|Steps|Time (s)|\n|Original|0.1|0.00001|0.1|10|2|97|22.83|\n|Changing $\\alpha$|0.05|0.00001|0.1|10|2|109|25.55|\n|Changing $\\alpha$|0.5|0.00001|0.1|10|2|88|13.66|\n|Changing $\\beta$|0.1|0.00002|0.1|10|2|91|22.01|\n|Changing $\\beta$|0.1|0.00003|0.1|10|2|85|13.41|\n|Changing $\\eta$|0.1|0.00001|0.5|10|2|88|12.82|\n|Changing $\\eta$|0.1|0.00001|0.01|10|2|199|51.99|\n|Changing $\\gamma$|0.1|0.00001|0.1|2|2|89|19.19|\n|Changing $\\gamma$|0.1|0.00001|0.1|100|2|90|15.3|\n|Changing $\\underline{c}$|0.1|0.00001|0.1|10|10|105|17.52|\n|Changing $\\underline{c}$|0.1|0.00001|0.1|10|50|107|17.22|\n\n(3) From the convergence analysis, I saw that the authors used the condition that $f(x,y)$ is a weakly convex. I suggest that the authors should add this condition in Assumption 3.2 of the paper.\n\nReply: \nWe appreciate your suggestion. As discussed in the paragraph prior to Section 3.2, the application of the descent lemma allows us to deduce that a function possessing a Lipschitz-continuous gradient is weakly convex. Consequently, Assumption 3.2(i) is sufficient for the weak convexity of the function $f(x, y)$.\n\n(4) The inequality (11) in Assumption 3.2 (ii) is a strict condition ? If not , please give an example.\n\nReply: \nWe would like to clarify that Assumption 3.2(ii) concerning the nonsmooth component $g(x,y)$ is not restrictive and is applicable to a wide range of practical nonsmooth regularizers, many of which have not been explored in prior research.\n\nIn our paper, under Assumption 3.2, we discuss practical instances that fulfill Assumption 3.2(ii). Notably, when the nonsmooth component of the lower-level objective is independent of the upper-level variable, exemplified as $g(x,y)=\\hat{g}(y)$, Assumption 3.2(ii) is satisfied for convex regularizers like $g(x,y)=\\lambda|y|_1$ and $g(x,y)=\\lambda|y|_2$ where $\\lambda>0$. Additionally, even for typical nonconvex regularizers such as the Smoothly Clipped Absolute Deviation (SCAD) and the Minimax Concave Penalty (MCP), Assumption 3.2(ii) remains valid.\n\nFurthermore, in cases where the nonsmooth component of the lower-level objective is dependent on the upper-level variable, we investigate a specific example: $g(x,y)= x\\|y\\|_1$. Here, $g(x,y)$ represents the $l_1$ regularizer, considering the regularization parameter as the upper-level variable. As demonstrated in Section A.9 of the Appendix, $g(x,y)= x\\|y\\|_1$ also adheres to Assumption 3.2(ii).\n\nThus, we assert that Assumption 3.2(ii) is a practical and non-restrictive condition, encompassing a broad spectrum of practical scenarios.\n\n(5) It is better to list some bilevel optimization examples in machine learning that have nonsmooth and weakly convex lower level functions, which will strength the motivation of this work.\n\nReply: \nThank you for your suggestion. Please refer to our previous response for addressing your concern."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700046270355,
                "cdate": 1700046270355,
                "tmdate": 1700046270355,
                "mdate": 1700046270355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kWXKsgD4Wj",
                "forum": "i6EtCiIK4a",
                "replyto": "BWWkfDPfTx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3145/Reviewer_y7ue"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3145/Reviewer_y7ue"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "Thanks for your responses. My concerns have been well-addressed, so I keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711961327,
                "cdate": 1700711961327,
                "tmdate": 1700711961327,
                "mdate": 1700711961327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eUqebTI186",
            "forum": "i6EtCiIK4a",
            "replyto": "i6EtCiIK4a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_am3o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_am3o"
            ],
            "content": {
                "summary": {
                    "value": "The paper applies the Moreau envelope for solving bilevel optimization with non-convex low-level optimization. The proposed algorithm extends such Moreau envelope framework from convex to non-convex settings, achieves a single loop structure, and avoids Hessian computation at the same time. A non-asymptotic convergence rate is derived and extensive numerical evaluations demonstrate faster convergence and superior performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Compared to previous bilevel optimization algorithms, the proposed MEHA algorithm is both single-loop and hessian-free, yielding an advantage in the efficiency of convergence. \n\n2. The numerical evaluation conducted is quite comprehensive, covering both synthetic experiments and various real-world tasks."
                },
                "weaknesses": {
                    "value": "The only weakness the reviewer sees is the lack of technical novelty, as the analysis is largely based on previous work utilizing the Moreau envelope for convex low-level objectives (Gao et al., 2023), and other Hessian-free works. As a result, the proposed method seems like an extension / combination of previous methods."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3145/Reviewer_am3o",
                        "ICLR.cc/2024/Conference/Submission3145/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698618496709,
            "cdate": 1698618496709,
            "tmdate": 1700659031865,
            "mdate": 1700659031865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xbUBZUhb1T",
                "forum": "i6EtCiIK4a",
                "replyto": "eUqebTI186",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to read and review our work. We appreciate your feedback and would like to provide further clarification.\n\n(1) The only weakness the reviewer sees is the lack of technical novelty, as the analysis is largely based on previous work utilizing the Moreau envelope for convex low-level objectives (Gao et al., 2023), and other Hessian-free works. As a result, the proposed method seems like an extension / combination of previous methods.\n\nReply: \nOur research indeed builds upon existing works, such as the use of the Moreau envelope for convex lower-level objectives (Gao et al., 2023) and other Hessian-free works. However, our contribution goes beyond merely synthesizing these existing approaches, as we introduce several novel elements.\n\nFirstly, while the Moreau envelope reformulation was initially proposed in Gao et al. (2023) with proposing a corresponding double-loop scheme, our work extends this to a non-convex setting and propose a new single-loop algorithm. Our single-loop algorithm's convergence analysis necessitated proving the Lipschitz continuity of the Moreau envelope solution $\\theta_{\\gamma}^*(x,y) :=  \\mathrm{argmin}_{\\theta \\in Y} \\{ \\varphi(x, \\theta) + \\frac{1}{2\\gamma}\\|\\theta-y\\|^2 \\}$, particularly challenging in the presence of a nonsmooth component, $g(x,y)$, in the lower-level objective (2). This analysis, detailed in Lemma A.5, represents a significant technical advancement.\n\nFurthermore, while previous Hessian-free approaches for BLO with non-convex lower-level problems, such as those by Ye et al. (2022) and Shen & Chen (2023), depended on the Polyak-\u0141ojasiewicz (PL) condition, our method does not. The Moreau envelope technique does not inherently satisfy the PL condition, prompting us to develop new techniques distinct from previous Hessian-free methods.\n\nMoreover, to the best of our knowledge, no existing Hessian-free algorithms have addressed scenarios where the lower-level problem objective includes a nonsmooth component, as investigated in our work. Our iteration scheme and convergence analysis for managing this nonsmooth component are thus unique contributions to the field of Hessian-free algorithm research.\n\nIn summary, our research extends the Moreau envelope-based algorithm from Gao et al. (2023) from a double-loop to a single-loop approach. Additionally, we are the first to explore a Hessian-free algorithm for bilevel optimization problems where the lower-level objective incorporates a nonsmooth component, all without relying on the PL condition."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700046166046,
                "cdate": 1700046166046,
                "tmdate": 1700046642935,
                "mdate": 1700046642935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "azn23vFpeQ",
                "forum": "i6EtCiIK4a",
                "replyto": "xbUBZUhb1T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3145/Reviewer_am3o"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3145/Reviewer_am3o"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. My concerns have been well-addressed and I'll keep my score and increase my confidence."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658999218,
                "cdate": 1700658999218,
                "tmdate": 1700658999218,
                "mdate": 1700658999218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "67mPRnsqjM",
            "forum": "i6EtCiIK4a",
            "replyto": "i6EtCiIK4a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_UhJL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3145/Reviewer_UhJL"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a single-loop algorithm for bilevel optimization problems.\nThe algorithm can handle non-smooth and smooth + non-smooth weakly convex inner functions. The authors provide convergence rates under very weak assumptions (smoothness of the outer and the inner problem)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "To my knowledge, this work contains two major novelties:\n- in the smooth case, convergence proof a single loop algorithm with weak assumptions (smoothness of the inner and the outer problem only, no need for bounded gradients)\n- in the non-smooth case, to my knowledge, this is the first single-loop algorithm proposed\nMaybe the authors and other reviewers can comment on this."
                },
                "weaknesses": {
                    "value": "While the proposed algorithm is clearly defined, authors could do a better job at providing the intuitions: the directions $d_x^k$ and $d_y^k$ are given with little context. The same comment applies to the merit function and Lemma 3.1."
                },
                "questions": {
                    "value": "- Could you comment on the novelty, is this the first analysis with such a weak set of assumptions? Or I am missing something?\n\n- Could you give intuitions on the proof? (which is currently 10 pages long in the appendix) and intuitions on the merit function.\nMaybe the authors could provide a proof sketch"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677653222,
            "cdate": 1698677653222,
            "tmdate": 1699636261614,
            "mdate": 1699636261614,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O04jHGusuA",
                "forum": "i6EtCiIK4a",
                "replyto": "67mPRnsqjM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your dedicated time in reviewing our work and your constructive comments. Below, we address each question.\n\n(1) While the proposed algorithm is clearly defined, authors could do a better job at providing the intuitions: the directions $d_x^k$\u00a0and\u00a0$d_y^k$ are given with little context. The same comment applies to the merit function and Lemma 3.1.\n\nReply: \nThank you for the suggestions. In the initial manuscript, we elucidated that the update scheme for variables $(x, y)$ can be interpreted as an inexact alternating proximal gradient approach, applied to a nonsmooth optimization problem:\n\\begin{equation*}\n\\min_{(x,y)\\in X \\times Y} \\frac{1}{ c_k}F(x,y) + f(x, y) + g(x,y) - v_\\gamma (x,y).\n\\end{equation*}\nSpecifically, $d_{x}^k$ approximates the gradient of $\\frac{1}{ c_k}F(x,y) + f(x, y) + g(x,y) - v_\\gamma (x,y)$ with respect to $x$ at point $(x^k, y^k)$, with $\\theta^{k+1}$ as an approximation for $\\theta^*_{\\gamma}(x^k,y^k) $. Similarly, $d_{y}^k$ approximates the gradient of $\\frac{1}{ c_k}F(x,y) + f(x, y) - v_\\gamma (x,y)$ with respect to $y$ at $(x^{k+1}, y^k)$, using $\\theta^{k+1}$ to approximate $\\theta^*_{\\gamma}(x^{k+1},y^k) $.\n\nRegarding the merit function $V_k$, employed in convergence analysis and Lemma 3.1, it is defined as the scalar sum of the penalized problem\u2019s objective (12):\n\\begin{equation*}\n\\phi_{c_k}(x,y) := \\frac{1}{ c_k}(F(x,y) - \\underline{F}) + f(x, y) + g(x,y) - v_\\gamma (x,y),\n\\end{equation*}\nand the proximity of iterate $\\theta^k$ to the actual solution defining the Moreau envelope of lower-level problem at iterate $(x^k, y^k)$, i.e., \n$\\mathrm{argmin}_{\\theta\\in Y} \\{\\varphi(x^k,\\theta)+\\frac{1}{2\\gamma}\\|\\theta-y^k\\|^2 \\}$.\n\n(2) Could you comment on the novelty, is this the first analysis with such a weak set of assumptions? Or I am missing something?\n\nReply: \nIn this study, we present a novel algorithm for BLO by leveraging the Moreau envelope-based reformulation initially introduced by Gao et al. (2023). To our knowledge, this is the first application of such a reformulation in the design of a single-loop, Hessian-free gradient-based algorithm for BLO with such weak assumptions. Significantly, our algorithm has a non-asymptotic convergence analysis, and can handle lower-level problems with potentially nonconvex and nonsmooth objectives. Notably, this result is achieved without the necessity of the PL condition on the lower-level problem."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700045918425,
                "cdate": 1700045918425,
                "tmdate": 1700046813613,
                "mdate": 1700046813613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IH6dOH9ed4",
                "forum": "i6EtCiIK4a",
                "replyto": "67mPRnsqjM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(3) Could you give intuitions on the proof? (which is currently 10 pages long in the appendix) and intuitions on the merit function. Maybe the authors could provide a proof sketch\n\nReply: \nIntuitions on the merit function $V_k$:\n\nThe update scheme for variables $(x, y)$ can be interpreted as an inexact alternating proximal gradient method applied to the penalized problem (12) :\n\\begin{equation*}\n\\phi_{c_k}(x, y) := \\frac{1}{c_k}\\big(F(x, y) - \\underline{F} \\big) + f(x, y) + g(x, y) - v_\\gamma(x, y).\n\\end{equation*}\nConsequently, we anticipate a decreasing value of $\\phi_{c_k}(x^k, y^k)$ for the iterates $(x^k, y^k)$.\n\nFurthermore, the update of variable $\\theta$ aligns precisely with a single proximal gradient step to the problem defining the Moreau envelope $v_{\\gamma}(x,y) :=\\inf_{\\theta \\in Y} \\{ \\varphi(x, \\theta) + \\frac{1}{2\\gamma}\\|\\theta-y\\|^2 \\}$. This alignment leads us to expect that the iterate $\\theta^k$ will converge towards the solution of the Moreau envelope at $(x^k, y^k)$, specifically $ \\mathrm{argmin}_{\\theta \\in Y} \\{ \\varphi(x^k, \\theta) + \\frac{1}{2\\gamma}|\\theta - y^k|^2 \\}$.\n\nBuilding on these insights into the update schemes for $(x, y)$ and $\\theta$, we examine the merit function $V_k$ defined as the sum of the objective value of the penalized problem (12), $\\phi_{c_k}(x, y)$, and the distance between the iterate $\\theta^k$ and the optimal solution $\\theta_{\\gamma}^*(x^k, y^k)$ for the Moreau envelope of lower-level problem at $(x^k, y^k)$.\n\nProof Sketch of non-asymptotic convergence result:\n\nThe proof of Theorem 3.1, which establishes the non-asymptotic convergence, fundamentally relies on the monotonically decreasing property of the merit function $V_k$, as delineated in Lemma 3.1. We succinctly outline the pivotal steps leading to this lemma.\n\nStep 1: We first consider the Moreau envelope $v_{\\gamma}(x,y)$, focusing on two of its critical properties: its weak convexity (referenced in Lemma A.1) and the associated gradient formulas (outlined in Lemma A.2). These properties enable us to derive an upper bound for the descent of the objective value of the penalized problem (12), $\\phi_{c_k}(x, y)$ with incorporating the error term $\\left(\\frac{\\alpha_k}{2} (L_f + L_g)^2 + \\frac{\\beta_k}{\\gamma^2} \\right) \\left\\| \\theta^{k+1} - \\theta_{\\gamma}^*(x^k,y^{k}) \\right\\|^2$, as formulated in Equation (35) in Lemma A.7.\n\nStep 2: The subsequent step involves leveraging the Lipschitz continuity of the Moreau envelope solution $\\theta_{\\gamma}^*(x,y)$ (as per Lemma A.5) along with the contraction properties of $\\theta^k$ towards this solution (discussed in Lemma A.6). This approach is instrumental in controlling the aforementioned error term $\\left\\| \\theta^{k+1} - \\theta_{\\gamma}^*(x^k,y^{k}) \\right\\|^2$ as presented in Equation (35).\n\nUltimately, these steps culminate in confirming the monotonically decreasing nature of the merit function $V_k$ as in Lemma 3.1."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700045963101,
                "cdate": 1700045963101,
                "tmdate": 1700046094094,
                "mdate": 1700046094094,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]