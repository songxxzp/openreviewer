[
    {
        "title": "Hyperbolic Visual-Semantic Alignment for Structural Visual Recognition"
    },
    {
        "review": {
            "id": "eJcVpeNUvQ",
            "forum": "o1YIpFkPSf",
            "replyto": "o1YIpFkPSf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1310/Reviewer_SCte"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1310/Reviewer_SCte"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method named HvsA for structural visual recognition. HvsA consists of two stages. In \u201chierarchy-agnostic visual-semantic alignment\u201d, it first encodes images and semantic labels in the hyperbolic space. Then it utilizes a Gaussian mixture VAE to align visual embedding with label embedding to yield a shared representation space. While in \u201chierarchy-aware semantic learning\u201d, it leverages triplet metric learning over label space to push the label embeddings away for those without hierarchical relationships while pull close those with similar semantics. Experiments are conducted over six datasets, showing the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The concept of encoding hierarchical label embeddings and aligning visual embeddings is reasonable. Furthermore, the introduction of a triplet loss designed to operate within the label space, thereby facilitating the alignment of visual embeddings in a more hierarchical manner, is sound.\n3. The experimental results are competitive."
                },
                "weaknesses": {
                    "value": "1. The definition of q_\u03b8 in Eq 7 and 8 are missing.\n2. The training and inference computation efficiency is not presented.\n3. Minor issue:  \n  3.1. Typo: Section 3.2, Task Setting: \u201cAn undirected edges (vi, vj) \u2208 E indicates that the class i is a superclass of label i\u201d It should be \u201clabel j\u201d instead of \u201clabel i\u201d.  \n  3.2. Typo: Section 3.2.1 Hyperbolic Gaussian Mixture VAE: \u201cwe seek to align visual embedding with visual embedding to yield a shared representation space.\u201d Is it \u201calign visual embedding with LABEL embedding\u201d?"
                },
                "questions": {
                    "value": "Are the authors planning to release the source code for this work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1310/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1310/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1310/Reviewer_SCte"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1310/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723100058,
            "cdate": 1698723100058,
            "tmdate": 1699636058357,
            "mdate": 1699636058357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4NsfULDdnY",
                "forum": "o1YIpFkPSf",
                "replyto": "eJcVpeNUvQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1310/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-Point Response to Reviewer SCte"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback and suggestions, and appreciate the positive comments. We address your comments below, and start with the following question:\n\n> 1. The definition of $q_{\\phi}$ in Eq 7 and 8 are missing.\n\nSorry about this. Here we follow the conventional notation in VAE [ref1]:  $q_{\\phi}(z|x)$ represents the variational posterior distribution that  is an\napproximation to the intractable true posterior $p_\\theta(z|x)$. It typically refers to a probabilistic encoder, which produces a distribution (in our paper is a Gaussian) of latent representation $z$ given a feature $x$. Relevant parts have been carefully revised to reflect this. \n\n[ref1] Auto-Encoding Variational Bayes. ICLR 2014\n\n> 2. The training and inference computation efficiency is not presented.\n\nTo address your concern, we provide comparisons of training/inference cost in below table on Cityscapes val. The table has been incorporated into Appendix Table 11.\n\n|Method |  mIoU$^1$, mIoU$^2$  | #Param (M) | FLOPs (G) | FPS | \n|:----------|:------------:|:------------:|:---------:|---------:|\n| DeepLabV3+  | 82.08, 92.16 | 62.7 | 83.40 | 8.34 |\n| HSSN  | 83.02, 93.31 | 64.3 | 87.39 | 6.38 |\n| HvsA  | 84.31, 93.97 | 68.1 | 89.11 | 6.11 |\n\nAs seen, our model basically exhibits a similar level of training and inference efficiency as the other two methods. Though it has more parameters, a larger FLOPs, and runs slower (smaller FPS), the gap is minor. Considering the superior performance and generalization capability of our model, the additional cost is acceptable.\n\n\n> 3. Minor issue:\n> 3.1 Typo: Section 3.2, Task Setting: \u201cAn undirected edge $(v_i, v_j) \\in E$ indicates that the class $i$ is a superclass of label $i$\u201d It should be \u201clabel $j$\u201d instead of \u201clabel $i$\u201d. 3.2 Typo: Section 3.2.1 Hyperbolic Gaussian Mixture VAE: \u201cwe seek to align visual embedding with visual embedding to yield a shared representation space.\u201d Is it \u201calign visual embedding with LABEL embedding\u201d?\n\nThank you so much for your careful review! They are fixed and a thorough proofreading is made.\n\n\u201cAn undirected edge $(v_i, v_j) \\in E$ indicates that the class $i$ is a superclass of label $i$\u201d $\\rightarrow$ \u201cAn undirected edge $(v_i, v_j) \\in E$ indicates that the class $j$ is a superclass of _label $i$_\u201d\n\n\u201cwe seek to align visual embedding with visual embedding to yield a shared representation space.\u201d $\\rightarrow$ \u201cwe seek to align visual embedding with _label_ embedding to yield a shared representation space.\u201d\n\n> Are the authors planning to release the source code for this work?\n\nDefinitely! We will release the code and model weights to the public."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600907796,
                "cdate": 1700600907796,
                "tmdate": 1700600907796,
                "mdate": 1700600907796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "S2ZPTyJrAs",
            "forum": "o1YIpFkPSf",
            "replyto": "o1YIpFkPSf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1310/Reviewer_Lirp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1310/Reviewer_Lirp"
            ],
            "content": {
                "summary": {
                    "value": "The authors of the paper introduced a method for visual-semantic alignment in hyperbolic space. They presented a hierarchy-agnostic approach to visual-semantic alignment, complemented by hierarchy-aware semantic learning. To demonstrate the effectiveness of their proposed model, experiments were carried out on classification and segmentation tasks across six datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The visual-semantic alignment from a probabilistic perspective in hyperbolic space is novel. \n2. The hierarchy-agnostic idea is innovative. \n3. Extensive experiments were conducted, covering multiple tasks."
                },
                "weaknesses": {
                    "value": "1. The logic flow is broken, one can hardly see the connection between 3.1, 3.2. It will be very difficult (even for a hyperbolic learning researcher) to reproduce the method by reading the ``Method'' section. \n\n2. Many important descriptions are missing or even incorrect, e. g. ,\n\n>  \" ... the mean of the wrapped normal mixture distribution is calculated by M\u00f6bius addition. \" is not logically correct.\n\n>  \" ... resulting in $y_{l,i} = \\mathbf{\\mu}_l + \\mathbf{\\Sigma}_l$ ...\", you cannot add a mean vector to a covariance matrix.\n\n>  The reconstruction loss is neither explained nor linked to a reference\n\n3. The tables basically show \"we have better numbers\", but the analysis lacks in-depth understanding of why each part works."
                },
                "questions": {
                    "value": "Please see the weaknesses. besides, I have one more question regarding the motivation\n\n> What is the aim of hierarchy-agnostic alignment when we already have the hierarchy information? Why is the ``agnostic'' part important?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1310/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1310/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1310/Reviewer_Lirp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1310/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744241171,
            "cdate": 1698744241171,
            "tmdate": 1699636058260,
            "mdate": 1699636058260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TnCgXCBzzj",
                "forum": "o1YIpFkPSf",
                "replyto": "S2ZPTyJrAs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1310/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-Point Response to Reviewer Lirp"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for pointing out certain issues of clarity in the technical part which we have revised in the latest revision and provide clarifications below.\n\nTo begin with, we address the following question that is relevant to our motivation:\n\n> What is the aim of hierarchy-agnostic alignment when we already have the hierarchy information? Why is the ``agnostic'' part important?\n\nSorry for the confusion. Please bear with our clarifications below.   Our main idea is to learn a shared, hierarchical, multimodal feature space. Our model achieves this based on two major insights:  (1) each visual entity (e.g., image, pixel) can be represented through a composition of multiple semantic labels, and (2) these labels are correlated in nature, collectively forming a tree-shaped hierarchy. Based on these insights, we develop two components: hierarchy-agnostic visual-semantic alignment addresses (1), and hierarchy-aware semantic learning addresses (2). \n\n**Difference and relation between the two components:** the hierarchy-agnostic component __handles multimodal alignment, but ignores hierarchical information__; the hierarchy-aware component __only interprets the hierarchical structure of semantic concepts (visual information is not taken into account)__. These two objectives operate on different granularity and synergistically contribute visual-semantic structural alignment. Notably, direct aligning the hierarchical structures between two modalities is very challenging, and our decoupled design alleviates this and helps model converge. \n\nNext, we address your comments on paper writing:\n\n> The logic flow is broken, one can hardly see the connection between 3.1, 3.2. It will be very difficult (even for a hyperbolic learning researcher) to reproduce the method by reading the ``Method'' section.\n\nOur apologies. We agree that the writing of mentioned parts can be improved upon rereading.  We have worked to improve our writing. \n\n- _connection between 3.1 and 3.2_: Sec. 3.1 is only to present the basic formulations for hyperbolic learning. It helps readers to understand basic concepts, and the formulations are also reused in Sec. 3.2 (_e.g.,_ Eq. 5, Eq. 6, Eq. 8). There are no inherent connections between them. To avoid confusion, we take an action to move Sec. 3.1 into a new Sec. 3 and the methodology part into a new Sec. 4. Thanks.\n\n- writing improvements in the methodology part: \n\n  (1) in Sec. 4.1, we have added missed details, including an explicit definition of covariance, i.e., $\\Sigma_l=diag(\\sigma_l^2)$, as well as the definition of the reconstruction loss. \n\n  (2) in Sec. 4.2, we re-written most parts, and make the section more logical. Concretely, we compile hierarchy-aware semantic learning into two major parts, one is hyperbolic metric learning, and the other is how to construct samples for metric learning. \n\nWe believe that these improvements make the methodology more logical and easier to understand. Beyond this, we promise that we will release the code to help  reproducibility. Thanks.\n\n> a) \" ... the mean of the wrapped normal mixture distribution is calculated by M\u00f6bius addition. \" is not logically correct.\n\nWe agree with the reviewer: this statement is not reasonable and is redundant; we removed it in the revised version. \n\n> b) \" ... resulting in $y_{l,i} = \\mu_l + \\epsilon\\Sigma_l \\in \\mathcal{Y}$...\", you cannot add a mean vector to a covariance matrix.\n\nThank you for your careful review. In the original writing, we accidentally omitted the explicit definition of $\\Sigma_l$, which is a diagonal matrix: $\\Sigma_l=diag(\\sigma_l^2)$. Here $\\mu_l\\in\\mathbb{R}^d$ and $\\sigma_l\\in\\mathbb{R}^d$ are derived from the semantic encoder. With this respect, the formula of the reparameterization trick should be more strictly revised to $y_{l,i} = {\\mu}_l + \\epsilon\\sigma_l$. The relevant parts have been carefully revised. \n\n> c) The reconstruction loss is neither explained nor linked to a reference\n\nThe reconstruction loss is a standard negative log-likelihood. We provide its formal definition in Eq. 8 in the updated article. Thanks.\n\n> The tables basically show \"we have better numbers\", but the analysis lacks in-depth understanding of why each part works.\n\nThanks for pointing this out. Following your comments, we have carefully improved the experimental parts to offer more in-depth discussions on the results. Please refer to Sec. 5.2 and Sec. 5.3 for details."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600746239,
                "cdate": 1700600746239,
                "tmdate": 1700660684850,
                "mdate": 1700660684850,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hFWW9PqC4A",
            "forum": "o1YIpFkPSf",
            "replyto": "o1YIpFkPSf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1310/Reviewer_rCAE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1310/Reviewer_rCAE"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach named HVSA (Hyperbolic Visual-Semantic Alignment) for the task of structural visual recognition. HVSA consists of two key components: hierarchy-agnostic visual-semantic alignment and hierarchy-aware semantic learning. Experimental results on various tasks and datasets demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provides a compelling motivation for incorporating the hierarchical nature of features into visual recognition tasks.\n\n2. The authors perform comprehensive experiments across various tasks and datasets, yielding convincing results."
                },
                "weaknesses": {
                    "value": "1. The paper primarily discusses the advantages of conducting feature learning in hyperbolic space compared to Euclidean space in a theoretical manner, highlighting the inherent exponential growth characteristic of hyperbolic embeddings in capturing hierarchical structures. However, there is a lack of experimental evidence to support this design choice. A potential baseline approach could involve performing all the loss terms in Euclidean space, such as triplet loss in Euclidean space.\n\n2. The paper does not provide explicit details on how the taxonomy of labels in different datasets is obtained. It would be beneficial to include information regarding the acquisition of label taxonomy and potentially include tree structures to visually illustrate the hierarchical relationships within the taxonomy.\n\n3. The experiment section lacks a brief introduction to the metric terms CV and CMAP."
                },
                "questions": {
                    "value": "This is not a question regarding this paper's issue, but more like a open discussion. Since the paper demonstrates the benefits of incorporating taxonomy as prior knowledge for recognition tasks in a close-vocabulary setting. It raises an intriguing question about the scalability of this approach to an open-vocabulary setting. Considering the superior performance achieved by visual-semantic alignment methods like CLIP on in-the-wild data, I am wondering whether this work can provide similar advantages in the open-vocabulary domain."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1310/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1310/Reviewer_rCAE",
                        "ICLR.cc/2024/Conference/Submission1310/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1310/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700383574472,
            "cdate": 1700383574472,
            "tmdate": 1700633765119,
            "mdate": 1700633765119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ly4jtEZN7L",
                "forum": "o1YIpFkPSf",
                "replyto": "hFWW9PqC4A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1310/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1310/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-to-Point Response to Reviewer rCAE"
                    },
                    "comment": {
                        "value": "Thank you for your review. We have responded to your questions below, and wherever feasible, revised the paper to reflect these answers.\n\nTo begin with, we focus on the following comment:\n\n> The paper primarily discusses the advantages of conducting feature learning in hyperbolic space compared to Euclidean space in a theoretical manner, highlighting the inherent exponential growth characteristic of hyperbolic embeddings in capturing hierarchical structures. However, there is a lack of experimental evidence to support this design choice. A potential baseline approach could involve performing all the loss terms in Euclidean space, such as triplet loss in Euclidean space.\n\nSorry for the confusion.  We had already provided the experiments to examine the choice of latent space. Please refer to **Table 7** (copied below), in which `Hyperbolic` indicates our approach, while `Euclidean` means that all losses are computed in Euclidean space. The results show that using a hyperbolic space in our approach yields **consistently** and **notably** improvements against using a Euclidean space, across four datasets. This supports our theoretical analysis and evidences the efficacy of our algorithmic design. Thanks.\n\n|Geometric space | City (mIoU$^1$, mIoU$^2$) | PASCAL (mIoU$^1$, mIoU$^2$, mIoU$^3$)  | ImCLEF07A (mAP, CmAP, CV) | ImCLEF07D (mAP, CmAP, CV) | \n|:----------|:------------:|:------------:|:---------:|:------------:|\n| Euclidean  | 83.04, 92.41 | 74.17, 87.11, 96.42 | 88.47, 90.14, 5.47 | 87.61, 88.47, 6.98 |\n| Hyperbolic | **84.63**, **94.27** | **76.37**, **88.94**, **97.88** | **92.21**, **92.44**, **3.18** | **90.76**, **91.42**, **5.71** | \n\n\nNext, we address the two comments on writing:\n\n> The paper does not provide explicit details on how the taxonomy of labels in different datasets is obtained. It would be beneficial to include information regarding the acquisition of label taxonomy and potentially include tree structures to visually illustrate the hierarchical relationships within the taxonomy.\n\nThanks for pointing this out.  All the hierarchical structures we used are officially defined in respective datasets. We directly use them without any modification. Following your suggestion, we have illustrated detailed semantic hierarchies on the datasets in Appendix Fig. 2 and Fig. 3. Thanks.\n\n> The experiment section lacks a brief introduction to the metric terms CV and CMAP.\n\nWe had provided explanations to CV and CMAP in Appendix B.2. In the revised article, we additionally include formal definitions of the two terms (see Appendix Eq. 15 and Eq. 16). Thanks.\n\nWe guess that the reviewer missed this part due to the lack of proper reference in the main text, and we have fixed this. Thanks.\n\n\nFurther, we offer our thoughts to the open question:\n\n> This is not a question regarding this paper's issue, but more like a open discussion. Since the paper demonstrates the benefits of incorporating taxonomy as prior knowledge for recognition tasks in a close-vocabulary setting. It raises an intriguing question about the scalability of this approach to an open-vocabulary setting. Considering the superior performance achieved by visual-semantic alignment methods like CLIP on in-the-wild data, I am wondering whether this work can provide similar advantages in the open-vocabulary domain.\n\nThis is an interesting question and our insights are in two aspects:\n\nFirst, theoretically, our algorithm can be promoted to open-vocabulary domain, since its basic idea is consistent with CLIP. However, in practice, there are challenges: 1) models like CLIP require very large-scale training data like image-text pairs, and explicit modelling the hierarchical structure within the data (as our method does) is hard if not impossible. Hence, implicit hierarchal structure learning appears to be more favorable and the proposed triplet loss that relies on pre-defined label taxonomy might be not suitable. 2) in comparison with contrastive-like loss used in CLIP, end-to-end optimization of VAE for large models is more difficult and expensive. Therefore, greedy optimization algorithms seem to be crucial as well. \n\nSecond, though not closely relevant to your question, we highlight that our model can serve as a more trustworthy system to handle open-vocabulary cases, as compared to current recognition models. Consider a simple label taxonomy: `Animal` $\\rightarrow$ `Okapi`. For an Okapi image, even we might be not sufficiently trained to identify rare animal `Okapi`, our model can still hold a high confidence to interpret the image belonging to a broader category of `Animal'.  This feature makes our algorithm fit to safety-critical applications (e.g., autonomous driving), even it itself is not purposely trained for open-vocabulary recognition.  Thanks."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600242331,
                "cdate": 1700600242331,
                "tmdate": 1700600242331,
                "mdate": 1700600242331,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lCmrcqsZAN",
                "forum": "o1YIpFkPSf",
                "replyto": "Ly4jtEZN7L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1310/Reviewer_rCAE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1310/Reviewer_rCAE"
                ],
                "content": {
                    "title": {
                        "value": "Thank you and another question"
                    },
                    "comment": {
                        "value": "Thank you for providing clarification. I have one more question that may go beyond the scope of this paper: In your discussion, you mentioned that \"Hence, implicit hierarchical structure learning appears to be more favorable.\" Could you please provide further elaboration on how to perform implicit hierarchical structure learning? Are there any recommended literature on implicit hierarchical structure learning in open vocabulary that you can suggest?\n\nP.S. I have updated my scores as my concerns regarding this paper have been addressed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1310/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633751971,
                "cdate": 1700633751971,
                "tmdate": 1700633751971,
                "mdate": 1700633751971,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]