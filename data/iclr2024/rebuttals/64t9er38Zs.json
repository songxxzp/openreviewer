[
    {
        "title": "Learning Deep O($n$)-Equivariant Hyperspheres"
    },
    {
        "review": {
            "id": "HPGgTPmz5S",
            "forum": "64t9er38Zs",
            "replyto": "64t9er38Zs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5920/Reviewer_gZ3c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5920/Reviewer_gZ3c"
            ],
            "content": {
                "summary": {
                    "value": "The paper demonstrates how to build neural networks with cascaded layers of spherical neurons (neurons with spherical decision surfaces) that are equivariant to the action of the orthogonal group $O(n)$. Earlier works have only considered $O(3)$-equivariant spherical neurons in the first layer (i.e., no cascaded spherical neuron layers). The paper also describes how to layers with bias and normalization and proves exact equivariance to $O(n)$ action in all cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is easy to follow and the ideas are developed and presented well.\n\n2. The proposed architecture seems to produce good results for some tasks tested on compared to strong baselines including Vector Neurons and EMLPs in many cases."
                },
                "weaknesses": {
                    "value": "1. One of the weaknesses is the motivation for this architecture. I am not able to see why spherical neurons, their equivariant versions, and cascaded layers of such neurons are an improvement over conventional multi-layer perceptrons (MLPs) and Equivariant MLPs (EMLPs), other than looking at experimental results. \n\n2. Although the architecture is technically novel compared to the earlier work on equivariant spherical neurons in 3D for the single layer case, the generalization appears to be straightforward, the arguments follow just by increasing the dimension from 3 to $n$. \n\n3. The limitation of the architecture is quite large. In the case of the convex hull prediction task, many algorithms outperform the proposed one when the training set is large. The authors say this is because higher-order interactions are not present in this architecture. Both VN and GVP outperform the proposed algorithm, but do not model second-order interactions. Furthermore, VN and GVP are tested only for the convex hull prediction task, not the other two tasks, so this leaves some questions about how good this architecture actually is."
                },
                "questions": {
                    "value": "No additional questions, but it would be good if the authors can address the weaknesses I mentioned."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5920/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5920/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5920/Reviewer_gZ3c"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782179410,
            "cdate": 1698782179410,
            "tmdate": 1700664938312,
            "mdate": 1700664938312,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TzNyVv3LSP",
                "forum": "64t9er38Zs",
                "replyto": "HPGgTPmz5S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and assessment of our work!  \n\n\nBelow we address the Weaknesses you pointed out:  \n\n\n1. - Our intuition as to why spherical neurons and equivariant hyperspheres in $n$D offer advantageous modeling is as follows: the relation between a point and a spherical decision surface (as the generalization of planar decision surfaces represented by the vanilla MLPs) is isometric (Melnyk et al., 2021), which allows for an O($3$)-equivariant construction of the steerable spherical neuron (Melnyk et al., 2022a), preserving the geometry of the input space. This was previously demonstrated only for $n=3$ and only for the input layer (Melnyk et al., 2022a,b).   \n\n     \n\n    - In our manuscript, we extrapolate these ideas: Melnyk et al. 2022a show that the output of an O(3) layer is a subgroup of O(4), therefore again an isometry.  \n\n    - Thus, similar to the argument in Melnyk et al. 2021, a 4D sphere is more appropriate than hyperplanes. Setting an O(4)-equivariant layer on top produces an isometric representation in 5D. We show that we can do this in arbitrary dimensions. \n  \n  \n2. Indeed, the proposed neurons are the generalization of the 3D neurons. The *correctness* of the generalization to $n$D, however, is non-trivial and requires the rigorous proof that we provide in the manuscript (Theorem 4).  \n\n3. - We focused on the single point case to verify our hypothesis, which we still believe is confirmed. This verification is necessary for any higher-order extension. If the absolute level of performance is considered to be the most important aspect, we can include our suggested two-point version (and add all the other details). For this, please see the \u201cLow performance\u201d part of the general comment as well as the results (the updated Figure 2 in the manuscript) for the two-point version of our model, showing the superiority of our method over all the models, including both VN and GVP, in the O(5) Convex Hulls task, as well as in the O(3) Action recognition task, including CGENN.  \n\n    - In relation to competing work, the experimental setup and results for O(5) Regression and O(5) Convex Hulls are used as-is from Ruhe et al. (2023), as we stated in Sections 4.2 and 4.3, as well as the general comment. We do not have access to the code of the specific models used in those experiments other than those provided in https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks. We will, however, provide all the code for the experiments and models that we had access to and ran ourselves.  \n\n    - If the time before the rebuttal deadline allows and it is suggested to be necessary, we will try to conduct additional experiments for some of the competing models, which we will need to create ourselves (since no code is available for further experiments except for those we already included). \n\n  **UPD**: We have now conducted additional experiments with VN. We will include them and update the manuscript before the rebuttal deadline.\n\nPlease do let us know if there is anything we need to clarify further."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084353439,
                "cdate": 1700084353439,
                "tmdate": 1700609827632,
                "mdate": 1700609827632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "10pF6v3qpO",
                "forum": "64t9er38Zs",
                "replyto": "TzNyVv3LSP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5920/Reviewer_gZ3c"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewer_gZ3c"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the response. \n\nI am still not very convinced on the theoretical novelty going from three dimensions to an arbitrary dimension. I also don't see the clear advantage of isometry between the input space and the feature space.\n\nI do appreciate the additional experimental results that clarify some of the contributions and increase my rating by one level."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664908668,
                "cdate": 1700664908668,
                "tmdate": 1700664908668,
                "mdate": 1700664908668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cXcYc9fCw7",
            "forum": "64t9er38Zs",
            "replyto": "64t9er38Zs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5920/Reviewer_sbi9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5920/Reviewer_sbi9"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel Deep Equivariant Hypersphere model that naturally exhibits group equivariance and invariance properties for arbitrary O(n) groups encompassing rotation and reflection operations in n-dimensional spaces. This achievement is made possible through the utilization of spherical neurons and the generalized steerable function theorem. While prior research predominantly concentrated on symmetry within O(3), which involves rotation and reflection operations in three-dimensional space, this paper distinguishes itself by extending the concept to a broader context, the O(n) groups. The authors delve into various theoretical aspects of the proposed model and substantiate its capabilities through experiments, showcasing its effectiveness in learning O(n) equivariance and excelling in tasks such as O(n) invariance regression and classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In the field of geometric deep learning, it is essential to develop a model that can effectively capture O(n) symmetry. This paper achieves this goal by utilizing well-defined geometric techniques, specifically spherical neural networks and steerable functions."
                },
                "weaknesses": {
                    "value": "I have doubts about whether the model proposed in this paper can be easily applied to real-world mixed-transformed datasets.\n\nIf I understand correctly, the overall proposed approach can be summarized as follows: First, a spherical neuron-based model is created to match the dimensionality of the given dataset. Then, n + 1 transformed copies of this baseline model are generated through applying O(n) transformations (using a form of n-simplex). These transformed n + 1 models serve as the basis functions for constructing steerable functions, resulting in the creation of an O(n) equivariance model called the Deep Equivarint Hyperspheres.\n\nConsidering the definition of steerable functions, these generated bases represent rotations of the original function in specific directions, implying that each copy of the spherical neuron model should have learned data aligned with a single specific orientation. In other words, to create a Deep Equivariant Hypersphere, it is essential to have a dataset aligned with a canonical frame, to initially train the baseline spherical neurons. Indeed, the experiments in this paper demonstrate that a model trained on data aligned with the canonical orientation can effectively generalize to randomly transformed test datasets. This experiment is certainly meaningful.\n\nHowever, in real-world scenarios, we often need to learn equivariance or invariance for datasets that are already randomly transformed. In most cases, we cannot know the angles or group actions applied to instances within a given dataset beforehand. Obtaining a dataset aligned with the canonical orientation is often challenging in practice. I believe that in such situations, the proposed Deep Equivariant Hypersphere may not perform effectively.\n\nAnother concern with this paper is that, while the paper rigorously explains the theoretical properties of the proposed model, it seems to lack sufficient explanation of how to construct a practical model and training algorithm in practice. At the very least, I suggest that such information should be added to the appendix. The current version of the paper is quite challenging to follow, especially for those who are not familiar with the two prior papers authored by Melynk et al [1, 2].\n\n***\n\n[1] Melnyk, P., Felsberg, M., & Wadenb\u00e4ck, M. (2021). Embed me if you can: A geometric perceptron. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1276-1284).\n\n[2] Melnyk, P., Felsberg, M., & Wadenb\u00e4ck, M. (2022, June). Steerable 3D spherical neurons. In International Conference on Machine Learning (pp. 15330-15339). PMLR."
                },
                "questions": {
                    "value": "The paper primarily emphasizes O(n) groups and acknowledges their importance in real-world scenarios. However, within the realm of geometric deep learning literature, a significant challenge lies in developing models that can generalize effectively for broader Lie groups beyond O(n). I am curious to know if the proposed Deep Equivarint Hyperspheres can effectively learn and handle such general Lie groups.  I understand that concepts such as the spherical neuron or n-simplex used in this paper may not easily apply to other Lie groups. However, I would like to hear the authors' thoughts on the extension of steerable functions for arbitrary Lie groups."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698942486655,
            "cdate": 1698942486655,
            "tmdate": 1699636629521,
            "mdate": 1699636629521,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s59fNWEJnF",
                "forum": "64t9er38Zs",
                "replyto": "cXcYc9fCw7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5920/Authors",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First of all, thank you for your positive review and assessment! \n\nYour summary accurately lists our main contributions. \n\n_______\nWe begin by addressing the Weaknesses you pointed out: \n\n- We learn a model consisting of deep equivariant hyperspheres $B(S)$ defined in equation (13): that is, each $B(S)$ is a matrix $ \\in \\mathbb{R}^{(n+1)\\times d}$ and has only one row $S \\in \\mathbb{R}^d$ as a learnable parameter vector \u2013 the rest adhere to the n-simplex construction. \nTherefore, we don\u2019t need to copy the trainable spherical decision surfaces after training \u2013 it is already encapsulated in $B(S)$. \n\n- It is not necessary to have training data aligned:  \n(For the 3D case, it\u2019s been demonstrated in \u201cTetraSphere\u201d by Melnyk et al. (2022), in which the first layer consists of 3D equivariant neurons $B(S)$.) \n    - To demonstrate this in our case, we conducted the Action recognition experiment, in which we both trained the model on the transformed (\u201cO(3)\u201d) training data and tested it on the transformed test data (\u201cO(3)\u201d, as before), resulting in an O(3)/O(3) train/test augmentation setup (as compared to no-rotation augmentation during training, \u201cI\u201d, in the original experiment, I/O(3)).  \n\n    -\tHere are the test accuracies of our DEH_PI model (for the maximum number of training data samples): \n\n|I/I (for reference)   |     I/O(3) (the original)     |     O(3)/O(3)| \n|-------------|------------- |------------- | \n|69.9%        |                                  69.9%      |                                    69.8% | \n\n \n\n- We train all the models end-to-end, using Adam optimizer. \nIn this regard, our model is no different from the vanilla MLP or any other model in the experimental comparison presented in the paper. \n\n \n_______\nAddressing your Questions: \n\n- In general, the main limitation is the required compactness: \n    - Our concept can be generalized, but with a finite number of basis functions, we can only cover a compact group (or compact interval of a non-compact group). O(n) (presumably even U(n)) is a very powerful group that covers many other groups as subgroups. \n\n \n_______\nWe sincerely hope that this clarifies our method. \nPlease let us know should you have additional questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699990604309,
                "cdate": 1699990604309,
                "tmdate": 1700131595810,
                "mdate": 1700131595810,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bSDK6e4Q1F",
            "forum": "64t9er38Zs",
            "replyto": "64t9er38Zs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5920/Reviewer_MJcE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5920/Reviewer_MJcE"
            ],
            "content": {
                "summary": {
                    "value": "This paper generalizes spherical neurons to the orthogonal group of dimension greater than 3, yielding a scalable $O(n)$-equivariant architecture. Their architecture creates steerable filter banks based on the higher-dimensional simplex. They evaluate their architecture on an $O(3)$-invariant classification dataset, as well as on two synthetic $O(5)$-equivariant datasets, and obtain strong results in the low sample complexity regime."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The extension of spherical neurons to higher dimensions is novel, and this work presents an $O(n)$-equivariant architecture that is distinct from previous $O(n)$-equivariant architectures and requires nontrivial theoretical development. Their experimental results are better than their selected baselines in low sample regimes. The proofs are clearly written and thorough."
                },
                "weaknesses": {
                    "value": "1. Generally, there is a severe lack of comparison to other O(n)-invariant and -equivariant architectures, such as EGNN, EMLP, vector neurons, canonicalization (alt. frame-averaging), and for invariance, simply taking inner products. O(n) equivariance is not new, so what is superior about this particular framework? If the benefit over baselines like CGENN is inference speed, this should be recorded in a table or figure. Many of these baselines were not included in the experiments, either, especially the only non-synthetic experiment (O(3)-invariant skeleton classification). It would also be helpful to articulate a theoretical comparison between the hyperspheres approach and existing approaches \u2014 which is more expressive, which has better scaling, which (if any) can be considered a special case of hyperspheres or vice versa. \n2. The presentation of the paper could be significantly improved. Not having seen spherical neurons or TetraSphere before, I found the paper\u2019s writing and architecture description very hard to follow \u2014 even after checking these original papers. I remain unsure what the guiding intuition for the hyperspheres equivariant framework is. Also, I would suggest making this paper more self-contained in a future revision. For example, steerability in section 2.3 is a crucial concept that is defined only with high-level words, whereas an equation would be important to properly understand the concept (especially since it is so overloaded). The subsequent claim that a \u201c3D steerable filter consisting of spherical neurons needs to comprise a minimum of four 3D spheres\u201d is also unclear; the background on spherical neurons in section 2.1 does not provide enough detail to resolve the meaning of this sentence without checking the original paper. Moreover, the proofs in the main body of the paper could be moved to the appendix, to make space for figure and writing that better clarifies the method and appeal of hyperspheres. For example, Proposition 7 simply states that composing equivariant layers yields and equivariant end-to-end function; this is quite standard and intuitive, and probably does not merit a full paragraph\u2019s explanation in the main body.\n3. The only experimental comparisons to a real dataset is on an O(3)-invariant action recognition dataset. However, the main innovation of the paper is for O(n) with n>3. Justification of the practical applications of O(n)-equivariance would much better motivate this work. \n4. Even on the simulated O(5) regression datasets, performance is worse than the CGENN baseline."
                },
                "questions": {
                    "value": "1. How expressive is the proposed $O(n)$-equivariant architecture? Is it universal, i.e. can it represent any continuous $O(n)$-equivariant function? \n2. As the authors find suboptimal experimental performance relative to methods like CGENN which have higher order interactions between the points, is it possible that there\u2019s an expressivity gap between equivariant hyperspheres and methods which enjoy higher order equivariance?\n3. How well did each method perform without any test-set rotation augmentation on the real dataset (UTKinect-Action3D)?\n4. Why is there no comparison to equivariant baselines on the real dataset (UTKinect-Action3D)? Moreover, why is there no comparison to equivariant methods other than EMLP, such as vector neurons, EGNN, etc? \n5. How are equivariant hyperspheres related to the irreducible representations of $O(n)$, if at all? \n6. What are some practical applications of $O(n)$-equivariance, for $n>3$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698990940235,
            "cdate": 1698990940235,
            "tmdate": 1699636629414,
            "mdate": 1699636629414,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T9TwVJyVUz",
                "forum": "64t9er38Zs",
                "replyto": "bSDK6e4Q1F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your detailed feedback and assessment of our work! \nWe appreciate that our contributions are accurately identified. \n\nWe first address the Weaknesses: \n _____\nW1. \n- Why our framework is superior: \n       \n     - as we state in the last paragraph on p. 1, *\u201cSimilarly to the approach of Ruhe et al. (2023), Deep Equivariant Hyperspheres directly operate on the basis of the input points, not requiring constructing an alternative one, ..., which is a key limitation of many related methods (Anderson et al., 2019; Thomas et al., 2018; Fuchs et al., 2020).\u201d*, as well as the \n      \n     - inference speed: below we provide a comparison of the inference time of the models in the O(5) Convex Hulls task (the models available to us), for which we used the NVIDIA A40: \n\n|DEH | DEH_PI | DEH_$\\Delta$_PI | CGENN |  \n|-------------|-------------|-------------|-------------|  \n| 4.3 ms | 2.7 ms | 3.1 ms | 8.2 ms |     \n \n, where DEHs are our models (see the updated Figure 2). \n\n- Including baselines in the experiments: \n\n    - O(5) Regession and O(5) Convex hulls: the experimental setup and results are used as-is from Ruhe et al. (2023), as we stated in Sections 4.2 and 4.3, as well as the general comment. We do not have access to the code of the specific models used in those experiments other than those provided in https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks. We will, however, provide all the code for the experiments and models that we had access to and ran ourselves. **UPD** We have conducted additional VN experiments, and will include them in the manuscript before the rebuttal deadline.\n\n    - O(3) Action recognition: we have now included a permutation-invariant version of CGENN (please see the updated Figure 2). \n\n- Theoretical comparison: in addition to the theoretical comparison presented in the last paragraph on p. 1 and the first paragraph on p. 2, we add that  the EMLP is a representation method building equivariant feature maps by computing an integral over the respective group (which is intractable for continuous Lie groups and hence, requires coarse approximation); the E(n)-MLP version of EGNN in the comparison is similar to our method in that it operates on scalars: it updates the vector information by learning the parameters conditioned on scalar information and multiplying the vectors with it;  the vector neurons, in contrast to our equivariant scheme, operate on latent features of each point in an equivariant manner, and the invariant features are obtained by taking the scalar product of the latent features per point; the vanilla MLP representing planar decision surfaces is, in fact, a particular case of spherical decision surfaces (Perwass et al., 2003; Melnyk et al., 2021), and thus, can be seen as a particular case of the proposed hyperspheres. \n\nW2.    \n-  In general, the presented model enjoys PointNet-like point-wise feature extraction (but with our equivariant hyperspheres).\nIn addition to the architecture description in the second paragraph in Section 4.1, we will provide (**UPD** the illustrations in the updated manuscript before the rebuttal deadline) and the code, which should facilitate the reader\u2019s understanding. \n- We appreciate the suggestion of making our paper more self-contained. We will add more background to it and adjust the relative sections accordingly. \n- The guiding intuition behind the hypersphere framework is as follows:  \n         - the relation between a point and a spherical decision surface (as the generalization of planar decision surfaces represented by the vanilla MLPs) is isometric (Melnyk et al., 2021), which allows for an O(3)-equivariant construction of the steerable spherical neuron (Melnyk et al., 2022a), preserving the geometry of the input space; \n         - we extrapolate these ideas: Melnyk et al. 2022a shows that the output of an O(3) layer is a subgroup of O(4), therefore again an isometry. Thus, similar to the argument in Melnyk et al. 2021, a 4D sphere is more appropriate than hyperplanes. Setting an O(4)-equivariant layer on top produces an isometric representation in 5D. We show that we can do it in arbitrary dimensions. \n \nW3. Experiments and practical applications for $n>3$: \n\n- the choice of the experiments for $n>3$ is motivated by the related work (Finzi et al., 2021; Ruhe et al., 2023); \n\n- a potential practical application for $n>3$ would be for the cases when 3D points (e.g., a point cloud) contain invariant features with $d$ dimensions (e.g., color or density information). Then one could consider the input signal to be $(n+d)$-dimensional and apply the O(n)-equivariant framework. However, there are no results or code for competing methods available for that case and the effort for re-writing the original code is too substantial to be done during the rebuttal.\n\nW4. Please see the \u201cLow performance\u201d part of our general comment and the updated Figure 2. \n______________________________________"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241593188,
                "cdate": 1700241593188,
                "tmdate": 1700610220615,
                "mdate": 1700610220615,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6RWTCKUnfo",
                "forum": "64t9er38Zs",
                "replyto": "bSDK6e4Q1F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Addressing the Questions"
                    },
                    "comment": {
                        "value": "Q2. We attribute the suboptimal performance not to the equivariant hyperspheres (i.e., the neurons in equation 13) themselves, but to the absence of higher-order interactions between the extracted equivariant features of the points:  \n- As indicated by the performance of the two-point version of our model (which we will add with all the details if advised; see the \u201cLow performance\u201d part of our general comment and Figure 2), which maintains the original equivariant feature extraction scheme, our model outperforms the related methods in the O(3) Action recognition and O(5) Convex Hulls tasks.  \n\nQ1. Given the time constraints, we can only provide an answer based on the experimental verification. The expressiveness of the original architecture is limited due to the absence of higher-order interaction modeling (see \"Limitations\" on p.9 and our answer to Q2). The two-point version of our model outperforms, among other models, GVP, which has been proven to be a universal function approximator for O(3) (Jing et al., 2021), and other expressive equivariant architectures like VN (Deng et al., 2021) and CGENN (Ruhe et al., 2023).\n\nQ3. O(3) Action recognition experiment without transforming test set (\u201cI\u201d) vs. with O(3)-transformation (\u201cO(3)\") for the maximum number of training samples: \n\n|Test aug.\\Model| DEH_PI   |  DEH_$\\Delta$_PI  |   CGENN_PI | MLP| MLP+Aug|  \n|-----|:-------------:|:-------------: |:-------------: | :-------------:|  :-------------:|  \n|I|   69.9% | 84.7%  |70.2% | **91.8%** | 72.6% | \n|O(3) | 69.9% |**84.7%** | 70.2% | 14.1% | 71.7% |                    \n\n, where all the models are permutation-invariant, and DEHs are our models; please see the general comment. \n\n(Since the models DEH_PI, DEH_$\\Delta$_PI , and CGENN_PI produce O(n)-invariant predictions, the results are identical to those presented for randomly transformed test data in Figure 2 in our manuscript.) \n\nQ4. We have now added the equivariant CGENN, which initially outperformed all other models in the two O(5) tasks (please see Figure 2), to the experiments. \nIf the time before the rebuttal deadline allows, we will try to add another. \n\nQ5. Relation to the irreducible representation of O$(n)$: \n- we appreciate the interesting question! ~We are looking into it and will respond shortly.~\n- UPD: Our $\\textbf{M}$-$\\textbf{\\textit{R}}_{O}$ decomposition (14) appears to be unrelated to the Clebsch-Gordan decomposition, and hence, our method is not based on the obtained irreducible representations.\n \nQ6. Please see the second bullet in our answer to W3. \n\n________\nWe sincerely hope that our answers address the concerns.\nWe are working on incorporating the suggested changes into the manuscript.\n\nPlease let us know if we need to provide additional clarification."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243328398,
                "cdate": 1700243328398,
                "tmdate": 1700489954497,
                "mdate": 1700489954497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NeUuBy3RMN",
                "forum": "64t9er38Zs",
                "replyto": "6RWTCKUnfo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5920/Reviewer_MJcE"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewer_MJcE"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their detailed response. A few responses below:\n\n* Why is \"directly operat[ing] on the basis of the input points\" a limitation, independently of the forward pass time? I was under the impression this was the primary advantage.\n* The inference speed results shown are encouraging! In a final version of the paper, it would be helpful to have a scatter plot of inference time vs test error, where each point is a method, to validate whether this method achieves a better speed/error tradeoff at any point on the plot. Other baselines should also be added to make this point.\n* An articulation of the expressiveness of O(n)-equivariant hyperspheres, as compared to other baselines, is still absent. This seems to me an important facet to understand before a new method is published. \n* The two-point version of the network inspired by Li et al indeed seems to work better, although this is a departure from the submitted work; an explanation of the two-point version in the revision is necessary.\n* The proposed method is less accurate than an MLP on the original UTKinect-Action3D dataset (and this new comparison lacks VN, GVP etc).\n* In the second bullet point response to W3, do the authors mean an $O(n+3)$-equivariant network, not an $O(n)$-equivariant network? And if so, if the $n$ features are invariant, this essentially sounds to me like $O(3)$ equivariance, but maybe I am misunderstanding.\n\nWith these points in mind, I will retain my original rating. However, I encourage the authors to resubmit to a later venue, especially after having had a chance to perhaps add more extensive experiments (including the two-point version) with baselines and timing tests, a more thorough theoretical comparison to baselines in terms of expressivity, and hopefully a more self-contained presentation of the method."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623342887,
                "cdate": 1700623342887,
                "tmdate": 1700623342887,
                "mdate": 1700623342887,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k8X1XsCtlx",
            "forum": "64t9er38Zs",
            "replyto": "64t9er38Zs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5920/Reviewer_RNHY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5920/Reviewer_RNHY"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Deep Equivariant Hyperspheres - a class of neural networks equivariant under $O(n)$ transformation group. The model is based on spherical neurons and regular $n$-simplexes. The authors present the idea of $O(n)$ equivariant features of a special type, then they study their properties when such features are composed one after another and form a deep model. \n\nWhile the theory is correct and coherent, the experimental evaluation of the method is very limited. It does not allow a reader to infer the advantage of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written. It uses mathematical notation when it's required and the rest is clearly explained with plane text. It makes it easy to follow the ideas of the authors\n- The presented method is alternative to more common MLP-based or graph-convolution-based models. It makes the field richer in terms of approaches. \n- The theory is mathematically correct"
                },
                "weaknesses": {
                    "value": "The main weakness of the paper is that the authors do not clearly demonstrate the real-life fields where this method is most advantageous and what its limitations are. To address this, it makes sense to:\n\n1. **Organize a proper related work section:** The current version uses the Introduction and Background to position the current paper in the field of previously developed methods. This approach is rather implicit and doesn't allow the reader to properly understand which papers served as inspiration for the authors, which ones are direct competitors, and which ones are not relevant at all but are mentioned.\n\n2. **Discuss the limitations:** The authors can significantly improve the quality of the paper by explicitly discussing the limitations of the approach or even demonstrating them experimentally.\n\n3. **Describe the hyperparameters:** The current paper lacks a description of the hyperparameters of the method. It would improve the quality if the authors described the main hyperparameters of the proposed approach and conducted ablation studies on them.\n\nAnother significant weakness is the **limited experimental evaluation** of the proposed method. The current experimental results demonstrate some improvement over a subset of the competitors. However, it is achieved on small datasets, with small models and only in the very-low-data regime. The common number of training samples in the experiments for which the proposed method outperforms the others is 1000. A datasets of 1000 samples is unrealistically small for the field of machine learning nowadays."
                },
                "questions": {
                    "value": "- In $O(5)$ regression experiment, CGENN significantly outperforms your method. Is the use of higher order tensor features in CGENN the only reason it performs better? If so, can you train a modification of CGENN which uses only scalar and vector features?\n- In $O(5)$ convex hulls experiment, your method perfoms worse than other, when the number of training samples increases. Can you demonstrate the effect with $10^6$ samples? \n- In both $O(5)$ experiments the proposed method reaches a plateau. While some of the other methods continue to improve. I can't infer the advantage of the proposed method from the plots. All 3 plots should be extended to a larger number of samples, because so far it looks like you demonstrated only a small part of the X-axis where your method performs well. \n- Figure 2. Left plot: your method seems to be outperformed by MLP Aug in the next step. The plot requires an extension to a larger number of samples.\n- So far, the presented method performs well only of small datasets and with small neural networks. What is the main limitation of the method? \n- What is the time consumption of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5920/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699107985981,
            "cdate": 1699107985981,
            "tmdate": 1699636629322,
            "mdate": 1699636629322,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IUIPBji1Eq",
                "forum": "64t9er38Zs",
                "replyto": "k8X1XsCtlx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission5920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5920/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed review and positive assessment of our work!  \n\n  \n\n____  \n\n  \n\nWe first address the Weaknesses:  \n\n  \n\n1. We will update the manuscript and create a separate related work section.  \n\n  \n\n2. Please have a look at the Limitations paragraph on p. 9 in our manuscript. \n\n  \n\n3. The major hyperparameters of our model are named in Sections 4.1, 4.2, and 4.3, and are summarized as follows:  \n\n  \n\n- the depth and width of the model (just like in a vanilla MLP),  \n\n  \n\n- the bias (True/False),   \n\n  \n\n- sphere representation (normalized/non-normalized (default), see the last paragraph on p. 2),   \n\n  \n\n- invariant feature computation (norm/sum).  \n\n  \n\n  \n\n     - Initially, we conducted only a quick hyperparameter search for each task. If the time before the rebuttal deadline allows, we will conduct an ablation study. To this end, what setting specifically is required to be experimented with? \n\n  \n\n  \n\n  \n\n4. Please see our general comment (the \u201cLow performance\u201d part), the updated Figure 2 in the manuscript, and our responses to the Questions below. \n\n  \n\n____  \n\n  \n\nQ1. We appreciate the suggestion to reduce CGENN for a fairer comparison, but as an extension of our approach was requested by other reviewers anyway, we compare instead the original CGENN with our extended method modeling higher-order interactions --- the two-point scheme that we mention in our general comment. This add-on does not change the original equivariant feature extraction scheme. It allows our model to get significantly closer to CGENN in the O(5) regression task and outperform it in the two other tasks (see the updated Figure 2 in the manuscript).  \n\n  \n\nQ2 and Q3. Please refer to our answer to Q1. The experimental setup and datasets for the O(5) Convex Hulls experiment (and O(5) regression) are borrowed from Ruhe et al. (2023), as we stated in Sections 4.2 and 4.3 as well as the second part of the general comment. We only have access to the results of the models, not the full training code, so we have no option to show the competing methods for bigger datasets (except CGENN). \n\n  \n\nQ4. The right-most value on the x-axis is the maximum number of training samples in that dataset. As we present in the updated Figure 2, our two-point model outperforms both MLP+Aug and CGENN.  \n\n  \n\nQ5. As we addressed in the Limitations paragraph (p.9) in the manuscript,  \n\n*\"More specifically, in the proposed feature propagation, we did not model higher-order interaction between the equivariant features explicitly, which limits the expressiveness of our model\"*. We have now addressed it in the general comment (please see the \u201cLow performance\u201d part).  \n\n  \n\nQ6. Below we provide a comparison of the time complexity of the models in the O(5) Convex Hulls task (the models available to us):  \n\n  \n\n|DEH          | DEH_PI   |  DEH_$\\Delta$_PI  |   CGENN |  \n|-------------|-------------|-------------|-------------|  \n|    4.3 ms   |   2.7 ms      |   3.1 ms   |      8.2 ms             |     \n\n  \n\n, where DEHs are our models (see the updated Figure 2). \n\n  \n\n(For this, we used the NVIDIA A40.) \n\n  \n\n_______ \n\n  \n\nWe sincerely hope this provides clarity and addresses the weaknesses and questions. \n\n  \n\nPlease let us know should we need to provide any additional details or clarification."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5920/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700131415635,
                "cdate": 1700131415635,
                "tmdate": 1700132287218,
                "mdate": 1700132287218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]