[
    {
        "title": "PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning"
    },
    {
        "review": {
            "id": "GK3ETvwu9U",
            "forum": "OZ3syNYe7D",
            "replyto": "OZ3syNYe7D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1453/Reviewer_hXvK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1453/Reviewer_hXvK"
            ],
            "content": {
                "summary": {
                    "value": "The authors present PEAR, which first relabels expert demonstrations to obtain a more effective set of subgoals, and then optimizes a hierarchical reinforcement learning (HRL) agent using reinforcement learning and imitation learning on the relabeled expert demonstrations. The authors theoretically and experimentally validate their approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors present a nice and simple idea that performs well.\n- The authors theoretically validate their relabeling algorithm in the appendix.\n- The authors demonstrate their algorithm's performance on an extensive set of tasks, including on real world robot tasks.\n- The authors qualitatively show the subgoal predictions in Figure 2."
                },
                "weaknesses": {
                    "value": "- The learning curve figures are too small and very difficult to see. It would be much appreciated if the authors could fix this.\n- In Algorithm 1 for Lines 5-13 it could improve clarity if the authors included comments on what each line is doing. I found this part a bit difficult to understand.\n- It seems it is necessary to manually set $Q_{\\text{thresh}}$ for each environment."
                },
                "questions": {
                    "value": "- I'm curious why PEARL-IRL outperforms PEAR_BC on Maze, Pick Place, Bin, and Hollow, but PEAR-BC outperforms PEARL-IRL on Kitchen. Do the authors have any ideas why this is?\n- Have the authors experimented with different numbers of expert demonstrations? I'm curious how the method would perform with more/less demonstrations.\n- How is $Q_{\\text{thresh}}$ chosen for each environment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1453/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1453/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1453/Reviewer_hXvK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698628535318,
            "cdate": 1698628535318,
            "tmdate": 1699636074314,
            "mdate": 1699636074314,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kiv5aKvIgi",
                "forum": "OZ3syNYe7D",
                "replyto": "GK3ETvwu9U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response:"
                    },
                    "comment": {
                        "value": "Reviewer 4:\n\nWe thank the reviewer for their detailed, constructive feedback. We address the reviewer\u2019s concerns as follows:\n\n**1. \"The learning curve figures are too small and very difficult to see. It would be much appreciated if the authors could fix this.\"**\n\nResponse:\n\nWe apologize for the figure size and clarity issues. We have now fixed the issues in the paper. We hope this improves the overall clarity.\n\n**2. \"In Algorithm 1 for Lines 5-13 it could improve clarity if the authors included comments on what each line is doing. I found this part a bit difficult to understand.\"**\n\nResponse:\n\nWe thank the reviewer for pointing this out. We have now added the comments in Algorithm 1 wherever applicable. We hope that this improves the clarity of the algorithm psuedo-code.\n\n**3. \"It seems it is necessary to manually set $Q_{thresh}$ for each environment.\"**\n\nResponse:\n\nThe proposed approach does require setting $Q_{thresh}$ hyper-parameter but we empirically found that the performance is not unstable with varying hyperparameter value. Intuitively, this is a reasonable trade-off that renders the higher level subgoal generation a self-supervised process, thereby eliminating the need for expert guided annotations for segmenting subgoals. Additionally, please note that we empirically found that after we normalize $Q_{\\pi^L}$ values of a trajectory, the $Q_{thresh}$ hyper-parameter value of $0$ works consistently well across all environments, which exhibits that setting $Q_{thresh}$ value does not cause significant overhead. We have added the hyper-parameter ablation analysis in Appendix A.4 Figure 10 for your consideration. Additionally, in real robotic experiments, we used the default $Q_{thresh}$ value of $0$ and PEAR was able to outperform the baselines in all experiments.\n\n**4. \"I'm curious why PEARL-IRL outperforms PEAR_BC on Maze, Pick Place, Bin, and Hollow, but PEAR-BC outperforms PEARL-IRL on Kitchen. Do the authors have any ideas why this is?\"**\n\nResponse:\n\nOur motivation for using inverse reinforcement learning (IRL) is that although behavior cloning (BC) works efficiently in many scenarios, it sometimes fails to perform well for complex tasks that require long term planning. However, the advantages of IRL are sometimes counterbalanced by the fact that they are really difficult to train. We believe that since it is a really difficult environment to train on, PEAR-BC slightly outperforms PEAR-IRL in Franka kitchen environment:\n\nSuccess Rates:\n| Method    | Success Rate |\n| -------- | ------- |\n| PEAR-IRL  | **0.89 $\\pm$ 0.06**   |\n| PEAR-BC | **1.0 $\\pm$ 0.0**    |\n\nThus, although in general, PEAR-IRL works better than PEAR-BC in long horizon complex tasks, PEAR-BC might work better in some cases where it is hard to train an IRL objective. However, the performance does not differ significantly and we found that in real world tasks, PEAR-IRL consistently outperforms PEAR-BC in the experiments. \n\n**5. \"Have the authors experimented with different numbers of expert demonstrations? I'm curious how the method would perform with more/less demonstrations.\"**\n\nResponse:\n\nYes, we have performed ablations to analyze the effect of varying the number of expert demonstrations for each task (Please refer to Appendix A.4 Figure 9). Although it is subject to availability, we increase the number of expert data until there is no significant performance boost.\n\n**6. \"How is $Q_{thresh}$ chosen for each environment?\"**\n\nResponse:\n\nFirstly, we select $100$ randomly generated environments for training, testing and validation. Using this, for calculating $Q_{thresh}$, we compute success rate plots for various $Q_{thresh}$ values to select the best $Q_{thresh}$ parameter (the ablation analysis is shown in Appendix A.4 Figure 10). \n\nWe hope that the response addresses the reviewer\u2019s concern. Please let us know, and we will be happy to address additional concerns if any."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236860006,
                "cdate": 1700236860006,
                "tmdate": 1700604570116,
                "mdate": 1700604570116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UmtwJ7H3JT",
                "forum": "OZ3syNYe7D",
                "replyto": "VUVOKkszwy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_hXvK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_hXvK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the changes and the discussion! For Algorithm 1, I see that you have added comments; it would improve clarity if you prepended a # or // to each comment line. Otherwise, it currently blends in with the actual pseudocode. I also appreciate that you have made the learning curve figures in the main text more clear, but the learning curves in the Appendix could also be made much more clear (although these changes will not affect my score)."
                    }
                },
                "number": 39,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713322543,
                "cdate": 1700713322543,
                "tmdate": 1700713322543,
                "mdate": 1700713322543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vp5zYMeBiL",
            "forum": "OZ3syNYe7D",
            "replyto": "OZ3syNYe7D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1453/Reviewer_cZN6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1453/Reviewer_cZN6"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an approach that conducts adaptive re-labeling on a handful of expert demonstrations to address complex long-horizon tasks in hierarchical reinforcement learning. It offers a bound for the method's suboptimality. The proposed method is experimented on simulated tasks and surpasses several hierarchical reinforcement learning benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper introduces a simple method for generating the subgoal dataset from demonstrations and merges existing solutions for HRL training. By using ower-level policy to adaptively segment expert state-demonstrations into skills, the method provides an appealing feature of replacing the expert annotation. The paper also provides the bounds for the suboptimality of both the higher-level and lower-level policies."
                },
                "weaknesses": {
                    "value": "The core idea of generating the subgoal by thresholding an environment-specific value has natural constraints. The method of determining reachability via the low-level Q function concurs the approach in [*]. In optimizing the hierarchical policies, the paper echoes [**] by generating achievable subgoals through adversarial learning on relabeled subgoals in the HRL context. The authors are encouraged to provide a more in-depth discussion about the novelty of their method and its distinction from existing literature. The result figures provided are too small to interpret.\n\n[*] Kreidieh, Abdul Rahman, et al. \"Inter-level cooperation in hierarchical reinforcement learning.\" arXiv preprint arXiv:1912.02368 (2019).\n[*] Wang, et al. \"State-conditioned adversarial subgoal generation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 8. 2023."
                },
                "questions": {
                    "value": "See the above section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674273851,
            "cdate": 1698674273851,
            "tmdate": 1699636074217,
            "mdate": 1699636074217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Pyd9CUsrBY",
                "forum": "OZ3syNYe7D",
                "replyto": "Vp5zYMeBiL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response 1/2:"
                    },
                    "comment": {
                        "value": "Reviewer 3:\n\nWe thank the reviewer for their detailed, constructive feedback. We address the reviewer\u2019s concerns as follows:\n\n**1. \"The core idea of generating the subgoal by thresholding an environment-specific value has natural constraints.\"**\n\nResponse:\n\nThe proposed approach does require setting $Q_{thresh}$ hyper-parameter but we empirically found that the performance is not unstable with varying hyperparameter value. Intuitively, this is a reasonable trade-off that renders the higher level subgoal generation a self-supervised process, thereby eliminating the need for expert guided annotations for segmenting subgoals. Additionally, please note that we empirically found that after we normalize $Q_{\\pi^L}$ values of a trajectory, the $Q_{thresh}$ hyper-parameter value of $0$ works consistently well across all environments, which exhibits that setting $Q_{thresh}$ value does not cause significant overhead. We have added the hyper-parameter ablation analysis in Appendix A.4 Figure 10 for your consideration. Additionally, in real robotic experiments, we used the default $Q_{thresh}$ value of $0$ and PEAR was able to outperform the baselines in all experiments.\n\n**2. \"The method of determining reachability via the low-level Q function concurs the approach in [Inter-level cooperation in hierarchical reinforcement learning.]\"**\n\nResponse:\n\nThe paper titled \"Inter-level cooperation in hierarchical reinforcement learning\" proposes an approach based on inter-level cooperation between multi-level agents by jointly optimizing the higher level and lower level functions using RL. Similar to our approach, the paper considers the lower level Q function to motivate the higher level agent to produce reachable subgoals for the lower level policy. However in their approach, the subgoals are encountered as a result of environment exploration of the higher level policy, which may be quite random. This leads to two major issues: (i) either the generated subgoals are very hard, which impedes effective lower level policy learning, or (ii) the subgoals are too easy, due to which the higher level has to do all the heavy lifting to solve the task. The issue is exacerbated in the beginning of training, due to which the policies have to extensively explore the environment before being able to come across good subgoal predictions. Due to these issues, the approach might be unable to perform well on multi-stage long horizon tasks. \nIn our approach, we use a handful of expert demonstrations and employ our novel \"primitive enabled adaptive relabeling\" procedure for picking reachable subgoals for the lower level policy, and effectively utilize them for training higher level policy using additional imitation learning regularization. Notably, expert demonstrations can not be directly leveraged in raw trajectory form to train the higher level, and therefore require efficient data relabeling to segment them into meaningful subgoals. Moreover, since we periodically perform data relabeling, our approach always generates reachable subgoals for lower level policy, thereby mitigating non-stationarity issue prevalent in vanilla hierarchical learning approaches, and thus devising a practical HRL algorithm. In order to prevent over-estimation of the values on out-of-distribution states, we also employ an additional margin classification objective as explained in detail in Section 4.1. We also provide theoretical guarantees that justify the benefits of periodic re-population using adaptive relabeling, which are largely missing in recent contemporary work. We also demonstrate the efficacy of our approach in complex long horizon tasks like Franka kitchen environment, and also include environments with challenging dynamics and policy, especially the soft rope manipulation, which goes beyond some peer work that focuses on navigation-like tasks on 2D plane, where the goal space is limited. Finally, we demonstrate that PEAR is able to demonstrate good performance in challenging real world environments. We have improved the related work section based on the reviwer's concern and the discussion above, and added a brief discussion summarizing above points in the the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236557338,
                "cdate": 1700236557338,
                "tmdate": 1700236557338,
                "mdate": 1700236557338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aKbMxSVPHl",
                "forum": "OZ3syNYe7D",
                "replyto": "Vp5zYMeBiL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_cZN6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_cZN6"
                ],
                "content": {
                    "comment": {
                        "value": "I recognize and appreciate the authors' efforts to address the initial concerns raised. However, my reservations regarding the distinction between this paper and the prior work \"State-conditioned adversarial subgoal generation\" as well as the environment-specific hyper-parameter remain.\n\nThe response provided does not fully clarify how this work significantly diverges from the aforementioned prior work, particularly in terms of optimizing hierarchical policies. While the method of generating relabeled subgoals might differ, the core concept of using adversarial learning in the HRL context to generate achievable subgoals seems to align closely with the previous work. The novelty of this paper appears to hinge on the specific method of generating relabeled subgoals, but this alone may not constitute a substantial advancement.\n\nFurthermore, my concern regarding the method of generating subgoals by thresholding an environment-specific value has not been adequately addressed. Without more comprehensive evaluations, it's challenging to assess the potential limitations or strengths of this approach. This aspect is crucial for understanding the practical applicability and robustness of the proposed method in diverse scenarios."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670256572,
                "cdate": 1700670256572,
                "tmdate": 1700670256572,
                "mdate": 1700670256572,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pcFjfvXx43",
                "forum": "OZ3syNYe7D",
                "replyto": "Vp5zYMeBiL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response:"
                    },
                    "comment": {
                        "value": "1. **The response provided does not fully clarify how this work significantly diverges from the aforementioned prior work, particularly in terms of optimizing hierarchical policies**\n\nResponse:\n\nWe understand the concern and provide clarifications to differentiate our method from the aforementioned work. Although our approach and the mentioned work both use adversarial training to generate achievable subgoals, our approach differs from the previous work in the following manner:\n\n1. As explained before, the approach in the mentioned paper may yield degenerate solutions. However, we side-step this issue by using a handful of expert demonstrations for picking reachable subgoals for the lower level policy, and effectively utilizing them for training higher level policy using additional imitation learning regularization.\n\n2. The relabeling approach in the mentioned paper relies on the approach in the paper HIRO, which assumes additional assumptions on the environment subgoal generation. In contrast, our approach assumes no such limiting assumptions.\n\n3. We propose a hierarchical curriculum learning based approach, which always generates a curriculum of achievable subgoals for the lower primitive.\n\n4. Apart from adversarial training, we derive a general imitation learning based regularization objective, where we can plug in various distance metrics to yield various imitation learning algorithms. We perform theoretical and empirical analysis for inverse reinforcement learning and behavior cloning regularization objectives.\n\n5. Our approach provides a plug-and-play framework where we can plug in off-policy RL algorithm and IL algorithm of choice, which ultimately yields various joint optimization objectives for hierarchical reinforcement learning.\n\n6. We also provide theoretical guarantees that justify the benefits of periodic re-population using adaptive relabeling, which is missing from the mentioned work. \n\n7. We also demonstrate the efficacy of our approach in complex long horizon tasks, and also include environments with challenging dynamics and policy, which goes beyond some peer work where the goal space is constrained.\n\n8. Additionally, we would like to point out that this work is actually concurrent to our work, as our work was in submission when this work was published.\n\nWe hope that the above mentioned points differentiate our work from the previous work, and elucidates the contributions of our approach.\n\n\n\n2. **Furthermore, my concern regarding the method of generating subgoals by thresholding an environment-specific value has not been adequately addressed**\n\nResponse:\n\nIn our approach, $Q_{thresh}$ is used for automatically relabeling expert demonstrations to generate expert subgoal dataset for training higher level policies. Some prior works use fixed window based relabeling to generate expert subgoal demonstration dataset for the higher level policy [https://arxiv.org/abs/1910.11956]. In such approaches, the window size parameter $k$ is an environment specific value which has to be set. Moreover, this approach is basically a brute force approach where we consider multiple window sizes, and select the best window size from among them. However, intuitively, such approaches suffer from two reasons: (i) it requires environment specific window size hyper-parameter to be set, and (ii) fixed parsing approaches may generate sub-optimal expert subgoal dataset. Although our method uses environment specific $Q_{thresh}$ value, it side-steps the fixed parsing issue by using primitive enabled adaptive relabeling, which leads to better policies learnt from adaptively relabeled expert demonstrations. Moreover, our higher level subgoal generation process (primitive enabled adaptive relabeling) is a self-supervised process, which does not require an expert for segmenting subgoals.\n\nAdditionally, please note that the $Q_{thresh}$ hyper-parameter value of $0$ works consistently well across all environments, which demonstrates that setting $Q_{thresh}$ value does not cause significant overhead. Thus, we effectively just need to normalize $Q_{\\pi^L}$ values of a trajectory, and then select $Q_{thresh}$ hyper-parameter value of $0$, which works consistently well across all environments. \n\nWe hope that the response addresses the reviewer\u2019s concerns and clearly explains its contributions."
                    }
                },
                "number": 37,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695409760,
                "cdate": 1700695409760,
                "tmdate": 1700695500803,
                "mdate": 1700695500803,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ydKW9G1AiG",
            "forum": "OZ3syNYe7D",
            "replyto": "OZ3syNYe7D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel joint optimization approach to address non-stationarity in hierarchical reinforcement learning. By comparing the Q-values of the low-level policy with the environment-specific Q-value, adaptive relabeling of expert samples is performed for both high- and low-level policy training. The authors claim that this approach can mitigate the non-stationarity in HRL. Additionally, the authors provide performance difference bounds under the $\\phi_D$ common policy assumption, and simulation as well as real-world experiments demonstrate the comparable performance of PEAR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think it is a new approach, and this paper provides some theoretical explanations. It also includes experiments in real-world environments."
                },
                "weaknesses": {
                    "value": "1. **[Deal with the non-stationarity]** I don't agree that this work ameliorates non-stationarity. Since PEAR can access expert samples when training the high-level policy by BC or IRL, the high-level dynamics remain consistent with the dynamics that generate the expert sample trajectories without any change. This is inconsistent with the claim of non-stationarity made by the authors, which raises concerns about the contribution in this regard.\n2. **[Performance limitation from expert]** Since the high-level policy is optimized only through BC or IRL using expert samples and does not participate in the environment exploration process, the performance of expert samples significantly limits the upper-performance limit of PEAR. Therefore, the authors need to analyze the impact of expert performance on the upper-performance limit of PEAR and conduct experiments with expert samples of varying performance.\n\n3. **[How to get $Q_{thresh}$]** When performing adaptive relabeling, the authors require additional environment-specific Q values, which are not available in standard experimental settings and may limit the practicality of PEAR.\n4. Many of the algorithm designs are not well supported:\n   * Why does the additional margin classification objective in low-level policy optimization prevent over-estimation?\n   * In equation (2), why does the low-level policy need the BC regularization objective? In my view, the goal of the low-level policy is to reach sub-goals more effectively, and this regularization is confusing.\n   * Why is the joint value function formulated as a summation of $J$ and $J_{BC}$?\n5. The clarity of the figures in the experiments is poor, especially in Figure 5."
                },
                "questions": {
                    "value": "1. In section 3, the authors define the high-level reward, $r_{ex}$. However, the subsequent high-level policy optimization process does not appear to utilize $r_{ex.\" Please clarify the purpose of the high-level reward.\n\n2. I recommend that the authors survey and include some of the most recent works in the related works section.\n\n   [1] Lee S, Kim J, Jang I, et al. DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning[J]. Advances in Neural Information Processing Systems, 2022, 35: 13668-13678.\n\n   [2] Chane-Sane E, Schmid C, Laptev I. Goal-conditioned reinforcement learning with imagined subgoals[C]//International Conference on Machine Learning. PMLR, 2021: 1430-1440.\n\n   [3] Zhang T, Guo S, Tan T, et al. Generating adjacency-constrained subgoals in hierarchical reinforcement learning[J]. Advances in Neural Information Processing Systems, 2020, 33: 21579-21590.\n\n   [4] C-Learning: Learning to Achieve Goals via Recursive Classification\n\n3. In the experimental setup, since PEAR requires training with expert samples (possibly high-performance expert samples), while other baselines like HAC do not depend on expert data, please clarify the fairness of the experimental comparisons."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1453/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd",
                        "ICLR.cc/2024/Conference/Submission1453/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754268858,
            "cdate": 1698754268858,
            "tmdate": 1700714278068,
            "mdate": 1700714278068,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rBaUB1Jdei",
                "forum": "OZ3syNYe7D",
                "replyto": "ydKW9G1AiG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response 1/2:"
                    },
                    "comment": {
                        "value": "Reviewer 2:\n\nWe thank the reviewer for their detailed, constructive feedback. We address the reviewer's concerns as follows:\n\n**1. \"[Deal with the non-stationarity] I don't agree that this work ameliorates non-stationarity. Since PEAR can access expert samples when training the high-level policy by BC or IRL, the high-level dynamics remain consistent with the dynamics that generate the expert sample trajectories without any change. This is inconsistent with the claim of non-stationarity made by the authors, which raises concerns about the contribution in this regard.\"**\n\nResponse:\n\nWe understand the reviewer's confusion and explain the claim regarding non-stationarity as follows: as explained in detail in Section 4, PEAR jointly optimizes HRL agents by employing reinforcement learning (RL) with imitation learning (IL) regularization. Since the higher level policy does not have direct access to expert subgoal dataset, PEAR using primitive enabled adaptive relabeling to generate efficient subgoal supervision for the higher level policy. While PEAR leverages imitation learning regularization using expert demonstrations to bootstrap learning, it also employs reinforcement learning to explore the environment and come up with efficient actions to execute in complex task environments. Due to this exploration and joint optimization, PEAR significantly outperforms the imitation learning baselines, as stated in the Experiment Section 5 and Table 1. Moreover, PEAR mitigates non-stationarity by periodically re-populating subgoal transition dataset $D_g$ after every $p$ timesteps according to the goal achieving capability of the current lower primitive. This generates a natural curriculum of achievable subgoals for the lower primitive, thereby ameliorating non-stationarity. We hope that this elucidates the motivation behind how and why PEAR deals with non-stationarity. Please let us know and we would be happy to clarify any other doubts and concerns regarding the motivation. \n\n**2. \"[Performance limitation from expert] Since the high-level policy is optimized only through BC or IRL using expert samples and does not participate in the environment exploration process, the performance of expert samples significantly limits the upper-performance limit of PEAR. Therefore, the authors need to analyze the impact of expert performance on the upper-performance limit of PEAR and conduct experiments with expert samples of varying performance.\"**\n\nResponse:\n\nAs clarified in the previous answer, PEAR employs reinforcement learning along with imitation learning regularization to explore the environment and devise efficient actions to execute in the complex task environments. Due to this, PEAR significantly outperforms various hierarchical and non-hierarchical baselines that may or may not employ imitation learning. Additionally, we perform ablations to analyze the effect of varying the number of expert demonstrations for each task (Please refer to Appendix A.4 Figure 9). Although it is subject to availability, we increase the number of expert data until there is no significant performance boost. We also conduct experiments where we vary the quality of demonstrations by adding in some g number dummy/garbage demonstrations with the N expert demonstrations, but since the algorithm is unable to learn from such garbage demonstrations, empirically the method performs as if it is being trained using (N-g) demonstrations, which is similar to the ablations analyzing the effect of varying the number of expert demonstrations for each task, and hence we omit the ablation results from the paper.\n\n\n**3. \"[How to get $Q_{thresh}$] When performing adaptive relabeling, the authors require additional environment-specific Q values, which are not available in standard experimental settings and may limit the practicality of PEAR.\"**\n\nResponse:\n\nThe proposed approach does require setting $Q_{thresh}$ hyper-parameter but we empirically found that the performance is not unstable with varying hyperparameter value. Intuitively, this is a reasonable trade-off that renders the higher level subgoal generation a self-supervised process, thereby eliminating the need for expert guided annotations for segmenting subgoals. Additionally, please note that we empirically found that after we normalize $Q_{\\pi^L}$ values of a trajectory, the $Q_{thresh}$ hyper-parameter value of $0$ works consistently well across all environments, which exhibits that setting $Q_{thresh}$ value does not cause significant overhead. We have added the hyper-parameter ablation analysis in Appendix A.4 Figure 10 for your consideration. Additionally, in real robotic experiments, we used the default $Q_{thresh}$ value of $0$ and PEAR was able to outperform the baselines in all experiments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236386613,
                "cdate": 1700236386613,
                "tmdate": 1700236386613,
                "mdate": 1700236386613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "52VxWCb2sG",
                "forum": "OZ3syNYe7D",
                "replyto": "ydKW9G1AiG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "content": {
                    "title": {
                        "value": "Further discussion on Q7"
                    },
                    "comment": {
                        "value": "The clarity of the figures in the experiments is still poor in your revision. It is hard for the readers to read the lines and legends."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485423657,
                "cdate": 1700485423657,
                "tmdate": 1700485612141,
                "mdate": 1700485612141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PpkaRKST3S",
                "forum": "OZ3syNYe7D",
                "replyto": "YcQxh0PkpB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "content": {
                    "title": {
                        "value": "Further discussion on Q8"
                    },
                    "comment": {
                        "value": "You define $r_{ex}$ and claim it has been used for high-level learning. Could you point out the exact objective function that using it? As  $r_{ex}$ is not revealed in Eq(1)-(8) in your paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485584452,
                "cdate": 1700485584452,
                "tmdate": 1700485584452,
                "mdate": 1700485584452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xblImgU6UA",
                "forum": "OZ3syNYe7D",
                "replyto": "rBaUB1Jdei",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "content": {
                    "title": {
                        "value": "Further discussion on Q2"
                    },
                    "comment": {
                        "value": "What I want to know is not whether adding some negative samples to the expert data has a huge impact on performance, as this is easy to go through the classifier. What I want to know is, if your expert policy is a RANDOM policy or MEDIUM expert (refer to D4RL datasets), how much does your performance change? Or, how much is your performance improvement over expert policy?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485846294,
                "cdate": 1700485846294,
                "tmdate": 1700485846294,
                "mdate": 1700485846294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sKW7Y6ScR7",
                "forum": "OZ3syNYe7D",
                "replyto": "ydKW9G1AiG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response:"
                    },
                    "comment": {
                        "value": "**\"What I want to know is not whether adding some negative samples to the expert data has a huge impact on performance\"**\n\nResponse:\n\nWe conducted separate experiments where we incrementally added bad/negative demonstrations to the expert demonstration dataset and compared its success rate performance. As expected, the performance degrades when the quality of number of bad demonstrations increases. However, since the approach also uses reinforcement learning along with the imitation learning objective, the performance drop is not significant and the policy is still able to show robustness to bad demonstration trajectories. We have added an ablation study in Appendix Figure 11 for maze navigation, pick and place, rope manipulation and franka kitchen environments (The experiments for bin and hollow environments are still running and we will upload the results soon)."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520707541,
                "cdate": 1700520707541,
                "tmdate": 1700520903810,
                "mdate": 1700520903810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oWspnLecjq",
                "forum": "OZ3syNYe7D",
                "replyto": "ydKW9G1AiG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response:"
                    },
                    "comment": {
                        "value": "**\"You define $r_{ex}$ and claim it has been used for high-level learning. Could you point out the exact objective function that using it? As $r_{ex}$ is not revealed in Eq(1)-(8) in your paper.\"**\n\nResponse:\n\nAs briefly mentioned in Section 3 para 1 in the paper, the overall off-policy reinforcement learning objective is to maximize expected future discounted reward distribution: $ J = (1-\\gamma)^{-1}\\mathbb{E}_{s \\sim d^{\\pi}, a \\sim \\pi(a|s,g), g \\sim G}\\left[r(s_t,a_t,g)\\right]$. \n\nFor the high level policy, $r_{ex}$ will denote $r(s_t,a_t,g)$, which is the the extrinsic reward that the higher level policy receives from the environment. This represents the RL objective $J$ which is used in the joint optimization objectives in  Eqns 5-8. The reviewer is right to point out that we do not explicitly use the term $r_{ex}$ later in the paper which may cause confusion, but we used this term to clearly differentiate between the higher level extrinsic reward $r_{ex}$ (the reward that the higher level policy achieves from the environment), and the lower level intrinsic reward $r_{in}$ (the reward that the lower policy gets from the higher level policy) in the hierarchical setup. We hope that this clarifies the doubt. Please feel free to let us know and we would be happy to further clarify."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521740166,
                "cdate": 1700521740166,
                "tmdate": 1700521802243,
                "mdate": 1700521802243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w07sA1sXf9",
                "forum": "OZ3syNYe7D",
                "replyto": "ydKW9G1AiG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "content": {
                    "title": {
                        "value": "Further discussion on \"\"What I want to know is not whether adding some negative samples to the expert data has a huge impact on performance\"\""
                    },
                    "comment": {
                        "value": "I am a bit confused by the response. I guess the authors ignore the \"not\" term in my question.\n\nI'd like to know the effectiveness of your method in two different cases. \nFirst, when using a RANDOM policy or a MEDIUM expert policy dataset from the D4RL datasets, how does your performance vary? \nSecond, compared to the expert policy, how much improvement does your performance exhibit?\nJust adding some negative samples to the expert data is not expected, as this can easily be done with a classifier or low-weighted when RL training."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573021457,
                "cdate": 1700573021457,
                "tmdate": 1700573049455,
                "mdate": 1700573049455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sCmOWLzzJX",
                "forum": "OZ3syNYe7D",
                "replyto": "8JARQqSK6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "content": {
                    "title": {
                        "value": "The figures still need more refinements"
                    },
                    "comment": {
                        "value": "It seems there might be some misunderstandings regarding the figures' clarity. It would be beneficial to consult the figures in other published papers. The figure appear to be some screenshots, which present several issues: they are blurry, the resolutions are not optimal, and there are noticeable stretching. Also, the error bar's color doesn't match the corresponding mean value's solid line, leading to confusion. Furthermore, the tick labels are difficult to read, negatively impacting the paper's readability and overall quality."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573574769,
                "cdate": 1700573574769,
                "tmdate": 1700573574769,
                "mdate": 1700573574769,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jdom7yEzr8",
                "forum": "OZ3syNYe7D",
                "replyto": "z2bFWvW4Wl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their rebuttal. I would raise my score."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659159433,
                "cdate": 1700659159433,
                "tmdate": 1700659159433,
                "mdate": 1700659159433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f0oCf5Aon0",
            "forum": "OZ3syNYe7D",
            "replyto": "OZ3syNYe7D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a combination of hierarchical reinforcement learning (HRL) and imitation learning (IL) for solving complex long-horizon tasks. A small number of expert demonstrations consisting of sequences of states is assumed to be available. The proposed hierarchy consists of a higher level deciding on subgoals and a lower level trying to achieve these subgoals. Target subgoals are extracted from an expert demonstration by plugging states from the trajectory into the low-level Q-function and using its value as a proxy for reachability. The last state which still has a Q-value above a threshold is selected as subgoal and the procedure, referred to as \u201cprimitive enabled adaptive relabeling\u201d, is repeated starting from this state. This results in a sequence of expert subgoals. This relabeling procedure is repeated periodically to take the improved performance of the lower level into account. The authors furthermore provide a bound for the suboptimality of the learned policies. Experiments in simulated and real robotics environments show good performance of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method for extraction of expert subgoals from expert demonstrations is well motivated and intuitive. The integration of expert demonstrations into HRL frameworks is furthermore a promising research direction as demonstrated by the good empirical results.\n\nThe paper moreover provides a wealth of empirical results in simulated and real robotics environments. The conducted ablation studies add to an understanding of how to choose the hyperparameters of the method."
                },
                "weaknesses": {
                    "value": "Unfortunately, some parts of the paper are difficult to understand or even seem contradictory. For example, in section three it is stated that no actions are part of the demonstrations:\n> Notably, we only assume access to demonstration states and not actions.\n\nHowever, in section 4.2 expert data $(s^f, a^f, s^f_\\text{next})$ is used for imitation learning on the lower level. $a^f$ is explicitly used as an expert action here.\n\nIn Algorithm 1 in line 12 there are furthermore triplets $(s_j, s_w, s_k)$ added to the expert subgoal dataset $D_g^e$ where $k$ runs from the initial to the current goal index. Adding $s_k$ seems to be unnecessary as in section 4.2 only the first two entries in the triplet are actually used for imitation learning. It therefore seems to me that Algorithm 1 could be significantly simplified. In the second to last paragraph of section 4.1 there are furthermore triplets $(s_0^e, s_i^e, a_i)$ sampled from $D_g$ to train the lower level. This is again in contradiction to what was actually added to the dataset $D_g$.\n\nIn section 4.2 BC parameters $\\zeta_L$ and $\\zeta_H$ for the lower and higher parameters are introduced which are distinct from the parameters $\\theta_L$ and $\\theta_H$ of the low- and high-level policies. It is not clear what is meant by that. For example, the BC objective in equation (1) optimizes $\\zeta_H$ but nothing in it actually depends on $\\zeta_H$.\n\nThe theoretical analysis in section 4.3 is unfortunately really difficult to follow. For example, consider the two sentences:\n> Let $\\Pi^H_D$ and $\\Pi^L_D$  be higher and lower level probability distributions which generate datasets $D_H$ and $D_L$ , and $\\pi^H_D$ and $\\pi^L_D$ are policies from datasets $D_H$ and $D_L$ . Although $D_H$ and $D_L$ may represent any datasets, in our discussion, we use them to represent higher and lower level expert demonstration datasets.\n\nSo $\\Pi^H_D$ is a probability distribution which generates a dataset $D_H$ but then $\\pi^H_D$ is a policy from $D_H$. So does $D_H$ consist of policies then? Apparently not because the second sentence says it is representing the expert dataset. So how can $\\pi^H_D$ be from that dataset then? Is it some kind of empirical policy only defined on the data? This is completely unclear at this point.\n\nThere is also the issue of the distribution $\\kappa$. It is not quite clear what it represents but I assume it is either the expert data or the distribution induced by the current policy. This would mean that the factor $\\lVert \\frac{d_c^{\\pi*}}{\\kappa} \\rVert_\\infty$ would almost certainly be infinite or at least ridiculously large which would render the bound completely vacuous.\n\nThere are several claims that PEAR mitigates the non-stationarity issue of HRL (in the introduction and in section 4.1). However, the transitions in the replay buffer of the SAC algorithm used on the higher level are getting outdated when the lower level changes. This is not addressed by PEAR so it seems questionable to me whether the algorithm really mitigates non-stationarity.\n\nWhen it comes to the experiments it is great that many environments have been considered but it seems like the baselines are not suitable for these tasks. The rewards seem to be extremely sparse in the more complex environments which makes it very difficult to make any learning progress without demonstrations. I would therefore suggest to incorporate strong IL baselines into the experiments. It is also not quite clear how the hyperparameters for PEAR and the baselines have been tuned.\n\nIn Figure 5 there are no shaded regions in the second and third row. Does that mean that only one seed was used for these experiments? That might mean the results are too noisy to be able to interpret them properly.\n\nThe overall writing seems unfinished in some parts of the paper. For example, articles are frequently missing or verbs are singular when they should be plural or vice versa. These issues are already present in the first paragraph of the introduction and continue throughout the paper. The plots in figures 3 and 5 and some of the plots in the appendix are furthermore way too small to be legible when printed out.\nThe use of notation is unfortunately not consistent throughout the paper. For example, the threshold for the Q-function during relabeling has been introduced as $Q_\\text{thresh}$ in section 4.1 but in section 5 in the paragraph \u201cAblative analysis\u201d a $D_\\text{thresh}$ makes an appearance. I would assume they refer to the same thing but it is not entirely clear. There are also smaller problems with the notation like $G$ vs $\\mathcal{G}$ for the goal space (which seems to be identical to the state space as states and goals are being subtracted from each other in section 3 but this is not made explicit).\n\nFigure 2 is difficult to interpret as the subgoals appear to be somewhat arbitrary. Perhaps a video would be better suited for demonstrating the improvement of the subgoals over training.\n\nWhile there seems to be enough material for a publication, it is unfortunately not presented in a comprehensible way. Parts of the paper are contradictory and unclear and I therefore cannot recommend it for acceptance in its current form."
                },
                "questions": {
                    "value": "* Why is the lower level referred to as a primitive?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1453/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk",
                        "ICLR.cc/2024/Conference/Submission1453/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698846598598,
            "cdate": 1698846598598,
            "tmdate": 1700676822386,
            "mdate": 1700676822386,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tbQtcu1nPo",
                "forum": "OZ3syNYe7D",
                "replyto": "f0oCf5Aon0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response Part 1/3:"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed, constructive feedback. We address the reviewer's concerns as follows:\n\n**1. \"For example, in section three it is stated that no actions are part of the demonstrations: Notably, we only assume access to demonstration states and not actions. However, in section 4.2, expert data $(s^f, a^f, s^f_{next})$ is used for imitation learning on the lower level. $a^f$ is explicitly used as an expert action here.\"**\n\nResponse:\n\nWhen we train the higher level policies using IRL or BC, the higher level joint optimization procedures (Eqn 1 and Eqn 3) do not require expert actions $a^f$, which is what we initially wanted to convey through the statement. However, expert actions are indeed required when lower level policies are trained, using IRL or BC joint optimization. We understand that our previous statement may lead to confusion for the reader, and thank the reviewer for pointing this out. We have removed the statement and clarified in the paper draft wherever applicable (Section 3 and otherwise).\n\n**2. \"In Algorithm 1 in line 12, there are furthermore triplets $(s_j,s_w,s_k)$ added to the expert subgoal dataset $D^e_g$ where runs from the initial to the current goal index. Adding $s_k$ seems to be unnecessary as in section 4.2, only the first two entries in the triplet are actually used for imitation learning. It therefore seems to me that Algorithm 1 could be significantly simplified.\"**\n\nResponse:\n\nConsidering the triplets $(s_j,s_w,s_k)$, the reviewer is absolutely right that $s_k$ is not used in the joint optimization objectives. Nevertheless, we had added the triplets to the dataset $D^e_g$ so that the dataset transitions are consistent with the replay buffer transition storage format used by Soft Actor critic for off policy reinforcement learning. We could remove the $s_k$ entry from the transitions, which simplifies the notation and improves space complexity, but honestly we are not sure how this would simplify Algorithm 1 significantly. Please feel free to let us know if we missed something and we would be happy to add the improvements.  \n\n**3. \"In the second to last paragraph of section 4.1 there are furthermore triplets $(s^e_0, s^e_i,a_i)$ sampled from $D_g$ to train the lower level. This is again in contradiction to what was actually added to the dataset $D_g$.\"**\n\nResponse:\n\nIn the second to last paragraph of section 4.1, in $Q_{\\pi^{L}}(s^e_0,s^e_i,a_i)$, $s^e_0$ and $s^e_i$ are sampled from dataset $D_g$, whereas $a_i = \\pi^{L}(s^e_{0},s^e_i)$, which is the action generated by the lower primitive policy. We have also shown this explicitly in Algorithm 1 when computing $Q_{\\pi^{L}}(s^e_{init},s^e_i,a_i)$ where $a_i = \\pi^{L}(s^e_{i-1},s^e_i)$.\n\n**4. \"In section 4.2 BC parameters $\\zeta_{L}$ and $\\zeta_{H}$ for the lower and higher parameters are introduced which are distinct from the parameters $\\theta_{L}$ and $\\theta_{H}$ of the low- and high-level policies. It is not clear what is meant by that. For example, the BC objective in equation (1) optimizes $\\zeta_{H}$ but nothing in it actually depends on $\\zeta_{H}$.\"**\n\nResponse:\n\nInitially, we had used $\\zeta_{L}$ and $\\zeta_{H}$ parameters to explicitly depict the policy parameters in the BC objective, but we understand now that this causes confusion, as correctly pointed out by the reviewer. Hence, we have removed the $\\zeta$ variables from the draft and now $\\theta$ are used to consistently depict the policy parameters.\n\n**5. \"So $\\Pi_{D}^{H}$ is a probability distribution which generates a dataset $D_H$ but $\\pi_{D}^{H}$ then is a policy from $D_H$. So $D_H$ does consist of policies then? Apparently not because the second sentence says it is representing the expert dataset. So how can $\\pi_{D}^{H}$ be from that dataset then? Is it some kind of empirical policy only defined on the data? This is completely unclear at this point.\"**\n\nResponse:\n\n$\\Pi_{D}^{H}$ represents some unknown distribution over policies from which we can sample a policy $\\pi_{D}^{H}$. The expert dataset $D_H$ does not consist of policies; rather, it consists of state action transitions. Let us assume that these transitions are generated from an unknown empirical policy $\\pi_{D_H}^{H}$. Thus, if we know $\\pi_{D_H}^{H}$, we can use it to predict actions for the state transitions present in the dataset $D_H$. We hope that this clarifies the confusion."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235810828,
                "cdate": 1700235810828,
                "tmdate": 1700236216006,
                "mdate": 1700236216006,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pugNN4cB4M",
                "forum": "OZ3syNYe7D",
                "replyto": "f0oCf5Aon0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussy:"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe hope our response clarified your initial concerns/questions. We would be happy to provide further clarifications where necessary."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565838590,
                "cdate": 1700565838590,
                "tmdate": 1700565838590,
                "mdate": 1700565838590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eiZd2yFyZa",
                "forum": "OZ3syNYe7D",
                "replyto": "f0oCf5Aon0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
                ],
                "content": {
                    "comment": {
                        "value": "Response to rebuttal\n\nI thank the authors for their detailed answers to my questions and concerns.\n\n1. Ok, great that this point could be clarified.\n\n2. As reviewer hXvK also remarked, Algorithm 1 is quite hard to understand with its four nested four loops. Introducing $w=i-1$ as an additional variable does not seem to help with that nor does adding $s_w$ to $D_g^e$. You are right, removing these parts would not be a significant simplification but it would be a start for making the algorithm more readable (together with the comments you added).\n\n3. I mean the second term where it explicitly says $(s_0^e, s_i^e, a_i) \\sim D_g$ where $a_i$ is used as a low-level action. This seems to be in contradiction to Algorithm 1 which adds triplets of states to $D_g$.\n\n4. Thank you for clarifying the notation. This makes it easier to understand the algorithm.\n\n5. Thank you for the explanation. The formulation in the paper is not in line with what you write here, however. The paper reads \u201cLet $\\Pi^H_D$ and $\\Pi^L_D$  be higher and lower level probability distributions which generate datasets $D_H$ and $D_L$ , and $\\pi^H_D$ and $\\pi^L_D$ are policies from datasets $D_H$ and $D_L$ .\u201d, i.e., it explicitly states that $\\Pi^H_D$ generates a dataset $D_H$ and $\\pi^H_D$ is a policy from $D_H$. This paragraph does not properly introduce these objects but rather mixes them up.\n\n6. Notation (like $\\kappa$) should be introduced properly, in my opinion, even if it is also used in other papers. In the OPAL paper $\\kappa$ is assigned a clear role (the initial state distribution), in Theorem 1 in the submission, it just appears without an explanation. It is not clear to me, why $\\kappa$ as an arbitrary distribution makes sense in this context (and why the importance sampling ratio should be bounded).\n\n7. What is usually meant by \u201cthe non-stationarity issue of HRL\u201d is the changing behavior of the lower level during training. As the proposed algorithm uses the off-policy method SAC on the higher level, it suffers from this non-stationarity issue. Repopulating the subgoal dataset $D_g$ does not mitigate this kind of non-stationarity as it is caused by the lower level, not the higher level. That is not to say that the contribution of how $D_g$ is repopulated is not valuable, but the claim that PEAR mitigates the non-stationarity issue of HRL seems misleading to me.\n\n8. I agree that sparse rewards are highly relevant. I just wanted to point out that the baselines without IL components do not stand a chance with very sparse rewards. It is good, however, that an IL baseline was considered. Thank you for the explanation regarding the hyper parameter tuning.\n\n9. Thank you for the explanation and for adding missing seeds. I agree with Reviewer uEcd that the figures are still not ideal. The color of the shaded regions does not match the color of the mean, for example.\n\n10. to 14. Great, thank you for your effort. I will give some examples where the article is missing. First paragraph of the introduction \u201ca large amount of environment interactions\u201d, \u201cthe higher level policy predicts subgoals\u201d. In the same paragraph this should be plural like this \u201cHRL suffers\u201d. There are many more such small issues in the paper. I know these are small things but they are distracting while reading the paper. I would encourage the authors to try to fix these.\n\nOverall, the proposed algorithm seems interesting to me and the experimental results look good. However, I still have concerns about the presentation of the paper that could not be resolved by the response of the authors. I would appreciate further clarficiations."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583797140,
                "cdate": 1700583797140,
                "tmdate": 1700583959881,
                "mdate": 1700583959881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O8GIqh61qR",
                "forum": "OZ3syNYe7D",
                "replyto": "wmwaIqhqgz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
                ],
                "content": {
                    "comment": {
                        "value": "2. Thank you, I think the readability of Algorithm has been improved significantly compared to the initial version.\n\n3. The notation $D_g$ is clearly used for the higher level expert subgoal dataset. This is evident in line 17 of Algorithm 1 where $D_g^e$ is added to $D_g$. It is also clear from lines 10 and 11 of Algorithm 2 which state that the higher level is trained using $D_g$. It looks to me like the same notation is used for two different things which causes confusion. I would encourage the authors to fix this.\n\n5. Thank you for fixing the explanation of what $\\pi^H_D$ and $D_H$ etc. mean. That makes the theory part more readable.\n\n6. Ok, thank you for explaining the role of $\\kappa$. It is not immediately clear what is gained by using importance sampling here as the $\\lambda_H$ still depends on the distribution of $\\pi^*$ and the infity norm of the importance sampling ratio can be huge.\n\n9. and 10. Thank you for improving the figures and fixing some of the typos.\n\nNow that I have a better understanding of what the notation in the theory part means, I see an issue with the argument. The derivation up to equation (11) just suggests doing IL under the assumption that the expert data is near optimal.\n\nTechnically, equation (10) can be rearranged into equation (12). Nevertheless, equation (12) is actually trivial because $J(\\pi^*) \\geq J(\\pi^H_{\\theta_H})$ by definition of the optimal policy. The additional terms that are subtracted on the RHS of equation (12) are greater or equal to zero which yields equation (12) without any need for the derivation done up to equation (11).\n\nSo the issue I see is this: The theoretical analysis you did just suggests doing IL when the data is good enough. Equation (12) that you take as a motivation to also do RL is unfortunately trivial, and upon closer inspection, simply suggests to do RL. If you can optimize $J(\\pi^H_{\\theta_H})$ with RL, then you don't need IL. The RHS of equation 12 is essentially just a sum of the RL objective, a constant term and the IL objective for the higher level. This sum is ultimately ad-hoc, however. For this reason I am not quite convinced by section 4.3.\n\nI think the proposed algorithm is motivated well on an intuitive level but section 4.3 does not seem to add much to this while appearing a bit misleading."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666323605,
                "cdate": 1700666323605,
                "tmdate": 1700666323605,
                "mdate": 1700666323605,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4AJXjKEZrn",
                "forum": "OZ3syNYe7D",
                "replyto": "O8GIqh61qR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
                ],
                "content": {
                    "comment": {
                        "value": "I have raised my score in response to the improvements to the paper and the additional information provided by the authors. I still have concerns about the presentation, in particular the notation, and the alignment of the theory section with the actual algorithm."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676913678,
                "cdate": 1700676913678,
                "tmdate": 1700676913678,
                "mdate": 1700676913678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tX4ujsGA4Q",
                "forum": "OZ3syNYe7D",
                "replyto": "y1KdTnx6m0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing points 3. and 5. I think they are clear now.\n\nAbout the theory: The crux here is the rearrangement of equation (10) into equation (12) which seems to involve a step that results in a trivial equation. \n\nSince $J(\\pi) - J(\\pi^L_{\\theta_L}) \\geq 0$ we have for the RHS for equation (10), $|J(\\pi) - J(\\pi^L_{\\theta_L})| = J(\\pi) - J(\\pi^L_{\\theta_L})$. Hence, rearranging equation (10) should result in an upper bound for $J(\\pi)$ like this: $J(\\pi) \\leq J(\\pi^L_{\\theta_L}) + \\lambda_L\\phi_D + \\lambda_L \\cdot (\\ldots)$ and not in a lower bound like in equation (12). Equation (12) still holds trivially because of $J(\\pi) - J(\\pi^L_{\\theta_L}) \\geq 0$ but it is not actually derived from equation (10) and is also not particularly well motivated because anything greater or equal to zero could be subtracted from $J(\\pi^L_{\\theta_L})$ on the RHS and equation (12) would still hold.\n\nI hope this clarifies my concerns."
                    }
                },
                "number": 43,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737765653,
                "cdate": 1700737765653,
                "tmdate": 1700737765653,
                "mdate": 1700737765653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]