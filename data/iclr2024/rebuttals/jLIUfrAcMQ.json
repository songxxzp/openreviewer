[
    {
        "title": "Debiasing Attention Mechanism in Transformer without Demographics"
    },
    {
        "review": {
            "id": "DfIZgxXri6",
            "forum": "jLIUfrAcMQ",
            "replyto": "jLIUfrAcMQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims at the fairness issue when deploying models. This paper presents an approach to debiasing transformers using their inherent structure. The authors propose some methods to handle the queries, keys and values to reduce the bias. Also, the memory efficiency in the training phase is enhanced."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The overall writing is clear. \n2. The problem of fairness issue when deploying models is important."
                },
                "weaknesses": {
                    "value": "The biggest problem is the experiment settings and results.\n\n1. In the experiment tables, which method belongs to \"Fairness without Demographics\", which method requires Demographics?\n2. The proposed method cannot beat SOTA methods. For example, in Table 2, the proposed method is worse than LfF on EOp dataset. In Table 4, the proposed method is worse than JTT method on EOp and EOd dataset.\n3. Following problem 2, the authors may claim that they have much less energy consumption. However, the comparison is not straightforward. The authors need to show one of the two results to claim this point: 1) same energy consumption and higher accuracy; 2) same accuracy and less energy consumption. If the authors can show these results compared to LfF on EOp dataset, and JTT method on EOp and EOd dataset. I will raise my score.\n4. Energy consumption is not a stable indicator when comparing the models. The hardware may have big influence. I would recommend to use FLOPs to compare these methods."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6565/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6565/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6565/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739665283,
            "cdate": 1698739665283,
            "tmdate": 1700635296310,
            "mdate": 1700635296310,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "paxOgtpnVr",
                "forum": "jLIUfrAcMQ",
                "replyto": "DfIZgxXri6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer Djw2"
                    },
                    "comment": {
                        "value": "The authors would like to thank to the reviewer's for thorough and careful reviewing our work. We appreicate the insightful feedback and suggestions provided. To address the concerns raised, we offer the following clarifications:\n\n>**Q1**: Which method belongs to \"Fairness without Demographics\", which method requires Demographics?\n\n**Response:** We note that for all the methods we compared, none of those utilize demographics in training procedures. We provide detailed illustration on implementation of all methods in Appendix Section C.\n\n>**Q2**: Comprision with SOTA methods.\n\n**Response:** We recognize that our method does not outperform all other methods across every dataset. As shown in Table 2, Learning from Failure (LfF) attains better Equality of Opportunity (EOp) results, albeit at the cost of a roughly $6\\%$ decrease in accuracy compared to the Empirical Risk Minimization (ERM) model. We also observe that the average accuracy of $87.97\\%$ is only marginally higher than the class rate, where $p(y=0) = 84.62\\%$ in the test set. However, our method achieves comparable utility to the ERM model while enhancing group fairness.\n\nIn Table 4, as we discussed in Section 4.2, the significant disparity between the rates $p(a=0)=92.9\\%$ and $p(a=1)=7.1\\%$ leads to an increased theoretical upper bound. As specified in Theorem 1, where $\u03bb_0=p(a=0)$ and $\u03bb_1=p(a=1)$, this upper bound is directly proportional to $\\frac{\u03bb_0}{\u03bb_1}$. This disparity accounting for the performance drop in this test. Although Just Train Twice (JTT) shows superior fairness outcomes, it requires considerably more computational resources compared to other baseline methods.\n\n\n>**Q3**:\nExperiments on the energy controlled setting.\n\n**Response:** In line with the reviewer's recommendation, we have conducted additional experiments with a specific focus on energy consumption. We set a predetermined energy budget and, within this constraint, trained various methods to observe their performance. The other experimental settings remain consistent with those detailed in Table 2 and Table 4 (in the paper). To ensure a fair comparison, we established the energy budget at the mean level of energy consumption observed across all methods. For the CelebA dataset, we allocate a power energy of 363.63 Wh, and for MultiNLI dataset, we allocate a power energy of 2742.78 Wh for BERT large, 867.46 Wh for BERT base. The results could be found in Table 1, 2, and 3.\n\nTable 1: Results of the MultiNLI Experiment under a Limited Energy Budget of 2742.78 Wh (Backbone: BERT Base).\n\n|  | DP \u2193    | EOp \u2193 | EOd \u2193  | ACC \u2191 | Energy   |\n|--------|--------|-------------|-------------|------|-----|\n| JTT  | 47.42 | 12.54| 11.37 | 83.37| 2313.27\n| Ours  | 41.19| 10.15| 8.12| 83.96| 1903.87\n\n\nTable 2: Results of the MultiNLI Experiment under a Limited Energy Budget of 867.46 Wh (Backbone: BERT Base).\n\n|  | DP \u2193    | EOp \u2193 | EOd \u2193  | ACC \u2191 | Energy   |\n|--------|--------|-------------|-------------|------|-----|\n| JTT  | 46.28 | 14.63 | 13.22 | 78.88 | 766.92\n| Ours  | 46.94 | 13.27 | 12.33| 81.38| 684.42\n\n\nTable 3: Results of the CelebA Experiment under a Limited Energy Budget of 363.63 Wh.\n\n|  | DP \u2193    | EOp \u2193 | EOd \u2193  | ACC \u2191 | Energy   |\n|--------|--------|-------------|-------------|------|-----|\n| LfF  | 21.07 | 45.48 |25.92 |92.67 | 258.65\n| Ours  | 14.26| 39.70 | 20.84 | 93.59 | 175.23\n\nTables 1, 2, and 3 indicate that, under a limited energy budget, our method not only achieves improved utility but also yields enhanced fairness results simultaneously. In contrast, under the same energy constraints, the early stopping for JTT impedes its ability to achieve fair results. For LfF (Learning from Failure), the reduced number of iterations improves its utility. However, this comes at the cost of its fairness performance.\n\n>**Q4**:\nHardware influence on Enery metrics.\n\n**Response:**\n(1) We would like to clarify that in this study, we conducted controlled experiments, ensuring that all methods were executed under identical configurations. The number of Floating Point Operations (FLOPs) is used to quantify the total count of elementary machine operations. In line with the reviewer's recommendation, we employed FLOPs as the metric to evaluate all methods. We carry out the experiment on CelebA dataset, with $y=$Blond Hair and $a=$Male. We use the *thop* library to estimate the total training FLOPs.\n\nTable 4: Comparative Analysis of FLOPs Required by Different Methods During the Training Stage\n\n|  | ERM | DRO | ARL | KD| JTT| LfF| Ours\n|-------|---------|----|-----|-------------|-------------|------------|-------------|\n| FLOPs  | $3.10\u00d710^{13}$ | $3.10\u00d710^{13}$ | $5.14\u00d710^{13}$ |  $7.74\u00d710^{13}$ | $1.34\u00d710^{14}$| $8.26\u00d710^{13}$|$4.13\u00d710^{13}$|"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493840208,
                "cdate": 1700493840208,
                "tmdate": 1700493840208,
                "mdate": 1700493840208,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uJIUniMSIQ",
                "forum": "jLIUfrAcMQ",
                "replyto": "paxOgtpnVr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your effort in responding.\nI have a question about the setting in the experiment in the response. Why did the authors choose JTT for the MultiNLI Experiment and LfF for the CelebA Experiment? Does the JTT provide a better energy/accuracy trade-off on MultiNLI than other methods such as *Distributionally robust optimization (DRO) (Hashimoto et al., 2018), Adversarially reweighted learning\n(ARL) (Lahoti et al., 2020), Fairness without demographics through knowledge distillation (KD)\n(Chai et al., 2022), and Learning from failure (LfF)*. If not, please compare the best method that has the best energy/accuracy trade-off."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537543284,
                "cdate": 1700537543284,
                "tmdate": 1700537543284,
                "mdate": 1700537543284,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1APLLDETee",
                "forum": "jLIUfrAcMQ",
                "replyto": "BSOfrtxJ7O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the results. Although the accuracy is not as good as DRO and ARL, other metrics show improvement.\n\nI will raise my score to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635255526,
                "cdate": 1700635255526,
                "tmdate": 1700635255526,
                "mdate": 1700635255526,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wlhZtX3ANV",
            "forum": "jLIUfrAcMQ",
            "replyto": "jLIUfrAcMQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6565/Reviewer_mDe1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6565/Reviewer_mDe1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method to address fairness issues in vision transformers and natural language processing transformers without requiring access to sensitive demographic attributes during training. The key contributions are:\n\n- They identify two sources of bias in transformers: misallocation of attention weights and bias in value vector representations. \n\n- To address attention weight bias, they normalize and take the absolute value of query and key vectors before computing attention. This is motivated by theoretical analysis showing it reduces disparity in attention weights between groups.\n\n- For value vector bias, they use a supervised contrastive loss on the core value vectors to encourage consistency between groups. \n\n- The method is evaluated on vision and NLP tasks, showing improved fairness metrics compared to prior work without demographics. It also enables efficiently debiasing pretrained models by only retraining the last encoder layer.\n\n- Overall, the method provides a simple and effective way to improve transformer fairness without needing sensitive attributes, auxiliary networks, or restrictive constraints. The ablation studies demonstrate tradeoffs between fairness and accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper tackles an important problem - improving fairness in transformers without needing sensitive attributes. This is challenging but highly relevant given privacy regulations. \n\n- The approach of debiasing attention weights and value vectors specifically is novel. Prior work either operates on the full representations or relies on adversarial training, which can be unstable. Deconstructing the transformer in this way is creative.\n\n- The method is simple, leveraging existing operations like normalization and contrastive loss. Avoiding complex auxiliary networks is a plus for efficiency.\n\n- Results on vision and NLP datasets demonstrate improved fairness over prior art like DRO and knowledge distillation. The method also enables efficient debiasing of pretrained models.\n\n- Theoretical analysis provides justification for the attention weight debiasing, and empirically shows the approach achieves near optimal fairness-accuracy tradeoffs."
                },
                "weaknesses": {
                    "value": "- For NLP, the scheme of picking top value vectors may be less effective for long sequences. Dynamic selection based on attention may work better.\n\n- The last layer retraining is convenient but provides no guarantees. Analyzing how bias propagates through the full network could further improve this.\n\n- The contrastive loss operates locally on values. Extending the alignment more globally could potentially improve fairness further without sacrificing accuracy.\n\n- No rigorous ablation study is provided to analyze the individual effects of the attention and value debiasing components. Their relative contributions are unclear."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6565/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699251529928,
            "cdate": 1699251529928,
            "tmdate": 1699636743776,
            "mdate": 1699636743776,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BlITvhzDJY",
                "forum": "jLIUfrAcMQ",
                "replyto": "wlhZtX3ANV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer mDe1"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's dedicated and thorough review of our work, as well as their constructive suggestions for its improvement. We would like to address the following concerns raised:\n\n>**Q1**: For NLP, the scheme of picking top value vectors may be less effective for long sequences. Dynamic selection based on attention may work better.\n\n**Response:** We agree with the reviewer's opinion that dynamic selection could be an effective mechanism for long sequences.  In our current method, we select the top $k$ vectors based on attention weights and concatenate these vectors. Subsequently, these concatenated vectors are transformed into the contrastive space using a nonlinear function $g(\u22c5)$, which is implemented via a Multilayer Perceptron (MLP). However, due to the constrain of the fixed length input of MLP models, incorporating dynamic selection (may vary the number of concatenated vectors) can be very challenging. We consider it a valuable area for future exploration.\n\n>**Q2**:\nThe last layer retraining is convenient but provides no guarantees. Analyzing how bias propagates through the full network could further improve this.\n\n**Response:** For the last layer retraining we integrate our debiasing encoder with a pre-trained model. During the training phase, we freeze the pre-trained model, focusing on training the debiasing encoder and classifier for the downstream task. As the pre-trained model remains frozen during training,  biases inherent in its representations are unavoidable. Our proposed method functions as a debiasing encoder, designed to minimize sensitive information in the final representation. For details, we kindly direct the reviewer to our general response.\n\n\n>**Q3**:\nThe contrastive loss operates locally on values. Extending the alignment more globally could potentially improve fairness further without sacrificing accuracy.\n\n**Response:** Extending contrastive loss to include all patches is effectively the same as applying it to the representation $\\mathbf{z}$. However, it's important to note that directly aligning the representation $\\mathbf{z}$ does not necessarily contribute to fairness. Park et al [1] demonstrate that with the use of Supervised Contrastive Learning (SupCon) to pre-train a ResNet model, unfairness issue still exists.  We attribute this phenomenon to the correlation between the target and sensitive labels. Since the network contrasts the entire representation, it can still leverage sensitive-related information.\n\nIn contrast, our proposed method involves two steps. The first is debiasing the attention weight, a crucial step as it plays a role in selecting high attention patches. The second step involves using screened local values for contrastive learning, focusing on target-related information, thereby enhancing fairness. Our ablation study, detailed in section F of the Appendix, reveals that employing local value alignment alone has a limited impact on promoting fairness.\n\n\n>**Q4**:\nMissing ablation study\n\n**Response:** Kindly refer to our Appendix, we provide an ablation study in section F.\n\nReferences:\n\n- [1] Park, S., Lee, J., Lee, P., Hwang, S., Kim, D., & Byun, H. (2022). Fair contrastive learning for facial attribute classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10389-10398)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493706952,
                "cdate": 1700493706952,
                "tmdate": 1700493706952,
                "mdate": 1700493706952,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hAjEF39pBN",
            "forum": "jLIUfrAcMQ",
            "replyto": "jLIUfrAcMQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6565/Reviewer_sKv2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6565/Reviewer_sKv2"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on the attention mechanism to debias the transformers without assuming access to the sensitive attribute information. For this, two steps have been performed. First, they propose a weight relocation mechanism by normalizing (subtracting mean and dividing by standard deviation) and taking the absolute value of query ank key vectors in the attention module. \nA theoretical insight is also provided to show that this bounds the discrepancy for various sensitive attributes. \nThen, a nonlinear mapping is applied on tokens with higher attention values to map them to a latent representation $v \\rightarrow z$. Then, supervised contrastive learning is applied to the latent representation to make sure that the embedding for the samples from the same class has core similarity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is generally written well and easy to follow, even though some parts are missing (will discuss later).\n\nThe idea of focusing on the attention module and debiasing the transformer without assuming access to the sensitive attributes is really interesting. \n\nAs this approach is not computationally intensive (based on the experimental results provided in section 4) and is not making significant changes to the architecture, it can be easily added during training to most of the transformer-based structures and improve fairness."
                },
                "weaknesses": {
                    "value": "This paper has some weaknesses and I believe addressing these weaknesses can improve the quality of the paper:\n\n1. Intuitively I understand that as the attention mechanism shows the importance of different patches during training/ inference, it can have a large impact on introducing bias. However, authors need to justify in a more systematic way, why using only the attention mechanism is powerful enough to debias a transformer structure. As an example, an analysis in a controlled setup (when we have access to sensitive attributes) can be provided to show that the attention module is the one that mostly affects the bias, or similar experiments to justify this.\n\n2. Similarly, authors need to justify why the optimization objective in Eqn. (2) (minimizing the disparity in attention weights) can be a good approximation to the fairness metrics? I believe there should be an approximation error as instead of considering the output, we are just considering the attention weights. A detailed analysis is required to justify this alternative definition.\n\n3. The statistics of the $q$ and $k$ are estimated during training and then used as an estimation during inference. Authors should provide more details on the accuracy of this choice, as the distribution shift during training and inference might affect both fairness and classifier performance.\n\n4. In section 3.4., more details are required regarding debiasing the pre-trained network by inserting the encoder layer. This part was not clear to me.\n\n5. The experimental results are not convincing enough as a very limited number of combinations for label $y$ and sensitive attributes $A$ are used. \n- Authors should provide various combinations to show the generalizability of the results. \n- In addition, for different datasets, different combinations are used which may give a bad impression of cherry picking.\n- On average, the proposed method is not better than previous approaches and the main benefit is less compute. I wonder wether this approach can be combined with previous methods to give better performance in terms of reducing bias and preventing the degradation in the classifier accuracy?\n\nsmall typo in Figure 1: dotted line $\\rightarrow$ solid line\n\nI am willing to increase my score if the authors provide proper responses."
                },
                "questions": {
                    "value": "please refer to weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6565/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6565/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6565/Reviewer_sKv2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6565/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699522016297,
            "cdate": 1699522016297,
            "tmdate": 1699636743590,
            "mdate": 1699636743590,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LPLBvJxMLA",
                "forum": "jLIUfrAcMQ",
                "replyto": "hAjEF39pBN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer sKv2 (Part 1)"
                    },
                    "comment": {
                        "value": "The authors would like to thank to reviewer sKv2 for your careful review and constructive suggestions. We would like to address the following questions:\n\n\n>**Q1 & Q2**: Justify debiasing only on the attention mechanism is powerful enough. Why the optimization objective in Eqn. (2) can be a good approximation to the fairness metrics?\n\n\n**Response:** Please kindly refer to our general response Q1 for details.\n\n\n>**Q3**: The difference between the training set estimated statistics and test set statistics. Also concern about distribution shift.\n\n\n**Response:** Following the reviewer's suggestion, we run the experiment on CelebA, with the task $y$=Blond hair, $a=$Male. We report $||\\mathbb{E}[\\mathbf{Q}]||_F$, $||\\mathbb{E}[\\mathbf{K}]||_F$, $||\\sigma[\\mathbf{Q}]||_F$ and $||\\sigma[\\mathbf{K}]||_F$ in the test set.  We also report running mean, and running standard deviation estimated in the training set.\n\n\n\nTable 2: Statistics in the test set and estimated statistics in the training set\n\n| $eq$ | $eq_r$ | $diff_{eq}$ \u2193 | $ek$ | $ek_r$ | $diff_{ek}$\u2193  |\n|--------|----------------|--------------|--------------|--------------------------|-------------|\n| 143.91| 142.86 | 23.60 | 180.39 | 176.39  | 14.92  |\n\nwhere $eq$ = $||\\mathbb{E}[\\mathbf{Q}]||_F$,   \n\n$eq_r= ||\\mathbf{e_{\\text{running mean for query}}}||_F$, \n\n$diff_{eq}$ = $||\\mathbb{E}[\\mathbf{Q}]-\\mathbf{e}_{\\text{running mean for query}}||_F$,\n\n$ek = ||\\mathbb{E}[\\mathbf{K}]||_F$, \n\n$ek_r$= $||\\mathbf{e}_{\\text{running mean for key}}||_F$, \n\n$diff_{ek}$ = $||\\mathbb{E}[\\mathbf{K}]-\\mathbf{e}_{\\text{running mean for key}}||_F$\n\n| $sq$ | $sq_r$ | $diff_{sq}$ \u2193 | $sk$ | $sk_r$ | $diff_{sk}$\u2193  |\n|--------|----------------|--------------|--------------|--------------------------|-------------|\n| 231.22 | 233.05| 14.13 | 167.23  | 157.03  | 25.01  |\n\n$sq = ||\\sigma[\\mathbf{Q}]||_F$, \n\n$sq_r$= $||\\mathbf{s}_{\\text{running std for query}}||_F$, \n\n$diff_{sq}$= $||\\sigma[\\mathbf{Q}]-\\mathbf{s}_{\\text{running std for query}}||_F$,\n\n$sk$ = $||\\sigma[\\mathbf{K}]||_F$, \n\n$sk_r$= $||\\mathbf{s}_{\\text{running std for key}}||_F$, \n\n$diff_sk$ = $||\\sigma[\\mathbf{K}]-\\mathbf{s}_{\\text{running std for key}}||_F$.\n\n\n\nIt is important to highlight that in practice, we can only estimate the statistics of the test set based on the running statistics from the training set, a common approach also utilized in batch normalization and layer normalization. Table 3 is presented as a reference to evaluate the quality of this estimation. Our observations from Table 3 indicate that the distribution shift in the CelebA dataset, for the task $y=$Blond Hair, $a=$Male, is relatively minor. We acknowledge that significant deviations between the statistics estimated by the running mean and standard deviation and those in the test set will have an impact on our method's performance, potentially leading to a decreased utility. In fact, the distribution shift is also a challenging problem in the ML community.\n\n>**Q4**: Clarification is needed on the  \"last encoder training\" section.\n\n**Response:** We please reviewer kindly refer to our Q2 in the general response."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493288209,
                "cdate": 1700493288209,
                "tmdate": 1700493288209,
                "mdate": 1700493288209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NQ4qaBq8WF",
                "forum": "jLIUfrAcMQ",
                "replyto": "hAjEF39pBN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer sKv2 (Part 2)"
                    },
                    "comment": {
                        "value": ">**Q5**:\n(1). Limited evalutaion. (2). Concern about task selection (3). Compatibility of existing work.\n\n**Response:**\n- Following the reviewer's suggestion, we have carried out additional experiments on the CelebA dataset, which is notable for its 40 attributes and widespread use in fairness research. Specifically, we focused on tasks commonly explored in fairness studies. For the CelebA dataset, the tasks are $y=$ Wavy Hair $a=$ male [1] and $y=$ Bags under eyes $a=$ Young [2]. The results can be found in Tables 3 and 4.\n- For different datasets, we do not pick a particular task. Instead, we followed the experiments outlined in [3, 4, 5], which are widely recognized in fairness research.\n- We provide an additional experiment to demonstrate the compatibility of our method. We conducted tests combining our approach with the Just Train Twice (JTT), the results are in Table 5. We thank the reviewer for the suggestion and will add full results to our final paper.\n\nTable 3: Classification results on CelebA dataset: $a$= male, $y$ = Wavy hair.\n\n|    | ERM    | DRO    | ARL   | KD    | JTT  | LfF*  | Ours           |\n|--------|----------|---------|--------|-----------|------|------|---------|\n| DP \u2193  | 30.76 \u00b1 1.51   | 34.85 \u00b1 1.43   | 27.67 \u00b1 1.18   | 29.91 \u00b1 3.77   | 29.80 \u00b1 1.45    |   -  | 28.96 \u00b1 2.74   |\n| EOp\u2193         | 40.52 \u00b1 1.80   | 44.63 \u00b1 2.62   | 40.54 \u00b1 1.44   | 41.26 \u00b1 1.13   |  29.16 \u00b1 0.44    |  -  | 34.93 \u00b1 0.51   |\n| EOd \u2193        | 28.14 \u00b1 1.38   | 31.79 \u00b1 1.82   | 27.26 \u00b1 1.23   | 28.02 \u00b1 2.07   | 24.23 \u00b1 0.96     |  -  | 24.82 \u00b1 1.32   |\n| ACC \u2191        | 74.11 \u00b1 0.14   | 73.97 \u00b1 0.32   | 72.11 \u00b1 0.04   | 73.46 \u00b1 0.86   |  69.87 \u00b1 0.36  |   63.60*  | 73.26 \u00b1 0.38   |\n\n*LfF makes all predictions to the same target group. We did a grid search on the learning rate from \\{$10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}$\\} with optimizer \\{$Adam, AdamW$\\}.\n\nTable 4: Classification results on CelebA dataset: $a$ = young, $y$ = Bags under eyes.\n\n|              | ERM            | DRO            | ARL            | KD             | JTT           | LfF  | Ours           |\n|--------------|----------------|----------------|----------------|----------------|---------------|------|----------------|\n| DP \u2193         | 15.70 \u00b1 1.13   | 9.13 \u00b1 0.50    | 7.92 \u00b1 0.85    | 10.83 \u00b1 1.49   | 10.57 \u00b1 0.23  |   8.95 \u00b1 2.46  | 6.34 \u00b1 0.97    |\n| EOp\u2193         | 15.03 \u00b1 1.68   | 6.15 \u00b1 2.35    | 6.97 \u00b1 1.08    | 6.98 \u00b1 1.47    | 2.27 \u00b1 0.88   |  7.75 \u00b1 2.81    | 4.10 \u00b1 0.50    |\n| EOd \u2193        | 11.56 \u00b1 0.72   | 5.35 \u00b1 1.07    | 5.65 \u00b1 0.78    | 6.18 \u00b1 0.92    | 4.71 \u00b1 0.54   |  6.23 \u00b1 2.06    | 3.70 \u00b1 0.41    |\n| ACC \u2191        | 83.15 \u00b1 0.13   | 82.49 \u00b1 0.38   | 81.57 \u00b1 0.24   | 82.96 \u00b1 0.23   | 78.78 \u00b1 0.76  |   81.66 \u00b1 0.47   | 81.80 \u00b1 0.52   |\n\n\nTable 5: Compatible experiment on CelebA: $a$ = Male, $y$ = Blond Hair.\n\n|    | ERM   |   JTT  |Ours| Ours+JTT  |\n|----------|----------------|----------------|---------------|-----------|\n| DP \u2193  | 16.92 \u00b1 0.55  | 18.04 \u00b1 0.32 | 16.34 \u00b1 1.94 | 16.91 \u00b1 0.45  |\n| EOp\u2193   | 42.82 \u00b1 0.47  | 36.17 \u00b1 1.08 | 38.01 \u00b1 1.32   | 28.77  \u00b1 2.44   |\n| EOd \u2193  | 22.90 \u00b1 0.10 | 20.26 \u00b1 1.72  | 20.43 \u00b1 0.68  |  16.47  \u00b1 1.07   |  \n| ACC \u2191  | 94.26 \u00b1 0.06 | 92.75 \u00b1 0.02 | 94.06 \u00b1 0.43  |   91.83  \u00b1 0.91   |\n\nAnalysis of Tables 3 and 4 shows that our method outperforms other baseline models in various tasks. While Just Train Twice (JTT) maintains good fairness, it experiences a slight drop in accuracy. Interestingly, as Table 5 illustrates, combining our method with JTT leads to enhanced outcomes. This improvement is attributed to JTT's ability to modify the distribution among sensitive groups, thereby balancing the data distribution across different groups and potentially reducing the unfairness upper bound as outlined in Theorem 1. Moreover, our method's use of contrastive learning techniques further enhances the utility of the JTT approach.\n\nReferences:\n- [1] Han, Xiaotian, et al. \"FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods.\" arXiv preprint arXiv:2306.09468 (2023).\n- [2] Park, Sungho, et al. \"Fair contrastive learning for facial attribute classification.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n- [3] Liu, Evan Z., et al. \"Just train twice: Improving group robustness without training group information.\" International Conference on Machine Learning. PMLR, 2021.\n- [4] Hong, Youngkyu, and Eunho Yang. \"Unbiased classification through bias-contrastive and bias-balanced learning.\" Advances in Neural Information Processing Systems 34 (2021): 26449-26461.\n- [5] Baldini, Ioana, et al. \"Your fairness may vary: Pretrained language model fairness in toxic text classification.\" arXiv preprint arXiv:2108.01250 (2021)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493470822,
                "cdate": 1700493470822,
                "tmdate": 1700493470822,
                "mdate": 1700493470822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IzF7Uz8dHP",
            "forum": "jLIUfrAcMQ",
            "replyto": "jLIUfrAcMQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6565/Reviewer_JY1q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6565/Reviewer_JY1q"
            ],
            "content": {
                "summary": {
                    "value": "**Rebuttal Update** After reading the author's rebuttal, I improved my score from a 5 to a 6. However, I would like to see the writing and clarity improve for a final paper if accepted. Specifically, the introduction and use of notation needs cleaning, as well as the motivations in the introduction. I would also like a bit more explanation of the contrastive learning method and its motivations in the context of the whole work.\n\nIn this paper, the authors propose a new method for debiasing the attention mechanism to achieve fairness without prior subgroup definitions. Their method consists of two components. First, they normalize the token embeddings in their Query and Key matrices and absolute value them to bound the attention weight difference across sensitive attributes. Second, they use a contrastive loss to encourage the embeddings of samples from the same class to be similar to each other, encouraging equal representation across sensitive attributes while maintaining performance. Then, the authors provide extensive empirical evaluation of their fairness method across two vision and two language tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper builds on a wide literature of debiasing methods, and does a great job of synthesizing methods from fairness aware transformers with methods in fairness without demographics. Furthermore, the method is straightforward and the first part can be applied easily to any attention mechanism, whereas the second requires a small amount of fine tuning of a nonlinear head. The entire system can be applied out of the box to existing models with little or no training. Another thing I really liked about the paper is the green analysis of the power consumption of the method. I think this is a great step forward for researchers in the field."
                },
                "weaknesses": {
                    "value": "One weakness of the paper is the motivation. While debiasing models / fairness is generally a strong motivation, the authors do not explain in much detail why it is important to debias attention mechanisms. Instead, most of the introduction feels like a \"related works\" section, where the main motivation of the paper is the failings of previous methods. I would like to see a better written introduction that explains why the failings of previous methods are bad or costly, and why a new method is needed. \n\nOther parts of the paper are not very clear as well. What motivates the fairness optimization problem (2)? Why do we care about attention weight values when in fairness we traditionally care about outcomes? While having the same activations results in the same outputs of course, it seems to me to be extreme to limit the expressivity of the model across sensitive attributes.\n\nFurthermore, the notation in the paper is pretty messy and unclear. In section 3.1, in the first paragraph the vectors $\\textbf{q}, \\textbf{k}$ are not introduced as slices of the $Q$ and $K$ matrices. Also the relationship between the dataset $\\mathcal{D}$ and how it is inputted into the attention mechanism/transformer model is not mentioned at all (it just jumps straight from dataset notation to attention notation with no connection between the two). In Section 3.2 near the end, what is $q_{cls}$?\n\nI would like to see a better figure explaining the pipeline or mathematical equation of the entire model. $g$ is only mentioned before eqn. (7), but not ever shown visually, this makes it very unclear as to how to implement the second modification and is a bit misleading as we require an additional layer to train to align the model. \n\nFinally, I would like to see an ablation study of the two mechanisms to compare which one impacts fairness more."
                },
                "questions": {
                    "value": "What is the practical/fairness motivation behind the fairness optimization problem, and the motivation behind debiasing attention as a whole? \n\nAlso, it seems as though instead of using subgroup attributes in the contrastive loss, you use classes. However, what happens if each class is dominated by a single sensitive group attribute? For example, if class 1 is all male and class 2 is all female, then requiring all class 1 (male) representations to be similar with each other and different than class 2 (female) will actually cause more disparity by pushing the representations away from each other. Don't we have to assume sensitive groups are balanced within classes?\n\nFinally, just as a curiosity, won't normalizing reduce the expressivity of the model? I would love to see the distribution of attention weights before and after normalizing, as well as before and after absolute valuing (as well as an ablation study of the two steps)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6565/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6565/Reviewer_JY1q",
                        "ICLR.cc/2024/Conference/Submission6565/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6565/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699604370633,
            "cdate": 1699604370633,
            "tmdate": 1700526411564,
            "mdate": 1700526411564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uoanMRKKHc",
                "forum": "jLIUfrAcMQ",
                "replyto": "IzF7Uz8dHP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To reviewer JY1q (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for carefully reviewing our work, also thank you for your valuable suggestions. We want to take this opportunity to address the following concerns:\n\n>**Q1**:\nMotivation: Why is it important to debias attention mechanisms? Why do we need a new method?\n\n**Response:** The Transformer architecture is well-known for its effectiveness, but studies [1, 2] have noted that it is affected by statistical biases. For a detailed discussion on the significance of debiasing attention mechanisms, kindly please refer to the general response to Question 1.\n\n Current methods addressing fairness issues without demographics can be broadly divided based on the use of auxiliary networks. Methods that do not require auxiliary networks include Distributionally Robust Optimization (DRO) [10] and Just Train Twice (JTT) [9], while those employing auxiliary networks including Adversarial Representation Learning (ARL) [3], Knowledge Distillation (KD) [7], and Learning from Failure (LfF) [8].\n\nOur approach is specifically designed to address fairness without demographics and without the need for an auxiliary network, thereby reducing parameters for better efficiency. Regarding existing non-auxiliary network methods, DRO, as pointed out by [3], is likely to be affected by outliers, leading to a drop in performance. JTT, on the other hand, involves a costly double-training procedure, with the second phase incorporating a larger training dataset.\nIn terms of the method for an auxiliary network, Knowledge Distillation (KD) utilizes a teacher model that is often larger than the student network performing the downstream task. Learning from Failure (LfF) alternates optimization between two networks of the same size, leading to parameter inefficiency. Although the auxiliary network in Adversarially Reweighted Learning (ARL) is smaller compared to the main network, its incorporation of adversarial training methods can introduce instability issues. In contrast, our approach obviates the need for such auxiliary networks, thereby enhancing parameter efficiency. Furthermore, it eliminates the necessity for multiple training phases, thus aligning with the demands of time efficiency.\n\n>**Q2**:\nThe motivation behind Eqn (2). Does imposing constraints on attention limit the expressivity of the model?\n\n**Response:** Please refer to our general response of Q1.\n\n\n>**Q3**: Unclear notation. The relationship between input and attention mechanism needs clarification. What is $\\mathbf{q}_{cls}$?\n\n**Response:** We appreciate the feedback from the reviewer. In our work, we denote matrices using bold uppercase letters and vectors with bold lowercase letters. For instance, the vector $\\mathbf{q}$ represents a vector sliced from the matrix $\\mathbf{Q}$.\n\nWe will include additional illustrations to clarify how this vector is inputted into the attention mechanism. Specifically, each input sample first passes through an Embedding layer. Then, it goes a transformation via three linear modules, which map it to $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$, respectively.\n\nFor $\\mathbf{q}_{cls}$, it represents the first token in the sequence, which is utilized for downstream tasks.\n\n\n\n>**Q4**:\nLack of a detailed model illustration and clearer explanation of the function $g$.\n\n\n\n**Response:** We provide an algorithm for the entire model in the revised version. The function $g(\\cdot)$ in our model represents a nonlinear projection head, a component utilized in contrastive learning [4,5]. We follow this protocol and employ $g(\\cdot)$ to map the top attention vectors into the contrastive space. In our implementation, $g(\\cdot)$ is a two-layer MLP with ReLU activation.\n\n\n>**Q5**:\nLack of ablation study.\n\n**Response:** We provide an ablation study in Appendix section F."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492369079,
                "cdate": 1700492369079,
                "tmdate": 1700492369079,
                "mdate": 1700492369079,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bV2TbH2PO7",
                "forum": "jLIUfrAcMQ",
                "replyto": "IzF7Uz8dHP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6565/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To reviewer JY1q (Part 2)"
                    },
                    "comment": {
                        "value": ">**Q6**: Do you have to assume sensitive groups are balanced within classes?\n\n\n**Response:** We do not pre-suppose that sensitive groups are evenly distributed within classes. The reasons are as follows: (1) If we assume sensitive groups are balanced within classes, research by [6] has shown that group unfairness can be significantly mitigated through ERM training. (2) In cases where each class primarily consists of a single sensitive group, samples from minority groups tend to have higher losses. This occurs because these samples are contrasted with those from different sensitive groups. By optimizing this loss, our proposed framework effectively mitigates the disparities in representation among different sensitive groups. (3) However, in the extreme case where class distinctions rely only on sensitive information, our method may not be effective. In such instances, other comparative methods like JTT also fail and work similarly to ERM training.\n\n\n>**Q7**: I would love to see the distribution of attention weights before and after normalizing, as well as before and after absolute valuing (as well as an ablation study of the two steps).\n\n**Response:** We provide a visual attention allocation comparison of our method and ERM in Appendix Section E. We will also provide more ablation results on the distribution of attention weight of the two steps in the revised version.\n\nReferences:\n- [1] Sudhakar, Sruthi, et al. \"Mitigating bias in visual transformers via targeted alignment.\" arXiv preprint arXiv:2302.04358 (2023).\n- [2] Qiang, Yao, et al. \"Fairness-aware Vision Transformer via Debiased Self-Attention.\" arXiv preprint arXiv:2301.13803 (2023).\n- [3] Lahoti, Preethi, et al. \"Fairness without demographics through adversarially reweighted learning.\" Advances in neural information processing systems 33 (2020): 728-740.\n- [4] Khosla, Prannay, et al. \"Supervised contrastive learning.\" Advances in neural information processing systems 33 (2020): 18661-18673.\n- [5] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n- [6] Idrissi, B. Y., Arjovsky, M., Pezeshki, M., & Lopez-Paz, D. (2022, June). Simple data balancing achieves competitive worst-group-accuracy. In Conference on Causal Learning and Reasoning (pp. 336-351). PMLR.\n- [7] Chai, Junyi, Taeuk Jang, and Xiaoqian Wang. \"Fairness without demographics through knowledge distillation.\" Advances in Neural Information Processing Systems 35 (2022): 19152-19164.\n- [8] Nam, Junhyun, et al. \"Learning from failure: De-biasing classifier from biased classifier.\" Advances in Neural Information Processing Systems 33 (2020): 20673-20684.\n-[9] Liu, Evan Z., et al. \"Just train twice: Improving group robustness without training group information.\" International Conference on Machine Learning. PMLR, 2021.\n-[10] Hashimoto, Tatsunori, et al. \"Fairness without demographics in repeated loss minimization.\" International Conference on Machine Learning. PMLR, 2018."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492411270,
                "cdate": 1700492411270,
                "tmdate": 1700492411270,
                "mdate": 1700492411270,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RDoxPFeDb4",
                "forum": "jLIUfrAcMQ",
                "replyto": "bV2TbH2PO7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6565/Reviewer_JY1q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6565/Reviewer_JY1q"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Reponse"
                    },
                    "comment": {
                        "value": "Thank you for answering my questions!\n\nI appreciate the inclusion of ablation studies as well as empirical justification of why minimizing eqn. (2) improves fairness performance. Furthermore, I appreciate the visualization of attention weights with and without your method to understand how your method impacts performance. \n\nIt is clear to me now that your method directly addresses issues of bias, in that attention mechanisms can be sources of unfairness in decisionmaking systems. In the final paper, I would love to see a representative Figure 1 making it clear that these attention mechanisms can be biased and your method corrects for this bias, similar to the Figure 1 of the Debiased Self Attention paper you cited in your rebuttal. You included it in the Appendix, but I think this will really help the paper's message. \n\nI also appreciate your justification of why using class-labels for contrastive learning does not just proxy demographic attributes. I think including your justification of this in the main paper would be useful, as it would link this second method more closely to your motivation and your overall system.\n\nWith these technical updates, I will improve my score from a 5 to a 6. However, I still think the writing in this paper needs to be cleaned up, specifically the notation (explain the lowercase bold q, k, label q_cls, and connect the setup with the dataset to the transformer architecture section). Furthermore, I would like greater clarity in how your contrastive loss component connects to the system and is motivated by your fairness goals. Finally, I think a lengthier discussion of motivations would greatly improve the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6565/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526276967,
                "cdate": 1700526276967,
                "tmdate": 1700526276967,
                "mdate": 1700526276967,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]