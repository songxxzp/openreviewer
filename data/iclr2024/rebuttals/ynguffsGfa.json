[
    {
        "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes"
    },
    {
        "review": {
            "id": "yAfoPoS79f",
            "forum": "ynguffsGfa",
            "replyto": "ynguffsGfa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the problem of improving ML performance on ultra small tabular datasets via synthetic data augmentation. The authors introduced a method that (1) generates new data by using the small training set as the context and querying an LLM, and then (2) further filters the synthesized data by looking at the learning dynamics of the synthetic data. The authors compare the proposed method to existing methods of data synthesis and augmentation, and show the superiority of the proposed method in the ultra-low data regime (n < 100)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The topic on data synthesis for ultra-small dataset is interesting and has far-reaching consequences.\n- The concepts touched upon along the way, specifically data prior and data quality flags, offer a framework to think about this problem.\n- The writing is clear, and the paper is well organized."
                },
                "weaknesses": {
                    "value": "- The effect and origin of the LLM\u2019s prior are unclear, so it is difficult to confidently attribute the performance gain to in-context learning.\n- The curation method seems to be model-dependent and may intensify the weaknesses of the ML model.\n- Because the method proposed is quite simple to use, I feel we need to know when the methods will break down in order to understand its pitfalls. There is not really anything on that in the paper.\n\n**On the prior:**\nSection 2.1 suggests that the LLM has a strong prior, as it is able to extrapolate synthesis to regions without any training data. The question that immediately follows is whether the in-context learning is flexible enough to learn the nuances in the ultra-small dataset, or will the LLM\u2019s prior overwhelm those nuances? Note that Table 3 shows that performance does not improve much as n increases for LLM models relative to\u00a0 non-LLM models. This observation is consistent with the prior being too strong. Of course, it is also consistent with the in-context learning being very good. I hope the authors could tease these two scenarios apart.\n\nAnother question relates to the scenario of \u201ctruly\u201d scarce data, i.e., data that is really not in the training data of the LLM. Although the authors mention that 4 of the datasets are unlikely to be in the training of the LLM due to their being closed-source, we will need a way to quantify that likelihood to get at the answer. In fact, Table 3 shows that for n=20, most of the non-LLM\u00a0 Uncur methods have worse utility than D_train, whereas most of the LLM Uncur methods have higher utility than D_train. This observation is consistent with LLM having memory of the data. Of course, this is also consistent with good in-context few-shot learning. It will be important to distinguish between memorization and in-context few-shot learning for the truly scarce scenario.\u00a0\n\nFor both questions above, one idea to get at them is to create datasets that are definitely not in the training of the LLM by manually manipulating the column dependencies and marginal distributions of existing data, then see if the results still hold.\n\n**On the curation:**\nIt seems that the curation depends on the model via the f_e(x) (see equations 1 and 2), therefore, the augmentation is optimized for a model and can make the model\u2019s specific inductive biases more pronounced. Some thoughts related to addressing this include: (1) curating based on a ML model and evaluating on many models, (2) include more metrics such as resemblance. I hope the authors could provide a clear rationale for addressing this issue.\n\nThe selection criteria based on high aleatoric uncertainty makes it similar to active learning. This means that the curation will make the model perform better even from random sampling, as evidenced by curation increasing performance for all the methods. It may be worth it to make this connection.\u00a0\n\nLooking at the two criteria, do they not have a 1-to-1 relationship, as [f_e(x)]_y is the only variable in both quantities? That is, are the two criteria equivalent to selecting a range of confidence or a range of uncertainty? Maybe worth clarifying.\n\n**On pitfalls:**\nAs mentioned, demonstration of pitfalls are extra important because the method proposed is very easy to use. Such demonstration will likely also shed light on the previous questions."
                },
                "questions": {
                    "value": "**Questions and comments:**\n- Does the LLM handle continuous variables well?\n- Figure 2: it\u2019s hard to read with solid markers occluding each other.\n- Table 1: what quantity is the performance number?\n- Figure 3: surprisingly smooth, and the dip around 6 samples stands out. What\u2019s the explanation? What is the actual performance after augmentation \u2014 want to check if the gain is substantial?\u00a0\n- Don\u2019t think ICL is ever introduced as an acronym."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The method proposed is very easy to apply, and the paper is framed in terms of using it in fields like medicine. Without a good understanding of the adverse effects of the method, it may unknowingly cause problems."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7282/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE",
                        "ICLR.cc/2024/Conference/Submission7282/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7282/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698253280849,
            "cdate": 1698253280849,
            "tmdate": 1700621480020,
            "mdate": 1700621480020,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e0acXgq3SE",
                "forum": "ynguffsGfa",
                "replyto": "yAfoPoS79f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4AgE [Part 1/5]"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4AgE\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following points in turn and highlight the updates to the revised manuscript. In addition, we have uploaded the revised manuscript. We hope this response alleviates your concerns, but please let us know if there are any remaining concerns.\n\n- (A) Identifying possible pitfalls of CLLM **[Part 2/5]**\n- (B) Priors of LLMs for data augmentation **[Part 3/5]**\n- (C) Details on data curation **[Part 4/5]**\n- (D) Answers to the other questions **[Part 5/5]**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411606871,
                "cdate": 1700411606871,
                "tmdate": 1700411606871,
                "mdate": 1700411606871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6EJo8v13jI",
                "forum": "ynguffsGfa",
                "replyto": "yAfoPoS79f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4AgE [Part 2/5]"
                    },
                    "comment": {
                        "value": "### (A) Identifying possible pitfalls of CLLM\nThank you for raising the point about the possible pitfalls of LLMs. We agree that there are risks when using large language models (LLMs), as explained in Sec. 1 and Sec. 2.2 in our manuscript, especially for sensitive applications like healthcare and finance. This motivated our contribution of post-generation curation as a step towards addressing this issue. That said, we agree more should be done to discuss these challenges of using LLMs in the paper and possible \"pitfalls\".\n\nWe wish to highlight our *'Hardness' Proxy Signal* introduced in **Section 3.2**, which is a way to identify failure cases of data augmentation (i.e. proactively flagging the \"pitfall\"). \n\nIn this part of the paper, we highlight that, given the black-box nature of an LLM, we might not trust generated datasets and specifically those that will lead to poor downstream performance.  The inclusion of our 'hardness' proxy serves as a proactive measure to flag potentially problematic datasets reflecting our commitment to ensuring the quality and reliability of the data used in our models. To build this proxy signal, we directly leverage the output of the curation module, with the rationale stated in _Section 3.2_: $D_{syn}$ should intuitively be considered imperfect if curation discards many of its samples since the number of discarded samples measures the quality of samples with respect to the small but goldstandard $D_{train}$.\n\nThis proxy signal aligns with the results in Table 3, where we show that CLLM struggles with the Adult dataset before curation. **Figure 6** shows that the proportion of discarded samples is the highest for this dataset.\n\nConsequently, this shows a practitioner _could_ have used this proxy signal to flag this failure case of the LLM before using it to train a downstream predictive model.\n\nWe hope this clarifies how we proactively identify pitfalls and failure cases from the augmentation.\n\n\n**Limitations and warning readers.**\nOf course, we note that CLLM is only a first step towards better synthetic data. In practice, we thus agree that the output of generative models, including CLLM, should always be exhaustively evaluated on quality, fairness, and other forms of bias, before application to sensitive areas like healthcare, to avoid any potentially harmful applications. \n\nWe have consulted an academic ethicist to discuss how these ethical concerns should be best communicated. To ensure readers may not inadvertently cause bias and harm in the real world through applying CLLM, we have included the following warning at the end of the introduction:\n\n*LLMs may make errors and may reflect or exacerbate societal biases that are present in their data [R1]. Though the curation in CLLM improves synthetic data quality, it does not directly aim to remove biases. The quality and fairness of generated data should always be evaluated. More research into LLM bias is required before methods like CLLM should be applied to real-world sensitive settings like healthcare and finance.*\n\nFurthermore, we extended the Ethics statement to include this warning and will include the same warning in the code repository.\n\nThank you again for bringing up this important issue and we hope that our clarifications have addressed your concerns. \n\n**UPDATE:** we have updated Section 1 and our Ethics statement in our revised manuscript with a warning."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411673053,
                "cdate": 1700411673053,
                "tmdate": 1700411673053,
                "mdate": 1700411673053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XsAd4OaGrN",
                "forum": "ynguffsGfa",
                "replyto": "yAfoPoS79f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4AgE [Part 3/5]"
                    },
                    "comment": {
                        "value": "### (B) Priors of LLMs for data augmentation\nWe would like to thank the Reviewer for bringing this interesting point on the prior modeled by the LLM. We agree with the Reviewer that two components can be attributed to the good performances of CLLMs: the prior given by the LLM, and the in-context examples present in the prompt.\n\nThe reviewer also rightfully points to possible memorization as the source of performance. We highlight that this aspect of memorization motivated the choice of datasets in our manuscript, with $4$ private datasets. These \"medical datasets\" are proprietary datasets which not in the public domain and hence we have ensured they are not accessible as part of the GPT-4 or GPT-3.5 training corpora which would have originated from web scraping or \"actual release\" of these datasets. Hence, we cannot attribute strong performance on these datasets to memorization but rather due to background knowledge on \"similar data\" or via the in-context examples.\n\nWe thank the Reviewer for the suggestion regarding the prior. We have conducted a new experiment to understand the effect of the LLMs background knowledge (e.g. prior).\n\nWe considered the Covid dataset (private medical dataset, to avoid memorization issues) and generated data with GPT-4 (same as Sec 2.1). We ablate the prompt used in our work (detailed in Appendix B.5), and solely provide one in-context example in the prompt, in order to give the LLM the *minimal* amount of information about the desired structure of the dataset. \n\nThis lack of examples forces the LLM to rely on its own prior (background knowledge), and removes the effect of in-context examples which could be used to build a data model. \n\nWe report the results for CLLM in the following table:\n| In-context samples |  Downstream accuracy  |\n|--------------------|----------------|\n| n=1 (Prior)        | 70.20+-1.60    |\n| n=20               | 73.87+-0.50    |\n| n=40               | 73.95+-0.67    |\n| n=100              | 74.71+-0.34    |\n| $D_{\\mathrm{oracle}}$| 74.6 +- 0.15 |\n\n\n\nFrom these results, we conclude the following:\n\n(1) The LLM prior permits to obtain good downstream performance, but it is outperformed by $\\mathcal{D_{oracle}}$ by a margin of $4.4\\%$. Hence, we cannot solely rely on the prior.\n\n(2) Downstream performance increases as the number of in-context samples increases. This shows it is indeed important to include the in-context examples if we wish to obtain downstream performance close to $\\mathcal{D_{oracle}}$, as the LLM can build a good data model.\n\nThis implies that while the LLM does use background knowledge of similar datasets, it still requires in-context samples to refine its prior by creating a good data model.\n\n**UPDATE**: We included these results in Appendix C.3 of our updated manuscript"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412003232,
                "cdate": 1700412003232,
                "tmdate": 1700412003232,
                "mdate": 1700412003232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a0L3rbjogv",
                "forum": "ynguffsGfa",
                "replyto": "yAfoPoS79f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4AgE [Part 4/5]"
                    },
                    "comment": {
                        "value": "### (C) Details on data curation\n\nWe provide answers to three aspects of data curation: (i) Curation model, (ii) Relation to active learning and (iii) Clarifying difference of criteria/metrics\n\n\n**(i) Curation model**\n\nWe thank the reviewer for the point on curation and the model used to perform it. The curation indeed requires training a model to compute the metrics of confidence and aleatoric uncertainty.\n\nThe suggestion of the reviewer (\"curating based on an ML model and evaluating on many models\") is an approach that we have adopted in our submitted manuscript, and the detailed results are given in **Appendix C.1**. We apologize if this was unclear.\n\nThe curation, on the one hand, was performed by training an XGBoost model, leading to a curated dataset.\n\nOn the other hand, we evaluated the downstream performance with $4$ different types of classifiers namely: XGBoost, Random forest, Logistic regression and Decision Tree, which were trained on the curated dataset.\n\nAs we can see, the conclusion that curation helps improve downstream performance holds for each these various downstream models, as is indicated by the green arrows in Tables 6,7,8,9.\n\n**Update:** We have made this point clearer by adding this discussion in Appendix C.\n\n**(ii) Relation to Active Learning**\nWe also thank the reviewer for bringing up this interesting point about active learning and our curation step. While we have already discussed some differences to active learning in the Extended related work section (Appendix A), we now provide a detailed discussion on how they differ on the uncertainty metrics they use. \n\nActive learning primarily focuses on the iterative process of selecting data samples that, when labeled, are expected to most significantly improve the model's performance. This selection is typically based on criteria such as uncertainty sampling which focuses on **epistemic uncertainty** [R1-R4]. The primary objective is to minimize labeling effort while maximizing the model's learning efficiency. Additionally, active learning would aims to label instances based on epistemic uncertainty where the model struggles to make accurate predictions, yet the samples themselves are _correct_.\n\nIn contrast, CLLMs leverage training dynamics based on **aleatoric uncertainty** and confidence and is designed to discard samples that might jeopardize the downstream accuracy. These samples can be considered to have inherent issues or are erroneous, such as being \"mislabeled\".\n\nTo summarize, in active learning, epistemic uncertainty is used to identify data points that, if labeled, would yield the most significant insights for model training. In our approach, they serve to identify and exclude/filter data points that could potentially deteriorate the model's performance.\n\n**UPDATE:** We have included this discussion on uncertainty metrics in Appendix A in our revised manuscript.\n\n**(iii) Clarifying difference of criteria/metrics**\n\nFinally, let us explain why confidence and the aleatoric uncertainty do not have a 1-to-1 relationship. \n\n Let us consider a setting with $e \\in$ {1,2}, and  \n\n$[f_{1}(x)]_{y}=0.1$ and  \n\n$[f_{2}(x)]_{y}=0.9$ \n\nThen the confidence is equal to $\\frac{0.1 + 0.9}{2} = 0.5$ and the aleatoric uncertainty is equal to $\\frac{0.1 * 0.9 + 0.1 * 0.9}{2} = 0.09$. \n\nConsider an alternative case where:\n\n$[f_{1}(x)]_{y}= 0.5$ and\n\n$[f_{2}(x)]_{y}=0.5$\n\nThe confidence is also equal to $0.5$ but the aleatoric uncertainty is equal to $\\frac{0.5 * 0.5+0.5 * 0.5}{2} = 0.25$. Hence these two examples share the same confidence but differ in their aleatoric uncertainty, which demonstrates why these two notions do not capture the same aspect of the learning dynamics."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412548533,
                "cdate": 1700412548533,
                "tmdate": 1700412548533,
                "mdate": 1700412548533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iWRmbRO0l2",
                "forum": "ynguffsGfa",
                "replyto": "UEwTApEMED",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their response. I especially appreciate the additional experiment with n=1 example and the explanation on differences to active learning. Having said that, I feel most of my concerns are only touched upon but not fully addressed. I still have little knowledge about what the limitation and breaking points of CLLM are. The n=1 results further confirm that the prior of the LLM is overwhelmingly strong and learning to move away from the prior may require MANY examples. Lastly, for the curation, what happens if the curation is done using Logistic regression (instead of XGBosst), and then tested on XGBoost, Random forest, Logistic regression and Decision Tree? In general, I am hoping the authors would demonstrate that the curation model does not taint the curated data with its weaknesses. Using Logistic regression (a linear, less expressive model than XGBoost) as a curation model will better convince me."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455977547,
                "cdate": 1700455977547,
                "tmdate": 1700455977547,
                "mdate": 1700455977547,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DEIamIVvMq",
                "forum": "ynguffsGfa",
                "replyto": "XSjo4utNfy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors' quick response and extra experiment! The result confirms the model-dependent effect of the curation method. I hope this limitation would be mentioned in the paper.\n\nMy remaining concern is about the overwhelming prior. I totally understand that the strong prior is important for the ultra-low regime. But the strong prior is a double-edge sword. Perhaps the downside I am thinking about is clearest illustrated by an example. Suppose covid effects for a particular population is distinct to that for the general population. Suppose further that the LLM DOES NOT know about this particular population because it is truly rare. When we use CLLM to obtain more data, my worry is that ICL query would just generate data for the general population but fails to capture the distinct features of the particular population. In this case, the nuances in this particular population would be overwhelmed by the LLM's prior and harder to learn. This is the kind of situation I hope the authors could directly address."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530884587,
                "cdate": 1700530884587,
                "tmdate": 1700530884587,
                "mdate": 1700530884587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QSAvfppWCg",
                "forum": "ynguffsGfa",
                "replyto": "yAfoPoS79f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response & new experiment"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4AgE\n\n\nThank you for your response!\n\nAs you suggested, we have **updated our manuscript** with the discussion on the choice of the curation model, both in Appendix B.3 and in Section 2.2 in our revised manuscript. \n\nWe agree that the prior encoded in the LLM can be a double-edged sword, but we also have reasons to be optimistic. First, a realistic prior is better than no prior at all, which is the status quo in deep generative modeling for tabular data. Second, the LLM\u2019s prior may not be as strong as you suggest. We have conducted a new experiment to quantify and visualize the strength of the prior, by studying how much the LLM's output distribution adapts to the in-context samples we provide.  More specifically, we show t-SNE plots for data generated by the prior of the LLM ($n=1$), and for added in-context examples of $n=20,40,100$ on the Covid dataset.\n\nThe plots are included in the updated manuscript (Appendix C.3) and at the following link:\n\n**https://i.imgur.com/x00toXC.png**\n\n\n**Interpretation:** \nIn particular, we observe a region in the oracle data that is _not_ captured by the LLM\u2019s prior output (the left part of the leftmost blob, circled in blue in our plot). However, as the number of in-context examples (real data) increases in the prompt of the LLM, we observe that the LLM is steered to generate data that covers this region. \n\nWe investigated this region and noticed that it is associated with the subgroup of people older than $87$ years old, and having many severe comorbidities (e.g. Diabetes, Cardiovascular diseases) and respiratory symptoms. This subgroup, in the Oracle dataset, represents less than $3.5\\%$ of the data, and is completely ignored by the GPT-4 prior. In particular, the prior defaults to more typical old patients in the range of $70$-$80$ years old.\n\nOn the contrary,  as $n$ increases, the LLM is guided by the in-context samples and generates samples from this subgroup, which as you mention are \"rarer\" or different from the general population. \n\nThis demonstrates that the LLM captures the distinct features of this particular region, and hence is _not overwhelmed by the prior_, but instead, the data in the form of in-context samples adapts it, hence aligning the augmented dataset with the ground-truth distribution.\n\n\nAdditionally, similar to the main paper, we assess the synthetic data using the Precision (Quality) and recall (Diversity) metrics [R1] with respect to $D_{\\mathrm{oracle}}$:\n\n| $n_{\\mathrm{samples}}$ in $\\mathcal{D_{train}}$ | Precision |   Recall | \n| ------------------ | ------------------- | ------------------- |\n| Prior ($n_{\\mathrm{samples}}=1$) |   0.29 +-  0.026   |   0.64 +- 0.09        |\n| 20                                 | 0.41+-0.04      | 0.87+-0.03      | 0.74+-0.01     | 0.13+-0.0          | 0.82+-0.01         | 0.66+-0.01         | 0.33+-0.07 | 0.50+-0.03 | 0.59+-0.02 |\n| 40                                 | 0.40+-0.01      | 0.91+-0.01      | \n| 100                                 | 0.42+-0.01      | 0.86+-0.02          |\n\nThese results further validate the adaptation of the LLM to capture the nuances of the data, as more in-context samples are provided.\n\n\n**UPDATE:** We have updated Appendix C.3 which now dissects the role of the prior along two dimensions:\n\n(1) The prior alone cannot match the oracle downstream performance and needs in-context samples [``last update``]\n\n(2) The in-context samples guide the LLM generation and is not overwhelmed by the prior --- allowing us to generate rarer subgroups. [``new update``]\n\n----\n\n_We thank the reviewer for encouraging us to disentangle this important aspect, which has helped us improve the paper!_ \n\n_We hope that our clarifications and new experiment have addressed your concerns, please let us know if there is anything else we could do before the discussion period ends._\n\n\nPaper 7282 Authors\n\n----\n\n[R1] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. Advances in neural information processing systems, 31,2018."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601989896,
                "cdate": 1700601989896,
                "tmdate": 1700604303145,
                "mdate": 1700604303145,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ARsNqVwEnU",
                "forum": "ynguffsGfa",
                "replyto": "QSAvfppWCg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_4AgE"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks to the authors for the direct and insightful response. I feel my main concerns have been addressed adequately, and I am happy to raise my score to 6. \n\nI do want to stress that part of me is still cautious: The method presented in this paper is very simple and seems effective; hence, it could see quick and wide spread. It is from this perspective that I would really like to know all the conditions under which the approach might produce adverse effects."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621423583,
                "cdate": 1700621423583,
                "tmdate": 1700621423583,
                "mdate": 1700621423583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7RV59qlA5K",
                "forum": "ynguffsGfa",
                "replyto": "yAfoPoS79f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response & score increase"
                    },
                    "comment": {
                        "value": "Dear ``Reviewer 4AgE``\n\nThank you for your response and for raising your score. Your suggestions have greatly helped us improve the paper!\n\n\nWe agree that CLLM is only a first step towards better synthetic data. In practice, we thus agree that the output of generative models, including CLLM, should always be exhaustively evaluated on quality, fairness, and other forms of bias, before application to sensitive areas like healthcare, to avoid any potentially harmful applications.\n\nAs mentioned during the discussion period, and based on the reviewer's suggestion, we included the following warning at the end of the introduction to ensure readers may not inadvertently cause bias and harm in the real world by applying CLLM:\n\n>\"LLMs may make errors and may reflect or exacerbate societal biases that are present in their data [R1]. Though the curation in CLLM improves synthetic data quality, it does not directly aim to remove biases. The quality and fairness of generated data should always be evaluated. More research into LLM bias is required before methods like CLLM should be applied to real-world sensitive settings like healthcare and finance.\"\n\nFurthermore, we extended the Ethics statement to include this warning and will include the same warning in the code repository.\n\nWe are optimistic that CLLM will spur further research, including improving the curation mechanism (e.g. for algorithmic fairness or privacy) and developing proxy signals for failure conditions (such as the Hardness proxy in Section 3.2).\n\n---\n\n_Thank you again for bringing up this important issue and thank you again for your time and feedback on the paper!_"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689368158,
                "cdate": 1700689368158,
                "tmdate": 1700699653507,
                "mdate": 1700699653507,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JwWB7I4ygP",
            "forum": "ynguffsGfa",
            "replyto": "ynguffsGfa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the problem of using LLMs to generate tabluar data for medical applications where labeled data are extremely scarse. Specifically, this work considers the \"ultra-low resource\" scenario where the task data is <100 samples. This work proposes to use these samples as example with context information to prompt LLMs to generate synthetic data. The work claims the model is able to leverage its medical and other knowledge to generate usable tabular data. Then, the generated data goes through a data curation process, where the data is examined at multiple checkpoint during the model training process. By throwing out data that is inconsistent to the model or nearby samples, the work shows that the curated synthetic data can help improve model performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem being investigated is clearly of interest. Data generation for low-resource scenarios is highly relevant and could be particularly helpful for disadvantaged or marginal groups or rare scenarios. Data generation and curation are also crucial for improving the fairness of ML/AI systems. The potential of foundation models is especially promising.\n\nThe language is very clear. The paper is beautifully formatted. It is a comfortable read. The paper is well-motivated and well-contextualized.\n\nThe research need is valid and important and the proposed approach is relevant and timely. The idea of post-generation curation is smart. Related work is well introduced and comparisons are thorough"
                },
                "weaknesses": {
                    "value": "My major concerns are\n\n1. I don't see the technical contribution of this work;\n\n- This essentially comes down to the few-shot generation problem, where there is much more existing work beyond tabular data. If it is a traditional NLP task such as generating data for sentiment analysis, there are already abundant success cases. Whether the data supports training a model depends on what model you are training and what you want to achieve.\n\n- With less than 100 data but 30 attributes, it is impossible for LLMs to effectively infer the pattern of data. And I doubt the generation is trivial. I wonder how the proposed generation process compares to just adding random noise to augment existing samples before curation. D_train in many cases is very high, while the baseline performance is significantly worse. This raises doubt on whether the setup is correct. I suspect using small random noise to augment the dataset would perform better than many baselines.\n\n2. The proposed approach has a significant overlook for the risk of biases and untrustworthiness of LLMs. The proposed methods for medical applications pose major ethical concerns.\n\n- I strongly doubt the validity of this approach in medical applications. Specifically, what is the rationale for the generated data to be considered \"correct\" or incorporate the right knowledge or information? **If the LLM's output suggests a high correlation between certain marginal groups and a high prevalence of sexually transmitted diseases (STDs), how do you tell whether this is based on medical publications or social biases? Factual tracing for LLMs is currently known as a very hard problem. Active research is ongoing and there are not yet effective ways to relate the model's output to its training samples.**  Without due effort investigating this issue, using data generated by black-box models as the foundation for building medical applications is irresponsible and poses ethical concerns. Given that the work targets ultra-low resource scenarios, it is especially alerting to associate the risk with historically disadvantaged or marginal groups.\n\n- Without specific treatments, LLMs are generally quite poor in logical/mathematical reasoning. It is often challenging for these models to identify the simplest patterns in input data, which is consistent with the reported no-context generation scenario. For the generation example provided in the appendix, I found it rather concerning. \n\n- GPT models are trained overwhelmingly on web text, which contains a high level of subjective arguments, biases, and ungrounded claims especially for major social topics such as COVID or COVID patients. It could easily incorporate bias between demographic attributes and medical conditions and outcomes. The most concerning part is I don't see discussions on it at all, which makes me worry that the authors may not be fully aware of the tool they are leveraging. Given the seriousness of medical applications, this level of overlook is worrisome."
                },
                "questions": {
                    "value": "- There are existing notions such as \"low-resource\" refers to less than 5k annotated samples and \"strict few-shot\" refers to <=16 labeled samples per class. I'm not aware of the definition of \"ultra-low-resource\". Based on the illustration of this paper, it seems to be < 100 samples. The title suggests ultra-low, but without a definition for it, the abstract and introduction talk about \"low-resources\". **What is ultra-low? Is this an existing notion or is it proposed by this work?** This range of samples seems to be the case that is often considered \"few-shot\".\n\n- To have diversity in the generated samples, I would expect the researchers to look into decoding strategies (such as temperature or sampling). It usually does not work by just \"asking the model to generate diverse samples\".\n\n- Table 3, in \"adult\" row in the first section, the best performer are not marked.\n\n- It is better to provide average performance (with standard deviation) for each method for easy comparison between methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The proposed approach has a significant overlook for the risk of biases and untrustworthiness of LLMs. The proposed methods for medical applications pose major ethical concerns.\n\nI strongly doubt the validity of this approach in medical applications. Specifically, what is the rationale for the generated data to be considered \"correct\" or incorporate the right knowledge or information? **If the LLM's output suggests a high correlation between certain marginal groups and a high prevalence of sexually transmitted diseases (STDs), how do you tell whether this is based on medical publications or social biases? Factual tracing for LLMs is currently known as a very hard problem. Active research is ongoing and there are not yet effective ways to relate the model's output to its training samples.**  Without due effort investigating this issue, using data generated by black-box models as the foundation for building medical applications is irresponsible and poses ethical concerns. Given that the work targets ultra-low resource scenarios, it is especially alerting to associate the risk with historically disadvantaged or marginal groups.\n\nWithout specific treatments, LLMs are generally quite poor in logical/mathematical reasoning. It is often challenging for these models to identify the simplest patterns in input data, which is consistent with the reported no-context generation scenario. For the generation example provided in the appendix, I found it rather concerning. \n\nGPT models are trained overwhelmingly on web text, which contains a high level of subjective arguments, biases, and ungrounded claims especially for major social topics such as COVID or COVID patients. It could easily incorporate bias between demographic attributes and medical conditions and outcomes. The most concerning part is I don't see discussions on it at all, which makes me worry that the authors may not be fully aware of the tool they are leveraging. Given the seriousness of medical applications, this level of overlook is worrisome."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7282/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr",
                        "ICLR.cc/2024/Conference/Submission7282/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7282/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782273726,
            "cdate": 1698782273726,
            "tmdate": 1700721844782,
            "mdate": 1700721844782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v8GBD4mX4N",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nkcr [Part 1/5]"
                    },
                    "comment": {
                        "value": "Dear Reviewer Nkcr,\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following points in turn and highlight the updates to the revised manuscript. In addition, we have uploaded the revised manuscript. We hope this response alleviates your concerns, but please let us know if there are any remaining concerns.\n\n- A) Risk of biases and untrustworthiness of LLMs **[Parts 2,3/5]**\n- B) Technical contributions of CLLM **[Part 4/5]**\n- C) Answers to the other questions  **[Part 5/5]**"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411848182,
                "cdate": 1700411848182,
                "tmdate": 1700411848182,
                "mdate": 1700411848182,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XX3KcN3GZH",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nkcr [Part 2/5]"
                    },
                    "comment": {
                        "value": "### (A) Risk of biases and untrustworthiness of LLMs\nThank you for raising this important concern. We agree that there are risks of bias, untrustworthiness or noisy generation when using large language models (LLMs), as explained in Sec. 1 and Sec. 2.2 in our manuscript, especially for sensitive applications like healthcare and finance. This motivated our contribution of post-generation curation as a step towards addressing this issue. That said, we agree more should be done to discuss these challenges of using LLMs in the paper.\n\nWe also believe that this should not prohibit research involving LLMs in these areas. In particular, the contribution of this work is the use of curation for improved synthetic data quality. Thereby we aim to improve existing LLM-based generation methods like baseline GReaT, published at ICLR 2023, which generates synthetic data through LLMs without any curation or bias considerations. Similarly, for other generative models considered in our paper. Though CLLM does not target bias directly, let us elaborate on how it can already help *reduce* bias in baselines:\n\n1. **CLLM can reduce discriminatory correlations.** Let us assume the LLM is biased, e.g. an LLM produces unfair correlations due to historical bias in the LLM's training data. If $D_{train}$ is unbiased, the CLLM curation step will aim to filter out this bias: the curation step analyzes each generated sample based on its predictive confidence and uncertainty with respect to a model trained on **real data** $D_{train}$, hence filters out correlations that do not fit the $D_{train}$ distribution (including discriminatory correlations in the LLM output). Specifically, Figure 5 in our manuscript shows the value of curation to align _any_ synthetic data with the correct true data $P(y|x)$, i.e. feature-label relationships. Hence, a key consideration for the practitioner is carefully construction of $D_{train}$.\n2. **CLLM can reduce representational bias in real data.** Additionally, we show that CLLM improves the quality of synthetic data for underrepresented groups in the population. This hints at the possibility of using CLLM to remove representational bias---i.e. we could increase the $D_{train}$ with synthetic CLLM data from underrepresented groups. As we show quantitatively in Table 1 and Table 2 in our manuscript, data generated with CLLMs has a better alignment with the ground-truth distribution, compared to using a conventional generative approach (TVAE) --- already widely used in practice. \n\n\nAdditionally, in terms of responsible validation, we want to highlight the \"train on synthetic, test on real (TSTR)\" approach adopted in our paper (see Table 3) --- a well-established approach in the synthetic data literature. This method serves as a crucial validation step for our curation process. By training a downstream model on the curated synthetic data and subsequently testing on real-world data, we establish a practical \"gold-standard\" benchmark to assess whether our augmented data reflects the same properties as real data. i.e. If our curation process failed to adequately filter out biased or inaccurate samples, this would be reflected in poor performance when the models are tested on real data. We show across multiple real-world datasets in Table 3 that curation greatly improves TSTR performance over a vanilla LLM and in fact mirrors real-world Oracle data performance. This implies the curation step reflects the properties of the real data."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411876921,
                "cdate": 1700411876921,
                "tmdate": 1700411876921,
                "mdate": 1700411876921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gk1LqolMiW",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nkcr [Part 3/5]"
                    },
                    "comment": {
                        "value": "**Limitations and warning readers.**\nNonetheless, we agree that CLLM is only a first step towards better synthetic data and more can be done to build on top of it. For instance, it does not target fairness directly, e.g. if training data $D_{train}$ contains bias, this will be reflected in the curated data. Similarly, the filtering out of harmful correlations will generally not be perfect. In practice, we thus agree that output of generative models, including CLLM, should always be exhaustively evaluated on quality, fairness, and other forms of bias, before application to sensitive areas like healthcare. \n\nWe have consulted an academic ethicist to discuss how these ethical concerns should be best communicated. To ensure readers may not inadvertently cause bias and harm in the real world through applying CLLM, we have included the following warning at the end of the introduction:\n\n*LLMs may make errors and may reflect or exacerbate societal biases that are present in their data [R1]. Though the curation in CLLM improves synthetic data quality, it does not directly aim to remove biases. The quality and fairness of generated data should always be evaluated. More research into LLM bias is required before methods like CLLM should be applied to real-world sensitive settings like healthcare and finance.*\n\nFurthermore, we extended our previous Ethics statement which covered consultation with stakeholders, to additionally include this warning and will include the same warning in the code repository.\n\nThank you again for bringing up this important issue and we hope that our clarifications have addressed your concerns. If you have any other suggestions for reducing possible harm, please do let us know.\n\n**UPDATE:** we have updated Section 1 and our Ethics statement in our revised manuscript with a warning."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411897691,
                "cdate": 1700411897691,
                "tmdate": 1700411897691,
                "mdate": 1700411897691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OiJsvz6sO9",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nkcr [Part 4/5]"
                    },
                    "comment": {
                        "value": "### (B) Technical contributions of CLLM\n\nOn technical contributions, we discuss the two dimensions mentioned by the reviewer: (i) Contrast to NLP augmentation and (ii) Comparison to (random) noise perturbation.\n\n**(i) Contrast to NLP augmentation**\nWe would like to thank the Reviewer for bringing up the point about data augmentations in modalities such as NLP. While similar in objectives, we want to stress that data augmentation for tabular data and data augmentation in \"traditional NLP\" are two very different problems that have traditionally been tackled with solutions of different natures. While language models can generate coherent and contextually relevant text, tabular data augmentation is a challenging problem where the difficulty lies in preserving and extrapolating the relational dependencies and statistical characteristics within the data. This is all the more arduous as we posit for the ultra-low data regime ($n_{lab}<100$, see answer below). \n\nFurthermore, these inherent challenges entailed by tabular augmentation in the ultra-low data regime motivate the key technical contribution of CLLM which is **post-generation curation**. We demonstrate in Table 3 across 7 real-world datasets that our curation mechanism plays a key role in improving data augmentation quality and utility and is an aspect overlooked by previous works. In particular, we show that with very few samples, the augmented and curated data generated by CLLM leads to downstream performance close to that of the Oracle baseline. Furthermore, we highlight our curation mechanism tackles an overlooked aspect that not only benefits LLM tabular generation but is also applicable and beneficial to other baseline tabular generation methods. \n\n\n**(ii) Comparison to (random) noise perturbation**\nWe appreciate the suggestion of the Reviewer for an experiment with a *random noise perturbation* baseline. We clarify that we considered this point in our manuscript, but realized that it has not been made clear enough. In our experiments in Sec.  3.1, we use the baseline **SMOTE** [R2], which consists in augmenting the dataset with random interpolations between samples in $D_{\\mathrm{train}}$, and is thus noise injection around a given sample for data augmentation. As we show in Table 3, CLLM outperforms SMOTE by a large margin on almost all the datasets, both with and without the curation. This performance gap highlights the capacities of the LLMs to extrapolate and perform data augmentation, as we explain in Sec. 2.1, and shows that CLLM should be prefered over this baseline."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411922050,
                "cdate": 1700411922050,
                "tmdate": 1700411922050,
                "mdate": 1700411922050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6x5sVcoMW4",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nkcr [Part 5/5]"
                    },
                    "comment": {
                        "value": "### (C) Additional questions\n\n\n * \"What is the ultra-low data regime?\n \n We apologize if this point has not been made clear enough in our submitted manuscript. We used the words \"ultra-low\" for settings where $\\vert D_{\\mathrm{train}}\\vert = n_{lab} < 100$, and updated our manuscript accordingly. \n \n We also want to emphasize that our setting differs from the typical few-shot learning literature, as we are not using the LLMs to issue predictions on given test samples. Instead, we prompt the LLM to generate new samples for data augmentation given a small $D_{\\mathrm{train}}$, where the output is higher-dimensional than for few-shot prediction which only outputs labels and can typically operate with very few samples. \n    \n* Diversity in generated samples & decoding strategies (temperature or sampling)    \n    \nWe desire diversity in the generated samples, defined by the distribution that the LLM is sampling from. This is why we include the instruction of generating diverse samples in the prompt. Indeed, this information is part of the context window of the LLM and hence conditions the distribution that the LLM models, as explained in **Section 2** in our manuscript: the prompt and the in-context examples can be compactly grouped in a vector $\\Phi$, defining a distribution $P_{\\Phi}$.\n\nWe appreciate the Reviewer's suggestion on decoding strategies. We notice that these strategies are not orthogonal to our prompting choices. Indeed, sampling assumes that a given distribution is already specified. Temperature scaling also assumes that a distribution is given, and is a way to modify its entropy in a post-hoc manner. In these two cases, we need to define an initial distribution, which we do via our prompting choices.\n\n* Clarify Table 3 (\"adult row\")    \n    \nWe apologize for not making this point clearer in the manuscript. In Table 3, we bolded the best performer for each augmented dataset when the performance of a downstream model trained on that dataset was better than a model trained on $D_{\\mathrm{train}}$. Hence we did not bold numbers in the \"adult\" row with $n=20$, as the model trained on $D_{\\mathrm{train}}$ outperformed the different augmented datasets. Note, as we show in Sec. 3.2. we can also flag such poor performance proactively using our \"hardness proxy\".\n\n\n* Update to include standard deviation\n\nSince we have many results across many baselines, for clarity we reported average performance. However, we agree with the reviewer and have added the detailed results with standard deviations in Table 10 in our revised manuscript.\n\n**UPDATE:** We have added these results in the updated manuscript Table 10, in Appendix C.2.\n\n\nWe hope this answers your points, please let us know if there are any remaining concerns.\n\n\n----\n\n### References\n[R1] Li, Hanzhou, John T. Moon, Saptarshi Purkayastha, Leo Anthony Celi, Hari Trivedi, and Judy W. Gichoya. \"Ethics of large language models in medicine and medical research.\" The Lancet Digital Health 5, no. 6 (2023): e333-e335.\n\n[R2] Fern\u00e1ndez, Alberto, et al. \"SMOTE for learning from imbalanced data: progress and challenges, marking the 15-year anniversary.\" Journal of artificial intelligence research 61 (2018): 863-905."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411944548,
                "cdate": 1700411944548,
                "tmdate": 1700412485167,
                "mdate": 1700412485167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U0OSo1xKUW",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors and Reviewer AZSZ"
                    },
                    "comment": {
                        "value": "I thank the authors for the detailed response and I personally appreciate this informative discussion. I also appreciate other high-quality reviews this work enjoyed. I would like to make more input into this constructive discussion. I apologize for my limited bandwidth. So please forgive me for not being able to write a more formal response. \n\n1. Regarding the tabular data generation question raised by Reviewer AZSZ, no, I do not know better about how to effectively generate data in low-resource scenarios. But I think this is orthogonal to my comment on this manuscript. I included in my review that this problem is very important and timely. My concern is I do not see substantial **technical insights** offered by this work and I also have doubts about the experiment results. \n\n2. On technical contribution, my concerns are two-fold. First, this work does not provide information gained on techniques with language models. As I mentioned in my review, I am aware of the difficulty with generating training data with LLMs in some other tasks. [1] is an example for reference. As can be seen, even for the simplest task of generating data for sentiment analysis, which might be the most natural and suitable case for LLMs, it takes substantial expertise to tune multiple aspects of the model/decoding/promoting to achieve good results. Given that inferring patterns from numerical data seems a much harder task, I would expect work contributing to this topic\u2013even not to go further\u2013to at least have some technical insights for the generation process. I think it will have a substantial effect on the quality of the data generation pipeline built on top of this. This work left out this part completely. \n\n[1] Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning\n\nFor post-generation curation, the idea of incorporating this stage into the data processing pipeline is favorable. However, this data curation technique by itself isn't particularly novel compared to existing work dedicated to data evaluation/curation (e.g., [2]). To my simple understanding, the curation process is to examine the consistency of the samples with other data or the model's prediction and track through model checkpoints.\n\n[2] TRIAGE: Characterizing and auditing training data for improved regression\n\nSo, my afterthoughts of reading this paper are\u2013this is an interesting topic, this is a pioneer work. It is a bit unfortunate the authors cannot go further into the data generation process and share some insights into techniques with language models, which would be really desirable.\n\n3. My doubt remains about the experiment results. SMOTE is different from augmentation with random noises. SMOTE is often not very effective in populating insufficient data and often leads to worsening performance. So I am not surprised SMOTE performance is somewhat lower\u2013it is often the case. **My concern is most of the baselines listed here achieve a worse performance than D_train\u2013without any generation/curation at all.** So I wonder if these are valid baselines or results. At the same time, the performance improvement from CLLM is marginal. I think with careful augmentation of D_train and proper curation, it is possible to match the performance of CLLM. This goes back to my point in the beginning\u2013I do not see the technical insights with LLM I think are needed for the generation process to succeed (in a meaningful way). \n\nConditioning on my prior doubt about the factuality of LM-generated contents (I worked on fact tracing/attribution for generative models),  I don't have enough confidence that the data is generated in the way claimed in this paper. \n\nAs much as I respect the hard work of the authors, I don't have the evidence I need to support this work.\n\n4. My previous score of 1 is for the lack of discussion on ethical concerns, which I perceive as a critical issue. Now this has been appropriately resolved, I raise my score to 5.\n\nI am not passionate about supporting this work for publication at its current stage and I am not passionately against it, either. This is a decent work and I appreciate the authors' and reviewers' hard work and dedication. My honest opinion is I would like to see work on language technology to go beyond intuitive prompting."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630933376,
                "cdate": 1700630933376,
                "tmdate": 1700631808816,
                "mdate": 1700631808816,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2n5rUl3QGI",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response and score increase [Part 1/2]"
                    },
                    "comment": {
                        "value": "Dear ``Reviewer Nkcr``,\n\n\n\nThank you for engaging with us and for your time in the review process! We are glad to have addressed your concerns about the ethical considerations and want to thank you for helping us improve the paper on this important aspect. \n \n \nIn this response we aim to achieve two things: to answer the points in your last response, but also to make the case for you to consider moving from a 5 to a 6 \ud83d\ude42.\n\nThank you for your honesty in saying you are _\"not passionately against [publication]\"_. We would like to try and convince you to favour publication (even if not passionately \ud83d\ude09). We agree with you that this topic is _\"highly relevant and [...] particularly helpful for disadvantaged or marginal groups or rare scenarios\"_, and that the _\"research need is valid and important\"_ due to it being understudied by the ML community. We thank you for acknowledging it as an _\"interesting topic\"_ and a _\"pioneer work\"_, yet we acknowledge that technical improvements are possible. **We are optimistic that the publication and simplicity of CLLM will spur further debate and research, including into improving prompts and other curation mechanisms** (e.g. for algorithmic fairness or privacy). We hope this alone makes it a paper worthwhile publishing.\n\n\n \n\nLet us respond to the different points you raised in your last comment.\n\n----\n\n\n### Using LLMs requires expertise and tuning, and [1]\n\n\n\nWe agree that the use of LLMs is not straightforward, that tuning and prompting can have a large influence on the output quality, and that we should have highlighted this better in the paper. Though we find different techniques for tuning and prompting LLMs interesting, it is beyond the focus of our work---i.e. improving LLM synthetic data through data curation. Despite the difficulties of LLM prompting and tuning, we hope to have shown that CLLM's simple prompting approach compares very favourably to existing tabular generative models. Some of the insights our paper _does_ provide with respect to LLM generation are listed below:\n\n(1) **Importance of context:** Figure 4 and Table 2 show that CLLM leverages the contextual information of the in-context samples, which motivates our prompt design.\n(2) **Extrapolation capacities of the LLM:** Figure 2 and Figure 8 show the capacity of the LLM to extrapolate beyond the limited in-context samples.\n(3) **Feature-label relationship is captured accurately by the LLM:** Figure 5 shows that the data generated by the LLM captures the feature-label relationship better than the baselines.\n\n**To clarify to readers that LLMs are not an easy silver bullet, we will add the following to the Discussion's new limitations paragraph:** \n\n>_\"We have shown how CLLM improves data augmentation through curating the LLM. Nonetheless, further improvements may be achieved through different tuning and prompting of the LLM, as evidenced in different domains (e.g. see [1] and (Liu et al, 2023)). Improving LLM tuning and prompting is beyond the scope of our work, but we regard this as a promising avenue for future work.\"_\n\n----\n\n### Curation contribution and comparison to [2]\n\n\n\nWe agree that data curation is a general idea in ML and thank the reviewer for referencing the related work [2]. We agree both approaches draw inspiration from learning theory and leverage the idea of learning dynamics via checkpoints. Otherwise, we would like to argue that [2] and our paper are quite distinct in contribution:\n\n(1) **Problem and aim**: We are (to the best of our knowledge) the first to use curation for data augmentation, whereas [2] focuses on real data selection focussed on improving regressors. Additionally, we compute the training dynamics of $D_{syn}$ with respect to a model trained on a small gold-standard $D_{train}$ to assess quality and remove inconsistent samples, whereas [2] aims to directly operate on and audit $D_{train}$---discarding _real_ samples that it thinks are inaccurate.\n(2) **Types of samples flagged**: We flag samples that have the incorrect $P(y|x)$ with respect to a real dataset, whereas [2] detects samples that would be systematically over or under-estimated by a regressor.\n(3) **Different curation metrics:** We use the confidence and aleatoric uncertainty computed on the training dynamics, whereas the authors of [2] compute a conformal predictive distribution for a regressor---hence is not applicable to our setting.\n\nAs an additional point, we wish to highlight that **[2] has only become available online on October 29th after the ICLR submission deadline.** Regardless, we will include it in our related work.\n\nTo summarize, we wish to highlight that the idea of curation for tabular data *augmentation* has not been explored prior to this work and we hope CLLM will open up further exploration in this area.\n\n``response continues``"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689890028,
                "cdate": 1700689890028,
                "tmdate": 1700699639243,
                "mdate": 1700699639243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DthT8gCP3g",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response and score increase [Part 2/2]"
                    },
                    "comment": {
                        "value": "### Choice of baselines and random noise \n\n**(i) Choice of baselines:** We first clarify our existing baselines, which encompass a broad and representative spectrum of different classes of generative modeling techniques used for tabular data augmentation (Qian et al, 2023): GAN, VAE, Normalizing Flow, LLM, and Score-based modeling, as well as classical augmentation (e.g. SMOTE). As the Reviewer rightfully mentions, the baselines struggle in the ultra-low data regime. This illustrates a key research gap: traditional data augmentation approaches struggle in such ultra-low sample settings, which CLLM addresses with the LLM and its generative capabilities. \n\n**(ii) Random noise baseline:** We thank the Reviewer for the suggestion of the random noise baseline. We performed a new experiment, where we augment the dataset with random additive Gaussian noise. In order to capture the correlations between the different features, we fit a Kernel Density Estimator with a Gaussian kernel and bandwidth given by Scott's rule. We then sample $1000$ points to create an augmented dataset $D_{\\mathrm{syn}}$.\n\nWe report the performance gap between CLLM and this baseline (with and without curation) for the Covid and Compas datasets. The result can be found at the following link: **https://i.imgur.com/iUkTZ5w.png**\n\n\nWe observe that the random noise baseline does not match the performance of CLLM, although the baseline naturally improves as the dataset $D_{\\mathrm{train}}$ grows in size.\n\n**UPDATE**: We include this result in a new Appendix C.5 in our revised manuscript.\n\n---\n\n_We thank you for your time and energy in the reviewing process, please let us know if you have any additional questions._\n\n---\n\n### References\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9), 1-35.\n\nQian, Zhaozhi, Bogdan-Constantin Cebere, and Mihaela van der Schaar. \"Synthcity: facilitating innovative use cases of synthetic data in different data modalities.\" arXiv preprint arXiv:2301.07573 (2023)."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689981942,
                "cdate": 1700689981942,
                "tmdate": 1700699685681,
                "mdate": 1700699685681,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NLBpfGcfPM",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_Nkcr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the response and additional results. I have read this in full. I still have to maintain my standing on this work. As much as I appreciate the authors for their dedication to this work, which is inspiring to me, I cannot go further with my review without the evidence I need to establish scientific confidence. \n\nI am not as familiar with the state-of-the-art performance of tabular data generation methods in this specific \"ultra-low data regime\". I remain a scientific doubt about this result. I decrease the confidence of my review by 1 and leave the evaluation of this part to other reviewers and the AC who have this expertise."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722337568,
                "cdate": 1700722337568,
                "tmdate": 1700722479002,
                "mdate": 1700722479002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1MVyRcXPQv",
            "forum": "ynguffsGfa",
            "replyto": "ynguffsGfa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use strong LLMs such as GPT-4 for tabular data augmentation and generation. The paper also proposes a generated data curation technique based on predictive confidence and aleatoric uncertainty metrics. The paper shows that the resulting method CLLM is able to effectively leverage prior knowledge of LLMs and achieves good experimental performance in low-data regimes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n* The paper proposes an intuitive and well-motivated approach to leverage background knowledge of GPT-4 for informed tabular data generation\n* The experimental results are promising and cover a representative set of baseline tabular data generation methods\n* The idea of generated data curation is very interesting. Discarding samples with both low predictive confidence and data uncertainty improves performance both of the GPT-4-based data generation and of other tabular data generation model\n* The additional insight into GPT-4\u2019s ability to extrapolate to unseen regions of the manifold is interesting, even though the evidence is somewhat anecdotal and based on a TSNE plots.\n* Importantly, the paper shows that LLM-based tabular data generation is most helpful for generation of underrepresented samples which traditionally is a challenge for other tabular data generation methods. \n* The presentation is excellent."
                },
                "weaknesses": {
                    "value": "Weaknesses and Questions:\n* The paper only tests generation with GPT-4. It would be interesting to see if the findings in the paper are consistent across LLMs, for example, including Claude 2 or LLAMAv2 would be helpful.\n* Could you provide an explanation for GPT-4 generated data outperforming D_oracle in the Train-on-synthetic-Test-on-real setting in Table 3?\n* What is your dataset selection logic? It would be helpful to include more datasets from the papers of other tabular data generation baselines.\n* Do the performance gains come solely from the background knowledge of GPT-4 or is GPT-4 also able to build a strong data model? Have you tested CLLM on any datasets where the background knowledge of GPT-4 would not be as useful, for example, on datasets with anonymized features? Such an experiment would help further understand the source of performance gains.\n* Could you explain the utility drop for GPT-4-no-context on 200 samples in Table 2? Why would more data lead to degraded performance? Could this be simply caused by randomness in the results?\n* If the results in Table 2 are indeed volatile, it would be very helpful to include error bars and bold all statistically significant winners. The error bars could be constructed by, for example, running the simulation for multiple seeds or resampling D_train. Right now, for example Recall of 0.89 is bolded for GPT-4 w/context for 40 samples and not bolded for GPT-no-context for 40 samples. This makes the results in Table 2 unconvincing in their current form. Although it is reasonable to hypothesize that including the background information about a dataset is helpful, as I mentioned above, a better validation of that would be helpful.\n* Related, which dataset are the results in Table 2 based on? Are there similar results for other datasets? What about the dataset behind Figure 3 and Table 1?\n* Related work is currently limited and would benefit from including other related papers. While a few examples of tabular data generation methods are included and used as baselines, at least citing other prominent tabular data generation methods such as STaSy[1] would be helpful. Additionally, even though the introduction mentions that the low-data problem is undervalued, a few tabular deep learning works in fact address extreme low-data regimes, some of them also include experiments in the medical domain [2,3,4,5]. For example, [2,3] are tabular transfer learning approaches with experiments in extreme low-data regimes in the medical domain, while [4] and [5] are knowledge-graph-augmented tabular approaches enabling performance improvements in low-data regimes of wide and short datasets. The idea of leveraging a knowledge graph is similar to the idea of using an LLM in that they both rely on prior knowledge. It would definitely be helpful to cite these works, including them in experiments might be tricky because of the knowledge graph construction.\n\n\nReferences:\n\n[1] STaSy: Score-based Tabular data Synthesis, ICLR 2023 (https://openreview.net/forum?id=1mNssCWt_v)\n\n[2] Levin, R., Cherepanova, V., Schwarzschild, A., Bansal, A., Bruss, C.B., Goldstein, T., Wilson, A.G. and Goldblum, M., 2022. Transfer learning with deep tabular models. arXiv preprint arXiv:2206.15306.\n\n[3] Benchmarking Tabular Representation Models in Transfer Learning Settings, NeurIPS 2023 Tabular Representation Learning Workshop (https://openreview.net/forum?id=HtdZSf1ObU)\n\n[4] Margeloiu, A., Simidjievski, N., Lio, P. and Jamnik, M., 2022. Graph-Conditioned MLP for High-Dimensional Tabular Biomedical Data. arXiv preprint arXiv:2211.06302.\n\n[5] Ruiz, C., Ren, H., Huang, K. and Leskovec, J., 2023. Enabling tabular deep learning when $ d\\gg n $ with an auxiliary knowledge graph. arXiv preprint arXiv:2306.04766."
                },
                "questions": {
                    "value": "Please, see the weaknesses section for both weaknesses and questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no ethics concerns"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7282/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7282/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7282/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827845928,
            "cdate": 1698827845928,
            "tmdate": 1700524157082,
            "mdate": 1700524157082,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WT9EbQcB9D",
                "forum": "ynguffsGfa",
                "replyto": "1MVyRcXPQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AZSZ [Part 1/4]"
                    },
                    "comment": {
                        "value": "Dear Reviewer AZSZ,\n\nThank you for your thoughtful comments and suggestions! We give answers to each of the following points in turn and highlight the updates to the revised manuscript. In addition, we have uploaded the revised manuscript. We hope this response alleviates your concerns, but please let us know if there are any remaining concerns.\n\n- A) Background knowledge of LLMs **[Part 2/4]**\n- B) Related work **[Part 2/4]**\n- C) LLM backbone **[Part 3/4]**\n- D) Answers to the other questions **[Parts 3,4/4]**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411658462,
                "cdate": 1700411658462,
                "tmdate": 1700411658462,
                "mdate": 1700411658462,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uUjOeYLQ3g",
                "forum": "ynguffsGfa",
                "replyto": "1MVyRcXPQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AZSZ [Part 2/4]"
                    },
                    "comment": {
                        "value": "### (A) Background knowledge of LLMs\n\nWe would like to thank the Reviewer for the astute point on disentangling the background knowledge of LLMs. As the Reviewer rightfully points out, two components can be attributed to the good performances of CLLMs: the background knowledge of the LLM, and its capacity to build a strong data model. Additionally, the Reviewer asks how would anonymized features affect the results.\n\nLet us tackle in what follows (i) anonymized features and (ii) background knowledge.\n\n(i) **Anonymized features:** Firstly, in terms of anonymized features, let us clarify that our ablation study in **Section 2.1 (page 5)** tackles this point. We assess the importance of contextual information in the prompt, where we test a variant called *prompt w/ no context* which anonymizes the features as feat1, feat2 etc and only provides the numerical in-context examples, without explanation of what these features are. The results in Table 2 (on the Covid dataset) show that the lack of feature information reduces precision and recall and also reduces the performance of a downstream model (utility). This shows the importance of providing contextual information to exploit the potential of the LLM, and hence motivates the prompt that we use in our work.\n\nWe have also run an additional experiment on the Compas dataset, similarly ablating with anonymized features and similarly demonstrating the importance of contextual information in the prompt itself. We report the results in our response below (in **D) Answers to the other questions**) in reference to one of the other Reviewer's questions.\n\n**UPDATE**: We have updated Appendix C.4 to show the result on the Compas dataset.\n\n(ii) **Background information vs ICL to build a data model**:\nWe followed the suggestion of the Reviewer and conducted a new experiment to understand the effect of the LLMs background knowledge (e.g. prior).\n\nWe considered the Covid dataset (private medical dataset, to avoid memorization issues) and generated data with GPT-4 (same as in Sec 2.1). We ablate the prompt used in our work (detailed in Appendix B.5), and solely provide one in-context example in the prompt, in order to give the LLM the *minimal* amount of information about the desired structure of the dataset. \n\nThis lack of examples forces the LLM to rely on its own prior (background knowledge), and removes the effect of in-context examples which could be used to build a data model. \n\nWe report the results for CLLM in the following table:\n| In-context samples |  Downstream accuracy  |\n|--------------------|----------------|\n| n=1 (Prior)        | 70.20+-1.60    |\n| n=20               | 73.87+-0.50    |\n| n=40               | 73.95+-0.67    |\n| n=100              | 74.71+-0.34    |\n| $D_{\\mathrm{oracle}}$| 74.6 +- 0.15 |\n\n\n\nFrom these results, we conclude the following:\n\n(1) The LLM prior permits to obtain good downstream performance, but it is outperformed by $\\mathcal{D_{oracle}}$ by a margin of $4.4\\%$. Hence, we cannot solely rely on the prior.\n\n(2) Downstream performance increases as the number of in-context samples increases. This shows it is indeed important to include the in-context examples if we wish to obtain downstream performance close to $\\mathcal{D_{oracle}}$, as the LLM can build a good data model.\n\nThis implies that while the LLM does use background knowledge of similar datasets, it still requires in-context samples to refine its prior by creating a good data model.\n\n**UPDATE**: We included these results in Appendix C.3 of our updated manuscript \n\nWe thank the reviewer for their suggestion, where this examination of different aspects of the LLM's background knowledge has strengthened the paper.\n\n----\n\n### (B) Related work\n\n\nWe would like to thank the Reviewer for pointing these five references out. We agree with the Reviewer that using a knowledge graph is a relevant way to inject prior knowledge in the low-data regime, even though constructing them is not always straightforward. As such, we thank the Reviewer for rightfully mentioning that the LLM could serve a similar purpose (while being flexible with regard to prior knowledge).  \n\n**UPDATE**: We have included the references in our Related Work section as well as covered them further in the Extended related work section in Appendix A."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411704448,
                "cdate": 1700411704448,
                "tmdate": 1700411704448,
                "mdate": 1700411704448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XEXMQ2jaKG",
                "forum": "ynguffsGfa",
                "replyto": "1MVyRcXPQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AZSZ [Part 3/4]"
                    },
                    "comment": {
                        "value": "### (C) LLM backbone\nWe acknowledge the reviewer's suggestion to test our approach with other large language models such as Claude 2 or LLAMA2.  We agree that exploring the consistency of our findings across various LLMs such as Claude and LLAMA2 is an interesting avenue for future research. We want to highlight that CLLM aims to underscore the broader applicability of using an LLM with post-generation curation for data augmentation and used both GPT 3.5 and GPT-4 as examples. Extending our experiments to include Claude 2 or LLAMA2 could indeed provide additional insights, though we want to clarify that our paper's primary contribution lies in demonstrating and validating the general concept of leveraging LLMs for data augmentation in ultra-low data regimes, as well as highlighting the value of our post-generation curation mechanism of the LLM output, rather than conducting a comparative analysis between different LLMs.\n\n----\n\n\n### (D) Answers to the other questions\n\n* Why does GPT-4 sometimes outperform $\\mathcal{D}_{oracle}$ in TSTR?\n\nAs rightfully noted by the Reviewer, GPT-4 outperforms $D_{\\mathrm{oracle}}$ on the dataset Compas for some settings, in Table 3. We attribute this phenomenon to the inherent noisiness (e.g. label noise) of this dataset, as we also notice that this dataset leads to the worst downstream accuracy out of the 7 datasets in Table 3, on $D_{oracle}$.\n\n* What is your dataset selection logic?\n \nWe wish to clarify our selection of the 7 datasets we use --- both open-source and private datasets.\n\n(1) **Open-source:** Adult, Drug and Compas are widely used open-source datasets used in the tabular data literature. Adult and Drug are both UCI datasets that have been used in many papers, while Compas is part of OpenML [R1]. Our reason for selecting them is that, despite them being open-source, they are highly reflective of domains in which we might be unable to collect many samples --- hence in reality would often be in an ultra-low data regime. \n\n(2) **Private datasets:** We wanted to disentangle the possible role of memorization in the strong performance of the LLM. To ensure the datasets are not in the LLMs training corpus, we selected 4 private medical datasets that need an authorization process to access. Hence, these datasets would not be part of the LLMs training corpus given their proprietary nature and hence would be unseen to the LLM. While the private and unseen aspect was the main motivation, we also wish to highlight that these are real-world medical datasets. Consequently, this allows us to test a highly realistic problem setting. \n\nWe apologize if this was unclear and have attempted to better clarify this in our description of dataset selection in Appendix B.1.\n\n**UPDATE**: We have added this discussion in Appendix B.1 of our revised manuscript."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411745735,
                "cdate": 1700411745735,
                "tmdate": 1700411745735,
                "mdate": 1700411745735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rxboCXazvw",
                "forum": "ynguffsGfa",
                "replyto": "1MVyRcXPQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AZSZ [Part 4/4]"
                    },
                    "comment": {
                        "value": "* Error bars on Table 2\n\n\n\nWe thank the reviewer for the suggestion and have reported the results with error bars across multiple seeds with the data resampled. Please see the table below, which shows results for the Covid dataset.\n\n\n| $n_{\\mathrm{samples}}$ in $\\mathcal{D_{train}}$ | GPT-4 w/ context Precision | GPT-4 w/ context Recall | GPT-4 w/ context Utility | GPT-4 no context Precision | GPT-4 no context Recall | GPT-4 no context Utility | TVAE Precision | TVAE Recall | TVAE Utility |\n| ---------------------------------- | ------------------- | ------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------ | ------ | ------ |\n| 10                                 | **0.41+-0.04**      | **0.87+-0.03**      | **0.74+-0.01**     | 0.13+-0.0          | 0.82+-0.01         | 0.66+-0.01         | 0.33+-0.07 | 0.50+-0.03 | 0.59+-0.02 |\n| 20                                 | **0.40+-0.01**      | **0.91+-0.01**      | **0.76+-0.0**      | 0.11+-0.0          | 0.89+-0.0          | 0.69+-0.0          | 0.27+-0.01 | 0.68+-0.01 | 0.62+-0.03 |\n| 50                                 | **0.42+-0.01**      | 0.86+-0.02          | **0.75+-0.01**     | 0.11+-0.01         | **0.90+-0.01**     | 0.74+-0.01         | 0.39+-0.02 | 0.67+-0.03 | 0.64+-0.06 |\n| 100                                | 0.44+-0.02          | 0.85+-0.02          | **0.75+-0.0**      | 0.08+-0.01         | **0.90+-0.0**      | 0.60+-0.01         | **0.47+-0.0** | 0.73+-0.01 | 0.65+-0.02 |\n\n**UPDATE:** We have updated Table 2 in the revised manuscript with the standard deviations.\n\n\n\n\n\n* Clarifying the dataset for Table 2\n\nThe results in Table 2 are for the dataset Covid. This is the same dataset that we used for Table 1 and Figure 3. We apologize if the mention of Covid as a running example didn't make this clear enough. Hence, we have updated individual captions in the manuscript to clarify this point.\n\n**UPDATE**: We have updated the caption of Table 2 in the updated manuscript to make this clear. \n\n* Additional dataset for an experiment in Table 2 (Ablation of contextual information)\n\nAs suggested we have also included a result for an additional dataset (Compas dataset) to illustrate to ablate the context. The results similarly show the value of the contextual information in the prompt.\n\n| $n_{\\mathrm{samples}}$ in $\\mathcal{D_{train}}$ | GPT-4 w/ context Precision | GPT-4 w/ context Recall | GPT-4 w/ context Utility | GPT-4 no context Precision | GPT-4 no context Recall | GPT-4 no context Utility | TVAE Precision | TVAE Recall | TVAE Utility |\n| ---------------------------------- | ------------------- | ------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------ | ------ | ------ |\n| 10                                 | **0.69**+-0.02      | **0.88**+-0.02      | **0.69**+-0.02     | 0.27+-0.03         | **0.89**+-0.03     | 0.60+-0.03        | 0.43+-0.02 | 0.43+-0.05 | 0.55+-0.04 |\n| 20                                 | **0.70**+-0.0       | **0.92**+-0.01      | **0.65**+-0.03     | 0.31+-0.06         | 0.84+-0.03         | 0.57+-0.01        | 0.54+-0.02 | 0.80+-0.02 | 0.50+-0.04 |\n| 50                                 | **0.69**+-0.02      | **0.89**+-0.02      | **0.69**+-0.01     | 0.34+-0.1          | 0.85+-0.05         | 0.62+-0.01        | 0.60+-0.03 | 0.86+-0.02 | 0.59+-0.03 |\n| 100                                | **0.70**+-0.01      | **0.89**+-0.02      | **0.69**+-0.01     | 0.31+-0.05         | 0.87+-0.03         | 0.58+-0.05        | 0.65+-0.02 | 0.88+-0.01 | 0.63+-0.01 |\n\n\n**UPDATE**: We have included this result in a new Appendix C.4 in our revised manuscript.\n\n\nWe hope this answers your points, please let us know if there are any remaining concerns.\n\n\n----\n\n## References\n[R1] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: networked science in machine learning. SIGKDD Explorations 15(2), pp 49-60, 2013."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411773519,
                "cdate": 1700411773519,
                "tmdate": 1700411773519,
                "mdate": 1700411773519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p94Kg4q8UA",
                "forum": "ynguffsGfa",
                "replyto": "rxboCXazvw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the thorough response"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you very much for your thorough response, additional experiments, ablations, and clarifications. Your experiments disentangling the helpfulness of the background LLM knowledge and ICL provide valuable insight for anyone looking to apply this approach in practice. Thank you for the explanation about D-oracle performance on Compas -- it makes sense. Also, thank you for clarifying your dataset selection logic -- you have a great point about using non-open data to alleviate memorization concerns, please mention it in the main body of the paper.\n\nGiven the cost of GPT-4 and the API access limiting control of reproducibility of its outputs, I still believe it would be interesting to see if smaller open-source LLMs can be leveraged for the data augmentation purpose and how far their performance would be from GPT-4 because it may be beneficial for users of your framework to have complete control over the model and data (for privacy, security and other reasons). However, I agree that there is only marginal value in running Claude experiments and I welcome your point that the paper's primary contribution lies in demonstrating and validating the general concept of leveraging LLMs for data augmentation in ultra-low data regimes (and demonstrating the value of curation) rather than conducting a comparative analysis between different LLMs for this purpose. I'd like to suggest this for your consideration as a direction for your future work and/or your code base on GitHub as this may significantly increase the impact of your work for practitioners. \n\nThank you for providing the error bars for the tables I requested and for providing the error bars for Table 3 in Table 10 of the Appendix. I trust that between the main body and appendix you have now included error bars for all reported results and comparisons of methods.\n\nWith that, I am happy to strongly recommend acceptance and increase my score to 8."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525786596,
                "cdate": 1700525786596,
                "tmdate": 1700525786596,
                "mdate": 1700525786596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rbXowkuHa1",
                "forum": "ynguffsGfa",
                "replyto": "JwWB7I4ygP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7282/Reviewer_AZSZ"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications"
                    },
                    "comment": {
                        "value": "Dear Reviewer Nkcr,\n\nThank you for your points. As I wanted to consider your review in making a decision about my score, I'd like to ask for a few clarifications.\n\n1. Regarding your criticism of the technical contribution of this work and your point that *\"this essentially comes down to the few-shot generation problem, where there is much more existing work beyond tabular data.\"*, would you please be able to provide a reference to an existing work that successfully solves the tabular data augmentation use case in the low-data regime that you believe would be more promising than the method proposed by the authors? \n\n2. Thank you for pointing out the LLM bias concerns. While bias considerations are of paramount importance, this paper proposes a method for tabular data augmentation which helps improve performance on downstream tasks. In fact, the paper shows that underrepresented subpopulations in the dataset benefit the most from the proposed method. Additionally, bias and fairness assessment of models in applications is quite an established and rigorous process with many available definitions of fairness for ML models [1]. While the importance of fairness audit cannot be overstated, the responsibility for final bias and fairness assessment lies with the developers of the downstream models in applications. If a predictive model leverages the data augmentation approach proposed in this paper and passes the fairness audit conducted by its developer (or independent entity), would it be a problem to use such a model just because it leveraged CLLM for data augmentation? Would you argue that any LLM-related paper in general should then be denied publication because of the potential to be misused?\n\nNevertheless, I would like to thank Reviewer Nkcr for bringing up the ethical considerations and I would like to commend the Authors on their excellent response and consulting an academic ethicist on communicating these ethical concerns in the ethics statement of the paper. \n\n**I would like to emphasize to the AC, Reviewer Nkcr, and Authors that I considered this review and especially the discussion on ethical concerns in my decision to recommend acceptance of the paper. In fact, the response of the authors on the ethical concerns was one of the reasons I raised my score. Regarding the criticism of the technical contribution of this work, in the absence of references supporting it, I decided to not deduct any points from my score for now.**\n\n**Question to Reviewer Nkcr:** Given the authors' responses, is 1 still a fair assessment of the quality of this work in your opinion?\n\nThank you!\n\nReferences:\n[1] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K. and Galstyan, A., 2021. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6), pp.1-35."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530177428,
                "cdate": 1700530177428,
                "tmdate": 1700530177428,
                "mdate": 1700530177428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]