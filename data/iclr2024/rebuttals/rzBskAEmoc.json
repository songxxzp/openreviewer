[
    {
        "title": "CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images"
    },
    {
        "review": {
            "id": "JHlqnc7fL2",
            "forum": "rzBskAEmoc",
            "replyto": "rzBskAEmoc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5726/Reviewer_15GF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5726/Reviewer_15GF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multi-instance framework for WSI classification that incorporates the spatial context of patch instances into the architecture of the model via a combination of transformer and neighbor-constrained attention with constrastive learning. The input images are preprocessed into features using a model trained with self-supervised contrastive learning (SimCLR) on pathology images with an ImageNet-pre-trained ResNet18. The transformer module follows the NystroFormer architecture. The attention module is the main contribution of this paper and trains an feature-similarity-weighted adjacency matrix. Experiments are performed on two well-known pathology datasets (CAMELYon-16 and TCGA-NSCLC) for a binary classification task (cancer/non-cancer). SOTA accuracy and AUC are reported for both datasets. An ablation study examines the separate effects of the Nystroformer and the attention module, showing the attention module to provide the largest improvement."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The main idea behind this paper (leveraging neighbor correlations among patches in a transformer framework) is substantiated by domain-grounded intuition.\n* The chosen architecture appears to provide SOTA results on two well-known pathology benchmarks.\n* The ablation study shows the strong effect of the neighbor attention module."
                },
                "weaknesses": {
                    "value": "* The number of datasets used for benchmarking is too small (2). [update: the authors have added the C17 dataset]\n\n* The paper does not discuss data augmentation within MIL training. Several recent papers have explorer ways to inflate the number of bags in MIL with various augmentation scheme.\n\n* The ablation study lacks an experiment showing the contribution of the pathology-specific self-supervised feature-extraction module. It would be easy to compare to a straight imagenet-pretrained ResNet18. [update: the authors have added an ablation study as requested]["
                },
                "questions": {
                    "value": "* The published TransMIL accuracy numbers for CAMELYON16 and TCGA-NSCLC are: (ACC=0.8837, ACC=0.8835). The paper cites: 0.905 and 0.905. I did not verify the other numbers, but please double-check. [update: this question has been answered - the papers uses different feature extractors SimCLR]\n\nOverall, this is a solid paper implementing a grounded intuition using a MIL framework and demonstrating SOTA results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Reviewer_15GF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698099927067,
            "cdate": 1698099927067,
            "tmdate": 1700685127753,
            "mdate": 1700685127753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o5f7d1YxS1",
                "forum": "rzBskAEmoc",
                "replyto": "JHlqnc7fL2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to  Reviewer 15GF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive recognition and constructive criticism. We address the weaknesses and questions defined by the reviewer below:"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432986329,
                "cdate": 1700432986329,
                "tmdate": 1700433024872,
                "mdate": 1700433024872,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b209ode8ct",
                "forum": "rzBskAEmoc",
                "replyto": "JHlqnc7fL2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The number of datasets:"
                    },
                    "comment": {
                        "value": "We have now conducted an experiment using the CAMELYON17 [1] dataset, which consists of 1000 Whole Slide Images (WSI) of a similar type as the CAMELYON16 dataset. However, only half (500 WSI) of these images are labeled and accessible publicly. These images are expertly annotated by pathologic lymph node classification into pN-stage:\n- pN0: No micro-metastases or macro-metastases or isolated tumour cells (ITCs) found.\n- pN0(i+): Only ITCs found.\n- pN1mi: Micro-metastases found, but no macro-metastases found.\n- pN1: Metastases found in 1\u20133 lymph nodes, of which at least one is a macro-metastasis.\n- pN2: Metastases found in 4\u20139 lymph nodes, of which at least one is a macro-metastasis.\n\nAdopting the approach discussed in [2], we created a binary classification problem by treating pN0 as normal and unifying all classes that were not pN0 into a single class, cancerous. CAMELYON17 is a particularly challenging dataset, mainly because of the tumor heterogeneity it exhibits. There are WSIs, where the tumor is highly localized, and others, where only a handful of ITCs, such as those in the pN0(i+) class, are scattered across a sea of normal cells.  We specifically chose to conduct our evaluations on CAMELYON17 as, given the time of the rebuttal period, we were unable to train a new feature extractor on a different dataset. Thus, a reasonable alternative was to use our SimCLR feature extractor trained on cancer metastasis images in CAMELYON16. \n\nWe used the SimCLR-trained feature extractor trained on CAMELYON16. Even though both CAMELYON16 and CAMELYON17 focus on lymph node metastasis detection, and transfer learning is, therefore, a reasonable choice, the two datasets contain different image sets and demonstrate distinct distributions. For example, it is well-known that there is an inevitable variability in H&E staining as a natural result of differing staining procedures and reagents among pathology labs and differences among digital WSI scanners that are being used. CAMELYON16 uses data collected from two centers, while CAMELYON17 uses data collected from five centers. \n\nDespite using an embedder that was not directly trained on CAMELYON17 images, we are pleased to report optimal results, highlighting the adaptability and robustness of our model across diverse datasets. We tested a 4-fold validation reporting the average test ACC and AUC as shown in the Table below:\n\n|                     |CAMELYON17 |                               |\n|---------------------|--------------------------------|--------------------------------|\n| Method              | ACC($\\uparrow$)                | AUC($\\uparrow$)              |\n| CLAM-SB             | 0.802            | 0.849           |\n| CLAM-MB             | 0.803              | 0.858           |\n| TransMIL            | 0.804              | 0.873            |\n| DTFD-MIL            | 0.797              | 0.884            |\n| GTP                 | 0.800          | 0.762            |\n| DSMIL               | 0.815              | 0.863            |\n| **CAMIL-L** | 0.828              | 0.881        |\n| **CAMIL-G** | 0.818              | 0.875            |\n| **CAMIL**   | 0.843   | 0.881 |\n\nCAMIL outperforms all the baseline models in both ACC and AUC metrics, except for DTFD-MIL in AUC with a 0.003 difference. GTP  didn not perform well on this dataset, possibly due to its size, as was the case with CAMELYON16. We suspect that this drop in performance could be attributed to the MinCut pooling [3] operation, which could result in substantial information loss. DTFD-MIL performed well, leveraging the power of pseudo-bags, which could be beneficial when dealing with WSIs containing many tiles. As expected, TranMIL's performance is similar to CAMIL-G, since both models use the same Nystromformer. We presume that the superiority of CAMIL-G performance over TransMIL in accuracy may arise from the distinct aggregation strategies they employ, with CAMIL-G utilizing attention pooling before classifier input. Finally, the amalgamation of both elements in the CAMIL model yields the best performance overall.\n\n[1] P. B\u00e1ndi et al., \"From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The CAMELYON17 Challenge,\" in IEEE Transactions on Medical Imaging, vol. 38, no. 2, pp. 550-560, Feb. 2019, doi: 10.1109/TMI.2018.2867350.\n\n[2] Yufei CUI, Ziquan Liu, Xiangyu Liu, Xue Liu, Cong Wang, Tei-Wei Kuo, Chun Jason Xue, and Antoni B. Chan. Bayes-MIL: A new probabilistic perspective on attention-based multiple instance learning for whole slide images. In The Eleventh International Conference on Learning Representations, 2023.\n\n[3] Bianchi FM, Grattarola D, and Alippi C. Spectral clustering with graph neural networks for graph pooling. In Proc. ICML, 2020, pp. 874\u2013883."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433113507,
                "cdate": 1700433113507,
                "tmdate": 1700503872505,
                "mdate": 1700503872505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i6QIJLi8tL",
                "forum": "rzBskAEmoc",
                "replyto": "JHlqnc7fL2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ablation study on ResNet-18 pre-trained on ImageNet:"
                    },
                    "comment": {
                        "value": "SimCLR feature extraction backbone is an integral part of our model, and it plays a pivotal role in generating concise and descriptive feature representations. Leveraging the pre-trained SimCLR weights lays the foundation for generating meaningful similarity scores and, therefore, is crucial for the optimal performance of the neighbor-constrained attention mask similar to many similarity-based histopathology approaches. Incorporating ImageNet weights directly into our model notably decreases its performance (as it does with most models [1, 2]), emphasizing the necessity of SimCLR within our model architecture. \n\n| Dataset    | ACC               | AUC               |\n|------------|-------------------|-------------------|\n| CAMELYON16 | 0.723\t  | 0.743 |\n| TCGA-NSCLC | 0.692 | 0.798 |\n\n\nWe believe that this decline in performance isn't model-specific but rather feature-specific, consistent with findings from other studies that utilize ResNet-18 pre-trained on ImageNet, such as those in the CAMELYON16 dataset. Works in [1] saw DS-MIL [3] achieve an AUC=0.478 \u00b1 0.130 and ACC=0.478 \u00b1 0.130 with the ResNet18 pre-trained on ImageNet only. Furthermore, works in [2] showed GTP performance on TCGA with the ResNet18 feature extractor pre-trained on ImageNet lead to ACC= 44 \u00b1 0.280 and AUC: 60.4 \u00b1 0.330. Typically, Resnet-50 with ImageNet weights is the backbone model used as the feature extractor.\n\nHowever, we firmly believe this performance drop does not devalue our model's overall capabilities. Furthermore, we must stress that identical features were used across all baselines, ensuring fair model comparisons and enhancing evaluation integrity. Additionally, the availability of publicly available generalizable self-supervised histopathology models further simplifies the extraction of salient features for pathology analysis [4]. \n\n[1] Paul Tourniaire, Marius Ilie, Paul Hofman, Nicholas Ayache, Herv\u00e9 Delingette, MS-CLAM: Mixed supervision for the classification and localization of tumors in Whole Slide Images, Medical Image Analysis, Volume 85, 2023, 102763, ISSN 1361-8415.\n\n[2] Y. Zheng et al., \"A Graph-Transformer for Whole Slide Image Classification,\" in IEEE Transactions on Medical Imaging, vol. 41, no. 11, pp. 3003-3015, Nov. 2022, doi: 10.1109/TMI.2022.3176598.\n\n[3]  Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14318\u201314328, 2021.\n\n[4] Chen RJ, et al. A General-Purpose Self-Supervised Model for Computational Pathology. ArXiv [Preprint]. 2023 Aug 29:arXiv:2308.15474v1. PMCID: PMC10491320."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433406717,
                "cdate": 1700433406717,
                "tmdate": 1700434545986,
                "mdate": 1700434545986,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "88OXfJiuch",
                "forum": "rzBskAEmoc",
                "replyto": "JHlqnc7fL2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to question on TransMIL reported metrics"
                    },
                    "comment": {
                        "value": "> Q1: *\"The published TransMIL accuracy numbers for CAMELYON16 and TCGA-NSCLC are: (ACC=0.8837, ACC=0.8835). The paper cites: 0.905 and 0.905. I did not verify the other numbers, but please double-check.\"*\n\nA1: TransMIL used a ResNet-50 pre-trained on ImageNet as their backbone feature extractor. We used a ResNet-18 pre-trained on ImageNet and then fine tuned using SimCLR on CAMELYON16. We used the exact same features across the different MIL models in our evaluations. That is, for every model that was compared, we used features extracted from our SimCLR model on the same patches using the same folds. This will cause the difference in the metrics reported in TransMIL and our manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433784966,
                "cdate": 1700433784966,
                "tmdate": 1700433818641,
                "mdate": 1700433818641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OIpAEzpeJU",
                "forum": "rzBskAEmoc",
                "replyto": "88OXfJiuch",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_15GF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_15GF"
                ],
                "content": {
                    "title": {
                        "value": "Please update the manuscript"
                    },
                    "comment": {
                        "value": "I appreciate the author's response to the reviewers criticism. Most of the issues have been addressed constructively. In the comments above, the authors have reported two additional experiments and clarified some misunderstandings. Once the manuscript is correspondingly updated, i will update my review."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593209324,
                "cdate": 1700593209324,
                "tmdate": 1700593209324,
                "mdate": 1700593209324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5w9PKPHS6M",
                "forum": "rzBskAEmoc",
                "replyto": "nplWDVjzz8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_15GF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_15GF"
                ],
                "content": {
                    "title": {
                        "value": "I increase my score"
                    },
                    "comment": {
                        "value": "The manuscript has been appropriately updated. I therefore increase my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685243419,
                "cdate": 1700685243419,
                "tmdate": 1700685243419,
                "mdate": 1700685243419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2KQC2eEXBq",
            "forum": "rzBskAEmoc",
            "replyto": "rzBskAEmoc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5726/Reviewer_2WUS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5726/Reviewer_2WUS"
            ],
            "content": {
                "summary": {
                    "value": "The paper propose a new method for multiple-instance learning. The main novelty is the idea of adjusting the attention score using surrounding context (neighbor tiles). The proposed method achieved improved performance on two datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I appreciate the concept of incorporating contextual information to enhance the performance of Multiple Instance Learning (MIL). In particular, the intuition that 'tiles with high attention scores, surrounded by other high-scoring tiles, should be recognized as significant' sounds reasonable with the dataset the paper primarily focuses on."
                },
                "weaknesses": {
                    "value": "1. The intuition that \"tiles with high attention scores that are surrounded by other high scoring tiles should be recognized as important. Conversely, the presence of a tile classified by the model as important in a low-scoring neighborhood ....could attributed to noise..\" make sense for the particular dataset and task (\" tumor cells are interspersed within large regions of normal cells\") ,and also the crop size, that the paper looks at. However, it remains unclear how well this method generalizes to datasets beyond the particular choices of dataset and task focuses on in this paper.\n\n2. Currently the neighbor-contrained idea is operationalized by masking the attention matrix (from self-attention) using the neighborhood mask. However, the attention scores from self-attention typically represent how much a tile attends to other tiles, not necessarily how much the slide-level prediction attends to that specific tile. It is not clear how directly masking on this attention matrix relates to the intuition above.\n\n3. Feature extractor backbone is an important factor for final performance. Comparing with prior method TransMIL that uses similar transformer backbone, the performance gain from the proposed method is minimum."
                },
                "questions": {
                    "value": "1. In the compared baselines, are self-supervised learning also employed to enhance the feature extractors?\n2. In page4, what is Q1 and K1, did you define it before? What are the m selected landmarks?\n3. Right below eq2, you talked about tile representations T, did you ever define it before? How is it different from H?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Reviewer_2WUS",
                        "ICLR.cc/2024/Conference/Submission5726/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719265266,
            "cdate": 1698719265266,
            "tmdate": 1700704663043,
            "mdate": 1700704663043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zqZzvSV8st",
                "forum": "rzBskAEmoc",
                "replyto": "2KQC2eEXBq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2WUS"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comment and the reviewers acknowledgement that the intuition of CAMIL should be recognised as significant for the tasks defined in our manuscript. Below, we address the weaknesses and questions:"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514650689,
                "cdate": 1700514650689,
                "tmdate": 1700665831838,
                "mdate": 1700665831838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q23ETTSzyo",
                "forum": "rzBskAEmoc",
                "replyto": "2KQC2eEXBq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1. Generalizability:**\nWe appreciate the method proposed in this study might not generalize well across datasets where contextual information of instances within a bag is not necessary. However, we have proposed this method to be specific for this task, where the context of neighboring/contextual information between instances in a bag is essential. To prove generalizability on this task, but across datasets, we have now compared CAMIL against other models on a third commonly used datasets where this is the case (CAMELYON17 [1]). \n\nCAMELYON17 is a particularly challenging dataset, mainly because of the tumor heterogeneity it exhibits. There are WSIs, where the tumor is highly localized, and others, where only a handful of isolated tumor cells (ITCs), are scattered across a sea of normal cells. \n\nSimilar to the other datasets, CAMIL outperforms all other models in terms of ACC and AUC, except for DTFD-MIL in AUC with a 0.003 difference. Please see the table below for the results on CAMELYON17.\n\n|                     |CAMELYON17 |                               |\n|---------------------|--------------------------------|--------------------------------|\n| Method              | ACC($\\uparrow$)                | AUC($\\uparrow$)              |\n| CLAM-SB             | 0.802            | 0.849           |\n| CLAM-MB             | 0.803              | 0.858           |\n| TransMIL            | 0.804              | 0.873            |\n| DTFD-MIL            | 0.797              | 0.884            |\n| GTP                 | 0.800          | 0.762            |\n| DSMIL               | 0.815              | 0.863            |\n| **CAMIL-L** | 0.828              | 0.881        |\n| **CAMIL-G** | 0.818              | 0.875            |\n| **CAMIL**   | 0.843   | 0.881 |\n\nWe have not compared models on MIL datasets outside of digital pathology, as we do not know of any datasets where this contextual information relating to neighboring instances may be considered important.\n\nWe believe that achieving state-of-the-art performance across three datasets proves the generalizability of CAMIL for the task at hand, which is cancer detection and sub-typing in whole slide images.\n\n[1] P. B\u00e1ndi et al., \"From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The CAMELYON17 Challenge,\" in IEEE Transactions on Medical Imaging, vol. 38, no. 2, pp. 550-560, Feb. 2019, doi: 10.1109/TMI.2018.2867350.\n\n**2. Operationalisation of neighbor-constrained idea:**\nMasking the attention matrix with a neighborhood mask aims to limit a tile's attention to its neighbors. This constraint guides the attention mechanism by controlling which neighboring tiles influence a specific tile's representation. Although this approach doesn't perfectly align with how slide-level predictions relate to individual tiles, it helps the model capture relevant local context within the broader context of slide-level predictions, facilitating the model's understanding of local and contextual information within the larger slide-level context. This can be seen by the improved performance over TransMIL regarding slide-level accuracy and localization.\n\n**3. Feature extractor and comparison to TransMIL:**\nThe reviewer correctly states that the feature extractor is a significant performance factor. For this reason, all models were tested using the exact same features extracted from our SimCLR-trained ResNet18 model on the same patches for the same folds. Thus, any differences in model performances are down to model architecture. CAMIL outperforms TransMIL (the previous state-of-the-art) on Camelyon16 and TCGA-NSCLC regarding ACC AUC, with a 1.2\\% and 0.9\\% improvement in ACC and AUC on Camelyon16 and a 1.1\\% and 0.01\\% improvement on ACC and AUC on TCGA-NSCLC. Although this performance gain may be seen as the minimum for these metrics, most models already score relatively high, so any improvement may be regarded as significant. Furthermore, we have now shown a comparison between all models on a much more difficult dataset, Camelyon17, where CAMIL outperforms TransMIL by 3.9\\% in ACC and 0.8\\% in AUC. Moreover, the strength of CAMIL is both its accurate whole slide image classification and its localization. We have now provided a quantitative evaluation of the localization of all models."
                    },
                    "title": {
                        "value": "Response to weaknesses raised by Reviewer 2WUS"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515246891,
                "cdate": 1700515246891,
                "tmdate": 1700665813519,
                "mdate": 1700665813519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y2uyBFXTuu",
                "forum": "rzBskAEmoc",
                "replyto": "2KQC2eEXBq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to questions from reviewer 2WUS"
                    },
                    "comment": {
                        "value": "> Q1: ***\"In the compared baselines, are self-supervised learning also employed to enhance the feature extractors?\"***\n\nA1: We have used the exact same features for all models as we are aware that the feature set has a significant impact on the models' performance. That is, for all models, the SimCLR self-supervised learning feature extractor was used in part 2. in Figure 1 in the manuscript.\n\n\n\n> Q2: ***\"In page4, what is Q1 and K1, did you define it before? What are the m selected landmarks?\"***\n\nA2: We apologize for the misunderstanding and thank the reviewer for spotting an error in our mathematical notation. We have now clearly defined $\\boldsymbol{Q_1}$, $\\boldsymbol{K_1}$, $\\boldsymbol{V_1}$, and $m$ in the revised manuscript. To clarify here, in CAMIL, after features ($\\boldsymbol{H}$) are extracted from the SimCLR feature extractor, they are first passed through a Transformer to capture the relationships between features and tiles and facilitate a more comprehensive feature aggregation. $\\boldsymbol{Q_1}$, $\\boldsymbol{K_1}$, $\\boldsymbol{V_1}$ are the query, key, and value representations according to self-attention defined in [1]. We use the subscript $1$ as this is the first query, key, value representations in the CAMIL architecture. However, to address the challenge of memory overload due to the long-range dependencies in large WSIs, we adopt the Nystromformer architecture defined in [2]. This produces a \"transformed\" feature set $\\boldsymbol{T}$= { $ \\boldsymbol{t}_1, \\ldots , \\boldsymbol{t}_i, \\ldots ,\\boldsymbol{t}_N  $}, with each $\\boldsymbol{t}_i \\in \\mathbb{R}^{n \\times d}$, where, \n\n$$\n\\boldsymbol{t}_i = \\text{softmax}(\\frac{\\boldsymbol{Q_1}(\\boldsymbol{h_i}) \\boldsymbol{\\tilde{K}}_1^{T}(\\boldsymbol{h_i})} {\\sqrt{d_k}}) \\left( \\boldsymbol{A} \\right)^+\n\\text{softmax}\\left(\\frac{\\boldsymbol{\\tilde{Q}}_1(\\boldsymbol{h_i}) \\boldsymbol{K}_1^{T}(\\boldsymbol{h_i})} {\\sqrt{d_k}}\\right)\\boldsymbol{V_1}(\\boldsymbol{h_i}),\n$$\nwhere $\\boldsymbol{\\tilde{Q}}_1(\\boldsymbol{h_i})$ and $\\boldsymbol{\\tilde{K}}_1(\\boldsymbol{h_i})$ are the $m$ selected landmarks (see [2]) from the original $n$-dimensional sequence of $\\boldsymbol{Q}_1$ and $\\boldsymbol{K}_1$, $ \\boldsymbol{A}^+=\\text{softmax}\\Bigg(\\frac{\\boldsymbol{\\tilde{Q}}_1(\\boldsymbol{h_i}) \\boldsymbol{\\tilde{K}}_1^{T}(\\boldsymbol{h_i})} {\\sqrt{d_k}}\\Bigg)^+ $ is the approximate inverse of $\\boldsymbol{A}$.  Softmax is applied along the rows of the matrix. $\\boldsymbol{K_1}(\\boldsymbol{h_i})$, $\\boldsymbol{Q_1}(\\boldsymbol{h_i})$, and $\\boldsymbol{V_1}(\\boldsymbol{h_i})$ are the first key, query, and value representations of $\\boldsymbol{h_i}$ shown as $T(\\boldsymbol{h})$ in step 3 in Figure 1 of the manuscript.\n\nWe have now updated our manuscript to show this clearly.\n\n\n> Q3: ***\"Right below eq2, you talked about tile representations T, did you ever define it before? How is it different from H?\"***\n\nA3: We hope that the answer to Q2 above clarifies this. The tile representations $\\boldsymbol{T}$ are the the output after $\\boldsymbol{H}$ (the SimCLR extracted features) are passed through the Nystromformer $\\boldsymbol{T(h)}$ in Figure 1 in the manuscript. $\\boldsymbol{T}$ are then again transformed into new value, key, and query representations which are used to calculate the attention matrix. This attention matrix is the one that is masked by the neighborhood mask. Importantly, the neighborhood mask is calculated from the SimCLR representations $\\boldsymbol{H}$. \n\nIn our ablation studies we show how CAMIL-G (when the neighborhood attention module is removed from CAMIL) performs similarly to TransMIL [3], with the difference most likely being due to the feature aggregation as this is the only difference between CAMIL-G and TransMIL. We also show that CAMIL-L (removing the Nystromformer from CAMIL) outperforms CAMIL-G and the amalgamations of both the Nystromformer and neighborhood attention module lead to superior performance across all datasets.\n\n[1]  Vaswani et al., Attention is all you need. Advances in Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998\u20136008, 2017.\n\n[2] Xiong et al., Nystr \u0308omformer: A nystr \u0308om-based algorithm for approximating self-attention. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 14138\u201314148. AAAI Press, 2021. \n\n[3] Zhuchen Shao, et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 2136\u20132147, 2021."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516539872,
                "cdate": 1700516539872,
                "tmdate": 1700672296818,
                "mdate": 1700672296818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sLuwso5rWI",
                "forum": "rzBskAEmoc",
                "replyto": "y2uyBFXTuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_2WUS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_2WUS"
                ],
                "content": {
                    "comment": {
                        "value": "I  have read the authors' response. I thank the author for clarifying several key details. I will consider increase my score"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704631603,
                "cdate": 1700704631603,
                "tmdate": 1700704631603,
                "mdate": 1700704631603,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tXLoWaaWSN",
            "forum": "rzBskAEmoc",
            "replyto": "rzBskAEmoc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5726/Reviewer_SSWm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5726/Reviewer_SSWm"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes CAMIL - a MIL framework that applies neighborhood constraints to the transformer based MIL method, thus adding spatial context information. The method is tested on two MIL datasets - Camelyon16 and TCGA-NSCLC subtyping, where it compares favorably to existing methods. Ablations explore the importance of adding the neighborhood constraint and Nystromformer module."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The central idea behind the paper is simple yet effective; current methods do not account for the spatial dependencies among the patches in a WSI. This information is something that human experts use, and this work proposes an intuitive similarity mask to modify the attention values to reflect the local context.\n- The paper is well-written and easy to read.\n- The ablation studies serve to motivate the need for adding the neighborhood constraint module."
                },
                "weaknesses": {
                    "value": "- In the appendix, the F1 score for TransMIL is better than CAMIL for Camelyon16 dataset. Can the authors explain about the level of imbalance in this dataset, and whether accuracy is the correct metric for this?\n- Regarding the effectiveness of localization with this technique, the authors have produced some qualitative evidence in figure 3 and 4. However, since one of the main merits of this work is around better localization esp for problems with small foci of interest, can the authors a quantitative evaluation like in Table 1 in [1]?\n- Currently, a comparison with DS-MIL [1] is missing. This would help to evaluate this work against a model that uses multi-scale features and motivate the need for local context even with a multi-scale featurizer. \n\n\n\n[1] - Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning - Li et al"
                },
                "questions": {
                    "value": "- There is typo in table 1 heading: it should be 'TCGA-NSCLC'\n- There are some spelling mistakes in the work. Example - 'are impirative in successfull performance` in Conclusion section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Reviewer_SSWm",
                        "ICLR.cc/2024/Conference/Submission5726/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719358418,
            "cdate": 1698719358418,
            "tmdate": 1700693557679,
            "mdate": 1700693557679,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ESsTpDtp7L",
                "forum": "rzBskAEmoc",
                "replyto": "tXLoWaaWSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SSWm"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback. In particular, we appreciate the reviewer acknowledging the simplicity and effectiveness of our approach including its sound intuition. Below, we address the weaknesses and questions raised. \n\n**CAMELYON16 imbalance and comparison with TransMIL**\nThe reviewer is correct in pointing out that the dataset is imbalanced. The average training imbalance is 59:41, the validation imbalance is 59:41, and the testing imbalance is 63:37 (normal: tumor) across the folds. Therefore, metrics such as the F1 score and the area under the receiver operating score may be more suitable than ACC for this task. In the context of cancer metastasis detection, where both false positives and false negatives carry significant costs (with false negatives arguably a greater cost), metrics like F1 score are needed. However, the AUC and ACC are still important to gain a comprehensive understanding of the model's performance. We report on all metrics similar to the literature. \n\nWhile it is true that TransMIL outperforms CAMIL in terms of F1 score, CAMIL outperforms TransMIL in both accuracy, AUC, and (leading to the reviewer's next point on the quantitative evaluation of localization) localization. While TransMIL has a very high mean specificity, indicating its ability to minimize false positives on normal slides, it lacks the ability to pinpoint tumourous regions accurately, as evidenced by the very low Dice score. The latter is also visually corroborated by the attention maps provided in the paper"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665529767,
                "cdate": 1700665529767,
                "tmdate": 1700665529767,
                "mdate": 1700665529767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NREgmDbkCG",
                "forum": "rzBskAEmoc",
                "replyto": "tXLoWaaWSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continued response to reviewer SSWm"
                    },
                    "comment": {
                        "value": "**Quantitative evaluation of localization:**\nNumerous papers [1, 2] rely on the FROC curve for providing localizations, yet it wasn't evident to us from those studies which methodology to adopt to replicate the prior findings. Hence, we opted for a more direct method to assess our model's localization abilities. Following a methodology similar to [3], we used the dice score to qualify the model's ability to identify positive instances for the cancerous slides. For normal slides, we relied on computing specificity. This approach is straightforward, intuitive, and easily interpretable. We first computed the reference tile-accurate masks from the expert tumor delineations, considering a tile as a tumor if it contains at least 20\\% annotated tumor. This allows a fairer comparison between predicted and reference masks since the models are not pixel-level accurate. To produce predicted masks, we used the scaled attention scores for each tile CAMIL, both CLAM models, TransMIL and DSMIL, the tile level logits for DTFD-MIL, and the tile-level GradCAM for GTP. For TransMIL, we used the attention values of class tokens in the self-attention matrix to the rest of the feature tokens, just as the authors describe. We applied a threshold of 0.5 to the model's output probabilities to generate the masks from the tile-level predictions. The results for the Dice score and Specificity are shown in the table below:\n\n| Method            | Dice($\\uparrow$)             | Specificity($\\uparrow$)      |\n|-------------------|------------------------------|------------------------------|\n| CLAM-SB           | 0.459\t         | 0.987            |\n| CLAM-MB           | 0.406          | 0.573         |\n| TransMIL          | 0.103           | 0.999 |\n| DTFD-MIL          |0.525| 0.999 |\n| GTP               | 0.418           | 0.851           |\n| DSMIL             | 0.259          | 0.863           |        |                              |\n| CAMIL | 0.515            | 0.980          |\n\nCAMIL outperforms all except DTFD-MIL in terms of Dice score on cancerous slides. CAMIL shows a significant improvement over TransMIL in terms of tumour localization. However, TransMIL is more effective in terms of specificity on normal slides, meaning that it produces few false positives on normal slides. \n\nCAMIL outperforms DTFD-MIL on CAMELYON16 and TCGA-NSCLC in terms of ACC, F1 and AUC. Whole slide predictive accuracy, as well as cancerous localization, are necessary for these DL-models to be effective in the clinic. CAMIL represents a strong overall model that is only slightly surpassed in terms of F1 score on one dataset by one model and slightly surpassed in terms of localization by another model. \n\nWe believe the decreased localization performance in CAMIL when compared to DTFD-MIL might be attributed to integrating the Nystromformer module in our model design, akin to its role in TransMIL. TransMIL, as indicated by the attention maps in our qualitative assessment and its slide-level performance, demonstrates the ability to grasp the general patterns within a WSI and distinguish between normal and cancerous slides. However, despite this, confirmed by its low Dice score, it falls short in effectively pinpointing specific cancerous evidence within slides. Integrating the Nystromformer into our model design might introduce a trade-off between slide-level accuracy and localization performance, resulting in improved slide-level accuracy with an expense of slightly decreased localization performance. CAMIL significantly improves cancerous localization over TransMIL, which can be attributed to our neighborhood attention module.\n\n\n[1] Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14318\u201314328, 2021.\n\n\n[2] Hongrun Zhang et al. DTFD-MIL: double-tier feature distillation multiple instance learning for histopathology whole slide image classification. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 18780\u201318790, 2022. doi: 10.1109/CVPR52688.2022.01824.\n\n\n[3] Paul Tourniaire, Marius Ilie, Paul Hofman, Nicholas Ayache, Herv\u00e9 Delingette, MS-CLAM: Mixed supervision for the classification and localization of tumors in Whole Slide Images, Medical Image Analysis, Volume 85, 2023, 102763, ISSN 1361-8415."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666115188,
                "cdate": 1700666115188,
                "tmdate": 1700666115188,
                "mdate": 1700666115188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ov4kPghpSt",
                "forum": "rzBskAEmoc",
                "replyto": "tXLoWaaWSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continued response to Reviewer SSWm"
                    },
                    "comment": {
                        "value": "**DSMIL comparison**\n\nWe have included a comparison of DSMIL on all datasets using our feature set on our folds (single-scale features). Below is the table of results for DSMIL and CAMIL on all datasets. As requested by **Reviewer: 15GF*, we have added another dataset, CAMELYON17. Please see the table below:\n\n| Model | Dataset    | ACC               | F1                | AUC               | Dice (patch-level) | Specificity (patch-level) |\n|-------|------------|-------------------|-------------------|-------------------|-------------------|-------------------|\n|**DSMIL**| CAMELYON16 | 0.874  | 0.848 | 0.949 | 0.259 \t| 0.863\n|           | TCGA-NSCLC | 0.853 | 0.864 | 0.954 |                |                      | \n| **CAMIL** | CAMELYON16 | 0.917 | 0.881 | 0.959| \t0.515 \t|0.980|\n|       | TCGA-NSCLC | 0.916 | 0.918 | 0.975|\n\n\n\n\n\nOn the same features that we use for comparisons for each dataset (single-scale features), CAMIL outperforms DSMIL in ACC, F1, AUC regarding slide-level classification for all datasets, and in both Dice and Specificity, regarding localization on CAMELYON17."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666659751,
                "cdate": 1700666659751,
                "tmdate": 1700667583410,
                "mdate": 1700667583410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dklcMF23Kh",
                "forum": "rzBskAEmoc",
                "replyto": "tXLoWaaWSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to questions from Reviewer SSWm"
                    },
                    "comment": {
                        "value": "We thank the reviewer for spotting the typos in our manuscript. We have now corrected those mentioned and other minor ones we found upon further inspection."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672036202,
                "cdate": 1700672036202,
                "tmdate": 1700672036202,
                "mdate": 1700672036202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gtaQO4bCki",
                "forum": "rzBskAEmoc",
                "replyto": "dklcMF23Kh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_SSWm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_SSWm"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "The authors have added - \na) a new dataset (Camelyon 17) \nb) comparative studies with DS-MIL\nc) quantitative evaluation of localization \nIn all of these, CAMIL shows superior performance, making the contributions of this work stronger. The authors have addressed my concerns, thus I improve my rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693776916,
                "cdate": 1700693776916,
                "tmdate": 1700693776916,
                "mdate": 1700693776916,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3mn6O5LzCq",
            "forum": "rzBskAEmoc",
            "replyto": "rzBskAEmoc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5726/Reviewer_5Vaj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5726/Reviewer_5Vaj"
            ],
            "content": {
                "summary": {
                    "value": "The authors highlight one of the central limitations of multiple instance learning that the relative position of tiles are not typically considered and have addressed this by implementing the neighbor-constrained attention module.  This augments the weighting of each individual tile based on the relative attention weighting of the neighbors of the tile from the whole slide image.  These local features are then integrated back with the global features for the slide level prediction. CAMELYON16 and TCGA-NSCL were used as benchmarking datasets and the performance of CAMIL model exceeded all other MIL frameworks except GTP on TCGA-NSCL dataset. It is noted that the local and global features independently perform well compared to other models but the combination show good outperformance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Transmil had suggested that context is important on the whole slide level, perhaps as much as the context within individual tiles.  What the authors have done appears to have better defined this context awareness which better constrains the concept of neighborhood awareness for important information in a whole slide image."
                },
                "weaknesses": {
                    "value": "I would like to see performance on more challenging datasets for which the slide level labels are not visual features, such as mutation prediction or something similar."
                },
                "questions": {
                    "value": "You use SimClr to train the feature extraction encoder.  Have you test performance from other self-supervised pretrained encoders? Recent pre-prints suggest that larger ViT SSL trained encoders can benefit performance on less sophisticated aggregation functions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5726/Reviewer_5Vaj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818483902,
            "cdate": 1698818483902,
            "tmdate": 1699636599363,
            "mdate": 1699636599363,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HYLAkGuSJ8",
                "forum": "rzBskAEmoc",
                "replyto": "3mn6O5LzCq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 5Vaj"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and for acknowledging the strengths of our paper, particularly how our context-aware module better constrains the concept of neighboring information among tiles in a whole slide image. \n\nWe address the questions and weaknesses below:\n\n**Performance on Challenging Datasets:**\n\nWe appreciate the reviewers\u2019 suggestion to test the model on more challenging datasets. While we agree that mutation prediction from whole slide images will be an exciting task to explore, we were unable to explore this in the time frame given. However, we have evaluated our model against all other models on CAMELYON17 [1] - another metastasis prediction dataset that has been more challenging than its previous counterpart, CAMELYON16 (as seen by lower classification metrics across models). We have provided the same comparison as the other datasets to CAMELYON17. Similar to the other datasets, CAMIL outperforms all other models in terms of ACC and AUC, except for DTFD-MIL in AUC with a 0.003 difference. Please see the table below:\n\n|                     |CAMELYON17 |                               |\n|---------------------|--------------------------------|--------------------------------|\n| Method              | ACC($\\uparrow$)                | AUC($\\uparrow$)              |\n| CLAM-SB             | 0.802            | 0.849           |\n| CLAM-MB             | 0.803              | 0.858           |\n| TransMIL            | 0.804              | 0.873            |\n| DTFD-MIL            | 0.797              | 0.884            |\n| GTP                 | 0.800          | 0.762            |\n| DSMIL               | 0.815              | 0.863            |\n| **CAMIL-L** | 0.828              | 0.881        |\n| **CAMIL-G** | 0.818              | 0.875            |\n| **CAMIL**   | 0.843   | 0.881 |\n\n\n**Questions:**\n\n> Q1: *\"You use SimClr to train the feature extraction encoder. Have you test performance from other self-supervised pretrained encoders? Recent pre-prints suggest that larger ViT SSL trained encoders can benefit performance on less sophisticated aggregation functions.\"*\n\nA1: We agree that the type of pre-trained encoder used in the pipeline affects model performance. It is worth mentioning that in all our reported results, we used the exact same features across the different MIL models. That is, for every model that was compared, we used features extracted from our SimCLR model on the same patches using the same folds. To evaluate how the SimCLR-based feature extraction encoder affects performance across models, we have conducted an ablation study using a ResNet18 model pre-trained on ImageNet, as suggested by **Reviewer 15GF**. Please see the response to that review for a full analysis.\n\nAlthough we have not explored other self-supervised pre-trained autoencoders such as the mentioned ViT encoders, we will update our manuscript to discuss that this is an avenue for future work and cite those papers doing so, particularly works in [2]. \n\n[1] P. B\u00e1ndi et al., \"From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The CAMELYON17 Challenge,\" in IEEE Transactions on Medical Imaging, vol. 38, no. 2, pp. 550-560, Feb. 2019, doi: 10.1109/TMI.2018.2867350.\n\n[2] Wang, S., Gao, J., Li, Z., Zhang, X., Hu, W., 2023. A closer look at self-supervised lightweight vision transformers, in: Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., Scarlett, J. (Eds.), Proceedings of the 40th International Conference on Machine Learning, PMLR. pp. 35624\u201335641. URL: https://proceedings.mlr.press/v202/wang23e.html."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432572417,
                "cdate": 1700432572417,
                "tmdate": 1700516114379,
                "mdate": 1700516114379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MCd9Vnr2C6",
                "forum": "rzBskAEmoc",
                "replyto": "HYLAkGuSJ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_5Vaj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5726/Reviewer_5Vaj"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you.  I retain my recommendation to accept this paper. It is a significant advance in the field and has major conceptual merit."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665568578,
                "cdate": 1700665568578,
                "tmdate": 1700665568578,
                "mdate": 1700665568578,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]