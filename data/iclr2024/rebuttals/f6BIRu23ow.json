[
    {
        "title": "TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images"
    },
    {
        "review": {
            "id": "WLP2KMEDMV",
            "forum": "f6BIRu23ow",
            "replyto": "f6BIRu23ow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2087/Reviewer_Qnji"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2087/Reviewer_Qnji"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces the TriSAM method, which is a zero-shot 3D segmentation method named TriSAM that relies on the Segment Anything Model (well-known as SAM). The framework can segment objects in an image given a point or bounding box as input. The designed framework is designed to segment blood vessels, hence the work proposes to integrate a multi-seed training strategy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Significance: The paper focuses on a very relevant problem that, to date, still remains unsolved.\n- originality: The idea of combining a tracking approach with SAM sounds novel."
                },
                "weaknesses": {
                    "value": "Clarity: The paper misses to provide precise details about how the method works. While the overall outline of the steps within TriSAM are very clear, how each of them are designed and formalized is not well explained in the paper. \nQuality: There are aspects of the paper (see questions) that are not well justified. The experimental results do not consider state of the art methods on vessel segmentation (e.g. [1-3] to illustrate just a few examples), including some that reduce the annotation effort (see [2]). \n\n\n[1] Livne, Michelle, et al. \"A U-Net deep learning framework for high-performance vessel segmentation in patients with cerebrovascular disease.\" Frontiers in neuroscience 13 (2019): 97.\n[2] Dang, Vien Ngoc, et al. \"Vessel-CAPTCHA: an efficient learning framework for vessel annotation and segmentation.\" Medical Image Analysis 75 (2022): 102263\n[3] Tetteh, Giles, et al. \"Deepvesselnet: Vessel segmentation, centerline prediction, and bifurcation detection in 3-d angiographic volumes.\" Frontiers in Neuroscience 14 (2020): 1285."
                },
                "questions": {
                    "value": "- What do the authors mean by this sentence \" Moreover, imaging the whole mouse brain using VEM technology is under planning\"? \n- How are turning points detected?\n- How is the tracking approach integrated with SAM?\n- How is the model trained? The zero-shot aspect does not come across clear\n- The paper states that : \"By choosing the best plane during tracking, the shape and size will not change dramatically\". This seems like a flawed argument. Across neighboring slides, the vessels should not dramatically change of size but progressively. However, as the brain vessels are tortuous, the change of shape can always occur."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2087/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2087/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2087/Reviewer_Qnji"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2087/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698599391410,
            "cdate": 1698599391410,
            "tmdate": 1700750699372,
            "mdate": 1700750699372,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z65LtUumLi",
                "forum": "f6BIRu23ow",
                "replyto": "WLP2KMEDMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2087/Reviewer_Qnji"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2087/Reviewer_Qnji"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have not submitted a rebuttal addressing the concerns raised during the first round of reviews. Hence, I will maintain my original score."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2087/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560983586,
                "cdate": 1700560983586,
                "tmdate": 1700560983586,
                "mdate": 1700560983586,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OHujba14sd",
                "forum": "f6BIRu23ow",
                "replyto": "WLP2KMEDMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2087/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Qnji"
                    },
                    "comment": {
                        "value": "Thanks for the insightful feedback. We clarify the issues and address the questions accordingly as described below.\n\n**Response Highlight**\n\nWe would like to point out that one of our **main contributions is our constructed BvEM dataset**, which is the largest-to-date public benchmark for cortical blood vessel segmentation in VEM images. It addresses a significant gap in the neuroimaging field.\n\n**Q1: Experiment**\n\n**[Reply]**  We have conducted more experiments, such as nnUNET and MAESTER. Please refer to the answer to Q1 of General Response. We have cited and discussed the provided references [1-3] in Section 5. \n\nWe also would like to highlight that the main focus of this paper is **zero-shot** vessel segmentation. As annotating 3D volumes for blood vessels is not only labor-intensive but also requires well-trained domain experts, efficient deep-learning algorithms are urgently needed. End-to-end training methods require annotating data for training in any new scenario. We deeply believe that in the era of foundational models, the zero-shot setting is the correct path to solving this problem. The emergence of the SAM model and the success of our proposed TriSAM have demonstrated this point. Comparing our approach to supervised learning methods is meaningless because supervised learning methods fall far short of the generalization capacity of our approach, and the annotation-training paradigm of supervised learning is far from what we desire. Therefore, the proposed BvEM dataset is also designed for evaluating zero-shot model performances, not only for training deep learning models. We reported the performance of 3D UNet solely for the completeness of the paper.\n\n\n**Q2: The meaning of \"Moreover, imaging the whole mouse brain using VEM technology is under planning\".**\n\n**[Reply]**  In response to the BRAIN Initiative, NIH has proposed a series of transformative projects. A major goal is scaling up the VEM technology to the whole mouse brain. A group of leading scientists calls for mapping a whole mouse brain in the next ten years. In response, a project mapping the connectome in a whole hippocampus has been granted as the first step. Considering the deep neuroscience background that is not familiar to computer scientists, we have deleted this sentence to avoid confusion.\n\n\n**Q3: Turning point detection**\n\n**[Reply]**  When tracking along the z-axis, we apply SAM to segment blood vessels in the zx and zy planes. Potential turning points are identified as points in these segments with the smallest and largest z-values. The process is detailed at the end of Section 3.\n\n\n**Q4: SAM-based tracking**\n\n**[Reply]**  SAM-based tracking: For the next slice, we use prompts derived from the enlarged bounding box and the segment's center in the current slice. Specifically, we pad the bounding box of the segment to create the bounding box prompt and calculate the segment's center for the point prompt. These prompts are used to segment blood vessels in the subsequent slice, as detailed in Section 3.2 \"SAM-based Tracking.\"\n\n**Q5: Model training**\n\n**[Reply]**  Model training: Our proposed zero-shot method **eliminates** the need for model training. We utilize a pre-trained SAM for segmentation, leveraging its ability to segment objects from a point or bounding box. The initial points for segmentation are generated using an unsupervised color thresholding method. While this thresholding may miss significant portions of the blood vessel, these areas are effectively detected and segmented through SAM-based tracking.\n\n\n**Q6: Dynamics**\n\n**[Reply]** Blood vessels gradually change their shape and size in 3D, but the variations can significantly differ across different 2D planes, as illustrated in Figure 10 in the revised paper.\nWe aim to highlight that along the optimally selected plane, changes in shape and size of the blood vessel are more stable **compared to other planes**. This statement has been updated in the revised version. We also compare the scale variation for different methods. Please refer to the answer to Q2 of the General Response.\n\nWe hope our response clarifies your initial concerns/questions. We would be happy to provide further clarifications where necessary."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2087/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677316495,
                "cdate": 1700677316495,
                "tmdate": 1700678331252,
                "mdate": 1700678331252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fSXDcS1jO9",
                "forum": "f6BIRu23ow",
                "replyto": "WLP2KMEDMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2087/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Late Rebuttal"
                    },
                    "comment": {
                        "value": "We apologize for the delay in submission, as fulfilling the reviewers' request for additional experiments on our proposed large-scale dataset proved to be a time-consuming process. Nonetheless, we managed to submit our response before the rebuttal deadline. As we have clarified some misunderstandings and addressed all your questions about our paper, we sincerely hope that you can reconsider the rating. Please feel free to let us know if you have any further questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2087/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677378811,
                "cdate": 1700677378811,
                "tmdate": 1700678591973,
                "mdate": 1700678591973,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z0DxWVqMFb",
            "forum": "f6BIRu23ow",
            "replyto": "f6BIRu23ow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2087/Reviewer_BJ42"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2087/Reviewer_BJ42"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the BvEM benchmark that provides Volume Electron Microscopy (VEM) image volumes from adult mice, macaque, and humans. Also, this proposes a zero-shot cortical blood vessel segmentation method, called TriSAM, which consists of Tri-Plane selection, SAM-based tracking, and recursive redirection. By choosing the best plane for tacking, this method enables effective long-term 3D blood vessel segmentation. The proposed method is demonstrated on the BvEM benchmark and shows superiority over the comparative methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper proposes a new dataset, BvEM, which contains VEM images and their blood vessel segmentation labels verified by the experts. \n- The proposed method extends the work of the Segment Anything Model (SAM) to 3D vessel segmentation, which addresses the problem of requiring large amounts of annotated training data.\n- For SAM-based tracking, the authors select seeds in which the shape and size do not have dynamic changes by looking into three different planes.\n- The proposed method is verified on the proposed benchmark dataset and achieves higher performance than the comparative methods."
                },
                "weaknesses": {
                    "value": "- In the proposed method, the initial seed generation and triplane selection seem to be quite heuristic in that the selections depend on the threshold.\n- The dynamics of the planes are not investigated in detail. The shape and size may be different along the images.\n- There are many learning-based blood vessel segmentation methods, but only 3D UNet is used as a comparative method. The other Color Thresholding and SAM+IoU Tracking methods are not deep learning-based methods.\n- There is a lack of description of why the proposed method adopts the SAM approach."
                },
                "questions": {
                    "value": "- How the threshold for each initial seed selection and plane selection is determined? It seems difficult to find the optical threshold manually. Also, how are segmentation results different according to the threshold?\n- Please discuss about the dynamic changes along the images for tracking blood vessels.\n- Is the proposed TriSAM SOTA even when compared to the supervised image segmentation methods such as nnUNET?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2087/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2087/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2087/Reviewer_BJ42"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2087/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802672515,
            "cdate": 1698802672515,
            "tmdate": 1699636141038,
            "mdate": 1699636141038,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JZe8oYH9tr",
                "forum": "f6BIRu23ow",
                "replyto": "Z0DxWVqMFb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2087/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BJ42"
                    },
                    "comment": {
                        "value": "Thanks for the constructive comments. We answer your questions below:\n\n**Q1: Threshold**\n\n**[Reply]** The ablation study on the threshold for initial seed selection and triplane selection is detailed in Section 4.1, with results in Figure 7. We observed consistent method performance across various thresholds.\n\nAs shown in Section 4.1,  the performance of the proposed method is largely insensitive to threshold variations, except at extremely high values, indicating our method's robustness to threshold settings.\n\n\n**Q2: Tracking Dynamics**\n\n**[Reply]** Please refer to the answer to Q2 of General Response. \n\n**Q3: Experiments**\n\n**[Reply]** We have conducted more experiments, such as nnUNET and MAESTER. Please refer to the answer to Q1 of General Response.\n\nWe also would like to highlight that the main focus of this paper is **zero-shot** vessel segmentation. As annotating 3D volumes for blood vessels is not only labor-intensive but also requires well-trained domain experts, efficient deep-learning algorithms are urgently needed. End-to-end training methods require annotating data for training in any new scenario. We deeply believe that in the era of foundational models, the zero-shot setting is the correct path to solving this problem. The emergence of the SAM model and the success of our proposed TriSAM have demonstrated this point. Comparing our approach to supervised learning methods is meaningless because supervised learning methods fall far short of the generalization capacity of our approach, and the annotation-training paradigm of supervised learning is far from what we desire. Therefore, the proposed BvEM dataset is also designed for evaluating zero-shot model performances, not only for training deep learning models. We reported the performance of 3D UNet solely for the completeness of the paper.\n\n\n**Q4: Why SAM**\n\n**[Reply]** We chose SAM for its efficacy in segmenting objects with varied prompts like a point, box, or mask, and its excellent generalization to unseen data. In the era of foundational models, we firmly believe that for various segmentation tasks, a foundational model is all we need, and adaptations specific to different downstream tasks can be made based on this foundational model. As a foundational model for image segmentation, SAM has already been widely demonstrated for its strong generalization capabilities. More details have been included in the revised manuscript. \n\nWe hope our response clarifies your initial concerns/questions. We would be happy to provide further clarifications where necessary."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2087/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677218735,
                "cdate": 1700677218735,
                "tmdate": 1700678243889,
                "mdate": 1700678243889,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nksZXeY2It",
            "forum": "f6BIRu23ow",
            "replyto": "f6BIRu23ow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2087/Reviewer_NUw2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2087/Reviewer_NUw2"
            ],
            "content": {
                "summary": {
                    "value": "The proposed TriSAM is based on the multi-seed tracking framework, which leverage specific image planes for tracking, while employing others to detect possible turning points. This framework is a combination of Tri-Plane selection, SAM-driven tracking, and recursive redirection. Evaluated on the proposed BvEM dataset, the proposed TriSAM is able to achieve long-term 3D blood vessel segmentation without the need for model training or fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A new benchmark of BvEM is introduced for blood vessel segmentation in volume electron microscopy images.\n2. The proposed TriSAM works effectively and is able to achieve zero-shot 3D blood vessel segmentation.\n3. The paper is well written and clearly organized."
                },
                "weaknesses": {
                    "value": "1. The proposed method is more like an engineering implementation than a scientific research. It consists of four steps: Initial seed generation, Tri-plane selection, SAM-based tracking, and recursive redirection. Even though the rationale is simple, it works effectively.\n\n2. The lack of comparison with sota VEM segmentation methods. The discussion of existing VEM segmentation methods are quited limited, more discussion should be provided to facilitate the understanding of existing researches. The proposed method might be compared with more SOTA zero-shot segmentation methods.\n\n3. How to determine the threshold in Tri-Plane selection and SAM-based tracking? Do we need to change the value of threshold when applied to other data sets?"
                },
                "questions": {
                    "value": "1.  The existing VEM segmentation methods might be discussed in detail. What is the difference between the proposed method and existing works?\n\n2. The threshold plays a vital role in the SAM-based tracking and recursive redirection. An ablation study of threshold could be provided to determine the influence of different values of threshold."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2087/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2087/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2087/Reviewer_NUw2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2087/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809243298,
            "cdate": 1698809243298,
            "tmdate": 1699636140946,
            "mdate": 1699636140946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9mX7wfCGpG",
                "forum": "f6BIRu23ow",
                "replyto": "nksZXeY2It",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2087/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2087/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2087/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NUw2"
                    },
                    "comment": {
                        "value": "Thanks for the constructive comments. We address the questions and clarify the issues accordingly as described below.\n\n**Q1: Novelty**\n\n**[Reply]** ***(a) Zero-shot approach***: One major challenge in 3D blood vessel segmentation is the limited labeled data to train effective deep learning models, as labeling 3D volumes is extremely time-consuming. Although existing unsupervised learning approaches eliminate the need for manual labeling, their 3D segmentation performance is not adequate. In contrast, we convert the 3D segmentation task into the tracking task and propose to integrate the existing foundation model, SAM, into our zero-shot pipeline achieving SOTA performance without any manual annotation.\n***(b) SAM Adaptation***: Directly applying SAM to track blood vessel segments over slices does not achieve optimal performance (Table 2). Popular SAM-adaption methods need labeled data for finetuning. In contrast, the proposed SAM-based tracking pipeline is both effective and data efficient.\n\n**Q2: Experiment**\n\n**[Reply]** We further compare with nnUNET and MAESTER, please refer to the answer to Q1 of General Response.\n\n**Q3: Threshold**\n\n**[Reply]** The ablation study concerning the threshold is presented in Figure 7 and detailed in Section 4.1. We applied consistent hyperparameters across all species, which demonstrated effective generalization.\n\n**Q4: Discussion of Existing VEM Segmentation Methods**\n\n**[Reply]** Most existing VEM segmentation methods are supervised and need extensive labeled data, often limiting generalization across datasets. We summarized the existing VEM segmentation works in Section 5. Our method, requiring no labeled data, shows superior generalization to different datasets.\n\nWe hope our response clarifies your initial concerns/questions. We would be happy to provide further clarifications where necessary."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2087/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677142923,
                "cdate": 1700677142923,
                "tmdate": 1700678197801,
                "mdate": 1700678197801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]