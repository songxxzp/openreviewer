[
    {
        "title": "Zero-Shot Robustification of Zero-Shot Models"
    },
    {
        "review": {
            "id": "RFXOFKHPNX",
            "forum": "fCeUoDr9Tq",
            "replyto": "fCeUoDr9Tq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1606/Reviewer_1hyF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1606/Reviewer_1hyF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a zero-shot method to improve the robustness of pre-trained model embeddings. The key idea is to leverage insights obtained from language models based on task descriptions. After extracting the insights, they use the insights to modify the image embeddings, removing harmful components and enhancing useful ones, without any supervision. To achieve this goal, the method encourages invariant representation (to spurious features) by projecting the pre-trained model embeddings onto the subspace orthogonal to the subspace spanned by spurious feature descriptions. Experiments demonstrate that the proposed method improves multi-modal and language model zero-shot performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Novel and useful setting:** The setting of improving the robustness of pretrained model embeddings with task description is novel. RoboShot offers a unique approach that preserves the out-of-the-box usability of pretrained models, which is a key advantage.\n\n- **Extensive experiments and analyses:** The authors demonstrated the efficacy of the proposed method and setting with extensive experiments and analyses, in terms of both datasets and settings.\n\n- The paper is well-written."
                },
                "weaknesses": {
                    "value": "The robustification relies on the insights provided by language models. However, if the language model does not identify the potential failure cases of the model, the method cannot remedy it. For instance, if the LM does not propose background as a spurious feature, can the method still mitigate such spurious correlation?"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698455608080,
            "cdate": 1698455608080,
            "tmdate": 1699636088975,
            "mdate": 1699636088975,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bFRxo278Oy",
                "forum": "fCeUoDr9Tq",
                "replyto": "RFXOFKHPNX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for noting **the novelty of our method**, and the strength of our evaluations!\n* **On scenarios where language models fail**. There are two cases to examine. \n\n    First, the reviewer asks about scenarios where the language model is not familiar with what causes, for example, CLIP, to fail. This is not a problem for our model: we do not need the LM to be aware of the intricacies and impacts of spurious correlations on a particular model. We only ask for relevant information about a task, not about a particular model's behavior on the task.\n    \n    Second, there are scenarios where the language model cannot provide any task-specific insights. In this case, our method cannot directly produce any improvements, because we do not have anything to use for determining what the harmful subspace should be. *However, even in this setting, all hope is not lost:* There are ways to **transfer insights** from one task to another. This requires a form of meta-prompting: we ask language models for what tasks are similar to the given tasks, and then ask for insights about all of these similar tasks. For example, suppose that the language model provided no insights whatsoever for waterbird and landbird categorization. We could simply ask the language model for what type of task (and task entities) are similar. We may receive an answer about distinguishing mammals from lizards as a potentially similar task. In this case, background would be a transferable insight, since mammal/lizard categorization is similarly not affected by background. There is a large space for such approaches."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700274049047,
                "cdate": 1700274049047,
                "tmdate": 1700274049047,
                "mdate": 1700274049047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T0f2Nm8w1q",
                "forum": "fCeUoDr9Tq",
                "replyto": "RFXOFKHPNX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe thank you again for your feedback, questions, and suggestions! We believe we have answered all of your questions in our responses and the updated draft. If you have additional questions, we would love to answer them!\n\nThe Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583916016,
                "cdate": 1700583916016,
                "tmdate": 1700583916016,
                "mdate": 1700583916016,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RYxbq9JfRe",
            "forum": "fCeUoDr9Tq",
            "replyto": "fCeUoDr9Tq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1606/Reviewer_R1Zx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1606/Reviewer_R1Zx"
            ],
            "content": {
                "summary": {
                    "value": "The work tackles a very interesting and impactful topic, namely the improvement of zero-shot models without any additional labelled data, training or manual intervention. This goal is accomplished by leveraging existing pretrained Language Models (LMs) to infer positive and negative insigths from the task description, using their embeddings to obtain helpful, harmful and neutral subspaces and finally editing the representations to remove the harmful components while boosting the helpful ones. The paper then presents a theoretical analysis to characterize the conditions under which the framework allows correcting a wrong prediction. Finally, the framework is evaluated in a wide set of experiments, showcasing its benefits on worst-group accuracy when plugged on top of a varied set of baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Originality\n\n- The paper proposes novel methodology to improve the robustness of zero-shot models such as CLIP without fine-tuning or using extra data.\n\n### Quality\n\n- The simplicity of the debiasing techniques makes it a cheap solution that can be easily employed by any practitioner.\n- Accepting the assumptions, the theoretical analysis makes intuitive sense.\n\n### Clarity\n\n- The paper makes for a pleasing read: it is well written, easy to read and mostly clear.\n- The framework is clearly explained in detail with a straightforward formalization and algorithmic outline, making it easy to reproduce. Source code is also provided.\n\n### Significance\n\n- The method is employed on a varied set of baselines (CLIP ViT-B-32, CLIP ViT-L-14, ALIGN, AltCLIP for zero-shot image classification and BERT and ADA for zero-shot text classification) assessing its general applicability\n- The experimental evidence covers multiple datasets, namely Waterbirds, CelebA, PACS, VLCS and CXR14 for zs image classification and CivilComments, HateXplain, Amazon and Gender Bias for zs text classification. The datasets cover different domains."
                },
                "weaknesses": {
                    "value": "- The assumption of concept embeddings being orthonormal doesn\u2019t seem well motivated; do we expect the concept embedding of \u2018waterbird\u2019 to be orthogonal to \u2018water\u2019? I find the experiment in Appendix F.5 to be inconclusive due to the simplicity of the considered concepts, and I can\u2019t immediately understand why the average of the images having a higher cosine similarity should give any insight on the decomposition of the space in harmful, helpful and neutral subspaces.\n    - Unfortunately, the overall motivation and analysis seems to lay on this assumption, making it a core criticity of the work.\n- The 15.98% improvement claim in the abstract actually regards the increase in Worst Group accuracy and not the overall improvement which is probably not positive, I find it should be stated clearly to avoid misleading the reader.\n- The qualitative assessment does not really immediately convey the effect of increasing $u^k$. Some quantitative metrics would help, e.g. class separability measure such as the ratio of the inter-class distance to the average intra-class distance $\\frac{d_{\\text{inter}}}{\\frac{1}{2} (d_{\\text{intra}{C_1}} + d{\\text{intra}_{C_2}})}$\n- From the presentation perspective, the captions could be improved. Figure 1 could use a textual description to clarify what\u2019s going on in the image, e.g. how does it go from having two projected embeddings and a single one in the right part.  Analogously, the caption of Figure 2 doesn\u2019t state what are $Y_0$ and $Y_1$\n- The framework fails in several cases to maintain the average accuracy of the baseline, being therefore only advisable when worst group accuracy is the metric of interest. Is this realistic? It would be nice to have a method that fell back to the standard setting when the approach proves to be detrimental."
                },
                "questions": {
                    "value": "Table 1\n\n- why does the model perform so much worse on Waterbirds except that for CLIP-B?\n\nFigure 2\n\n- what are Y_0 and Y_1? I guess they are some sort of class prototypes, but where are they defined?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1606/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1606/Reviewer_R1Zx",
                        "ICLR.cc/2024/Conference/Submission1606/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764570479,
            "cdate": 1698764570479,
            "tmdate": 1700464275444,
            "mdate": 1700464275444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PHZKhFlQuc",
                "forum": "fCeUoDr9Tq",
                "replyto": "RYxbq9JfRe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for noting the **novelty of our method**! We appreciate the kind comments and valuable feedback. We have make adjustments to figure captions as suggested by the reviewer in our updated draft.\n\n* **On the orthonormality assumption of concept embeddings**. We described these vectors as orthonormal purely for simplicity of explanation and derivation. However, **our method does not require the orthonormality assumption**. In general, if the $z_i$ are not orthonormal, we could describe them with a change of basis. The key requirement is that the subspaces where the helpful/harmful concepts live are not identical (i.e., as captured by the rank). Our method will fail if the subspaces of harmful concepts are, for example, identical to the subspace of helpful concepts. This is a weak assumption, however. Indeed, if it does not hold, harmful and helpful prediction concepts are totally aligned, so that good prediction is hopeless no matter what. We do not observe this to be the case in practice. \n    \n    In terms of the underlying theoretical model, we offer two arguments in support of our position. First, the empirical measurements (given below). Second, similar findings have been proposed in a variety of recent work [1, 2, 3, 4, 5]:\n[1] Trager, Matthew, et al. \"Linear spaces of meanings: compositional structures in vision-language models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[2] Chuang, Ching-Yao, et al. \"Debiasing vision-language models via biased prompts.\" arXiv preprint arXiv:2302.00070 (2023).\n[3] Park, Kiho, Yo Joong Choe, and Victor Veitch. \"The Linear Representation Hypothesis and the Geometry of Large Language Models.\" arXiv preprint arXiv:2311.03658 (2023).\n[4] Jiang, Yibo, Bryon Aragam, and Victor Veitch. \"Uncovering Meanings of Embeddings via Partial Orthogonality.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023.\n[5] Dev, Sunipa, et al. \"OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings.\" Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.\n\n* **On Appendix F5 results**. The purpose of this experiment is to establish further evidence for our theoretical model. We sample what we expect to be *cleaner* or less contaminated datapoints expressing concepts (e.g., an image of all yellow pixels, an image of a square). In other words, we expect that these datapoints are likely to have smaller coefficients for irrelevant concepts (and fewer relevant concepts). We hypothesize that  word embeddings with smaller harmful coefficient will be more similar in the embedding space with these *cleaner* images, and thus have a higher cosine similarity. \n\n    To address the reviewer's concern on concepts simplicity, we perform the same experiment using CIFAR10 test set.\n\n|Class|Original embedding|Average embedding|\n|-|:-:|:-:|\n|Horse|0.258|**0.261**|\n|Automobile|0.238|**0.251**|\n|Dog|0.237|**0.240**|\n|Bird|0.248|**0.260**|\n|Cat|0.232|**0.245**|\n|Airplane|0.241|**0.262**|\n|Ship|0.244|**0.253**|\n|Truck|0.243|**0.248**|\n|Deer|0.243|**0.269**|\n|Frog|0.235|**0.256**|\n\nIndeed, even on more complicated concepts like those in the CIFAR10 labels above, the text embeddings with noise component averaged out have higher cosine similarities to the corresponding *cleaner* image embeddings. This result increases our confidence in the theoretical model. \n\n*  **On the improvement claim.** Thank you for pointing this out! RoboShot produces average improvement of worst-group accuracy% of 15.95% with a trivial average decrease in average accuracy% of 1.44%. As suggested by the reviewer, have updated this number in the current abstract. We note that our findings are generally consistent with the robustness literature, where improving worst-group accuracy is the goal, and a decrease in accuracy overall is common. At the same time, we highlight that in several scenarios *both of thes metrices are improved.* We describe the reason for this in more depth below."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700273998311,
                "cdate": 1700273998311,
                "tmdate": 1700273998311,
                "mdate": 1700273998311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B5HhZvd8zF",
                "forum": "fCeUoDr9Tq",
                "replyto": "PHZKhFlQuc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1606/Reviewer_R1Zx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1606/Reviewer_R1Zx"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their rebuttal. While I am still not totally convinced about concept embeddings being orthogonal, I understand that the method doesn't require them to be. \nI find my concerns to be addressed by the response, and I am therefore raising my score to accept."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464255214,
                "cdate": 1700464255214,
                "tmdate": 1700464255214,
                "mdate": 1700464255214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p7QhyMx1Go",
            "forum": "fCeUoDr9Tq",
            "replyto": "fCeUoDr9Tq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1606/Reviewer_umVa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1606/Reviewer_umVa"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to improve zeroshot performance of various foundation models by trying to segregate the representations into 3 set of orthonormal basis\u2014 harmful,helpful and benign vectors. Using a language model, the authors try to identify the set of harmful and helpful basis, and then try to remove/boost those basis accordingly. Overall, the problem is well motivated and gives good results. The method section however needs some efforts in writing to clarify some of the intricacies of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The overall motivation and idea of the paper is quite novel with interesting applications across various foundational models like CLIP and LLM.\n- The empirical results also are quite strong."
                },
                "weaknesses": {
                    "value": "- $X_{proj}$ has not been defined in LFA section.\n- The authors should clarify how the basis vectors ($z$) are identified in the experiments, as the decomposition of insight vectors is based on that.\n- I understand that the proposed approach is poised to give major gains in class imbalance settings or well known setting with spurious features. However, I encourage the authors to also provide results in standard classification tasks like imagenet using CLIP. I fear that in many standard tasks, removing these spurious features might hurt as well. However, one can always choose to not remove these. \n- Do the authors have any insights or ablations as to how many insight vectors ($m$) is needed. I couldn\u2019t see those details."
                },
                "questions": {
                    "value": "See weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771322939,
            "cdate": 1698771322939,
            "tmdate": 1699636088813,
            "mdate": 1699636088813,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p9fW8HnPb9",
                "forum": "fCeUoDr9Tq",
                "replyto": "p7QhyMx1Go",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank your for pointing out the novelty of our method and the strength of our empirical results!\n* **On $X_{proj}$'s definition.** $X_{proj}$ is the sample embeddings post-application of the RoboShot procedure. We defined $X_{proj}$ in the parameters of Algorithm 2.\n* **On basis vectors ($z$).** The key strength of our method is that **it does not assume access to basis vectors $z$**. These vectors are part of our theoretical model only---but our approach does not require identifying them. In our model, we decompose embeddings as combinations of harmful, helpful, and benign components: $x = \\sum_{s=1}^S \\alpha_s^{\\text{harmful}} z_s + \\sum_{r=S+1}^{S+R} \\alpha_r^{\\text{helpful}} z_r + \\sum_{b=S+R+1}^{S+R+B} \\alpha_b^{\\text{benign}} z_b.$ \n\n    The goal of our procedure is to reduce $\\alpha_s^{\\text{harmful}}$ and increase $\\alpha_{r}^{\\text{helpful}}$ (the coefficient of harmful and helpful basis vectors). Since indeed these cannot be directly identified (i.e., we never know what $z_1$ is, for example), we query LLMs in the hope of text whose embeddings produce some information about these. Specifically, we obtain text from the LLM that we embedded. Such embeddings ideally span some subspace of the harmful $z$s that can be used to decrease $\\alpha_s^{\\text{harmful}}$ (and similary for the helpful concepts.). This does not ever require us to identify what $z_1, z_2, \\ldots$ are. \n\n* **On standard classification tasks.** As suggested by the reviewer, we performed an experiment on ImageNet (using CLIP ViT-B-32). The performance of our method is roughly the same (we saw a ~1.1% difference). We do not expect RoboShot to produce improvement in this setting---we anticipate that many of the CLIP models have seen ImageNet or variants at training time and that no substantial biases are present that can be corrected. Fortunately, the result indicates that RoboShot largely retains performance in standard classification settings.\n\n* **On the number of insight vectors needed**. To address the reviewer's question, we do an ablation experiment and provide a theoretical characterization on number of insights needed. We provide the deatils in this response, and updated our manuscript with Appendix F7 that provides *empirical ablation results for Waterbirds an CelebA datasets*. As expected, we **observe that performance improves as the number of insights increases**.\n\n    First, we point out that individual insights have different qualities---further suggesting the need for multiple insight vectors. The folowing table shows the different performance results from using one individual insight on harmful vector rejection\n\n|Insight pair| AVG% | WG%|\n|-|:-:|:-:|\n['a bird with coastal migration', 'a bird with inland migration']|76.0%|26.0%\n['a bird with keratin feathers physiology', 'a bird with hydrophobic feathers physiology']|80.8%|29.3%|\n['a bird that eats bugs.', 'a bird that eats mainly fish.']|79.6%|30.2%|\n['a bird that lives in watery environments', 'a bird that lives on land.']|86.2%|52.2%|\n\nOn the theoretical characterization: Increasing the number of insights is beneficial until the span of insight vectors becomes the subspace of helpful/harmful features. Thus, for optimality, we need insight vectors up to the rank of the helpful/harmful subspaces. A synthetic experiment where we vary the number of insights validates this hypothesis. Here, the number of helpful/harmful/benign concepts is 12 respectively, with the embedding dimension 36 (12 + 12 +12). We sequentially increase the number of insights based on a similar synthetic experiment setup in Appendix F.2. In Appendix F.7 Figure 8, We can observe that the AVG/WG performance improves until the number of insights is the same with the rank of harmful/helpful subspace. After this, due to the presence of noise in superfluous insights, helpful components are removed / harmful components are added, which results in a performance decrease. \n\n\nThis suggests that *there is indeed a sweet spot for the number of insight vectors*. While our method works particularly well if each insight from the LLM exactly targets a helpful/harmful component, there can be multiple insights for each concept. To resolve this issue, our ongoing work includes the application of robust subspace learning, replacing the QR decomposition step. This step uses the entire subspace that insight vectors span. In constrast, robust subspace learning only use the subspace that has strong signals to mitigate the challenges induced by noise [1]. We are also developing a robust version of the theoretical characterization---but we leave this for future work. \n\n[1] Lerman, Gilad, and Tyler Maunu. \"An overview of robust subspace recovery.\" Proceedings of the IEEE 106.8 (2018): 1380-1410."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700273842052,
                "cdate": 1700273842052,
                "tmdate": 1700273842052,
                "mdate": 1700273842052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T6F8bpDnTN",
                "forum": "fCeUoDr9Tq",
                "replyto": "p9fW8HnPb9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1606/Reviewer_umVa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1606/Reviewer_umVa"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the clarifications."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546163782,
                "cdate": 1700546163782,
                "tmdate": 1700546163782,
                "mdate": 1700546163782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bEmoHgtZ4H",
            "forum": "fCeUoDr9Tq",
            "replyto": "fCeUoDr9Tq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1606/Reviewer_bCze"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1606/Reviewer_bCze"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose ROBOSHOT, a method for improving the robustness of pretrained models in zero-shot settings. It uses language models to obtain insights from task descriptions and uses these insights to remove harmful components and boost useful ones in model embeddings without any supervision.\n\nThe method is evaluated on nine image and NLP classification tasks and shows an average improvement of 15.98% over several zero-shot baselines. It is compatible with various pretrained and large language models and can further boost performance with a zero-shot label-free adaptation variant where there are a large number of unlabeled examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed ROBOSHOT method is an interesting novel approach that improves the robustness of zero-shot models against harmful concepts without the manual identification of harmful concepts. It leverages insights obtained from large language models to refine embeddings, and address inherited biases.  I also find the theoretical arguments interesting  which characterizes the conditions under which ROBOSHOT can outperform existing methods in zero shot learning.\n\n- The  paper presents a well-structured and rigorous experimental evaluation across various datasets and model architectures.\n\n- The paper is written in a clear and accessible manner, I like the detailed explanations of the ROBOSHOT algorithm, the theoretical framework, and the evaluation methodology.\n\n- ROBOSHOT consistently outperforms several zero-shot baselines on multiple datasets. Having a powerful zero-shot learning method can address many real-life image classification tasks where labels are hard to come by."
                },
                "weaknesses": {
                    "value": "- No large scale datasets like imagenet\n\n- The benchmarks are limited to zero shot classification which is an easy task compared to zero-shot semantic segmentation and instance segmentation where this method could struggle.\n\n- I don't see this work as actual zero shot because the pretrained model has so much information about the classes present in the chosen datasets. This work would be more impactful if the experiments were conducted on rare classes to test whether this method generalizes well.  ChatGPT has been trained on the internet, so this work is far from zero shot learning unless we include classes that are least likely to be seen by ChatGPT."
                },
                "questions": {
                    "value": "Please address the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1606/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699279181696,
            "cdate": 1699279181696,
            "tmdate": 1699636088730,
            "mdate": 1699636088730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ccEsb9AUg1",
                "forum": "fCeUoDr9Tq",
                "replyto": "bEmoHgtZ4H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for noting the **novelty of our work** and the strong evaluation!\n* **On large scale datasets like ImageNet**. Our technique *does not require training*---so *large datasets are not a problem*. In general, the costs of our method are quite low for any dataset size. We obtain the insights and embed them, which is performed just once per dataset. These embeddings are re-used for modifying each datapoint's representation, which comes at only a marginal cost on top of inference. Indeed, some of the datasets in our experiments are reasonably large (e.g., CivilComments-WILDS has 133782 samples, Amazon-WILDS has 90078 samples). We list all dataset details in Appendix D1. \n\n    The reason we did not focus on ImageNet in particular is that we do not anticipate our method to produce any substantial improvements for this dataset. This is to be expected for two reasons: (i) CLIP-style models already perform well on ImageNet, and (ii) it is likely that ImageNet or variants are in the training data used for these models, so that biases or spurious correlations are not as harmful. We verified this by running our method on ImageNet, where we observed similar performance as with vanilla zero-shot classification. \n\n* **On zero-shot classification versus other tasks**. We anticipate that our technique is usable in such settings as well. In general, the goal of our technique is to remove some harmful aspects of pretrained models (and boost some helpful ones) --- this is done by the harmful feature rejection (Algorithm 1 line 3-4), and useful feature addition (Algorithm 1 line 7-8). The overall difficulty of the task is not the crucial ingredient, but rather the presence or absence of these components. For our ongoing work, we are investigating using our technique in concert with segmentation.\n\n* **On the zero-shot terminology**. We agree that there are a variety of uses of the term \"zero-shot\". The usage that we refer to is using a pretrained model **without** (1) modifying the prompt with additional examples (i.e., few-shot prompting), (2) fine-tuning a pretrained model with labeled data, (3) continual pretraining with additional domain-specific data. All of these methods are commonly used for task-specific predictions. \n\n    This usage is consistent with much of the literature. For example, CLIP's authors describe CLIP in the following way: \"We ... turn CLIP into a zero-shot classifier.\""
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700273645941,
                "cdate": 1700273645941,
                "tmdate": 1700273645941,
                "mdate": 1700273645941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3d5ELin8jr",
                "forum": "fCeUoDr9Tq",
                "replyto": "bEmoHgtZ4H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1606/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe thank you again for your feedback, questions, and suggestions! We believe we have answered all of your questions in our responses and the updated draft. If you have additional questions, we would love to answer them!\n\nThe Authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1606/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583897278,
                "cdate": 1700583897278,
                "tmdate": 1700583897278,
                "mdate": 1700583897278,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]