[
    {
        "title": "Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation"
    },
    {
        "review": {
            "id": "dAz7rrHpgb",
            "forum": "iAW2EQXfwb",
            "replyto": "iAW2EQXfwb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6772/Reviewer_cq3K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6772/Reviewer_cq3K"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes NCERL, an ensemble Reinforcement Learning (RL) method for game level generation with more diversity. NCERL uses a set of different Gaussian actors which outputs actions in the latent space, and the output is decoded into different level segments by a GAN decoder. The final output segment is chosen with the probability generated by a learned selector. To encourage diversity between different actors, a regularizer of Wasserstein distance between policy distributions (and weighted by selector) is added to the reward. The paper also gives a modified gradient and convergence proof on the new reward, as the regularizer depends on the whole policy instead of a single action. On a well known level-generation benchmark, NCERL achieves comparable reward (measuring game-design goals) with better level diversity, and can also perform a well trade-off between diversity and reward by controlling its regularization coefficient."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. As this paper uses a latent space for policy and existing decoder for segment generation, the problem addressed by this paper is not only interesting, but actually quite general: how to achieve a trade-off between policy diversity and performance, with game level generation being one of its real-world applications.\n\n2. The paper has a sound theoretical basis, with a convergence proof for the new reward (which, similar to soft actor-critic, depends on policy distribution instead of action) and a rigorous, modified version of gradient update.\n\n3. The proposed work is scalable, with a well-designed and parallelized reward evaluation framework."
                },
                "weaknesses": {
                    "value": "**Some details of the papers are not presented clearly enough.**\n\n1. Figure 1 has many math symbols and no caption on high-level ideas of each component; in addition, the meaning of $i\\leftarrow 2$ in the figure is unclear.\n\n2. There is no clear definition on what a state is in the paper; the readers can only speculate that it is a latent vector from the end of Section 2.1 (\"... a randomly sampled latent vector is used as the initial state\").\n\n3. There is no description on how Wasserstein distance is calculated. It is true that 2-Wasserstein distance between Gaussian distributions can be easily calculated, but it would be better if the Gaussian property can be emphasized at the beginning of Section 3.2, a formula can be given to make the paper self-contained, and \"2-Wasserstein\" instead of \"Wasserstein\" is specified.\n\n4. typos: in conclusion, bettwe -> better."
                },
                "questions": {
                    "value": "I have two questions:\n\n1. In Table 1, the trade-off of NCERL between reward and diversity in some of the environments does not follow the trend; for example, in Mario Puzzle, the diversity with $\\lambda$ seems to have two peaks (0.2 and 0.5), and the reward at $\\lambda=0.1$ is the worst despite of low diversity. Could the author explain this?\n\n2. Currently, there is only comparison between NCERL, ensemble RL methods and non-ensemble RL methods, and the encoding is over GAN. What is the performance of non-RL solutions, such as scripted (possibly with learnable parameters) solution, or supervised/self-supervised learning over more recent generative models such as diffusion models or VAEs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6772/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6772/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6772/Reviewer_cq3K"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697508689277,
            "cdate": 1697508689277,
            "tmdate": 1699636781170,
            "mdate": 1699636781170,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3yKvApKjhr",
                "forum": "iAW2EQXfwb",
                "replyto": "dAz7rrHpgb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6772/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer cq3K"
                    },
                    "comment": {
                        "value": "Thank you for the insightful comments and we greatly appreciate your constructive suggestions for improving the presentation. Our paper has been carefully revised according to your suggestions and we also checked through the paper again for typos. The changes are highlighted in blue. For your questions, we hope the following responses address them properly. \n\n**Q1: In Table 1, the trade-off of NCERL between reward and diversity in some of the environments does not follow the trend; for example, in Mario Puzzle, the diversity with \u03bb seems to have two peaks (0.2 and 0.5), and the reward at \u03bb=0.1 is the worst despite of low diversity. Could the author explain this?**\n\nWe consider this may be caused by some local optima during the training. Therefore, the training results of NCERL can be a little bit unstable. The black-box decoder further makes the training results hard to predict. The $\\lambda$ factor does not affect the trade-off between reward and diversity but also affects the exploration efficiency, making the effect of $\\lambda$ on the performance more non-monotonous. The relative poor performance of the generator trained with $\\lambda = 0.1$ can be attributed to an abnormal trial which only yielded $30.23$ of average reward and $1723$ of diversity. For future work, we consider integrating our method with multi-objective reinforcement learning [1] to train a set of non-dominated policies with mutable regularisation coefficients. This could potentially bypass the problem of unstable and non-monotonous effects of the regularisation weight. To facilitate a more in-depth analysis of the results, we present the reward and diversity of each independent trial. Full results are presented and discussed in Appendix E.1.\n\n**Q2: Currently, there is only a comparison between NCERL, ensemble RL methods and non-ensemble RL methods, and the encoding is over GAN. What is the performance of non-RL solutions, such as scripted (possibly with learnable parameters) solutions, or supervised/self-supervised learning over more recent generative models such as diffusion models or VAEs?**\n\nDue to the limited time, we are not able to compare our approach with all the non-RL approaches listed by the reviewer, but we made an effort to conduct a meaningful comparison. We use the code of [2] to train diffusion models (DDPM) for five independent trials. Specifically, we utilized the code from [2] to train DDPM for five independent trials. In this process, we randomly sampled 25 noises to generate 25 level segments, then concatenated them with an initial segment used in the RL generator testing. This procedure was applied to each of the 500 initial segments used for testing RL generators, resulting in a test set of 500 levels. We evaluated these test sets in terms of both reward and diversity. \n\n|Task|Criterion|DDPM|$\\lambda$=$0.0$|$\\lambda$=$0.1$|$\\lambda$=$0.2$|$\\lambda$=$0.3$|$\\lambda$=$0.4$|$\\lambda$=$0.5$|\n|-------------|-----------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|\n|MarioPuzzle|Reward|*-29.45*|**55.24**|51.42|53.78|53.22|54.59|53.26|\n|MarioPuzzle|Diversity|1630|*1342*|1570|1940|1688|1698|**1967**|\n|MultiFacet|Reward|*-119.4*|**46.39**|46.16|45.35|37.87|40.86|35.77|\n|MultiFacet|Diversity|**1630**|*401.6*|492.3|620.2|1024|889.6|1142|\n\nAccording to the results, DDPM exhibited good diversity scores but performed poorly in terms of reward. This is attributed to the fact that the training of DDPM does not consider reward functions that evaluate the quality of generated levels. To our knowledge, DDPM can not optimise certain objectives to cater to customised objectives, while the scripted method relies on domain knowledge and needs significant development costs. The generated samples of DDPM are available in our anonymous code repository (https://anonymous.4open.science/r/NCERL-Diverse-PCG-4F25/, the generation_results folder). \n\n[1] Hayes, Conor F., et al. \"A practical guide to multi-objective reinforcement learning and planning.\" Autonomous Agents and Multi-Agent Systems 36.1 (2022): 26.\n\n[2] Lee, Hyeon Joon, and Edgar Simo-Serra. \"Using Unconditional Diffusion Models in Level Generation for Super Mario Bros.\" 2023 18th International Conference on Machine Vision and Applications. IEEE, 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726819992,
                "cdate": 1700726819992,
                "tmdate": 1700726819992,
                "mdate": 1700726819992,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9h3SCvHZeX",
            "forum": "iAW2EQXfwb",
            "replyto": "iAW2EQXfwb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6772/Reviewer_t3p4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6772/Reviewer_t3p4"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an ensemble reinforcement learning approach for generating diverse game levels. The approach uses multiple sub-policies to generate different alternative level segments, and stochastically selects one of them following a selector model. The paper also integrates a novel policy regularisation technique, which is a negative correlation regularisation that increases the distances between the decision distributions determined by each pair of actors. The regularisation is optimised using regularised versions of the policy iteration and policy gradient, which provide general methodologies for optimising policy regularisation in a Markov decision process. The paper's contributions are:\n1. The proposed ensemble reinforcement learning approach for generating diverse game levels.\n2. The novel policy regularisation technique that encourages the sub-policies to explore different regions of the state-action space.\n3. The regularised versions of the policy iteration and policy gradient algorithms that provide general methodologies for optimising policy regularisation in a Markov decision process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "originality: the paper proposes a novel approach for generating diverse game levels using ensemble reinforcement learning and policy regularisation. The paper develops two theorems to provide general methodologies for optimizing policy regularisation in a Markov decision process. The first theorem is a regularised version of the policy iteration algorithm, which is a classic algorithm for solving MDPs. The second theorem is a regularised version of the policy gradient algorithm, which is another classic algorithm for solving MDPs.\n\nquality: the paper provides a detailed description of the proposed approach and the regularisation technique. The paper also provides theoretical proofs of the regularised versions of the policy iteration and policy gradient algorithms. \n\nclarity: the paper is well-written and easy to follow. The authors provide clear explanations of the proposed approach and the regularisation technique."
                },
                "weaknesses": {
                    "value": "the proposed approach assumes that the reward function is known and fixed. However, in practice, the reward function may be unknown or may change over time. Therefore, the proposed approach may not be applicable in such scenarios.\n\nthe paper only considers a single game genre (platformer) and a single game engine (Super Mario Bros.). The proposed approach may not be directly applicable to other game genres or engines."
                },
                "questions": {
                    "value": "Q1: can the proposed approach be directly applicable to 3D game levels or other types of game content? or what are the difficulties in this extension, such as complex reward design or high computational burden?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6772/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6772/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6772/Reviewer_t3p4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697718374674,
            "cdate": 1697718374674,
            "tmdate": 1699636781032,
            "mdate": 1699636781032,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PdTQPaw4cz",
                "forum": "iAW2EQXfwb",
                "replyto": "9h3SCvHZeX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6772/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer t3p4"
                    },
                    "comment": {
                        "value": "Thank you for providing insightful comments. We hope the following response addresses your concerns. Our paper is revised and updated according to reviewers\u2019 comments, the changes are highlighted in blue.\n\n**Q1: can the proposed approach be directly applicable to 3D game levels or other types of game content? or what are the difficulties in this extension, such as complex reward design or high computational burden?**\n\nOur method can be directly applicable to 3D game levels like Minecraft generation [1] and other types of game content like music [2] and narrative generation [3] since RL has been applied to generate that content. Those applications could benefit from our proposed method since we only change the RL algorithm while maintaining the problem setting unchanged.\n\nWhen applying our approach to other scenarios, the design of the reward can be challenging, as indicated by the reviewer. However, the procedural content generation (PCG) community has proposed a range of evaluation metrics [4], which can serve as the reward function in our framework. Taking Minecraft as an example again, Jiang et al. [1] have proposed some reward functions to train controllable RL-based game level generators, while there is also a set of evaluation metrics verified based on human evaluation scores [5], those metrics could be used as reward functions. For the issue of high computational burden, we have devised and implemented an asynchronous framework to speed up the training. On the other hand, the use of an action decoder, proposed in [6], contributes to faster generation speeds. There are some generative models that can generate high-quality 3D game levels, such as world-GAN [7] for Minecraft world generation and VAE-GAN for 3D indoor scene generation [8]. By employing these models as decoders, our method can be directly applied to the corresponding application scenarios with rapid generation speeds.\n\n[1] Jiang, Zehua, et al. \"Learning Controllable 3D Level Generators.\" Proceedings of the 17th International Conference on the Foundations of Digital Games. 2022.\n\n[2] Jaques, Natasha, et al. \"Generating music by fine-tuning recurrent neural networks with reinforcement learning.\" (2016).\n\n[3] Huang, Qiuyuan, et al. \"Hierarchically structured reinforcement learning for topically coherent visual story generation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.\n\n[4] Shaker, Noor, Julian Togelius, and Mark J. Nelson. \"Procedural content generation in games.\" (2016): 978-3.\n\n[5] Herv\u00e9, Jean-Baptiste, and Christoph Salge. \"Comparing PCG metrics with Human Evaluation in Minecraft Settlement Generation.\" Proceedings of the 16th International Conference on the Foundations of Digital Games. 2021.\n\n[6] Shu, Tianye, Jialin Liu, and Georgios N. Yannakakis. \"Experience-driven PCG via reinforcement learning: A Super Mario Bros study.\" 2021 IEEE Conference on Games (CoG). IEEE, 2021.\n\n[7] Awiszus, Maren, Frederik Schubert, and Bodo Rosenhahn. \"World-gan: a generative model for minecraft worlds.\" 2021 IEEE Conference on Games (CoG). IEEE, 2021.\n\n[8] Li, Shuai, and Hongjun Li. \"Deep Generative Modeling Based on VAE-GAN for 3D Indoor Scene Synthesis.\" International Journal of Computer Games Technology 2023 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726212891,
                "cdate": 1700726212891,
                "tmdate": 1700726212891,
                "mdate": 1700726212891,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZhWv4N9YLa",
            "forum": "iAW2EQXfwb",
            "replyto": "iAW2EQXfwb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6772/Reviewer_qX6Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6772/Reviewer_qX6Q"
            ],
            "content": {
                "summary": {
                    "value": "**Problem Setting**\n\nWe want to do level generation but we want to induce some diversity in how the levels are generated. Here the policies define level generators which build the level through an MDP.\n\n**Algorithm / NN Structure**\n\nThe policies are defined as mixtures of Gaussians, implemented by creating a set of sub-policies along with a weighting. The weighting itself id modelled by a selector policy, creating a form of hierarchy.\n\nThe sub-policies are regularized to be diverse from each other using a Wasserstein distance. This distance is clipped, encouraging policies to be diverse only if their decisions are too close. Sub-policies have Gaussian action heads. Regularization is implemented as an auxilliary reward.\n\nA regularized version of policy iteration of the policy gradient are presented. Thus the agent is trained not only to optimize for diversity in the current timestep, but in future timesteps via a reguarlized value function.\n\nThe practical implementation is built off SAC.\n\nExperiments are presented on the Mario level generation benchmark. Results show that NCERL is able to achieve comparable reward to other methods, and outperform in terms of diversity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a clean and thorough study of a method to induce diverse level generation. The idea is to define policies as a mixture of sub-policies, then regularize those sub-policies so that diversity is increased. While this is a straightforward idea, it is especially applicable in a domain such as level generation where diversity is desired in itself rather than as simply a means towards exploration. The quality of the writing and presentation is solid and clear. Theoretical results are presented re-deriving the policy iteration and policy gradient update explicitly in terms of regularizing the diversity between sub-policies, and proofs are presented regarding convergence. The significance of this work stems from its thorough theoretical contributions."
                },
                "weaknesses": {
                    "value": "Because the experiments are largely domain-specific and improve on diversity rather than pure performance, the significance of this work is limited. \n\nWhile there are novel derivations and a clean interpretation of regularizing the policy gradient, the idea of representing an agent as sub-policies has been explored in fields such as skill discovery and hierarchical reinforcement learning, which were not referenced in this work. \n\nThe description of the domain is unclear to me as a reader, e.g. what is the action space of an agent generating Mario levels? What are the criteria used to evaluate reward and diversity? I would have liked to see examples of the generated levels."
                },
                "questions": {
                    "value": "See above for questions related to the experimental section.\n\nThe section on asynchronous evaluation seems orthogonal to the main contribution of the work. Asynchronous RL has been explored in the actor-critic setting (e.g. A3C), which this work uses as it builds off SAC. Is there a specific connection between the asynchronous implementation and the novel contribution here?\n\nWhat does the behavior of the weighting-selector policy look like? It would provide more clarity into the method to showcase how often this selector policy utilizes specific sub-policies, or if certain sub-policies go unused.\n\nIt may help to label the other comparison methods in Figure 4."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778387012,
            "cdate": 1698778387012,
            "tmdate": 1699636780900,
            "mdate": 1699636780900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XTDKR95e4s",
                "forum": "iAW2EQXfwb",
                "replyto": "ZhWv4N9YLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6772/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer qX6Q"
                    },
                    "comment": {
                        "value": "Thank you for providing insightful comments. We are especially encouraged by your appreciation of our theoretical contribution. We hope the following response addresses your concerns. Our paper is revised and updated according to reviewers' comments, the changes are highlighted in blue.\n\n**Q1: The section on asynchronous evaluation seems orthogonal to the main contribution of the work. Asynchronous RL has been explored in the actor-critic setting (e.g. A3C), which this work uses as it builds off SAC. Is there a specific connection between the asynchronous implementation and the novel contribution here?**\n\nIndeed, the asynchronous evaluation is orthogonal to the main contribution of this work. Therefore, we removed it from the contribution statements in Section 1 of the revised paper.\n\n**Q2: What does the behaviour of the weighting-selector policy look like? It would provide more clarity into the method to showcase how often this selector policy utilizes specific sub-policies, or if certain sub-policies go unuse**\n\nIf the $\\lambda$ used in training is large, the probabilities of selecting sub-policies change over time step, and all the sub-policies are used. While if the $\\lambda$ is small, it is possible that some sub-policies go unused. To showcase the selection probabilities, we pick two NCERL generators trained with $m=5, \\lambda= 0.5$ and $m = 5, \\lambda=0.1$ and record their selection probabilities during stochastically generating two levels from the same initial state, respectively.\n\nRegarding the generator trained with $\\lambda = 0.5$, all the sub-policies are activated multiple times within the two trials. Regarding the generator trained with $\\lambda = 0.1$, some of the selection probability is near zero and sub-policies 2 and 3 are never used within the two trials. As the regularisation coefficient $\\lambda$ increases, the probabilities of selecting sub-policies become more uniform. The tables are included in Appendix E.2  of our revised paper with the images of generated levels in those two trials. We also present them as follows for reference."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724430819,
                "cdate": 1700724430819,
                "tmdate": 1700724430819,
                "mdate": 1700724430819,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rjDWhsidFM",
            "forum": "iAW2EQXfwb",
            "replyto": "iAW2EQXfwb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6772/Reviewer_NGar"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6772/Reviewer_NGar"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method for online generating diverse game levels through an ensemble of negatively correlated RL generators. The authors derived a policy update operator under the diversity bonus. Apart from that, the authors propose an async framework for speeding up the training. Experiments show that the method is able to generate a wide range of policies through tunning the diversity coefficient $\\lambda$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is written in clarity. Hypotheses are well supported by the experiments.\n2. Originality looks good to me (or maybe I am not following the OLG line of research, but I study MARL diversity, in which no noticeable significantly similar methods to my knowledge)"
                },
                "weaknesses": {
                    "value": "To my understanding, this method adds a reward bonus/diversity constraint to the diversity among the policies, where the proof is kind of established in the literature. The effect of adding diversity regularization is similar to the quality diversity methods, where you are pursuing optimal in the new reward space. The role of $\\lambda$ is close to the Lagrange multiplier in the dual formulation of the original problem with a diversity constraint. I think it is ok to include them as contributions to the paper but building theoretical analysis on the interactions among ensemble policies or regularization effect would be more interesting. My ratings are subject to change."
                },
                "questions": {
                    "value": "It is similar to (adversarial) diversity in populations of policies that learn incompatible policies or impose distance(entropy) regularization. Can the authors provide their view of how it compares to population-based methods with diversity regularization?\n\n[1] Xing, D., Liu, Q., Zheng, Q., Pan, G., & Zhou, Z. H. (2021). Learning with Generated Teammates to Achieve Type-Free Ad-Hoc Teamwork. In IJCAI (pp. 472-478).\n\n[2] Lupu, A., Cui, B., Hu, H. &amp; Foerster, J.. (2021). Trajectory Diversity for Zero-Shot Coordination. <i>Proceedings of the 38th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 139:7204-7213 Available from https://proceedings.mlr.press/v139/lupu21a.html.\n\n[3] Cui, B., Lupu, A., Sokota, S., Hu, H., Wu, D. J., & Foerster, J. N. (2022, September). Adversarial Diversity in Hanabi. In The Eleventh International Conference on Learning Representations.\n\n[4] Rahman, A., Fosong, E., Carlucho, I., & Albrecht, S. V. (2023). Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity. Transactions on Machine Learning Research.\n\n[5] Charakorn, R., Manoonpong, P., & Dilokthanakul, N. (2022, September). Generating Diverse Cooperative Agents by Learning Incompatible Policies. In The Eleventh International Conference on Learning Representations.\n\n[6] Rahman, A., Cui, J., & Stone, P. (2023). Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents. arXiv preprint arXiv:2308.09595."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813928192,
            "cdate": 1698813928192,
            "tmdate": 1699636780781,
            "mdate": 1699636780781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6RGavbDgTj",
                "forum": "iAW2EQXfwb",
                "replyto": "rjDWhsidFM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6772/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer NGar"
                    },
                    "comment": {
                        "value": "Thank you for providing insightful comments. We hope the following response addresses your concerns. Our paper has been revised according to reviewers' comments, the changes are highlighted in blue.\n\n**Q1: It is similar to (adversarial) diversity in populations of policies that learn incompatible policies or impose distance(entropy) regularization. Can the authors provide their view of how it compares to population-based methods with diversity regularization?**\n\nWe appreciate the recommended papers from the reviewers; they have been very helpful. We have checked all of them and found [2,3,5] are the most related ones and have included them in the \"Population-based RL\" (Renamed from \"Policy Ensemble in RL\") part of Section 2 in our revised paper. This section also includes a discussion of population diversity in ensemble RL. For papers [2], [3] and [5], we discuss them as follows.\n\n* Paper [2] uses JS divergence of individual agents\u2019 trajectories as a diversity regularisation. It is similar to our diversity regularisation, while we use the regularisation with a different distance metric and to the decision distribution rather than trajectories. \n\n* Paper [3] considers an adversarial diversity, which makes a policy different from an \u201crepulser\u201d policy. This is realised by modifying the TD target, while our approach uses distance between decision distributions of the sub-policies. \n\n* Paper [5] learns incompatible policies within a joint policy, meaning that substituting a policy in the joint policy with the incompatible policies causes a significant deterioration in performance. Our sub-policies can be viewed as a sort of incompatible policies but the measurement is a distance metric instead of performance. \n\nThe core differences between our approach and those approaches are: 1. our method makes decisions with all individual policies as a whole, whereas in those methods, each individual policy makes its own decisions; 2. those works consider the diversity in the policy population as the goal, while our work focuses on the diversity of generated levels as the goal.\n\n[2] Lupu, A., Cui, B., Hu, H. & Foerster, J.. (2021). Trajectory Diversity for Zero-Shot Coordination. <i>Proceedings of the 38th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 139:7204-7213 Available from https://proceedings.mlr.press/v139/lupu21a.html.\n\n[3] Cui, B., Lupu, A., Sokota, S., Hu, H., Wu, D. J., & Foerster, J. N. (2022, September). Adversarial Diversity in Hanabi. In The Eleventh International Conference on Learning Representations.\n\n[5] Charakorn, R., Manoonpong, P., & Dilokthanakul, N. (2022, September). Generating Diverse Cooperative Agents by Learning Incompatible Policies. In The Eleventh International Conference on Learning Representations.\n\nRegarding the comment **\"... I think it is ok to include them as contributions to the paper, but building theoretical analysis on the interactions among ensemble policies or regularization effect would be more interesting.\"**: Indeed, theoretical analysis of the interactions among ensemble policies or the regularisation effect is an interesting future direction. Unfortunately, we were unable to conduct a theoretical analysis of these aspects during this rebuttal period. In our future work, we plan to delve into such theoretical analysis. For now, we provide experimental evidence showcasing the interactions among ensemble policies in Appendix E.2 of the revised paper. Specifically, we report the selection probabilities of each sub-policy during generating a level. We observe the selection probabilities of the sub-policies can be adaptively adjusted during generating levels. The selection probabilities also become more uniform as the regularisation coefficient $\\lambda$ increases. We hope this added analysis can partially address your concern."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723441184,
                "cdate": 1700723441184,
                "tmdate": 1700723441184,
                "mdate": 1700723441184,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]