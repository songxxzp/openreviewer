[
    {
        "title": "Tailoring Mixup to Data using Kernel Warping functions"
    },
    {
        "review": {
            "id": "AHXPxTsjPC",
            "forum": "QhsZwzBYaU",
            "replyto": "QhsZwzBYaU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3312/Reviewer_djAy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3312/Reviewer_djAy"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a Kernel Warping Mixup technique toward a flexible distribution adjustment by considering the sample similarity. The proposed method changes the underlying distributions used for sampling interpolation coefficients by defining warping functions, allowing inputs and labels to be disentangled, and providing a framework that encompasses several variants of Mixup. To this end, the author introduces a similarity kernel that considers the distance between points when selecting a parameter for the warping function. This paper demonstrates the applicability of the Kernel Warping Mixup across classification and regression tasks, and cites improvements in performance, calibration, and efficiency while it requires less computation, additionally."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper provides a comprehensive analysis and comparison of related work in the field of data augmentation. The organization of the related work into different topics provides all-sided evaluations of the contributions.\n\n+ The proposed idea is somehow novel since few existing works link the coefficients of interpolation with data geometry. \n\n+ The proposed method has been proven effective in improving the model calibration while the improvement of in-distribution generalization is not very significant."
                },
                "weaknesses": {
                    "value": "- Compared to RegMixup, the proposed method brings marginal improvement in classification task accuracies on different models (Tables 2 and 3). Plus, the benchmark result of the Manifold Mixup in classification is missing. The concern is on the expressivity of the proposed method since general data augmentation aims to enrich the diversity of data and cover more uncertain data points as much as possible instead of encouraging similar data to be mixed more.\n\n- The warped Gaussian Kernel into the original Beta distribution for the mixup coefficient $\\lambda$ highly depends on batched data similarity. If the batch size is small (for large-scale tasks), the relative distances among the data will collapse, i.e. the same samples will be mixup with different $\\lambda$ whether an outlier appears. In Eqn (6), if $n$ is small, the total distance is sensitive to the largest sample distance. It weakens the robustness of the proposed method.\n\n- The evaluation and explanation of how the proposed method enhances calibration over vanilla Mixup are insufficient. For example, (Thulasidasan et al., 2019) have concluded that label smoothing in Mixup training significantly contributes to improved calibration, supported by comprehensive observations such as \"score of winning class along the time\" and \"overconfidence\". C-Mixup (Yao et al., 2022a) provides the theoretical justification for Theorem 3. To enhance the comparison, it would be better to establish a more meaningful connection between the proposed method and calibration, moving beyond only the presentation of ECE and Brier scores. \n\n=====================Typos==============\n\n- Missing reference to the initial mention of \"MIT\" in Section 2.2.\n- In Eqn(6), the LHS should indicate the dependence of $n$ if the measure on RHS depends on all $n$ samples."
                },
                "questions": {
                    "value": "1. Since general data augmentation aims to enrich the diversity of data and cover more uncertain data points, is adjusting the distribution of coefficients based on data distribution reasonable to enhance the expressivity? Please see weakness 1.\n\n2. Why choose different benchmarks between Tables 3 and 4, since Manifold Mixup can also applied to classification problems?\n\n3. As suggested by (Wang et al., 2023), Mixup fails to improve the calibration in terms of other metrics such as Calibrated ECE and Optimal ECE. Can the author explain more or provide more evidence that the method in this paper does help the calibration? Why does altering the underlying distribution of $\\lambda$ yield a better calibration over vanilla Mixup, even with the same similarity measure of input and output (Row 2 in Table 1)?\n\nI am keenly awaiting the author's response to see how they address my concerns, and I am open to increasing my score based on the response."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical concerns, since interpolation methods use accessible authorised data."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3312/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3312/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3312/Reviewer_djAy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698147026628,
            "cdate": 1698147026628,
            "tmdate": 1700707059886,
            "mdate": 1700707059886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j7FW5D5utr",
                "forum": "QhsZwzBYaU",
                "replyto": "AHXPxTsjPC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer djAy 1/2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedbacks on our work.\n\n- W1:\n  - 1. **Comparison with RegMixup:** We would like to refer the reviewer to Appendix B in which we show that compared to both RegMixup and MIT, our proposed approach is much more efficient both in terms of computation time and memory usage. Our method relies on a single batch of augmented data, while the augmentations in RegMixup and MIT double the size of the batch, leading to more compute and more required memory, as discussed in section 4.1 in the paper. \n  - 2. **Manifold Mixup on classification (Table 1):** We reproduced results of Manifold Mixup (Verma et al., ICML 2019) on CIFAR 10 and CIFAR100 using a Resnet50. We achieve similar performance improvements, but with better calibration. As for RegMixup, we can consider combining our method with Manifold Mixup in future work.\n  - 3. **Data augmentation aims to cover uncertain data points (Table 2):** The assumption underlying this work is that there is a trade-off in data augmentation procedures between adding *diversity* and introducing *uncertainty*. While we agree that it is important to cover more regions of the space, it is important to do it without adding errors and avoid *manifold intrusion* (Guo et al., 2019; Baena et al., 2022), *i.e*, conflicts between the synthetic labels of the mixed-up examples and the labels of original training data. Our intuition is that mixing data that are too far introduces uncertainty on the true underlying label of the new point, for which a linear combination of the labels might not be an accurate target. We confirm this hypothesis by the following experiment that we report in Table 2, in which we compare *accuracy* and *calibration* metrics when mixing *only* pairs of points with distances lower or higher than a given quantile of the overall pairwise distances within the batch, using a Resnet34 on CIFAR10 and CIFAR100 datasets. Note that one should compare results of \"Lower $q$\" with \"Higher $1 - q$\" to have equivalent numbers of possible element to mix with (*diversity*). We can see that, in general, *mixing pairs with lower distances leads to better calibration than mixing pairs with higher distances* (*uncertainty*). The introduction of uncertainty when mixing pairs with high distance can be explained by *manifold intrusion* (Guo et al., 2019; Baena et al., 2022). These results confirm that there is a trade-off between adding *diversity* and *uncertainty with* data augmentation.\n\n- W2. This is an interesting remark for which several points need to be discussed. \n  - 1. **Statistics can be computed before training (Table 3):** First, if one can use a meaningful distance between data points different from the distance in the embedding space of the model to train, statistics such as mean or median of the pairwise similarity matrix can be computed beforehand to normalize the distance during training. In our experiments on regression tasks, we also compared with normalizing using the mean over the whole training set instead of the batch. We found slightly improved results on Airfoil, for which the batch size is 16, but no meaningful improvements for the two other datasets, for which the batch size is 128. These results are provided in Table 3.\n  - 2. **Median can be used to be more robust to outliers:** Second, our rationale behind the normalization by the mean is mainly to rescale the distances to similar scales between tasks and models, and same for $\\tau_{max}$ and $\\tau_{std}$. If the batch size is too small, one could use the median instead of the mean to be more robust to outliers.\n  - 3. **Distance impact distribution of $\\lambda$:** Third, regarding the remark \"the same samples will be mixup with different $\\lambda$ whether an outlier appears\", we would like to point that since we are sampling different $\\lambda$ at each iteration, it is very likely that the same pair of samples will be mixed with different $\\lambda$ at each iteration, whatever the mixup approach we are using. In our case, the *underlying distribution* of the $\\lambda$ might vary for the same pair of samples between iterations depending on the distances in each batch, but we do not think that it hinders the robustness of the approach. If the distance between two samples is high, it is *more likely* that the $\\alpha$ parameter will be small and thus that $\\lambda$ will be close to 0 and 1, even if the batch size is small.\n\n- W3. **See W1.3** We provide in Table 2 an analysis to back up our intuition that mixing data that are too far introduces uncertainty on the true underlying label of the new point, and that a linear combination of the labels might not be an accurate target."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559297612,
                "cdate": 1700559297612,
                "tmdate": 1700559297612,
                "mdate": 1700559297612,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IuemUIPjjt",
                "forum": "QhsZwzBYaU",
                "replyto": "j7FW5D5utr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3312/Reviewer_djAy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3312/Reviewer_djAy"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have carefully answered my questions, supplementing their explanations with additional experiments. Given my limited expertise in Calibration, it is hard for me to access the contribution in the context thoroughly. Consequently, I would like to raise my score to 6; however, my confidence has decreased to 3."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707159179,
                "cdate": 1700707159179,
                "tmdate": 1700707159179,
                "mdate": 1700707159179,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6znXukklDo",
            "forum": "QhsZwzBYaU",
            "replyto": "QhsZwzBYaU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3312/Reviewer_2Wwo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3312/Reviewer_2Wwo"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to improve the mixup algorithm by improving its data interpolation policy. Particularly, the paper proposes Kernel Warping Mixup that can dynamically change the sampling distribution of interpolation coefficient $\\lambda$, so that when mixing data ponts that are \"closer\" under a certain metrics, the choice of $\\lambda$ can have a higher degree of freedom; when mixing data points that are \"farther\", $\\lambda$ should be chosen closer to 0 or 1. Experiments on both classification tasks and regression tasks are conducted, and the results show that the proposed Kernel Warping Mixup improves both the test accuracy and the calibration compared with conventional Mixup."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Proposed a variant mixup algorithm that has shown improvement on both generalization and calibration compared to conventional mixup.\n\n2. Adequate experiments on common datasets, both of classifrication tasks and regression tasks.\n\n3. The thought process of designing the proposed algorithm is explained clearly."
                },
                "weaknesses": {
                    "value": "1. The idea of dynamically controlling the sampling of $\\lambda$ in mixup is not novel, and the idea of controlling the pairing of the data examples based on distributional similarities (like k-mixup) is also well-investigated. As a result, combining these two types of ideas to formulate a new algorithm, like the one proposed in this paper, is also intuitively and empirically straightforward, and doesn't seem to be much surprising or interesting. In my opinion it the amount of contributions in this work is not sufficient to be a full paper in ICLR.\n\n2. As a experiment-based work, the datasets used in the experiments are not adequate. For example, in classifications the authors only investigated their proposed algorithm on image datasets. In fact, some datasets, especially the ones with lower dimensionalities, tend to benefit less or even negatively from mixup, and also L2 distance between data points in these datasets may be more statistically meaningful. Such datasets are also worth of experiment verifications of the Kernel Warping Mixup. This also applies to the regression tasks.\n\n3. The essential hyperparameters $\\tau_{max}$ and $\\tau_{std}$ used are chosen through cross-validation to find the optimal values. This may cost extra time before the real training is even started."
                },
                "questions": {
                    "value": "1. In the last paragraph of Section 1, \"... improve both performance and ...\", what performance exactly? Is it refering to generalization performance?\n\n2. Section 3.3. Why $\\tau$ should be exponentially correlated with the distance?\n\n3. Is there any principle or strategy to select the metrics of similarity? Like L2 or optimal transports or else?\n\n4. How is the cross-validation used to find the hyperparameters conducted in details? Is it conducted before the real training? Or is it conducted simultaneously during the training? What objective is considered in finding the \"optimal\" $\\tau$'s?\n\n5. Algorithm 1. It seems that the values of $\\tau$'s for the inputs and the targets are computed in the identical way. Then what is the point of defining them separately? And also, if the $\\tau^o$ is computed separately, how would one define the similarity between two one-hot labels in classifications? Is it going to be like a simple equity indicator function?\n\n6. Section 4.2. Why is the input distance or embedding distance not taken into consideration here? For some regression problem, I believe the similarity between two targets doesn't necessarily indicate a comparable similarity between their inputs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773998307,
            "cdate": 1698773998307,
            "tmdate": 1699636280574,
            "mdate": 1699636280574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aCNgTNsknl",
                "forum": "QhsZwzBYaU",
                "replyto": "6znXukklDo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Wwo"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking time to review our work and for their constructive feedback.\n\n- (W1) May we ask the reviewer for references linked to the claim on the novelty of our work ? As far as we know, at the time of submission, we did not find related work that also proposed to dynamically change the underlying distribution of $\\lambda$ during training. Furthermore, we are not controlling the *pairing of points*, but how strongly the data are mixed by changing the sampling distribution of the $\\lambda$, using the **distance between the two points** to mix, and **not distribution similarities unlike k-mixup** (Greenewald et al., 2021).\n- (W2) Our goal is to study the impact of mixup on calibration in general for both classification and regression tasks. We considered datasets for classification that are widely used in the literature to accurately compare with other baselines, and they are mainly image datasets. However, it is not the case for the *regression tasks*, where the datasets used are *tabular* and *times series* data. Specifically, on *Airfoil* (tabular data), we can see that **both mixup and manifold mixup have negative effects** on performance and calibration, but not with neither C-Mixup nor our approach. In this case, we are using L2 distance between labels since it works best in this case as shown in Yao et al. (2022).\n- (W3) While we agree that finding the correct values for $\\tau_{max}$ and $\\tau_{std}$ through cross validation beforehand takes additional time, we would like to point that **it is the case for any hyperparameter** and that **other mixup methods also have additional hyperparameters**, such as $\\alpha$ in the Beta distributions or the strength of the regularization in RegMixup, or the layers selected to apply mixup in Manifold Mixup. In our case, **we do not have to search for a good potential $\\alpha$** since it is already reflected in both $\\tau_{max}$ and $\\tau_{std}$, which makes one less hyperparameter.\n- (Q1) When stating that our method \"improves both performance and calibration\", we are referring to generalization performance in-distribution, i.e. accuracy on the test set, *and* that we improve calibration at the same time. \n- (Q2) **The exponential correlation is linked to the warping function that we consider**, i.e. in our case, the Beta CDF function. As can be seen in figure 2 of the paper, there is a *logarithmic* correlation on the shape of the Beta CDF and the parameter $\\tau$. Based on this, we defined the similarity kernel with an *exponential correlation* with the distance. As mentioned in the paper, the correlation with the distance will depend on the warping function considered in the framework. \n- (Q3) We present in the paper several possible distance metrics that we tried, L2 distance between inputs, between embeddings of input, between classification weights of the associated true class or between labels. We also tried Cosine distance instead of L2 distance, but did not observe meaningful difference between the two. However, we agree that one could consider optimal transport distances as well *if each data can be seen as a distribution* (since we are computing similarity for a pair of data), which could be possible for images using distributions of pixel values.\n- (Q4) The cross-validation is conducted before training, with a stratified sampling on a 90/10 split of the training set, and the results are averaged across 4 different splits. The hyperparameters are selected to have **a good trade-off between calibration and accuracy**. As mentioned in Pinto et al. (2022), *cross-validating hyperparameters based solely on the ECE can prefer models with lower accuracy but better calibration*. However, a method improving calibration should avoid degrading accuracy. We will clarify the process of the cross validation in the paper.\n- (Q5) We thank the reviewer for pointing out this typo in the algorithm. We compute $\\tau_i^{(i)}$ with $\\tau^{(i)}(\\mathbf{x}, i, \\sigma; \\tau^{(i)}\\_{max}, \\tau^{(i)}\\_{std})$, and $\\tau_i^{(o)}$ with $\\tau^{(o)}(\\mathbf{x}, i, \\sigma; \\tau^{(o)}\\_{max}, \\tau^{(o)}\\_{std})$. If one wants to use different $\\tau^{(i)}$ and $\\tau^{(o)}$, one has to use different pairs of $(\\tau^{(i)}\\_{max}, \\tau^{(i)}\\_{std})$ and $(\\tau^{(o)}\\_{max}, \\tau^{(o)}\\_{std})$. \n- (Q6) While we agree that we could have considered input or embedding distance for regression tasks, it was shown in Yao et al. (2022), that label distance works best in this case. We thank the reviewer for this remark and will make it clear in the revised version.\n\n### References\n\n- Pinto et al., RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy and Out Distribution Robustness. NeurIPS, 2022\n- Yao et al., C-Mixup: Improving generalization in regression. NeurIPS, 2022.\n- Greenewald et al., k-Mixup Regularization for Deep Learning via Optimal Transport. arXiv:2106.02933, 2021"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559110860,
                "cdate": 1700559110860,
                "tmdate": 1700559110860,
                "mdate": 1700559110860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PQmZz6I5i0",
                "forum": "QhsZwzBYaU",
                "replyto": "aCNgTNsknl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3312/Reviewer_2Wwo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3312/Reviewer_2Wwo"
                ],
                "content": {
                    "title": {
                        "value": "Confirmation of the response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. For (W1), this paper[1] can be a reference in which the authors investigate a new algorithm to adaptively learn the mixing policy from the training data.\n\nAlso for (Q5), it's still unclear to me how the similarity between two one-hot labels can be defined or computed.\n\n[1]Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization. arXiv:1809.02499v3"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670280994,
                "cdate": 1700670280994,
                "tmdate": 1700670280994,
                "mdate": 1700670280994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "deX4DBQFuq",
                "forum": "QhsZwzBYaU",
                "replyto": "eHZWqPMNvb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3312/Reviewer_2Wwo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3312/Reviewer_2Wwo"
                ],
                "content": {
                    "comment": {
                        "value": "Page 7: \"In Table 1, we compared results for each of these choice and for different combinations of similarity\nbetween inputs and targets.\" Here you have mentioned similarity between inputs and targets, and you have also indicated \"output similarity\" in the following sentence.\", now it turns out you don't intend to compute the distance or similarity between the actual \"labels\" or \"model outputs\", I assume there is an abuse of terminology here? Or I would suggest that you clarify the concept of \"target similarity\" or \"output similarity\" in this context. For example, in Table 1, under the column of \"output similarity\" there is a \"input (distance)\", this will cause some misunderstandings"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685411274,
                "cdate": 1700685411274,
                "tmdate": 1700685411274,
                "mdate": 1700685411274,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xpwKpqrphn",
            "forum": "QhsZwzBYaU",
            "replyto": "QhsZwzBYaU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3312/Reviewer_YFeL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3312/Reviewer_YFeL"
            ],
            "content": {
                "summary": {
                    "value": "This paper generally contributes a new way to data augmentation by change distributions of training data.\n\nHere is a general summary:\n1. Authors define warping functions to change the underlying distributions used for sampling\ninterpolation coefficients. This defines a general framework that allows to disentangle inputs\nand labels when mixing, and spans several variants of mixup.\n2. Authors proposed to then apply a similarity kernel that takes into account the distance between\npoints to select a parameter for the warping function tailored to each pair of points to mix,\ngoverning its shape and strength. This tailored function warps the interpolation coefficients\nto make them stronger for similar points and weaker otherwise.\n3. Authors show that our Kernel Warping Mixup is general enough to be applied in classification as\nwell as regression tasks.\n\nThis major contribution of this paper is mixing the idea of regularization and kernel function and try to \nsolve the problem from data perspective. If experiments result is convincible, this is a new way to consider \ndata augmentation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The ideology of this paper is quite plausible and is theoretically robust."
                },
                "weaknesses": {
                    "value": "I don't think regression task can be convincing as downstream task for novelty. Regression task, in somehow, can already achieve very good result. I think a challenging downstream task such as object detection can make this paper more attractive."
                },
                "questions": {
                    "value": "How do you define/calculate distance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698890644089,
            "cdate": 1698890644089,
            "tmdate": 1699636280501,
            "mdate": 1699636280501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3WRgtcjqjA",
                "forum": "QhsZwzBYaU",
                "replyto": "xpwKpqrphn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YFeL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their kind comments on our work.\n\n- (W1) The main goal of the paper is to study and improve the impact of mixup on calibration in general. We present experiments for both classification and regression tasks, since it is **two very different settings**, and not that well studied in regression. Nevertheless, we show the *flexibility* of our framework, which can improve calibration in both tasks. However, as mentioned in the conclusion of our paper, we are interested to apply our method on more difficult regression tasks such as Monocular Depth Estimation, but leave that to future work. \n- (Q1) As presented in Section 3.3, we use an L2 distance, on either input data, embedding of inputs, classification weights or labels depending on the task."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558035265,
                "cdate": 1700558035265,
                "tmdate": 1700558035265,
                "mdate": 1700558035265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2m2ZUFMYmY",
            "forum": "QhsZwzBYaU",
            "replyto": "QhsZwzBYaU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3312/Reviewer_iaPi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3312/Reviewer_iaPi"
            ],
            "content": {
                "summary": {
                    "value": "Mixup data augmentations are a widely used technique for deep learning, and most methods are focused on selecting the right points to mix or designing favorite mixing strategies. This paper tries to improve mixup by mixing similar data points more frequently than less similar ones and proposes a dynamically changing underlying distribution of mixing interpolation coefficients through warping functions, dubbed Kernel Warping Mixup. Extensive experiments for classification and regression tasks demonstrate the effectiveness and generalization abilities of the proposed mixup."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* (S1) It is an interesting and novel perspective of improving mixup augmentations by mixing more similar samples than less similar ones (but it also needs verification and empirical analysis to ensure the importance, as mentioned in W1). Experiment results show the effectiveness of the proposed method in comparison to classical mixup variants on both classification and regression tasks.\n\n* (S2) Various analyzing metrics are used to verify the effectiveness of mixup classification and regression tasks, e.g., ECE, UCE, and NLL, which are not well studied in previous works.\n\n* (S3) The overall writing is fruentcent and easy to follow. The implementation details and source code are available."
                },
                "weaknesses": {
                    "value": "* (W1) Is the studied problem really important to mixup augmentations, i.e., it is practically useful to conduct mixup interpolation with similar samples? I cannot find any empirical analysis demonstrating that previous mixup methods will encounter serious drawbacks (e.g., performances, calibration abilities, generalization abilities to tasks) because of not mixing similar samples well. For example, the authors should provide some visualizations of effects or statistics to demonstrate the problem in addition to Figure 1.\n\n* (W2) Weak experiments. In comparison to recently published works on mixup augmentation, this paper lacks comprehensive and solid comparison experiments to verify the effectiveness from three aspects. (a) The compared baselines are restricted to classical methods in classification tasks, and the performance gains are limited in all comparison results. These results make me doubt the importance of the studied problem in this paper, i.e., mixing more similar mixup samples than less similar ones. (b) The experiments are small-scale (e.g., CIFAR-10/100) with classical network architectures (ResNet variants) and old-fashioned baselines. There are many open-source mixup methods and benchmarks for classification and regression tasks [1, 2], and I suggest the authors consider more practical and modern experiment settings (e.g., large-scale experiments on ImageNet, modern Transformer backbones on CIFAR-100). (c) Since the proposed method is orthogonal to these mixup methods that improve mixup policies of samples (e.g., CutMix variants [3, 4], or randomly combining Mixup and CutMix [5]) or labels (e.g., TransMix [6], Decouple Mixup [7], and MixupE [8]), the authors should verify whether the proposed method can improve these existing mixup algorithms, rather than only test upon the vanilla Input Mixup.\n\n* (W3) Hyper-parameter sensitivity. As shown in Appendix D and E (Table 6), the hyper-parameters of the proposed method vary significantly on different datasets and tasks. The ablation and sensitivity analysis of these hyper-parameters should be added. Meanwhile, I wonder how the authors determine the two hyper-parameters, which should be detailed in the appendix.\n\n* (W4) The related work section is not well presented. I suggest the author combine Sec. 2.2 and Sec. 3.1 as a new section called Preliminary. Meanwhile, the authors may include more recently published mixup algorithms in different categories.\n\n### Reference\n[1] OpenMixup: A Comprehensive Mixup Benchmark for Visual Classification. arXiv, 2022.\n\n[2] C-Mixup: Improving Generalization in Regression. NeurIPS, 2022.\n\n[3] CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features. ICCV, 2019.\n\n[4] PuzzleMix: Exploiting Saliency and Local Statistics for Optimal Mixup. ICML, 2020.\n\n[5] Training data-efficient image transformers & distillation through attention. ICML, 2021.\n\n[6] TransMix: Attend to Mix for Vision Transformers. CVPR, 2022.\n\n[7] Harnessing Hard Mixed Samples with Decoupled Regularizer. NeurIPS, 2023.\n\n[8] MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. UAI, 2023."
                },
                "questions": {
                    "value": "Please refer to the weaknesses I mentioned."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699221631145,
            "cdate": 1699221631145,
            "tmdate": 1699636280381,
            "mdate": 1699636280381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SrtbH6WEWD",
                "forum": "QhsZwzBYaU",
                "replyto": "2m2ZUFMYmY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iaPi"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review and extensive feedback on our work.\n\n- (W1) The assumption underlying this work is that there is a trade-off in data augmentation procedures between adding *diversity* and introducing *uncertainty* through **manifold intrusion** (Guo et al., 2019; Baena et al., 2022), *i.e*, conflicts between the synthetic labels of the mixed-up examples and the labels of original training data. To show that, we conduct in Table 1 an empirical analysis that compares *accuracy* and *calibration* metrics when mixing *only* pairs of points with distances lower or higher than a given quantile of the overall pairwise distances within the batch, using a Resnet34 on CIFAR10 and CIFAR100 datasets. Note that one should compare results of \"Lower $q$\" with \"Higher $1 - q$\" to have equivalent numbers of possible element to mix with (*diversity*). We can see that, in general, **mixing pairs with lower distances leads to better calibration than mixing pairs with higher distances** (*uncertainty*). The introduction of uncertainty when mixing pairs with high distance can be explained by *manifold intrusion* (Guo et al., 2019; Baena et al., 2022). These results confirm that there is a trade-off between adding *diversity* and *uncertainty with* data augmentation.\n- (W2) Along with the empirical analysis detailed above, we also provide experiments on the Tiny-Imagenet benchmark in Table 2. We can see that our approach improves both performance and calibration over Mixup baselines on this dataset as well, and reach similar calibration than RegMixup. We are working on adding the other baselines for this dataset as well. As mentioned in the conclusion of the paper, we are working on combining our approach with other mixup methods. Regarding the references mentioned by the reviewer:\n    - The OpenMixup paper [1] and library mentioned is still under construction, but **we plan to contribute to this library** once our paper is accepted.\n    - **We are already comparing against C-Mixup [2]** on the same datasets in the experiments on regression tasks.\n    - CutMix [3,5], PuzzleMix [4] and TransMix [6] are **methods focused on image data**. As our main goal is to study and improve the impact of mixup on *calibration in general*, we do not want to restrict our evaluation to image datasets and image methods. We consider regression tasks on tabular and time series data, to evaluate calibration of Mixup methods in this setting, which have rarely been investigated in the literature, but all of the above methods are not adapted to these types of data.\n    - As both MixupE [8] and Decouple Mixup [7] have been published **very recently**, implementing these methods will require further work.\n- (W3) *As mentioned in Section 4.1*, we search for the best hyperparameters through **cross-validation**. The cross-validation is conducted before training, with a stratified sampling on a 90/10 split of the training set, and the results are averaged across 4 different splits. The optimal values will vary between datasets, since they depend on the statistics of the pairwise distances. We assume that the behavior of these parameters is also impacted by the number of class and their separability in each dataset. We provide the resulting heatmaps of the cross validation in Figure 5 and Figure 6 for a better visualization, but we will also include the exact values obtained for all the cross validations in the Appendix. \n- (W4) We will make sure to add the latest relevant papers that we might have missed since the moment of submission. Regarding the two subsections mentioned by the reviewer, we will try to combine them, but it might be difficult since they cover different topics. While Sec. 2.2 presents related work on calibration, Sec. 3.1 introduces notations used throughout the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557764835,
                "cdate": 1700557764835,
                "tmdate": 1700557764835,
                "mdate": 1700557764835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CmeDTR4qaR",
                "forum": "QhsZwzBYaU",
                "replyto": "2m2ZUFMYmY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3312/Reviewer_iaPi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3312/Reviewer_iaPi"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Rebuttal Feedback"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response to my concerns. Sorry for the late reply at the end of the rebuttal period! My concerns about W3 and W4 have been addressed (it is better for the authors to update them in the revision). However, I am not convinced by the replies to the first two issues, and I decided to maintain my rating at the current stage.\n\n* (W1) Despite the authors explaining the motivation and the main problem in this paper, I think some empirical studies are required to demonstrate the studied problem really existed in previous works. This is directly related to the effectiveness of the proposed method. Since the performance gains of the proposed method are not significant or even worse than existing methods,  I really need the relevant evidence to support the problems and statements in the authors' response.\n\n* (W2) I have acknowledged the purpose of the proposed methods and the reasons for not conducting the comparison experiments I recommended. This leaves me wondering what is the practical value of the proposed approach if it mainly addresses small-scale image data, time series data, and tabular data. From my perspective, it makes this article's contribution seem inadequate to be accepted by the ICLR community.\n\nExcept for the unsolved issues, I appreciate the improved perspective of this article, the comprehensive evaluation metrics, and the overall presentations."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732148835,
                "cdate": 1700732148835,
                "tmdate": 1700732213195,
                "mdate": 1700732213195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lcGwNOGN2O",
                "forum": "QhsZwzBYaU",
                "replyto": "2m2ZUFMYmY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3312/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to reply.\n\n(W1)  \n   1. **Empirical evidence of the impact of distance on calibration:** We have conducted the empirical studies mentioned by the reviewer, in Table 1 of our first answer, to precisely study the impact on calibration of the distance between the points to mix. These experiments bring empirical evidence that, indeed, **mixing pairs with lower distances leads to better calibration than mixing pairs with higher distances**.\n   2. **On the importance of the contribution:** The problem of the trade-off between performance and calibration with Mixup have been **extensively studied in previous work** (Thulasidasan et al. 2019; Zhang et al., 2022; Wang et al., 2023), as mentioned in Section 2.2. The motivation of our paper is to propose **a more efficient approach to achieve a better trade-off in terms of performance and calibration** by controlling the impact of distance on mixing.  \n\n(W2) We would like to point that what the reviewer calls \"small-scale image data, time series data and tabular data\", encompasses a lot of possible applications and represents a wider scope than previous Mixup papers limited to image data. During the rebuttal, we have also included experiments on Tiny-ImageNet, which is on a bigger scale than CIFAR. Nothing prevents applying our approach to larger scale, this is something that we are working on, but needs more time.\n\nThe paper is now updated with the changes.\n\nReferences:\n- Thulasidasan et al., On mixup training: Improved calibration and predictive uncertainty for deep neural networks. NeurIPS 2019\n- Zhang et al., When and how mixup improves calibration. ICML 2022\n- Wang et al., On the pitfall of mixup for uncertainty calibration. CVPR 2023"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737025048,
                "cdate": 1700737025048,
                "tmdate": 1700738815544,
                "mdate": 1700738815544,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]