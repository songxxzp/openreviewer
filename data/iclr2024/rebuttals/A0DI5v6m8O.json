[
    {
        "title": "Black-Box Gradient Matching for Reliable Offline Black-Box Optimization"
    },
    {
        "review": {
            "id": "wUTFcfBqaY",
            "forum": "A0DI5v6m8O",
            "replyto": "A0DI5v6m8O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7067/Reviewer_eLsu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7067/Reviewer_eLsu"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenge of offline design optimization in various scientific and engineering contexts, where physical or computational evaluations are costly, making real-time optimization impractical. The traditional solution has been to utilize surrogate models based on offline data to predict the objective function for unknown inputs. However, these surrogate models often show discrepancies from the true objective function, particularly outside the range of the offline data. This study introduces a novel theoretical framework to understand this discrepancy by looking at how well the surrogate models match the latent gradient of the true function. Based on this theoretical insight, the authors propose a new algorithm, MATCH-OPT, that aims to match the gradients of the oracle more closely. The efficacy of this approach is supported by experiments on real-world benchmarks, showing that MATCH-OPT outperforms existing methods in offline optimization tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The concept of employing \"fundamental line integration\" as a technique for gradient estimation presents an intriguing approach.\n \n2. The overall ranking performance of the method is commendable, underscoring its efficacy."
                },
                "weaknesses": {
                    "value": "1. The paper frequently refers to \"the gradient fields of the Oracle (i.e., the true objective function).\" It's essential to note that a true objective function doesn't inherently possess a gradient. Continual mention of the oracle's gradient is foundational to this paper, and this assumption should be explicitly clarified by the author. I think this is the key drawback. If the author can clarify this point, I will increase my score.\n\n2. The paper seems to overlook crucial baselines. It would be beneficial to reference and juxtapose the presented work against established benchmarks such as NEMO (https://arxiv.org/abs/2102.07970), CMA-ES, BO-qEI, BDI (https://arxiv.org/abs/2209.07507), and IOM (https://openreview.net/forum?id=gKe_A-DxzkH). These baselines are pivotal in this domain and warrant inclusion for a comprehensive analysis.\n\n3. For better structuring, consider relocating the in-depth experimental results pertaining to the second question from the introduction to the exp section. This would make the introduction more concise and allow readers to delve into the specifics at the appropriate juncture."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7067/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7067/Reviewer_eLsu",
                        "ICLR.cc/2024/Conference/Submission7067/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7067/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697854724829,
            "cdate": 1697854724829,
            "tmdate": 1700604030345,
            "mdate": 1700604030345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l4QA35eIlJ",
                "forum": "A0DI5v6m8O",
                "replyto": "wUTFcfBqaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review. We address all your concerns below."
                    },
                    "comment": {
                        "value": "**1. Assumption that the oracle has gradient.** \n\nWe agree with the reviewer that not all oracle functions possess a gradient. \n\n**However, we want to point out that the implicit premise of the entire offline optimization field is that the developed techniques are meant for oracle functions which are continuous and differentiable. This premise is set the moment we set out to optimize the oracle using a differentiable surrogate such as the neural network.** \n\nThis is the common theme in most (if not all) prior work. \n\n**This is also not an unreasonable assumption because attempting to optimize an oracle function with discontinuities is otherwise an ill-posed problem. Indeed, without the assumed differentiability, the oracle can be discontinued at any unseen data points at which its output behavior can be arbitrary, giving us no solid ground to relate between the training and (unseen) test output.** \n\nWith this, we hope the reviewer would agree with us that we had in fact not used any assumptions prior work had not used, and consider re-evaluating the rating of our work.\n\n**2. Discussing and providing empirical comparisons with suggested baselines.**\n\nWe would like to point out that we did compare with CMA-ES. We will include in our revision the following positioning with BO-qEI, BDI, NEMO, and IOM.\n\n**(1)** BDI uses forward and backward mappings to distill knowledge from the offline dataset to the design. BDI employs infinite width (neural tangent kernels) neural nets as surrogate models which might be limited in representation power for some applications. \n\n**(2)** IOM uses domain adaptation methods to enforce representation invariance between the training dataset and the distribution of optimized designs. Figuring out the right notion of distribution over optimized designs is an open challenge and the one used by IOM (points found during gradient ascent with a given surrogate) might not be the most effective one.\n\n**(3)** BO-qEI is standard Bayesian optimization (BO) run with a surrogate model as the objective function. \n\n**(4)** NEMO is an uncertainty quantification approach that uses normalized maximum-likelihood\n(NML) estimator which requires quantizing the output scores y. This quantization might be sub-optimal.\n\nFurthermore, we have also compared our method with BO-qEI & BDI and reported the results below.\n\n**100-th percentile**\n\n|           | Ant   | Dkitty | Hopper | Superconductor | tfbind-8 | tfbind-10 |\n|-----------|-------|--------|--------|----------------|----------|-----------|\n| BO-qEI    | 0.812 | 0.896  | 0.528  | 0.576          | 0.607    | 0.864     |\n| BDI       | **0.967** | 0.940  | **1.706**  | **0.735**          | 0.973    | OOM       |\n| MATCH-OPT | 0.931 | **0.957**  | 1.572  | 0.732          | **0.977**    | **0.924**     |\n\n**50-th percentile**\n\n|           | Ant   | Dkitty | Hopper | Superconductor | tfbind-8 | tfbind-10 |\n|-----------|-------|--------|--------|----------------|----------|-----------|\n| BO-qEI    | 0.568 | 0.883  | 0.360  | **0.490**          | 0.439    | 0.557     |\n| BDI       | 0.583 | 0.870  | **0.400**  | 0.480          | **0.595**    | OOM       |\n| MATCH-OPT | **0.611** | **0.887**  | 0.393  | 0.439          | 0.594    | **0.720**     |\n\nNote: OOM means out of memory.\n\nThe results show that MATCH-OPT performs the best in 6 out of 12 cases (across both the 100-th and 50-th percentile settings) while BO-qEI only performs best in 1 out of 12 cases. BDI performs best in 5 out of 12 cases, runs out of memory in 2 out of 12 cases. Overall, MATCH-OPT appears to perform more stable than BDI and is marginally better than BDI. It is also more memory-efficient than BDI as it does run successfully in all cases, while BDI runs out of memory in 2 cases. MATCH-OPT also outperforms BO-qEI significantly in 11 out of 12 cases. These new results will be incorporated into our main text.\n\nIn addition, we would also be happy to run comparative analysis with NEMO and IOM but the code for those baselines has not been released. We hope the reviewer would sympathize with us on this point since offline optimization is a fast-growing field and providing an exhaustive comparison with all prior work, including those with no release code is too difficult. Nonetheless, we have tried our best to compare with most prior work that provides released code.\n\nWe hope the reviewer would take into account such practical difficulties and re-consider the rating of our work.\nWe are looking forward to hearing back from you & will be happy to continue the discussion if you still have other questions for us.\n\n**3. Restructuring the experiment to make the introduction more concise.** We agree with the reviewer and will adjust the content accordingly in our revision.\n\nThank you again for the detailed review & suggestions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700083644207,
                "cdate": 1700083644207,
                "tmdate": 1700083742580,
                "mdate": 1700083742580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s0WArZi2VP",
                "forum": "A0DI5v6m8O",
                "replyto": "l4QA35eIlJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Reviewer_eLsu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Reviewer_eLsu"
                ],
                "content": {
                    "title": {
                        "value": "\u201cAssumption that the oracle has gradient\u201d"
                    },
                    "comment": {
                        "value": "Could the authors list the works that explicitly assume task oracles are continuous & differentiable? According to the reviewer's understanding in this paper \"Design-Bench: Benchmarks for Data-Driven Offline Model-Based Optimization\", the oracles are generally black-box function and some tasks are even discrete."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700094991408,
                "cdate": 1700094991408,
                "tmdate": 1700094991408,
                "mdate": 1700094991408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CdMezVUNzd",
                "forum": "A0DI5v6m8O",
                "replyto": "t0pkdDqNBH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Reviewer_eLsu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Reviewer_eLsu"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the author feedback"
                    },
                    "comment": {
                        "value": "Thanks for the authors' feedback.\n\nI think there is a key difference between previous work and the authors regarding the assumption.\n\nCOMs does not use the \"oracle differentiable assumption\" in its method design and only uses it for analysis. This also holds for BONET.\n\nAs for ROMA, it only uses a differentiable NN to approximate the oracle, not saying the oracle is differentiable. \n\n\"as long as a method uses a differentiable surrogate to model the objective function, it already implicitly assumes that the true objective function has gradient. \" I do not think this is true. Even if the objective function does not have gradient (for example, has some discontinuities), we can still use NN to approximate it. \n\nIn fact, there are many derivation-free baselines such as evolutionary algos and they are proposed based on the \"derivation-free\" assumption. Further, some tasks are not differentiable in this context. Take the TFBind8 as an example, and it uses logits as x and the y is the binding affinity.  Adding a small perturbation to x might change its original DNA sequence and leads to a sudden change of y. Regarding this point, the oracle is not differentiable.\n\nThe reviewer will further discuss with other reviewers regarding this point."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599672350,
                "cdate": 1700599672350,
                "tmdate": 1700599672350,
                "mdate": 1700599672350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eGZoV7H5kw",
                "forum": "A0DI5v6m8O",
                "replyto": "wUTFcfBqaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the quick follow-up"
                    },
                    "comment": {
                        "value": "Thank you for the quick follow-up. From your response, you said that\n\n**COMs does not use the \"oracle differentiable assumption\" in its method design and only uses it for analysis. This also holds for BONET**\n\nThis is true for us as well. If you take a look at Eq. (11), it does not involve the oracle's gradient anywhere. It only involves the gradient of the surrogate and the output difference between two sampled data points.\n\nFurthermore, you have also agreed that:\n\n**Even if the objective function does not have gradient (for example, has some discontinuities), we can still use NN to approximate it**\n\nThis means it is fine to use the NN's gradient in place of the objective function's gradient (even if it does not exist). This is no different from our approach. So, once again, we do not use any assumptions that prior methods did not use.\n\n**We hope the reviewer could reconsider the rating given the above point**"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600598298,
                "cdate": 1700600598298,
                "tmdate": 1700601104471,
                "mdate": 1700601104471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6MmU7xSoNF",
                "forum": "A0DI5v6m8O",
                "replyto": "eGZoV7H5kw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Reviewer_eLsu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Reviewer_eLsu"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback"
                    },
                    "comment": {
                        "value": "Although Eq (11) does not use the oracle gradient, Eq (10) uses it for approximation.\n\nIf I understand it correctly, COMs and BONET only use the assumption for further analysis, not using the assumption to derive the main method.  I did agree that \"Even if the objective function does not have gradient (for example, has some discontinuities), we can still use NN to approximate it\". In this case, you can not use the term \"oracle gradient\". You should say we have an ideal differentiable proxy which is the closest one to the ground-truth. This ideal proxy is what you call \"oracle\".\n\nSome task oracle in this context like TFB8 are not differentiable even if you use logits. \n\nI increase my score from 3 to 5. I think the paper still needs a lot to refine for publish. The authors should not only consider incorporating the above additional experiments but also should clarify the \"oracle gradient\" in the following version."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603995403,
                "cdate": 1700603995403,
                "tmdate": 1700603995403,
                "mdate": 1700603995403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QUAd0nSrm4",
                "forum": "A0DI5v6m8O",
                "replyto": "wUTFcfBqaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the prompt response"
                    },
                    "comment": {
                        "value": "Thank you for increasing the score.\n\nWe understand that the reviewer\u2019s suggestion is although (implementation-wise) we can use the NN\u2019s gradient in place of the objective function\u2019s gradient, it is better to write in the paper that \n\n**We have an ideal differentiable proxy which is the closest one to the ground-truth and this ideal proxy is what is called the oracle.** \n\nWe have put this sentence in a new revision of our main text (see Section 2). This will solve the problem. The additional experiments in our original response will also be included. With this, we hope the reviewer would still reconsider the assessment that this paper needs a lot of refinement to publish. \n\n--\n\nIn addition, we respectfully do **not** agree with the reviewer that COMS does not use the gradient assumption in its method design. It is written at the beginning of its Section 3 that \n\n**COMs learn estimates of the true function that do not overestimate the value of the ground truth objective on out-of-distribution inputs in the vicinity of the training dataset. As a result, COMs prevent erroneous overestimation that would drive the optimizer (Equation 2) to produce out-of-distribution inputs with low values under the ground-truth objective function**\n\nThe last part is true only if its main theorem holds, which is under the assumption that the true function is L-Lipschitz, which implies the gradient assumption. In fact, **the regularizer term in the objective function -- see Eq.(4) -- is equivalently transformed to the more expressive form in Eq. (7) which is exactly what is bounded in the main theorem**. Thus, similar to our paper, the intriguing theoretical analysis of COMS informs its algorithm design in this case.\n\nWhat COMS assumed is fundamentally not different from the approximation step we used in Eq. (10). More importantly, we can remove the first equality in Eq. (10) so that there is no explicit writing of $\\nabla g(\\mathbf{x}) \\simeq \\nabla g_\\phi(\\mathbf{x})$ in the algorithm design and the rest of the technical narrative is still correct. But, we think the clarification statement above is already enough to make it clear."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692807030,
                "cdate": 1700692807030,
                "tmdate": 1700692945098,
                "mdate": 1700692945098,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "50EgseTB1W",
            "forum": "A0DI5v6m8O",
            "replyto": "A0DI5v6m8O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7067/Reviewer_1MFJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7067/Reviewer_1MFJ"
            ],
            "content": {
                "summary": {
                    "value": "In this manuscript, the authors propose an offline black-box optimization method by gradient matching.  In addition, the authors provide a bound of the difference between the function value at the solution of $m$-step true gradient update and that of $m$-step surrogate gradient update."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed offline black-box optimization method via gradient matching is novel. \n\n2.  The paper is well-written and well-organized."
                },
                "weaknesses": {
                    "value": "1.  $\\textbf{The optimum of the objective in Eq.(11) can not guarantee the learned gradient is close to the true gradient }$.\n\n  Minimizing the term $(\\Delta z - \\Delta x ^\\top \\int _0^1\\nabla g _\\phi (tx + (1-t)x') dt)^2$  in Eq.(10) and Eq.(11) can not guarantee the gradient is close.   To be clear,  we have \n\n\\begin{equation}\n(\\Delta z - \\Delta x ^\\top \\int _0^1\\nabla g _\\phi (tx + (1-t)x') dt)^2 = (\\Delta x ^\\top \\int _0^1\\nabla g  (tx + (1-t)x') dt - \\Delta x ^\\top \\int _0^1\\nabla g _\\phi (tx + (1-t)x') dt)^2  \n\\end{equation}\n\\begin{equation}\n= ( \\Delta x ^\\top  \\int _0^1 (\\nabla g  (tx + (1-t)x') -  \\nabla g _\\phi (tx + (1-t)x') )dt   )^2\n\\end{equation}\nThe optimum is  $\\Delta x ^\\top  \\int _0^1 (\\nabla g  (tx + (1-t)x') -  \\nabla g _\\phi (tx + (1-t)x') )dt = 0$   not necessary $ \\nabla g (\\cdot)  = \\nabla g _\\phi (\\cdot)  $.   Thus, when $\\Delta x$ is orthogonal to $\\int _0^1 (\\nabla g  (tx + (1-t)x') -  \\nabla g _\\phi (tx + (1-t)x') )dt$, we get a trival  solution. The difference between $  \\nabla g (\\cdot)   $ and  $  \\nabla g _\\phi (\\cdot)$ at the trivial solution can be arbitrarily large. \n\nThe true objective used in the manuscript is Eq.(13) instead of the gradient matching objective (11).  The objective (13) contains a standard regression objective term.   According to the above issue, the reviewer guesses that the regression objective term in Eq.(13) is still a key effective component. \n\n\n2. $\\textbf{The bound in Theorem 1 is very loose}$ \n\nThe bound in Theorem 1 is trivial and loose, which exponentially grows w.r.t. the number of update steps $m$.  In addition, the term  $\\max_{x}|| \\nabla g(x) - \\nabla g_\\phi (x)  ||$ in Theorem 1 can grow to infinity. \n\n3. $\\textbf{The empirical improvement is not significant}$.\n\n The empirical results are not convincing enough to demonstrate the claimed advantage of the proposed method.  In Table 1 and Table 2, the proposed method does not consistently outperform other baseline methods. \n\n4. $\\textbf{Recent related baselines are missing}$. \n\nA comparison with the recent related offline black-box optimization method [1] is missing.   The experimental setup in [1] is quite similar to this manuscript. \n\n[1] Krishnamoorthy et al. Diffusion Models for Black-Box Optimization. ICML 2023"
                },
                "questions": {
                    "value": "Q1.  Could the authors address the concerns in the above section?\n\nQ2.  Could the authors include more comparisons with the related baseline [1] ?\n\nQ3. In this paper, the authors assume the search method given the learned surrogate is gradient ascent.  Does the gradient ascent search be the unique search method? Could the authors include additional comparisons with other search methods given the learned surrogate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7067/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634454018,
            "cdate": 1698634454018,
            "tmdate": 1699636832179,
            "mdate": 1699636832179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SdkM0fOlmX",
                "forum": "A0DI5v6m8O",
                "replyto": "50EgseTB1W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review. We address all your concerns below."
                    },
                    "comment": {
                        "value": "**1. Optimizing Eq. (11) might not guarantee gradient match.** Thank you for the question. We will prove below that **optimizing Eq. (11) does necessarily guarantee gradient match** to address this concern. \n\nFirst, we understand the reviewer's key concern here is that there might exist a parameter $\\phi$ such that the integrated vector\n\n$\\mathbf{F}_{\\phi}(\\mathbf{x},\\mathbf{x}') \\ = \\ \\int_0^1 \\nabla g(t\\mathbf{x} + (1-t)\\mathbf{x}')\\mathrm{d}t$ \n\n$- \\int_0^1 \\nabla g_\\phi(t\\mathbf{x} + (1-t)\\mathbf{x}')\\mathrm{d}t$\n\nis non-zero but instead orthogonal to $\\Delta\\mathbf{x} = \\mathbf{x} - \\mathbf{x}'$. We can show that **such a parameter does not exist** in what follows.\n\nFirst, following your argument, for such a parameter phi, we have \n\n$\\mathbf{F}_{\\phi}(\\mathbf{x}, \\mathbf{x}\u2019)^\\top(\\mathbf{x} - \\mathbf{x}\u2019) = 0$ for all $(\\mathbf{x}, \\mathbf{x}\u2019)$\n\nNext, by the line integration theorem, we have:\n\n**(1)** $g(\\mathbf{x}) - g(\\mathbf{x}\u2019) = (\\mathbf{x} - \\mathbf{x}\u2019)^\\top \\int_0^1 \\nabla g(t\\mathbf{x} + (1-t)\\mathbf{x}\u2019)\\mathrm{d}t$\n\n**(2)** $g_\\phi(\\mathbf{x}) - g_\\phi(\\mathbf{x}\u2019) = (\\mathbf{x} - \\mathbf{x}\u2019)^\\top \\int_0^1 \\nabla g_\\phi(t\\mathbf{x} + (1-t)\\mathbf{x}\u2019)\\mathrm{d}t$\n\nfor all $(\\mathbf{x}, \\mathbf{x}\u2019)$. As such, combining **(1)** and **(2)** leads to\n\n$\\mathbf{F}_\\phi(\\mathbf{x}, \\mathbf{x}\u2019)^\\top(\\mathbf{x} - \\mathbf{x}\u2019) = $ \n\n$g(\\mathbf{x}) - g(\\mathbf{x}\u2019) - g_\\phi(\\mathbf{x}) + g_\\phi(\\mathbf{x}\u2019)$\n\nOr equivalently, $g(\\mathbf{x}) - g(\\mathbf{x}\u2019) - g_\\phi(\\mathbf{x}) + g_\\phi(\\mathbf{x}\u2019) =  0$ since $\\mathbf{F}_\\phi(\\mathbf{x}, \\mathbf{x}\u2019)^\\top(\\mathbf{x} - \\mathbf{x}\u2019) = 0$ for all $(\\mathbf{x}, \\mathbf{x}\u2019)$.\n\nWe can thus rewrite the above as $g(\\mathbf{x}) - g_\\phi(\\mathbf{x}) = g(\\mathbf{x}\u2019) - g_\\phi(\\mathbf{x}\u2019)$ for all $(\\mathbf{x}, \\mathbf{x}\u2019)$, which means there must exist a constant $c$ such that $g(\\mathbf{x}) - g_\\phi(\\mathbf{x}) = c$ for all $\\mathbf{x}$.\n\nTaking gradient with respect to $\\mathbf{x}$ on both sides leads to\n\n$\\nabla g(\\mathbf{x}) - \\nabla g_\\phi(\\mathbf{x}) = 0$ or equivalently, $\\nabla g(\\mathbf{x}) = \\nabla g_\\phi(\\mathbf{x})$\n\nThis means for any parameter $\\phi$ such that \n\n$\\mathbf{F}_\\phi(\\mathbf{x}, \\mathbf{x}\u2019)^\\top(\\mathbf{x} - \\mathbf{x}\u2019) = 0$ \n\nfor all $(\\mathbf{x}, \\mathbf{x}')$, the gradients of $g(\\mathbf{x})$ and $g_\\phi(\\mathbf{x})$ are guaranteed to match. \n\nThus, **the case that the reviewer is concerned about does not exist**. Please let us know if this has addressed your concern.\n\n--\n\nFurthermore, please also refer to Appendix C where we show an ablation study between the  training objective with and without the regression term. This addresses the concern of the reviewer that the performance might come solely from the regression term. This is not true according to our experiment in Appendix C: including the regression term improves the performance as expected but even without it, the performance is still very competitive, asserting the impact of the gradient match term, which matches with the implication of Theorem 1.\n\n**2. The bound in Theorem 1 seems to grow exponentially in $m$ and hence, might be loose.** Thank you for another critical question. We understand the concern here is that the bound seems to grow exponentially in the no. of iterations $m$, and might be loose as a result. To address this concern, **we want to assure the reviewer that the bound will NOT grow exponentially in the no. of iterations with a standard choice of the learning rate  $\\lambda$**. Here is how.\n\nLet us choose $\\lambda$ such that it is no more than $1/m$, which is the case in all our experiments where $m = 200$ and $\\lambda = 10^-4$, as stated in the appendix. With this choice, we have:\n\n$(1 + \\lambda \\cdot \\mu)^{(m - 1)} \\leq (1 + \\mu / m)^{(m-1)} < (1 + \\mu/m)^m$ \n\nwhich will approach $e^mu$ in the limit of $m$. Here, we use the known fact that $\\mathrm{lim}_{m\\rightarrow\\infty}(1 + \\mu/m)^m = e^\\mu$ with $\\mu > 0$.\n\nAs such, when $m$ is sufficiently large the bound in Theorem 1 is upper-bounded with\n\n$m \\cdot\\lambda \\cdot \\ell \\cdot (1 + \\lambda \\cdot \\mu)^{(m - 1)} \\cdot \\text{gradient-gap} \\simeq \\ell \\cdot e^\\mu \\cdot\\text{gradient-gap}$\n\nwhich asserts that the worst-case performance gap of our offline optimizer is approaching (in the limit of $m$) $\\ell \\cdot e^\\mu \\cdot \\text{gradien-gap} = \\mathbf{O}(\\text{gradient-gap})$ which is not dependent on the number of gradient steps.\n\nThus, the bound is not growing exponential in $m$ and hence, it is not loose and trivial. Please let us know if this has addressed the reviewer\u2019s concern.\n\n\n**We will address your remaining concerns in the next comment**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086206663,
                "cdate": 1700086206663,
                "tmdate": 1700086206663,
                "mdate": 1700086206663,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FQTfQXfBe3",
                "forum": "A0DI5v6m8O",
                "replyto": "50EgseTB1W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Reviewer_1MFJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Reviewer_1MFJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' detailed response.   Part of my concerns have been addressed. I am happy to see the additional insight provided by the authors. However, some of my concerns remain unsolved.\n\n$1. \\textbf{Optimizing Eq. (11) might not guarantee gradient match}$\n\nIt seems that the authors assume  $\\mathbf{F}_{\\phi}(\\mathbf{x}, \\mathbf{x}\u2019)^\\top(\\mathbf{x} - \\mathbf{x}\u2019) = 0$ for all $(\\mathbf{x}, \\mathbf{x}\u2019)$.  \n\nHowever,  we may only guarantee  $\\mathbf{F}_{\\phi}(\\mathbf{x}, \\mathbf{x}\u2019)^\\top(\\mathbf{x} - \\mathbf{x}\u2019) = 0$ for finite $(\\mathbf{x}, \\mathbf{x}\u2019)$ in the training set, even if we assume a strong enough optimizer. \n\nAs a result, we only know $g(\\mathbf{x}) - g_\\phi(\\mathbf{x}) = c$ for finite samples $\\mathbf{x}$ in the training set instead of $\\forall \\mathbf{x} \\in \\mathbb{R}^d$. \n\nThus, we may not conclude $\\nabla g(\\mathbf{x}) - \\nabla g_\\phi(\\mathbf{x}) = 0$ for all $\\mathbf{x}$, especially for the high-dimensional cases that the training data is too sparse to cover the entire domain."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098976532,
                "cdate": 1700098976532,
                "tmdate": 1700099983249,
                "mdate": 1700099983249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jprphMCcyB",
                "forum": "A0DI5v6m8O",
                "replyto": "50EgseTB1W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the quick follow-up. The gradient gap is either zero or bounded as detailed below."
                    },
                    "comment": {
                        "value": "Thank you for the prompt response. Previously, we had proved that in the limit of data if we can find the true optimal solution that zeroes out the generalized loss, the gradient is matched.\n\nNow, suppose that $\\mathbf{F}_\\phi(\\mathbf{x}, \\mathbf{x}\u2019)^\\top(\\mathbf{x} - \\mathbf{x}\u2019)$ is not zero everywhere (i.e., the generalized loss is not zero), we will show that the gradient gap is still bounded by a decreasing quantity.\n\nTo see this, note that $\\mathbf{F}_\\phi(\\mathbf{x}, \\mathbf{x}\u2019)^\\top(\\mathbf{x} - \\mathbf{x}\u2019)$ \n\n$= (g(\\mathbf{x}) - g_\\phi(\\mathbf{x})) - (g(\\mathbf{x}\u2019) - g_\\phi(\\mathbf{x}\u2019))$ \n\nwhich was established in the 5th equation of our previous message. \n\nLet $h(\\mathbf{x}) \\triangleq g(\\mathbf{x}) - g_\\phi(\\mathbf{x})$. It follows that $|\\mathbf{F}_\\phi(\\mathbf{x}, \\mathbf{x}\u2019)^\\top(\\mathbf{x} - \\mathbf{x}\u2019)| = |h(\\mathbf{x}) - h(\\mathbf{x}')|$ which means our loss function is working towards minimizing $|h(\\mathbf{x}) - h(\\mathbf{x}')|^2$ over $(\\mathbf{x},\\mathbf{x}')$. Intuitively, this will make $h(\\mathbf{x})$ smoother as the output distance between different inputs are being reduced. \n\nAs a result, this process will reduce the Lipschitz constant $\\epsilon$ of $h(.)$. Here, we are alluding to the hypothesis that if a model behaves smoothly on the training data, it will also be smooth on the test data. We believe this is reasonable in the broader context as well as this context because most work in offline optimization present some forms of smoothing out the behavior of the model on test data and it is often achieved via conditioning the model on the training data.\n\nNow, **we will show that the gradient gap $||\\nabla g(\\mathbf{x}) - \\nabla g_\\phi(\\mathbf{x})||$ is bounded by $\\epsilon$:**\n\nFirst, by definition, $|h(\\mathbf{x}) - h(\\mathbf{x}')| \\leq \\epsilon \\cdot ||\\mathbf{x} - \\mathbf{x}'||$ which implies\n\n$|(g(\\mathbf{x}) - g(\\mathbf{x}')) - (g_\\phi(\\mathbf{x}) - g_\\phi(\\mathbf{x}'))| \\leq \\epsilon ||\\mathbf{x} - \\mathbf{x}'||$  -- due to the definiton of $h(.)$\n\nDividing both sides by $||\\mathbf{x} - \\mathbf{x}'||$ leads to:\n\n$\\left|\\frac{(g(\\mathbf{x}) - g(\\mathbf{x}'))}{||\\mathbf{x} - \\mathbf{x}'||} - \\frac{(g_\\phi(\\mathbf{x}) - g_\\phi(\\mathbf{x}'))}{||\\mathbf{x} - \\mathbf{x}'||}\\right| \\leq \\epsilon $\n\nSince this holds for any $(\\mathbf{x}, \\mathbf{x'})$, we can choose $\\mathbf{x'} = \\mathbf{x} + t\\cdot \\mathbf{e_j}$, where $t$ is some scalar and $\\mathbf{e_j}$ denotes the $d$-dimensional one-hot vector with the hot component at the $j$-th coordinate. Here, $d$ denotes the input dimension. This gives:\n\n$-\\epsilon \\leq \\frac{(g(\\mathbf{x} + t \\cdot \\mathbf{e_j} ) - g(\\mathbf{x}))}{t||\\mathbf{e_j}||} - \\frac{(g_\\phi(\\mathbf{x} + t \\cdot \\mathbf{e_j}) - g_\\phi(\\mathbf{x}))}{t||\\mathbf{e_j}||} \\leq \\epsilon $\n\nNow, taking $\\mathrm{lim}_{t\\rightarrow 0}$ on all terms in the inequality and using the definition of directional gradient, we will arrive at:\n\n$-\\epsilon \\leq (\\nabla g(\\mathbf{x}) - \\nabla g_\\phi(\\mathbf{x}))^\\top \\mathbf{e_j} \\leq \\epsilon$ , or $\\left| (\\nabla g(\\mathbf{x}) - \\nabla g_\\phi(\\mathbf{x}))^\\top \\mathbf{e_j} \\right| \\leq \\epsilon $\n\nFor each value of $j = 1, 2, \\ldots, d$ , we will obtain one such inequality. Summing all these inequalities gives:\n\n$||\\nabla g(\\mathbf{x}) - \\nabla g_\\phi(\\mathbf{x})||_1 \\leq \\epsilon \\cdot d = \\mathbf{O}(\\epsilon) $\n\nFinally, since norm-2 is upper bounded by norm-1, we also have $||\\nabla g(\\mathbf{x}) - \\nabla g_\\phi(\\mathbf{x})||_2 \\leq \\epsilon $.\n\nThus, **the gradient gap will be bounded by the Lipschitz constant of the function gap $h(\\mathbf{x}) = g(\\mathbf{x}) - g_\\phi(\\mathbf{x})$ and this constant decreases as we optimize the loss function**. In addition, if $\\mathbf{x}$ is within the training data, the gradient gap zeros out as established previously. \n\n**Please let us know if this has addressed your concern. Thank you again for keeping this discussion alive.**"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516862798,
                "cdate": 1700516862798,
                "tmdate": 1700709606515,
                "mdate": 1700709606515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OezdO8QW3T",
            "forum": "A0DI5v6m8O",
            "replyto": "A0DI5v6m8O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7067/Reviewer_ehny"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7067/Reviewer_ehny"
            ],
            "content": {
                "summary": {
                    "value": "This work provide a solid investigation on important problem in offline black-box optimization. The paper is full of insights and is enjoyable to read."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is full of insights and is enjoyable to read."
                },
                "weaknesses": {
                    "value": "I am satisfied with current version."
                },
                "questions": {
                    "value": "1. Could you share more insight on why organizing training data into monotonically increasing trajectories is able to mimic optimization paths? In particular, could you shed more light on the equation (13)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7067/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788385700,
            "cdate": 1698788385700,
            "tmdate": 1699636832055,
            "mdate": 1699636832055,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ls2eRJwpHs",
                "forum": "A0DI5v6m8O",
                "replyto": "OezdO8QW3T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for recognizing our contribution. We answer your question below"
                    },
                    "comment": {
                        "value": "Thank you for the strong support of our work. We are very happy to know that you find our work insightful!\n\nTo answer your question, organizing training data into monotonically increasing trajectories encourages the model to learn the behavior of a gradient-based optimization algorithm, which in fact produces (mostly) monotonically increasing trajectories. This allows the gradient matching algorithm to focus more on strategic input pairs that are more relevant for gradient estimation.\n\nFurthermore, Eq. (13) combines both the gradient match loss and the regression loss. The regression loss helps as a regularizer that amplifies the importance of accurate gradient matching along the sampled trajectories which encodes our bias that the optimal trajectories would be sufficiently close to one of those sampled trajectories."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087589536,
                "cdate": 1700087589536,
                "tmdate": 1700087589536,
                "mdate": 1700087589536,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3upBGzXCvL",
            "forum": "A0DI5v6m8O",
            "replyto": "A0DI5v6m8O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7067/Reviewer_7Vbx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7067/Reviewer_7Vbx"
            ],
            "content": {
                "summary": {
                    "value": "The paper first provides a bound for the performance gap between oracle and a chosen surrogate as a function of how well the surrogate matches the gradient field of the oracle on the offline training data and then using the analysis done to bound the performance gap formulates an algorithm which uses multiple monotonic trajectories of hopping over training points after binning and splitting them based on percentile values for the task of black-box offline optimization where access to oracle is unavailable in the sense that no new input and output value cannot be acquired or sampled. The authors show that the algorithm does better compared to other baselines on tasks and datasets proposed in Trabucco et al., 2022 both in terms of Mean Normalized Rank and Mean Normalized score over multiple percentiles of candidate solutions provided by the algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is mostly well written and the relevant literature and references are covered well.\n2. The math looked sound to me as far as I could see.\n3. The algorithm performs well over baselines both on Mean Normalized Rank and Mean normalized score metrics for both 50 percentile and 100 percentile.\n4. The paper gives complexity analysis of the proposed algorithm.\n5. Figures look good and support the narrative and many(9) baseline algorithms are tried and compared with the proposed algorithm."
                },
                "weaknesses": {
                    "value": "I am not familiar with this field of research and so my comments should be taken with a grain of salt. I am ready to revise the score after reading other reviews and authors' rebuttal.\n\n1. Typo: Page 9, missing reference.\n2.  There is some repetition in the section Evaluation Methodology and section on Results and Discussion, the readibility of those sections can be improved.  \n3. The paper does not compare the memory and time complexity of the algorithm with baselines. This will also depend on the hyperparameters of the optimization algorithm like the discretization parameter.\n4. Some questions are unanswered which I list below.\n5. The paper does not explicitly state its limitations compared to baselines especially since no baseline and proposed algorithm consistently outperforms the other methods on all tasks. Can the end-user make a judgment ?"
                },
                "questions": {
                    "value": "Some questions, the answers to which can help the paper\n1. How does the discretization parameter affect the performance of the algorithm ?\n2. How do the optimization hyperparameters affect the performance, also how do you choose or tune $\\alpha$, the term balancing the two loss terms ?\n3. Right now, the objective contains a sum of two terms: value matching loss and gradient matching loss as shown in Fig. 2, what effect does each term have and are they of the same scale or their scales vary a lot on these tasks ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7067/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7067/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7067/Reviewer_7Vbx"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7067/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798825420,
            "cdate": 1698798825420,
            "tmdate": 1699636831913,
            "mdate": 1699636831913,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DjKJ9sY2Y0",
                "forum": "A0DI5v6m8O",
                "replyto": "3upBGzXCvL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review and favorable initial rating. We would like to address your question below"
                    },
                    "comment": {
                        "value": "**1. Compare running time with other baselines.** Thank you again for the suggestion. We will quote the memory and time complexity of the baselines in the revised version. Some of these are not made explicitly in their corresponding paper but we can look into their formulation to deduce their complexities. \n\nWe do, however, want to remark that such complexity comparison is only tangential to our main contribution. Our main focus is on building optimizer with better and more stable performance overall, even at an affordable increase of running time. Furthermore, we want to point out that as some of the baselines (such as BONET) use an entirely different model which has a different number of parameters than ours, the complexity comparison might not be apple-to-apple. \n\nBut regardless, we want to report the averaged running time of the baselines below.\n\n|      | OUR  | BO-qEI | CMA-ES | ROMA | MINS | CBAS | BONET | GA | ENS-MEAN | ENS-MIN | DDOM |\n|------|------|--------|--------|------|------|------|-------|----|----------|---------|------|\n| Time | 4785 | 111    | 3804   | 489  | 359  | 189  | 614   | 45 | 179      | 179     | 2658 |\n\nAll reported running times are in seconds. Our algorithm incurs more time than other baselines but its total running time is still affordable in the offline setting: 4785s = 1.32hr.\n\nOnce again, **we want to emphasize that (1) it is not our claim contribution to be faster than the baseline; and (2) as we are optimizing expensive-to-evaluate objective functions (e.g., evaluating candidate materials through physical experiments), providing more stable performance (i.e., MNR) in exchange for an affordable increase (around 1hr) of offline computational time is still preferred by the practitioners.**  \n\n**2. Limitations compared to baselines.** One potential limitation of our approach in comparison to other baselines is that our gradient match algorithm learns from pairs of data points. Thus, the total number of training pairs it needs to consume grows quadratically in the number of offline data points. For example, an offline dataset with N examples will result in a set of O(N^2) training pairs for our algorithm, which increases the training time quadratically. However, an intuition here is that training pairs are not equally informative and, in our experiments, it suffices to get competitive performance by just focusing on pairs of data along the sampled trajectories with monotonically increasing objective function values. This allows us to keep training cost linearly with respect to N. \n\nOn another note, while it is true that none of the existing baselines (including our algorithm) outperform others on all tasks, we believe that at least on these benchmark datasets, our algorithm tends to perform most stably across all tasks, as measured by the mean averaged rank reported in each of our performance tables. This is a single metric that is computed based on the performance of all baselines across all tasks. The end-user can make a judgment based on such metrics. In practice, by looking at how existing baselines perform overall on a set of benchmark tasks that are similar to a target task, one can decide empirically which baseline is most likely to be best for the target task.\n\n**3.How does the discretization parameter affect the performance of the algorithm?** Our empirical inspections suggest that a discretization $\\kappa = 5$ in Eq. 12 is sufficient to get good performance. Further increasing $\\kappa$ only results in negligible changes in performance. In addition, increasing kappa will increase the time complexity linearly, as detailed in our Complexity Analysis paragraph following Eq. (13). \n\n**We will address your remaining questions in the next comment.**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191575159,
                "cdate": 1700191575159,
                "tmdate": 1700191575159,
                "mdate": 1700191575159,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]