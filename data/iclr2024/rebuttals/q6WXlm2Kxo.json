[
    {
        "title": "Masked Diffusion as Self-supervised Representation Learner"
    },
    {
        "review": {
            "id": "5tBeVjxdou",
            "forum": "q6WXlm2Kxo",
            "replyto": "q6WXlm2Kxo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission401/Reviewer_caGX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission401/Reviewer_caGX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes masked diffusion model (MDM) for self-supervised learning. Masked diffusion is inspired by the use of the denoising diffusion as a representation learning approach. However, instead of using different levels of Gaussian noise, it uses masks, which at different timesteps of the forward process, blocks out more of the image. The inverted process is then learning to undo this masking operation.\nThe ability of masked diffusion to learn a useful representation for image segmentation is investigated on public datasets of both medical images (GlaS and MoNuSeg) as well as natural images (FFHQ-34 and CelebA-19) and compared both to standard supervised approaches (recent variations of convolutional and transformer based models) and to representation learning approaches based on the Masked AutoEncoder (MAE), the standard denoising diffusion probabilistic model (DDPM) and self-supervision approaches such as SwAV."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Self-supervised learning is highly relevant and the manuscript is well written and easy to read.\n\n- The approach appears to be novel and while the idea is perhaps not extremely original given prior work, the contribution is still solid.\n\n- The approach is compared to a large number of other relevant approaches and consistently appears to do best, whether in the fully supervised case where 100% of the data is used for training the downstream segmentation task or in a more limited case where only 10% of the data is available for training the downstream segmentation task.\n\n- The experiments appear to be easy to reproduce as the authors plan to release the code on acceptance."
                },
                "weaknesses": {
                    "value": "- \"This paper decomposes the interrelation between the generative capability and representation learning ability inherent in diffusion models.\" - I am not really sure what is meant by this or why it is significant.\n\n- \"a scalable self-supervised representation learner...\" - What is meant or referred to with scalable here? It does not appear to be mentioned elsewhere.\n\n- \"While the prediction task demands a focus on high-level, low-frequency structural aspects of images,\" - I would have liked this to be better explored or more well supported to make statements such as this.\n\n- \"the representation ability of diffusion models does not originate from their generative power.\" - I don't see where this is investigsted in the proposed manuscript. E.g. the generative power of the proposed model is not shown.\n\n- Will trained model weights also be released on acceptance?\n\n- I am concerned by the apparent lack of a validation set. There seems to be a number of hyper parameters involved such as the choice of timesteps, blocks, patch sizes and training schedules covered in section B. Other choices such as loss function (SSIM or MSE) were also made. It is unclear if test sets were involved in these choices and if so can we expect the reported results to generalize?"
                },
                "questions": {
                    "value": "- See the question in the above related to the use of validation sets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788854526,
            "cdate": 1698788854526,
            "tmdate": 1699635966980,
            "mdate": 1699635966980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zG9WUpDNhy",
                "forum": "q6WXlm2Kxo",
                "replyto": "5tBeVjxdou",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer caGX"
                    },
                    "comment": {
                        "value": "We would like to thank you for your positive evaluation of our work and providing valuable and insightful comments. Please find our responses to your comments in what follows:\n\n**Q1:**\n\"This paper decomposes the interrelation between the generative capability and representation learning ability inherent in diffusion models.\" - I am not really sure what is meant by this or why it is significant.\n\nWe apologize for the confusion.\n\nRecent works [1, 2, 3] which use diffusion models as representation learners usually assume that the superior generative quality of diffusion models supports them as meaningful discriminative learning methods. However, in this paper, we analyze the diffusion models from the self-supervised learning perspective and extend denoising diffusion models into a self-supervised pre-training algorithm, using denoising as a preliminary task. Then, we do not need to stick to denoising, as we did in our paper, if our objective is representation learning rather than image synthesis. This conclusion is significant since it allows researchers not to focus too much on the generative performance of diffusion models if their goal is to acquire meaningful representations. Instead, future works can explore various corruptions and adapt them to the diffusion framework for various data and downstream tasks. In practice, many works [1, 4, 5] which use extracted diffusion representations for downstream segmentation tasks can be potentially improved.\n\n**Q2:**\n\"a scalable self-supervised representation learner...\" - What is meant or referred to with scalable here? It does not appear to be mentioned elsewhere.\n\nWe are sorry not making this information clear enough. The scalibility of MDM has been discussed in Appendix B of the original paper. We find that the representations learned by MDM continue to improve with longer training (Figure 7), which suggests that MDM is a scalable approach.\n\n**Q3:**\nWhile the prediction task demands a focus on high-level, low-frequency structural aspects of images,\" - I would have liked this to be better explored or more well supported to make statements such as this.\n\nThanks for pointing this out. We revised this statement in the manuscript to make it clearer. We made the statement in the context of the few-shot scenarios where a lightweight network is used to avoid over-fitting. Due to the limited model capacity and labels, the model is not able to learn a good dense prediction if it has to process low-level, high-frequency details of images (e.g., texture). Therefore, it is essential that MDM learns high-level, low-frequency structural information in pre-training and provides the learned meaningful representations during fine-tuning.\n\n**Q4:**\n\"the representation ability of diffusion models does not originate from their generative power.\" - I don't see where this is investigsted in the proposed manuscript. E.g. the generative power of the proposed model is not shown.\n\nWe did not show the generative power of MDM because MDM is not designed to be a generative model. As shown in Section 3.1, the\ngenerative power of a denoising diffusion model is based on two prerequisites: (1) The forward process should be Gaussian so that Equation 2 and the whole derivation hold. (2) $q(x_{T})$ should be close to $N(0, I)$ so that we can sample from a standard Gaussian distribution and iteratively apply Equation 2 to generate images. MDM apparently does not fulfill these prerequisites (prerequisite 1 does not hold since MDM does not use Gaussian noise; prerequisite 2 does not hold since $q(x_{T})$ in MDM is not $N(0, I)$) and therefore should not be expected to have a good generative capability. Despite that, as discussed in Q1, MDM shows robust representation learning ability. That's why we claim that the representation ability of diffusion models does not originate from their generative power.\n\n**Q5:**\nWill trained model weights also be released on acceptance?\n\nYes, we will release our code with checkpoints on acceptance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460462678,
                "cdate": 1700460462678,
                "tmdate": 1700460462678,
                "mdate": 1700460462678,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ksRg9RiD5K",
            "forum": "q6WXlm2Kxo",
            "replyto": "q6WXlm2Kxo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission401/Reviewer_Sbaz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission401/Reviewer_Sbaz"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel approach to self-supervised learning using diffusion models. The authors introduce \"masked diffusion\", where portions of data are probabilistically masked and subsequently reconstructed through a diffusion process. This dynamic masking strategy offers a distinct advantage over traditional static methods, enabling the capture of complex data patterns.\n\nEmpirical results showcase the method's excellence, outperforming established self-supervised benchmarks and achieving state-of-the-art performance on multiple datasets. \n\nIn essence, this work suggests a potential paradigm shift in self-supervised learning, emphasizing the significance of dynamic masking and diffusion models in data reconstruction and representation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n\n1. **Novel Concept**: The paper presents a new approach in self-supervised learning using diffusion models. The probabilistic mask for data occlusion offers an alternative to traditional static methods, suggesting a different way to approach representation learning.\n\n2. **Empirical Evidence**: The results and ablation studies provide evidence of the method's performance. The proposed technique shows improvements over vanilla MAE, DDPM, and certain traditional models on segmentation datasets."
                },
                "weaknesses": {
                    "value": "Areas of Improvement for the Paper:\n\n1. **Benchmarking for Segmentation Tasks**: The primary focus on segmentation necessitates benchmarking against specialized self-supervised learning (SSL) methods designed for this task, both at the instance-level and pixel/patch-level. A direct comparison with methods such as Leopart, IIC, MaskContrast, DenseCL, MoCoV2, and DINO on standard datasets like COCO and PVOC would provide a holistic evaluation. Refer to paper: Self-Supervised Learning of Object Parts for Semantic Segmentation for methods specific to semantic segmentation.\n\n2. **General SSL Representation Benchmarking**: If general SSL representation learning is the core objective, the method should be contrasted with prevailing SSL techniques (e.g., DINO, MoCoV2) across a spectrum of downstream tasks, including classification, object detection, and linear probing.\n\n3.**Demonstration of Versatility**: To set this work apart, the authors can also consider to showcase results across diverse data domains. Presenting outcomes on datasets related to audio, text, or time-series would emphasize the method's adaptability.\n\n4. **Robustness Against Noisy Data**: The paper could benefit from an evaluation of the model's robustness against noisy or corrupted data, providing a measure of its real-world applicability.\n\n5. **Interpretability and Visualization**: Including a section on interpretability, with visualizations illustrating the diffusion process or the dynamic masking strategy, would help readers better grasp the underlying mechanisms.\n\n6. **Discussion on Limitations**: A more explicit section discussing the method's limitations, potential pitfalls, or scenarios where it might not be the best fit would offer a balanced perspective."
                },
                "questions": {
                    "value": "If some points in the weaknesses section are addressed, the score can be increased."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799690719,
            "cdate": 1698799690719,
            "tmdate": 1699635966883,
            "mdate": 1699635966883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fapojF4TpL",
                "forum": "q6WXlm2Kxo",
                "replyto": "ksRg9RiD5K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sbaz"
                    },
                    "comment": {
                        "value": "Thank you for your careful reading of our paper and your valuable comments, which\nhave helped us to improve the presentation of our manuscript. Please find our answers to your\nremarkable comments in the following:\n\n**Q1:**\nBenchmarking for Segmentation Tasks.\n\nWe totally agree that these experiments are important and meaningful. In our manuscript, we focus on the semantic segmentation under a label-efficient setting. We plan to conduct a direct comparison with methods such as Leopart on COCO and PVOC in our future work.\n\n**Q2:**\nGeneral SSL Representation Benchmarking.\n\nThanks so much for the constructive suggestion.\n\nIn the revised manuscript, we constrain our objective to representation learning specifically for segmentation, rather than general SSL representation learning. \n\nNevertheless, we evaluate our method on other downstream tasks and get promising results compared to other prevailing SSL techniques.\n\t\tIn particular, we conduct the MDM pre-training on CIFAR-10 and then use linear probing to test how well MDM performs on classification task. The experimental details and results are shown in Appendix C. We provide the results below:\n| Learning Type      | Method   | Network    | Accuracy (%) |\n|------------------- |----------|------------|--------------|\n| Contrastive        | MoCov1   | ResNet-50  | 85.0         |\n| Contrastive        | MoCov2   | ResNet-50  | 93.4         |\n| Contrastive        | SimCLR   | ResNet-50  | 94.0         |\n| Contrastive        | SimCLRv2 | ResNet-101 | 96.4         |\n|                   |          |            |              |\n| Generative         | DDPM     | U-Net      | 90.1         |\n| Generative         | MDM      | U-Net      | 94.8         |\n\nThe table above shows that MDM achieves much better linear evaluation accuracy than DDPM which has the same learning type and network and competitive performance compared to the contrastive learning methods.\n\n**Q3:**\nVisualization of the dynamic masking strategy.\n\nIn the revised manuscript, we have added the visualization of the dynamic masking strategy in Figure 1. \n\n**Q4:**\nDemonstration of Versatility.\n\n We totally agree that adapting the method to other data domains can be interesting. However, many extra modifications need to be made due to the domain gap. We think it will be an interesting and meaningful direction of further investigation.\n\n**Q5:**\nRobustness Against Noisy Data to Measure The Method's Real-world Applicability.\n\nThanks for giving this constructive suggestion. We agree with the reviewer that a robustness analysis is helpful. \n\nWe added robustness analysis in Appendix B. We apply 15 types of corruption from [1] to the test data\n (Gaussian Noise, Shot Noise, Impulse Noise, Defocus Blur, Frosted Glass Blur, Frost, Brightness, Contrast, Elastic, Pixelate, JPEG, Speckle, Gaussian Blur, Spatter and Saturat). As shown in Figure 8, MDM demonstrates strong robustness against various types of corruption, even without employing any data augmentation related to the applied corruption (e.g., noise) during both pre-training and downstream segmentation training. \n\n**Q6:**\nDiscussion on Limitations.\n\nWe added the discussion of the method's limitations and future work in section 6. \n\n[1] D. Hendrycks and T. G. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460407918,
                "cdate": 1700460407918,
                "tmdate": 1700460407918,
                "mdate": 1700460407918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O1qq4jvmIl",
            "forum": "q6WXlm2Kxo",
            "replyto": "q6WXlm2Kxo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission401/Reviewer_x7rC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission401/Reviewer_x7rC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a pre-training strategy for image segmentation motivated by the recent success of diffusion models. First, an image is masked with different masking ratios, t. Then, the masked image and the mask ratio are given as input to a Unet to restore the original image with SSIM loss. This architecture is called the Masked Diffusion Model (MDM). Once MDM is trained with unlabeled data, a small segmentation network that takes the representation of a decoder as input is trained with a supervised loss on the available labeled data. \nThe experiments are presented on both medical and natural image datasets. The results demonstrate that MDM achieves better performance compared to some SoTA self-supervised learning methods such as MAE and DDPM."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper present an extensive experimental evaluations on 2 natural and 2 medical image data sets with ablation studies."
                },
                "weaknesses": {
                    "value": "- My main concern is the novelty of the method. The paper mentions that with the fixed t, the method degrades to a vanilla masked autoencoder with SSIM loss. This basically means that the only contribution of the paper is masking the image with a dynamic masking ratio during training, which concerns me regarding the contribution of the paper.\n\n- Although the improvement achieved by this small change is interesting on Glas 10% case (IOU is 76.19 for MAE and 82.70 MDM with MSE; which is quite a significant improvement with only this small change (assuming MAE in table 1 is trained with MSE loss)), the results on FFHQ-34 shows that this finding cannot be generalized across datasets (IoU of MAE is 57.06 while MDM with MSE is 55.06).\n\n- Table 4 shows that using SSIM improves the performance significantly both for DDPM and MDM while MDM is still being better. I think a more fair comparison of MDM w/ SSIM loss would have been with MAE w/ SSIM loss, since it would show the effect of changing t dynamically.\n\n- It is not very clear from the paper which decoder level representation is given as input to the segmentation network. Is it only the last layer or some combinations of the last few layers? I also think it would be interesting to know the performance change as a function of using different decoder level features. Additionally, for fair comparison, the same level decoder features should be used for also DDPM and MAE. I didn't see a clear statement about this in the paper."
                },
                "questions": {
                    "value": "- Please address my concerns in the weaknesses section, especially the ones related to the contribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission401/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission401/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission401/Reviewer_x7rC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698855441212,
            "cdate": 1698855441212,
            "tmdate": 1700738289695,
            "mdate": 1700738289695,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pPtd5SGt6V",
                "forum": "q6WXlm2Kxo",
                "replyto": "O1qq4jvmIl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x7rC"
                    },
                    "comment": {
                        "value": "We would like to thank you for reviewing our paper and providing valuable and helpful comments. Please find our responses to your comments in what follows:\n\n**Q1:**\nThe contribution of this paper.\n\nThe reviewer proposes that the contribution is limited because the technical implementation is simple. However, we respectfully disagree with this assessment as Reviewer Sbaz and Reviewer caGX agree unanimously that the proposed method is indeed novel. From the self-supervised learning perspective, even the famous denoising diffusion probability models (DDPM) can also be treated as denoising noisy image with different levels of noise during training, and the technical implementation is really simple as well. The simplicity of implementation does not diminish the effectiveness of MDM as a robust self-supervised representation learning method, much like DDPM's simplicity doesn't detract from its prowess as an impressive image synthesizer.\n To alleviate the reviewer's concern, we summarize our contributions below:\n\n   - In this paper, we analyze the diffusion models from the self-supervised learning perspective and extend denoising diffusion models into a self-supervised pre-training algorithm, using denoising as a preliminary task. We then empirically verify that the representation ability of diffusion models does not originate from their generative power by proposing our MDM which does not have generative power but still possesses powerful representation learning ability. This conclusion is significant since it allows researchers not to focus too much on the generative performance of diffusion models if their goal is to acquire meaningful representations. In practice, many works [1, 2, 3] which use extracted diffusion representations for downstream segmentation tasks can be potentially improved.\n   - Our MDM offers a different way to approach representation learning. We conduct extensive experiments to validate the effectiveness of our method. The state-of-the-art segmentation performance and competitive classification performance show the superiority of the proposed MDM.\n   - The label-efficient attribute of MDM has a wide range of applications. In particular, when developing medical AI, it is usually easy to collect large number of images but not feasible to make the corresponding labels for all the images. The proposed method can be a perfect solution for such scenarios.\n\n**Q2:**\nThe fair comparison with MAE should be added and the results on FFHQ-34 implies the improvement brought by MDM cannot generalize.\n\nWe appreciate this constructive suggestion. We agree that MDM w/ SSIM loss should be compared with MAE w/ SSIM loss for a more fair comparison. We updated our new results in Table 4:\n\n| Method | Loss Type | GlaS 10% (8) | FFHQ-34 |\n|--------|-----------|--------------|---------|\n| DDPM | MSE | 82.32\u00b10.77 | 58.75\u00b10.16 |\n| | | | |\n| DDPM* | MSE | 78.77\u00b11.01 | 51.63\u00b10.16 |\n| | **SSIM** | **81.79\u00b11.13** | **56.97\u00b10.18** |\n| | | | |\n| MAE | **MSE** | **79.47\u00b11.08** | **57.06\u00b10.20** |\n| | SSIM | 76.72\u00b11.75 | 56.55\u00b10.13 |\n| | | | |\n| MDM | MSE | 82.70\u00b10.79 | 55.06\u00b10.21 |\n| | **SSIM** | **84.51\u00b11.15** | **59.18\u00b10.11** |\n\nWe can draw two conclusions from this table:\n\n   - SSIM loss is not suitable for MAE. From our empirical results, SSIM loss can encourage the model to learn the structure information of the whole image because it takes into account structure while MSE loss assumes pixel-wise independence, which is beneficial for downstream dense prediction tasks. However, MAE calculates the loss per patch and only on invisible parts. Using SSIM loss to substitute MSE loss therefore does not make sense in MAE. Similarly, it is also not reasonable to use SSIM loss for DDPM when its reconstruction objective is the added noise.\n\n  - When adopting each corresponding suitable loss function, MDM achieves better performance than MAE and DDPM on both GlaS and FFHQ-34.\n\n**Q3:**\nQuestions regarding the layers of decoder representation.\n\n We are sorry not making this information clear enough.\n \n  In fact, we discussed the block choosing strategy in Appendix B of our original paper. Also, we visualized the clustering of features extracted from different levels of the decoder for MDM and DDPM in Figure 6. The features from the deeper layers (blocks with smaller values) exhibit coarse semantic masks, while those from shallower layers (blocks with larger values) reveal finer details yet lack the same level of semantic coherence for coarse segmentation. Therefore, we choose the middle blocks B = \\{8, 9, 10, 11, 12\\} and B = \\{5, 6, 7, 8, 9, 10, 11, 12\\} among the 18 decoder blocks for two medical image datasets and two natural image datasets, respectively. \n\n  For a fair comparison, MDM and DDPM use the same blocks for feature extraction. Due to the fact that MAE uses a different architecture (ViT) from MDM and DDPM (U-Net), we extract feature maps from the deepest 12 ViT-L blocks for MAE, following [1]."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460349813,
                "cdate": 1700460349813,
                "tmdate": 1700460349813,
                "mdate": 1700460349813,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1WLW2FbQxn",
                "forum": "q6WXlm2Kxo",
                "replyto": "pPtd5SGt6V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission401/Reviewer_x7rC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission401/Reviewer_x7rC"
                ],
                "content": {
                    "title": {
                        "value": "Further comments"
                    },
                    "comment": {
                        "value": "Thanks to the authors for addressing my comments. \n\nI am still not very convinced by the experiments where MAE is trained with SSIM. I understand SSIM is unsuitable for the vanilla MAE because the loss is only computed over the masked patches. However, the SSIM loss for MAE can be computed in a manner similar to that employed for the MDM method by fixing t in MDM.\n\nSuch an experiment already exists in the paper for the Glas dataset in Table 3. I assume the setting \"MDM with fixed t\" corresponds to MAE with SSIM loss. However, I have concerns regarding this experiment:\n\n- T is fixed to 50 for MDM. This means that MAE with SSIM is only tested at T=50. However, we do not know that T=50 is the best hyperparameters for MAE with SSIM. I doubt this because  MAE with SSIM with T=50 is even lower than MAE with MSE (e.g. IoU 79.67 vs 81.35 in Glas 100%). If this is the case, then there is the question of why SSIM contributes more to MDM than MAE. The authors' answer to this question is, \"However, MAE calculates the loss per patch and only on invisible parts. Using SSIM loss to substitute MSE loss therefore does not make sense in MAE.\", however, this is not the case anymore since SSIM for should be computed as in the MDM experiments. \n\n- The above question can be answered with empirical results (by carefully tuning T). For this, we need results on multiple datasets such as FFHQ and CelebA. Currently, there are only results for the Glas dataset in Table 3."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568218434,
                "cdate": 1700568218434,
                "tmdate": 1700568218434,
                "mdate": 1700568218434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TYgDnuMyuu",
                "forum": "q6WXlm2Kxo",
                "replyto": "7nRMsXn4Gn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission401/Reviewer_x7rC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission401/Reviewer_x7rC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response and clarification. \n\n- I apologize for my confusion. I overlooked that the values IoU 79.67 vs 81.35 in Glas 100% are not directly comparable because the architecture, loss, and masking ratios are different in both experiments. These results do not show the effect of SSIM loss.\n\n- Let me clarify my previous comments. I would like to see the answers to the following questions in the paper:\n\nQ1) How does MAE perform if it is trained with SSIM loss? \n- This can be shown in 2 ways:\n1.1. Training the original MAE with ViT architecture with SSIM loss by applying the loss to all patches as in MDM. The table in the authors' first response was obtained with SSIM loss applied on the masked patches only, which is unsuitable, as also stated by the authors.\n1.2. Performing experiments by fixing t in MDM. This setting corresponds to MAE with a UNet instead of ViT. However, t should be selected carefully based on a validation set accuracy. Such an experiment already exists in the paper in Table 3 for the Glas dataset with t=50. This corresponds to roughly a 5% masking ratio which is significantly lower than the suggested (75%) in the original MAE paper. I understand that setting t using a validation set might require some time, but then the ration should be set to 75% as suggested in the MAE paper.\n\nQ2) Does the result in Q1 hold for other datasets?\nThe experiments to answer Q1 should be done for multiple dataset to show the results generalizes to multiple datasets.\n\n- I have one additional experiment that might be useful for the future could be MDM with the ViT architecture as in the MAE paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592990673,
                "cdate": 1700592990673,
                "tmdate": 1700592990673,
                "mdate": 1700592990673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4aKf8JHEjv",
                "forum": "q6WXlm2Kxo",
                "replyto": "5jFW9qBQwO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission401/Reviewer_x7rC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission401/Reviewer_x7rC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the additional experiments. Now, we can see that MAE w/ UNet+SSIM outperforms MAE w/ ViT+MSE despite using a more lightweight architecture, even without a carefully tuned t for the former. \n\nMDM still benefits from using dynamic t since t is a crucial parameter and it eliminates the need for fine-tuning this parameter. I think this is quite useful, and I now view the paper more positively. I will update my score accordingly."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738256373,
                "cdate": 1700738256373,
                "tmdate": 1700738256373,
                "mdate": 1700738256373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C496Mylfdk",
            "forum": "q6WXlm2Kxo",
            "replyto": "q6WXlm2Kxo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission401/Reviewer_V2xK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission401/Reviewer_V2xK"
            ],
            "content": {
                "summary": {
                    "value": "This paper applies masking to diffusion models and shows improvement in segmentation tasks for both medical and natural images. The authors also try to use SSIM instead of MSE for pre-training in order to improve the downstream segmentation performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The statistical results shown in Table 1 look promising if all the methods are compared fairly."
                },
                "weaknesses": {
                    "value": "1. The writing of this paper needs to be improved. Many claims in the introduction section are not very well-supported (e.g. \"such efforts risk deviating from the theoretical underpinnings of diffusions\") and are not very well organized.\n2. It is not convincing enough to conclude that the representation learned is better while only tested on segmentation downstream tasks.\n3. The choice of SSIM over MSE is rather empirical and not well justified."
                },
                "questions": {
                    "value": "1. The authors chose SSIM over MSE to improve the segmentation performance. However, there are also other types of choices such as normalized cross-correlation, MAE etc. It is also more of a trade-off between learning a general representation than tuning toward specific downstream tasks (e.g. segmentation in this case). The authors need to justify more about this.\n\n2. Since the goal of the proposed method is to learn a better representation, how well the method performs on other downstream tasks?\n\n3. Figure 2 and Figure 3 are quite repetitive and it is very hard to conclude which methods are better. Reporting Dice scores along each figure will be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission401/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission401/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission401/Reviewer_V2xK"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699029345657,
            "cdate": 1699029345657,
            "tmdate": 1699635966756,
            "mdate": 1699635966756,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l7UvZAe6PV",
                "forum": "q6WXlm2Kxo",
                "replyto": "C496Mylfdk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission401/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer V2xK"
                    },
                    "comment": {
                        "value": "We would like to thank you for reviewing our paper and providing valuable and helpful comments. Please find our responses to your comments in what follows:\n\n**Q1:**\nThe statistical results shown in Table 1 look promising if all the methods are compared fairly.\n\n- Thanks for the positive comment. We tried our best to make sure all the experiments being fair:\n  - We use the same settings for all methods unless it is infeasible.\n  - We use each method's official implementation and checkpoints (if released).\n  - We conduct all the experiments 10 times using 10 random seeds and report the standard deviation and mean.\n\n**Q2:**\nThe writing of this paper needs to be improved.\n\n- We have revised the manuscript for most parts which may cause confusion and reorganized the manuscript. The new manuscript is much clearer and the claims are well-supported.\nRegarding the specific claim you mentioned, we say that ''such efforts risk deviating from the theoretical underpinnings of diffusion'' for the reason that: As shown in Section 3.1, the whole theoretical derivation of denoising diffusion model is based on the assumption that the tractable posterior used to generate images from a standard Gaussian distribution is Gaussian, which is fulfilled in denoising diffusion by adding Gaussian noise in the forward process. However, recent methods such as Cold Diffusion [1] have replaced the Gaussian noise with other corruptions which can not be described by Gaussian distributions, making them deviate from the theoretical underpinnings of diffusion and suffer from worse sample quality.\n\n**Q3:**\nHow well the method performs on other downstream tasks?\n\n- To evaluate the performance of our method on other downstream tasks, we conduct the MDM pre-training on CIFAR-10 and then use linear probing to test how well MDM performs on a classification task. The experimental details and results are shown in Section C of the Appendix. We provide the results below:\n\n| Learning Type      | Method   | Network    | Accuracy (%) |\n|------------------- |----------|------------|--------------|\n| Contrastive        | MoCov1   | ResNet-50  | 85.0         |\n| Contrastive        | MoCov2   | ResNet-50  | 93.4         |\n| Contrastive        | SimCLR   | ResNet-50  | 94.0         |\n| Contrastive        | SimCLRv2 | ResNet-101 | 96.4         |\n|                   |          |            |              |\n| Generative         | DDPM     | U-Net      | 90.1         |\n| Generative         | MDM      | U-Net      | 94.8         |\n\n\n&ensp; &ensp; The table above shows that MDM achieves much better linear evaluation accuracy than DDPM which has the same learning type and network and competitive performance compared to the contrastive learning methods. \n\n**Q4:**\nJustify the choice of SSIM over MSE. Is it a trade-off between learning a general representation than tuning toward specific downstream tasks?\n\n- From the experimental results, we observe a phenomenon that even the MSE loss is quite low during pre-training (which indicates that the image restoration is good based on MSE loss), the segmentation performance of the downstream task using the learned representations is not always good. The reason could be that MSE loss assumes pixel-wise independence. As another commonly used loss function in image restoration task, SSIM loss can encourage the model to learn the structure information of the whole image because it takes into account structure, which is beneficial for downstream dense prediction task. \n   \n    Regarding the trade-off between learning a general representation than tuning toward specific downstream tasks, we believe that better loss functions than SSIM loss must exist for specific datasets and downstream tasks. We do not intend to claim SSIM loss is only choice.\n\n    However, we do not think using SSIM loss in our MDM is a trade-off for the reason that our MDM with SSIM loss not only achieves state-of-the-art semantic segmentation performance on all four datasets we used, but also demonstrates competitive classification performance on CIFAR-10, as discussed above.\n\n**Q5:**\nFigure 2 and Figure 3 are quite repetitive and it is very hard to conclude which methods are better. Reporting Dice scores along each figure will be helpful.\n\n- Thanks for pointing this out. It is indeed hard to conclude which methods are better from Figure 2 and Figure 3 in the original manuscript. In the revised manuscript, we report the dice score of each prediction in the corner for Figure 3 and Figure 4 (Figure 2 and Figure 3 in previous version).\n\n[1] A. Bansal, E. Borgnia, H.-M. Chu, J. S. Li, H. Kazemi, F. Huang, M. Goldblum, J. Geiping, and T. Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460154858,
                "cdate": 1700460154858,
                "tmdate": 1700460154858,
                "mdate": 1700460154858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]