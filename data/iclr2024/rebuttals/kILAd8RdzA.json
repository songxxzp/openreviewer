[
    {
        "title": "On the Generalization and Approximation Capacities of Neural Controlled Differential Equations"
    },
    {
        "review": {
            "id": "NaLjMivEhg",
            "forum": "kILAd8RdzA",
            "replyto": "kILAd8RdzA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1211/Reviewer_V1D4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1211/Reviewer_V1D4"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the generalization bound for neural-controlled differential equations(NCDE). Given an NCDE that minimizes the empirical risk from some sampled data, the author first derives a generalization bound between the empirical and expected risk given the sampling process for the input time series, and then an upper bound on the difference between the expected risk and the optimal risk from the ground truth predictor."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. novel generalization bound for neural-controlled differential equations.\n2. an upper bound on the excessive risk from sampling-induced bias and approximation bias."
                },
                "weaknesses": {
                    "value": "1. The presentation in Theorem 4.1 could be improved, as it is hard to see how the bound explicitly depends on depth $q$, and discretization $D$. Right now it is hidden in the constant $C_q$ and $K_1^D, K_2^D$. \n2. The experiments seem very preliminary: the sampled time series only have $K=5$ points. I am not sure how interesting it is to investigate this regime as the generalization error would be poor with this few samples."
                },
                "questions": {
                    "value": "1. Any assumption on the continuous $x$, or its distribution, if it is random.\n2. $|D|$ appears in Remark 3.5, but defined much later."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1211/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698612155442,
            "cdate": 1698612155442,
            "tmdate": 1699636047682,
            "mdate": 1699636047682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kC97F6emO6",
                "forum": "kILAd8RdzA",
                "replyto": "NaLjMivEhg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1211/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1211/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer V1D4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging our contributions and results. Let us first address the two mentioned weaknesses. We agree that the presentation of Theorem 4.1 can be improved and we will try to do so within the page constraints. In particular, as this has also been highlighted by Reviewer 1, we will make the constants explicit. Concerning the experimental part, we make two remarks. First, as the discretization gap $|D|$ vanishes at linear speed - i.e. $1/M$ when sampling $M$ points in a equidistant fashion - the effect of discretization is almost null when short time series are sampled at high frequency. We thus have chosen this low sampling regime to obtain a visible effect. Secondly, our article is of theoretical nature ; we have chosen to give an extensive presentation of our theoretical results, which are complex and require long developments, instead of a more developed experimental section. We are planning on conducting a follow-up work benchmarking various methods designed for supervised learning with irregular time series, which will be the empirical companion paper to this work. We also refer the reader to the growing body of applied works on NCDEs, which we extensively cite in the introduction. \n\nWe now answer the two questions raised by the reviewer. First, the underlying continuous path $x$ is supposed to be unknown and observed at a finite number of sampling times. It is continuous of bounded variation, which is required for NCDEs to be well defined (Assumptions 1 and 2). We make the simplifying assumption that the paths are Lipschitz, but this can be weakened to include paths paths of bounded variation with non linear modulus of continuity. We work in a fixed design setting, i.e. we consider our data to be fixed throughout the article. As highlighted in our response to Reviewer 2, this stands in contrast to settings in stochastic control theory, where one learns over the distribution of random paths (i.e. stochastic processes with parametrized vector fields). Secondly, we thank the reviewer for catching the typo on $|D|$, which we have fixed."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127471341,
                "cdate": 1700127471341,
                "tmdate": 1700127471341,
                "mdate": 1700127471341,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "knqjQKsB02",
            "forum": "kILAd8RdzA",
            "replyto": "kILAd8RdzA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1211/Reviewer_QrUR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1211/Reviewer_QrUR"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript presents a theoretical investigation into the generalization and approximation capacities of neural controlled differential equations (NCDEs). It aims to answer critical questions about the generalization and approximation capabilities of NCDEs and how these are affected by irregular sampling. The theoretical contributions are backed by experiments. The authors provide a rigorous mathematical framework to offer insights into the capabilities and limitations of NCDEs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\nThe paper stands out for its focus on providing a theoretical framework for NCDEs. This can serve as a foundational work for further investigations.\nQuality:\nThe mathematical rigor of the paper is high, with well-laid-out proofs and theoretical discussions.\nClarity:\nDespite the mathematical complexity, the paper is organized in a way that progressively builds up the reader's understanding of the topic.\nSignificance:\nThe theoretical insights offered could be highly impactful, providing guidelines for both practical applications and future research in machine learning and control theory."
                },
                "weaknesses": {
                    "value": "In Abstract: ``However, no theoretical analysis of their performance has been provided yet''. This is not true. \n\nThe paper could do a better job of situating its contributions within the existing literature, especially to clarify its novelty.\n\nWhile the focus is theoretical, some discussion on the practical implications of these theories could make the paper more well-rounded.\n\nThere are some grammar/format issues, such as, ``since y is the sum of a linear transformation of the endpoint of a CDE an a noise term \u03b5 bounded by \u2026\u2019\u2019"
                },
                "questions": {
                    "value": "Could you clarify how your theoretical contributions on the approximation part diverge from existing works on the approximation ability of NCDE like [1] or [2]?\n\n[1] Patrick Kidger. On neural differential equations. arXiv preprint arXiv:2202.02435, 2022.\n\n[2] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. Advances in Neural Information Processing Systems, 33:6696\u20136707, 2020. \n\nWhat are the potential practical applications that could benefit from the theoretical insights provided in this paper? Could these insights lead to more efficient or accurate NCDE models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1211/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649779870,
            "cdate": 1698649779870,
            "tmdate": 1699636047604,
            "mdate": 1699636047604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HqUoWKlu9h",
                "forum": "kILAd8RdzA",
                "replyto": "knqjQKsB02",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1211/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1211/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer QrUR"
                    },
                    "comment": {
                        "value": "We warmly thank the reviewer for acknowledging the originality, quality and clarity of our work. \n\n**Relation to earlier work.** We agree that our presentation of the related work by Kidger (2022) and Kidger (2020) is incomplete and that the abstract is misleading. The contributions of these two works can be summarized as follows. First, the authors proves the existence of solutions to neural CDEs. This result follows from the Picard-Lindelh\u00f6f theorem. Secondly, the authors of this article also prove a universal approximation property for neural CDEs i.e. that sufficiently wide NCDEs can approximate any continuous function mapping the space of paths to $\\mathbb{R}$ (Theorem 3.1. in Kidger 2020, and Theorem C.25 in Kidger 2022). We have emphasised these contributions in the abstract and the introduction. In contrast to this, our approximation bound provides a precise decomposition of the approximation error in the setup where the function linking the path to the outcome is a CDE. \n\nWe also thank the reviewer for noticing typos, which we have fixed. \n\n**Implications of our work.** A deeper discussion of practical implications is indeed needed, as suggested by the reviewer. We see three principal implications of our work. \n\n1. **Designing new architectures for NCDEs.** Our paper offers performance guarantees and sheds light on the theoretical behaviour of NCDEs. We think that a promising application of our work lies in the design of new architectures for the neural vector field $\\mathbf{G}_\\psi$. In our work, as in most applications of NCDEs, this neural vector field is chosen as a simple MLP. However, a recent and rich literature has highlighted the benefits of more structured architectures (for instance through orthogonal weights - see Jia 2019). Our work could help to understand the theoretical implications of such extensions to NCDEs. On a sidenote, we would like to mention the recent article by Chiu et al. (\"Exploiting Inductive Biases in Video Modeling through Neural CDEs\", 2023), which was unpublished by submission date, and which takes a first step in this direction by using a U-Net as a neural vector field for learning from video data.  \n\n2. **Quantifying the impact of sparse sampling.** On an applied level, our results can serve as a guideline to practitioners that use irregular time series. For instance, from our personal experience, MDs often wonder whether they have collected enough longitudinal data in a clinical setting to conduct inference with NCDEs. Our results on the sampling bias may help to make an informed decision in such a case. Additionally, our results can help a practitioner facing a \"decide now or sample more\" dilemma, in which an agent needs to decide whether she defers a decision in order to acquire more information through time-consuming sampling (see for instance Sch\u00e4fer, \"TEASER: early and accurate time series classification\", 2020, for a discussion of similar problems).\n\n3. **Links with Control Theory.** The reviewer mentions control theory, which can be seen as an inversion of the setup of NCDEs as noted by Kidger 2022 (see Section 3.1.6.2). In control theory, the dynamics are fixed (known or unknown) and one tries to optimise the input path. In our setup, the path is fixed and the dynamics are optimised. We think, however, that our results can transfer to control theory: our theoretical framework can indeed accommodate many situations in which one learns with continuous time data. Consider, for instance, a setup with fixed dynamics in which one minimises a criterion by optimising over the space of paths by choosing the vector fields of a neural SDE (see Zhang, \"Neural Stochastic Control\", 2022 for a close setup). One could use our results on the continuity of the flow to bound the difference between two outputs - driven by two paths with different vector fields - by the difference between the two vector fields, and hence derive an upper bound on the covering number of such a class of predictors. To perform this extension, one would however need to leverage additional tools from stochastic calculus and rough analysis.  \n\nTo satisfy page constraints,**we have included these remarks in Appendix F.1** and include a reference to this part in the conclusion. If the reviewer sees other implications, we would be more than happy to include them as well."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126930764,
                "cdate": 1700126930764,
                "tmdate": 1700126930764,
                "mdate": 1700126930764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p9jh9ImTN9",
                "forum": "kILAd8RdzA",
                "replyto": "HqUoWKlu9h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1211/Reviewer_QrUR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1211/Reviewer_QrUR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558857197,
                "cdate": 1700558857197,
                "tmdate": 1700558857197,
                "mdate": 1700558857197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IF7l5BBHXs",
            "forum": "kILAd8RdzA",
            "replyto": "kILAd8RdzA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1211/Reviewer_c9Ry"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1211/Reviewer_c9Ry"
            ],
            "content": {
                "summary": {
                    "value": "This paper makes a first-of-its-kind attempt at bounding the generalization error of NCDE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The analysis done is thorough and it is definitely an achievement to have setup the formalism required to compute a generalization bound like this."
                },
                "weaknesses": {
                    "value": "In Section 4, Theorem 4.1 the upper bound for the generalization error varies with both the width and depth of the neural network and unlike Chen et al. (2019a), where the growth w.r.t the depth and width is logarithmic, in this case it\u2019s polynomial. This exponential blow-up in the value of bound makes the whole exercise somewhat underwhelming. \n\nAlso its worth mentioning that the discretization dependence in Theorem 4.1 is through the constants K1^D and K2^D\n - and these two critical quantities don't seem defined anywhere in the main paper! This makes the theorem highly opaque! \n\nThe entire point why a generalization error bound is interesting is because it reveals the generalization error to be independent of some natural size parameter of the system. Like, most often the baseline achievement of a Rademacher bound for a neural system is to show that at a fixed depth and norm bound on the weights involved the generalization is width independent. In this bound, in this work, there is absolutely no such thing happening - the bound in Theorem 4.1 is worsening with every such parameter with which naively one would have expected worsening to happen! \n\nThe numerical Illustrations also seem incomplete in the absence of any demonstration of how well the bounds hold up in the cases mentioned there. In the bare minimum, in Figure 4 kind of experiments one would have expected to see a scatter plot of the theoretical bound and the true generalization error as the sampling gap, time and avg. max path variation is changed. Such a scatter plot would have revealed if there is at all any correlation between the theoretical bound and the truth."
                },
                "questions": {
                    "value": "Q1.\nCan the authors explain why there is an exponential degradation of the bound's dependence on width as compared to the result in Chen et al. (2019a), which seems to be the closest literature to this? Can you argue that this degradation is inevitable? \n\nQ2.\nCan the authors point out as to what is the surprise or the non-triviality about the dependencies that are visible in the bound given in Theorem 4.1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1211/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718169034,
            "cdate": 1698718169034,
            "tmdate": 1699636047526,
            "mdate": 1699636047526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zhhv5PIvYy",
                "forum": "kILAd8RdzA",
                "replyto": "IF7l5BBHXs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1211/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1211/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer c9Ry"
                    },
                    "comment": {
                        "value": "We thank the reviewer for this thorough review of our paper and its theoretical part which raises interesting questions, and for acknowledging our contributions. Before turning to the core of the review, we agree that giving the explicit form of all constants will make Theorem 4.1. more clear, as it has also been noted by Reviewer 3. We have added references to the constants in Theorem 4.1. \n\n**Comparison with earlier work by Chen et al. (2020).** We first address the comparison made with the earlier results of Chen et al. (\"On Generalization Bounds of a Family of Recurrent Neural Networks\", 2020 - we refer to AISTATS version from now on). We would first like to emphasise that the parameter $q$, which in our setting corresponds to the depth of the neural vector field applied at each update of the latent state, is completely absent from the aforementioned article: Chen et al. only consider shallow vector fields (i.e. \"vanilla RNN\" in their phrasing). Their neural vector field has no hidden layers but is only parameterized by matrices $W$ and $U$ which have resp. $d_h \\times d_x$ and $d_h \\times d_h$ parameters. We strongly encourage the reviewer to compare the definition of Chen et al.'s vector field (p.2, left column) with ours (Equation 1, page 4) to convince himself of the difference between our models.\n\nWith this clarification in mind, we argue that the first bounds obtained by Chen et al. and ours are of identical nature. Let us take a close look at Theorem 2 in Chen et al. (p. 4, left column): one can see that the complexity term scales as the square root of the number of parameters $d$, times some log factors. We obtain a similar result (see our Theorem 4.1 on page 6): the complexity term, in our bound, grows as the square root of the number of parameters times some log factors. As stressed above, the factor $q$ appears here since having $q$ layers in the neural vector field multiplies the number of parameters by $q$. This is not a surprise since both works use the same fundamental technique (Lipschitz-based complexity measures). In the last part of their paper, Chen et al. provide a slightly improved bound at the price of stronger restrictions on the parameter space. We leave an extension of these techniques to NCDEs to future work. We thank the reviewer for noticing this precise point, which we had missed. **We have added a mathematical and precise comparison of the results in Appendix F.2**."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127184823,
                "cdate": 1700127184823,
                "tmdate": 1700127184823,
                "mdate": 1700127184823,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QYS0z4AOof",
                "forum": "kILAd8RdzA",
                "replyto": "IF7l5BBHXs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1211/Reviewer_c9Ry"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1211/Reviewer_c9Ry"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "Let me explain again what the issue is with the experiments :\u00a0\nthe second and the third figures of Figure 4 are the only place where the true generalization error is plotted\u00a0- but we have no idea from this plot what the theoretical bound is for these same experiments.\u00a0The empirical Rademacher complexity should have been computable on the trained nets considered in these experiments. \n\nThe authors have themselves\u00a0said the following in the added response at the bottom of page 36,\n\"Our bounds are not depth independent since increasing the depth of the vector field G\u03c8 worsen our generalization bounds which scale with the square root of the number of parameters.\"\n\nThis is the point I have been trying to make.\n\nA generalization bound needs to be \"non-trivial\"\n\u00a0- that it needs to show independence from some parameter on which one would have naively expected the bound to scale with.\n\nThe whole point of generalization theory is to answer the fundamental question - \"When are bigger models better?\"\n\nThis point is simply not getting made here - not at all readable from the given bound.\n\nEven if this is missing, the paper could have been acceptable\u00a0had the experiment made the point of the theoretical bound being correlated to the true generalization error.\n\nWith both these key points missing I am unable to change my scores."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326228578,
                "cdate": 1700326228578,
                "tmdate": 1700326468248,
                "mdate": 1700326468248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]