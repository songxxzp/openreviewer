[
    {
        "title": "IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs"
    },
    {
        "review": {
            "id": "xucf5Gd6nM",
            "forum": "6RR3wU4mSZ",
            "replyto": "6RR3wU4mSZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_nr9P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_nr9P"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach for adaptively identifying the most important keys for each query using a k-NNS approach with a ranking-based algorithm. This method allows mapping keys and queries to a higher-dimensional space, eliminating the constraint that all keys must have the same norm. Furthermore, the authors extend the implementation of prioritized DCI to support incremental updates to the database."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper combines several techniques to accelerate the computation of the attention matrix by pruning the k matrix.\n* The proposed approach is comprehensively evaluated on LRA and LLM benchmarks, and it outperforms SOTA approximation methods, demonstrating its effectiveness."
                },
                "weaknesses": {
                    "value": "The adaptation of the k-NNS method may potentially make the computation less parallelizable. It is essential to consider the scalability of the proposed method, even on CPU-based platforms. Leveraging multi-threading and distributed computing for scalability is a relevant concern."
                },
                "questions": {
                    "value": "* It would be valuable if the paper provided insights into the scalability of the proposed method in terms of performance (compute time) on different CPU platforms with varying numbers of cores. How well can the approach be parallelized or scaled up on different hardware configurations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698420210642,
            "cdate": 1698420210642,
            "tmdate": 1699636039944,
            "mdate": 1699636039944,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V4mBrhuAFC",
                "forum": "6RR3wU4mSZ",
                "replyto": "xucf5Gd6nM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable review. Please find our responses as follows.\n\n**Q1: The adaptation of the k-NNS method may potentially make the computation less parallelizable. It is essential to consider the scalability of the proposed method, even on CPU-based platforms. Leveraging multi-threading and distributed computing for scalability is a relevant concern.**\n\nA1: Actually k-NNS does not affect the parallelizability. At the level of tokens, neither vanilla self-attention nor k-NNS parallelizes over them in decoder-only models because they are autoregressive. At the level of heads and token dimensions, both vanilla self-attention and k-NNS can parallelize over them in a straightforward way. In our implementation, we do utilize multi-threading within the k-NNS algorithm. To illustrate the scalability of the proposed method, we added the new experiments as per the reviewer\u2019s suggestions and present the results below.\n\n**Q2: It would be valuable if the paper provided insights into the scalability of the proposed method in terms of performance (compute time) on different CPU platforms with varying numbers of cores. How well can the approach be parallelized or scaled up on different hardware configurations?**\n\nA2: We appreciate the reviewer\u2019s insightful suggestion and tested the proposed IceFormer on a different machine. We reran all the experiments of ZeroSCROLLS benchmark on an AMD Ryzen 9 5950X CPU with 16 CPU cores (where we used 32 threads in total to take advantage of simultaneous multithreading), and compare to the results in the original paper, which were obtained on an AMD Ryzen 9 5900X CPU with 12 cores (where we used24 threads in total):\n\n|               **Task**               | **GvRp (8k)** | **SSFD (8k)** | **QMsm (9k)** | **SQAL (8k)** | **Qspr (5k)** | **Nrtv (10k)** | **QALT (7k)** | **MuSQ (3k)** | **SpDg (7.5k)** | **BkSS (7.5k)** |\n|:------------------------------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:--------------:|:-------------:|:-------------:|:---------------:|:---------------:|\n|                                      |               |               |               |               |               |                |               |               |                 |                 |\n| **AMD Ryzen 9 5900X (12 CPU-cores)** |   (old)       |               |               |               |               |                |               |               |                 |                 |\n| **Vicuna-7b-v1.5-16k**               |   5.39        |   5.75        |   7.11        |   5.12        |   2.49        |   7.64         |   4.17        |   0.70        |   4.72          |   4.77          |\n| **Iceformer**                        |   2.24        |   2.14        |   2.67        |   2.15        |   1.06        |   3.39         |   1.85        |   0.49        |   2.09          |   1.96          |\n| **Speed-up**                         |   2.4x        |   2.7x        |   2.7x        |   2.4x        |   2.3x        |   2.3x         |   2.3x        |   1.4x        |   2.3x          |   2.4x          |\n|                                      |               |               |               |               |               |                |               |               |                 |                 |\n| **AMD Ryzen 9 5950X (16 CPU-cores)** |   (new)       |               |               |               |               |                |               |               |                 |                 |\n| **Vicuna-7b-v1.5-16k**               |   5.07        |      5.02         |   6.47        |   5.01        |   2.03        |   6.82         |   3.76        |   0.70        |   4.43          |   4.52          |\n| **Iceformer**                        |   1.89        |   1.81        |   2.51        |   1.92        |   0.89        |   2.85         |   1.26        |   0.37        |   1.47          |   1.55          |\n| **Speed-up**                         |   2.7x        |   2.8x        |   2.6x        |   2.6x        |   2.3x        |   2.4x         |   3.0x        |   1.9x        |   3.0x          |   2.9x          |\n\n\nAs shown by the results in the table above, Iceformer can scale effectively to CPUs with more cores. It is notable that the speed-up of Iceformer applied to Vicuna-7b-v1.5-16k compared to the vanilla Vicuna-7b-v1.5-16k model is larger on a CPU with more cores (16) than the CPU used in for the experiments in the original paper, namely a CPU with 12 cores."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742248153,
                "cdate": 1700742248153,
                "tmdate": 1700742291371,
                "mdate": 1700742291371,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PslTmneGqR",
            "forum": "6RR3wU4mSZ",
            "replyto": "6RR3wU4mSZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_d3mB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_d3mB"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for accelerating inference of long-sequence attention-based Transformers. The proposed method works with pretrained Transformers and does not require retraining. The critical point is to use Prioritized DCI as the k-NNS algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method does not require retraining the model. The retraining may be expensive and unaffordable.\n2. It can be applied to a broad range of LLMs since it has no assumptions on the Transformer architecture.\n3. The proposed method provides a Pareto optimal solution in terms of time and task-related performance (e.g., accuracy), compared with the baselines."
                },
                "weaknesses": {
                    "value": "### Major issues\n1. The proposed method seems orthogonal to the inference platforms. Why do the authors focus on the CPU settings? What about other inference engines, such as GPUs, TPUs, and other accelerators?\n2. The authors discuss many related methods in Section 2. However, only few of them are used as the baselines in empirical comparisons? Could the authors add more baselines? Some methods may need retraining. It is better to list more results even if some baseline needs retraining or specific architectures.\n3. Could we apply the proposed method into training?\n\n### Minor issues\n1. It is not clear how IceFormer will perform on short and medium length of sequences.\n2. Could the authors visualize Tables 1 and 2?\n3. What are the limitations and potential negative impact of the proposed method?"
                },
                "questions": {
                    "value": "1. What may be the future directions and extensions?\n2. The authors mentioned that the code will be released upon acceptance. I do not find implementation in the supplementary material. Is it possible to release it for the reviewers' reference?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1134/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1134/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1134/Reviewer_d3mB"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647448258,
            "cdate": 1698647448258,
            "tmdate": 1700761775289,
            "mdate": 1700761775289,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AuUF3hDxiI",
                "forum": "6RR3wU4mSZ",
                "replyto": "PslTmneGqR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review. Please find our responses as follows.\n\n**Q1: The proposed method seems orthogonal to the inference platforms. Why do the authors focus on the CPU settings? What about other inference engines, such as GPUs, TPUs, and other accelerators?**\n\nA1: Since our main focus is on the deployment of Transformers to end-user devices, we optimized our method for CPUs, since end-user devices are often not equipped with GPUs. \n\nOther hardware platforms such as GPUs and TPUs are fundamentally different from CPUs, with GPUs and TPUs being able to execute a much larger number of parallel threads than CPUs, but also incurring a much higher communication overhead. Therefore, changes need to be made to take advantage of the GPU/TPU features and avoid the performance penalties specific to GPUs/TPUs. Therefore, the optimization approaches that are applicable to CPUs vs. other platforms are very different. On CPUs, the focus is on maximizing cache efficiency and parallelism, both at the instruction level (SIMD) and the thread level (multi-threading). In contrast, on GPUs, the focus is on effectively utilizing the large number of threads without unnecessary communication and synchronization overhead and optimizing accesses to different levels of the memory hierarchy. There isn\u2019t much documentation or literature on TPUs, so effective optimization approaches are somewhat unknown. As a result, performance optimization for different hardware platforms are typically treated separately in the literature, with some papers focusing specifically on CPU optimization, e.g., [1] and others focusing specifically on GPU optimization, e.g., [2]. \n\nWhile it may be possible to extend our method to GPUs, very different optimization approaches will need to be used and changes to the algorithm may be required to minimize communication overhead between different thread blocks. This is beyond the scope of our paper, and we leave it to future work. \n\n**Q2: The authors discuss many related methods in Section 2. However, only few of them are used as the baselines in empirical comparisons? Could the authors add more baselines? Some methods may need retraining. It is better to list more results even if some baseline needs retraining or specific architectures.**\n\nA2: We added Linformer [3], Performers [4] and Longformer [5] as additional baselines. Please see Table 1 and Table 3 in our updated manuscript for details.\n\n**Q3: Could we apply the proposed method into training?**\n\nA3: Yes, it is possible to apply the proposed method to training. Specifically, the same procedure can be used to speed up the forward pass. We did not focus on this use case, however, because we focused on deployment to end-user devices, whereas training is typically done centrally in the cloud.\n\n**Q4: It is not clear how IceFormer will perform on short and medium length of sequences.**\n\nA4: We showed performance on different input lengths in Figure 5. As shown, IceFormer is consistently faster and achieves comparable accuracy compared to the vanilla Vicuna-7b-v1.5-16k model across all input lengths. \n\n\n**Q5: Could the authors visualize Tables 1 and 2?**\n\nA5: Thanks for the suggestion - we added visualizations to the new manuscript. Please refer to the appendix section D.\n\n**Q6: What are the limitations and potential negative impact of the proposed method?**\n\nA6: One limitation is that the value of $k$ needs to be picked manually for each Transformer to best balance efficiency and accuracy. However, this is a one-time cost paid before deploying the model, so it does not affect the performance of the method on the end-user device. \n\nA potential negative impact is that making Transformer inference faster may encourage the development of more inefficient Transformer architectures, because inference speed using such inefficient Transformer architectures may become more tolerable after applying our method. \n\n**Q7: What may be the future directions and extensions?**\n\nA7: One future direction is on developing an extension that can handle non-standard attention masks, beyond simple causal masks explored in this paper. To do so, keys must be inserted into and possibly deleted from the k-NNS database in the appropriate order, so that keys that are masked out for a query cannot be selected by that query.  \n\n**Q8: The authors mentioned that the code will be released upon acceptance. I do not find implementation in the supplementary material. Is it possible to release it for the reviewers' reference?**\n\nA8: The process for releasing the code takes some time, and is unfortunately notready by the rebuttal deadline. But we are happy to answer any questions related to the code."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742140978,
                "cdate": 1700742140978,
                "tmdate": 1700742281126,
                "mdate": 1700742281126,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jK99mN5580",
            "forum": "6RR3wU4mSZ",
            "replyto": "6RR3wU4mSZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_dxGt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_dxGt"
            ],
            "content": {
                "summary": {
                    "value": "Transformers, especially large language models (LLMs) like GPT-4, have shown great promise in handling long input sequences. However, their deployment on CPUs is challenging due to the quadratic time and space complexity of the self-attention mechanism. Existing methods to address this either require model retraining, are not generalizable, or compromise accuracy. The proposed method, IceFormer, aims to accelerate inference time without retraining, maintaining accuracy, and offering fast inference across various LLMs. It achieves this by optimizing the self-attention mechanism, leveraging the sparsity of the attention matrix, and using k-Nearest Neighbor Search (k-NNS). Experimental results on multiple benchmarks show that IceFormer efficiently outperforms other methods in both speed and accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- This method can accelerate inference time (only CPU) for pretrained transformers without the need for expensive and time-consuming retraining.\n- Unlike some other methods, IceFormer ensures that there is minimal approximation error, crucial for LLMs where errors in initial layers can cascade through subsequent ones.\n- Beyond just accuracy, the method also guarantees rapid inference times, making it particularly suitable for CPUs.\n- By capitalizing on the sparsity of the attention matrix and utilizing k-Nearest Neighbor Search (k-NNS), IceFormer effectively reduces the computational burden of the self-attention mechanism."
                },
                "weaknesses": {
                    "value": "This paper has multiple concerns for acceptance at ICLR 2024:\n\n1) The most glaring issue is its reliance on outdated methods from 2020 and 2021, with some even referencing the 2017 vanilla transformer. This dated focus suggests a lack of recent advancements in the field. One must ponder why the topic of 'efficient transformers' isn't garnering contemporary attention. Historically, efforts to enhance transformer efficiency via attention layer optimization waned with the introduction of Large Language Models (LLM). Moreover, these methods face challenges in adhering to the scaling-law (https://arxiv.org/abs/2207.10551\n).\n\n2) Although the paper touts the efficiency of LLM, it primarily showcases small-scale toy examples or, at best, the 7B LLaMa model. Notably, the latter, when applied to a long context, demands an extensive duration, upwards of several seconds, merely for the attention computation. The practical relevance of such scenarios is debatable.\n(Honestly, I'm confusing about the exact meaning of 'wall clock time of attention module'.)\n\n3) Presently, in the realm of cloud-based LLMs, MQA/GQA (https://arxiv.org/pdf/2305.13245.pdf) emerges as the leading approach for attention layer optimization. I would be interested in hearing your views on this."
                },
                "questions": {
                    "value": "included in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760911151,
            "cdate": 1698760911151,
            "tmdate": 1699636039679,
            "mdate": 1699636039679,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ivXQxNfqMw",
                "forum": "6RR3wU4mSZ",
                "replyto": "jK99mN5580",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1/N"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review. Please find our responses as follows.\n\n**Q1: The most glaring issue is its reliance on outdated methods from 2020 and 2021, with some even referencing the 2017 vanilla transformer. This dated focus suggests a lack of recent advancements in the field. One must ponder why the topic of 'efficient transformers' isn't garnering contemporary attention. Historically, efforts to enhance transformer efficiency via attention layer optimization waned with the introduction of Large Language Models (LLM). Moreover, these methods face challenges in adhering to the scaling-law (https://arxiv.org/abs/2207.10551).**\n\nA1: \n\n1. It is not true that we only compared to outdated methods. In fact, we compared to LARA, which is from  2022. To our knowledge LARA is the latest method in the literature. We only included the vanilla Transformer as a baseline because prior work did - it is not the comparison of primary interest. \n\n2. Actually efficient Transformers are garnering contemporary attention \u2013 there have been multiple survey papers on the topic in 2022 and 2023 [1,2] and leading researchers from OpenAI working on LLMs have written multiple tutorials on the topic in 2023 [3,4]. So efficient Transformers are actually quite topical and are increasingly important due to the development of LLMs.  The reason why research efforts waned with the introduction of LLMs is not due to the lack of importance, but because existing methods require changing the architecture and retraining, which is not feasible for LLMs due to the extremely high computational expense of doing so. Devising a way of improving the efficiency of attention layers without changing the architecture has historically been difficult, since it requires developing a new computational technique to perform the same attention operation used in vanilla Transformers, as opposed to devising a different attention operation. This is precisely the problem we tackle \u2013 our method does not need architectural changes or retraining and approximates the same attention operation used in vanilla Transformers without introducing extra parameters, and yet it can still achieve higher efficiency.  \n\n3. We note that our paper does not propose a new efficient Transformer architecture \u2013 instead it proposes a general-purpose technique for accelerating Transformers without changing the architecture. The referenced paper (https://arxiv.org/abs/2207.10551) studied the scaling behaviour of different Transformer architectures. It found that only switch Transformer, GLUTransformer and Funnel Transformer achieve comparable results to vanilla Transformer in terms of scaling. This in fact reinforces the need for our method \u2013 our method can be applied to speed up the vanilla Transformer without needing to change the architecture, and so the efficiency gain we achieve is *on top of* the scaling benefits offered by the vanilla Transformer, rather than *in place of* them. Our method can also be applied to speed up the switch Transformer, GLUTransformer and Funnel Transformer, since the changes they make relative to the vanilla Transformer do not affect the self-attention layer, which remains unchanged. Therefore, our method is orthogonal to particular Transformer architectures and can be combined with the Transformer architectures with good scaling behaviour to realize the benefits offered by each. \n\n**Q2: Although the paper touts the efficiency of LLM, it primarily showcases small-scale toy examples or, at best, the 7B LLaMa model. Notably, the latter, when applied to a long context, demands an extensive duration, upwards of several seconds, merely for the attention computation. The practical relevance of such scenarios is debatable. (Honestly, I'm confusing about the exact meaning of 'wall clock time of attention module'.)**\n\nA2:\n\nWe respectfully disagree with the reviewer on the characterization of the 7B LLaMa 2 model as \"small-scale\" or a \"toy example\". The 7B LLaMa 2 model is in fact considered a *large* language model (LLM) by the original authors [5] and is commonly built upon by other LLMs [6,7,8], so it is far from being a \"toy example\". \n\nIn general, it is not meaningful to speak of latency outside of the context of the latency of the original model that our method is used to accelerate. Originally the model took ~5 seconds, and our method reduced it to ~2 seconds. The original model used Intel MKL to perform the attention operation, which is a highly optimized state-of-the-art implementation of BLAS, so the fact that our method is able to achieve a speedup of ~2.5x relative to MKL is quite significant. Furthermore, our work represents just one step towards this direction \u2013 future work may build on our work to achieve further speedups. \n\n\u201cwall clock time of attention module\u201d refers to the length of time it takes in the real world to perform attention."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741716640,
                "cdate": 1700741716640,
                "tmdate": 1700742270271,
                "mdate": 1700742270271,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZiGf1tFJ3n",
                "forum": "6RR3wU4mSZ",
                "replyto": "jK99mN5580",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2/2"
                    },
                    "comment": {
                        "value": "**Q3: Presently, in the realm of cloud-based LLMs, MQA/GQA (https://arxiv.org/pdf/2305.13245.pdf) emerges as the leading approach for attention layer optimization. I would be interested in hearing your views on this.**\n\nA3: MQA/GQA is a promising approach for attention layer optimization and works by reducing the number of query and key heads.  MQA uses a single set of key and value heads for all query heads, whereas GQA utilizes a single set of key and value heads for each group of query heads. Our method is general-purpose and can be applied independently of the architecture \u2013\nin particular, it can be applied regardless of the specific number of key, value and query heads. Hence, it can be applied on top of MQA/GQA to achieve a further speedup on top of the benefits offered by MQA/GQA.As of today, MQA/GQA are still relatively new, so their effectiveness in other Transformer models have not been fully demonstrated. As they gain greater adoption, our method can be applied on top of MQA/GQA to further enhance their efficiency.\n\n**References:**\n\n[1]: Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. Efficient Transformers: A Survey. ACM Comput. Surv. 55, 6, Article 109 (June 2023), 28 pages. https://doi.org/10.1145/3530811\n\n[2]: Quentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. 2023. A Practical Survey on Faster and Lighter Transformers. ACM Comput. Surv. 55, 14s, Article 304 (December 2023), 40 pages. https://doi.org/10.1145/3586074\n\n[3:]{https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/}\n\n[4:]{https://lilianweng.github.io/posts/2023-01-10-inference-optimization/}\n\n[5]: Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023).\n\n[6:]{https://www.mosaicml.com/blog/mpt-7b}\n\n[7:]{https://www.together.ai/blog/redpajama-7b}\n\n[8:]{https://huggingface.co/tiiuae/falcon-7b}"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741774711,
                "cdate": 1700741774711,
                "tmdate": 1700741909244,
                "mdate": 1700741909244,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fPySskQyW0",
            "forum": "6RR3wU4mSZ",
            "replyto": "6RR3wU4mSZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_ad3y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_ad3y"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to address the computational challenges associated with the quadratic complexity of self-attention in long sequences. It proposes using k nearest-neighbor search as a method for approximating a sparse attention matrix, thereby reducing the computational cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-motivated, tackling a pertinent issue in the deployment of large language models.\n2. The idea of employing ranking-based algorithms over bucketing-based algorithms presents an interesting potential for complexity reduction."
                },
                "weaknesses": {
                    "value": "1. The paper does not adequately support its claim that Prioritized DCI outperforms LSH, lacking both theoretical and empirical evidence.\n2. There is insufficient clarity in the algorithm's implementation details, making it difficult to understand the actual complexity and the mechanics of the proposed method.\n3. The evaluation methodology for measuring inference time is not comprehensive. It appears the method is optimized for CPUs but lacks evidence of similar efficacy on GPUs."
                },
                "questions": {
                    "value": "1. Are the baseline results for execution time from original papers or are they reproduced results specific to CPU performance?\n2. Could you provide the typical and range of values for k and m used in the experiments?\n3. What specific settings were used for CPU inference time evaluation? Why limit the experiments to only four CPU threads? Were any BLAS kernels like MKL used in the implementation?\n\nWhile the paper is well-motivated and presents a potential approach to an important problem, there are significant issues that undermine its contributions. Specifically, the paper lacks rigorous evidence to support its claims and fails to provide a thorough methodology for evaluating its proposed solution. Further revision is needed to substantiate the claims and provide a more comprehensive understanding of the algorithm and its performance across various hardware."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1134/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1134/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1134/Reviewer_ad3y"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790243707,
            "cdate": 1698790243707,
            "tmdate": 1699636039594,
            "mdate": 1699636039594,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b8I5bucM8w",
                "forum": "6RR3wU4mSZ",
                "replyto": "fPySskQyW0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1/ N"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review. Please find our responses as follows.\n\n**Q1: The paper does not adequately support its claim that Prioritized DCI outperforms LSH, lacking both theoretical and empirical evidence.**\n\nA1: From a theoretical perspective, the query time complexity of Prioritized DCI can be found in the original Prioritized DCI paper [1], which is $O\\left(d \\max \\left(\\log n, n^{1-m / d^{\\prime}}\\right)\\right.$. The query time complexity of LSH can be found in the original LSH paper for Euclidean spaces [2], which is $\\approx O\\left(d n^{1 /(1+\\epsilon)^2}\\right)$. The epsilon denotes the maximum amount of error that is tolerable, and since it is important to tell apart similar keys that are ranked differently in our use case, epsilon needs to be very small, which makes LSH very slow.  \n\nFrom an empirical perspective, we incorporated LSH into the kNNS comparison experiment detailed in section 5.1, and consequently updated Figure 3 in the new manuscript. As shown, Prioritized DCI substantially outperforms LSH in terms of both efficiency and recall. The original Prioritized DCI paper also included an empirical comparison with LSH in its experiments section. For more details, we kindly refer the reviewer to the literature mentioned above.\n\n**Q2: There is insufficient clarity in the algorithm's implementation details, making it difficult to understand the actual complexity and the mechanics of the proposed method.**\n\nA2: We thank the reviewer for pointing this out. To improve the clarity, we have included the pseudocode for our method in the updated appendix section A in the new manuscript.\n\n**Q3: The evaluation methodology for measuring inference time is not comprehensive. It appears the method is optimized for CPUs but lacks evidence of similar efficacy on GPUs.**\n\nA3: CPU code cannot be run on the GPU and vice versa, so one cannot simply evaluate performance of a CPU implementation on the GPU without creating a separate implementation optimized specifically for GPUs. CPUs and GPUs are fundamentally different hardware platforms, with GPUs being able to execute a much larger number of parallel threads than CPUs, but also incurring a much higher communication overhead. Therefore, changes need to be made to take advantage of the GPU features and avoid the performance penalties specific to GPUs. Therefore, the optimization approaches that are applicable to one platform vs. the other are very different. On CPUs, the focus is on maximizing cache efficiency and parallelism, both at the instruction level (SIMD) and the thread level (multi-threading). In contrast, on GPUs, the focus is on effectively utilizing the large number of threads without unnecessary communication and synchronization overhead and  optimizing accesses to different levels of the memory hierarchy. As a result, performance optimization for CPUs vs. GPUs are typically treated separately in the literature, with some papers focusing specifically on CPU optimization, e.g., [3] and others focusing specifically on GPU optimization, e.g., [4]. \n\nSince our main focus is on the deployment of Transformers to end-user devices, we optimized our method for CPUs, since end-user devices are often not equipped with GPUs. While it may be possible to extend our method to GPUs, very different optimization approaches will need to be used and changes to the algorithm may be required to minimize communication overhead between thread blocks. This is beyond the scope of our paper, and we leave it to future work. \n \n**Q4: Are the baseline results for execution time from original papers or are they reproduced results specific to CPU performance?**\n\nA4: As reflected in our paper\u2019s title, i.e., \u201cAccelerated Inference with Long-Sequence Transformers on CPUs\u201d, we focus on deployment of Transformers to end-user devices equipped only with CPUs, so the baseline are the results of running the official code accompanying the original papers or widely-used HuggingFace implementations on the CPU.\n\n**Q5: Could you provide the typical and range of values for k and m used in the experiments?**\n\nA5: We list the range of values for $k$ in Table 6 in the appendix for the LRA benchmark, and in Eq. 29 under Section E of the appendix, for the LLM experiments. The sequence length $m$ of the LRA benchmark can be found in Table 4 in the appendix. The $m$ of the LLM experiments can be found in Table 3 and Figure 5."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740978944,
                "cdate": 1700740978944,
                "tmdate": 1700740978944,
                "mdate": 1700740978944,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0QwvTp50S5",
            "forum": "6RR3wU4mSZ",
            "replyto": "6RR3wU4mSZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_5Bmg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1134/Reviewer_5Bmg"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present IceFormer, a novel method for improving the inference efficiency of pretrained Transformers on the CPU without requiring re-training. Leveraging the insight that k most important keys can be identified by performing k-NNS on the key\nembeddings using the query embedding, IceFormer employs the Prioritized DCI algorithm to identify the top-k keys. The evaluations on three benchmarks illustrate the effectiveness of this approach in reducing the quadratic time and space complexity of vanilla Transformers with competitive performance compared to other efficient-attention variants."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper sheds new insights on an important problem -- choosing the top-k keys in sparse attention for inference acceleration. The authors show that the exact kNNS algorithm is critical for the success of sparse attention. \n2. The paper is well-written and easy to follow.\n3. The evaluation section shows strong performance compared to other efficient attention works."
                },
                "weaknesses": {
                    "value": "1. The use of Prioritized DCI k-NNS algorithm needs more experimental or theoretical justification. The authors claim \"ranking-based algorithms is better aligned with how attention weights\", if so, how would other ranking-based algorithms perform? On top of that, the authors show an evaluation of different kNNS algorithms on fashion-mnist-784 dataset in Section 5.1. It would be better to show the exact setup (eg. model architecture) they used, and compare them on a few more tasks (for instance, text generation, token classification). \n2. Fig.5's y-axis needs to be annotated with units (especially for latency)."
                },
                "questions": {
                    "value": "1. Why is CPU the target platform for IceFormer? Are there CPU-specific architectural optimizations, on top of the time/space saved due to sparse attention? \n2. I hope to clarify with the authors: in the long-context evaluation (Fig.5), is the baseline (vanilla Transformer) referring to the Vicuna-7b-v1.5-16k model, or the original Transformer model? \n3. This paper describes a novel method for improving the inference efficiency and evaluates on CPU. But it seems that this method could potentially also apply to GPU, which is more often used. Are there specific reasons/constraints to use CPU?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831202939,
            "cdate": 1698831202939,
            "tmdate": 1699636039461,
            "mdate": 1699636039461,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "alHq9mQEcO",
                "forum": "6RR3wU4mSZ",
                "replyto": "0QwvTp50S5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1134/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable review. Please find our responses as follows.\n\n**Q1: The use of Prioritized DCI k-NNS algorithm needs more experimental or theoretical justification. The authors claim \"ranking-based algorithms is better aligned with how attention weights\", if so, how would other ranking-based algorithms perform?**\n\nA1: There have only been two ranking-based algorithms, vanilla DCI [1] and Prioritized DCI [2]. To respond to the reviewer\u2019s question, we have incorporated both algorithms into the k-NNS algorithm comparison  detailed in Section 5.1, and updated Figure 3 accordingly. As shown in the figure, Prioritized DCI is significantly more efficient than vanilla DCI.\n\n**Q2: On top of that, the authors show an evaluation of different kNNS algorithms on fashion-mnist-784 dataset in Section 5.1. It would be better to show the exact setup (eg. model architecture) they used, and compare them on a few more tasks (for instance, text generation, token classification).**\n\nA2: For the evaluation of different k-NNS algorithms, we followed the evaluation protocol of the ANN benchmarks (https://github.com/erikbern/ann-benchmarks). More details can be found on the website of ANN benchmarks. ANN benchmarks is a general benchmark for k-NNS algorithms, and is not specific to a particular model architecture. It also includes the references for each k-NNS algorithm tested.\n\nAs per the reviewer\u2019s request, we compare the second best k-NNS algorithm as measured by ANN benchmarks in Figure 3, on two tasks, the text classification task in the LRA benchmark and the GvRp (text generation) task in the ZeroSCROLLS. The second best k-NNS algorithm is the hnsw(nmslib) algorithm. The latency for hnsw(nmslib) is 1.06 seconds on the text classification task and 15.76 seconds on the GvRp task, significantly slower than the vanilla self-attention (0.69 seconds and 5.39 seconds) and Iceformer (0.09 seconds, and 2.24 seconds). This is because evaluating each attention layer requires constructing a new k-NNS database, which takes a long time forhnsw(nmslib) and many other k-NNS algorithms. On the other hand, constructing a new k-NNS database is much faster with Prioritized DCI. This result is consistent with our k-NNS comparison results in Figure 3 and the literature [3, 4].\n\n**Q3: Fig.5's y-axis needs to be annotated with units (especially for latency).**\n\nA3: We followed the reviewer\u2019s suggestion and added the units for latencyin Figure 5 of the updated manuscript.\n\n**Q4: Why is CPU the target platform for IceFormer? Are there CPU-specific architectural optimizations, on top of the time/space saved due to sparse attention?**\n\nA4: Since our main focus is on the deployment of Transformers to end-user devices, we optimized our method for CPUs, since end-user devices are often not equipped with GPUs. We used various CPU-specific optimizations to minimize the wall-clock time, namely instruction-level parallelism (SIMD), bitwise operations and cache-optimized data structures. \n\n**Q5: I hope to clarify with the authors: in the long-context evaluation (Fig.5), is the baseline (vanilla Transformer) referring to the Vicuna-7b-v1.5-16k model, or the original Transformer model?**\n\nA5: We thank the reviewer for pointing this out. The baseline (vanilla Transformer) refers to the vanilla Vicuna-7b-v1.5-16k model. We have fixed the labels in Table 3 and Figure 5.\n\n**Q6: This paper describes a novel method for improving the inference efficiency and evaluates on CPU. But it seems that this method could potentially also apply to GPU, which is more often used. Are there specific reasons/constraints to use CPU?**\n\nA6: While there is certainly potential to extend our method to GPUs, very different optimization approaches will need to be used and changes to the algorithm may be required to minimize communication overhead between different thread blocks. This is beyond the scope of our paper, and we leave it to future work. As mentioned earlier, our implementation uses CPU-specific optimizations, such as instruction-level parallelism (SIMD), bitwise operations and cache-optimized data structures. Different optimizations will need to be used to minimize the wall-clock time on GPUs. \n\n**References:**\n\n[1]: Li, Ke, and Jitendra Malik. \"Fast k-nearest neighbour search via dynamic continuous indexing.\" International conference on machine learning. PMLR, 2016.\n\n[2]: Li, Ke, and Jitendra Malik. \"Fast k-nearest neighbour search via prioritized DCI.\" International conference on machine learning. PMLR, 2017.\n\n[3]: Echihabi, Karima, et al. \"The lernaean hydra of data series similarity search: An experimental evaluation of the state of the art.\" arXiv preprint arXiv:2006.11454 (2020).\n\n[4]: Echihabi, Karima, et al. \"Return of the lernaean hydra: Experimental evaluation of data series approximate similarity search.\" arXiv preprint arXiv:2006.11459 (2020)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1134/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741528874,
                "cdate": 1700741528874,
                "tmdate": 1700742305612,
                "mdate": 1700742305612,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]