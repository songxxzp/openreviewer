[
    {
        "title": "Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable"
    },
    {
        "review": {
            "id": "MxxkMkOa0q",
            "forum": "pEGSdJu52I",
            "replyto": "pEGSdJu52I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8145/Reviewer_Nr6n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8145/Reviewer_Nr6n"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors try to address the following important question: are marginal improvements in test accuracy (after a certain point) on benchmark datasets actually indicative of better models (with respect to the underlying data distribution)? Towards this end, the authors conduct a series of large-scale experiments on CIFAR-10 and ImageNet which show that trained model performance becomes uncorrelated on disjoint splits of test data. They also connect this variance in model performance with the notion of calibration of ensembles of models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Originality:** The experimental setups in this paper, as well as the connection introduced between model variance and calibration, are to the best of my knowledge new and contain several interesting ideas.\n2. **Quality:** The authors have run extensive experiments to verify their hypotheses (at least within the context of the benchmarks they restrict themselves to), and their theoretical predictions track the experiments quite closely (for the appropriate regimes). \n3. **Clarity:** Overall the paper is very easy to read, and justification/sufficient detail are provided for the experiments conducted in the paper.\n4. **Significance:** This paper studies the important problem of how to assess a trained model's future test performance, and introduces useful formalisms for thinking about this problem. The main drawback here is that the paper's experiments and hypotheses are restricted to two image classification benchmarks (which are widespread - this is still a useful contribution), so it is tricky to assess how well the observations will generalize."
                },
                "weaknesses": {
                    "value": "## Main Weaknesses\n1. **Generalization of takeaways.** The practical takeaway provided in the paper in Section 3 is that marginal test performance improvements for well-trained models on CIFAR-10/ImageNet can be uncorrelated with performance on the underlying data distribution. I have some concerns with this claim (i.e. fixed test splits vs resampling), but even assuming this to be true - what can be said for more general machine learning (or even image classification) tasks? For example, in the experiments in Section 4, as the authors note there is a clear correlation between BERT-Large validation performance and test performance. I am not really sure how this should be interpreted in the context of the paper; while the analysis in the paper shows empirically that BERT-Large fine-tuning has more variance than BERT-Base, I don't see how the analysis in the paper would allow one to predict (without running many experiments) whether val performance will be correlated with test performance or not.\n2. **Hyperparameter choices/sensitivity for results.** Of course it is not possible to be entirely comprehensive with respect to hyperparameters, but I have concerns with some choices made by the authors. Particularly, the training horizons considered are 0, 4, 16, and 64 epochs. We see in the experimental results a clear trend of decreasing variance as the training horizons are extended - do any of the results still hold for longer training horizons? I would anticipate that the independence of network predictions surely shrinks as training horizons are extended. \n\n## Minor Comments\n- In the proof of Lemma 1, the exponent should be inside the expectation. Additionally, it's probably helpful to justify the penultimate step (interchange of expectation) by saying that Fubini's Theorem applies.\n- It would be better to include a definition of ECE in Section 3.4, or at least verbally describe it in the context of Hypothesis 2.\n\n## Overall Recommendation\nMy concerns with the paper are mostly with the actual significance of the practical recommendations in the paper (as discussed above). That being said, I think the paper contains sufficiently many interesting ideas and experiments to possibly warrant acceptance (I am borderline because again, I'm not sure how well these ideas generalize). Thus, I recommend **weak accept**."
                },
                "questions": {
                    "value": "- Is the test set split fixed? In other words, when the authors write \"we split the test-set into two halves of 5000 examples each\", does this mean there is a single fixed split of the test set? If so, this seems to not make sense in the context of the proposed formalism ($S \\sim \\mathcal{D}^n$), which considers randomly sampled test-sets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8145/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8145/Reviewer_Nr6n"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698166489238,
            "cdate": 1698166489238,
            "tmdate": 1699637009591,
            "mdate": 1699637009591,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iUNoCNwnf1",
                "forum": "pEGSdJu52I",
                "replyto": "MxxkMkOa0q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Nr6n"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable comments, which give us an opportunity to clarify the generality of our results. We also thank the reviewer for noting the originality of our experiments and the significance of the subject being studied. We hope that these clarifications will satisfactorily address the reviewer\u2019s concerns.\n\nSummary:\n* The result does generalize to longer training durations. In particular, Figure 5 shows that it generalizes to durations of 128 and 256 epochs, where the distribution-wise variance continues to be small.\n* The reviewer mentioned a concern that the takeaway did not generalize to finetuning BERT-Large, which we analyzed in Section 4.1. We comment that we included BERT-Large because it is a well-recognized case of pathological training instability [1, 2], so it is not surprising or problematic that the takeaway does not generalize to it.\n* Our results thus allow one to predict that for standard (not pathologically unstable) trainings which run to convergence, there will be little correlation between the performance on two splits of test data.\n\n\n**Clarification of the main experiment, regarding the fixed test-set split**\n\nThe reviewer asked whether our test-set split is fixed. Yes, in Figure 2 we use a single fixed split of the test-set, into two halves.\n\nTo say why this is the case, we\u2019ll first give a summary of the main experiment, which proceeds as follows: \n\nWe first generated a fixed split of the CIFAR-10 test-set into two halves of 5,000 examples each. Then we trained 60,000 models on the CIFAR-10 training set, and evaluated them all on both test splits.\n\nWe found that the best-performing model on split 1 reached significantly higher performance than average - almost 95%, vs. an average of 94.2%. But this best model on split 1 didn\u2019t do any better than average on split 2. In fact, we measured that the correlation between performance on the two splits is close to zero.\n\nResampling the split does not change this result - the resulting correlation will always be near the same value.\n\nWe also note that the mathematical analysis which proceeds in section 3.3 does not depend on using splits at all. Instead, the estimator given in Theorem 2 directly computes the average pairwise correlation.\n\nWe hope that this clarifies how and why splits are used in our analysis, and shows that using a fixed split is not a problem.\n\n\n**Improvements to the presentation of the proof of Lemma 1**\n\nWe have updated the draft to add extra parentheses inside of the expectation brackets, such that the square is moved inside. We also noted as suggested that Fubini\u2019s theorem is what allows the interchange of expectations.\n\n\n**Generalization to longer training durations**\n\nThe reviewer mentioned a concern that our results do not generalize to even longer durations, suggesting that perhaps when the training horizon is extended even further, the test-set variance may significantly continue to decrease, rather than plateauing as we have suggested.\n\nWe would like to respond that both Figure 3 (right) and Figure 5 do present the results for training horizons all the way to 256 epochs, where it can be seen that the test-set accuracy does indeed plateau. From 64 to 256 epochs (which is roughly after the point of convergence, i.e. accuracy stops going up), we observe that test-set variance plateaus and examplewise predictions vary approximately independently between runs of training.\n\n\n**Generalization of takeaways**\n\nThe reviewer mentioned that they have some concerns with our main claim, which is that for well-trained models on CIFAR-10/ImageNet, there is significant variance between runs in terms of performance on the test-set, but almost zero variance on the test-distribution.\n\nWe hope that we have addressed the question about fixed test splits, and demonstrated that the claim does hold for standard CIFAR-10 and ImageNet trainings.\n\nThe reviewer mentioned that their concern also stems from our study of BERT-Large, where indeed the claim does not hold, as there is a clear correlation between two splits of data.\n\nThis is because BERT-Large is a case of pathological training instability, as has been studied in prior works [1, 2]. Any intervention which creates training instability will invalidate our claim by creating massive variance with respect to both the test-set and test-distribution. For example, our claim also would not generalize to trainings which use a huge learning rate, or a crazy network initialization which causes instability.\n\nInstead, our claim applies to standard trainings, which we directly confirmed for CIFAR-10 and ImageNet.\n\nWe hope that this clarifies the generality of our takeaways."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564090312,
                "cdate": 1700564090312,
                "tmdate": 1700564090312,
                "mdate": 1700564090312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kCuFgazDmR",
                "forum": "pEGSdJu52I",
                "replyto": "iUNoCNwnf1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8145/Reviewer_Nr6n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8145/Reviewer_Nr6n"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the detailed response; my apologies for missing that Figures 3 and 5 covered longer training horizons. I have no further questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601151906,
                "cdate": 1700601151906,
                "tmdate": 1700601151906,
                "mdate": 1700601151906,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hokD8Nb5mR",
            "forum": "pEGSdJu52I",
            "replyto": "pEGSdJu52I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8145/Reviewer_yu6g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8145/Reviewer_yu6g"
            ],
            "content": {
                "summary": {
                    "value": "The authors analyze the variance between trained networks on the test set and find that minor variations are insignificant and explained by finite sample noise due to a finite test set. Further, they show that when considering the test distribution, the variations in validation set performance are not well correlated with the test distribution performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The work formalizes what I think many people would ultimately suspect in that the minor variations between runs are insignificant when looked at through the larger perspective of the test distribution. Even though I think this would be expected, it is nice to see a work which empirically verifies it. \n\n- The derived bounds are useful for estimating the expected variance between runs."
                },
                "weaknesses": {
                    "value": "- The two main itemized contributions contain way too many points to be in a list. They are paragraph sized. I do not think something of that size should be presented in a list. I believe they either need to be broken up into smaller components, or discussed in plain text instead of a list. \n\n- What is $n$ in theorem 3, 4, and 5? I suppose it should be the size of the dataset, but it would be nice to include this right by the theorems in order to avoid any possible confusion. \n\n- I think it would be interesting to see the difference in Theorem 3 when varying the number of models in the ensemble. For instance, the bias in the estimate may be small, and a two model ensemble may give good results, or it may actually require quite a large ensemble in order to be close to correct. Can you add this experiment? I believe it can be done by only randomly choosing and expanding the ensemble of the models which are already trained.\n\n- Many of the claims listed as main contributions are in the appendix. Even in the conclusion, the things in the appendix as main contributions which have been demonstrated even though there has been nothing said about them up until this point in the text. I do not think this is fair to the reader and it should be reorganized or rewritten such that this does not happen. \n\n- In section 3.4 it says: \u201cThat is, if we let S \u2032 be the subset of test images which are classified by 30-40% of independently trained networks as \u201cdog\u201d, then approximately 30-40% of the images in S \u2032 really will be dogs.\u201d I do not think that is how calibration is phrased in most calibration works. In fact, all networks may correctly classify the image as a dog (meaning that the highest predicted probability is the dog class), but the predicted probability (confidence) should equal the empirical distribution of the positive class in $S^\\prime$, then the model is calibrated."
                },
                "questions": {
                    "value": "- In figure 2, why is (odd examples) in parentheses? I cannot figure out what this means since there are only two splits and they seem to be uniformly random.\n\n---\n\nUltimately, I like the findings of this work, but I find part of the presentation problematic as noted in the weaknesses section above. If these things can be fixed, I would probably raise my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698483788193,
            "cdate": 1698483788193,
            "tmdate": 1699637009469,
            "mdate": 1699637009469,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2SLQfjQfIq",
                "forum": "pEGSdJu52I",
                "replyto": "hokD8Nb5mR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer yu6g"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback, which has prompted us to improve the presentation of our results. We also thank the reviewer for noting that our derived bounds are useful for estimating the variance between runs, and prompting us (alongside reviewer Wzaj) to derive an additional new theorem with practical implications. We hope that these corrections and additions shall satisfactorily address the reviewer\u2019s concerns, in which case we hope that the reviewer will raise their score.\n\n**Improvements to the presentation**\n\nWe thank the reviewer for pointing out several problems with the presentation, which prompted us to make the following improvements to the draft:\n\n* We have converted the list of contributions in the introduction into two paragraphs.\n* We have clarified in the statements of Theorem 3-5 that $n$ refers to the size of the test-set.\n* Regarding the claims listed in the conclusion (regarding connections between variance and other phenomena), we have made it more explicit that two of them are presented only in the appendix. First, in the beginning of Section 4 we refer to the two additional experiments in the appendix. Second, in the conclusion we have added explicit pointers to the section that each claim is presented in. We regret that space issues precluded them being put directly in the main text.\n\n**Regarding the caption of odd vs even examples in Figure 2**\n\nWe have added the following sentence to section 3.1, which describes the reason for this: \u201cCIFAR-10 is already shuffled, so for convenience we simply use the odd and even-indexed examples as the two halves.\u201d We have confirmed that if we use a newly selected random split instead, it makes no difference for the results.\n\n**Regarding the definition of calibration**\n\nWe agree with the reviewer that our notion of calibration, called \u201censemble-calibration\u201d and referencing [1], is different from the standard version of calibration defined for single models.\n\nIndeed, the standard single-model calibration described typically *does not hold*, as neural networks tend to be overconfident [2]. On the other hand, the ensemble-calibration we describe *does* approximately hold, as observed in [1] and implicitly in [3].\n\nThis is good because ensemble-calibration is mathematically what is needed to prove Theorem 3-5, whereas standard calibration is not useful.\n\nWe hope this has clarified our usage of the notion of calibration.\n\n\n**Applying Theorem 4 to ensembles of two models**\n\nIn the paper, we focused on the variance in performance between models that are members of large ensembles of >1,000 independently trained networks. The reviewer made the interesting suggestion that we should look at smaller ensembles of two networks.\n\nWe found this suggestion interesting and have added a new theorem to the paper which derives the expected difference in accuracy between two models.\n\nThis can be found as Theorem 6 in the appendix. We prove that for binary classification, the expected squared difference between the test-set accuracies of two independently trained models is equal to the average error divided by the size of the test set.\n\nWe proved this under the assumptions of theorem 4, which are shown to approximately hold by the main experiments of the paper.\n\nWe hope that this theorem will provide practitioners with a guideline for how much variance to expect to see between pairs of trained models. In particular, if two models are trained with different hyperparameters, and their accuracy differs by significantly more than the prediction of Theorem 6, then there may be a genuine difference between the quality of the two hyperparameters. Otherwise, there may be no statistically significant difference.\n\n[1] Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. \"Simple and scalable predictive uncertainty estimation using deep ensembles.\" Advances in neural information processing systems 30 (2017).\n\n[2] Guo, Chuan, et al. \"On calibration of modern neural networks.\" International conference on machine learning. PMLR, 2017.\n\n[3] Nakkiran, Preetum, and Yamini Bansal. \"Distributional generalization: A new kind of generalization.\" arXiv preprint arXiv:2009.08092 (2020)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564017705,
                "cdate": 1700564017705,
                "tmdate": 1700607719766,
                "mdate": 1700607719766,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o4nsWDCiBa",
                "forum": "pEGSdJu52I",
                "replyto": "hokD8Nb5mR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8145/Reviewer_yu6g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8145/Reviewer_yu6g"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response.\n\n---\n\n> The reviewer made the interesting suggestion that we should look at smaller ensembles of two networks.\n\nThis is not what I suggested. As stated in the original review...\n\n>I think it would be interesting to see the difference in Theorem 3 when **varying the number of models in the ensemble**. For instance, the bias in the estimate may be small, and a two model ensemble may give good results, **or it may actually require quite a large ensemble** in order to be close to correct.\n\nI suggested to *vary* the number of models in the ensemble to analyze the bias/variance tradeoff of ensemble size.\n\n---\n\n> We agree with the reviewer that our notion of calibration, called \u201censemble-calibration\u201d and referencing [1], is different from the standard version of calibration defined for single models.\n\nI do not think the paper referenced has a 'different from the standard' view of calibration. The paper referenced creates an equally weighted mixture of ensemble outputs\n\n > **[from section 2.4 of [1]]**: For classification, this corresponds to averaging the predicted probabilities.\n\nTherefore calibration is measured by the average of the predicted probabilities and not as you described in the paper.\n\n---"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718268432,
                "cdate": 1700718268432,
                "tmdate": 1700718287140,
                "mdate": 1700718287140,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KSQE9pDuzV",
                "forum": "pEGSdJu52I",
                "replyto": "hokD8Nb5mR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer yu6g"
                    },
                    "comment": {
                        "value": "We have updated the draft (in section 3.4) in order to correctly reference the notion of calibration that was previously explored in Lakshminarayanan et al. 2017.\n\nWe thank the reviewer for pointing out this issue, and regret not investigating it more closely before our first reply.\n\nWe believe that all of the presentational issues brought up by the reviewer have now been addressed. We thank the reviewer for helping us improve the presentation of the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727939824,
                "cdate": 1700727939824,
                "tmdate": 1700728022584,
                "mdate": 1700728022584,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UtU1bI9uwN",
            "forum": "pEGSdJu52I",
            "replyto": "pEGSdJu52I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8145/Reviewer_Wzaj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8145/Reviewer_Wzaj"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the variance observed in test-set performance across repeated runs of neural network training, a phenomenon that has raised concerns about hyperparameter comparison and training reproducibility. Key findings of the study are:\n\n- Although there's significant variance in test-set performance, the variation on the underlying test-distributions (like CIFAR-10 and ImageNet) is minimal. This suggests that in practical applications, the variance might not be as concerning as assumed.\n\n- The study puts forth an \"examplewise independence\" hypothesis, suggesting that a network's correct prediction for a given test example is akin to a biased coin flip and is independent of its predictions on other examples.\n\n- The paper states that prior works have noted that predictions from ensembles of networks, trained repeatedly, are typically calibrated. They argue that this calibration inevitably leads to some variance in test-set accuracy. For binary classifications, they provide a formula to determine this variance.\n\n- Other observations include the reduction in distribution-wise variance with longer training durations, the correlation between optimal learning rate and absence of excess distribution-wise variance, the effect of data augmentation in reducing variance, and the increase in variance when test-sets differ from the training set."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper covers an interesting topic: the variation of test accuracy over random seeds (stochasticity of deep learning experiments). It tells the test accuracy on the whole test set from the test accuracy on the test distribution (random subsets of test sets). An important conclusion is that accuracy gain in the whole test set from random seeds does not generalize to the whole test distribution. Another interesting conclusion is that the result of trained model on each example can be approximated by binomial distribution with biased flip probability.\n\nOverall, I find this paper interesting and novel. The paper is easy to understand."
                },
                "weaknesses": {
                    "value": "Althouth I enjoy reading this paper, the analysis is mainly posterior: we can only get these insights after running the same algorithm for hundreds of runs. As far as I can see, the insights from this paper can tell us if a training algorithm is good despite the impact of random seed, but it cannot tell if a trained model is better than another despite the impact of random seed. If this paper can further achieve the latter application, I would be more than happy to raise my score.\n\nAnother concern is how well does the conclusion itself generalize to other settings. In Figure 1, it seems the variance becomes smaller as training goes on. But I think it is caused by the learning rate schedule: the authors say that they \"always linearly ramp the learning rate down to zero by the end of training\", so it is expected that the accuracy has less variation in the end."
                },
                "questions": {
                    "value": "- How does the conclusion generalize to other training setting or other network architecture (like Transformers)?\n\n- How can the insights of random seeds help practical development? E.g. if they can be used to tell the quality of individual model, to separate the accuracy into random part and true accuracy."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698590795848,
            "cdate": 1698590795848,
            "tmdate": 1699637009340,
            "mdate": 1699637009340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ww6fgNb53u",
                "forum": "pEGSdJu52I",
                "replyto": "UtU1bI9uwN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to reviewer Wzaj"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback, and for noting that our paper is interesting and novel along with being easy to understand. The reviewer's feedback that the paper was lacking in practically useful results motivated us to make the addition of a new theorem which can be used as an applied tool. We hope that our updated draft and response will satisfactorily address the reviewer\u2019s concerns, in which case we hope that the reviewer will raise their score.\n\nResponse summary:\n* To improve the practical applicability of our results, we have added a new theorem (Theorem 6 in Section C of the updated draft) which predicts the expected squared difference in accuracy between two runs of training. For example, this can be used as a tool to check whether a pair of hyperparameters yields significantly differing performance.\n* Regarding generalization to other settings, we clarify that even for our short trainings we anneal the learning rate to zero. That is, for our four-epoch trainings, we ramp the learning rate to zero by the end of the fourth epoch. We also have preliminary evidence that our results extend to an NLP/transformer setting.\n\n**Practical application of our analysis**\n\nThe reviewer expressed a concern that although our main analysis yields important conclusions, it is mainly posterior, the only implication for practice being that the practitioner should beware that variation on the test-set likely will not translate to real performance gains on the test-distribution.\n\nHowever we submit that Theorem 4, and the newly added variant which is Theorem 6, provide a significant practical tool. In Theorem 4, we prove that under certain assumptions which we find to hold empirically, it is the case that for binary classification we can expect the variance between runs to be precisely the average error divided by twice the number of test examples. This prediction was found to be empirically quite accurate in Figure 6.\n\nAs a corollary, the expected squared difference between the performance of two trained models is twice this, i.e., the average error divided by the number of test examples (Theorem 6). We suggest that this can be used as a tool to test the significance of hyperparameter ablations: Suppose we train two models with hyperparameters A and B and measure their squared difference in performance. Then if this value is less than the expectation given by Theorem 6, then there may be no statistically significant difference between A and B. On the other hand, if it is much more, then this may suggest a genuine difference in hyperparameter quality.\n\nWe acknowledge that it remains an open question how to extend this result to k-way classification beyond k=2, and how to derive precise concentration inequalities giving statistical significance values, rather than the simple predicted expectation given by Theorem 6. But we hope that the theorem resolves the reviewer\u2019s main practical concern, given that it already constitutes a significant practical application of our analysis.\n\n\n**Clarification of the learning rate schedule**\n\n> In Figure 1, it seems the variance becomes smaller as training goes on. But I think it is caused by the learning rate schedule: the authors say that they \"always linearly ramp the learning rate down to zero by the end of training\", so it is expected that the accuracy has less variation in the end.\n\nWe have updated the draft (in section 2) to clarify that this means that, even for the four-epoch duration, we ramp the learning rate down to zero by the end of the four epochs. Thus, none of our trainings, no matter how short, end with a nonzero learning rate. We hope that we have correctly understood the reviewer\u2019s concern.\n\nWe note that if the learning rate were not ramped all the way down, then indeed the variation would be greatly increased for the shorter trainings. But this isn\u2019t what we did.\n\n\n**Generalization to transformers**\n\nIn Section 4.1, we confirm that for a canonical transformer-encoder finetuning scenario (BERT-base finetuned on MRPC), our results do empirically generalize, with test-distribution variance being small relative to test-set variance. For BERT-Large finetuning our results do not generalize, but this seems to be explained by this being a widely recognized case of pathological training instability [1, 2].\n\nThus, we have at least preliminary evidence that our results generalize to this quite different other setting. We agree with the reviewer that, for example, it is an interesting direction for future work to extend our study to transformers trained from scratch. (e.g. LLM pretrainings)\n\n\n[1] Mosbach, Marius, Maksym Andriushchenko, and Dietrich Klakow. \"On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines.\" arXiv preprint arXiv:2006.04884 (2020).\n\n[2] Dodge, Jesse, et al. \"Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.\" arXiv preprint arXiv:2002.06305 (2020)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563914827,
                "cdate": 1700563914827,
                "tmdate": 1700607789637,
                "mdate": 1700607789637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O2pLU9bWT0",
                "forum": "pEGSdJu52I",
                "replyto": "Ww6fgNb53u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8145/Reviewer_Wzaj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8145/Reviewer_Wzaj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. The extension of Theorem 6 is too restricted, and the generalization results are not very good. I will keep my initial score.\n\nI don't understand the clarification of the learning rate schedule. My thoughts are that, the observed behavior might be correlated with learning rate schedule."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645364314,
                "cdate": 1700645364314,
                "tmdate": 1700645364314,
                "mdate": 1700645364314,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U04r5lNNFV",
            "forum": "pEGSdJu52I",
            "replyto": "pEGSdJu52I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8145/Reviewer_G5ah"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8145/Reviewer_G5ah"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows that the variance for true test distribution among multiple training runs is actually smaller than those observed from the test dataset. They also proposed a statistical assumption and derived an estimator to estimate the stand deviation of the test distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is easy to follow. The paper provides many empirical analysis to support their points and also include some theoretical analysis."
                },
                "weaknesses": {
                    "value": "Since most parts of the paper are based on the empirical analysis, it might be better the have the experimental setup in the main paper including all the hyperparameters and all the models in those 60000. However, it might be important to include results for different architectures since some of the models may have better reproductions. It might be interesting to have more use cases for getting the variances of the true test distribution."
                },
                "questions": {
                    "value": "Have we tried different architectures? Do they have similar results? I am curious do we really need to get the true variance? For example, if for all the models, we have a similar trend between variance from test-set and test-distribution, then why do we need to get that?\n\nDo we have some cases that the variance among the test distribution gives us new insight? For example, does flatness of the minimizer related with this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803685695,
            "cdate": 1698803685695,
            "tmdate": 1699637009153,
            "mdate": 1699637009153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aLxnM5jTED",
                "forum": "pEGSdJu52I",
                "replyto": "U04r5lNNFV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer G5ah"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback, and for noting that our paper is easy to follow. We provide the following response, which we hope will satisfactorily address the reviewer\u2019s concerns, in which case we hope that the reviewer will raise their score.\n\nRegarding the concerns raised, we have the following summary:\n\nFirst we confirm that there is **no simple trend relating test-set and test-distribution variance**, making our in-depth analysis necessary.\nWe are also happy to confirm that the other datasets and architectures that we did study in the paper do have similar results.\nFinally, we are happy to report that our analysis does yield new insights relating variance to other phenomena in deep learning: for example, section 4.2 studies the relationship to data augmentation and section D.1 studies the relationship to the learning rate. We agree that the flatness of minimizer is an interesting future direction.\n\n### Hyperparameter details\n\nWe thank the reviewer for pointing out that it would be better to have full hyperparameters in the main text, and have therefore added full detail for our 240,000 total runs of training on CIFAR-10 into Section 2 (Setup).\n\n### On whether there exists simple trend connecting test-set and test-distribution variance\n\nThe reviewer expressed a concern regarding the possibility that the relationship between variance on the test-distribution and the test-set may follow a simple trend, making it less interesting to study.\n\nWe can confirm that no such trend exists, using the following evidence:\n\nAs our first example, we compare the 16-epoch and 64-epoch training durations on CIFAR-10. These have similar test-set variance, but very different test-distribution variance.\n\nIn particular, as shown in Figure 5, the 16-epoch training duration has an observed test-set std of 0.187%, which is 1.28x the observed test-set std of the 64-epoch trainings, which is 0.146%. So in terms of *test-set* variance, these two training configurations appear similar. But their estimated variance on the *test-distribution* is significantly different - the 16 epoch duration has 0.09%, while the 64-epoch duration has almost 3x less true variance, having 0.034%.\n\nSo the trend is certainly not a linear one. It is not even monotonic either. For another example, training on 80% less training data for 64 epochs (figure 7) causes more test-set variance than training on full data for 16 epochs (figure 3) \u2013 but the latter causes more distribution-wise variance.\n\n### New insights into connection between variance and other phenomena\n\nThe reviewer asked whether studying the test-distribution variance can give any new insights, for example possibly connecting with flatness of minimizer. We agree that flatness of minimizer is an interesting future direction of study.\n\nWe point out that our paper contains the following **connections between variance and other phenomena in deep learning**, which may be of interest:\n* The inclusion of data-augmentation is crucial for minimizing distribution-wise variance, moreso than one would expect given only its impact on error rate. (Section 3.2)\n* The optimal learning rate is the largest one that does not induce excess distribution-wise variance. (Appendix D.1 - unfortunately we couldn\u2019t include this in the main text due to space constraints)\n\n### Other architectures\n\nRegarding studying variance between runs of training using other neural network architectures, we point out that we studied finetuning the BERT (transformer-encoder) architecture in Section 4.1. We additionally replicated our results for ImageNet using a larger ResNet in Appendix D.2."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563475147,
                "cdate": 1700563475147,
                "tmdate": 1700607739699,
                "mdate": 1700607739699,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]