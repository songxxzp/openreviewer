[
    {
        "title": "Two-Stage Diffusion Models: Better Image Synthesis by Explicitly Modeling Semantics"
    },
    {
        "review": {
            "id": "N41CeAVXC3",
            "forum": "fH2wf2w2Ss",
            "replyto": "fH2wf2w2Ss",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7930/Reviewer_c6YU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7930/Reviewer_c6YU"
            ],
            "content": {
                "summary": {
                    "value": "The paper revisits the unCLIP paradigm proposed by Ramesh et al., 2022, which consists of two cascaded diffusion models, one trained on CLIP latent text embeddings and another one mapping from latent text embeddings to the image space. Unlike unCLIP, the paper proposes to train the latent diffusion models on CLIP image embeddings rather than text embeddings, which enables unconditional image generation. The proposed model is evaluated on different variants of AFHQ, FFHQ, and ImageNet, and is compared to EDM (Karras et al. 2022) among other baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Improving unconditional image generation models is an active research area, and lags substantially behind class/text conditional generation. Making progress in this area is important. Further, techniques relying on multi-stage modeling/latent modeling like the proposed one have proven effective in making diffusion models more efficient."
                },
                "weaknesses": {
                    "value": "I see two main weaknesses: \n1. The lack of novelty. The proposed method is very similar to unCLIP. \n2. The method is arguably more complicated than (Hu et al., 2022) which simply clusters image embeddings obtained by a self-supervised representation and uses the cluster indices as conditioning signal. While the paper compares to this approach on AFHQ/FFHQ, I\u2019m not fully convinced that the proposed method is superior. I would expect a comparison on ImageNet to be convinced that the additional complexity of the proposed approach is justified, since Hu et al. get similar improvements.\n\nGiven these two points, I\u2019m leaning towards rejecting this paper.\n\n\nMinor points:\n- Typo page 2 bottom \u201cthat the all images\u201d\n- I found the terms lightly/strongly conditional somewhat confusing. Maybe it would be simpler to just use class/text conditional?"
                },
                "questions": {
                    "value": "- Do the authors have any explanation why the 2SDM outperforms 2SDM with oracle in Figure 5 right?\n- Did the authors consider any other image embeddings besides CLIP? For example DINO might be better aligned with ImageNet. Also it would be interesting to see how well the first diffusion model can learn the embedding, and how this affects the quality of the end-to-end model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781439988,
            "cdate": 1698781439988,
            "tmdate": 1699636974050,
            "mdate": 1699636974050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kuC3VCEk99",
                "forum": "fH2wf2w2Ss",
                "replyto": "N41CeAVXC3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7930/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and very helpful comments. We will withdraw this submission, but will also reply to your comments here for posterity.\n\n*Comparison to Hu et al. on ImageNet., and comparison with other image embeddings* - Thank you for these suggestions! We had not tried using other embeddings like DINO but expect that they would work well based on the results in [Self-Guided Diffusion Models](https://arxiv.org/abs/2210.06462). Unfortunately we (due to a bug in our codebase introduced while running new experiments) have not been able to present them by the end of the discussion period.\n\n*Typo* - Fixed, thanks for catching this!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709473316,
                "cdate": 1700709473316,
                "tmdate": 1700709473316,
                "mdate": 1700709473316,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xyrBIwldgZ",
            "forum": "fH2wf2w2Ss",
            "replyto": "fH2wf2w2Ss",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7930/Reviewer_5Hdf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7930/Reviewer_5Hdf"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an unconditional image generation pipeline which is split into two parts: first, a model generates a random condition (in this case a CLIP image embedding), then, a second model is conditioned on this condition to generate the actual image. Compared to baseline unconditional single-stage models, the new approach performs better while leading only to a small overhead in training and sampling cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The approach tackles unconditional image generation, and improvements in that area could potentially also translate to conditional generation pipelines.\n\nThe approach of splitting the unconditional generation into two parts seems novel and the results indicate that this does indeed lead to improvements, at least on the relatively small datasets and image resolutions that it was tested on."
                },
                "weaknesses": {
                    "value": "While the approach seems to lead to improved performance it's not clear to me why this is the case and there is only very little analysis around this.\nIs it that unconditional sampling of CLIP image embeddings is somehow important or easier than sampling an image directly? Or is it the two-stage pipeline itself that is the important part? Could the condition generation and subsequent image generation be done in a single pipeline with end-to-end training? What exactly is the interaction between the first and second stage models?"
                },
                "questions": {
                    "value": "How well do you think this would work for more complicated domains and datasets?\nHow do you think this approach could benefit/improve conditional generation pipelines such as text-to-image?\nHow well do you think this would work with more specific conditions in the first stage (e.g., depth maps, edge maps, etc)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804100496,
            "cdate": 1698804100496,
            "tmdate": 1699636973933,
            "mdate": 1699636973933,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gkKPmNm7Gn",
                "forum": "fH2wf2w2Ss",
                "replyto": "xyrBIwldgZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7930/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and very helpful comments. We have decided to withdraw our submission but will also reply to your comments here for the record.\n\n\n*\u201cIs it that unconditional sampling of CLIP image embeddings is somehow important or easier than sampling an image directly\u201d* - Yes, we believe that the distribution over CLIP embeddings of natural images is much simpler and easier to model than the distribution over natural images themselves. One reason for this is that they are lower-dimensional than natural images. Another, we suspect, is that CLIP embedding space is much smoother than pixel space. See e.g. how CLIP embeddings can be reasonably manipulated with simple \"vector arithmetic\" in Section 3.3 of [DALL-E 2](https://arxiv.org/abs/2204.06125).\n\n*Could the condition generation and subsequent image generation be done in a single pipeline with end-to-end training?* - We tried training the encoder/CLIP embedder jointly with the auxiliary model and conditional image model in Appendix B. Our results were substantially worse than when we have a pretrained CLIP embedder, and we observed severe overfitting on AFHQ. We suspect that the pretrained features from CLIP, and their alignment to features noticeable to humans, are a large part of why 2SDM works well.\n\n*What exactly is the interaction between the first and second stage models?* - These models can be trained separately and independently. As long as they are both trained with the same CLIP embedder, the outputs from the first stage model can be fed into the second stage model at inference time to produce coherent output images.\n\n*How well do you think this would work for more complicated domains and datasets?* - The benefit of 2SDM over our baselines is related to how much additional information the CLIP embedding $\\mathbf{y}$ can provide over $\\mathbf{a}$. In the unconditional case, 2SDM\u2019s advantage should grow with the diversity of a dataset, since e.g. having a CLIP embedding describing that an image is of a man\u2019s face is much more informative in a dataset like ImageNet where few of the images are of faces than it would be in FFHQ, where roughly 50% of the images are men\u2019s faces. This is supported by our greater reduction in FID on Uncond. ImageNet-64 than on FFHQ-64. On the other hand, 2SDM\u2019s advantage will decrease as the conditioning information $\\mathbf{a}$ becomes more complex, since this will reduce the amount of information that $\\mathbf{y}$ provides and $\\mathbf{a}$ doesn\u2019t. This is supported by our greater reduction in FID on Uncond. ImageNet-64 than on Class-cond. ImageNet-64.\n\n*more specific conditions e.g., depth maps* - We think it\u2019s likely that conditioning on this type of information can certainly improve generation quality. Training an auxiliary model which outputs depth maps is likely to be more difficult, though, than one which outputs CLIP embeddings, and so it is not clear to us whether a two-stage model like 2SDM will outperform a one-stage model if $\\mathbf{y}$ is of this form."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709338687,
                "cdate": 1700709338687,
                "tmdate": 1700709338687,
                "mdate": 1700709338687,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RJPZCJDXJv",
            "forum": "fH2wf2w2Ss",
            "replyto": "fH2wf2w2Ss",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7930/Reviewer_WB76"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7930/Reviewer_WB76"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a two-stage approach, 2SDM, for sampling from diffusion models. The goal is to improve the performance of unconditional generation, which has a gap in performance compared to conditional generation. In the first stage, an auxiliary diffusion model is used to generate an embedding, which is subsequently used in the second stage by a conditional diffusion model to synthesize an image. The authors demonstrate that 2SDM yields better performance across almost all experiments in terms of quality and diversity with little to no increase in sampling speed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed two-stage approach is straightforward, extending UnCLIP to the unconditional setting.\n- The authors provide some context and insight into what it means for conditional generation to be better than unconditional generation, and why this may be the case.\n- Experiments demonstrate superior performance over the baselines with negligible impact on the sampling speed."
                },
                "weaknesses": {
                    "value": "While the method is straightforward, the paper is a bit difficult to understand overall. The finer details are unclear. For example:\n- It is unclear what the authors mean when they mention \"discarding\" the conditional embedding y after sampling.\n- The details about the auxiliary model in Section 4 are unclear. For example, what is a_\\sigma? Maybe reiterating some of the variables in Equation 4 would be helpful, too.\n- In the results overview of Section 5, the authors describe that Figure 4 (which seems to actually be referring to Figure 5) \"compares against 'Class-cond', which is an ablation of 2SDM that applies to unconditional tasks\". Given the label \"Class-cond\" it seems more intuitive that this would refer to the \"lightly-conditional\" task instead."
                },
                "questions": {
                    "value": "- For explicit clarification, are the two models (auxiliary and conditional image) trained sequentially?\n- The authors mention that they did not use classifier-free guidance in their results, which is common practice for diffusion sampling. It would be helpful to get some sense of how it affects the quality of the outputs.\n- The experimental results are compelling and the method is straightforward, but the paper could greatly benefit from clearer communication of the proposed ideas and details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7930/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7930/Reviewer_WB76",
                        "ICLR.cc/2024/Conference/Submission7930/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813826253,
            "cdate": 1698813826253,
            "tmdate": 1700686501910,
            "mdate": 1700686501910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "txk5hLYROo",
                "forum": "fH2wf2w2Ss",
                "replyto": "RJPZCJDXJv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7930/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very helpful comments. We have decided to withdraw this submission but will also reply to your comments here for posterity.\n\n*\"discarding\" the conditional embedding y after sampling* - Thanks for pointing out that this wasn\u2019t clear. We were simply meaning to make explicit that $\\mathbf{y}$ isn\u2019t used again after $\\mathbf{x}$ is sampled. We have rewritten this in Section 4.\n\n*details about the auxiliary model* - Apologies; a typo made this section confusing: $\\mathbf\\{a\\}\\_\\sigma$ should have been $\\mathbf{y}_\\sigma$. We have now fixed it and added an extra explanatory sentence.\n\n*'Class-cond' baseline description* - The ``class-cond.\u2019\u2019 method is a version of 2SDM in which $\\mathbf{y}$ is a discrete class label instead of a continuous CLIP embedding. We say it is particularly applicable to the unconditional setting because then $\\mathbf{y}$ can be reasonably sampled from the empirical distribution over $\\mathbf{y}$ represented by the training data without training any auxiliary model. We have now made this paragraph clearer.\n\n *are the two models (auxiliary and conditional image) trained sequentially?* - The two models can be trained sequentially or concurrently, since the training of the conditional image model does not rely on already having an auxiliary model, and vice versa.\n\n*classifier-free guidance* - Thank you for this suggestion! We unfortunately haven\u2019t been able to obtain these results before the end of the discussion period but in a future revision will add results in which we vary the classifier-free guidance scales for each of our baseline image model, 2SDM\u2019s auxiliary model, and 2SDM\u2019s conditional image model."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709229123,
                "cdate": 1700709229123,
                "tmdate": 1700709229123,
                "mdate": 1700709229123,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]