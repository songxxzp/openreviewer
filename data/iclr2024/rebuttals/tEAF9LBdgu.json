[
    {
        "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation"
    },
    {
        "review": {
            "id": "EtW65id9ss",
            "forum": "tEAF9LBdgu",
            "replyto": "tEAF9LBdgu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8071/Reviewer_rXeX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8071/Reviewer_rXeX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces AutoGen to enhance the development of LLM applications through the use of multiple conversational agents. These agents are customizable, capable of various modes of operation, and can interact with each other, human inputs, and tools to accomplish complex tasks. The framework supports programming interaction behaviors using both natural language and code, catering to a wide range of applications across different domains.\nContributions:\n- The framework provides a generic and extensible design for agents, enabling them to engage in conversations and multi-turn interactions seamlessly.\n- AutoGen introduces a conversation-centric programming paradigm, simplifying the development of complex LLM applications and providing adaptability to a wide range of needs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- AutoGen\u2019s conversation-centric programming paradigm can simplify and unify the development of complex LLM applications, showcasing originality in application workflow design.\n- AutoGen provides robust support for developers, including the ability to program agent interactions using both natural language and code, catering to a diverse set of development preferences and needs."
                },
                "weaknesses": {
                    "value": "While AutoGen presents a promising framework for multi-agent applications of LLMs, the paper tends to read more like a tech report rather than a traditional research paper, as it lacks a focused exploration of research-oriented problems. The novelty of the work seems constrained by this, as it primarily introduces and elaborates on the framework\u2019s capabilities without diving deep into scientific inquiries or hypotheses testing. Although the practical utility of AutoGen is clear, the analysis provided in the paper regarding its positioning and performance relative to existing solutions is not sufficiently comprehensive, and the discussion on scalability, performance overheads, and real-world applications remains superficial. The agent customization, while a strong feature, could benefit from clearer guidelines, and the human-AI interaction aspect requires more elaboration. These areas of improvement highlight a need for a more detailed, research-focused approach, and suggest that the work may find a more fitting audience at a venue like the System Demonstration track at ACL, where applied tools and frameworks are showcased and appreciated."
                },
                "questions": {
                    "value": "- Is there any scenarios in which the single agents outperform the multi-agent counterparts? For example, the Alfworld may be as easy as possible for a single agent to solve, maybe the assistant agent and executor agent can be the same one which generates the formatted [think + act] steps at the same time, which makes the multi-agent problem into a prompt template design problem.\n- How did the grounding agent in the A3 of Figure 4 know the crucial commonsense knowledge, Did you design the specific prompts/few-shot examples to teach it?\n- Why do only two methods have the whole dataset results in A1 of Figure 4?\n- In the scenario of the A6 of Figure 3, multi-agent players are playing chess, how to make sure every agent strictly follows the chess rules?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8071/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8071/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8071/Reviewer_rXeX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8071/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754267113,
            "cdate": 1698754267113,
            "tmdate": 1699636998830,
            "mdate": 1699636998830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RBprq5fmkq",
                "forum": "tEAF9LBdgu",
                "replyto": "EtW65id9ss",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses for Reviewer rXeX. Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and questions! Please find our responses below. We would like to answer any further questions you might have!\n\n## On weakness about venue fit\nAutoGen is motivated by the emerging need of multi-agent systems based on LLM, tools, and human input, and the challenges around building such systems. Given the focus on facilitating the development of multi-agent systems, we selected \u201cinfrastructure, software libraries, hardware, etc.\u201d as the primary area of this submission.\n\nOur contributions/novelty lies in: \n(1) the concepts/techniques we proposed for easily creating and orchestrating multi-agent systems, e.g., conversable agent, conversation programming, and conversation-centric way of building multi-agent systems. \n(2) the experiments we conducted to quantify the value of multi-agent settings.\n(3) Additionally, we open-source the library to encourage additional research and development to explore and accelerate progress in this emerging area.\n\n## Re Question 1 on single-agent vs multi-agent performance \nIn our experiments, we observed that the multiagent settings are comparable to or better than the single-agent counterparts. The cases where multi-agent settings do not provide additional value are typically cases where the tasks are easier and can be solved reliably with a single agent.\n\nFor example, we see that on ALFWorld (Figure 4.c), a two-agent setup is comparable to a single-agent (with ReAct). However, a three-agent setup (with a grounding agent) outperforms both the React agent and the 2-agent setup. We note the performance of multi-agent settings will vary depending on the agent design. Hence, it is conceivable that a badly designed multi-agent solution could underperform a single agent.\n\n## Re Question 2 on the grounding agent in ALFWorld\n\nWe hard-coded a set of common sense knowledge relevant to household tasks in the grounding agent in A4. E.g., \u201cYou must find and take the object before you can examine it.\u201d as shown in Figure 10 in Appendix D. The knowledge will be requested if the assistant\u2019s proposed solution fails three times or the task gets stuck. No example or prompt is used to teach it. \n\n## Re Question 3 on the evaluation of A1 in Figure 4\nNote that the whole MATH dataset includes 5000 questions. Running each method on the dataset costs between $500 and $800. Some methods (that use ChatGPT Plus) require substantial manual effort and are also restricted by message and token hourly rates - see footnote 4 on page 7.\n\nOur experiments using the level-5 problems (the smaller dataset)  indicated a significant performance gap between the methods AutoGen, LangChain, and ReAct and that GPT-4 performs slightly better. As such, we prioritized comparing AutoGen to GPT-4 on the larger (5000 questions) dataset.\n\n## Re Question 4 on how to make agents follow chess rules \nTo ensure adherence to chess rules in our multi-agent chess game, we have introduced a 'Board Agent' that leverages the Python chess library for move validation.\nThis agent operates by extracting the UCI (Universal Chess Interface) move from each player's response at every turn. It then verifies the legality of this move using the chess library. Should a move be deemed illegal, the Board Agent issues an error message to the respective player, requesting a new legal move. For an illustrative example of the Board Agent's functionality, both with and without its intervention, please refer to Figure 16 in Appendix D, located on page 31 of our submission."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532438457,
                "cdate": 1700532438457,
                "tmdate": 1700541235091,
                "mdate": 1700541235091,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HF39q54YqV",
            "forum": "tEAF9LBdgu",
            "replyto": "tEAF9LBdgu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8071/Reviewer_xV6g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8071/Reviewer_xV6g"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an open-source toolkit AutoGen, It introduces a tool to facilitate developing multi-agent LLM backed systems. The tool considers three different backend handlers for requests including LLM agent, human and some tools like code executor. It provides developers an opportunity to use both natural language or code for interaction. It covers 6 different use cases and shows promising results against default GPT benchmarks and some other tools."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ Open-source multi-agent programming framework is definitely an interesting project.\n+ Authors include some empirical results to showcase their performance of the tools.\n+ Authors provide extensive documentation to explain different use cases."
                },
                "weaknesses": {
                    "value": "- Not much takeaway in terms of scientific learning.\nI don\u2019t find as a reader what lessons or results we can get from the paper. The main message is that we have a tool that can help developing multi-agent conversation. In my personal opinion, developing it based on a mature LLM agent is neither time-consuming nor scientifically challenging.\n\n- The results are not convincing.\nI found the comparison of results is not rigorously evaluated and not convincing. For example, the paper shows that by repeating the question again to LLM may potentially get better results. But the results are not convincing due to few sample they use nor making a lot of sense.\n\n- Too much brag about their system but little evidence is shown to support it.\nFor example, in the introduction, they aim to design a \u201ccapable, reusable, customizable, and effective\u201d system. I didn\u2019t see support towards it."
                },
                "questions": {
                    "value": "Can you specify why the interactive retrival can lead to better results? I am curious about what the result will look like if I try different variations of questions? As with interactive, I can think of the improvement is due to more trials other than better problem understanding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8071/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8071/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8071/Reviewer_xV6g"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8071/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793570458,
            "cdate": 1698793570458,
            "tmdate": 1699641383152,
            "mdate": 1699641383152,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gGurTvc4OP",
                "forum": "tEAF9LBdgu",
                "replyto": "HF39q54YqV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer xV6g (part 1). Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We thank the reviewer's comments and questions. Please find our responses below. We are happy to answer any more questions the reviewer might have. \n\n## (Re Weakness 1) On scientific contribution and practical utility \n> Comment: Not much takeaway in terms of scientific learning. I don\u2019t find as a reader what lessons or results we can get from the paper. The main message is that we have a tool that can help developing multi-agent conversation.\n\nIn addition to displaying engineering excellence, we believe AutoGen furthers science on multi-agents in the context of LLMs in many ways. \n- The concepts of conversable agents and conversation programming are novel and help systematically understand and make progress on the nascent topics of multi-agent for LLM applications. For example, as highlighted by reviewer SJWG, , \u201cAnother useful insight of this paper is to divide the main application workflow into small multi-agent conversations\u2026\u201d\n- Empirical evaluation in Section 3 and in Appendix D shows AutoGen's advantages over existing frameworks and also produces some new discoveries. For example, through the experiments introduced in A4, we discover that \u201ca multi-agent design is helpful in boosting performance in coding tasks that need safeguards\u201d. We provide further elaboration on the takeaways from this and other applications in Appendix D (Application Details). \n- The proposed concepts and abstractions combined with our OSS implementation enable other researchers to explore multi-agents and further science. E.g, as mentioned in A4 of Section 3 in the main paper, through AutoGen OptiGuide\u2019s implementation was reduced from over 430 lines to just 100 lines. This is a clear testament to the framework's ability to facilitate more efficient scientific experiments.\n\n> Comment: The main message is that we have a tool that can help developing multi-agent conversation. In my personal opinion, developing it based on a mature LLM agent is neither time-consuming nor scientifically challenging.\n\nAlthough the reviewer does not personally perceive the framework\u2019s value in supporting the development of multi-agent systems, the framework is well-received by the open-source community in general:  as of 20th November 2023, our OSS implementation \n- has been downloaded over 130,000 times within less than 2 months (we omit package name to remain anonymous.); \n- has been used to build many applications from the open-source community (used in more than 200 open-source projects). For example, GPT academic, an LLM-based service for assisting various academic-related activities such as scientific writing, research paper reading, etc,; and \n- has been awarded one of the top 100 open-source projects 2022-2023 by the International Open Benchmark Council under the following evaluation criteria: \n    - The key milestones that lay the foundation for open source movement or development; \n   - The original or pioneering open source works; \n   - The open source works that play a significant role in promoting the development of software and hardware; \n   - The open source works are widely used or cited by industry or academia.\n\nThese are all evidence of our framework's practical utility for AI applications and experiments. Although we totally understand and value the reviewer's personal preference, we hope the reviewer can take the general impact and utility this work could bring into consideration."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581110198,
                "cdate": 1700581110198,
                "tmdate": 1700582866849,
                "mdate": 1700582866849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jNcjqkus13",
                "forum": "tEAF9LBdgu",
                "replyto": "HF39q54YqV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer xV6g (part 2). Thank you for your comments!"
                    },
                    "comment": {
                        "value": "## (Re weakness 2) On evaluation and results\n\nThank you for your feedback. We understand the importance of rigorous evaluation in assessing the effectiveness of our framework.\nTo ensure a comprehensive understanding, our study encompasses a blend of qualitative and quantitative evaluations across a variety of applications. These evaluations are designed to provide a holistic view of the framework's performance and utility.\n\n> I found the comparison of results is not rigorously evaluated and not convincing.\n\nSince different evaluation types may yield varying insights, we would like to ask the reviewer which application and evaluation this comment is referring to here. \n\nTo further clarify, as mentioned in the paper, the evaluation of all four applications (A1-A4) with quantitative evaluation results followed established practices from existing literature. E.g.,\n\n- For A1, our evaluation used both the full MATH dataset [1], which includes 5000 problem instances, and a sub-sampled set including 120 problems. \n- For A2, our evaluation was performed on all the problem instances from the Natural Questions benchmark [2], which is a benchmark for Question Answering Research and includes 6775 question instances. We followed the evaluation procedure established in [3]. \n- For A3, our evaluation is performed on the ALFWorld dataset [4], following the evaluation procedure used in the ReAct [5] paper.\n- For A4, our evaluation is performed following the evaluation procedure established in the original OptiGuide paper [6]. \n \nIn addition to quantitative evaluation results, we also include a rich set of qualitative studies for A1-A4 (included in Appendix D Application Details).\n\nA5 and A6 are innovative applications for demonstration purposes (as there is no relevant benchmark for these two tasks) and thus are primarily supported by qualitative.\n\n[1] Hendrycks, Dan, et al. \"Measuring mathematical problem solving with the math dataset.\" NeurIPS 2021.\n\n[2] Kwiatkowski, Tom, et al. \"Natural questions: a benchmark for question answering research.\" ACL 2019.\n\n[3] Adlakha, Vaibhav, et al. \"Evaluating correctness and faithfulness of instruction-following models for question answering.\" arXiv preprint arXiv:2307.16877 (2023).\n\n[4] Shridhar, Mohit, et al. \"Alfworld: Aligning text and embodied environments for interactive learning.\" ICLR 2021\n\n[5] Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" ICLR 2023.\n\n[6] Li, Beibin, et al. \"Large language models for supply chain optimization.\" arXiv preprint arXiv:2307.03875 (2023).\n\n## (Re weakness 3) On the support of claims \n\nWe presented numerous applications developed with the proposed framework and provided very comprehensive evaluations of most of the applications in both Section 2 and Appendix D (from page 19 to page 43). Those are all evidence supporting the advantages of the proposed framework.  Regarding the example mentioned by the review on \u201ccapable, reusable, customizable, and effective\u201d, we do provide support for this claim:\n\n- Capable: In the second paragraph of Section 2.1, we introduced 'Agent capabilities powered by LLMs, humans, and tools', elaborating the supported agent capabilities.\n- Customizable: Section 2.1 discusses 'Agent customization and cooperation'. In the applications presented in Section 3, we developed agents customized from the built-in agent. This includes the Retrieval-augmented User Proxy agent in Application 2 and the ALF World Executor agent in Application 3.\n- Reusable: The built-in agent, Assistant, is reused in Applications 1 (including two scenarios), 2 (including two scenarios), and 4.\n- Effective: In Applications 1-4, we demonstrate how AutoGen enables the development of multi-agent systems that are effective in solving tasks such as math problem-solving, retrieval augmented question answering, decision-making, and coding with safety constraints."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582234017,
                "cdate": 1700582234017,
                "tmdate": 1700582234017,
                "mdate": 1700582234017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KIMjSFQ3S5",
                "forum": "tEAF9LBdgu",
                "replyto": "HF39q54YqV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer xV6g (part 3). Thank you for your comments!"
                    },
                    "comment": {
                        "value": "## Re question on interactive retrieval \nWe thank the reviewer for the insightful question! We agree with the reviewer\u2019s point that the improved performance with interactive retrieval ultimately stems from the additional trials requesting context. However, achieving this wisely and robustly is highly non-trivial and does require a proper understanding of the problem status.\n\u00a0\nTo better understand the challenge here, let\u2019s consider a naive approach: this approach performs multiple trials of retrieval right at the beginning when the question is asked. This gives us more trials but does not require problem understanding at all. However, this naive approach has one obvious limitation: it is hard to decide how many up-front trials one should try to include. Including context from too many trials can lead to the conversation exceeding the context length and may incur unnecessarily high inference cost. When the multiple up-front trials are conducted separately, one also needs a way to select the answer from it, which is also non-trivial. Conversely, too few trials may not provide adequate context. This latter case with a single trial is investigated in our ablation study in A2, with results illustrated in Figure 4(b) and Figure 10 in Appendix D. Regarding the former case, since it relies on the number of trials as a hyperparameter and an answer selection method, it is easy to construct scenarios that have undesirable results, e.g., failure due to context overflow or incurring a very high cost.\u00a0 Interactive retrieval, on the other hand, can be considered an online approach that naturally remedies this tension: It does not rely on a pre-specified trial number but makes the \u201cUPDATE CONTEXT\u201d request when necessary (based on LLM\u2019s understanding of whether the question can or cannot be answered without further context) as the conversation proceed until the question is considered answered.\n\nWe would like to hear if your comments are addressed, and the question is answered, and we would be happy to provide further elaboration if needed."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585842504,
                "cdate": 1700585842504,
                "tmdate": 1700589076943,
                "mdate": 1700589076943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "szqY0HCkd1",
                "forum": "tEAF9LBdgu",
                "replyto": "HF39q54YqV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer xV6g,\n\nThank you again for your valuable comments on our submission. In response to your comments, we have provided detailed responses. We are writing to follow up and inquire if our responses have adequately answered your question on interactive retrieval and addressed your concerns about the weaknesses of this work. If so, we respectfully request that you consider raising the rating of our work. We are fully open to any further elaboration or clarification you might need to facilitate this reevaluation. We greatly appreciate your time and effort in reviewing our work and look forward to your re-evaluation. Thank you and we look forward to hearing from you.\n\nSincerely,\n\nAuthors of this submission"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702950752,
                "cdate": 1700702950752,
                "tmdate": 1700702950752,
                "mdate": 1700702950752,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZJv4VvGxxF",
            "forum": "tEAF9LBdgu",
            "replyto": "tEAF9LBdgu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8071/Reviewer_AgUb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8071/Reviewer_AgUb"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents AutoGen, an open-source framework for building LLM applications via multi-agent conversations. The paper introduces two key concepts: conversable agents and conversation programming. Conversable agents are entities that can communicate with each other and have different capabilities powered by LLMs, humans, or tools. Conversation programming is a paradigm that allows developers to define the interaction behavior between agents using a fusion of natural and programming languages. The paper demonstrates six applications of AutoGen that span various domains and complexities and shows that AutoGen can achieve better performance, reduce development effort, and enable novel LLM usage scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1)\tThe proposed framework simplifies the overall complex LLM workflows and enables automation. By using conversable and customizable agents, it supports conversational modes for complex workflows. It provides a collection of working systems with different complexities. These systems cover a wide range of applications from various domains and complexities. \n\n(2)\tThe proposed method allows developers to use a fusion of natural and programming languages to define agent behaviors and conversation patterns.\n\n(3)\tThe paper demonstrates the effectiveness and generality of the framework in various domains and tasks. It showcases novel and innovative applications that are enabled by the multi-agent conversation framework."
                },
                "weaknesses": {
                    "value": "(1)\tThe paper does not address the issue of context length, which may become too long as the number of conversation turns increases. This could affect the performance and efficiency of the LLMs and the agents.\n\n\n(2)\tIt would be better to consider the cost issue, which is important for practical applications. The experiments are conducted on GPT-4 and GPT-3.5, which are expensive and not widely accessible. How would the framework perform on open-source LLMs with lower capacity?\n\n\n(3)\tIn my opinion, AutoGen appears to be an extension of CAMEL, both supporting agent role-playing and agent conversations. The authors discuss the related work of CAMEL in the appendix, and highlight two distinct advantages of AutoGen: 1) its capability for tool usage and 2) dynamic conversation. However, it deserves to give an in-depth discussion about CAMEL and AutoGen about their differences in the introduction section. Actually, I do not think the tool-usage is a big challenge if using the GPT4. It is expected to discuss the challenge from static conversation (of CAMEL) to dynamic conversation (of AutoGen)."
                },
                "questions": {
                    "value": "(1)\tPlease refer to Weakness (2). How would the framework perform on open-source LLMs with lower capacity?\n\n(2)\tPlease refer to Weakness (3). What is the technique challenge for introducing dynamic conversation compared with AutoGen? \n\n(3)\tCan this method be applied in other complex tasks, such as automatically using professional tools (Oracle, MATLAB)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8071/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835480051,
            "cdate": 1698835480051,
            "tmdate": 1699636998568,
            "mdate": 1699636998568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rTiyeHhMY6",
                "forum": "tEAF9LBdgu",
                "replyto": "ZJv4VvGxxF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer AgUb (part 1). Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments, constructive suggestions, and thoughtful questions! \n\n## (Re weakness 1) On context length\nThanks to the extensibility of our framework, we are able to work out two solutions to address this potential issue. \nWe are able to implement a `CompressibleAgent` under the proposed AutoGen framework that could compress long context when needed. Please find [a notebook example demonstrating how to use CompressibleAgent in this anonymous link](https://anonymous.4open.science/r/autogen-iclr2024/notebook/agentchat_compression.ipynb).\n\nAs mentioned in the expanded discussion section in Appendix B, we acknowledge the existence of scenarios/cases where other libraries/packages could help. The context length problem could be one such case. AutoGen has been integrated with MemGPT, which is a recent work that could teach LLMs to manage memory for unbounded context. Please find a notebook example [demonstrating how AutoGen can be used together with MemGPT](https://anonymous.4open.science/r/autogen-iclr2024/notebook/memgpt_coder_autogen.ipynb).\n\n##  (Re question 1) On performance with open-source LLMs \nIn general, open-source LLMs with lower capacity than GPT-3.5/4 would lead to lower performance if not meticulously utilized. Fortunately, our multi-agent framework provides the flexibility to use those open-source models in strategic ways.  In one of our follow-up work (we do not disclose the title of work due to anonymity concerns), we built a two-agent system using the built-in AssistantAgent and UserProxyAgent in AutoGen and evaluated the system on tasks [1] that require coding and external API calls. In this application, if we directly replace GPT-3.5/4 with LLAMA-2-13B-chat in the AssistantAgent, the system\u2019s performance indeed drops by a large margin. However, we also show that by including multiple AssistantAgent backed by GPT models or LLAMA-2-13B-chat into a multi-agent system, we can actually reduce the dollar cost of the system while improving the success rate. \n\n----\n\n| AssistantAgent     | Success Rate (averaged over 300 queries) | Average Cost \n|---------|----------|----------|\n| LLAMA-2-13B-chat  | 13.33% | 0.00\n| GPT-3.5-turbo | 34.11% | $0.41\n| GPT-4   | 77.22% | $13.93\n| GPT-4 +  GPT-3.5-turbo +  LLAMA-2-13B-chat  | 83.22% |  $8.98\n\n[1] ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539126543,
                "cdate": 1700539126543,
                "tmdate": 1700543806242,
                "mdate": 1700543806242,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yGa2Li2OPi",
                "forum": "tEAF9LBdgu",
                "replyto": "ZJv4VvGxxF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer AgUb (part 2). Thank you for your comments!"
                    },
                    "comment": {
                        "value": "## (Re question 2) On technical challenges \n\nWe thank the reviewer for the constructive suggestion and this insightful question.\n\nWe would like to first clarify that CAMEL has a different focus and positioning from AutoGen: CAMEL is primarily positioned as a framework for studying the cooperative behaviors of agents of different roles, not an infrastructure to support the development of LLM applications as the case in AutoGen. This difference can be better understood from the two frameworks\u2019 different behaviors in solving a task. E.g., under the task \u201c*Design a custom game using pygame*\u201d [demonstrated in CAMEL\u2019s official GitHub repo](https://colab.research.google.com/drive/1AzP33O8rnMW__7ocWJhVBXjKziJXPtim?usp=sharing#scrollTo=-IhYiAWDme66), we compared AutoGen vs. CAMEL and summarized in results in this [anonymized document](https://anonymous.4open.science/r/autogen-iclr2024/notebook/alternatives/README.md).  From the comparison, we can see that CAMEL is primarily simulating a conversation between an AI agent with the role \u201c*Computer Programmer*\", and an agent with the role \u201c*gamer*\u201d, but is not actually creating a meaningful game; while AutoGen is able to actually create games with pygame and save the created game into a file such that it can be directly executed and played. \n\nOne more fundamental distinction, which poses profound technical challenges, is AutoGen\u2019s general support for multi-agent systems with an agent number N > 2. CAMEL\u2019s inception prompting based role-playing framework currently primarily supports systems with two AI agents with potentially a critic in the loop. There is no general support for systems with more than 2 agents or with other conversation patterns.  \n\nNote that moving from 2 to N (N > 2) in an effective way that could support LLM applications is highly non-trivial and is technically challenging: When N = 2, the communication between the agents is straightforward. Supporting N > 2, in general, requires careful abstraction and implementation so that the framework can (1) possess the flexibility to meet various application needs (there is hardly a one-fit-all pattern), and (2) support conversation patterns that can make meaningful progress in task completion. AutoGen is so far the only framework that realizes both objectives decently well. \n\nWe will clarify the major differences and challenges in the main paper accordingly in a future draft. \n\n## (Re question 3) On the support of complex tasks and professional tools.\n\nYes, our framework is adaptable to other complex tasks involving the use of professional tools. In the Multi-agent Coding application introduced in A4, in addition to Python, the developed OptiGuide system uses several professional tools, including Gurobi for mathematical optimization used by the commander agent, and various tools in Docker used by the AssistantAgent. \nIn general, tools that can be accessed using Python code can be seamlessly incorporated into function calls, and can therefore be automatically used by agents. Both MATLAB and Oracle have officially supported Python APIs and could be supported."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539242271,
                "cdate": 1700539242271,
                "tmdate": 1700541650443,
                "mdate": 1700541650443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bjIvaFUx3P",
                "forum": "tEAF9LBdgu",
                "replyto": "ZJv4VvGxxF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer AgUb,\n\nThank you again for your encouraging and constructive comments, as well as the insightful questions regarding our submission. We hope our responses, together with the additional empirical results, have addressed your questions and concerns on the potential weaknesses. If so, we respectfully request that you consider raising the rating of our work in light of the added empirical results and additional support we provide for addressing the context length issue, which is highly non-trivial. \n\nWe greatly appreciate your time and effort in reviewing our work.\n\nBest,\n\n\nThe authors of this submission"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703869166,
                "cdate": 1700703869166,
                "tmdate": 1700703869166,
                "mdate": 1700703869166,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5L5H5OtELh",
            "forum": "tEAF9LBdgu",
            "replyto": "tEAF9LBdgu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8071/Reviewer_SJWG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8071/Reviewer_SJWG"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an open-source framework to develop LLM applications using multiple agents that interact with each other to complete tasks. The agents presented are customizable, can converse with each other and can operate in various modes using LLM, human input or tools. Through experiments, they show the effectiveness of this framework for several tasks like  Math Problem Solving, Question-Answering task etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe approach defines a generic design for agents that can use LLMs, human inputs, certain tools or combination of them. LLM agents can use capabilities such as role playing, progress making from conversation history, proving feedback. Human involvement can be configured at different levels e.g. frequency and conditions for when to request human input. Tools agents can execute code/functions (suggested by LLMs). \nCombining these agents in different configurations can result in powerful agents with very different capabilities.\n\n-\tThe another useful insight of this paper is to divide the main application workflow into small multi-agent conversations which they call Conversation programming. It consists of two components: computation which is the actions that the agents take to get their response, control-flow which defines the decisions on which agents to send messages to. These two components allow for control over the conversation flow in the application workflow."
                },
                "weaknesses": {
                    "value": "None"
                },
                "questions": {
                    "value": "-\tIn Figure 4(c), the performance of ReAct on Best of 3 is better than AutoGen (2 agents). I am curious as to what do you think would the reason for that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8071/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699433225221,
            "cdate": 1699433225221,
            "tmdate": 1699636998388,
            "mdate": 1699636998388,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I3Cl5WViCS",
                "forum": "tEAF9LBdgu",
                "replyto": "5L5H5OtELh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer SJWG. Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and the question. The difference between the performance of ReAct and AutoGen (2 agents) is actually quite marginal. The marginal difference could be caused by randomness from LLM output generation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532597641,
                "cdate": 1700532597641,
                "tmdate": 1700532597641,
                "mdate": 1700532597641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JaLgnxInDZ",
                "forum": "tEAF9LBdgu",
                "replyto": "5L5H5OtELh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8071/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer SJWG,\n\nThank you again for your encouraging comments, insightfully pointing out the strengths of this work. We are writing to follow up if your question on the performance difference between ReAct and AutoGen 2-agent has been addressed by our reponse. Please feel free to let us know if you need further clarification or elaboration.\n\nWe highly appreciate your time and effort spent on reviewing our paper!\n\nSincerely,\n\nThe authors of this submission"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704266573,
                "cdate": 1700704266573,
                "tmdate": 1700704591546,
                "mdate": 1700704591546,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]