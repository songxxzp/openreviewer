[
    {
        "title": "SCREWS: A Modular Framework for Reasoning with Revisions"
    },
    {
        "review": {
            "id": "FS6yOcT6bd",
            "forum": "Oho3UxCkKr",
            "replyto": "Oho3UxCkKr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1115/Reviewer_4Ly4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1115/Reviewer_4Ly4"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents SCREWS, a methodology for reasoning with revisions. The pipeline includes three stages: sampling, conditional resampling, and selection. The experiments demonstrated that using different strategies for sampling and conditional resampling can boost the reasoning performance of the GPT-3.5 language model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed framework is general and modular, meaning various techniques can be employed in different stages of it."
                },
                "weaknesses": {
                    "value": "1. > A student preparing for an exam may use deductive reasoning to solve problems and inductive reasoning to verify the results\n\n    This is surely the wrong way around?\n\n2. Figure 2 is too visually complicated to be helpful. It's better to present a simplified and more abstract pipeline than listing every component.\n\n3. This is the main thing I am unsure about: In tables 1 and 2, the results are supposed to demonstrate the usefulness of the resampling strategy. However, in table 1, only 4 out of 9 pairings are statistically significant. Also, when using Subq (Or) as the sampling strategy, it does not seem to matter much (or have statistical significance) as to which conditional resampling strategy is used. Does this maybe suggest that there is a saturation, or utility limitation of SCREWS, if the sampling strategy is good enough?\n\n    In table 2, although conditional sampling is cheaper, independent sampling does have significantly higher performances (and upper bounds given oracle selectors). The figure 4 provides further breakdown of the accuracy - cost relation and no strategy really beats CoT in absolute performance. This leaves the question of the overall usefulness of SCREWS uncertain."
                },
                "questions": {
                    "value": "Could you provide ablation studies to show clearly that given the same computational cost, SCREWS can perform better than naive CoT? If so, I can be convinced of its effectiveness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1115/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697898381978,
            "cdate": 1697898381978,
            "tmdate": 1699636037832,
            "mdate": 1699636037832,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6J9MAzOlVf",
                "forum": "Oho3UxCkKr",
                "replyto": "FS6yOcT6bd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1115/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1115/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Usefulness of SCREWS"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our work.\n\nWe can simplify the main figure by making it clearer and put it in the appendix. Thanks for the suggestion. \n\nAs mentioned above, refinement does not work well for reasoning tasks and often leads to worse results (Self-Refine (https://arxiv.org/abs/2303.17651) and recent work (https://arxiv.org/abs/2310.01798) ).\nIn our case, we improve the results with our novel component of heterogeneous resampling and selection. Although we have not reached the upper bounds, we have shown the usefulness of the framework to try different strategies, and it can be improved with more such combinations. \n\nSubquestion decomposition is an expensive strategy compared to CoT because we have to sample iteratively for each decomosed questions. Deciding when to resample and resampling only selected samples is a cost-effective strategy compared to resampling all samples, and thus the method is cost effective. We have shown in Fig. 3a that CoT with 5 samples can achieve an accuracy of 75, while our strategy with 3 samples can achieve 83. This is the effectiveness of the SCREWS strategy. We can add more such examples in the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1115/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076766114,
                "cdate": 1700076766114,
                "tmdate": 1700076766114,
                "mdate": 1700076766114,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2pEV93OQnr",
            "forum": "Oho3UxCkKr",
            "replyto": "Oho3UxCkKr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1115/Reviewer_Na8j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1115/Reviewer_Na8j"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies refinement and revision in reasoning. They propose a modular framework for improving reasoning with revisions. The proposed framework unifies several previous approaches under a common framework but also reveals several novel strategies for identifying improved reasoning chains. It consists of three main modules, Sampling, Conditional Resampling, and Selection, each consisting of sub-modules that can be hand-selected per task. The framework is then implemented with GPT-3.5-turbo and GPT-4 and is evaluated on multiple benchmarks for arithmetic reasoning, multi-hop question answering, and code analysis. The proposed strategies achieve substantial improvements over vanilla strategies. The heterogeneous sampling strategy is demonstrated useful in the experiments. They also discuss the importance of a model-based selection strategy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper studies the problem of revisions in reasoning, including reducing errors introduced by revision and alleviating homogenous revisions, which are important research questions for current large language model reasoning. The authors propose a unified framework to address the questions. Many previous works can be viewed as an instance of the proposed framework. As a result, the framework is convenient for ablating the strategies during the pipeline. The experiments and analyses are comprehensive. The proposed strategies are effective. And the experimental findings are inspiring."
                },
                "weaknesses": {
                    "value": "Please see the questions listed below."
                },
                "questions": {
                    "value": "Q1: How do you choose the specific sub-modules (e.g., self-ask/tool use for conditional resampling, LLM-based selection/Rule-based selection for selection) for each of the three modules in the framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1115/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1115/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1115/Reviewer_Na8j"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1115/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681699806,
            "cdate": 1698681699806,
            "tmdate": 1699636037752,
            "mdate": 1699636037752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0ghVGwJtFP",
                "forum": "Oho3UxCkKr",
                "replyto": "2pEV93OQnr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1115/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1115/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Extension of SCREWS modules/sub-modules"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our work.\n\nThis is an important question, and with SCREWS we are not limited to any number of submodules. We have only demonstrated those that have been used in previous work combined with our proposed new sub-modules (for example, majority voting is a commonly used sub-module for selection and we extend it to LLMs making decision by proposing self-select, ...).\nHowever, our framework can be easily extended to any new modules/sub-modules."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1115/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076635397,
                "cdate": 1700076635397,
                "tmdate": 1700076635397,
                "mdate": 1700076635397,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BSS5eHbixX",
            "forum": "Oho3UxCkKr",
            "replyto": "Oho3UxCkKr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1115/Reviewer_toqi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1115/Reviewer_toqi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework, called SCREWS, with modular components for reasoning tasks where revisions and selection are needed. The framework contains three modules, Sampling, Conditional Resampling, and Selection. Each of the modules can then be implemented with several alternatives. The authors conducted experiments on GSM8K, StrategyQA, and Auto Debugging, using gpt-3.5-turbo, aiming to see how different combinations of modules can affect the task performance. The important observations include: 1) conditional resampling helps when it is based on a different method than the sampling, 2) a good selection is promising for improving the task performance, but the current selection method still falls short in it, and 3) enabling tools is critical for StrategyQA where additional facts are beneficial."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper has touched upon a popular topic of LLM reasoning, especially when iterative revisions are needed. The proposed framework summarized the typical implementation of different modules.\n2. The paper conducted experiments with different combinations of module instantiations and investigated their effectiveness. The experimental results have led to several interesting takeaway messages.\n3. The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "The contribution of this paper seems to be incremental, as it is mainly an empirical exploration of existing module implementations. While the experimental results led to interesting observations, these observations are mostly expected, whereas the more critical questions, such as how to improve the existing selection method, are not well addressed."
                },
                "questions": {
                    "value": "I found the tool use experiment of StrategyQA a bit confusing.\n1. I wonder if the conditional resampling can still be helpful if the LLM is configured to access the retrieved fact in its initial sampling? \n2. The setup seems to directly provide relevant facts to the conditional resampler, and the model does not actually use any Web search tool for fact retrieval. Is this the major reason for task improvement? I wonder in the more realistic case of using an external tool, if the same improvement can be observed (considering the potential noise, long retrieved passages, etc.)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1115/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803725765,
            "cdate": 1698803725765,
            "tmdate": 1699636037669,
            "mdate": 1699636037669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fUybWzLRdY",
                "forum": "Oho3UxCkKr",
                "replyto": "BSS5eHbixX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1115/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1115/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Why refinement is not so simple for reasoning"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our work. \n\nAs mentioned in response to 2Dnf, the paper proposed heterogeneous sampling as an approach to refinement, and later a choice between the refined answer and the original prediction as an option to \"roll back\" in case of errors in refinement. These two components are missing in all previous work. The proposed framework allows different previous strategies to mix and match different components to get the best refinement strategy for a given task. The framework is one of the contributions, not just the contribution. \n\nMoreover, expecting improvement through refinement is a common misconception, and it often leads to worse results for reasoning tasks (Self-Refine (https://arxiv.org/abs/2303.17651) and recent work (https://arxiv.org/abs/2310.01798) ).\n\nIf the LLM already knows the facts, it will not make factual errors (we mention this in the paper and for strategyQA, providing facts in the first steps leads to 90% accuracy, higher than refinement). But we are testing if LLMs can judge their mistakes, and to avoid factual errors again, we provide the factual info. \nTesting this with any other tool was out of scope, as we need to disambiguate the errors due to tools vs. LLMs errors."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1115/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076560225,
                "cdate": 1700076560225,
                "tmdate": 1700076560225,
                "mdate": 1700076560225,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Li0mSujT7r",
            "forum": "Oho3UxCkKr",
            "replyto": "Oho3UxCkKr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1115/Reviewer_2Dnf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1115/Reviewer_2Dnf"
            ],
            "content": {
                "summary": {
                    "value": "SCREWS, a modular framework for reasoning with revisions.  The author observe that these revisions can introduce errors, in which case it is better to roll back to a previous result. So there should be a framework that decide we should accept the current revision or not. \n\nThe proposed approach consists of three steps: \n[1] Sampling instantiate SCREWS by fixing the submodules for each module  \n[2] Conditional Resampling, which decides whether to generate a revision conditioned on the initial sample, and does so if needed. \n[3] Selection: all samples and revisions are given to the Selection module, which selects the best one. \n\nEach of the above three modules include several existing effective methods: \nSampling: CoT, decomposition\nCondition resampling: Self ask, tool use\nSelection:  self-consistency, rule-based etc"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper works on an interesting problem. The paper collects a couple of well known approaches and integrate them into this framework, and provide suggestions on how to use them. The paper objectively reports results, and performs analysis and comparison. Regarding ideas, self-ask with respect to multiple steps of decomposition is quite interesting."
                },
                "weaknesses": {
                    "value": "1 the paper is a collection of existing approaches, the contribution is a bit incremental and the novelty is a bit limited. \n\n2 the effectiveness of the proposed approach is not quite conclusive yet. \n\n- Table 1 the conclusion is sampling and conditional reasamping should use different sampling approach, i.e. CoT + Subq (QG) or Subq (QG) + CoT. However, the improvement is rather incremental (i.e. 73-> 73.99). Especially considering SOTA of GSM8K IS 90+ https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k (although we understand the foundation models are different, the effectiveness of the approach is not clear)\n\n- Table 2 \u201cindependent sampling\u201d combines Subq (QG) and CoT (74.90) give the best performance than \u201cconditional sampling\u201d (73.99 table 1), which makes me unclear of the effective of conditional reasoning (i.e. combine two samplings are easy and just do majority vote on, i.e. no need to ask LLM whether to resample or not)\n\n- The right half of Tab. 2 shows Selection between the Sampled and Conditionally Resampled. Does that mean the selection module doesn\u2019t bring significant gain?"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1115/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699255488759,
            "cdate": 1699255488759,
            "tmdate": 1699636037594,
            "mdate": 1699636037594,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ab4PmHCVZY",
                "forum": "Oho3UxCkKr",
                "replyto": "Li0mSujT7r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1115/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1115/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Main Contribution"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our work. \n\n1. The paper proposed heterogeneous sampling as an approach to refinement, and later a choice between the refined answer and the original prediction as an option to \"roll back\" in case of errors in refinement. These two components are missing in all previous work. The proposed framework allows different previous strategies to mix and match different components to get the best refinement strategy for a given task. The framework is one of the contributions, but we propose ways to improve reasoning during refinement (main contribution). \n\nSoTA is definitely 90+ percent, and we also achieve 93+ accuracy in our work (see the *Larger LLM* part in the *Additional Analysis* section). \n\n2. Self-Refine (https://arxiv.org/abs/2303.17651) and more recent work (https://arxiv.org/abs/2310.01798) show that reasoning tasks cannot be improved by *direct refinement*, and often lead to worse results. Our component of heterogeneous resampling and selection not only improves refinement scores, but can be done at much lower cost (refinement is not always needed by judging when to refine). \n\nAlthough independent sampling gives the best performance, it resamples all the time. If we want a low-cost method, we can resample only when needed. But if budget is not a constraint, one should prefer independent sampling. Both are possible with our framework."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1115/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076408991,
                "cdate": 1700076408991,
                "tmdate": 1700076408991,
                "mdate": 1700076408991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]