[
    {
        "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation"
    },
    {
        "review": {
            "id": "q1mZdj1aBp",
            "forum": "EmQSOi1X2f",
            "replyto": "EmQSOi1X2f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9113/Reviewer_gQAk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9113/Reviewer_gQAk"
            ],
            "content": {
                "summary": {
                    "value": "This work address a specific hallucination problem in large language models: self-contradition. More specifically, the authors proposed prompt-based approach, including generating, detecting and revising generated text. Extensive analysis shows prevalence of self-contradictions when LMs generate text for open-domain topics and proposed detection and mitigation method are shown to be effective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The motivations of the paper is important.\n- This paper proposes a simple prompt-based method for triggering, detecting and migitating self-contradictions.\n- The authors proposed a new eval human-annotated dataset.\n-The authors provide code for reproducibility.\n- Although authors demonstrate the proposed method in open-domain text generation task, same approach could be applied to other NLG tasks.\n- The writing is clear"
                },
                "weaknesses": {
                    "value": "- Correct me if I'm wrong, the authors only evaluated aLM to detect output from gLM, how robust are aLMs to text generation tasks? that are not generated by \"generating initial text, defining context and trigger\" process? \n\n- Last highlight in Revise, Section 5 is wrong."
                },
                "questions": {
                    "value": "1. does the authors try other temperature in between? 0.5, 0.7 for both aLM and gLM?\n\n2. please refer to 1st point in **Weakness** section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9113/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9113/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9113/Reviewer_gQAk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779143145,
            "cdate": 1698779143145,
            "tmdate": 1700631347628,
            "mdate": 1700631347628,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ezI9wXvmTy",
                "forum": "EmQSOi1X2f",
                "replyto": "q1mZdj1aBp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We would like to thank reviewer gQAk for providing insightful feedback. We hope that our rebuttal has addressed all of the reviewer\u2019s questions, are happy to provide more details, and look forward to the reviewer\u2019s response to our rebuttal.\n\n> ### Is aLM robust to another text generation task?\n\nYes. We have added an evaluation on question answering, with the PopQA benchmark [1] suggested by reviewer a7zQ. The results show that our method can accurately detect a large number of self-contradictions, for both vanilla and retrieval-augmented generation. More details can be found in the global response.\n\n> ### Can you provide evaluation on different temperature values, for both aLM and gLM?\n\nYes. We have provided this evaluation in the global response. The results show that across various temperatures (0, 0.25, 0.5, 0.75 and 1.0), self-contradictions are consistently prevalent and our method for detecting self-contradictions is robust.\n\n> ### Is the last highlight in paragraph Revise of Section 5 wrong?\n\nWe have re-checked it and are confident that it is correct. According to https://wiki.factsider.com/william-t-freeman/, Freeman\u2019s birth country is indeed the USA. If the reviewer still disagrees, we kindly request the reason.\n\n[1] When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. ACL 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147048629,
                "cdate": 1700147048629,
                "tmdate": 1700147048629,
                "mdate": 1700147048629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mI425kY36H",
                "forum": "EmQSOi1X2f",
                "replyto": "ezI9wXvmTy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9113/Reviewer_gQAk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9113/Reviewer_gQAk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing the rebuttal.\n\nThe highlight section referred in the original comment is the green highlight section of \"a faithful and fluent sentence\". now after several thoughts, the highlight section is indeed correct. They were a bit confusing at first since the highlighted section is a description not math symbols.\n\nThanks for the authors for providing additional experiments on temperature and on QA task. This solves my concerns.\n\nAlthough I acknowledge the remaining concerns of reviewer 3teD is valid, there is still values of current work, and i therefore increased the rating of the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631318915,
                "cdate": 1700631318915,
                "tmdate": 1700631318915,
                "mdate": 1700631318915,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ej1G5bReil",
            "forum": "EmQSOi1X2f",
            "replyto": "EmQSOi1X2f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9113/Reviewer_a7zQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9113/Reviewer_a7zQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a prompting-based method to detect and mitigate self-contradictory hallucinations. The proposed method generates texts based on a given prompt and subsequently evaluates these for self-contradiction by generating additional sentences. The text is then iteratively revised with the result of self-contradiction. Experimental results involving four LLMs, including GPT-4 and Llama-2-70B, suggest that the proposed approach effectively addresses self-contradictions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clearly articulated and well-written. Moreover, it offers extensive experimentation with a range of LLMs, demonstrating a solid empirical foundation."
                },
                "weaknesses": {
                    "value": "The primary issues with this paper include:\n\n- The paper's central premise is that LLMs can produce self-contradictory hallucinations, and the proposed method aims to rectify using a prompting-based approach with LLMs. While it's acknowledged that LLMs can operate as zero-shot reasoners, their potential to induce hallucinations is also evident. Asserting that the authors correct LLM-induced hallucinations using LLMs (without external knowledge) appears paradoxical. This paper should incorporate a theoretical analysis explaining the efficacy of the proposed method in mitigating hallucinations.\n\n- In the experiments, the verification of the text against Wikipedia is assessed, and various ratios are presented. However, a more in-depth exposition of the experimental setup should be shown. Specifically, manual checks were mentioned, but it's questionable whether the annotators thoroughly scanned the entirety of Wikipedia.\n\n- While the paper claims that self-contradictions can be tackled without relying on external knowledge, it does utilize an \"external\" information extraction system to gather context.\n\n- The term \"topic\" is employed in the paper, but Figure 4 indicates that experiments focused on specific entities. It seems that this paper is focusing on entities than on broader topics. If the text generated is primarily entity-centric, perhaps the experiments should have considered data from existing entity-centric QA research [1] even if the paper's emphasis on the open-domain text generation.\n\n- The proposed iterative mitigation algorithm (Algorithm 3) requires an input n, which represents the number of mitigation iterations. The value of n seems to fluctuate based on the size of |x|. The paper should elucidate the chosen value for n and expound on how mitigation performance is influenced by this value.\n\n[1] When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories, ACL 2023"
                },
                "questions": {
                    "value": "Q1. How does the annotator manually verify contradictory information using Wikipedia?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9113/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9113/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9113/Reviewer_a7zQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806156764,
            "cdate": 1698806156764,
            "tmdate": 1700734271860,
            "mdate": 1700734271860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f6AgMocXta",
                "forum": "EmQSOi1X2f",
                "replyto": "ej1G5bReil",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We would like to thank reviewer a7zQ for providing insightful feedback. We hope that our rebuttal has addressed all of the reviewer\u2019s questions, are happy to provide more details, and look forward to the reviewer\u2019s response to our rebuttal.\n\n> ### Why does your mitigation even work at all?\n\nOur mitigation approach **does not correct** hallucinations. Instead, it **removes** contradictory information that already exists in the two sentences. Our removal can be achieved by comparing the two sentences and does not rely on external knowledge. For instance, in Figure 3, the two sentences provide conflicting information about the birth date for William T. Freeman. This inconsistency can be removed without knowing Freeman\u2019s actual birth date, by dropping the information about the birth date entirely from the sentence. It is rare but not impossible that LLMs introduce a new hallucination when generating the revised sentence. Since our mitigation algorithm is iterative, these new hallucinations are resolved in subsequent iterations.\n\nOur removal approach offers a novel perspective on mitigating hallucinations compared to the traditional correction approach. The motivation is to prevent LMs from generating mistakes when they are uncertain about specific facts. It aligns well with machine learning approaches that incorporate a reject or abstention option [2].\n\n> ### How do the annotators manually verify contradictory information using Wikipedia?\n\nThe annotators read relevant Wikipedia articles and perform verification through a combination of human understanding and text searches. To enhance coverage, they are asked to examine various articles: (i) articles of all entities in the contradictory sentences; (ii) articles of relevant entities mentioned in (i); (iii) If non-English articles provided more information than English counterparts, the annotators used Google Translate to review non-English versions translated to English. To improve annotation accuracy, each piece of contradictory information must be examined by two annotators independently. Then, the two annotators discuss to resolve inconsistent cases and reach final decisions.\n\nWe believe that the above protocol has resulted in high-quality verification annotations. We will incorporate this discussion into the paper.\n\nUPDATE: As suggested by reviewer 3teD, we additionally ask the annotators to use web search and examine broader online text. However, since Wikipedia already has a high coverage, the unverifiability numbers do not change significantly and our claims still hold.\n\n> ### Do you consider the information extraction system as external grounded knowledge?\n\nNo. First, the information extraction system CompactIE does not retrieve any additional, external knowledge at any point. It only sees the sentences generated by gLM as input and outputs fact triples that are guaranteed to be spans of the input sentences. For example, from the sentence \u201cX does Y\u201d, it would extract the triple (\u201cX\u201d, \u201cdoes\u201d, \u201cY\u201d). Second, CompactIE is based on a small BERT model with 110M parameters [3]. In contrast, external grounded knowledge typically refers to input-dependent text retrieved from considerably larger knowledge bases, such as Wikipedia. We will update the paper to clarify this. \n\n> ### For your work, \u201centity\u201d seems a better term than \u201ctopic\u201d.\n\nWe agree and will update the paper to replace \u201ctopic\u201d with \u201centity\u201d.\n\n> ### Can you provide experiments on PopQA [1], an existing entity-centric QA dataset?\n\nYes. We have provided this experiment in the global response. The results show that our method can accurately detect a large number of self-contradictions, for both vanilla and retrieval-augmented generation.\n\n> ### How is mitigation performance influenced by the value of n?\n\nThe initial submission includes an ablation study on this. Please refer to the paragraph \u201cAblation Study on Mitigation\u201d in Appendix B. Our ablation study shows that each iteration progressively removes self-contradictions, while preserving fluency and informativeness. The mitigation converges quickly after three iterations, which is why we choose $n=3$.\n\n> ### The value of n seems to fluctuate based on the size of |x|. \n\nWe believe that there is a misunderstanding. The value of n is fixed at $n=3$ throughout all experiments presented in the paper and we do not intend to suggest at any point that n should be adapted based on |x|.\n\n[1] When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. ACL 2023\n\n[2] Machine Learning with a Reject Option: A survey. arXiv:2107.11277\n\n[3] CompactIE: compact facts in open information extraction. NAACL 2022"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147013663,
                "cdate": 1700147013663,
                "tmdate": 1700680922218,
                "mdate": 1700680922218,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2lIpf7Bnbq",
                "forum": "EmQSOi1X2f",
                "replyto": "f6AgMocXta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9113/Reviewer_a7zQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9113/Reviewer_a7zQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\n\nSome of my concerns have been addressed, leading me to increase the score. However, I still have concern regarding the use of external knowledge in the information extraction (IE) system. Despite the size of the IE model, the proposed approach involves utilizing external knowledge. Therefore, it's not accurate to claim that these issues can be resolved without external knowledge."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734256572,
                "cdate": 1700734256572,
                "tmdate": 1700734256572,
                "mdate": 1700734256572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zBNBcjMoVI",
            "forum": "EmQSOi1X2f",
            "replyto": "EmQSOi1X2f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9113/Reviewer_3teD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9113/Reviewer_3teD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a pipeline to detect self-contradictory hallucinations generated by LLMs. Specifically designed prompts are used to ask LLMs to generate alternative answers to a question. Then an analysis LLM is asked to detect the potential contradiction between two answers and to revise them (to remove the contradiction).\n\nThe method is tested on a set of 30 topics and 360 descriptions generated by LLMs - a dataset created by the authors. The generated answers are verified manually. The method is compared with several baselines using other prompts. The results show that the proposed method can better detect contradictions than other baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper examines an important problem of LLMs - hallucination. Despite the huge impact of the problem, there is a limited number of studies about the its detection. This paper offers an interesting solution to one type of hallucination.\n\nThe proposed method relies on existing LLMs. It is easy to implement and to replicate.\n\nThe authors provide a dataset and the tool that can be reused by other researchers.\n\nThe experimental results show some level of success with the proposed method, which outperforms several other pipelines (prompts).\n\nSome ablation analysis is provided about the impact of different LLMs used in the generation and analysis steps."
                },
                "weaknesses": {
                    "value": "The proposed method may detect one type of hallucination where alternative answers are contradictory. In reality, many hallucinations do not contain verifiable contradictions. The coverage of the method on different types of hallucination may be limited.\n(edit after rebuttal: This is partially explained in the rebuttal)\n\nThe experiments are performed on a limited number of cases. Although a larger set of data is created, it cannot be used in a strict evaluation because of the lack of manual verification. I acknowledge the potential high cost for manual evaluation. However, a larger set of cases would make the experiments more meaningful. One possible solution is to leverage the existing datasets of hallucination. For example, a recent large dataset HalluEval (arXiv: 2305.11747) could be possibly used.\n(edit after rebuttal: The number of topics used in the test is still limited. Therefore, the coverage is still questionable.)\n\nThe experimental results may suggest that there is a strong relationship between gLM and aLM. When ChatGPT is used as both, we can see that the performance is higher in most cases. This contradicts the intuition that GPT-4 is a more powerful LLM and should perform better than ChatGPT. Unfortunately, the paper does not report the case of GPT-4 as both gLM and aLM to see if the combination of GPT-4+GPT-4 is better than ChatGPT + ChatGPT. In any case, the fact that the combination of the same LLM for the two steps is better than when different LLMs are used for the steps may suggest that it would be better to use the same LLM for both steps. Some analysis is required to better understand the relationship between the two steps.\n(edit after rebuttal: The answer in the rebuttal does not provide a plausible reason for this. It just restates the experiments.)\n\nThe revision tries to remove the contradictory part of the answers. This is a conservative revision. If one of the answer is correct, it would be better to keep it instead of removing it. From this perspective, it would be useful to refer to some external information source (e.g. Wikipedia). The authors argue that not doing so can make the approach independent of any external resource, but this may also make the method less reliable. Indeed, looking at the example of William T. Freeman, the generated birth dates are all wrong. A comparison with Wikipedia data would be able to tell that. An external resource would be valuable to the detection of hallucination.\n(edit after rebuttal: Despite the explanation in the rebuttal, there is still not a strong motivation for the revision process.)\n\nYou mention that \"a substantial portion of these self-contradictions (e.g., 35.8% for ChatGPT) cannot be verified on Wikipedia\" to motivate the decision not to rely on external resources. However, you can verify on a much broader set of external resources. So the argument not to use an external resource is not very strong.\n(edit after rebuttal: It is true that Wikipedia has a good coverage for general topics. It may miss many more recent or local events. Other web documents may be useful. Why not using web search to find possibly relevant information?)\n\nThe following description about informativeness is unclear (please revise): For informativeness, we calculate the ratio of non-contradictory sentence pairs (i.e., informative facts) in the revised text compared to the original text. Note that this ratio might exceed 100% because our mitigation can revise contradictory sentence pairs into non-contradictory ones. \n(edit after rebuttal: the revised definition of informativeness is clearer.)\n\nThe paper proposes some reasonable prompts to generate answers and verify contradictions. The proposed prompts should be further motivated. The authors have not provided strong reasons to choose the specific prompts. The only way to see their superiority is through the comparisons in experiments with other alternative prompts.\n(edit after rebuttal: The explanation about the prompts is reasonable. It still remains unclear why these specific prompts are chosen instead of other alternatives.)\n\nThe papers target long text generation in contrast to short answers targeted by some other papers. Why is the length of answers a key factor that distinguish this work from a set of existing studies? Would it still be possible to compare the proposed method with others on short answers?\n(edit after rebuttal: If the method can be applied to both long and short texts, the paper should not emphasize the ability of the method for long texts. In fact, no specific means is taken to deal with long texts vs short texts.)"
                },
                "questions": {
                    "value": "Have you tested with different temperatures in decoding? This may produce different answers.\n\nCan you compare the method with other methods that leverage external resources?\n\nWould it be possible and meaningful to compare the proposed method with others on short answers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9113/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9113/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9113/Reviewer_3teD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699328268894,
            "cdate": 1699328268894,
            "tmdate": 1700609561332,
            "mdate": 1700609561332,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QUKdgiEPvz",
                "forum": "EmQSOi1X2f",
                "replyto": "zBNBcjMoVI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Part 1"
                    },
                    "comment": {
                        "value": "We would like to thank reviewer 3teD for providing insightful feedback. We hope that our rebuttal has addressed all of the reviewer\u2019s questions, are happy to provide more details, and look forward to the reviewer\u2019s response to our rebuttal.\n\n> ### Limitation and significance of self-contradiction.\n\nWe agree with the reviewer that our work does not solve all types of hallucinations. In fact, we have explicitly stated this limitation in the initial submission (at the end of Section 7). However, we still believe our contribution is significant, because self-contradiction occurs frequently and our approach complements existing methods that retrieve external resources. More on the latter  is provided in subsequent answers and the global response.\n\n> ### Can you extend your evaluation to another existing dataset?\n\nYes. We have added another evaluation on PopQA [1], a question-answering benchmark suggested by reviewer a7zQ. We have also performed human annotation for this experiment. The results show that our method can precisely detect a large number of self-contradictions, for both vanilla and retrieval-augmented generation. More details can be found in the global response and will be discussed in subsequent answers.\n\n> ### How comprehensive is your human-labeled evaluation dataset?\n\nWhile having added more evaluation data, we would like to emphasize that our current human-labeled dataset is already comprehensive. It covers 4 different LLMs, meaning that we perform separate annotations for each LLM. To address sampling variance, we generate and annotate 3 descriptions for each entity. Furthermore, to improve quality, each annotation label results from a joint effort of 2 annotators.\n\n> ### Can you provide more analysis on the relationship between aLM and gLM?\n\nYes. The following table presents the detection results with all four combinations of ChatGPT and GPT-4 as gLM and aLM. We can observe (i) gLM and aLM should be different to achieve optimal F1 scores: ChatGPT-GTP-4 outperforms ChatGPT-ChatGPT, and GPT-4-ChatGPT outperforms GPT-4-GPT-4; (ii) Compared to ChatGPT, GPT-4 generates content that is more nuanced and more difficult to classify: in terms of F1 score, ChatGPT-ChatGPT outperforms GPT-4-ChatGPT, and ChatGPT-GPT-4 outperforms GPT-4-GPT-4.\n\n| gLM     | aLM     | P     | R     | F1    |\n|---------|---------|-------|-------|-------|\n| ChatGPT | ChatGPT | 84.2% | 83.2% | 83.7% |\n| ChatGPT | GPT-4   | 91.3% | 82.1% | 86.5% |\n| GPT-4   | ChatGPT | 80.1% | 79.7% | 79.9% |\n| GPT-4   | GPT-4   | 88.3% | 65.7% | 75.3% |\n\n> ### How does your conservative mitigation approach position in ML literature?\n\nOur approach for removing contradictory information offers a novel perspective on mitigating hallucinations compared to the traditional correction approach. The motivation is to prevent LMs from generating mistakes when they are uncertain about specific facts. It aligns well with machine learning approaches that incorporate a reject or abstention option [9].\n\n> ### If one of the answers is correct, it would be better to keep it instead of removing it.\n\nWe acknowledge the value of keeping the original answers, and indeed, we do have such a feature in our tool at https://iclr9113.com/. Users can easily access the original answers by clicking on the revised answer.\n\n> ### What is your argument on the relationship between your method and other methods that retrieve external resources?\n\nThe reviewer may have a misunderstanding of our paper, thinking that we advocate for an exclusive choice between our approach and methods that retrieve external resources. To clarify, our paper actually only argues that our approach **complements** retrieval-based methods. In other words, while these two approaches can be used alone, they can also co-exist and their combination can yield improved performance. For more details, please search for the word \u201ccomplement\u201d in our initial submission or examine the paragraph \u201cSelf-contradiction vs. Knowledge Retrieval\u201d.\n\nWe fully appreciate and agree with the reviewer on the value of retrieval-based methods. However, it is equally important to recognize their limitations, which have been extensively discussed [1, 2, 3, 4]. First, retrieval can be infeasible or prohibitively expensive in certain domains. Second, the retrieved information can be insufficient, inaccurate, or misleading, due to various factors such as the quality of the knowledge base or the retriever\u2019s imprecision. As shown in our paper, even a resource as comprehensive as Wikipedia falls short in verifying a large portion of self-contradictions. Therefore, our approach serves as a valuable complementary solution to address these shortcomings.\n\nWe have also strengthened the above argument experimentally with the PopQA benchmark [1], which can be found in the global response and covers self-contradiction detection on top of retrieval augmented model output."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147352924,
                "cdate": 1700147352924,
                "tmdate": 1700147352924,
                "mdate": 1700147352924,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "isE4JZZK7s",
                "forum": "EmQSOi1X2f",
                "replyto": "OJ9tUUhUg7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9113/Reviewer_3teD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9113/Reviewer_3teD"
                ],
                "content": {
                    "title": {
                        "value": "reaction to rebuttal"
                    },
                    "comment": {
                        "value": "The rebuttal provides some explanation to the work. The additional experiments on temperature and on QA are good, showing that the method can be used in a different setting.\n\nStill, the explanation about the revision process, the lack of use of retrieval results and the justification of the specific prompts is not very convincing.\n\nIt would also be useful to compare the method with others that deal with hallucinations for short texts. This should be possible, as the authors acknowledge that the method can be used for short texts. Then how does it compare to other existing methods?\n\nSee the reactions in the review after rebuttal.\n\nDespite the remaining questions, I acknowledge the value of the additional tests and some useful explanations in the rebuttal. I therefore increased the rating of the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609932231,
                "cdate": 1700609932231,
                "tmdate": 1700609932231,
                "mdate": 1700609932231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]