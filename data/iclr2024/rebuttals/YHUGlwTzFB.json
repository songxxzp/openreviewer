[
    {
        "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm"
    },
    {
        "review": {
            "id": "mhiqHpsTMd",
            "forum": "YHUGlwTzFB",
            "replyto": "YHUGlwTzFB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4620/Reviewer_yeNU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4620/Reviewer_yeNU"
            ],
            "content": {
                "summary": {
                    "value": "This paper points out that achieving domain generalization is theoretically impossible without additional information. Therefore, this paper introduces active test-time-training(ATTA), combining active learning with test-time-training, and proposes an effective ATTA algorithm, SimATTA, that innovatively integrates incremental clustering and selective entropy minimization to address catastrophic forgetting and real-time active sample selection issues."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\uff081\uff09This paper innovatively combines active learning with TTA, enhancing performance across test domains. and present sample entropy balancing to avoid catastrophic forgetting.\n\uff082\uff09The paper conducted extensive experiments and compared with the latest state-of-the-art methods on multiple datasets, achieving superior results.\n\uff083\uff09The paper is well-organized, and it provides extensive proofs for the theorems mentioned."
                },
                "weaknesses": {
                    "value": "1. In Section 3.2, the authors state that entropy is essentially a measure of the distribution distance between the model distribution and a test sample, which is not true. While entropy can provide information about the uncertainty of the model, it does not directly measure the distributional distance between the model's distribution and the test samples. This fundamental flaw casts doubt on the proposed method.\n2. Table 3 indicates that, in terms of efficiency, SimATTA takes longer than all previous methods, making it less efficient.\n3. In Appx. H.2., when B<=500, the performance of SimATTA is close to that of other methods, with no distinct advantage\n4. The ablation study is not comprehensive enough to effectively prove the efficacy of both the incremental clustering and selective entropy minimization methods."
                },
                "questions": {
                    "value": "Entropy does not directly measure the distributional distance between the model's distribution and the test sample\uff0cso the theoretical foundation presented in the paper is shaky. How do you view this issue?\n\nAfter reading the rebuttal, I decided to raise my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4620/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4620/Reviewer_yeNU",
                        "ICLR.cc/2024/Conference/Submission4620/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4620/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743928308,
            "cdate": 1698743928308,
            "tmdate": 1700806084512,
            "mdate": 1700806084512,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gdJAqVH1lm",
                "forum": "YHUGlwTzFB",
                "replyto": "mhiqHpsTMd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yeNU (Part I)"
                    },
                    "comment": {
                        "value": "Dear Reviewer yeNU,\n\nThank you for your constructive comments and valuable suggestions! We have revised the paper following your suggestions and we provide responses for each concern here.\n\n> W1. In Section 3.2, the authors state that entropy is essentially a measure of the distribution distance between the model distribution and a test sample, which is not true. While entropy can provide information about the uncertainty of the model, it does not directly measure the distributional distance between the model's distribution and the test samples. This fundamental flaw casts doubt on the proposed method.\n\nThank you for raising this point! We realize the potential misconception here and have revised the section for more rigorous descriptions following your suggestion. However, we would like to clarify that whether entropy directly measures a distributional distance does not impair the theoretical foundation in Sec. 3.2 and the later proposed method.\n\n- **Relationship between low-entropy samples and source samples:** The core point of Sec. 3.2 and mitigating catastrophic forgetting is the **assumption** that the low-entropy sample distribution selected by the source-pretrained-model, $D_{\\phi,S}(t)$, serves the model training similarly to source samples in terms of CF.\nWhen a model assigns low entropy to a sample, this indicates a high level of certainty or confidence in its prediction and can be interpreted as the sample being well-aligned or fitting closely with the model's learned distribution.\nThe pre-trained model is well-trained on abundant source data, and thus low entropy can be used as an indicator of a sample closely aligned with the model distribution, in this case, a source-like sample.\nThese source-like samples form a distribution that fits/supports the original model predictions. Intuitively, these supports prevent the model from violating the original high-confident predictions, thus maintaining the model's original distribution. This behavior is different from minimizing the distance between the current model parameters and the original model parameters. Instead, these samples only serve as the original model's \"anchor points\" to avoid catastrophic forgetting. More detailed explanations and clarification are in Appx.D.\n- **Theoretical validity:** With no further condition than the above assumption, the theoretical results in Sec 3.2 and the method design in Sec 4 hold, giving credence to the validity of our paper.\n- **Bridging the theory and practical implementations:** In Table 1, we **empirically validate our assumption** that low-entropy samples, similar to samples from the source distribution, can serve to avoid catastrophic forgetting. This assumption with its empirical validation is the bridge between our theorems and our practical implementations.\n\n\n> W2. Table 3 indicates that, in terms of efficiency, SimATTA takes longer than all previous methods, making it less efficient.\n\nWe have revised Sec 5 to add information and avoid misunderstandings.\n- Regarding Table 3, we'd like to clarify that ATTA is of very similar time efficiency with SAR on Tiny-ImageNet-C (time per step). ATTA does not have time-consuming steps, instead the training time is due to the number of interaction steps till convergence. Since SimATTA's training is statistically significant instead of one sample training, several training steps are necessary to make use of information and improve performance. This is a trade-off between effectiveness and efficiency, and we believe the performance improvement is worth this training.\n- **Real-world application:** In our opinion, many real-time applications like robotics, autonomous vehicles, medical imaging adaptations, and user interface personalizations, are not very time-sensitive. For example, retraining a personalized user interface recommendation system may require hours, days, or weeks, but ATTA can fine-tune the model in a second scale, which should be considered a real-time manner. As we have demonstrated (detailed in Appx B.Q8), the speed of ATTA is sufficiently fast for an autopilot system, and the speed requirements of the other applications listed above are safely within the limit of ATTA. Therefore, we believe ATTA is a real-time setting."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4620/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384380927,
                "cdate": 1700384380927,
                "tmdate": 1700384380927,
                "mdate": 1700384380927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d5fpjNSW7q",
            "forum": "YHUGlwTzFB",
            "replyto": "YHUGlwTzFB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4620/Reviewer_5EyE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4620/Reviewer_5EyE"
            ],
            "content": {
                "summary": {
                    "value": "The paper titled \"Active Test-Time Adaptation: Theoretical Analyses and An Algorithm\" delves into a novel approach for machine learning models that dynamically adapt during test-time  under domain shifts. Traditionally, models rely heavily on heuristic and empirical studies without further adaptation. The authors challenge this convention by introducing a mechanism that allows the model to actively query an oracle (typically a human) during test-time to obtain labels for certain instances. The objective is to enhance the model's performance on the test set by leveraging this limited interaction with the oracle.\n\nThe primary contributions of the paper are as follows:\n\n1. Introduction of the Active Test-Time Adaptation (ATTA) framework.\n2. A novel algorithm for determining which instances should be queried from the oracle, based on their potential impact on the model's performance.\n3. Experimental validation of the ATTA framework on several benchmark datasets, demonstrating significant improvement in performance over non-adaptive baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The concept of actively adapting a model during test-time based on interactions with an oracle is innovative. This breaks away from the conventional train-test paradigm, paving the way for more dynamic and adaptive models.\n\n2. The authors provide a theoretical foundation for the ATTA framework, making a compelling case for its viability and potential benefits.\n\n3. The proposed method is model-agnostic, meaning it can be applied to a wide range of machine learning algorithms, from simple linear classifiers to complex deep learning architectures.\n\n4. The extensive experiments on benchmark datasets provide strong empirical evidence supporting the effectiveness of the ATTA framework. The improvements over non-adaptive baselines are both statistically significant and practically relevant."
                },
                "weaknesses": {
                    "value": "1. The ATTA framework's effectiveness hinges on the availability and accuracy of an oracle. In real-world scenarios, obtaining such an oracle (especially a human expert) might be challenging, time-consuming, or expensive.\n\n2. While the approach shows promise on benchmark datasets, its scalability to very large datasets or real-world applications remains untested. The computational overhead of deciding which instances to query and updating the model during test-time could be prohibitive in some scenarios.\n\n3. The paper assumes a limited budget of queries to the oracle. In many real-world scenarios, determining this budget or ensuring its strict adherence might be challenging.\n\n4. Continually adapting the model during test-time based on feedback from the oracle could lead to overfitting, especially if the test set is not representative of the broader data distribution."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4620/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698891505642,
            "cdate": 1698891505642,
            "tmdate": 1699636441183,
            "mdate": 1699636441183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CXCBpVTIdr",
                "forum": "YHUGlwTzFB",
                "replyto": "d5fpjNSW7q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5EyE (Part I)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5EyE,\n\nThank you for your constructive comments and valuable suggestions! We have made paper revisions following your suggestions and would like to provide responses for each concern here.\n\n> W1. The ATTA framework's effectiveness hinges on the availability and accuracy of an oracle. In real-world scenarios, obtaining such an oracle (especially a human expert) might be challenging, time-consuming, or expensive.\n\n- Reliance on an oracle or expert input is a fundamental aspect of active learning, which is a long-standing field. While obtaining an oracle might be challenging or expensive, the cost should be **weighed against the benefits** of significantly improved model performance and reduced need for vast amounts of labeled data.\n- **Application of active strategies:** In many practical situations, the benefits of active learning outweigh the challenges of obtaining an oracle. For instance, in medical diagnosis, scientific research, or complex engineering problems, the cost and effort of consulting an expert are justified by the significant improvements in model accuracy and efficiency. Technological advancements, such as semi-automated systems or sophisticated algorithms, can also serve as oracles or assist human experts, making the process more feasible and efficient.\n- **Application of ATTA:** ATTA has its specific application scenarios. One potential application direction is for human-machine cooperated semi-automated applications, where we can obtain passive oracle input without extra efforts. One example is autopilot systems we mentioned in our FAQ (Appx B.Q8: What is the potential practical utility of ATTA?).\n- Finally, we regard this point as a general challenge of active learning rather than a drawback of the ATTA framework itself. Therefore, it can be viewed as a consideration/limitation in the application. We specify this discussion in Appx.J to add rigor to the paper.\n\n> W2. While the approach shows promise on benchmark datasets, its scalability to very large datasets or real-world applications remains untested. The computational overhead of deciding which instances to query and updating the model during test-time could be prohibitive in some scenarios.\n\n- **Time complexity:** As in Table 3, ATTA is of very similar time efficiency with TTA method SAR (time per step). ATTA does not have time-consuming steps, instead the training time is due to the number of interaction steps till convergence. This training time is a trade-off for significant improvements in performances.\n- **Real-world application:** In our opinion, many real-time applications like robotics, autonomous vehicles, medical imaging adaptations, and user interface personalizations, are not very time-sensitive. For example, retraining a personalized user interface recommendation system may require hours, days, or weeks, but ATTA can fine-tune the model in a second scale, which should be considered a real-time manner. As we have demonstrated (detailed in Appx B.Q8), the speed of ATTA is sufficiently fast for an autopilot system, and the speed requirements of the other applications listed above are safely within the limit of ATTA. Therefore, we believe ATTA is a real-time setting.\nRigorously, we acknowledge that the application scopes of ATTA and FTTA are not completely overlapping. FTTA can handle applications that are extremely time-sensitive, while ATTA provides results of much higher quality.\n- Finally, our paper focuses on catastrophic forgetting, domain shift solving, and the establishment of a new setting. We conduct experiments on domain generalization datasets and emphasize the comparisons for different settings. We acknowledge that scalability is not a focus of this paper. Therefore, we consider this valuable suggestion as a possible future direction instead of a weakness, which we discuss in FAQ (Appx.B.Q10: What are not covered by this paper?)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4620/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383324604,
                "cdate": 1700383324604,
                "tmdate": 1700384012735,
                "mdate": 1700384012735,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZdFuEs8nqb",
            "forum": "YHUGlwTzFB",
            "replyto": "YHUGlwTzFB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4620/Reviewer_BXz7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4620/Reviewer_BXz7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel, formal problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning to perform test-time adaptation (TTA). It attempts to mitigate distribution shifts and catastrophic forgetting, while not being provided access to source data, model parameters, or pre-collected target samples. \n\nA theoretical analysis of ATTA in the setting of binary classification is provided. First, the theory establishes a learning bound that has the notions of composition of training data, estimated distribution shift, and ideal joint hypothesis performance. Second, the fore-mentioned theory is utilized to shown that catastrophic forgetting can be mitigated by performing selective sample selection through entropy minimization. \n\nSimATTA, a practical algorithm built upon the ATTA theory, is then developed. It integrates incremental learning and selective entropy minimization techniques. Empirical evaluations on four benchmarks simulating distribution shifts demonstrate the effectiveness of SimATTA when compared to the existing work. It achieves state-of-the-art accuracy under distribution shifts while maintaining the computational complexity that is not significantly higher than that of the prior work."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "$\\textbf{Novelty and significance}$:\nIn my opinion, the empirical results, especially addressing RQ1, clearly set this paper apart from previous research, paving the way to overcome distribution shifts under the TTA setting. The proposed algorithm, SimATTA, significantly surpasses the existing TTA algorithms in terms of performance accuracy under distribution shifts. It also appears to exhibit greater resilience to catastrophic forgetting. Additionally, it is supported by a robust theoretical framework.\n\n$\\textbf{Completeness and comprehensiveness}$:\nThe main manuscript and the supplementary material offer a comprehensive context and detailed information about the proposed work. Furthermore, related work addressing distribution shifts under various settings is also adequately discussed."
                },
                "weaknesses": {
                    "value": "I found no major weakness from this paper. One minor aspect I would like to highlight concerns the clarity of the experimental settings, such as domain-wise data stream, random stream, post-adaptation, and so on. It took me some time to grasp all of these distinct settings. Perhaps including a dedicated section to explain about these settings would be more helpful."
                },
                "questions": {
                    "value": "The \u201cbudget\u201d term appears to be a significant factor in the algorithm. However, I\u2019ve not been able to identify its relationship to the cluster centroid number. Could the authors please provide clarification on this matter?\n\nI\u2019m looking forward to seeing the code implementation of SimATTA."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4620/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699405665259,
            "cdate": 1699405665259,
            "tmdate": 1699636441083,
            "mdate": 1699636441083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8aJwkgohZl",
                "forum": "YHUGlwTzFB",
                "replyto": "ZdFuEs8nqb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BXz7"
                    },
                    "comment": {
                        "value": "Dear Reviewer BXz7,\n\nWe would like to extend our gratitude for your acknowledgment! We have made paper revisions following your suggestions to address your presentation concerns and would like to answer your questions as follows.\n\n> Weakness: One minor aspect I would like to highlight concerns the clarity of the experimental settings, such as domain-wise data stream, random stream, post-adaptation, and so on. It took me some time to grasp all of these distinct settings. Perhaps including a dedicated section to explain these settings would be more helpful.\n\n- Thank you for the valuable suggestion! We have restructured Sec 5 to include a separate paragraph explaining experimental settings, as well as describe the experimental settings in detail in Appx.G.\n\n> Questions: The \u201cbudget\u201d term appears to be a significant factor in the algorithm. However, I've not been able to identify its relationship to the cluster centroid number. Could the authors please provide clarification on this matter?\n\n- Thank you for the question. We have revised descriptions to improve clarity. We also explain here.\n- The number of selected samples for each minibatch is decided jointly by the incremental clustering algorithm, the cluster centroid number $NC(t)$, and the stream in-coming distributions. Intuitively, this sample selection is a dynamic process, with **$NC(t)$ restricting the budget** and incremental clustering performing sample selection. For each batch, we increase $NC(t)$ as a **maximum limit**, while the exact number of the selected samples is given by the incremental clustering (by how many clusters are located in the scope of new distributions). *E.g.*, if the incoming batch does not introduce new data distributions, then we select zero samples even with increased $NC(t)$. In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering \"wants\" to select more samples than the $NC(t)$ limit, but is forced by the limit to merge multiple previous clusters into one new cluster. Therefore, $NC(t)$ can restrict the budget but cannot determine the budget alone since the budget is also strongly affected by the data stream distributions.\n- The incremental clustering is detailed in Alg 2 (Appendix F), and $NC(t)$ is naively increased by a constant hyper-parameter $k$. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by $k$, which is also the reason why we compare methods under a budget limit as shown in Table 2.\n\n> I'm looking forward to seeing the code implementation of SimATTA.\n\n- We would like to provide our code through this anonymous link: https://anonymous.4open.science/r/tta-C807/\n\nBesides the above points, we also provide further experiments and make an effort to clear the concerns and improve the clarity and organization of our paper in multiple aspects following the advice from other reviewers. We add visualization experiments and extend the ablation studies. Please refer to the revised paper and appendices for details.\n\nWe sincerely thank you for your time! Hope we have addressed your concerns through practical efforts and shown the significance of our work. We look forward to your reply and further discussions, thanks!\n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4620/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383169085,
                "cdate": 1700383169085,
                "tmdate": 1700383700503,
                "mdate": 1700383700503,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qcrVOTeLuJ",
                "forum": "YHUGlwTzFB",
                "replyto": "8aJwkgohZl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4620/Reviewer_BXz7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4620/Reviewer_BXz7"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the clarifications"
                    },
                    "comment": {
                        "value": "I appreciate the authors for addressing my concerns and offering greater clarity."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4620/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578437252,
                "cdate": 1700578437252,
                "tmdate": 1700578437252,
                "mdate": 1700578437252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mxERJIJBMH",
            "forum": "YHUGlwTzFB",
            "replyto": "YHUGlwTzFB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4620/Reviewer_cDV9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4620/Reviewer_cDV9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new setting, namely ATTA (Active Test-Time Adaptation), to integrate active learning (a limited number of labeled test samples) within test-time adaptation (TTA) to enhance TTA under domain shifts. The problem itself is practically important and the paper presents a theoretical guarantee and a sound solution (SimATTA) that combines incremental clustering and entropy selection to conduct online sample selection, avoiding catastrophic forgetting issues. The simplicity of the proposed algorithm and its successful application on real datasets are commendable. A thorough empirical study (including ablations) with encouraging results (both in the paper and its supplementary material) shows the effectiveness of the approach in terms of its effectiveness and generalization performance. The paper's clear contribution to the TTA community is evident, and I recommend its acceptance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "**Originality**: The paper introduces a novel setting, active test-time adaptation (ATTA) with theoretical guarantees for alleviating distribution shifts and mitigating catastrophic forgetting and extensive experiments on several benchmarks under domain generalization shifts. Additionally, the Section FAQ & Discussions in supplementary material is highly praiseworthy.\n\n**Quality**: The paper provides a thorough experimental evaluation of the SimATTA algorithm on four datasets (PACS, VLCS, Office-Home, and Tiny-ImageNet-C). The paper also conducts ablation studies to analyze the impact of different components of SimATTA. The paper demonstrates that SimATTA can achieve superior performance and maintain efficiency.\n\n**Clarity**: The paper provides sufficient background information including theory and closely related works to situate the contribution of the ATTA setting and the SimATTA algorithm.\n\n**Significance**: The paper proposes an important and challenging setting of active test-time adaptation (ATTA) and a detailed comparison with related settings (DA/DG, TTA, ADA, ASFDA, and AOL) highlights the value of the proposed ATTA. ATTA also has many potential practical utilities such as an autopilot system and a personalized chatbot discussed in supplementary material."
                },
                "weaknesses": {
                    "value": "**Insufficient visualization**: Though the authors have provided detailed algorithms (Alg. 1 and Alg. 2) to show the proposed SimATTA algorithm, it is still hard to follow the whole picture quickly. Thus it could be better to provide a clear diagram to illustrate the framework of the SimATTA algorithm.\n\n**Insufficient justifications**: For example, regarding the **efficiency** and **applicability** of ATTA, some justifications are missing in this paper. First, as shown in Tab. 3, the time cost of ATTA is around ten times than general FTTA (Tent: 68.83, EATA: 93.14, SimATTA: 736.28). The reason might be the clustering-based selection process and fully fine-tuning pre-trained models? Second, though the authors state that \"ATTA can be applied to any pre-trained models including large language models (LLMs)\", they provide no experimental results. \n\n**Inconsistent results**: results of SimATTA ($B\\le$500) in Table 2 (TTA comparisons on PACS) and Table 8 (Ablation study on PACS) are different."
                },
                "questions": {
                    "value": "1. As an active sampling algorithm, how to define an informative test sample, especially on streaming data? The authors might provide some visualization results for better understanding.\n\n2. How about the cost of the ATA training set?  It seems that the SimATTA algorithm will keep a training set (the maximum size is $\\mathcal{B}$?) during the test-time adaptation, would this strategy violate the nature of test-time adaptation, i.e., real-time?\n\n3. There are many hyper-parameters in this work, such as two entropy thresholds $e_l$ and $e_h$, number of cluster centroid budget $NC(t)$, centroid increase number $k$, etc. The question is how to choose them for different datasets. In some sense, this is not the weakness/limitation of this particular paper but rather applies to the whole AL paradigm.\n\n4. It is unclear what the meaning of `steps=10` is. And what is the config of SimATTA? SimATTA (`steps=1`) or SimATTA (`steps=10`)?\n\n5. How to deal with an extreme situation in which only one sample is in a mini-batch, i.e., the batch size is 1.\n\n6. Another minor question is, why the performance of Enhanced TTA on Tiny-ImageNet-C (severity level 5) is poor? Also, why the baseline results (CLUE) on VLCS of Tab. 4 are too low, even worse than that of the Random method?\n\n7. In Section 5.2, \"randomly select labeled samples and fine-tune them with `their selected pseudo-label samples.`\" Is it a mistake?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4620/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699437765133,
            "cdate": 1699437765133,
            "tmdate": 1699636441015,
            "mdate": 1699636441015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cB4Vvs00pV",
                "forum": "YHUGlwTzFB",
                "replyto": "mxERJIJBMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cDV9 (Part I)"
                    },
                    "comment": {
                        "value": "Dear Reviewer cDV9,\n\nThank you for your acknowledgment of our work and insightful comments! We have made paper revisions following your suggestions and would like to address your concerns by answering your questions one by one as follows.\n\n> W1. Insufficient visualization\n\n- Thank you for your suggestion. We have created a figure (Fig.2) in the revised paper to demonstrate the algorithm more intuitively. We inserted the figure in the main paper and moved Alg. 2 to the appendix due to the space limit. We hope this modification is satisfactory.\n\n> W2. Insufficient justifications\n\n- Following your suggestion, we have added additional justifications in the revised paper. For your convenience, we summarize important changes and explanations as follows.\n\n- **Efficiency**: ATTA's higher time cost is due to the difference in training steps. In this experiment, SimATTA has a training step of 10 and a similar time cost as SAR per step. In Table 3, we aim to show that simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problems. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training in Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting.\nIn brief, ATTA is able to apply statistically significant training processes without CF, which requires sufficient training and is the reason for the extra time cost.\n\n- **Applicability**: \"ATTA can be applied to any pre-trained models including large language models (LLMs)\" demonstrates a possible future direction. In essence, ATTA is **model agnostic**, which is theoretically applicable to various machine learning models. In contrast, many FTTA methods only allow training the batch normalization layers. However, we acknowledge that we do not cover LLM experiments in this work, and applying ATTA to LLM may encounter unknown results.\n\n> W3. Inconsistent results\n\n- Thanks for pointing out. These inconsistent results come from a different hyperparameter $k$ selection in the ablation study to ensure that different variants have the number of budgets as similar as possible. We adjusted one $k$ in the ablation study since, in that part of the experiment, the budget number was much less than other variants. To clarify this ablation comparison issue and eliminate confusion, we have extended the ablation study by conducting experiments with more budget-limit selections and using line charts instead of tables for comprehensive comparisons. Please refer to our revised ablation study (Appx I.2) for more details.\n\n> Q1\n\n- Thanks for the advice. We have added visualization results and explanations in Appx F.2 for better illustration. In each batch, after the high/low entropy sample selection, we examine the high entropy samples to select informative ones. Assume we have 3 previously selected anchors, and we now apply a Kmeans of 4 centroids. After clustering, assume 3 centroids are located near the 3 old anchors since the 3 anchors' weights are much larger than other samples in this batch. Then the left centroid should statistically locate at the center of the distribution representing new test samples, which should be far from the 3 old anchors (thus, has not been covered by them). These new samples that are far from the previously known anchors are **informative** to us. Finally, we choose the closest sample to the centroid as a new anchor, i.e., the informative test sample.\n\n> Q2\n\n- (1) Yes, the number of labeled training sets is limited by $\\mathcal{B}$, and we also keep an unlabeled low-entropy training set of similar scale. Therefore, the overall space complexity is $O(\\mathcal{B})$.\n- (2) In our opinion, many real-time applications such as robotics, autonomous vehicles, medical imaging adaptations, and user interface personalizations, are not very time-sensitive. For example, retraining a personalized user interface recommendation system may require hours, days, or weeks, but ATTA can fine-tune the model in a second (or seconds) that should be considered in a real-time manner. As we have demonstrated (detailed in Appx B.Q8), the speed of ATTA is sufficiently fast for an autopilot system, and the speed requirements of the other applications listed above are safely within the limit of ATTA. Therefore, we believe ATTA is a real-time setting.\n\n- Rigorously, we do acknowledge that the application scopes of ATTA and FTTA are not completely overlapping. FTTA can handle applications that are extremely time-sensitive, while ATTA provides results of much higher quality."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4620/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382961319,
                "cdate": 1700382961319,
                "tmdate": 1700383608155,
                "mdate": 1700383608155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kEcBxWX4cl",
                "forum": "YHUGlwTzFB",
                "replyto": "mxERJIJBMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4620/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cDV9 (Part II)"
                    },
                    "comment": {
                        "value": "> Q3\n\n- In experimental settings, $k$ is selected to approximate the budget limit $\\mathcal{B}$ we would like to observe, which is for comparison purposes. Selecting $e_l$ is a trade-off between accuracy and the number of low-entropy samples. We select $e_l$ as low as possible while the number of low-entropy samples can still maintain the low/high entropy balance. Therefore, the $e_l$ selection is related to the distribution of the entropy in the specific dataset.\n\n- The hyperparameter searching spaces used in the paper are provided as follows:\n\n  - $e_l: \\{10^{-4}, 10^{-3}\\}$\n\n  - $e_h: 10^{-2}$\n\n  - $k: [0, 3]$\n\n  - $NC(t)$ is controlled by $k$, $i.e.$, $NC(t)=10 + kt$, where 10 is the initial number of centroid.\n\n- In real-world scenarios, we should select $k$ according to our oracle resource available. Instead of using $e_l$ independently, we believe considering both the threshold $e_l$ and an extra selection ratio can be a good practice.\n\n> Q4\n\n- `steps=10` indicates 10 gradient backpropagation iterations for one test-time adaptation batch. The hyperparameter `steps` should be set for Tent and SAR not only because of their original settings but also because their pseudo-label training lacks statistical significance and thus requires a hyperparameter to control the training process. In experiments, we found that using a tolerance count (tol) is more natural because SimATTA maintains a statistically significant training set. SimATTA will stop updating once the loss does not decrease for more than 5 steps.\n- For TinyImageNetC in Table 3, SimATTA uses `steps=10` for time comparisons with others since other methods use at most 10 steps. We have added this information to the table.\n\n> Q5\n\n- ATTA handles this extreme situation by accumulating these samples into a buffer. Sample selections and training are executed once the buffer is full, e.g., setting it to the length of 100.\nThis process is less efficient than TTA but necessary since one of the fundamental differences between ATTA and FTTA is that ATTA tries to fine-tune the model using a statistically significant distribution. This is a trade-off between effectiveness and efficiency.\n\n> Q6\n\n- (1) Enhanced TTA, though having access to labeled samples, is essentially an FTTA setting, *i.e.*, it does not maintain a statistically significant training set. Therefore, these enhanced TTA methods still suffer from CF problems. The Tiny-ImageNet-C comparisons aim to address the potential question of whether access to labeled samples is the only reason for improvements and for corresponding fairness concerns.\n- (2) In 3 runs, CLUE has source domain results: $74.06$, $84.38$, and $98.66$. This indicates CLUE suffers from severe CF problems in 2 out of 3 runs. One simple explanation for these failures is that the selected samples are too far from the source distribution, which induces CF and overfitting problems. To be specific, the high entropy consideration of CLUE leads to the failure of choosing low-entropy samples. As demonstrated in Sec. 3.2, compared to our mitigation of CF with balanced high/low entropy samples, CLUE fails to maintain this balance in VLCS and shows catastrophic performances.\n\n- The reason why the method Entropy does not suffer from CF problems is that the ADA training process includes 10 selection rounds in their settings so that a possible minor CF problem after the first round leads to the abrupt entropy increase of source-like samples. These samples are then selected by the Entropy method in the 2nd round, thus preventing further CF problems. In contrast, CLUE fails to select these source-like samples in time (before severe CF) because of diversity considerations. Therefore, we can conclude the failure of CLUE as that, considering uncertainty and diversity leads to better informative sample selections but may cause worse source-like sample selections.\n\n> Q7\n\n- Sorry for the confusion. We have revised the unclear expression. This sentence means that, in addition to each TTA's original updates, we further enhance these TTA methods by fine-tuning the model with randomly selected labeled samples. Traditional TTA methods are generally designed to update models using their own pseudo-labeled samples.\n\n\nBesides the above points, we also provide further experiments and make an effort to clear the concerns and improve the clarity and organization of our paper in multiple aspects following the advice from other reviewers. Please refer to the revised paper and appendices for details.\n\nWe sincerely thank you for your time! Hope we have addressed your concerns through practical efforts and shown the significance of our work. We look forward to your reply and further discussions, thanks!\n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4620/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382998552,
                "cdate": 1700382998552,
                "tmdate": 1700383461548,
                "mdate": 1700383461548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6wKux5O6cu",
                "forum": "YHUGlwTzFB",
                "replyto": "kEcBxWX4cl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4620/Reviewer_cDV9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4620/Reviewer_cDV9"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed clarifications and revisions\uff01"
                    },
                    "comment": {
                        "value": "I now feel more strongly towards acceptance. Best of luck!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4620/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471004084,
                "cdate": 1700471004084,
                "tmdate": 1700471004084,
                "mdate": 1700471004084,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]