[
    {
        "title": "AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations"
    },
    {
        "review": {
            "id": "85Hon1y31W",
            "forum": "53kW6e1uNN",
            "replyto": "53kW6e1uNN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3105/Reviewer_TKXS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3105/Reviewer_TKXS"
            ],
            "content": {
                "summary": {
                    "value": "This paper draws attention to the challenges of over-smoothing and over-correlation in GNN-based collaborative filtering methods. In particular, the paper provides a detailed analysis of the over-correlation problem, which has been largely overlooked in existing works. Through rigorous theoretical analysis, the paper establishes a proportional association between the over-smoothing issue and the over-correlation issue, shedding light on their interconnected nature.\n\nTo tackle these issues, the paper proposes a model-agnostic constraint with adaptive weights. This constraint is designed to effectively mitigate over-smoothing and over-correlation problems in GNN-based collaborative filtering. The adaptive weights allow the constraint to dynamically adjust and optimize the learning process.\n\nComprehensive experiments are conducted to validate the effectiveness of the proposed constraint. The results demonstrate significant improvements in overall performance, enhanced training efficiency, and the efficacy of the adaptive approach. These findings provide strong evidence for the practical benefits of the proposed constraint in addressing the over-smoothing and over-correlation challenges in GNN-based collaborative filtering methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper highlights the issue of decorrelation in collaborative filtering, which has received little attention in previous works.\n- Through a comprehensive theoretical analysis, the paper establishes a clear association between the over-smoothing problem and the decorrelation issue.\n- To address the challenges of over-smoothing and decorrelation, the paper proposes an effective solution. The proposed scheme is extensively evaluated through rigorous experiments, demonstrating its effectiveness.\n- The paper is well-written and provides clear explanations. It includes illustrative figures and pilot experiments that enhance understanding and readability."
                },
                "weaknesses": {
                    "value": "- I have reservations regarding the dataset preprocessing approach employed in the paper. The authors chose to exclude users and items with fewer than 15/10 interactions in some datasets. However, in my experience, this approach has the potential to create highly dense datasets and introduce bias.\n- It would have been beneficial if the paper had explored the recent advancements in self-supervised learning for collaborative filtering, as these techniques have demonstrated superior performance in related studies."
                },
                "questions": {
                    "value": "I would expect the authors to clarify the two issues mentioned in the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3105/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3105/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3105/Reviewer_TKXS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3105/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698544610121,
            "cdate": 1698544610121,
            "tmdate": 1699636256816,
            "mdate": 1699636256816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x2oajDWdFm",
                "forum": "53kW6e1uNN",
                "replyto": "85Hon1y31W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3105/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3105/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TKXS"
                    },
                    "comment": {
                        "value": "Thank you very much for carefully reviewing our paper and providing valuable questions and suggestions. Regarding your inquiries, we would like to address them as follows:\n\n* **Answer to question 1:** Thanks for your suggestion. In fact, excluding users and items with fewer than 15/10 interactions is a widely adopted practice in the evaluation of collaborative filtering tasks [1,2,3,4,5]. Even after the exclusion process, our dataset still maintains a considerable level of sparsity, as evident from the density in the table below.\n\n|  Datasets   | User # | Item # | Interaction # | Density |\n| :---------: | :----: | :----: | :-----------: | :-----: |\n|  Movielens  | 6,040  | 3,629  |    836,478    | 3.8e-2  |\n|    Yelp     | 26,752 | 19,857 |   1,001,901   | 1.8e-3  |\n|   Gowalla   | 29,859 | 40,989 |   1,027,464   | 8.4e-4  |\n| Amazon-book | 58,145 | 58,052 |   2,517,437   | 7.5e-4  |\n\n* **Answer to question 2:** Thank you very much for your suggestions. Below, we provide additional performance comparisons related to representative self-supervised method SGL [5], and we plan to supplement this baseline in the subsequent version of the paper.\n\nAdditional performance comparisons of SGL on Movielens Dataset:\n\n|    Model     | Recall@10 | Precision@10 | NDCG@10 | MAP@10 |\n| :----------: | :-------: | :----------: | :-----: | :----: |\n|     DGCF     |  0.1819   |    0.1843    | 0.2477  | 0.1429 |\n|     SGL      |  0.1892   |    0.1901    | 0.2567  | 0.1496 |\n|   LightGCN   |  0.1886   |    0.1873    | 0.2540  | 0.1470 |\n| AFD-LightGCN |  0.1985   |    0.1969    | 0.2689  | 0.1594 |\n\nThank you once again for your valuable feedback and suggestions. We sincerely hope that our responses address some of your concerns. Should you have any further questions or suggestions, please feel free to share them with us. Your input is highly appreciated.\n\n\n\n[1] He, X., Deng, K., Wang, X., Li, Y., Zhang, Y., & Wang, M. (2020, July). Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (pp. 639-648).\n\n[2] Wang, X., Jin, H., Zhang, A., He, X., Xu, T., & Chua, T. S. (2020, July). Disentangled graph collaborative filtering. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval (pp. 1001-1010).\n\n[3] Wang, X., He, X., Wang, M., Feng, F., & Chua, T. S. (2019, July). Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval (pp. 165-174).\n\n[4] Chen, L., Wu, L., Hong, R., Zhang, K., & Wang, M. (2020, April). Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 01, pp. 27-34).\n\n[5] Wu, J., Wang, X., Feng, F., He, X., Chen, L., Lian, J., & Xie, X. (2021, July). Self-supervised graph learning for recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval (pp. 726-735)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207259104,
                "cdate": 1700207259104,
                "tmdate": 1700207259104,
                "mdate": 1700207259104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OnyiKFjr4z",
                "forum": "53kW6e1uNN",
                "replyto": "x2oajDWdFm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3105/Reviewer_TKXS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3105/Reviewer_TKXS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive response. I believe that the incorporated supplementary experiments would greatly enrich the evaluation section of this paper. Additionally, I suggest considering the inclusion of several additional self-supervised recommendation methods in future versions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699514199,
                "cdate": 1700699514199,
                "tmdate": 1700699514199,
                "mdate": 1700699514199,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GT4PF8MI1g",
            "forum": "53kW6e1uNN",
            "replyto": "53kW6e1uNN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3105/Reviewer_vpbW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3105/Reviewer_vpbW"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors focus on analyzing feature over-correlation in graph-based collaborative filtering, and propose an adaptive feature de-correlation regularization in graph-based collaborative filtering. Column-wise feature over-correlation will introduce redundant information for representation learning, the proposed feature de-correlation regularization can significantly improve the representation quality. Besides, the proposed feature de-correlation is very flexible and lightweight, which can coupled with representation-based CF. Experiments on several benchmarks show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Interesting research topic of this paper, tacking feature over-correlation in collaborative filtering is an effective direction.\n2. The proposed feature de-correlation regularization is flexible and effective in graph-based collaborative filtering. De-correlation is helpful in learning more high-quality representation for collaborative filtering.\n3. Experiments conducted on several graph-based backbones demonstrate the effectiveness of the proposed de-correlation regularization."
                },
                "weaknesses": {
                    "value": "1. The motivation of this paper should be highlighted. Why do the authors analyze over-correlation combined with over-smoothing? Does feature over-correlation only occur on graph-based collaborative filtering non other methods such as Matrix Factorization? \n2. The reason for existing over-correlation in low-dimensional collaborative filtering is not clear. It will be more interesting if the authors deeply explain the behind reasons. Besides, does alleviating over-correlation can help to reduce over-smoothing issues in graph-based collaborative filtering? The authors should give a more explanatory illustration.\n3. Lacking comparisons of related works, disentangled collaborative filtering should be involved. Besides, column-wise de-correlation can be also viewed as self-supervised learning[1]. The authors should discuss with current self-supervised graph collaborative filtering method[2,3,4].\n[1]Wang X, Jin H, Zhang A, et al. Disentangled graph collaborative filtering[C]//Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 2020: 1001-1010.\n[2]Wu J, Wang X, Feng F, et al. Self-supervised graph learning for recommendation[C]//Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 2021: 726-735.\n[3]Yu J, Yin H, Xia X, et al. Are graph augmentations necessary? simple graph contrastive learning for recommendation[C]//Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval. 2022: 1294-1303.\n[4]Yang, Y., Wu, Z., Wu, L., Zhang, K., Hong, R., Zhang, Z., ... & Wang, M. (2023). Generative-Contrastive Graph Learning for Recommendation."
                },
                "questions": {
                    "value": "Mentioned as the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3105/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767318423,
            "cdate": 1698767318423,
            "tmdate": 1699636256739,
            "mdate": 1699636256739,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2mNTftfF9N",
                "forum": "53kW6e1uNN",
                "replyto": "GT4PF8MI1g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3105/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3105/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vpbW"
                    },
                    "comment": {
                        "value": "Thank you very much for carefully reviewing our paper and providing valuable questions and suggestions. Regarding your inquiries, we would like to address them as follows:\n\n* **Answer to question 1:** The reason for simultaneously investigating over-smoothing and over-correlation is rooted in their significance as crucial issues regarding the performance of GNN-based collaborative filtering. Over-smoothing has been widely acknowledged in prior research as a primary factor affecting the performance of GNN-based collaborative filtering. By contrasting it with over-correlation, we aim to highlight the latter as another equally important factor. We attribute the occurrence of over-correlation primarily to the message-passing operations in GNNs, with the degree of over-correlation increasing with the number of message-passing iterations. As for the over-correlation issue in other methods, such as MF, we have not observed similar issues. These methods lack explicit operations, like message-passing, that significantly increase redundancy in feature representations.\n\n* **Answer to question 2:** The presence of over-correlation in low-dimensional collaborative filtering is primarily ascribed to the message-passing operations inherent in GNNs. The degree of over-correlation tends to escalate with an increase in the number of message-passing iterations. In fact, the method we proposed not only addresses over-correlation but also partially alleviates over-smoothing. Due to space constraints, we did not include this aspect in Figure 5. We have supplemented the SMV below, corresponding to Figure 5.\n\nThe SMV metric corresponding to Figure 5 illustrates that addressing over-correlation can help to mitigate over-smoothing issues in graph-based collaborative filtering.\n\n![](https://z1.ax1x.com/2023/11/15/piYt0yt.png)\n\n* **Answer to question 3:** Thanks for your suggestions. Below, we provide additional performance comparisons related to DGCF [1] and representative self-supervised method SGL [2], and we plan to supplement these baselines in the subsequent version of the paper.\n\nAdditional performance comparisons of DGCF & SGL on Movielens Dataset:\n\n| Model        | Recall@10 | Precision@10 | NDCG@10 | MAP@10 |\n| ------------ | --------- | ------------ | ------- | ------ |\n| DGCF         | 0.1819    | 0.1843       | 0.2477  | 0.1429 |\n| SGL          | 0.1892    | 0.1901       | 0.2567  | 0.1496 |\n| LightGCN     | 0.1886    | 0.1873       | 0.2540  | 0.1470 |\n| AFD-LightGCN | 0.1985    | 0.1969       | 0.2689  | 0.1594 |\n\nThank you once again for your valuable feedback and suggestions. We sincerely hope that our responses address some of your concerns and contribute to a more positive perception of our work. Should you have any further questions or suggestions, please feel free to share them with us. Your input is highly appreciated.\n\n \n\n[1] Wang, X., Jin, H., Zhang, A., He, X., Xu, T., & Chua, T. S. (2020, July). Disentangled graph collaborative filtering. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval (pp. 1001-1010).\n\n[2] Wu, J., Wang, X., Feng, F., He, X., Chen, L., Lian, J., & Xie, X. (2021, July). Self-supervised graph learning for recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval (pp. 726-735)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207187777,
                "cdate": 1700207187777,
                "tmdate": 1700207187777,
                "mdate": 1700207187777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SCkSPruu3O",
                "forum": "53kW6e1uNN",
                "replyto": "2mNTftfF9N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3105/Reviewer_vpbW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3105/Reviewer_vpbW"
                ],
                "content": {
                    "title": {
                        "value": "Resposes to authors"
                    },
                    "comment": {
                        "value": "After reading the author's responses, I also have some questions:\n(1) The major is the effectiveness of the proposed AFDGCF compared with other SOTA methods. Graph self-supervised methods are current SOTAs, I think the comparisons with SOTAs are insufficient. First, SGL is proposed in SIGIR 2021, and more advanced self-supervised methods have been proposed in recent years, such as SimGCL and more. Besides, comparisons with SGL were only conducted on Movielens-1M, which is not convincing. Movielens-1M is not suitable for conducting self-supervised experiments as it has too-high interaction sparsity(over 1% while most recommendation scenes are less than 0.01%). I think the authors should select more effective self-supervised methods and more general datasets for comparisons.  Nevertheless, I still recognize the author's contribution, to analyzing graph collaborative filtering~(GCF) from the perspective of column-wise de-correlation, which is effective in improving GCF backbones, such as LightGCN. Maybe combining self-supervised GCL methods with column-wise de-correlation is interesting. Looking forward to the authors' responses.\n(2) I stick to my point of view that column correlation also exists in matrix factorization. Why not add experiments that set LightGCN to 0 layer, which degenerates to BPR-MF? In my view, column correlation not only exists in GNN-based CF. So, it lacks explanations of the above."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552367148,
                "cdate": 1700552367148,
                "tmdate": 1700552367148,
                "mdate": 1700552367148,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hcf5K2xIpR",
            "forum": "53kW6e1uNN",
            "replyto": "53kW6e1uNN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3105/Reviewer_HBXd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3105/Reviewer_HBXd"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes the feature correlation issues in graph collaborative filtering. The author(s) present empirical studies on the smoothness and correlation of each layer of various graph collaborative filtering methods. Then, the author(s) propose AFDGCF that incorporates an auxiliary loss function to explicitly optimize the over-correlation issue. Extensive experiments on four public datasets and four popular GCF backbones show the effectiveness of the proposed method. Code is available and the author(s) promise to release all the code after the reviewing phase."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studies an important task, i.e., graph collaborative filtering.\n2. The proposed model is implemented by an open-source framework, making it easy to reproduce. Code is available during the reviewing phase.\n3. Extensive experiments on four public datasets and four popular GCF backbones show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Limited novelty. The paper seems like a straightforward application of existing literature, specifically the DeCorr [1] that focuses on general deep graph neural networks, in a specific application domain. The contribution of this study is mainly the transposition of DeCorr's insights into graph collaborative filtering, with different datasets and backbones. Although modifications like different penalty coefficients for users and items are also proposed, the whole paper still lack enough insights about what are unique challenges of overcorrelation in recommender systems.\n\n2. It could be better if one additional figure could be illustrated, i.e., how Corr and SMV metrics evolve with the application of additional network layers\u2014mirroring the Figure 2, but explicitly showcasing the effects of the proposed method\u2014the authors could convincingly validate their auxiliary loss function's efficacy.\n\n3. Presentation issues. The y-axis labels of Figure 2 lack standardization, e.g., 0.26 vs. 0.260 vs. 2600 vs. .2600.\n\n[1] Jin et al. Feature overcorrelation in deep graph neural networks: A new perspective. KDD 2022."
                },
                "questions": {
                    "value": "According to Theorem 1, there exists a proportional relationship between column correlation and row correlation of a matrix. So whether existing works on alleviating row correlation issues like contrastive learning also solve the correlation issues? Once the row correlation is alleviated, according to the proportional relationship, the column correlation should be alleviated as well. If so, why do we need the proposed auxiliary loss to explicitly alleviate the column correlation issue?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3105/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3105/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3105/Reviewer_HBXd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3105/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699141836356,
            "cdate": 1699141836356,
            "tmdate": 1699636256684,
            "mdate": 1699636256684,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CwsjkU87Pa",
                "forum": "53kW6e1uNN",
                "replyto": "hcf5K2xIpR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3105/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3105/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBXd"
                    },
                    "comment": {
                        "value": "Thank you very much for reviewing our paper. Regarding your inquiries, we would like to address them as follows:\n\n* **Reply to weakness 1:** It is important to note that we are pioneers in addressing feature over-correlation within the realm of recommendation systems. Diverging from prior research primarily focused on deep GNNs, our attention in the recommendation domain is how to integrate the advantages of GNN into collaborative filtering. Simultaneously, we delved into theoretical analyses of over-correlation and over-smoothing issues, establishing their interconnections. Conducting experimental analyses specifically targeting the over-correlation issue in GNN-based collaborative filtering models, we leveraged the unique characteristics of recommendation systems and introduced the Adaptive Feature Decorrelation method. \n* **Reply to weakness 2:** Thank you very much for your valuable suggestions. We will consider incorporating such a figure in subsequent version. Indeed, Figure 5 illustrates the Corr and performance with the application of additional network layers. Unfortunately, due to space constraints, the SMV was not presented in Figure 5. We have supplemented the SMV below, corresponding to Figure 5. This figure illustrates the variations in SMV among GNN-based collaborative filtering models with different numbers of layers, before and after the implementation of our AFD loss.\n\nThe SMV metric corresponding to Figure 5:\n\n![](https://z1.ax1x.com/2023/11/15/piYt0yt.png)\n\n* **Reply to weakness 3:** Thank you very much for your thoughtful suggestions. We will make adjustments to the y-axis labels in the subsequent version.\n* **Answer to question 1:** We appreciate your insightful observations. While we acknowledge that uniformity in contrastive learning can partially alleviate the over-smoothing issue, it only indirectly affects the correlation between features and cannot effectively address the issue of  over-correlation. In contrast, our approach directly alleviates the over-correlation problem, providing a more controllable solution. In practice, the impact on performance may not be entirely proportional between these two problems. To address your concerns more comprehensively, we provide additional performance comparisons related to representative self-supervised method SGL [1].\n\nAdditional performance comparisons of SGL on Movielens Dataset:\n\n|    Model     | Recall@10 | Precision@10 | NDCG@10 | MAP@10 |\n| :----------: | :-------: | :----------: | :-----: | :----: |\n|     DGCF     |  0.1819   |    0.1843    | 0.2477  | 0.1429 |\n|     SGL      |  0.1892   |    0.1901    | 0.2567  | 0.1496 |\n|   LightGCN   |  0.1886   |    0.1873    | 0.2540  | 0.1470 |\n| AFD-LightGCN |  0.1985   |    0.1969    | 0.2689  | 0.1594 |\n\nThank you once again for your valuable feedback and suggestions. We sincerely hope that our responses address some of your concerns and contribute to a more positive perception of our work. Should you have any further questions or suggestions, please feel free to share them with us. Your input is highly appreciated.\n\n\n\n[1] Wu, J., Wang, X., Feng, F., He, X., Chen, L., Lian, J., & Xie, X. (2021, July). Self-supervised graph learning for recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval (pp. 726-735)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206890944,
                "cdate": 1700206890944,
                "tmdate": 1700206890944,
                "mdate": 1700206890944,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pyezxnsVDQ",
                "forum": "53kW6e1uNN",
                "replyto": "CwsjkU87Pa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3105/Reviewer_HBXd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3105/Reviewer_HBXd"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer HBXd"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive and thoughtful responses. I appreciate the time and effort you put into addressing the specific points raised. I would like to maintain my evaluation regarding the novelty aspect. I still perceive the overall approach as a somewhat straightforward application of de-correlation techniques to a new domain (i.e., deep general GNN -> GNN for collaborative filtering)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640779914,
                "cdate": 1700640779914,
                "tmdate": 1700640779914,
                "mdate": 1700640779914,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NSYqyoNPMK",
            "forum": "53kW6e1uNN",
            "replyto": "53kW6e1uNN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3105/Reviewer_Xkxj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3105/Reviewer_Xkxj"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the possible connections between over-smoothing and over-correlation in graph neural networks-based recommender systems. Indeed, while over-smoothing has been debated in graph-based recommendation for quite some time now, the authors claim over-correlation is still not properly analysed as happening in graph representation learning. Through an initial empirical study, the authors demonstrate that the negative effects of the two issues seem to be directly dependent and go along with the performance degradation of the models (i.e., usually after the third message-passing layer). After that, the paper underlines how over-smoothing and over-correlation may present a direct mapping to rows and columns in the node embedding matrix, respectively, and mathematically proves that the two are proportional. In this respect, as alleviating one of the two would tackle also the other, the authors propose a loss function named adaptive feature decorrelation, that comes into a static and dynamic version. An extensive experimental setting comprising four recommendation datasets and nine baselines demonstrates the efficacy of the proposed approach. Indeed, when applied to existing graph-based recommender systems, the adaptive feature decorrelation loss function is beneficial to improve the performance in terms of recommendation accuracy and requiring much less epochs to reach convergence. Finally, an ablation study justifies the soundness of the proposed architectural choices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The addressed problem (i.e., over-smoothing and over-correlation in graph-based recommendation) is relatively new to the literature.\n+ The empirical analysis supported by the mathematical proofs help justifying the existing problem and opening to possible solutions.\n+ The experimental setting is extensive with numerous evaluation dimensions.\n+ The code and datasets are released at review time."
                },
                "weaknesses": {
                    "value": "- Some details about the introduced methodology need to be clarified.\n- The authors may have not considered other graph-based recommendation baselines whose solutions are like the proposed one.\n\n**After the rebuttal.** The rebuttal clarified all weaknesses."
                },
                "questions": {
                    "value": "* To the best of my understanding, I cannot find the reason why the authors state that \u201cit is crucial to maintain the smoothness of deep representations while restricting the feature correlations of the model\u2019s representations\u201d (beginning of page 7). The paper seems to claim that when reducing over-correlation for deeper representations, also over-smoothing will be tackled. In this sense, I cannot see the point in the quoted statement. Would you please elaborate on that?\n* Did the authors consider graph-based recommendation approaches which leverage decorrelation in a similar manner to the proposed one (e.g., disentangled graph collaborative filtering, DGCF [1]). In authors\u2019 opinion, what would it be (even intuitively) the effect of performing a double decorrelation if the proposed loss function was applied to DGCF? Would it have a positive or a negative impact, and why?\n\n[1] Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua: Disentangled Graph Collaborative Filtering. SIGIR 2020: 1001-1010\n\n**After the rebuttal.** The rebuttal answered all questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3105/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3105/Reviewer_Xkxj",
                        "ICLR.cc/2024/Conference/Submission3105/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3105/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699260046224,
            "cdate": 1699260046224,
            "tmdate": 1700651500409,
            "mdate": 1700651500409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EW4nED6WvL",
                "forum": "53kW6e1uNN",
                "replyto": "NSYqyoNPMK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3105/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3105/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xkxj"
                    },
                    "comment": {
                        "value": "Thank you very much for carefully reviewing our paper and providing valuable questions and suggestions. Regarding your inquiries, we would like to address them as follows:\n\n* **Answer to question 1:** Our decision to preserve the smoothness of deep representations while constraining the feature correlations in the model's representations is grounded in prior research highlighting the significance of embedding smoothness in the effectiveness of GNN-based RS [1,2,3]. This is because collaborative filtering methods rely on the similarity of user and item embeddings. Consequently, heedlessly diminishing representation smoothness, even with the aim of mitigating over-smoothing, is not wholly advantageous for the recommendation model's performance. Our approach aims to strike a balance between alleviating over-smoothing and embedding smoothness, while addressing the issue of feature correlations, seeking an optimal point to achieve the best recommendation performance. The advantages of doing so were demonstrated in our ablation experiments (Table 3).\n* **Answer to question 2:** Thanks for your suggestion. While both our approach and DGCF [4] employ decorrelation techniques, the motivation and modeling process of decorrelation differ between the two. In the case of DGCF, it partitions the representation matrix into several chunks, treating them as different intents. It combines this with intent-aware routing to learn disentangled embeddings, aiming to ensure relative independence among learned intents. However, within each intent, certain columns remain highly correlated. In contrast, our method focuses on decorrelating any two columns, aiming to address the issue of over-correlation through constraints. Furthermore, examining Figure 4 in the DGCF paper, we observed that finer-grained disentangling would lead to a performance decline in DGCF models. We speculate that the design of intent-aware routing in DGCF introduces a higher risk of overfitting, thus limiting its performance in finer-grained situations. Below, we provide additional performance comparisons related to DGCF, and we plan to supplement this baseline in the subsequent version of the paper. The additional performance comparisons demonstrate that our approach outperforms DGCF.\n\nAdditional performance comparisons of DGCF on Movielens Dataset:\n\n|    Model     | Recall@10 | Precision@10 | NDCG@10 | MAP@10 |\n| :----------: | :-------: | :----------: | :-----: | :----: |\n|     DGCF     |  0.1819   |    0.1843    | 0.2477  | 0.1429 |\n|     SGL      |  0.1892   |    0.1901    | 0.2567  | 0.1496 |\n|   LightGCN   |  0.1886   |    0.1873    | 0.2540  | 0.1470 |\n| AFD-LightGCN |  0.1985   |    0.1969    | 0.2689  | 0.1594 |\n\nThank you once again for your valuable feedback and suggestions. We sincerely hope that our responses address some of your concerns. Should you have any further questions or suggestions, please feel free to share them with us. Your input is highly appreciated.\n\n\n\n[1] He, X., Deng, K., Wang, X., Li, Y., Zhang, Y., & Wang, M. (2020, July). Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (pp. 639-648).\n\n[2] Zhu, T., Sun, L., & Chen, G. (2021). Graph-based embedding smoothing for sequential recommendation. IEEE Transactions on Knowledge and Data Engineering, 35(1), 496-508.\n\n[3] Kikuta, D., Suzumura, T., Rahman, M. M., Hirate, Y., Abrol, S., Kondapaka, M., ... & Loyola, P. (2022). KQGC: Knowledge Graph Embedding with Smoothing Effects of Graph Convolutions for Recommendation. arXiv preprint arXiv:2205.12102.\n\n[4] Wang, X., Jin, H., Zhang, A., He, X., Xu, T., & Chua, T. S. (2020, July). Disentangled graph collaborative filtering. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval (pp. 1001-1010)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206707391,
                "cdate": 1700206707391,
                "tmdate": 1700206707391,
                "mdate": 1700206707391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z4MV24lwiJ",
                "forum": "53kW6e1uNN",
                "replyto": "EW4nED6WvL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3105/Reviewer_Xkxj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3105/Reviewer_Xkxj"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nthank you for your rebuttal. In the following, I'll answer to the two points discussed in my review and addressed by your rebuttal.\n\n- Your answer makes sense (along with the proposed references) and sufficiently addresses my concern.\n- Regarding the joint adoption of DGCF and your framework, your answer is quite aligned with what I originally thought. Thank you also for the additional results, which further confirm the results you presented in the original paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3105/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485330907,
                "cdate": 1700485330907,
                "tmdate": 1700485330907,
                "mdate": 1700485330907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]