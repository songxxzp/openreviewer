[
    {
        "id": "-h0yB5JAINS",
        "original": null,
        "number": 1,
        "cdate": 1666609807939,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609807939,
        "tmdate": 1666609807939,
        "tddate": null,
        "forum": "Uk3zO5A-CSe",
        "replyto": "Uk3zO5A-CSe",
        "invitation": "ICLR.cc/2023/Conference/Paper655/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the problem of out-of-distribution object detection (OOD-OD), where an object detector has to detect unknown objects without relying on an auxiliary dataset. The authors propose a framework of Two-Stream Information Bottleneck (TIB), with a standard IB network to extract object-related information from in-distribution (ID) data, and a Reverse Information Bottleneck (RIB) to obtain OOD features. By exploiting IB, the method manages to obtain enhanced object-related information from ID data. At the same time, TIB obtains simulative OOD features by maximizing the discrepancy with the ID features. Experimental results show the method could significantly improve the performance of OOD-OD and incremental object detection over the baselines.",
            "strength_and_weaknesses": "**Strengths**\n+ The idea of using IB to extract object-related and OOD feature problems is novel and interesting. It seems that the idea could be applied to other object detection problems. The empirical results are encouraging.\n+ The paper is well-written and easy to follow. The key design choices in the paper are carefully described.\n\n**Weaknesses**\n+ The task of OOD-OD is not formally described in the paper.\n+ Does background differ from OOD instances?\n+ Is the OOD bounding box included when performing inference?\n+ The correlation between Information Bottleneck and the method is not clearly illustrated. Particularly, the KL terms in Eq. (5) do not express the IB where Z should have low mutual information with F but ignore the high mutual information with the label. So do the Q and P_in. Eq. (8) also has that problem. Also, we usually minimize a loss (with a non-negative value) not maximize a loss as in Eq. (8).\n+ The enhancing process (Eq. 4) appears to be unclear. Both A and \u03b1 are related to P_in, and the role of each term is not well described. An ablation study about this process may help to clarify.\n+ Regarding the datasets, how many classes are in the ID and OOD data?\n+ Images in Figure 4 only contain objects from ID classes, which makes the visualization of OOD maps uninformative. What do the OOD maps look like with objects from  OOD classes, and in the cases of mixed ID and OOD classes? It would definitely demonstrate the effectiveness of the proposed approach on OOD-OD.\n+ There is no discussion about the choice of hyper-parameters in Section A.3. The paper should include an explanation for each set of hyper-parameters.\n",
            "clarity,_quality,_novelty_and_reproducibility": "+ The paper's structure is generally well-organized. The problem and the motivation are well described. However, there are some problems with the clarity and quality, i.e., the authors use many notations without clearly summarizing or referring back to Figure 2. It would be much better to follow if the authors include a summarized table of notations and add the notation, e.g. F, P_in\u200b, E, \u2026.  to Figure 2.\n+ A limitation section should be included.\n+ The method of applying IB to out-of-distribution appears to be novel. The designs of TIB and RIB could also be useful to the community since they could be easily applied to other object detection problems.\n",
            "summary_of_the_review": "The idea in the paper is well-motivated and described. The paper addresses the problem of out-of-distribution object detection by exploiting Information Bottleneck, which has been carefully studied by prior work and shown promising results. Therefore, the idea has solid theoretical reasoning. However, when implementing the IB in terms of loss functions, not all properties are guaranteed. Also, lacking of a visual explanation of the OOD map with OOD instances significantly reduces the convincingness of the proposed approach. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper655/Reviewer_ajPF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper655/Reviewer_ajPF"
        ]
    },
    {
        "id": "uqCaA50A5Fd",
        "original": null,
        "number": 2,
        "cdate": 1666633535253,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633535253,
        "tmdate": 1666633535253,
        "tddate": null,
        "forum": "Uk3zO5A-CSe",
        "replyto": "Uk3zO5A-CSe",
        "invitation": "ICLR.cc/2023/Conference/Paper655/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "- This paper proposed a new information theoretic approach to tackle out-of-distribution object detection (OOD-OD). \n- Authors leverage Variational information bottleneck (IB) to suppress object-irrelevant representations to improve discriminability of in-distribution class (ID). \n- Next, to simulate OOD features to train an OOD detector in an unsupervised fashion, authors reverse the IB equation, called Reverse Information Bottleneck (RIB). In contrast to IB, which aims to extract task-relevant compact codewords/features, the goal of RIB is to simulate task-irrelevant OOD style features to train a strong OOD detector. \n- Extensive experiments on standard OOD-OD datasets and incremental object detection datasets show the effectiveness of the proposed approach for OOD-OD.",
            "strength_and_weaknesses": "Strengths\n- The proposed method is very simple and effective and is one of the major strengths of this work.\n- The use of information theoretic methods, the IB, for object detection is new and fresh.\n- All the claims made in the paper have been properly addressed with adequate experiments.\n- The paper tackles a very important practical problem with object detectors and achieves impressive performance on standard benchmarks.\n\nWeaknesses\n- I believe the comparisons in Table-1 are unfair as the current method introduces more trainable parameters than VOS. Not to diminish the impressive performance, I believe it is imperative to add a column with backbones and trainable parameters introduced for a more fair comparison.\n- Authors claim that the OOD map simulates outlier samples and the visualizations of the map doesn't seem to do that. After careful observation one can notice that the maximum of the OOD map still occurs either inside or around the object of interest. Additionally, these maps are not interpretable, it is not clear to me why these maps should look like noise. Can these maps be interpreted as adversarial noise (on the features) since they are obtained by maximizing the discrepancy between logits of the right class? (Like noise that can be added to make the feature lie closer to a different ID class different than the one in the feature map).\n- In the standard OOD-OD setup the number of OOD samples coming from COCO/Open-Images are usually less than 20% (930 OD samples for COCO with 4952 samples of ID val set). It is not clear how the proposed model does with higher levels of contamination from OOD classes. \n- The contribution of the first stage of IB is not clear from the ablation experiments. What happens if one were to simply pass F through two different convolutional blocks and keep everything the same? In Table 4, authors either keep the full branch or remove it entirely, but why do one need the first IB/RIB stage? One can apply the same L_dis, L_uncertainty losses on this architecture even without computing mean and standard deviation and combining them. I understand that the loss becomes uninterpretable but how does it fare against the proposed approach?\n\nMinor questions/comments to improve the paper (Not dependent on the final decision):\n- It is not clear how Eq. 5 is related to Variational Information Bottleneck. From the paper there should be a negated term and the second term is the KL divergence. In Eq. 5 there are two KL divergence terms and it is not clear how the authors end up with this equation. This step requires some more information to help the readers understand the math.\n- How is Eq. 8 related to Eq. 7. Do the authors mean to say that Eq. 8 approximates Eq. 7. In that case why is it hard to compute Eq. 7 directly without any approximation?\n- It would be beneficial to researchers implementing this work, if information about computing mutual information (I(.,.)) on 3d feature maps is provided in the supplementary. \n- Is the purpose of experiments on Incremental object detection just to show that IB can improve overall object detection performance?. If uncertainty loss is not computed for these experiments, then why would one need to perform RIB?\n- Kindly add the baseline to Table 4, readers would have to move back and forth to understand what the baseline is. \n- Line before \"Metrics\" in Section 4.1 \".... manually examined the OOD images to ensure ..\" -> \".... manually examined to ensure ... \".\n- What is the confidence threshold used to generate the results in Fig. 3. If its the same value for VOS and TIB?\n",
            "clarity,_quality,_novelty_and_reproducibility": "-The writing clarity is really good and the idea of leveraging information bottleneck to enhance object relevant information and suppress object irrelevant information is novel. \n- With a few additional details asked above being answered, I believe a skilled researcher/engineer should be able to reproduce this work.\n-  I believe with a few minor revisions, the paper is of great value to the conference.",
            "summary_of_the_review": "- Overall I'm very happy with the paper and its writing quality. \n- Authors leverage Information bottleneck to simultaneously enhance object relatedness and suppress (with reversal) object irrelevance in features to improve OOD-OD. \n- The proposed method is simple and very effective. The experiments support all the claims made by the authors. A few more explanations to understand the working of Reverse IB are required which I believe the authors can provide in reasonable time. \n- I'm willing to improve my ratings depending on the author's response.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper655/Reviewer_UuoW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper655/Reviewer_UuoW"
        ]
    },
    {
        "id": "1I0knkof-Fe",
        "original": null,
        "number": 3,
        "cdate": 1666898930518,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666898930518,
        "tmdate": 1666898975411,
        "tddate": null,
        "forum": "Uk3zO5A-CSe",
        "replyto": "Uk3zO5A-CSe",
        "invitation": "ICLR.cc/2023/Conference/Paper655/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper attempts to solve OOD problem in object detection.\nIt tries to reduce the impact of lacking unknown data for supervision and leverage in-distribution data to improve the model\u2019s discrimination ability with both Information Bottleneck and Reverse Information Bottleneck.\nIt first uses a standard IB network to disentangle instance representations that are beneficial for localizing and recognizing objects, and then use a RIB to obtain simulative OOD features to alleviate the impact of lacking unknown data.\nExperiments demonstrate good performance in incremental object detection. ",
            "strength_and_weaknesses": "Strength\n+ The motivation is clear, and the solution is novel.\n+ Good performance gain on incremental object detection setup.\n\nWeakness\n- The major concern is the scalability of such method. The author only uses PascalVOC and BDD as in-domain data, which is relatively small and simple. To verify its ability of handling real-world data, I would suggest the author can demonstrate a more complex setup, for example, use COCO as ID and Openimages as evaluation set.\n- Figure 2 is not very illustrative. It would be better to align equations into the flow. Or teardown to multiple small figures for better clarity.\n- In Figure 4, the visualization of OOD is confusing. It is more like white noise. How does simulate OOD feature extracted from it help?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, but packed with equations. It would be better to introduce more figures to increase the clarity.",
            "summary_of_the_review": "Overall, this is a solid paper with detailed experiments and visualizations. The major concern is the scalability in really-world data. I will increase score if the author can provide my evidence. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper655/Reviewer_D2nY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper655/Reviewer_D2nY"
        ]
    }
]