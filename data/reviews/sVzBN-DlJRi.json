[
    {
        "id": "VLP-g7O7Ml",
        "original": null,
        "number": 1,
        "cdate": 1666556694147,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556694147,
        "tmdate": 1666556694147,
        "tddate": null,
        "forum": "sVzBN-DlJRi",
        "replyto": "sVzBN-DlJRi",
        "invitation": "ICLR.cc/2023/Conference/Paper3835/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a method to train a Vision Transformer given a budget defined by a total training time or computation cost. Training a Vision Transformer from scratch is usually expensive, so designing a budgeted training is a way to make the training a Vision Transformer more accessible. The paper analyzes and identifies some redundancies in the attention heads, MLPs and visual tokens of Vision Transformer architectures. The redundancies are usually high at the beginning of the training and then gradually decrease. To make training faster, the paper introduces a 3 phase training strategy, where the training starts with a small architecture and the architecture grows after each phase. The proposed method is evaluated on ImageNet with multiple Vision Transformer architectures.",
            "strength_and_weaknesses": "**Strengths**\n- To reduce the training cost of Vision Transformer, the paper identifies some redundancies in the architectures. Section 3.2, 3.3 and 3.4 analyzes the redundancies in the attention heads, MLPs and visual tokens of Vision Transformer architectures. For each part, an empirical analysis performed on a subset of ImageNet is shown. The analyses show that the redundancies are usually high at the beginning of the training and then gradually decrease.\n- After identifying some redundancies, the paper introduces a 3 phase training strategy that takes advantage of the redundancies. The training starts with a small architecture and the architecture grows after each phase because small architectures can be trained faster than large architectures and large architectures have more redundancies.\n- I like the research area on budgeted training. I think that making from scratch training more accessible/affordable is positive for the community because more research labs will be able to develop new model architectures. I think it can also help to reduce the environmental impact of ML model training.\n- The proposed method is evaluated on ImageNet with multiple Vision Transformer architectures (DeiT, Pyramid Vision Transformer v2 (PVTv2), and Swin Transformer). The proposed training strategy improves the performances for several training budgets (25%, 50%, 75%, and 100%). The paper also contains ablation studies of the important hyper-parameters of the proposed method. \n\n\n**Weaknesses**\n- The performances are evaluated only on ImageNet. It could be interesting to evaluate the pre-trained models on downstream tasks to analyze the transfer learning performances because it will increase the quality of the paper.\n- The exponential function used to define the training budget for each phase does not seem well justified.  \n- The proposed training strategy has 3 phases but there is no justification about this design choice. It could be interesting to show results with more phases.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read and understand. Making training more efficient is an interesting research area and can have a lot of impact. The proposed method seems novel and shows good results for several architectures and training budgets. There are some implementation details in the appendix so it should not be too difficult to reproduce some experiments. ",
            "summary_of_the_review": "Overall, I think the paper is well motivated and is about an interesting research area. The proposed method seems novel and improves the performances in multiple scenarios but the experiment section could be improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3835/Reviewer_jJMJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3835/Reviewer_jJMJ"
        ]
    },
    {
        "id": "Oi3ZF13LpLl",
        "original": null,
        "number": 2,
        "cdate": 1666726231475,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666726231475,
        "tmdate": 1666726231475,
        "tddate": null,
        "forum": "sVzBN-DlJRi",
        "replyto": "sVzBN-DlJRi",
        "invitation": "ICLR.cc/2023/Conference/Paper3835/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents training strategy that allows to training optimal transformers under user-defined budgets.  The budget training strategy is based on investigating the redundancy in attention heads, hidden dimensions in MLP, and\nvisual tokens. The training strategy could adjust the activation rate of the model along the training process to make use of the redudancies. Extensive experiments have demonstrated effectiveness. ",
            "strength_and_weaknesses": "Strength:\n1. the analysis of the redundancy in modules of  VT will be very important not only restricted to this work, but potentially many research on designing efficient VT models.\n2. The paper is quite easy to follow and all the contents are easy to follow (despite a few typos: Sec 6, effusiveness -> effectiveness?)\n\nWeakness:\n1. the motivation could be improved. There are multiple solutions for reducing training cost, they could be but not limited to data set pruning, using smaller models, or more effective training schemes. Using training cost as motivation seems not enough to justify this paper. And this will make readers wonder how this proposed method compared to other treatments. Perhaps one scenario unique is continuous/online training?  \n2. more discussions might be needed to convince the effectiveness of the proposed training strategy: 1. how it compares to different original training schedules, e.g., what if we reduce the original training epochs from 300 to 150, using some quick schedule (cosine learning rate)?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear written, All the experiments are well described and should be straightforward to reproduce. ",
            "summary_of_the_review": "See strength/weakness",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3835/Reviewer_vaAP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3835/Reviewer_vaAP"
        ]
    },
    {
        "id": "liJ11ej6Fck",
        "original": null,
        "number": 3,
        "cdate": 1666959256661,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666959256661,
        "tmdate": 1666959256661,
        "tddate": null,
        "forum": "sVzBN-DlJRi",
        "replyto": "sVzBN-DlJRi",
        "invitation": "ICLR.cc/2023/Conference/Paper3835/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a framework for training Vision Transformers at any given budget by reducing the inherent redundancies of the model at the early training stages.\n\nThis paper proposes a training strategy to dynamically adjust the activation rate of the model along the training process.\n",
            "strength_and_weaknesses": "This paper investigates three redundancy factors in Vision Transformers, including attention heads, hidden dimensions in MLP, and visual tokens.\n\nExtensive experiments show the effusiveness of the proposed framework with competitive performances on a wide range of training budgets.",
            "clarity,_quality,_novelty_and_reproducibility": "I have no other concers.",
            "summary_of_the_review": "This paper take a step forward and focus on the problem of budgeted training and aims to achieve the highest model performance under any given training budget.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3835/Reviewer_H7vm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3835/Reviewer_H7vm"
        ]
    }
]