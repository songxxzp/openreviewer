[
    {
        "id": "0N-Jgl4pwWf",
        "original": null,
        "number": 1,
        "cdate": 1666586654925,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666586654925,
        "tmdate": 1666586654925,
        "tddate": null,
        "forum": "vDY5Y8HMNxO",
        "replyto": "vDY5Y8HMNxO",
        "invitation": "ICLR.cc/2023/Conference/Paper4529/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a Group Masked Model Learning (GMML), a Self-Supervised Learning (SSL) mechanism\nfor pretraining vision transformers with the ability to extract the contextual information\npresent in all the concepts in an image. This is achieved by manipulating\nrandomly groups of connected tokens, ensuingly covering a meaningful part of\na semantic concept, and then recovering the hidden semantic information from\nthe visible part of the concept. The paper suggests that GMML implicitly introduces a novel data augmentation\nprocess.\n\nExperimental results are shown for Image Classification, DA in Transfer Learning, and a few types of ablations studies.\n\nAt the end of the manuscript, the authors claim the following - \nGMML based pretraining makes the vision transformers data-efficient. Also, GMML is the first\nself-supervised pretraining work which consistently outperformed supervised pretraining for any\npretraining and finetuning dataset, regardless of their sizes.\n\nThe key impact of the proposed GMML is that it makes it possible for transformers\nto train on small and medium size datasets. It is not only data efficient, but its outstanding information\nextraction ability enables it to outperform state-of-the-art supervised and self-supervised\nmethods with large margins.\n",
            "strength_and_weaknesses": "Pros:\nTransformers are hard to train, and providing any means to make them work for SSL\ntasks, for DA/TL jobs is even harder.\nAuthors appears to address that problem to some extent,\nNumerous results have shown the potential.\n\nCons\nAlthough convinced by elaborate experimentations and ablations studies,\nits difficult to get convinced due to lack of sufficient analytical proofs and justifications.\n\nThe manuscript has just one Eqn - the loss function used - with just the L1 vs L2 norm being discussed.\nWhat about use of other loss functions - KLD and variants, Softmax, CE with variants etc. ?\n \n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper quite clearly written.\nNot much issues with the quality of presentation.\n\nEqn. (1) does not appear to be novel;\nNo other analytics provided for strengthening the proposed concept - convergence, impact of bias,..etc\n\nExperimentations appear to be reproducible, although not much time to explore myself\nand verify the same.",
            "summary_of_the_review": "\nI am not convinced with the paper.\nThere appears to be some contribution - but not substantial.\n\n\nNote for Area Chairs and PC chairs:\nThis paper is available in\nhttps://arxiv.org/abs/2205.14986\nin which the author names are revealed.\n\nThe authors of above are a subset of those in [2] in this submitted manuscript.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4529/Reviewer_8Ka6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4529/Reviewer_8Ka6"
        ]
    },
    {
        "id": "ZUW3wqQsWY",
        "original": null,
        "number": 2,
        "cdate": 1666631666905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631666905,
        "tmdate": 1666631666905,
        "tddate": null,
        "forum": "vDY5Y8HMNxO",
        "replyto": "vDY5Y8HMNxO",
        "invitation": "ICLR.cc/2023/Conference/Paper4529/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The work proposes Group Masked Model Learning framework for self supervised learning.",
            "strength_and_weaknesses": "Strength:\nThe work reports promising results for self supervised learning in low data regime. \n\n\n\nWeaknesses:\n1. The novelty of the work is confusing as the work looks quite similar to other popular approaches for self supervised learning (such as MAE). However, the authors actually claim that the already published MAE (and also SIMMIM) extended actually the proposed work. \n\n2. There are other strong frameworks for self supervised learning that are not referred to in this work, for instance, data2vec (Baevski, Alexei, et al. \"Data2vec: A general framework for self-supervised learning in speech, vision and language.\"\u00a0arXiv preprint arXiv:2202.03555\u00a0(2022).) \n\n3. It is difficult to understand the contribution of the proposed approach on larger datasets. For instance, how this work results are compared with the state-of-the-art on the full ImageNet. Particularly, where the proposed approach fits in the Table 1 of the  data2vec paper.\n\n4. Some parts are unclear, for instance, why the imput image is corrupted up to 70% in the case of \u201czeros\u201d and \u201cnoise\u201d, while in the case of \u201creplace\u201d it is corrupted up to 35%. Also not sure why the combination of \u201creplace\u201d and \u201cnoise\u201d works better.\n",
            "clarity,_quality,_novelty_and_reproducibility": "-",
            "summary_of_the_review": "-",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4529/Reviewer_rp7V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4529/Reviewer_rp7V"
        ]
    },
    {
        "id": "anPIM4ztSq",
        "original": null,
        "number": 3,
        "cdate": 1666672050115,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672050115,
        "tmdate": 1666672050115,
        "tddate": null,
        "forum": "vDY5Y8HMNxO",
        "replyto": "vDY5Y8HMNxO",
        "invitation": "ICLR.cc/2023/Conference/Paper4529/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper delivers a method called group mask model learning (GMML). GMML learns a representation of an image by partially corrupting it and reconstructing it with a transformer encoder-decoder structure. The authors conducted experiments that show GMML\u2019s superiority in learning representations from small-scale datasets with small-sized models.",
            "strength_and_weaknesses": "- (C1) Models those capable of dealing with small-scale datasets are always welcome. However, assessing the model\u2019s performance on large-scale datasets (at least ImageNet-1K) is needed to determine whether the model can also be used for pre-training those datasets. In that sense, the paper has its weakness in only delivering experiments on small-scale datasets and small-sized models.\n- (C2) GMML is almost identical to SimMIM. The authors go on to argue that structures like SimMIM all stem from the GMML structures presented in 2021, but it is questionable why this paper submitted to ICLR 2023 should take the contribution of GMML presented in 2021. Since SimMIM is already presented in CVPR 2022, it is appropriate to address that this paper is conveying the variant of SimMIM, regardless of the existence of GMML architecture, that I failed to find its reference in the paper.\n- (C3) Table 1 in the paper omits several papers, including SimMIM [1], BEiT [2], and SplitMask [3]. Especially, SplitMask shares topics with this paper: \u201cSSL method that is capable of being pre-trained on small-scale dataset\u201d and reports small datasets\u2019 metrics seemingly on par with this paper\u2019s result.\n- (C4) The authors said that most non-GMML approaches suffer from trivial constant solutions, but the representation collapsing problem only occurs for positive (BYOL-like) methods. Contrastive learning does not suffer from the collapse by design and the number of works adopting contrastive learning is not neglectable.\n- (C5) The manuscript has a self-contradicting statement that they have already suggested GMML in 2021 and picked a proposal of GMML as one of the paper's contributions.\n\n[1] Xie, Zhenda, et al. \"Simmim: A simple framework for masked image modeling.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2022.\n[2] Bao, Hangbo, et al. \"BEiT: BERT Pre-Training of Image Transformers.\" *International Conference on Learning Representations*. 2021.\n[3] El-Nouby, Alaaeldin, et al. \"Are Large-scale Datasets Necessary for Self-Supervised Pre-training?.\" *arXiv preprint arXiv:2112.10740* (2021).",
            "clarity,_quality,_novelty_and_reproducibility": "- (Clarity) I couldn't quite understand why the GMML model presented in 2021 should be a contribution to this paper. (see C2, C5)\n- (Quality) The experiment presented in this paper is only a classification experiment on small-scale datasets, and it is too insufficient to claim the title \"GMML is All you Need\". (see C1) Even in the experiments performed, there are papers omitted. (see C3)\n- (Novelty) I consider this model to be about the same as SimMIM.\n- (Reproducibility) Source code is not provided, but implementation details in the appendix provide reproducibility.",
            "summary_of_the_review": "Due to the various weaknesses described above (see C1-C5) I cannot give a high recommendation to this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4529/Reviewer_4sPn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4529/Reviewer_4sPn"
        ]
    },
    {
        "id": "wj7g5snkdh",
        "original": null,
        "number": 4,
        "cdate": 1666673235975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673235975,
        "tmdate": 1666673235975,
        "tddate": null,
        "forum": "vDY5Y8HMNxO",
        "replyto": "vDY5Y8HMNxO",
        "invitation": "ICLR.cc/2023/Conference/Paper4529/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a self supervised learning framework GMML for vision transformers. The idea is essentially the same as MAE and SimMIM (the authors claim that MAE and SimMIM are extensions of GMML). When masking the inputs, they use noise or alien patches from other images, rather than just zeros. In this way, the initial blocks of the networks have to model the alien and the non-alien concepts, and thus can better model the context.",
            "strength_and_weaknesses": "Strength\n\n1. The benefit from using noise or alien patches from other images, rather than just zeros when masking inputs is clear as shown in Figure 4(a).\n2. On small datasets, GMML converge faster and perform better (Table 1)\n\nWeakness\n\nThere are several critical weaknesses in experiments. \n\n1. Firstly, in the experiments, the authors only use ViT-tiny and ViT-small backbones. While it is more common to use a larger backbone for comparison (e.g. B, L, H) as in MAE and SimMIM.\n2. Secondly, the authors missed the comparison of ImageNet-1k in Table 1, which is probably the most important comparison with other baselines.\n3. Thirdly, some important baselines are missing for comparison, e.g. SimMIM, BEIT",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe authors claim that GMML is proposed \u201cat the beginning of 2021\u201d with no citation. They also claim that SimMIM and MAE are extensions of GMML. So, I have no choice but to search for GMML and found an unpublished paper on ArXiv [1].\n\nIgnore [1], in my opinion, the novelty of this paper mainly lies in using alien patches when masking inputs.\n\n[1] SiT: Self-supervised vIsion Transformer. 2104.03602",
            "summary_of_the_review": "Overall, the paper propose an interesting masking method using alien patches. However, the experiments are weak and the relationship with MAE and SimMIM is unclear.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4529/Reviewer_JV27"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4529/Reviewer_JV27"
        ]
    }
]