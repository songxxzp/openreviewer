[
    {
        "id": "6Vo8k0Wx2P",
        "original": null,
        "number": 1,
        "cdate": 1666153873804,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666153873804,
        "tmdate": 1668807568139,
        "tddate": null,
        "forum": "QrnDe_9ZFd8",
        "replyto": "QrnDe_9ZFd8",
        "invitation": "ICLR.cc/2023/Conference/Paper3731/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors investigate the problem of task ambiguity in prompting pre-trained LLMs. In which setting, the task descriptions sometimes contain insufficient information for humans/models to perform the task, they have to consequently look at a few examples to have a better guess on the task. The authors propose a new benchmark designed for studying the said problem, namely AmbiBench, which includes six manually designed sentence classification tasks. Using AmbiBench, they tested a set of pre-trained LLMs (covering a wide spectrum of training data sizes, training procedures, and model architectures) as well as a group of human annotators. Results suggest that only the largest LLM variant trained with additional learning from human feedback (text-davinci-002) can approach human performance on such tasks. Further, the authors show that the davinci model can quickly improve if fine-tuned on a small amount of in-context examples, even outperforming the text-davinci-002 model. ",
            "strength_and_weaknesses": "### Strength\n\n* Interesting and useful topic: I really like seeing work on this topic. Task ambiguity has never been a more important topic thanks to the flourishing of LLM research. In prompt-based learning where users/practitioners can use an inference-only LLM and prompt it to perform some never-before-seen tasks. At the same time of benefiting from greater accessibility, users have to describe their tasks (in language, as part of the prompt) in a way that maximizes LLM performance. I see this work quite related to the line of work of prompt engineering, but tackling a very specific aspect of prompting LLMs. Although using a toy-ish benchmark, I can see potential empirical value from such work. \n* Human performance: I really appreciate the authors who include human scores for the tasks, which greatly helped me get a clearer picture. Although the second paragraph of Section 4.3.1 (why text-davinci-002 outperforms humans in Figure 3) seems a bit hand wavy to me. \n* Clarity: I enjoy reading this paper, most of the content are easy to follow. \n\n\n\n### Weaknesses\n\nPlease see my comments below. ",
            "clarity,_quality,_novelty_and_reproducibility": "Let me list my concerns and questions here, please answer and help me better understand the paper :)\n\n1. In Section 2.2, the author mentioned a few prior works discussing LLMs' unfavourable behaviours such as \"recency bias\" as proposed in [1]. Indeed, for most in-context learning or few-shot prompting work, such biases can have non-negligible effects on model performance (e.g., models tend to copy labels from the most recent examples as its prediction, due to such effects, model performance on a data point can sometimes vary from random to near perfect). The authors state that \\\n`In this work, we attempt to control for such factors by constructing a benchmark where multiple tasks are consistent with a single instruction or example, requiring the model to leverage multiple signals to disambiguate the intended task without relying on learned priors.` \\\nPlease help me understand this, I fail to understand how AmbiBench avoids or mitigates such effects? To my understanding, as long as few-shot prompting are provided as a sequence, such effects can exist. \n2. What are some reasons in Figure 2 and Figure 3 human annotator scores have a much bigger variance?\n3. I like how the authors include many LLM variants for comparison. However, I'm a bit concerned on, e.g., in Section 4.2.1, what's a good way to decouple models' 1) inability of disambiguate the task description from 2) inability of generate meaningful output via prompting (maybe a model can do disambiguation, but it's not big enough to perform well on prompting?) Given some recent discussions such as [2], is few-shot prompting the best way to investigate certain abilities of smaller models? Compared to, e.g., using a linear/non-linear probe to see if the LM representations contains certain properties we want. If the LM has trouble to even generate meaningful outputs (as reported by the authors, sometimes models generates tokens other than `X` and `Y`), it's a bit hard to tell how capable the model is on task disambiguation. \n4. I'd like to hear more from the authors about the fine-tuning experiments. I assume that fine-tuning could potentially make the disambiguation task much easier, even with only a small set of training data (68 per task). I fail to gain too much information from Section 4.4 more than fine-tuning on in-context data can outperform a general inference-only model. \n5. As the authors mention, AmbiBench is a *minimal complexity* testbed. I understand and agree that this is to make the benchmark more controllable. However, for the same reason, I am less convinced about the generality of findings reported here. What are some other types of task ambiguities in NLP tasks, maybe more subtle than simply occluding a category name? Maybe brought by natural language instructions rather than templatic language? What about in tasks other than sentence classification? It would be nice to have such discussion, which can give readers a better idea how the proposed method/benchmark can shed light on understanding task disambiguation in general.\n6. (less important:) Figure 1 is a bit misleading in some sense, I was honestly expecting to see how a LLM can explicitly guess the task description --- this might not be a crazy idea, given a sequence of examples (as many as 20 here), is there a way to model/measure such thing?\n7. (less important:) The two prompt templates introduced in Section 3.3 are quite arbitrary, especially the `Q/A` format barely makes sense. \n\n\n### References:\n* [1] [Calibrate Before Use: Improving Few-Shot Performance of Language Models. Zhao et al., 2021.](https://arxiv.org/pdf/2102.09690.pdf)\n* [2] [Emergent Abilities of Large Language Models. Wei et al., 2022.](https://arxiv.org/pdf/2206.07682.pdf)",
            "summary_of_the_review": "In general, I think the current version of the paper is around (maybe slightly above) the borderline. I look forward to reading the authors responses to my questions so I can better understand the work and thus have a more precise evaluation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3731/Reviewer_o1Uh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3731/Reviewer_o1Uh"
        ]
    },
    {
        "id": "9JNPW28OAq",
        "original": null,
        "number": 2,
        "cdate": 1666843642781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666843642781,
        "tmdate": 1666922152441,
        "tddate": null,
        "forum": "QrnDe_9ZFd8",
        "replyto": "QrnDe_9ZFd8",
        "invitation": "ICLR.cc/2023/Conference/Paper3731/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies task ambiguity in NLP. Recent research has demonstrated the benefits of using natural language prompts to describe the task when fine-tuning a language model, but such prompts can be ambiguous in some cases. This paper proposes a benchmark with ambiguous instructions, and shows large language models can approach human accuracy on ambiguous tasks. Fine-tuning with a small number of in-context examples further improve the performance.\n",
            "strength_and_weaknesses": "Strength:\n\nNatural language prompts have been extensively studied, and the ambiguity of certain tasks, or the difficulty of designing prompts that can clearly describe the task, is an important challenge that may limit the utility of these methods in the real world.\n\nWeakness:\n\nThis paper only studies the ambiguity of a few toy tasks, which only require simple lexical understanding and the texts are synthetic and very short. It would be more interesting to see what the ambiguity is in real-world NLP tasks, and how they can be handled.\n\nBesides, the empirical observation and proposed fine-tuning method are pretty straightforward and not surprising. The \"uninformative instructions\" are very unnatural as they just replace the salient feature in a normal instruction with placeholder. Nevertheless, the instructions still contain some description of the task, and it is easy to imagine that both human and model would make guesses based on the available, incomplete information. The observation that the best language model achieve similar performance to human on such ambiguous task is not surprising, even though the setting is synthetic and simple. In my opinion, I don\u2019t see many new insights from this paper that would help the community to better resolve task ambiguity.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear, although the writing can be more fluent and concise.\n\nThe novelty is limited. The fine-tuning methods are standard practice in this domain.\n\nThe reproducibility is unclear, as the authors do not mention whether the benchmark will be released.",
            "summary_of_the_review": "This paper studies ambiguity in NLP tasks. Although it is an important challenge in fine-tuning language models, this paper only studies a few toy tasks and the proposed method is not novel.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3731/Reviewer_zof8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3731/Reviewer_zof8"
        ]
    },
    {
        "id": "jN1w1JZCJDq",
        "original": null,
        "number": 3,
        "cdate": 1666919652942,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666919652942,
        "tmdate": 1666919652942,
        "tddate": null,
        "forum": "QrnDe_9ZFd8",
        "replyto": "QrnDe_9ZFd8",
        "invitation": "ICLR.cc/2023/Conference/Paper3731/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the ability of large language models (LLMs) and humans to handle ambiguity in task prompts, of the type that are currently popular in the LLM literature.\n\nA small suite of six synthetic tasks are created. Each asks the model (or human) to identify whether a sentence contains a particular category (e.g. human subject, proper noun). Then both human and language models are tested in two settings: with the task fully defined in a prompt (e.g. 'output x if the sentence contains a proper noun, y otherwise'); and with the category of interest obfuscated in the prompt. Each task is designed such that there should be a 50% chance of correctly guessing the task type from the obfuscated prompt and a single example. This chance increases drastically as more examples are added.\n\nThe paper shows that humans are able to solve five out of six tasks perfectly, when the prompt is unambigious. The performances of LLMs are more varied, with an instruct tuned model, trained with reinforcement-learning from humans (RLFH), performing best but still signifcantly worse than human performance. When the prompt is ambiguous, human performance is closer to model performance. However, when an increasing number of examples are given, the best models are able to disambiguate the taksk much better, reaching >80% accuracy and outperforming humans, in the best case (which involves reading 20 examples). Similar performance is also attainable through fine tuning on 4/6 of the tasks and evaluating on the final 2. This suggests that tuning models to handle task ambiguity may result in generalization to new forms of test time ambiguity.",
            "strength_and_weaknesses": "#### Strengths:\n- This is a cleanly designed investigation of a specific type of task ambiguity, which adds to our understanding of how large language models make use of prompts and examples.\n- The inclusion of human annotators, assigned the same tasks as the LLMs, really helps us understand the models' capabilities.\n- The finetuning results are interesting, and suggest new strategies for increasing the robsutness of LLMs to non-expert prompt design.\n\n#### Weaknesses\n- It is quite hard to draw many conclusions about the best performing systems because their training strategies are not public, and it is not clear what types of task ambiguity have been seen during training.\n- The six synthetic tasks are quite simplistic and limited in scope. It is not clear how these findings would extend to more complex task types.\n\n#### Minor issues:\n- The choices of line color and hatching in Figure 3 are not very discriminative, and I struggled to untangle which line represents which model. I suggest that more disctriminative line types are used.\n",
            "clarity,_quality,_novelty_and_reproducibility": "#### Clarity\n- This paper is well written and easy to follow. The experiments are well justified and explained.\n\n#### Quality\n- The experiments are well designed but limited in scope.\n- The paper would be more valuable if it could could better describe how its findings may extend to broader types of task ambiguity, likely to be seen in downstream tasks of interest.\n\n#### Novelty\n- This is the first paper that I've seen that properly investigates task ambiguity of this type in large language models.\n\n#### Reproducability\n- The novel work in this paper should be reproducable.\n- However, the paper relies heavily on opaque models, and it is a little hard to draw meaningful conclusions about the best performing systems.\n",
            "summary_of_the_review": "This paper presents a well designed focused study of how LLMs handle one type of task ambiguity---identifying whether an unknown class of word exists in an input.\n\nA number of different LLMs are evaluated in ambiguous and unambiguous prompting settings, and compared to humans in the same scenarios. The results provide a nice insight into the effectiveness of examples in the context of ambiguous prompts, as well as the effectiveness of fine tuning.\n\nHowever, the current study focuses on a very narrow type of ambiguity, and it's not clear how much these insights will extend to other types of task ambiguity. This paper would be stronger if it could describe how this type of study could be extended to more diverse types of ambiguity, even if it did not directly address those types of ambiguity in the study presented.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3731/Reviewer_3BBz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3731/Reviewer_3BBz"
        ]
    },
    {
        "id": "KLtWabXt5hq",
        "original": null,
        "number": 4,
        "cdate": 1669664932037,
        "mdate": 1669664932037,
        "ddate": null,
        "tcdate": 1669664932037,
        "tmdate": 1669664932037,
        "tddate": null,
        "forum": "QrnDe_9ZFd8",
        "replyto": "QrnDe_9ZFd8",
        "invitation": "ICLR.cc/2023/Conference/Paper3731/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies how modern LLMs perform when given ambiguous task examples.\n\nThe model must learn to infer the task from the examples, vs having a clear task with ambiguous inputs. The paper introduces a dataset, AmbiBench, designed to measure how well models use the instrutions/few-shot examples to infer underlying task.\n\nAmbiBench consists of 6 sentence classification tasks of minimal complexity which allow controlling and measuring the amount of ambiguity in the tasks. The tasks are all binary classification tasks. Determining the correct underlying task requires either (a) an auxiliary instruction, or (b) multiple examples to learn from.  The examples are generated from a set of templates.\n\nExperiments are carried out with 2 prompt templates across all tasks, and compare a variety of open and closed LLMs, along with human evaluations.  \n\nThey find:\n- for uninformative instructions, both models & humans get 50% accuracy, but humans also get 100% on negation while models do no\n- for informative instructions, humans get perfect performance across all tasks but 1, with RLHF models (text-davinci-002) close behind.\n- Non-RLHF models do not do nearly as well: davinci does much worse than davinci instruct.\n- They further find that ambiguously-specified examples provide a much bigger boost to generalization, when compared to finetuning on unambiguous examples. This provides evidence that finetuning works more robustly than few-shot prompting, and that explicitly training to handle ambiguity helps generalization performance.\n- Best model is not able to consistently verbalize the task it is trying to accomplish\n\n\nMain takeaways from paper: Scaled RLHF models perform best at disambiguating tasks. Finetuning helps over just few-shot prompting",
            "strength_and_weaknesses": "Strengths:\n- Simple benchmark with extensive comparisons across model scales and with humans.\n  - comparisons to humans grounded the results\n- easy to reproduce (even easier assuming the benchmark will be released with camera ready)\n- demonstrates a current weakness of LMs in a purely synthetic setting which allow variation.\n- preliminary investigation of more real-world tasks, specifically an LM taking actions at command line based on human instructions\n- extensive auxiliary experiments in appendix, studying smaller models, alternative prompting choices, \n\nWeaknesses:\n- The tasks may be too simple / short / toy.\n- Results are not particularly surprising\n- Downside of using closed models (although no open models exist for this yet): we do not actually know the training procedure for the largest models.\n  - Recent reports suggest that the instruct models are not actually RLHF: https://www.alignmentforum.org/posts/mbGjzyy6eJXT4gFpm/update-to-mysteries-of-mode-collapse-text-davinci-002-not\n  - With this in mind, I worry about drawing general conclusions about RLHF\n- Small details\n  - page 2: what makes finetuning on in-context examples metalearning?\n  - page 15: \"could could verbalize\"\n\nMisc:\n- Potential improvement: run additional experiments on the FLAN-T5 models to see how instruction tuning helps smaller models (paper released after submission, so not required but would be useful additional evidence!)\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I am reading the updated version of the paper, which I find to be clearly written and thorough. The experiments are well done with extensive comparisons and a human baseline.\n\nThe results are straightforward to reproduce. It would be great to see more experiments with open RLHF models, but there unfortunately are not any available yet.\n\nThe synthetic dataset is novel, if simple. This is the first paper to do a clear and self-contained benchmark on task ambiguity (afaict). It evaluates a few known methods for getting the models to accomplish a task (few-shot prompting vs. finetuning) and confirms existing evidence that finetuning performs better.\n\n",
            "summary_of_the_review": "The paper presents a simple synthetic task which measures how many different LMs perform given ambiguous tasks. The synthetic benchmark allows testing this ability extensively and the community will benefit from it if evaluations are easy to run, although its simplicity may make it difficult to generalize the findings. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3731/Reviewer_6dKC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3731/Reviewer_6dKC"
        ]
    }
]