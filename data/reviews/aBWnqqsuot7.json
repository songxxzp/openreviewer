[
    {
        "id": "s6VA9IGh6L",
        "original": null,
        "number": 1,
        "cdate": 1666493671961,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666493671961,
        "tmdate": 1666493671961,
        "tddate": null,
        "forum": "aBWnqqsuot7",
        "replyto": "aBWnqqsuot7",
        "invitation": "ICLR.cc/2023/Conference/Paper2745/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper proposes a deep learning model for Kohn-Sham density functional theory. Unfortunately, I know nothing about this theory and related works, so I cannot accurately summaries this work. ",
            "strength_and_weaknesses": "Unfortunately, I am not in this area so cannot identify the possible strength and weaknesses. ",
            "clarity,_quality,_novelty_and_reproducibility": "It looks like the paper is well organized but I have no idea on the novelty and reproducibility. ",
            "summary_of_the_review": "Unfortunately, I am not in this area. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2745/Reviewer_GibD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2745/Reviewer_GibD"
        ]
    },
    {
        "id": "BTeLbEFBxW",
        "original": null,
        "number": 2,
        "cdate": 1666560727343,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560727343,
        "tmdate": 1666560787849,
        "tddate": null,
        "forum": "aBWnqqsuot7",
        "replyto": "aBWnqqsuot7",
        "invitation": "ICLR.cc/2023/Conference/Paper2745/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the computation of wave functions in Kohn-Sham Density Functional Theory (KS-DFT). Differently from conventional works, the authors use neural network (NN) models with a stochastic optimization method in machine learning to tackle the problem. The problem is formulated as the minimization problem of energy function depending on the set of orthogonal wave functions. The integral in the energy function is approximated by the finite sum based on the numerical quadrature. The stochastic gradient descent (SGD) is then employed to approximate the optimization for the objective function consisting of a finite sum. The parameter transformation with the Jacobian is introduced to deal with the orthogonal constraint among wave functions. The neural network model is used as the model for the transformation. Numerical experiments show that the proposed method provides comparable accuracy and high scalability to exciting methods.",
            "strength_and_weaknesses": "Strength\n- Overall the paper is clearly written. The significance of the problem is well introduced though the problem setting may not be well-known in the machine learning community. The modeling of the local scaling transformation using NNs is interesting. \n\n Weaknesses\n- There are some unclear points in the algorithm. The Jacobian appears in (19), which depends on a NN model. It is unclear how to compute the Jacobian matrix efficiently. I guess the back-propagation for NNs is applied to calculate the derivative w.r.t. the input vector instead of the weight parameters. A detailed description of the algorithm would be helpful to readers. \n- The authors provide theoretical contributions as some propositions, but the theoretical results are relatively straightforward. \n- Numerical experiments are not sufficient. Hyper-parameter tuning is important to achieve high accuracy in the computation. In the discussion, the author pointed out, \"One potential remedy for this problem is the second-order optimizers.\" However, the current paper does not provide a systematic study of that problem. Ablation studies should be provided to assess which factor is significant to attain satisfactory results. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. Even readers unfamiliar with quantum physics can understand most parts of the paper. The modeling for the basis functions is an interesting contribution. However, the theoretical novelty is limited. The algorithm proposed in the paper is simple. However, the reproducibility is unclear as the authors did not share the codes they used in their experiments. ",
            "summary_of_the_review": "The paper is clearly written. The neural basis function is an interesting model to deal with the problem considered in the paper. However, the theoretical insight is limited. Showing other applications of neural basis function would be beneficial for readers. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2745/Reviewer_BHRj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2745/Reviewer_BHRj"
        ]
    },
    {
        "id": "xVIRTvesFkM",
        "original": null,
        "number": 3,
        "cdate": 1666623200936,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623200936,
        "tmdate": 1670707728476,
        "tddate": null,
        "forum": "aBWnqqsuot7",
        "replyto": "aBWnqqsuot7",
        "invitation": "ICLR.cc/2023/Conference/Paper2745/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, a deep learning approach for solving KS-DFT was proposed, which converts the objective function for KS-DFT into an unconstrained equivalent by reparameterizing the orthogonal constraints as a feed-forward computation. By using stochastic gradient descent, the integral was amortized over the optimization steps. Its equivalence to the conventional SCF method was proven both empirically and theoretically. The comparison between this work and some other deep-learning-assisted DFT approaches were discussed. The accuracy and scalability were tested via experiments on molecules by comparing with two benchmarks (PySCF and JAX-SCF). The authors further raised several issues that await solutions based on the limitaions from this paper.",
            "strength_and_weaknesses": "Strength:\n\nThe approach proposed in this paper was proven to has the same expressivity as the SCF method, while reducing the computational complexity from O(N4) to O(N3). The methodology is expected to be also suitable for more complex neural-based wave functions.\n\nWeaknesses:\n\n1. For a fair comparison with the same software/hardware environment, the authors reimplemented the SCF method in JAX. Nevertheless, its equivalence with the conventional SCF solvers in terms of efficiency and scalability was not clarified.\n\n2. To demonstrate the efficiency and stability, the authors carried out some calculating experiments. However, the coverage of experiments was not sufficient. When evaluating the scalability of D4FT, carbon Fullerene molecules containing ranging from 20 to 180 carbon atoms were simulated. The size of the system seems to be not large enough. As the main limitation of the conventional SCF is its inefficiency for large molecules or solid cells, a machine learning based method is expected to behave better on large scale systems. This part of data might be implemented to show its advantage in reducing the computational complexity from O(N4) to O(N3).\n\n3. The approach raised was claimed to bring a stronger convergence guarantee compared with conventional SCF, while this was not supported by a related instance. A successfully converged calculation done by D4FT which failed in conventional SCF would be valuable.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and clearly written. The mathematical derivation is solid. Since several machine learned DFT schemes were developed previously, the novelty of a new one is largely determined by its efficiency. The corresponding information await replenishment. The calculations presented in this manuscript are reasonable and reproducible.",
            "summary_of_the_review": "The paper developed a deep learning approach for solving KS-DFT by converting the objective function for KS-DFT into an unconstrained equivalent by reparameterizing the orthogonal constraints as a feed-forward computation. Its equivalence to the conventional SCF method is proven both empirically and theoretically. The presentation in the manuscript is clear. I recommend it for publication if authors address the above mentioned comments and offer some more substantial experimental data.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2745/Reviewer_fw91"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2745/Reviewer_fw91"
        ]
    }
]