[
    {
        "id": "eWhLZz81oF",
        "original": null,
        "number": 1,
        "cdate": 1666628865839,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628865839,
        "tmdate": 1666628865839,
        "tddate": null,
        "forum": "x70_D-KGEMx",
        "replyto": "x70_D-KGEMx",
        "invitation": "ICLR.cc/2023/Conference/Paper3019/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a Langevin-style approach to posterior approximation in TD learning.\nThe idea is that using this style of MCMC learning we can get useful uncertainty estimates, and better decision making as we scale to deep RL.",
            "strength_and_weaknesses": "There are a few things to like about the paper:\n- This paper is taking on a \"big\" problem that would be of very high value to the ML community.\n- The general idea of using Langevin-style updates is appealing, and may hold promise in the future.\n- The general progression of algorithms, theorems and then empirical results is a good structure.\n\nHowever, there are some significant shortcomings in this paper:\n- There are many pieces of this algorithm that don't quite make sense... why go through all the problems of Bayesian inference to then use \"epsilon-Boltzmann exploration\"? This is clearly an inefficient dithering scheme, if you were going to use this then you really give up the main lure of Bayesian RL.\n- There are some important holes in the related literature, to highlight two:\n  1. Deep Exploration via Randomized Value Functions (Osband et al)\n  2. Langevin DQN (Dwaracherla + Van Roy)\n  - reviewing these papers + related works in the area will be very important to help make sense of this work. Paper (1) gives an overview of the importance of \"deep epxloration\" and possible ways to have tractable posterior approximation in deep RL. Paper (2) explores an approach very linked to the LKTD that you propose.\n- The quality/clarity of the experimental results is lacking... none of the curves are separated beyond the confidence intervals + the the effects are confusing overall. I would suggest using something like the \"DeepSea\" environments available in opensource \"bsuite\" to give you something clear and simple to get started with... similar to paper (2)",
            "clarity,_quality,_novelty_and_reproducibility": "See weaknesses above",
            "summary_of_the_review": "Overall I think this paper is taking on an interesting problem, with a potentially interesting approach.\nHowever, there are some glaring holes in the related work... also, the experimental results are a bit muddled/unclear.\nI think that, at a minimum, revisiting the paper in light of that related work should be the first step for this paper.\nIn its current form, I do not recommend acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3019/Reviewer_2cQs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3019/Reviewer_2cQs"
        ]
    },
    {
        "id": "4JKzjp6_Xim",
        "original": null,
        "number": 2,
        "cdate": 1666641646702,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641646702,
        "tmdate": 1666641646702,
        "tddate": null,
        "forum": "x70_D-KGEMx",
        "replyto": "x70_D-KGEMx",
        "invitation": "ICLR.cc/2023/Conference/Paper3019/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an approach to quantify the uncertainty in value based RL algorithms such as TD learning and Q-learning.  The approach is based on prior work that uses a state-space modeling approach to derive a Kalman filtering algorithm for TD learning (KTD and KOVA). Unless the function approximation is linear and a normality assumption holds, this requires approximations that have a quadratic complexity in the number of parameters.  Instead, the current work has the goal of applying Langevinized ensemble approach to the value estimation setting, in order to get a lower complexity.",
            "strength_and_weaknesses": "The most serious issue in my opinion is clarity and the assumptions made wrt theory that seem hard to justify. For example, Assumption 2 appears to be completely unreasonable -- $\\theta_t$ is the parameter for the model, and the assumption says that the environment changes at each time step to fit the current model.\n\nIn connection with dynamic policy, what is the precise definition of the map that induces a stationary distribution for a given $\\theta$? This is never specified.\n\nThere is also a serious problem with clarity, where the density function in Assumption is called $\\pi(z_t | \\theta_{t-1})$. My understanding is that the authors use $\\pi$ for a prior on the parameter space $\\theta$. How did this prior in the param space morph into a distribution on the samples?\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the paper has serious issues with clarity that make it hard to review which I indicated above.\n\nSome other nits I noted down while reading: \n\n* In section 2.2 above Eq (1) has $\\theta \\in \\theta$ \n\n* Using \\rho for policy and \\pi for prior might make the paper a bit less readable compared to following the usual notation of \\pi for policy.\n\n* Eq (2) has n in the expectation without definition.\n\n* The notation for $\\tilde{\\mathcal{F}}_z(\\theta)$ is inconsistent with a tilde missing sometimes (e.g. in first line of pg3), and the $\\theta$ dependence missing in some other places (eg. Eq (3)). \n\n* Add a citation for \"it has been proven under some regularity assumption\"... in L3 of pg 3.\n\n* Eq (5) looks odd, since its not a real conditional switch -- might be best to use one and say that the action ablated version is straightforward to generalize to? Same thing about simplifying Eqs (6) and (9) to use one particular setting since they are both identical.\n\n* What is the $\\pi$ in Eq (11)? Is it the same as $\\pi_*$?\n\n* Algorithm 1 lacks a description of $r^f_t$.\n",
            "summary_of_the_review": "I believe the assumptions are completely unreasonable as I mentioned above. Besides that, there are serious issues with the clarity and significance due to the incremental nature of applying prior work. The main delta compared to previous work appears to be in applying the prior work to the linear model defined by the TD loss, which seems minor even ignoring the assumptions and clarity. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No particular concerns",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3019/Reviewer_soqG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3019/Reviewer_soqG"
        ]
    },
    {
        "id": "dFHz-O9YrsB",
        "original": null,
        "number": 3,
        "cdate": 1666657563724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657563724,
        "tmdate": 1666657563724,
        "tddate": null,
        "forum": "x70_D-KGEMx",
        "replyto": "x70_D-KGEMx",
        "invitation": "ICLR.cc/2023/Conference/Paper3019/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": " This paper proposed a sampling framework LKTD for value-based RL that converges to a stationary distribution and enables efficient sample generation from the posterior. They show numerical experiments that indicate LKTD is comparable to Adam while outperforming it in robustness.",
            "strength_and_weaknesses": "## Strengths\n- This paper is in general well-written and relatively easy to follow. The proof seems to be theoretically sound. \n## Weakness\nI worry about the novelty and significance of this work. \n\n- For instance, it is stated in the major contributions that the new algorithm is scalable to dimension, and is computationally efficient, but I don't see a clear comparison of this work with KTD, KOVA in terms of computational complexity. \n- Also, all experiments are comparing LKTD with Adam rather than KTD, KOVA. I wonder is there a significant improvement compared with these two methods.",
            "clarity,_quality,_novelty_and_reproducibility": "As stated above, the novelty and significance of this work seem minor to me based on the current paper. At least I would like to see experiments that: (1) show LKTD is efficient compared with e.g. KTD, KOVA in terms of dimension dependence, hence validating the novelty stated at the end of section 1; (2) compare LKTD with KTD, KOVA in terms of MSE, SOAR to validate a non-trivial improvement in robustness and exploration. \n\nMinor questions/typos:\n- What's the definition of $W_2(\\cdot,\\cdot)$? \n- What's Figure 4b showing? Is 'figure 3b' in the last sentence on page 7 a typo?",
            "summary_of_the_review": "In all, although this paper is well-written and seems to be theoretically sound, I believe for now its novelty and significance (especially over KTD, KOVA) haven't been validated by its experiments. I would like to see more experiments focusing on these aspects in the revision if possible. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3019/Reviewer_ZkBy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3019/Reviewer_ZkBy"
        ]
    },
    {
        "id": "fdT6R8yqqb",
        "original": null,
        "number": 4,
        "cdate": 1667033498424,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667033498424,
        "tmdate": 1667033498424,
        "tddate": null,
        "forum": "x70_D-KGEMx",
        "replyto": "x70_D-KGEMx",
        "invitation": "ICLR.cc/2023/Conference/Paper3019/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Langivinized Kalman Temporal Difference (LKTD) learning, a new algorithm for value based reinforcement learning (RL). LKTD optimizes the mean squared Bellman error (MSBE) objective function using SGLD, in contrast to classical algorithm such as residual gradient which applies gradient descent, and temporal difference learning which uses semi-gradient update. The paper proves the convergence of LKTD and provides empirical studies in synthetic problems and simple control problems. ",
            "strength_and_weaknesses": "Strength \n\nThe paper provides a novel perspective to study value-based RL. The main idea is to optimize MSBE using SGLD which is novel to me. Following this perspective, convergence guarantee of the proposed algorithm can be derived. The paper also provides empirical study to show the effectiveness of the algorithm. \n\nWeakness\n\nAs discussed above, in my opinion the main contribution of the paper is to use SGLD to optimize MSBE. So the key to decide if the contribution is significant enough is to understand if this new method can introduce some advantage from either empirical or theoretical perspective. However this is not clear to me as explained in details below. Please correct me if I missed anything important. I am happy to adjust my score based on how well the authors answer the questions in the rebuttal. \n\n1. One classical method in RL is the residual gradient (RG), which directly optimizes MSBE using gradient descent [1]. In fact, as discussed in Remark 1, the gradient is the same as RG (plus a regularization of the parameter). As a gradient-based method, the convergence of RG is robust with both linear and non-linear function approximation. I am almost sure that under the data collection assumption used in the paper, it can also be proved that RG converges. If that\u2019s the case, what\u2019s the advantage of LKTD compared to RG? \n\n2. It is known that one problem of RG is the double sampling issue. That is, in stochastic environment, to obtain an unbiased gradient estimate, we need two independent samples. It seems to me that LKTD has the same problem? \n\n3. The author suggests that equation 23 can be used to connect LKTD with DQN. However, besides the semi-gradient TD update, DQN also has other components such as replay buffer and target network. Also, the data stored in the replay buffer is collected from a very different way from Assumption 1. How do these fit the analysis of LKTD? \n\n4. It seems that one of the main contributions is to prove LKTD converges under Assumption 1. However, Assumption 1 basically considers an on-policy setting, in which case the convergence of TD with function approximation can also be proved, see [2,3] for example. Again, what\u2019s the advantage of LKTD compared to TD?  \n\n5. There are no related works about RG and TD discussed in the paper. \n\n6. It\u2019s good to see that the paper also test LKTD empirically. But from the experiment results, it is now clear if LKTD significantly outperforms a TD based algorithm. Also, the test domains might be too simple. I think it will be very convincing if the author can show competitive performance compared to TD based algorithm in more challenging  problems. \n\n7. In the experiments the baseline is Adam. But Adam is an optimization algorithm, what's the objective used in the baseline algorithm? For the control problem, does it mean DQN with Adam as the optimizer? \n\n[1] Baird, L., 1995. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning Proceedings 1995 (pp. 30-37). Morgan Kaufmann.\n\n[2] Bhandari, J., Russo, D. and Singal, R., 2018, July. A finite time analysis of temporal difference learning with linear function approximation. In Conference on learning theory (pp. 1691-1692). PMLR.\n\n[3] Tsitsiklis, J. and Van Roy, B., 1996. Analysis of temporal-diffference learning with function approximation. Advances in neural information processing systems, 9.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized, and I think most of the writings are clear. However, I think some terminologies are kind of confusing when I first read the paper. For example, the title is \"A sampling framework\", but in fact the paper is about a new algorithm to learn parameters of a value function. The paper says \"a state $\\theta$\", but it's actually the parameter to be learned. My conjecture is that the authors are from another field. I am not saying that using these new terminologies is wrong, and it is always great to see researchers from other fields also care about RL and machine learning in general. However, I think maybe it's better to make some efforts to bridge the \"cultural gap\", so that the paper is more readable for the potential audience. \n",
            "summary_of_the_review": "I recommend rejecting this paper as the advantage of the proposed algorithm is not clear at this point. I hope the authors can address my questions in the rebuttal. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3019/Reviewer_Je2E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3019/Reviewer_Je2E"
        ]
    }
]