[
    {
        "id": "H3S8Rm_z-UJ",
        "original": null,
        "number": 1,
        "cdate": 1666575325245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575325245,
        "tmdate": 1670497925190,
        "tddate": null,
        "forum": "r90KYcuB7JS",
        "replyto": "r90KYcuB7JS",
        "invitation": "ICLR.cc/2023/Conference/Paper2774/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper concerns the approximation power of ReLU FNNs for Sobolev (or actually H\u00f6lder) smooth functions defined on spheres. This paper constructs ReLU FNNs with certain architectures to obtain their approximation results. The main idea is to leverage approximation capability of the homogeneous harmonic polynomials on spheres. The obtained results are mainly compared with those in Schmidt-Hieber (2020), which nevertheless concerns the ReLU FNN approximation on functions defined on $d$-cubes. By comparison, it is claimed that their approximation results can alleviate the dependence of network size (number of total parameters) on dimension $d$ to achieve similar approximation error bounds in Schmidt-Hieber (2020). In addition, the paper has also discussed on the approximation results when the smoothness of the target function $r$ varies w.r.t $d$.",
            "strength_and_weaknesses": "Strength:\n1.In general, the paper is well organized with a clear presentation.\n2.The paper derived new approximate results for Sobolev functions defined on spheres in the sense of tracking the explicit dependency on $d$ in the constant factor compared to existing ones. \n3.The paper talked about the results when the smoothness $r$ varies with respect to dimension $d$.\n\nWeakness\n1.The obtained estimation error rate does not achieve the expected optimal minimax rate.  Regarding to the comparison in Table 1, it looks a bit weird for me since sphere $\\mathcal{S}^{d-1}$ is a subset of the cube $[0,1]^d$, but the estimation rate on a subset \nIs even slower. Is there a way to fix it? Or is there a reasonable explanation?\n\n2.Regarding to the main result in Theorem 3.1, the approximation error has basically dependence $d^{N+d} \\times N^{d}/\\sqrt{M}$. This result seems also cursed by $d$ in numerator, otherwise one has to set a very large $M$, e.g. $M=d^d$ to get a good approximation accuracy. Please explain more on this.\n\n3.The condition $r=O(d)$ is restrictive. The author may explain more on this condition, especially it\u2019s better to give an example on in what scenarios the condition $r=O(d)$ holds. In addition, I wonder if the condition $r=O(d)$ can be plugged in other existing results, e.g., Schmidt-Hieber (2020) to have a comparison with the obtained results in these $d, r$ varying cases. If it\u2019s not possible, please also explain why $r=O(d)$ can not directly be plugged in other results.\n\n4.The comparison can be fairer since Schmidt-Hieber (2020) focus on the approximation on hyper-cubes instead of sphere. The authors can compare their results with Fang et al. (2020) and Feng et al. (2021) since a large part of the proof follows that of Fang et al. (2020). The most notable difference between Fang et al. (2020) and this paper is that this paper track the explicit dependence of the constant factor on $d$. In addition, though Fang et al. (2020) and Feng et al. (2021) considered CNN approximations, Zhou, D.X. (2020). has shown that any CNN can be equivalently computed by a FNN with parameters at most 8 times larger than that of CNN. In light of this, the author should compare their results with Fang et al. (2020) and Feng et al. (2021).\n\n5.Within the studies of ReLU FNN approximation, the author can also compare with more recent results which have already improved the results in Schmidt-Hieber (2020). For example, Shen, Yang, and Zhang. (2020) and Lu, Shen, Yang, and Zhang. (2021) have already demonstrated the clear dependence on dimension $d$ for ReLU approximation results. In line with these works, Jiao, Shen, Lin and Huang (2021) has also shown the explicit dependence of ReLU approximation results on $d$ as well as obtained polynomial dependence results.\n\nMinor comment:\n1.For reference, the issues about the dependence on dimension $d$ was also discussed in Ghorbani et al. (2020).\n2.The obtained approximation results are for Sobolev function space $W^r_\\infty$, which is actually H\u00f6lder function space. This is also mentioned in Appendix A.\n\nReference\n\nSCHMIDT-HIEBER, J. (2020). Nonparametric regression using deep neural networks with ReLU activation function(with discussion). Ann. Statist. 48 1875\u20131897.\n\nDing-Xuan Zhou. Theory of deep convolutional neural networks: Downsampling. Neural Networks,\n124:319\u2013327, 2020.\n\nSHEN, Z., YANG, H. and ZHANG, S. (2020). Deep network approximation characterized by number of neurons.\nCommun. Comput. Phys. 28 1768\u20131811.\n\nLU, J., SHEN, Z., YANG, H. and ZHANG, S. (2021). Deep network approximation for smooth functions. SIAM\nJournal on Mathematical Analysis 53 5465\u20135506.\n\nJiao, Y., Shen, G., Lin, Y., & Huang, J. (2021). Deep nonparametric regression on approximately low-dimensional manifolds.\u00a0arXiv preprint arXiv:2104.06708.\n\nGHORBANI, B., MEI, S., MISIAKIEWICZ, T. and MONTANARI, A. (2020). Discussion of: \u201cNonparametric regression using deep neural networks with ReLU activation function\u201d. Ann. Statist. 48 1898\u20131901.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The presentation is clear.\nQuality: Fair.\nNovelty: Fair.\nReproducibility: N.A.",
            "summary_of_the_review": "In general, the paper is well organized with a clear presentation. This paper concerns the approximation power of ReLU FNNs for H\u00f6lder smooth functions defined on spheres. This paper constructs ReLU FNNs with certain architectures to obtain their approximation results, while a large part of the proof follows that of Fang et al. (2020). The most notable difference between Fang et al. (2020) and this paper is that this paper track the explicit dependence of the constant factor on $d$. Besides, several major questions raised in \u201cWeakness\u201d part need to be addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2774/Reviewer_52e1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2774/Reviewer_52e1"
        ]
    },
    {
        "id": "PlrCu1jSZ-",
        "original": null,
        "number": 2,
        "cdate": 1666620110255,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620110255,
        "tmdate": 1666620110255,
        "tddate": null,
        "forum": "r90KYcuB7JS",
        "replyto": "r90KYcuB7JS",
        "invitation": "ICLR.cc/2023/Conference/Paper2774/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the approximation of functions using neural networks with ReLU activations. They showed that, for the functions defined on some Sobolev spaces over the d-dimensional unit, there exists a fast approximation error rate of $d^{-d^\\beta}$ by some neural networks, which is very sharp. Also, the construction of the suggested neural networks is given.",
            "strength_and_weaknesses": "The main results in this paper will certainly help us have a better understating of the approximation of deep neural networks from a theoretical way. The techniques in this paper may be extended to more general architectures.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is clearly written and well organized. I find it easy to follow.\n\nQuality: This paper is technically sound.\n\nNovelty: I think the results in this paper are significant, as explained above. ",
            "summary_of_the_review": "In summary, I think the contribution of this paper is enough and this paper is suitable for ICLR, as explained above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2774/Reviewer_JPQC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2774/Reviewer_JPQC"
        ]
    },
    {
        "id": "6iHc-_FECFj",
        "original": null,
        "number": 3,
        "cdate": 1666637185835,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637185835,
        "tmdate": 1666637185835,
        "tddate": null,
        "forum": "r90KYcuB7JS",
        "replyto": "r90KYcuB7JS",
        "invitation": "ICLR.cc/2023/Conference/Paper2774/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper investigates the theoretical properties of deep feed-forward neural networks (FNN) with ReLU activation function when the target function belongs to a Sobolev space defined over the $d$-dimensional unit sphere with smoothness index $r>0$. The bounds for both approximation error and excess risk are derived for two different regimes as $d\\rightarrow\\infty$, which explicitly depend on the data dimension $d$ as follows\n\n1. In the regime where $r=O(1)$: at most $d^d$ active parameters to get the approximation rate of $d^{-C}$ with $C>0$ being a constant; Moreover, the excess risk bound has a $d^d$ factor\n\n2. In the regime where $r=O(d)$: at most $d^2$ active parameters to get the approximation rate of $d^{-d^{\\beta}}$ with $0<\\beta<1$; Moreover, the excess risk bound has a $d^{O(1)}$ factor\n\nThe authors make comparisons to the case where the input space is d-dimensional cubes and highlight the novelty of their results.",
            "strength_and_weaknesses": "$\\textbf{Strength}$\n\n1. To my best knowledge this is a novel contribution that broadens the understanding of deep ReLU FNN for approximating Sobolev functions defined on the high-dimensional sphere ($d\\rightarrow\\infty$), a topic of great interest to the ICLR community\n\n2. The paper explicitly describes the dependence of the approximation error and excess risk on the data dimension $d$, which shows that the curse of dimensionality may be avoided when $r=O(d)$.\n\n3. Excess risk bounds are derived through the lens of pseudo-dimension, which removes the boundedness requirement of the weight parameters in the literature, e.g., Schmidt-Hieber (2020).\n\n4. A different phenomenon is observed compared to the case for $d$-dimensional cubes, which is interesting and novel.\n\n$\\textbf{Concerns}$\n\n1. Can the results or analysis be extended to other neural networks, e.g. convolutional neural networks?\n\n2. Will it make any difference if we consider the regime $r=O(d^\\alpha)$ for some $0<\\alpha<1$?\n\nMinor typos:\n\n1. In Table 1, 4th row & 3rd column, should $O(d^2)$ be $O(d^d)$?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written with a sufficient literature review. The theoretical analysis is neat and well-presented. The results are novel, significant, and of great interest.",
            "summary_of_the_review": "The paper presents novel results on how the approximation rate (excess risk) of deep ReLU FNN depends on the data dimension when the input space is a high-dimensional sphere, which provides new insights into the approximation analysis of deep neural networks and will be of great interest to the community of deep learning theory. The intuition is clearly explained and the theorems are technically sound.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2774/Reviewer_JVzW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2774/Reviewer_JVzW"
        ]
    }
]