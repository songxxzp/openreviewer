[
    {
        "id": "qkFEdGDZCZ",
        "original": null,
        "number": 1,
        "cdate": 1666371870471,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666371870471,
        "tmdate": 1666371870471,
        "tddate": null,
        "forum": "01KmhBsEPFO",
        "replyto": "01KmhBsEPFO",
        "invitation": "ICLR.cc/2023/Conference/Paper774/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper propose a new model architecture and training method to classify digital pathology whole-slide images (WSI). These images are in the giga-pixel range and cannot be classified directly by a model. The paper proposes a multi-instance learning approach with contrastive self-supervised learning for patch level feature extraction and an iterative low-rank attention model for the multi-instance feature aggregation. The method is tested on 3 public digital pathology datasets and shown to significantly outperform SOTA. In an ablation study, other feature extraction and MIL techniques are compared to the proposed one, showing superior results of individual components. In a parameter analysis, low-rank size, pooling method and number of iterations are studied.\n\n",
            "strength_and_weaknesses": "Pros:\n\n* A sophisticated MIL model that significantly beats SOTA on several public datasets\n\n* Both feature-extraction and MIL steps are improved, as shown by the ablation study.\n\n* An exploration of several hyper-parameters show a sweet spot for the number of iterations in MIL, an asymptotic behavior for the rank of the low-rank decomposition and the superiority of nonlocal pooling against regular max-pooling or local-attention-pooling.\n\n\nCons:\n\n* There are no speed information given for training and inference. How long does it take to classify a slide on average ? How long does it take to train the feature extractor, the MIL model ?\n\n* no details are given on which type of augmentation is used in the contrastive loss.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is very well written, well organized and clear.\n\n* The method proposed contains several contributions: improved loss on feature extraction (low-rank constraints added to SupCon loss); new MIL aggregator (their iterative gated attention low-rank blocks appears to be novel)\n\n* The authors claim that code will be available. Without it, i think it is hard to reproduce this approach as it consists of a complex loss for the self-supervised feature learning and a complex model for the MIL part. Also details on what augmentation to use for the contrastive learning are missing, \n\n",
            "summary_of_the_review": "Overall, a good paper with a clear and convincing approach that beats SOTA.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper774/Reviewer_j84C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper774/Reviewer_j84C"
        ]
    },
    {
        "id": "NpkI-tN0-Jk",
        "original": null,
        "number": 2,
        "cdate": 1666623770533,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623770533,
        "tmdate": 1666624691935,
        "tddate": null,
        "forum": "01KmhBsEPFO",
        "replyto": "01KmhBsEPFO",
        "invitation": "ICLR.cc/2023/Conference/Paper774/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the problem of WSI classification by optimizing the feature embedding and feature aggregation with low-rank properties. Firstly, the paper improves the vanilla contrastive loss with additional low-rank constraints to collect more positive samples for contrast. Then, the authors devise an iterative low-rank attention feature aggregator to make efficient non-local interactions among instances. Experiments show the effectiveness of the proposed model. ",
            "strength_and_weaknesses": "Strength:\n(1) The authors extend contrastive learning with a low-rank constraint (LRC) to learn feature embedding using unlabeled WSI data.\n(2) The authors design an iterative low-rank attention MIL (ILRA-MIL) to process a large bag of instances, allowing it to encode cross-instance interactions.\n(3) Experiments are conducted on multiple datasets and the effectiveness is well studied. \n\nWeakness:\n(1) In the Abstract, the authors state that \u201cwe highlight the importance of instance correlation modeling but refrain from directly using the transformer encoder considering the O(n^2) complexity\u201d. However, in the main body part, there are not any details to discuss the importance of instance correlation. Besides, in the experimental section, the importance should be investigated.\n(2) The authors should also provide more details to discuss cross-instance interaction, and whether is it helpful to boost the classification performance?\n(3) Some details of experimental settings are missing. For example, k is the total number of layers. Thus, in the experiments, how to set k? Besides, for the used contrastive loss, which type of augmentation is adopted in this study? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper leverages the properties of the apparent similarity in high-resolution WSIs, which essentially exhibit low-rank structures in the data manifold, to develop a new MIL with a boost in both feature embedding and feature aggregation. The authors have claimed that code will be available. ",
            "summary_of_the_review": "Overall, the structure is clear and the proposed model is well-validated. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper774/Reviewer_LxYk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper774/Reviewer_LxYk"
        ]
    },
    {
        "id": "nfX_q0zJxZX",
        "original": null,
        "number": 3,
        "cdate": 1666798776521,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666798776521,
        "tmdate": 1666798776521,
        "tddate": null,
        "forum": "01KmhBsEPFO",
        "replyto": "01KmhBsEPFO",
        "invitation": "ICLR.cc/2023/Conference/Paper774/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a novel framework for Whole Slide Image (WSI)  classification based on two\nlow-rank methods:\n1. A Low-Rank Constraint (LRC) feature embedder to extract features from pathological slides in\na self-supervised fashion.\n2. An Iterative Low-Rank Attention MIL (ILRA-MIL) model for bag-level classification , derived\nfrom the vision transformers architecture.\n\nBoth methods rely on low rank assumptions, i.e. the existence of a data representation in a smaller dimension space that exhibit more discriminatory characteristics for feature embedding on the one hand, and lower computational complexity for bag-level prediction on the other.",
            "strength_and_weaknesses": "Strength : \nThe approach presented here is quite new in the field of digital pathology, and the authors show\nthat the enforcement of low-rank constraints on top of existing self-supervision and attention-based\nMIL strategies improves the results on several whole slide image classification datasets.\n\nWeaknesses : some explanations are not clear enough or suffer from ambiguous notations, and overall there are several errors along the manuscript.",
            "clarity,_quality,_novelty_and_reproducibility": "The authors should carefully proofread their work again to correct all the spelling mistakes, missing words\nand other typos.\nFor instance :\n- In equation (5), the brackets should cover the entire set of conditions, and i and j should not be written as indices\n- In the same section, one can read: \\... over the most- and least-distant subspace C1(a), Cr(a) ...\".\nMost and least distant should be switched to be coherent with the rest of the explanations. The same mistake is repeated in the appendix.\n- Where does the index k come from after equation (6)? Is there a relation between i and k or j and k? Overall, this small paragraph after equation (6) is not very clear. Could the authors give more explanations as to why this threshold is needed?\n- The first sentence of section 3.2.3 does not make much sense. Could the authors rephrase? Perhaps\nthe word \\image\" is missing after \\low-rank\".\n- What exactly is xb? A linear layer? What is \\rho ? Is it the same \\rho as in equation (10)?",
            "summary_of_the_review": "The paper brings an interesting idea worth being introduced in the community. But it lacks clarity and is impaired by many typos, missing words and unprecise formulas.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper774/Reviewer_NvEE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper774/Reviewer_NvEE"
        ]
    },
    {
        "id": "P1HGDihVvNC",
        "original": null,
        "number": 4,
        "cdate": 1667051706162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667051706162,
        "tmdate": 1667051706162,
        "tddate": null,
        "forum": "01KmhBsEPFO",
        "replyto": "01KmhBsEPFO",
        "invitation": "ICLR.cc/2023/Conference/Paper774/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper tackles the problem of multiple instance learning in pathology imaging where annotations are available only at slide level and the goal is to predict it at instance level. The author exploits the low-rank structure in the data to design a new contrastive learning approach. LRC is proposed as an extension SupCon approach for low-rank data by defining the loss function on the most- and least-distant subspaces. At the feature aggregation level, an iterative approach is proposed using transformers. A learnable low-rank latent matrix L is used across the layers to encode global features. The transformer model named ILRA is based on two transformer modules GABf which reduces the space dimension and GABb restores it. The ILRA transformer layer is applied k times. The proposed approaches are benchmarked on three datasets, showing the improved performance ",
            "strength_and_weaknesses": "Strengths\nLow-rank property provides good inductive bias in applications such as in computational pathology\n\nWeakness\nNo validation with multi-center datasets\nThe computation time has not been discussed and could be prohibitive for real-world usage where inference time needs to be reduced.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not so easy to read. Its clarity could be further improved.\n",
            "summary_of_the_review": "I think the main drive for improvements in the proposed model is the contrastive learning loss LRC rather than the proposed global features. ILRA-MIL alone does not do better than the baseline methods. This would require further investigation and limits the novelty of the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper774/Reviewer_oayv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper774/Reviewer_oayv"
        ]
    }
]