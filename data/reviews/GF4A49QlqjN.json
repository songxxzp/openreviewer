[
    {
        "id": "wywLf7AHW-",
        "original": null,
        "number": 1,
        "cdate": 1666599538420,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599538420,
        "tmdate": 1669281882242,
        "tddate": null,
        "forum": "GF4A49QlqjN",
        "replyto": "GF4A49QlqjN",
        "invitation": "ICLR.cc/2023/Conference/Paper2843/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes SuperWeight Ensembles which generate parameters of diverse architectures from single SuperWeight parameters. To efficiently find the parameters, authors suggest making SuperWeight from templates with the linear combination using coefficients \\alpha. Then, cluster the weights based on the gradient similarity. After constructing SuperWeight, layer weight construction determines the coefficients for generating a layer-wise SuperWeight and assembled into diverse members of architecture. SuperWeight Ensembles are able to generate parameters for both homogeneous architectures and heterogeneous architectures.",
            "strength_and_weaknesses": "**Strength**\n\n- This paper propose parameter sharing from SuperWeight for both homogeneous architectures and heterogeneous architectures.\n- The proposed method is able to obtain better performance both in ensembling and in any time inference tasks while it shows a marginal improvement in ensembling compared to baselines.\n- The approach that the current paper is proposing seems reasonable and convincing.\n\n**Weakness**\n\n- Since the proposed methods contain several steps to construct diverse architectures with the sharing parameters, it was difficult to understand at first for me. I hope the authors provide a broad concept for each step which is more abstract than figure 2.\n- As mentioned in the related works, I think the author also should compare the performance with the zero-shot NAS which does not cost at all to find high-performance networks.\n- There are missing results of CIFAR-10-C and ImageNet-C. I hope the authors to provide the experimental results also in CIFAR-10-C and ImageNet-C.\n\n**Question**\n\nI might have missed the following information in the paper.\n- How to generate templates is unclear in section 3.1. Is \\alpha learnable variables? Or deterministic variables?\n- How to construct each member from the layer weight?\n- What is the stop criteria that is explained in the 4. of Section 3?",
            "clarity,_quality,_novelty_and_reproducibility": "- **Clarity:** The method is understandable. However, the motivation and the problem that the current paper is tackling compare to the previous works seem quite unclear to me. It is understandable but I hope the authors clarify what will be the main limitation of previous works and what is the difference and contributions of their approaches.\n- **Quality:** This paper shows a good quality presentation of their methods and results.\n- **Novelty:** The idea is very novel to me because this approach could generate parameters for both homogeneous networks and heterogeneous networks. However, the criteria, such as using gradient similarity seem to lack novelty.\n- **Reproducibility:** I think this paper shows quite a low reproducibility. It is difficult for me to understand the very details of each step of the approach. For example, how to set weight templates is unknown. How to learn \\alpha is also unknown.",
            "summary_of_the_review": "Overall, I recommend marginally above the acceptance threshold. I hope the authors could resolve my concerns in the rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2843/Reviewer_owxX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2843/Reviewer_owxX"
        ]
    },
    {
        "id": "rr7lAItwrgv",
        "original": null,
        "number": 2,
        "cdate": 1666646775210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646775210,
        "tmdate": 1666646775210,
        "tddate": null,
        "forum": "GF4A49QlqjN",
        "replyto": "GF4A49QlqjN",
        "invitation": "ICLR.cc/2023/Conference/Paper2843/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper builds from the work in [1] to make efficient ensembles. This paper proposes to build ensembles by using shared templates/layers across ensemble members. Unlike previous work in this domain, they mention that because they can use average weights of different dimensions [1], they can combine models with layers of different sizes which allows them to form ensembles of different sizes given fixed parameter setting, which can help tradeoff accuracy vs compute time. \n[1] Plummer et al. 2022\n",
            "strength_and_weaknesses": "- The paper proposes a new way to form deep ensembles which is not seen in the literature\n- The paper could benefit from an explanation of the results\n\n\nOpen questions and comments\n- From the paper, the reader is required to go over [1] to understand the template generating process, as this is not a standard approach I would suggest to devote a section of the paper to this part, which in turn would also make the paper accessible to a wider audience.\n- It woul be ideal if the authors could outline the contributions of the paper vs [1]. \n- In Fig 1 caption the authors claim their approach allows them to learn optimal soft parameter sharing. Why is it optimal? \n- Most results don\u2019t show confidence intervals.\n- Table 2. What would happen if the authors added an ablation study for different # of parameters for imagenet? Does adding more parameters match the performance level for deep ensembles or perform even better?\n- Table 1(b) is the text is referred as table 1(c). \n- The paper could also benefit from a discussion of the computational implications/limitations of the proposed approach during training time.  This would be informative for readers considering to use the proposed approach over standard deep ensembles.\n- The authors mention that because they need to cluster layers across models by layer similarity they use a coefficient lambda to check the gradient cosine similarity. In this case, the layers would need to have the same form don\u2019t they? Could be combine layers from a vit model with a can model using this approach? The authors cite Raghu which notes that there are several differences across the representations, so this part is confusing to me.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper could benefit from a clearer explanation of the results.",
            "summary_of_the_review": "Overall this paper proposes an interesting method motivated by an important problem (anytime prediction) not usually motivated in the literature. The paper would greatly benefit from a more detailed discussion of the methods and the results (see above) to improve its accessibility to a wider audience.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2843/Reviewer_Qjhr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2843/Reviewer_Qjhr"
        ]
    },
    {
        "id": "gLD4EeTF9Z7",
        "original": null,
        "number": 3,
        "cdate": 1666681305113,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681305113,
        "tmdate": 1666681305113,
        "tddate": null,
        "forum": "GF4A49QlqjN",
        "replyto": "GF4A49QlqjN",
        "invitation": "ICLR.cc/2023/Conference/Paper2843/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper propose SuperWeight Ensemble, a new method for parameter sharing and network ensemble which is more flexible. Better results are obtained in experiments.",
            "strength_and_weaknesses": "The proposed method is flexible, which allows ensembles of network with different architectures, and enables anytime inference, which is important.\n\nBetter performance with fewer parameters are obtained, which illustrates the effectiveness of the proposed method.\n\nWriting of section 3 needs to be polished, currently it is a little bit vague.\n\nSome ablation studies are suggested, how does each component/step of the algorithm contribute to the performance. For example, what will the result be if we only have one cluster, or only have one group in each cluster?\n\nHow about the training time needed compared to other related works.\n\nHow will the performance be on more complicated datasets other than CIFAR.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the clarity is good, but the writing of algorithm part needs to be polished. The experiments seems to be reproducible.",
            "summary_of_the_review": "The paper propose a useful method which leads to flexible and effective parameter sharing and ensemble, good experimental results are obtained.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2843/Reviewer_d45p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2843/Reviewer_d45p"
        ]
    }
]