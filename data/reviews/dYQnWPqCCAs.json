[
    {
        "id": "2PvJbhSuf-",
        "original": null,
        "number": 1,
        "cdate": 1666531244127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666531244127,
        "tmdate": 1666531244127,
        "tddate": null,
        "forum": "dYQnWPqCCAs",
        "replyto": "dYQnWPqCCAs",
        "invitation": "ICLR.cc/2023/Conference/Paper1990/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper propose a method to find the differences of how differently trained models make predictions. More precisely, it gives human-interpretable features (or patterns) on how different models recognize different classes. The proposed method heavily relies on how influences of training data on test data are measured. Authors showcase three applications of the proposed method.",
            "strength_and_weaknesses": "Strength\n- The algorithm is simple and the paper is overall well-written.\n- Comparison among models, instead of studying a single model, is a promising direction for model interpretation.\n\nWeakness\n- This work seems like to be related to model interpretation, or explainable machine learning, because it gives a human-understandable interpretation, or explaination of how models predict specific classes. However, related literature reivew and comparions are missing.\n- The process seems to heavily rely on the method used in Stage I, 1. Authors also mentioned the influence function, why it is not used but the datamodel is used?\n- Stage II seems to be non-deterministic for the human-in-the-loop part. What if different results maybe given by the same output of stage I? This weakens reproducibility somehow.\n- Authors lack describing how the method would benefit users in the three application cases. After finding the transformations, how would they give hints on improving any aspects of each scene?",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written and easy to follow, but maybe a little wordy, paragraphs before and after section 2.2 title are essentially the same. \n- The writing quality is good. The proposed method rather have limited novelty and significance.\n- The reproducibility of the human-in-the-loop part of stage II is doutful.\n",
            "summary_of_the_review": "Although being well-written, lacking enough discussion on related work and utility of the proposed method, I would like to reject the paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1990/Reviewer_1DZF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1990/Reviewer_1DZF"
        ]
    },
    {
        "id": "tcxPa8NmR0k",
        "original": null,
        "number": 2,
        "cdate": 1666675935694,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675935694,
        "tmdate": 1668752171638,
        "tddate": null,
        "forum": "dYQnWPqCCAs",
        "replyto": "dYQnWPqCCAs",
        "invitation": "ICLR.cc/2023/Conference/Paper1990/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThe authors present a way of comparing two learning algorithms by\nfiguring out which feature transformations/alterations, of test\ninstances, can alter the output (probability) of model trained by one\nmethod (on average) while not changing the output of another (as\nmuch).  This approach can shed light on how two different algorithms\n(such as whether or how to use pretraining, or data-augmentation, ...) can\ndiffer in specific learning problems, beyond just looking at model accuracy\n(summary performance numbers) and beyond just looking at the model\nbehavior (outputs) on a specific (test) instances, but getting insights into how features\nare being used differently (the patterns of emphasis on various features) by the two algorithms.",
            "strength_and_weaknesses": "Strengths:\n\n-- The contribution can be readily seen to be useful (diagnostics,\n could be important for troubleshooting).\n\n-- Mostly clear and easy to read paper.\n\nWeaknesses (More details after the list):\n\n-- Computational cost or complexity of the approach is not clear (eg\n   each of the 3 steps in stage 1), and may be too expensive.\n\n-- It would be good to have further experiments with regards to\n understanding the properties of the proposed technique:\n limits/weaknesses, strengths, say in discrimination power between two\n algorithms, etc.\n\n\n* Regarding computational cost: you say that the expectation of the\nmodel output on instances is taken with respect to random choises\nwithin algorithm when training as well as instance (Definition 2).\nBut it's not clear how many times you had to sample and retrain, and\nhow that impacts the variance (stability) over the results you\nobserved. I looked at the appendix, and that didn't help much.\n\n\n* With regards to how discriminating of your technique, among two\nalgorithms: If the two learning algorithms are very similar, does the\nmethod find a useful difference? can one set up controlled experiments\nchanging the degree of similarity? For instance, the accuracies of the\ntwo models can be nearly identical and relatively high, eg both at say\nnear 95%, and yet the two models could behave very differently.\n\nHow many dimensions of difference, via PCA, does the approach find?\n(as a function of problem and algorithm characteristics)\n\n-- Any thoughts on how your approach might help one diagnose a model's behavior (a single\n model) and remedy certain undesired model behavior?  (biases, etc).\n\ntypo:\n\nAppendix B.   \"small large rate\"  --> small learning rate \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is clear. It is organized well, and the techniques were\npresented clearly.  There was a good discussion of related work.",
            "summary_of_the_review": "\nThe approach is interesting in getting insights into how/why a model/algorithm behaves a certain way, \nin part by comparing it to another model/algorithm! (and going deeper than just looking at final output\non specific instances).  I raised a few issues (see strengths/weaknesses), but overall I am positive.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1990/Reviewer_UoE7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1990/Reviewer_UoE7"
        ]
    },
    {
        "id": "-0-PH4AXxp",
        "original": null,
        "number": 3,
        "cdate": 1666842138459,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666842138459,
        "tmdate": 1666842138459,
        "tddate": null,
        "forum": "dYQnWPqCCAs",
        "replyto": "dYQnWPqCCAs",
        "invitation": "ICLR.cc/2023/Conference/Paper1990/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a human-in-the-loop framework that first tries to find most distinguishable samples for each algorithm, and then let human summarize observable patterns from those samples. ",
            "strength_and_weaknesses": "Strength: The paper provides several applications of this method, which seems interesting.\n\nWeakness: It is unclear how exactly is the datamodel computed; It is unclear computation-wise whether this framework is practical in general settings; Regardless of computation front, it is unclear whether the framework can work in general settings, e.g., what if human can not find the patterns, does it also dependent on sample size? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is overall clear, but probably misses important details, e.g., how is the datamodel computed, how scalable this method is, how sensitive is the framework to sample size.\n\nQuality: The paper provides 3 interesting case study with concrete example which is good. On the flip side, it is unclear how generalizable the framework is and whether it would fail in other cases. The main idea seems to be an engineering idea more than a scientific idea. \n\nNovelty: The idea is straightforward based on the idea of datamodel from the literature. ",
            "summary_of_the_review": "In summary, the paper seems to be an incremental work built on top of the datamodel idea. However, some details are missing in the paper and the paper is an interesting engineering idea rather than a scientific idea. Therefore, I do not think the paper meets the acceptance bar. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1990/Reviewer_SMZu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1990/Reviewer_SMZu"
        ]
    },
    {
        "id": "FBIJhJ9FAv_",
        "original": null,
        "number": 4,
        "cdate": 1666880885565,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666880885565,
        "tmdate": 1670485513686,
        "tddate": null,
        "forum": "dYQnWPqCCAs",
        "replyto": "dYQnWPqCCAs",
        "invitation": "ICLR.cc/2023/Conference/Paper1990/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a technique for comparing two machine learning algorithms in terms of the most important features/data that influence the prediction of the model the most. The technique is based on the datamodel representation model (Ilyas et.al., 2022). Based on the distinguished features produced by the model, the technique utilizes human-in-the-loop distinguished feature transformation that represents the characteristics of the compared algorithms. The authors tested the proposed technique on three case studies: data augmentation, pretraining, and hyperparameter tuning.",
            "strength_and_weaknesses": "Strengths:\n- Interesting technique in exploring the characteristics of the algorithm and the relation to the datasets for algorithm comparison.\n- The technique is applicable to multiple scenarios.\n\nWeakness:\n- The authors claimed that the technique is applicable universally across different learning algorithms and any domain. However, within the paper, the authors only discuss neural network models and limit the technique to computer vision problems. Even in the human-in-the-loop stage, the technique involves visual inspection, which makes the technique only applicable to computer vision problems.\n- The authors do not introduce any baselines in the experiments. Therefore, it's hard to assess the contribution of the paper completely. \n- The technique involves a human-in-the-loop stage for designing feature transformation. However, there is no clear explanation of how to perform the step.\n\n\n------\n==Post rebuttal==\n\nThanks the authors for the feedback.\nI have read the authors' feedback and other reviewer comments. The feedback addressed some of my concerns. However, some other concerns remain.\n\nI think one of the weaknesses of the paper is in the human-in-the-loop part, where it makes the method not easily applicable to other cases. The method requires the analyst to infer a feature shared by given a subpopulation of inputs. Depending on the application, this stage may be hard to perform.\n\nThe application discussed in the paper is also limited to computer vison task. However, the authors branded the paper as a 'unified framework' for comparing (any) learning algorithms. I think the authors should provide more example from other domains, like text/tabular data, or tone down the claim they made.\n\nTherefore, my recommendation remain the same.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the presentation of the paper is clear, and it is easy to read. The reproducibility of the technique is a bit hard since it involves a human-in-the-loop stage.",
            "summary_of_the_review": "It's an interesting paper overall. But I have some concerns about its applicability. Therefore, I recommend weak rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1990/Reviewer_QoXq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1990/Reviewer_QoXq"
        ]
    }
]