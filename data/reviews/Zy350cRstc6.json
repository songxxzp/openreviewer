[
    {
        "id": "zt3Ivxp4Bl",
        "original": null,
        "number": 1,
        "cdate": 1666252825354,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666252825354,
        "tmdate": 1666252825354,
        "tddate": null,
        "forum": "Zy350cRstc6",
        "replyto": "Zy350cRstc6",
        "invitation": "ICLR.cc/2023/Conference/Paper1293/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper try to show that common state-of-the-art methods in continual learning suffer from substantial forgetting upon starting to\nlearn new tasks, except that this forgetting is temporary and followed by a phase of performance recovery. The main contributions can be summarized as :\n\n(1) This work defines a framework for continual evaluation that evaluates the learner after each update.\n\n(2) This work conducts an empirical study with the continual evaluation framework, which leads to identifying the stability gap for Experience Replay.\n\n(3) This work proposes a conceptual analysis as a hypothesis for causing the stability gap, by disentangling the gradients based on plasticity and stability.",
            "strength_and_weaknesses": "Strengths:\n\n(a) This paper is well-written and easy to read.\n\n(b) The phenomenon found in this work is easy to follow.\n\nWeaknesses:\n\n(a) Some notations are confusing. What  is $|T_{i}|$? In Section 3.2, what is the maximal accuracy increase (lack of detailed formulations)?\n\n(b) My main concern is about the phenomenon---stability gap. This work claims: ``we show that common state-of-the-art methods still suffer from substantial forgetting upon starting to learn new tasks, except that this forgetting is temporary and followed by a phase of performance recovery.'' I am afraid that this phenomenon can not cover most kinds of baselines in CL and only exists in replay-based methods. The phenomenon is trivial in replay-based methods and easy to be explained. The model reaches at a local minimum in the previous task and tries to approach to next task's local minimum quickly in the early several iterations, which leads to the substantial forgetting upon starting to learn new tasks. As replay-based methods replay previous experience, the model learns previous tasks' skills in the following iterations. These lead to the phase of performance recovery. The baselines  discussed in this paper is mainly replay-based (ER and GEM), and the phenomenon in LwF is not well explained.  I think this phenomenon is more likely to not exist in other kinds of baselines---regularization-based (EWC, SI and MAS) and parameter isolation based (PackNet and HAT). For example, the work [1] plots a similar accuracy curve of EWC in Fig.3(a) where there is no stability gap.\n\n[1] Kirkpatrick J, Pascanu R, Rabinowitz N, et al. Overcoming catastrophic forgetting in neural networks[J]. Proceedings of the national academy of sciences, 2017, 114(13): 3521-3526.",
            "clarity,_quality,_novelty_and_reproducibility": "Good quality, but the finding is a little bit trivial and limit to  replay-based methods.\nNice clarity.\nNice originality.",
            "summary_of_the_review": "marginally below the acceptance threshold",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1293/Reviewer_Jyqa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1293/Reviewer_Jyqa"
        ]
    },
    {
        "id": "FX_d25ZPGl",
        "original": null,
        "number": 2,
        "cdate": 1666646960113,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646960113,
        "tmdate": 1666646960113,
        "tddate": null,
        "forum": "Zy350cRstc6",
        "replyto": "Zy350cRstc6",
        "invitation": "ICLR.cc/2023/Conference/Paper1293/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The work is motivated by a problem that current continual learning metrics are mainly coarse-grained (e.g., task-based), which can lead to information loss. For instance, monitoring task-based metrics cannot show the stability gap (substantial but temporary forgetting upon learning a new task). The stability gap is defined as the sudden drop in performance right after facing a new task. \n\n\nFurther, the authors propose various metrics for continual learning that focus on worst-case performance, which can be helpful in practical scenarios. Finally, the paper briefly studies the implications of the stability gap from the task similarity perspective. Also, it shows that this phenomenon exists in the presence of other CL algorithms.",
            "strength_and_weaknesses": "### Strengths\n- The stability gap is an interesting problem in continual learning.\n- The provided metrics are helpful for some sensitive scenarios.\n- The experiments are well-desigend.\n\n\n### Weakness\n- The authors do not propose a mechanism to alleviate the stability gap (and potentially catastrophic forgetting). However, I believe as a first step, the analysis of Sec. 5 is enough to motivate others to work on this problem.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In terms of quality and novelty, I find the work decent. In addition, the paper is well-organized and easy to follow. Regarding the reproducibility, the authors provide comprehensive details of their setup in the paper.",
            "summary_of_the_review": "Overall, While there is room for improvement in a more in-depth analysis of the stability gap, I find this work a good first step that can contribute significantly to the field. Hence, I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1293/Reviewer_FwH6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1293/Reviewer_FwH6"
        ]
    },
    {
        "id": "MPaWTHP-ski",
        "original": null,
        "number": 3,
        "cdate": 1666725483607,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725483607,
        "tmdate": 1669704639517,
        "tddate": null,
        "forum": "Zy350cRstc6",
        "replyto": "Zy350cRstc6",
        "invitation": "ICLR.cc/2023/Conference/Paper1293/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes stability gap as a concept to measure the transient forgetting that happens for continual learning. It measures the trade-off between the knowledge conservation by learning new tasks (retaining knowledge) and getting new knowledge for the new task. The relation of stability gap with similarity between tasks has bee explored and its analysis also has been conducted.",
            "strength_and_weaknesses": "The idea of measuring the stability-gap which shows how much information is retained and how much new information is learnt after each iteration is a very interesting concept in continual learning since  by learning new tasks the model tries to recover its knowledge after losing it in first timestamps for that task. \nThe paper strengths are:\n1) The good explanation of the problem and the solution.\n\n2) Proposing a concept that tries to measure the tradeoff between forgetting and getting new knowledge after each iteration. \n\n3) The analysis of the proposed stability gap and its dependency with tasks' similarity make the idea more promising.\n\nSome weaknesses that I can mention for this work are:\n\n1) The idea and analysis have been implemented only for image-based datasets while the continual learning happens for other domains such as texts. The shortage of experiments and analysis of the metric on such datasets should be addressed to show the generalizability of the stability gap for different domains.\n\n2) To measure the relation between stability gap and tasks differences, a synthetic dataset is created by rotating images, however studying this relation on more real-cases tasks can be more beneficial and important to show the relevance.\n\nThis is a minor question and some explanations can make it more clear: according to Table 1 stability gap changes by increasing the evaluation periodicity, therefore what is the best periodicity that should be used in such evaluations, is it domain dependent?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the work and the proposed concept of stability gap is novel and can be beneficial in measuring the forgetting and gaining new information in continual learning. ",
            "summary_of_the_review": "This paper proposes a concept that tries to measure the tradeoff between forgetting and getting new knowledge in continual learning which is very important in such learnings. The analysis of this concept also helps to better understand it. Overall motivation and idea and experiments are convincing, the only concern is its analysis that is only limited to image-based datasets and some of the experiments are conducted for synthetic datasets that are not close to real-case scenarios.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1293/Reviewer_xcu2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1293/Reviewer_xcu2"
        ]
    },
    {
        "id": "KCRvMJPEPQ",
        "original": null,
        "number": 4,
        "cdate": 1666856198086,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666856198086,
        "tmdate": 1670786876275,
        "tddate": null,
        "forum": "Zy350cRstc6",
        "replyto": "Zy350cRstc6",
        "invitation": "ICLR.cc/2023/Conference/Paper1293/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper pointed out the shortcomings in the existing approaches for continual learning, mainly catastrophic forgetting of the previous task during new task adoption. It proposes per-iteration continual evaluation with new metrics that enable measuring worst-case performance. This work empirically studied five existing continual works and shows that they suffer from a significant performance loss on previous tasks. The stability gap increases significantly as subsequent tasks are more dissimilar. It also explains the stability-plasticity trade-off by proposing a new metric evaluation for continual learning. The conceptual analysis is provided for causing the stability gap by disentangling the gradients based on plasticity and stability.\n\n\n\n\n\n\n\n\n\n",
            "strength_and_weaknesses": "Strengths:\n1- The paper identifies the issues that occur in the existing task \nincremental continual learning approaches- and tries to address them by providing empirical experiments and detailed analysis.\n\n2- It proposes a framework for continual evaluation that evaluates the learner after each update and provides an ablation study to prove why evaluation frequency indicates continual evaluation is necessary to surface the stability gap.\n\n3- It provides an empirical study with the continual evaluation framework to identify the stability gap for Experience Replay.\n\n4- To quantify the stability gap in contrast to existing metrics in the existing approaches, it proposes novel metrics such as the minimum and worst-case accuracy (min-ACC and WC-ACC). It shows that the stability gap is significantly influenced by the degree of similarity of consecutive tasks in the data stream.\n\n5-  It also provided a conceptual analysis for causing the stability gap and conducted the experiments for several existing approaches such as Experience Replay (Chaudhry et al., 2019b), GEM (Lopez-Paz & Ranzato, 2017), and LwF (Li & Hoiem, 2017) for supporting the proposed hypothesis.\n\nWeaknesses:\n1- In the experimental setup for MNIST, as you mentioned (on page 6), \"each task constitutes the entire MNIST dataset.\" Is it mean that each task consists of all class examples (0,1...9)? If yes, then how is it valid for continual learning? Because there should be new class examples in the next task. How a buffer capacity affects ER performance?\n2- How your new evaluation setup work for online continual learning approaches such as [a],[b] and[c]? Is the newly proposed metrics (worst case and Min) helpful in identifying this stability gap?\n3- The extreme case for continual learning is where the data examples are available only once, unlike the replay-based approaches, where the old data may be available for future training [d]. It would be interesting to apply the proposed framework to this challenging continual setup [d].\n\n\n[a]- Online Class-Incremental Continual Learning with Adversarial Shapley Value, AAAI-2021.\n[b]- Online Continual Learning under Extreme Memory Constraints, ECCV 2020.\n\n[c]- Online continual learning from imbalanced data, ICML 2020.\n[d]- Efficient feature transformations for discriminative and generative continual learning, CVPR 2021.\n\n\n\n\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to understand. A sufficient ablation analysis is provided to support the claims. It provides a novel framework for evaluating continual learning approaches and the significance of the proposed evaluation framework in a continual learning setup. The authors have assured us they will share the code publicly after acceptance.",
            "summary_of_the_review": "The paper has pointed out the interesting shortcomings in the existing continual learning approaches and proposed a new evaluation framework to address them. I encourage the authors to include my concerns raised in the weaknesses subsection. For more detail, please see the strength and weaknesses section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1293/Reviewer_q7YA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1293/Reviewer_q7YA"
        ]
    }
]