[
    {
        "id": "v524V3gLdBX",
        "original": null,
        "number": 1,
        "cdate": 1666656958537,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656958537,
        "tmdate": 1666656958537,
        "tddate": null,
        "forum": "cMJo1FTwBTQ",
        "replyto": "cMJo1FTwBTQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4981/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper provides an interpretation of DINO as a mixture model with components with a circular von Mises-Fisher distribution.\nUsing this interpretation, the authors propose a normalization when computing cluster assignments, which improves stability and flexibility of the mixture model.",
            "strength_and_weaknesses": "The paper proposes a very interesting interpretation of DINO and the proposed modification increases flexibility of the method by including per-cluster shape parameter. The experimental section includes a number of results to demonstrate the effectiveness of the proposed method.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and authors claim they will release code and pre-trained models.\n\nMinor details:\n\nSection 3.1\n- Column notation is not consistent with vector notation in general. Column should be denoted as, for example, $\\mathbf{w}^{(k)}$ instead of $W^{(k)}$. In general, notation for vectors and matrices should be improved.\n\nSection 3.2\n- $|\\mu| = 1$ should be replaced with $\\|\\mu \\|_2$",
            "summary_of_the_review": "A useful and well-written paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4981/Reviewer_njCU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4981/Reviewer_njCU"
        ]
    },
    {
        "id": "GgE8NVcXn9",
        "original": null,
        "number": 2,
        "cdate": 1666659772326,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659772326,
        "tmdate": 1670491532282,
        "tddate": null,
        "forum": "cMJo1FTwBTQ",
        "replyto": "cMJo1FTwBTQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4981/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper suggests interpreting the recent self-supervised learning framework, DINO, as a mixture model of von Mises-Fisher components. Based on the interpretation, this paper proposes a modification to encourage DINO training to be more flexible and stable. This paper demonstrates the effectiveness of the modification under various experiments.\n",
            "strength_and_weaknesses": "Strengths\n- The interpretation is quite interesting, and the proposed modification makes sense.\n- This paper is easy to follow and well-written.\n\nWeaknesses\n- My major concern is about empirical results.\n  - This paper should compare the proposed method, DINO-vMF, with other SSL methods. The comparison with only DINO and MSN is not enough, and it makes some difficulty to check whether the obtained performance is good or not.\n  - The obtained gains seem marginal. For example, only 0.7\\% accuracy gain on ImageNet using ViT-Base, and no gain when using ViT-Small. Also, there is no meaningful gain in transfer learning (e.g., Table 5).\n  - There is no fine-tuning experiment.\n- Another concern is that this paper is too limited to DINO. This limited applicability would be critical in this field because there have been developed many SSL methods after DINO, e.g., MAE and iBOT. It would be better if this paper shows that the interpretation can be incorporated into other SSL frameworks.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity :: This paper is easy to follow.\n- Quality :: Empricial results are not convincing.\n- Novelty :: I think the proposed method is novel.\n- Reproducibility :: Implementation details are provided.\n",
            "summary_of_the_review": "Although the interpretation and the proposed modification is convincing, the empirical results are too marignal and not interesting. Hence I vote for weak rejection.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4981/Reviewer_aACz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4981/Reviewer_aACz"
        ]
    },
    {
        "id": "XA4PMPg937",
        "original": null,
        "number": 3,
        "cdate": 1666663885860,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663885860,
        "tmdate": 1668878903021,
        "tddate": null,
        "forum": "cMJo1FTwBTQ",
        "replyto": "cMJo1FTwBTQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4981/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper demonstrates an interpretation of DINO as a von Mises-Fisher mixture model on the unit hypersphere. Using this interpretation, the authors alter the architecture of DINO in order to incorporate a normalization term directly in the distribution, and keep the embeddings unnormalized. Using this approach, the authors demonstrate an improvement over the performance of the version of DINO that uses normalized embeddings.",
            "strength_and_weaknesses": "Strengths:\n\n- The interpretation of DINO as a mixture model is interesting. The authors formalize an interpretation of DINO as a mixture model based on the probabilities that the student and the teacher assign to each cluster, for each given sample. This formalization provides a nice theoretical interpretation of DINO, leading to an alternative design choice for normalization, namely normalizing the distribution of the mixture model arising from DINO. This is more theoretically founded than simple $L_2$ normalization of the representations.\n\n- The authors perform an extensive set of experiments to examine whether their approach improves over the baseline DINO formulation. More specifically, they evaluate the performance of their approach on the downstream ImageNet classification task, the few-shot learning capabilities of their method, as well as the transferability of their learned representations on a given task. This suite of experiments provides a sufficient understanding of the capabilities of their proposed method, and can be used to compare it to regular DINO, as well as other baselines.\n\nWeaknesses:\n- The main issue I have with this work is that the novelty of the proposed method is limited, in that it consists of simply reparametrizing DINO by changing the normalization used. I believe that as it stands, the work focuses a lot on this change in normalization, without enough evidence on why it is important aside from the provided experiments. If possible, I believe that incorporating a simple theoretical example on why regular DINO may fail while the proposed DINO-vMF may succeed would greatly improve the paper and further justify the proposed method.\n\n- Regarding the experiments, the benefit over regular DINO is not clear, and seems to be very dependent on the architecture, as can be seen in the Tables in the paper (where DINO-vMF underperforms in the smaller ViT-Small model). Namely, the benefits seem to only be apparent for large models, as stated by the authors. Since there seems to be a major effect derived by the architecture of the backbone of DINO, I believe that the authors should also examine different architectures for this backbone (different ViT sizes, or ResNets).\n\n- As a more minor issue, the section in which the authors examine the void prototypes is somewhat unclear in my opinion. The way I understand it, the authors state that DINO has a tendency to assign a large number of samples to similar representation, while DINO-vMF tends to spread the samples further apart. I would be grateful if the authors could clarify whether this is correct.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clear and easy to understand (with the exception of Section 5.2.1, as stated above). I would also suggest the authors to put Equations 8 and 9 in a more prominent position, such right before Section 3.4 (so at the point where they first mention how they alter the logits).\n\nAs stated above, I believe the insight provided by this paper regarding the vMF formulation of DINO is original. However, I also believe that the algorithmic novelty derived by this insight in this paper is somewhat limited, since it consists of only a small alteration in the training procedure.\n\nRegarding reproducibility, the authors use hyperparameters already publicly available for the training of their models.",
            "summary_of_the_review": "Overall, I feel that this paper provides an interesting interpretation of DINO that leads to an interesting trick to improve its performance. I lean slightly towards acceptance due to the clean addition of this method to regular DINO training, but I believe that the paper can overall be improved due its somewhat limited novelty.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4981/Reviewer_SyZr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4981/Reviewer_SyZr"
        ]
    },
    {
        "id": "H8_85Y35-P",
        "original": null,
        "number": 4,
        "cdate": 1666689496362,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689496362,
        "tmdate": 1666689496362,
        "tddate": null,
        "forum": "cMJo1FTwBTQ",
        "replyto": "cMJo1FTwBTQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4981/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper extends the previous work DINO by interpreting it a new way, as a von Mises-Fisher mixture model, and in turn proposes DINO-vMF model and achieves very good performance on numerous tasks. \n",
            "strength_and_weaknesses": "Strength: The interpretation of vMF is appealing, especially leading to removing the L2 regularization to enable more flexibility. Apart from the representative strength, the results are also strong, even compared to the recent works such as MSN. \n\nWeaknesses: \n\n1/ An important question is how practical this solution is: many well-understood and interpretable frameworks seem to be less competitive to their counterpart deep models because of their speed. It seems even with necessary approximations such as in 3.3, it is not trivial for the case of other formulas, which might lead to further approximation to, e.g., trade for more speed. \nLikewise, the accuracy in the tasks given are appealing, yet how about the other performance aspects such as training time, inference time and else? In my humble opinion, even some discussions on the drawback of this solution would make the paper more\u2013not less\u2013convincing. \n\n2/ Since the normalization of prototypes is key to this solution, various related studies have been conducted esp. in Section 5. However, I suggest Table 1 should be extended to be more convincing in 2 aspects. First, in this small scale with this large dataset, running some rounds and get the variances as well would be more helpful. Second, do it with small datasets or any others, just to confirm the consistent practical reflection of the intuition and explanations given. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well motivated and written, easy to follow the main logic. The vMF interpretation applied to vision is not entirely novel, such as also cited by the authors to Hasnat et al, yet applying to self-distillation is to my opinion new and helpful. The code is not provided, however, and thus is hard to evaluate reproducibility, given the implementation of probabilistic models is by no means without certain complication. \n",
            "summary_of_the_review": "\nThe work has an intuitive and well-motivated approach based on DINO, which shows impressive results on various tasks. The authors also conducted meaningful studies and analysis to back their intuition and rationale of their proposed method. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4981/Reviewer_Xj5D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4981/Reviewer_Xj5D"
        ]
    }
]