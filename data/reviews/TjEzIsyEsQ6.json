[
    {
        "id": "Y55zRZN8l1p",
        "original": null,
        "number": 1,
        "cdate": 1666583008912,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583008912,
        "tmdate": 1666583008912,
        "tddate": null,
        "forum": "TjEzIsyEsQ6",
        "replyto": "TjEzIsyEsQ6",
        "invitation": "ICLR.cc/2023/Conference/Paper1894/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper conducts a rigorous analysis on the fundamental properties of the policy induced value functions in multi-objective reinforcement learning (MORL). Based on the analysis, the authors further distinguish and consolidate three existing definitions of Pareto optimal policies and identify issues of training policies via linear scalarization (LS). The authors fix this issue by adding strongly concave terms to the immediate rewards and verify the effectiveness of the approach via experiments on simulated control tasks. ",
            "strength_and_weaknesses": "I am not an expert in MORL. But after reading this paper, I envision that the contributions of this paper will have a large impact on the MORL community. They can accelerate the research in this field. Furthermore, as many practical RL problems can be better formulated using multiple objectives, the contributions are very important to the RL community in general. \n\nThe authors offer several novel theoretical results, including\n\n- The induced value function\u2019s range for stationary policies is concave. This has been thought hard to characterize and of irregular shapes. \n- There are at least three existing definitions of Pareto optimal policies. The authors rigorously distinguish and consolidate them. \n\nI really like the new theoretical results. However, I have some questions regarding the Problems of the existing LS-based algorithms paragraph and the proposed algorithm. \n\n- The solutions to linear programs are almost always on the vertices. If this does not make solving them less numerically stable, why would LS-based algorithms be less numerically stable? \n- The motivation of identifying all SPE policies is not very clear to me. When is this required in practice? \n- Maybe derived from a different perspective, based on Eq (9), the proposed algorithm CAPQL is almost the same as entropy-regulated RL algorithms. Is this correct? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very pleasant to read and has high clarity and quality. Using a running toy example to provide intuition for the theoretical results is very helpful.\n\nThe theoretical results seem novel and significant, although the algorithm is not very novel. \n\nTypo:\n- at the bottom of page 6, g_s is not defined. \n\n",
            "summary_of_the_review": "Due to the strong theoretical contributions, I recommend acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1894/Reviewer_gKVf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1894/Reviewer_gKVf"
        ]
    },
    {
        "id": "JfFRy2HhrwG",
        "original": null,
        "number": 2,
        "cdate": 1666765657653,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666765657653,
        "tmdate": 1666765657653,
        "tddate": null,
        "forum": "TjEzIsyEsQ6",
        "replyto": "TjEzIsyEsQ6",
        "invitation": "ICLR.cc/2023/Conference/Paper1894/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the fundamental properties of the learning spaces in multi-objective reinforcement learning (MORL). The authors give a theoretical analysis of policy induced value functions and discuss three metrics of Pareto optimality. Their results imply the convexity of the induced value function, and show that training a policy based on linear scalarization (LS) can achieve any point of the Pareto front. The authors propose a new vector reward-based Q-learning algorithm, called CAPQL, which adds a strongly concave term to the immediate reward and solves the problems of existing LS-based algorithms.",
            "strength_and_weaknesses": "Strengths:\n1.\tThis paper is very well-written and easy to follow. The authors use an example (Example 1) throughout the paper to illustrate their insights, which facilitate well readers\u2019 understanding.\n2.\tThe investigation on the relationship between three types of Pareto optimality is novel and interesting, which is supported by theoretical analysis.\n\nWeaknesses:\n1.\tAccording to the results in Section 4, i.e., adding a strongly concave term can fix the problems of existing LS-based algorithms, it seems that the proposed algorithm CAPQL can add an arbitrary strongly concave term to the immediate reward. Why does CAPQL use the entropy operator $\\mathcal{H}(q)$? Can this entropy operator be changed to other strongly concave terms? It would be better if the authors can include more discussion on the strongly concave terms used for CAPQL.\n2.\tThe theoretical analysis for algorithm CAPQL is not sufficient. In the current version of this paper, the theoretical guarantee for CAPQL fully replies on the analysis for the limitations of existing LS-based algorithms and three types of Pareto optimality. It would be better if the authors can provide a more thorough theoretical analysis for CAPQL, e.g., a convergence or sample complexity guarantee.\n3.\tCan the authors provide the experimental results for the comparison to existing LS-based algorithms? The results in Section 4 will be more convincing if the authors can show some experimental phenomena that support the theoretical results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written. The analysis for three types of Pareto optimality for multi-objective reinforcement learning (MORL) is novel. The authors illustrate the implementation details for the experiments, but do not provide the code.",
            "summary_of_the_review": "I think that the theoretical analysis for the problems of existing LS-based algorithms and the relationship among three types of Pareto optimality for multi-objective reinforcement learning (MORL) is novel and interesting. But the proposed algorithm CAPQL lacks a sufficient discussion on the used concave term and a more in-depth theoretical analysis, e.g., a convergence or sample complexity guarantee. The experiments can be further enhanced by adding a comparison to existing LS-based algorithms and a discussion on how the empirical phenomena support the theoretical fundings in Section 4. Overall, I give borderline acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1894/Reviewer_Z4Pt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1894/Reviewer_Z4Pt"
        ]
    },
    {
        "id": "6KZsneTt44",
        "original": null,
        "number": 3,
        "cdate": 1666960618763,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666960618763,
        "tmdate": 1669030324700,
        "tddate": null,
        "forum": "TjEzIsyEsQ6",
        "replyto": "TjEzIsyEsQ6",
        "invitation": "ICLR.cc/2023/Conference/Paper1894/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper discusses the impact of multiple objectives. Pareto optimality guarantees are discussed, and actor-critic formulation is provided. ",
            "strength_and_weaknesses": "The paper analyses the dynamics of the induced value functions resultant from policy alterations in MORL problems. The analysis seems good. \n\nThe proposed algorithm mainly adds a concave function of the policy, which can help with determinism and numerical instability. The algorithms could have some sample complexity type results which will be helpful, with comparisons to the existing works. This is especially since many algorithms like policy gradient, actor critic, etc. have sample complexity guarantees. Even though this paper is different from CURL frameworks, it is not clear why we cannot use the model like there to model the multi-objective problem - and what benefits this formulation gives or what problems this could solve which the other cannot.",
            "clarity,_quality,_novelty_and_reproducibility": "The algorithm table and the corresponding novelty there could be in main text. ",
            "summary_of_the_review": "The paper provides some results on Pareto optimality of MORL with concave regularization of the policy. Sample complexity results would have been nice for the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1894/Reviewer_HSHv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1894/Reviewer_HSHv"
        ]
    },
    {
        "id": "pOTyUAqZWP",
        "original": null,
        "number": 4,
        "cdate": 1667279087896,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667279087896,
        "tmdate": 1667279087896,
        "tddate": null,
        "forum": "TjEzIsyEsQ6",
        "replyto": "TjEzIsyEsQ6",
        "invitation": "ICLR.cc/2023/Conference/Paper1894/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies some theoretical questions about multi-objective reinforcement learning. The focus is on policy-induced value functions. Section 3 develops several tools to characterize changes to value functions when one changes policies. Section 4 uses those tools to show the value function space is convex and provides insights into why linear scalarization methods fail to find Pareto-efficient policies. The authors present a concave-augmented Pareto Q-learning algorithm (CAPQL) and demonstrate its effectiveness on several multi-objective variants of the MuJoCo environments.",
            "strength_and_weaknesses": "Strength: overall the theoretical contribution is quite interesting and insightful. As the authors pointed out, recent multi-objective RL research has not presented a coherent view on this topic. This paper is a timely investigation of some fundamental properties in this space.\n\nWeakness: \nThe empirical evaluation section, in comparison, is weak. As the new algorithm proposes adding a strongly concave term to the reward function, it would be better to explain the choice of selecting the entropy operator in the implementation. Can one use any other operator in its place?\n\nWhile the results on the 4 presented environments look promising, I noticed the sample size is relatively small (10k steps). Are there longer runs so we can see result comparison at convergence?\n\nThe authors should also discuss a relevant paper on introducing an improved scalarization method beyond linear scalarization. \n\nKristof Van Moffaert; Madalina M. Drugan; Ann Now\u00e9: Scalarized multi-objective reinforcement learning: Novel design techniques\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and relatively easy to follow. The theoretical novelty is high and relevant to the multi-objective RL community.",
            "summary_of_the_review": "This a well-written paper with a good theoretical contribution. The empirical evaluation section could be strengthened to make the whole package stronger.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1894/Reviewer_ahKt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1894/Reviewer_ahKt"
        ]
    }
]