[
    {
        "id": "3PF1ZkQh0G",
        "original": null,
        "number": 1,
        "cdate": 1666553277474,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666553277474,
        "tmdate": 1666553277474,
        "tddate": null,
        "forum": "kt-dcBQcSA",
        "replyto": "kt-dcBQcSA",
        "invitation": "ICLR.cc/2023/Conference/Paper4716/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A method (TAME-GP) to capture task-related variance in neural firings observations is presented. The method has its roots on probabilistic canonical correlation analysis (pCCA), but proposes an extension for non-Gaussian noise. This is achieved by replacing pCCA\u2019s prior that assumes a normal distribution with a Gaussian process as well as adding an explicit hidden (latent) layer z for relating inputs x (spike counts) and the output y (task variables).\nTwo verification efforts on synthetic data and two validation efforts on experimental data are provided. Moreover, of the latter two one is of segregational nature (local activities), and the other of integrational nature (communication between regions).\n",
            "strength_and_weaknesses": "Strengths\n+ The novel way to generalize pCCA to non-Gaussian noise model.\n+ Results do accompany; the new model not only captures more observed variance but also appears to better decode task-related information.\nWeaknesses\n+ It is unclear how the mathematical decisions match physiological and or functional rationales, which means that even if the method works, its interpretation remains cumbersome.\n+ Conciseness\u2026.or the lack of it.\n",
            "clarity,_quality,_novelty_and_reproducibility": "+ Clarity is one of the weaknesses of the draft. I thought that I departed with good background to fully understand the paper, both in terms of neuroimaging as well as in terms of maths, but I found myself struggling a bit to follow what was going on during my first read. Yes, one manages to decode the paper after several reads, but I wonder whether the ideas are a bit obfuscated if even someone with related skills needs several reads. Unfortunately, I do not have a better suggestion\u2026 I can see the issue but not the solution.\n+ Quality: Good. There is adequate mathematical rigor and there are no major flaws that I can think of. The verification and validation efforts are sufficient in my opinion.\n+ Novelty: The extension that TAME-GP represents over its predecessor pCCA is certainly not obvious or trivial.\n+ Reproducibility: I believe that with a little bit of effort that I can certainly replicate the methods. The part of the experimental results are a bit less clear.\n",
            "summary_of_the_review": "+ Including the latent layer makes me wonder, how would hidden graphical models e.g. hidden Markov models or others sophisticated variants, perform compared to TAME-GP?\n+ Regarding the aforementioned weakness on the relation of the mathematical decisions having a physiological or functional counterpart, why n+1 latent variables? Why Gaussian processes? Why in the expectation step not using a transformation  e.g. Anscombe or Freeman-Tukey or other, rather than approximating the posterior again with a normal distribution? What would be the physiological interpretation of the model hyperparameters (for instance, but not limited to; the linear approximation Cz^j +d, the \\tau, etc)?\n+ Has Procrustes also been applied to the other methods being compared in Fig 2B? It seems that at least P-GPFA and pPCA are not correctly aligned?\n+ The order of presentation of some mathematical elements is confusing. For instance. Parameter K is introduced in pg 3 during the expectation step but not defined until the end of the maximization step pg 4.\n+ As one of the advantages of TAME-GP over pCCA, it is stated that pCCA only considers pairwise relations. This somewhat implies that TAME-GP handles relations of arity higher than 2 between regions. The k_j terms are clearly pairwise among latent z variables. It is not clear to me how this propagates to brain circuits of more than two regions. \n\nMinor details\n+ Cross-validation: Popular terminology but mathematically inaccurate. Suggestion: internal validity using cross-folding.\n+ In the discussion: Causality of the outcomes is claimed, but I can\u2019t see how any of the traditional causal assumptions for any popular probabilistic causal definition (Spirtes-Glymour, Pearl, Suppe, etc) are shown or demonstrated here.\n+ Appendices A.1.1 and A.1.2 are possibly unneeded. References should suffice.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "Two experiments on animal models are discussed (mice and monkeys). However, no ethical approval details are indicated.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4716/Reviewer_7WyW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4716/Reviewer_7WyW"
        ]
    },
    {
        "id": "TkYP6EdASZ3",
        "original": null,
        "number": 2,
        "cdate": 1666624298756,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624298756,
        "tmdate": 1669659810927,
        "tddate": null,
        "forum": "kt-dcBQcSA",
        "replyto": "kt-dcBQcSA",
        "invitation": "ICLR.cc/2023/Conference/Paper4716/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this submission, the authors proposed a method to partition neural response variability within and across brain areas.",
            "strength_and_weaknesses": "****Pros: \n- The paper is generally well-written and, for the most part, clear. \n- Understanding inter-area communications represents an interesting problem in neuroscience. This is generally a less studied topic, but has attracted more attention recently due to the increased experimental capacity to monitor the activities from multiple regions.  \n- The proposed method is pretty straightforward. \n- Several applications were considered, based on both simulations and neuroscience data.\n\n\n****Cons:\n- The way proposed in the paper to model the communication is not totally convincing. The signals across the different regions are only allowed to interact via a multiplicative modulation. It is unclear whether this is reasonable for neural systems. \n- The technical contribution presented in this paper is rather incremental. The model is largely linear and assembles multiple components from prior work. Most of the components are fairly standard. The main innovation seems to the interpretation of Eq (1) as a multi-region communication model (but see my next comment).\n- It is unclear what\u2019s special about the inter-area communication. Eq. (1) could also be interpreted as a model for a single brain area with increased latent dimension. This needs to be further clarified.\n- The real neuroscience data applications are detailed but unconvincing. Maybe the authors could articulate better what new insights were revealed. This is particularly important given the rather incremental technical contribution. \n- The authors showed examples in which case the model could be recovered. Are there also cases for which the model components can not be recovered? This seems to be an important question for interpreting the output of the model, but it is studied in the paper.\n\n\nOther comments:\n- Practically, how to set the number of latent dimensions in each area?\n- The title should be changed. It is unclear what \u201cINTRA- AND INTER-AREA NEURAL MANIFOLD\u201d really means. I understand that \u201cneural manifold\u201d is good for branding, but that should come with the cost of sacrificing the accuracy and faithfulness of the title.\n- It is unclear how to think about the mouse ADN recording in the context of inter-area or intra-area communication. This needs a clarification. \n- There should be a more thorough treatment of the literature on using the task variable to guide the estimation of latent variables. A few papers that would be useful to discuss:\nSani, Omid G., et al. \"Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification.\" Nature Neuroscience 24.1 (2021): 140-149.\n\nZhou, D., et al \"Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE.\" Advances in Neural Information Processing Systems 33 (2020): 7234-7247.\n\nHurwitz, Cole, et al. \"Building population models for large-scale neural recordings: Opportunities and pitfalls.\" Current Opinion in Neurobiology 70 (2021): 64-73.\n\n- Fig 1a. The model variables need to be better explained in the figure caption/legend.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has decent clarify and quality. \nThe novelty is a bit incremental. ",
            "summary_of_the_review": "On the one hand, it\u2019s a decent paper. I don\u2019t see fatal errors in this paper. However, I also don\u2019t feel the work to be too exciting, so perhaps not quite up to par for ICLR.  Overall, I consider this paper to be slightly below the border of acceptance. But I\u2019d also be ok if this paper gets accepted. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4716/Reviewer_1fJi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4716/Reviewer_1fJi"
        ]
    },
    {
        "id": "h3kTMwK3Vcx",
        "original": null,
        "number": 3,
        "cdate": 1666682764602,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682764602,
        "tmdate": 1666682810212,
        "tddate": null,
        "forum": "kt-dcBQcSA",
        "replyto": "kt-dcBQcSA",
        "invitation": "ICLR.cc/2023/Conference/Paper4716/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Neuroscientists are generating rich datasets with population recordings from multiple brain areas simultaneously while the animal is engaged in a complicated behavior. It urges for an analytical tool to extract interpretable information from these rich datasets. Latent variable models are popular approaches for generating a compressed summary of such data. However, the interpretability of the latent model is limited. In the current paper, the authors presented a novel latent variable model which combines dPCA and pCCA into a graphic model to learn task or stimulus-relevant latent variables of neuron population data. The author demonstrated the model successfully learns interpretable latent variables that could account for both inter- and intra- area variance. Overall, the current model achieved superior performance compared to several population models. The message of the paper is clear and the results are solid to me. ",
            "strength_and_weaknesses": "Strength: \n1.\tThe authors have a good sense in demonstrate the interpretability and application of the new model using various simulation data. I really like the part about the communication subspace (figure 3). The fact that the current model achieved better performance compared to Semedo 2019 and Keeley 2020 for both simulation datasets with or without trial repeats is exciting. \n2.\tThe authors have compared the current model with multiple popular models in fitting both simulation and real datasets, which makes the overall results more convincing. \n\nWeakness: \n1.\tSome recent RNN-based latent models eg. LFADs and Oerich 2020, were overlooked in the current manuscript. It would be great to discuss those. \n2.\tIt is not clear to me whether such a model could generate novel knowledge or testable hypothesis about neuron data.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the figures are laid out nicely. The performance of the current method is good compared to some popular methods from the same class. One of the real datasets in the current study is publicly available which will benefit reproducing the work. Will authors release their code if accepted?  Latent models from similar classes are available. But the current study is novel as it combines several dimensionality reduction approaches and achieved better performance and interpretability. ",
            "summary_of_the_review": "Overall, I recommend accept the current work as the authors have not only developed a new model, but also carried out thorough study on the model.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4716/Reviewer_4RNV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4716/Reviewer_4RNV"
        ]
    },
    {
        "id": "oe-rhW63N4",
        "original": null,
        "number": 4,
        "cdate": 1666707081211,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666707081211,
        "tmdate": 1666744626023,
        "tddate": null,
        "forum": "kt-dcBQcSA",
        "replyto": "kt-dcBQcSA",
        "invitation": "ICLR.cc/2023/Conference/Paper4716/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a probabilistic approach for learning interpretable task-relevant neural manifolds that capture both intra- and inter-area neural variability with single trial resolution. Task Aligned Manifold Estimation with Gaussian Process priors (TAME-GP) incorporates elements of demixed PCA and probabilistic CCA into a graphical model that additionally includes biologically relevant Poisson noise. The model uses a Gaussian Process (GP) prior to enforce temporal smoothness, which allows for robust reconstruction of single-trial latent dynamics. Experiments using both synthetic data and neural recordings from rodents and primates during naturalistic tasks demonstrate the robustness and flexibility of TAME-GP in comparison to alternative approaches. ",
            "strength_and_weaknesses": "Strengths:\nThe paper is clealy presented with excellent use of English. The technical contributions are significant. The pros and cons as well as the broader impact of the proposed method are discussed in depth.\n\nWeaknesses:\nIn Section 2, the notations may need to be explained a bit more.\n",
            "clarity,_quality,_novelty_and_reproducibility": "A few comments to improve the clarity of presentation:\nIn Eq.  (1), what is the subscript i?\nIn the third line below Eq. (1), \"to introduces\" should be \"to introduce\".\nIn Section 3, first line, \"due the Poisson noise\" should be \"due to the Poisson noise\".",
            "summary_of_the_review": "This is a solid piece of work. The paper has solid technical contributions which have been adequately demonstrated through extensive experiments and indepth discussions. The proposed TAME-GP could be very useful as solution/tool in practice for dissecting sources of variability within and across brain areas during behavior.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4716/Reviewer_RPBA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4716/Reviewer_RPBA"
        ]
    }
]