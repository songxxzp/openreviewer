[
    {
        "id": "mggm__EoWPT",
        "original": null,
        "number": 1,
        "cdate": 1666595228029,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666595228029,
        "tmdate": 1666595228029,
        "tddate": null,
        "forum": "im5YMG981ST",
        "replyto": "im5YMG981ST",
        "invitation": "ICLR.cc/2023/Conference/Paper4413/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper describes an approach to add ILP constraints into transformer-based models. The main idea is to allow for end-to-end training using a differentiable solver (DBCS) to solve the ILP. Here, DBCS produces can produce exact solutions to the ILP to approximate the gradient. The main hypothesis is that using these exact solutions to the ILP generates better explanations in multi-hop inference. \n\nSpecifically, the approach is described in the context multiple-choice question answering. Explanations are extracted in the form of weighted graphs. The weights encode the importance of a fact for a hypothesis. The subgraph that represents the explanations are selected using DBCS.\nThe transformer-based model (STrans) is used for computing embeddings needed to compute weights for the graph in the formulated ILP problem.\n\nExperiments are performed using the world tree corpus and results are reported for explanations on the dev-dataset (since test-set explanations are not available). Comparisons are made with BERT, a non-differentiable version of the approach (thus, no end-to-end training) and a recent similar approach (DiffExplainer) that uses approximate ILP solutions using transformations to a convex optimization problem. New metrics to measure consistency and faithfulness of explanations are also evaluated on all compared models.",
            "strength_and_weaknesses": "Strengths\n- Empirical results show improvements in explanation accuracy over existing state-of-the-art DiffExplainer\n- Using DBCS seems to be a new way to integrate symbolic constraints into transformers and the same could be applicable in other models as well.\n\nWeakness\n- Novelty seems somewhat limited since the ideas seem to be a similar to the ideas in Diffexplainer (Except the use of DBCS). I am not sure if there is only one dataset that is typically used for evaluation (since ARC does not measure explanations) to show more significant impact of the proposed method\n- In general, solving the ILP exactly may be hard (for different types of constraints), so is this approach generalizable or does it work due to the specific type of constraints formulate for the multiple-choice answering problem\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing could be improved a bit (I did notice typos and in some cases, the meaning of sentences was hard to read). The novel contribution could be highlighted better an particularly analyzed more as to why it impacts explainability since that was not so clear to me. Novelty in a bit limited though since it is based on a recent model that has a similar approach though uses a different type of ILP solver to integrate into the tranformer-model. Since standard benchmarks are used, they should be reproducible, however several hyperparameters are mentioned in the ILP formulation. The effect of varying these are not clear.",
            "summary_of_the_review": "The paper proposes an interesting idea to improve explainability in Multiple-choice question answering using ILPs that give us exact solutions and can be integrated into transformers for end-to-end training. Overall, the novelty and generalizability of the model seem to be the main weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4413/Reviewer_eQxh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4413/Reviewer_eQxh"
        ]
    },
    {
        "id": "MhPeIVKBla",
        "original": null,
        "number": 2,
        "cdate": 1666628089440,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628089440,
        "tmdate": 1666628089440,
        "tddate": null,
        "forum": "im5YMG981ST",
        "replyto": "im5YMG981ST",
        "invitation": "ICLR.cc/2023/Conference/Paper4413/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper concerns the problem of question answering via multi-hop inference, where multiple separate facts need to be taken into account to answer a complex question. To allow for interpretable structured explanations of the reasoning process, this and previous work explicitly encode multi-hop inference in the constraints of an Integer Linear Program.\n\nGiven a question, a potential answer, and a set of supporting facts, all the components are encoded as the nodes of an explanation graph, on which the edge weights are computed based on lexical overlap and transformer-predicted semantic relevance. The graph is then used as the input to a discrete optimization procedure that explicitly encodes multi-hop inference in its constraints, yielding as the optimal solution a subset of the provided facts to support the potential answer. Repeating this procedure for each potential answer allows to select the most promising answer along with its supporting facts.\n \nThe main difficulty of training such an architecture end-to-end lies in the non-informative jacobian of discrete optimization algorithms. To alleviate this, previous work relies on differentiable semi-definite programming relaxations. However, these can lead to sub-optimal solutions. In contrast, the authors of this work approach the differentiability issue by relying on Differentiable BlackBox Combinatorial Solvers, a technique for computing an informative replacement for the gradient. The resulting proposed framework is called Diff-Comb Explainer.\n\nIt is empirically tested on answer and explanation selection tasks with two datasets. The proposed method is shown to outperform previous methods in terms of all relevant metrics, and is also shown to be robust to an increased number of distracting facts. Some failure cases of the presented method and its competitors are also highlighted in a qualitative analysis.",
            "strength_and_weaknesses": "### Strengths:\n- The paper is well written, easy to understand, with a good presentation.\n- The demonstrated success of a hybrid approach for combining symbolic knowledge in the form of ILP constraints with neural representations opens exciting possibilities for further applications in natural language processing.\n\n### Weaknesses:\n- Technical novelty: The paper is extremely similar to Diff-Explainer [1], only the semi-definite programming relaxation of the ILP is replaced with the unrelexad ILP, and DBCS [2] is used to differentiate it. This limits the theoretical contribution to plugging one existing method into another, which is very incremental. \n- Empirical novelty: Currently the paper reports results only on experiments from [1], and additionally adds the novel consistency and faithfullness metrics. This is a reasonable empirical contribution, however, it is in my opinion not enough to outweight the limited theoretical novelty.\n- Differentiation of the combinatorial solver: Equation 13 only depends on the optimal value of the optimization problem (which is continuous), not the optimal solution (which is discrete). Therefore it is continuously differentiable w.r.t. the parameters $W$, and complicated methods like DBCS are not required here (in contrast to differentiation of Equation 14). Was this taken into account in differentiating Equation 13 or are the reported results produced by using the informative gradient replacement from DBCS? In the latter case, it would be important to report as a baseline the results of differentiating Equation 13 without the use of DBCS.\n\n### Additional questions:\n- Is there an interpretation for the performance drop from using supervision on the explanation selection in the consistency@3 column of Table 1?\n- What are the runtimes for the experiments? Solving a relxation of an ILP can be much faster than solving the true ILP, is the slight performance increase worth the additional runtime?\n\n\n[1]: M. Thayaparan, M. Valentino, D. Ferreira, J. Rozanova, and A. Freitas. Diff-Explainer: Differentiable Convex Optimization for Explainable Multi-hop Inference. In Transactions of the Association for Computational Linguistics, 2022.\n\n[2]: M. Pogancic, A. Paulus, V. Musil, G. Martius, and M. Rolinek. Differentiation of blackbox combinatorial solvers. In International Conference on Learning Representations, 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n- The paper has good clarity, it is well-written and easy to follow. The architecture overview is very helpful.\n### Quality\n- Overall the paper has relatively high quality, all the experiments appear to be reasonably chosen, but some questions are still open (see weaknesses).\n### Novelty\n- The novelty of the paper is limited. It extremely closely follows the previous work Diff-Explainer, and basically only replaces the semi-definite relaxation with the true ILP, using the DBCS method to differentiate it.\n### Reproducibility\n- The authors provide the pseudo-code for training the architecture and the hyperparameters that were used to produce the reported results. Code that produces the reported results is not provided.",
            "summary_of_the_review": "While exhibiting high clarity and reasonable quality of the proposed framework, along with a good evaluation, the presented novelty is limited. I suggest to add an additional contribution, potentially further theoretical or empirical insights into the discrepancy arising from using an ILP vs. a semi-definite relaxation. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4413/Reviewer_NWPP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4413/Reviewer_NWPP"
        ]
    },
    {
        "id": "EG9RdYH6CP3",
        "original": null,
        "number": 3,
        "cdate": 1666674359030,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674359030,
        "tmdate": 1666674359030,
        "tddate": null,
        "forum": "im5YMG981ST",
        "replyto": "im5YMG981ST",
        "invitation": "ICLR.cc/2023/Conference/Paper4413/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes \u201cDiff Comb Explainer\u201d: a neuro-symbolic architecture for selecting an answer (from a given set of candidates) as well as explanations (from a corpus of facts) for a multiple-choice QA task. \nEach candidate answer is first converted into a hypothesis by concatenating the question to it, and then scoring a hypothesis is cast as a subgraph selection problem that is solved via a standard ILP for subgraph selection problem with an added constraint that hypothesis node should be a part of the subgraph. The graph contains a node for the hypothesis and k more nodes, one each for a retrieved fact from the corpus. The edges represent lexical and semantic similarities between the nodes. The facts corresponding to the nodes selected in the subgraph represent the explanations, and the objective function of the ILP is the total edge weight of the subgraph.\n\n",
            "strength_and_weaknesses": "This paper demonstrates that using differentiable black-box solvers gives a slight improvement over using SDP relaxation for making ILPs differentiable.\n\nThe idea of using an ILP to solve the exact same subgraph selection problem for precisely the same task has already been explored in Diff Explainer.  The only difference is in the choice of technique to make the ILP differentiable: Diff Explainer uses an SDP relaxation to make ILP differentiable, whereas Diff Comb Explainer uses differentiable blackbox solvers, which is the same as using CombOptNet (Paulus et al 2021) with known constraints and learnable cost.  CombOptNet has been discussed in the Diff Explainer paper, and this work is just replacing  SDP relaxation with CombOptNet with known constraints (similar to blackbox differentiation).\n\n[Paulus et al 2021] CombOptNet: Fit the Right NP-Hard Problem by Learning Integer Programming Constraints\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\n\nThe paper is not very well written. Some sentences are not well-formed (see below for an example).  There are some gaps/inconsistencies in the notation.  \n\n1.   Below eqn 1, $X \\in Z^n$, instead it should be $X \\subseteq Z^n$.\n2.   Below eqn 3, what is $w$ in $f_\\lambda(w)$ ?\n3.   Eqn 7, need to define $W^i_{jj}$ for $j = h_i$. Below eqn 7, it should be (k+1) x (k+1) instead of (n+1) x (n+1) while defining $Y^i$\n4.   Eqn 14, is incorrect. BCE computation requires probabilities for the explanatory facts. From where do we get those? $\\hat{Y}^{ans}$ contains only binary assignments of the edges. Guessing from fig 2, BCE  should be computed using the edge weights $W^{ans}[0,j]$ for the $j^{th}$ fact.\n5. Eqn 16.. Is $e^i_t$ a fact or a set of facts? The numerator suggests that it should be a fact, as belongingness to retrieved explanations ($E_t$, which is a set of facts) is checked, but the denominator suggests that it should be a set of facts as its cardinality is being computed.\n6. \u201cExplanationLP explicit abstraction by grouping facts into abstract and grounding facts.\u201d  (pg. 4): not sure what it means.\n\n\n**Novelty:** \n\nGiven that Diff Explainer is already a published work, the novelty in Diff Comb Explainer is somewhat limited. \n\n**Reproducibility:**\n\nGiven that the gains over Diff Explainer are within 2-3 points, I would encourage the authors to repeat experiments using multiple seeds and confirm whether the gain is more than the standard deviation.\n\n",
            "summary_of_the_review": "The idea of creating a neuro-symbolic architecture by making an ILP differentiable for the task of multiple choice QA (with explanations) has already been demonstrated by Diff Explainer. This work is simply replacing the SDP relaxation with Differentiable Black Box Solver and showing slight improvement. Notably, CombOptNet, which essentially uses a black box solver for ILPs and makes it differentiable, has already been mentioned/discussed in the Diff Explainer paper, but not chosen over SDP relaxation technique to make ILP differentiable. In light of all this, I believe that the novelty of this work is somewhat limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4413/Reviewer_nF5J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4413/Reviewer_nF5J"
        ]
    },
    {
        "id": "5pdMXnsKgK",
        "original": null,
        "number": 4,
        "cdate": 1666919919079,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666919919079,
        "tmdate": 1670112994189,
        "tddate": null,
        "forum": "im5YMG981ST",
        "replyto": "im5YMG981ST",
        "invitation": "ICLR.cc/2023/Conference/Paper4413/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a differentiable model, namely Diff-Comb Explainer, that solves the integer linear programming based multi-hop QA problem. The proposed model utilizes the differentiable blackbox combinatorial solver, which solves the ILP problem in a differentiable fashion. In the experiments, the proposed method outperforms the baselines in terms of explanation and answer selection.",
            "strength_and_weaknesses": "Strength \n- The approach of generating explanations for QA tasks via graph construction is interesting\n\nWeaknesses\n- The scope of this work is to improve a specific type of approach to QA tasks and the impact could be limited\n- The paper is difficult to follow for readers unfamiliar with this line of research\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Novelty and Quality\n\nAs a reader unfamiliar with the ILP-based Multi-hop QA, I'm having difficulties assessing the novelty and the quality of this work. The proposed method is based on a very specific model (Diff-Explainer) which seems to use a unique stack of techniques (integer programming, multi-hop inference and etc.) in solving the QA task. Judging from the empirical results, the Diff-Comb indeed outperforms the prior work, but since the setting and prior work are not properly presented in the paper (see clarity), it is difficult for me to tell which part of the proposed method is intellectually novel.\n\n\n### Clarity\n\n\nI find the contents very difficult to follow. In particular,\n- I'm unable to relate the defined $H,F,G$ to the example in Fig 1. It would be nice if the authors could give a running example to illustrate the concepts.\n- Def 3.1 is taken from the prior work without elaboration. For example, it is unclear to me what is the purpose of finding the induced subgraph and how this is done in prior work. More importantly, I cannot relate the terms \"multi-hop inference\" and \"ILP\" to the contents. In fact, these terms can be misleading in the QA context as there are many QA methods that utilize multi-hop reasoning on knowledge graphs and one of the techniques used here is referred to as inductive logic programming (ILP).\n- Given that this work builds on top of a specific approach to the QA task with a specific technical stack, it would be nice to formally introduce the big picture and the intuitions behind them before presenting the proposed method.\n\nOther confusions:\n- What are abstract facts mentioned in Section 3?\n- What does it mean \"to facilitate grounding abstract chains\"? What is \"grounding\" mentioned in 3.1? Is it related to the grounding notion in first-order logic?\n",
            "summary_of_the_review": "This work focuses on improving the performance of QA tasks with explanations based on an existing approach. The proposed method relies on a unique stack of techniques and demonstrates better performance than the prior work in the experiments. That said, the impact of this work is limited. There might be some novelties for this particular line of research but I'm not familiar with the literature to be certain.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4413/Reviewer_sAEZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4413/Reviewer_sAEZ"
        ]
    }
]