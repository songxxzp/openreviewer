[
    {
        "id": "zUgWt8AcqiV",
        "original": null,
        "number": 1,
        "cdate": 1666524610474,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666524610474,
        "tmdate": 1666524610474,
        "tddate": null,
        "forum": "8Oun8ZUVe8N",
        "replyto": "8Oun8ZUVe8N",
        "invitation": "ICLR.cc/2023/Conference/Paper1131/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an interesting strategy for transferring the knowledge from a model pretrained on 2D images to the 3D point cloud data. In detail, the developed strategy consists of two stages, one fine-tuning the pretrained models with new modules designed for 3D data and one learning a new 3D model via distilling knowledge from the model obtained in the first stage. The method is evaluated on three datasets, including ModelNet40, ScanObjectNN, and S3DIS, covering classification and segmentation. \n\nThe main contribution of this paper is the novel knowledge transfer strategy across different modalities.",
            "strength_and_weaknesses": "== strength ==\n\n- this paper is well-written and easy to follow. this paper is organized clearly. code is also provided for reproductivity.\n\n- the experimental results on different tasks and datasets are convincing, showing the effectivenesses of the proposed method.\n\n- the motivation is reasonable. the analysis for model pretraining on 3d and 2d is insightful including architecture, data, and pattern. based on the observation, the devised training strategy is also reasonable and efficient.\n\n=== weaknesses ===\n\n- I wonder how the model after the first stage performs.\n\n- feature visualization is expected. ",
            "clarity,_quality,_novelty_and_reproducibility": "well-written\n\nnovel\n\nsee strengths and weaknesses.\n\n",
            "summary_of_the_review": "This paper is well-written, and the proposed method is novel.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1131/Reviewer_8fVd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1131/Reviewer_8fVd"
        ]
    },
    {
        "id": "dWlnWW-kQD",
        "original": null,
        "number": 2,
        "cdate": 1666557907723,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666557907723,
        "tmdate": 1666557907723,
        "tddate": null,
        "forum": "8Oun8ZUVe8N",
        "replyto": "8Oun8ZUVe8N",
        "invitation": "ICLR.cc/2023/Conference/Paper1131/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes to leverage 2D pre-trained transformer models to help 3D understanding. The whole process follows two steps:\n- The first is constructing a teacher model using the 2D / language knowledge via self-supervised prompt tuning. \n- The second is pre-training a student 3D network via masked point modeling with the features from the teacher model as the guidance.\n\n",
            "strength_and_weaknesses": "# Strength:\n- The paper leverages the knowledge from the image/language domains to improve the performance in 3D tasks, mitigating the lack of data in 3D.\n- Instead of directly using 2D networks in 3D tasks, the paper constructs a two-step process to distill knowledge from 2D networks to 3D.\n- Clear performance gain is observed in ScanObjectNN. Different settings (Full, MLP-Linear, MLP-3) are tested. \n\n# Weaknesses: \n- The performance on scene dataset S3DIS is far from state-of-the-art, though showing improvement over the baseline model. Is it possible to replace the backbone network with some stronger 3D networks to see if there is any improvement? \n- It is not clear what causes the performance diversity with different teacher choices. Why do the vision models and language models perform better than language-vision models?\n- In table 7, the DeiT-B dVAE without prompt has already achieved better performance than Point-BERT, especially the 'Freeze' one. In this case, what bridges the gap between 2D and 3D modalities? Is it related to the pre and post embedding network? Also, is it possible to change the g_pre and g_post to some more complicated structures to improve the performance?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clearly written and easy to follow in general. The best experimental results in ablation studies are expected to be highlighted to make it clearer.\n- The whole pipeline seems like a combination of VPT (Visual Prompt Tuning), Point-BERT, and Point-MAE. Yet the idea of combining existing works to make use of multi-modal knowledge in 3D is interesting. \n- Codes are provided.  ",
            "summary_of_the_review": "In general, I think this is an interesting paper to combine multi-modal knowledge. Some questions are listed in the weakness part. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1131/Reviewer_FxNn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1131/Reviewer_FxNn"
        ]
    },
    {
        "id": "wjHZ51-3su",
        "original": null,
        "number": 3,
        "cdate": 1666606911005,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606911005,
        "tmdate": 1666606911005,
        "tddate": null,
        "forum": "8Oun8ZUVe8N",
        "replyto": "8Oun8ZUVe8N",
        "invitation": "ICLR.cc/2023/Conference/Paper1131/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a cross-modal feature distillation framework for 3D representation learning. Specifically, the proposed framework performs masked modeling as feature distillation from pre-trained 2D image Transformer to 3D Transformer students. Experiments show that the proposed framework has satisfactory generalization performance. ",
            "strength_and_weaknesses": "### Strength\n* The paper is well-written and very enjoyable to read. \n* The proposed approach is novel and technically sound. This is the first work showing that a pre-trained 2D vision transformer can\nhelp 3D representation learning without accessing any 2D data.\n* The experiment setups and analysis are comprehensive. Moreover, the paper offers informative insights with several interesting discussions.\n* The improvements are significant. The proposed framework ACT consistently performs well against the other baselines by a large margin.\n### Weaknesses\nI found no major weaknesses.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n* Again, the paper is well-written. Adequate references to earlier contributions are also provided.\n### Novelty\n* As mentioned in the strength section, the proposed approach is technically sound and novel.\n### Reproducibility\n* The code has been included in the supplemental material with clear documentation, which should be sufficient to reproduce the experiments.\n\n",
            "summary_of_the_review": "This paper may inspire future work in transferring cross-modal knowledge in a self-supervised fashion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1131/Reviewer_8SEp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1131/Reviewer_8SEp"
        ]
    },
    {
        "id": "76kUUodUXo",
        "original": null,
        "number": 4,
        "cdate": 1666904219448,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666904219448,
        "tmdate": 1666904219448,
        "tddate": null,
        "forum": "8Oun8ZUVe8N",
        "replyto": "8Oun8ZUVe8N",
        "invitation": "ICLR.cc/2023/Conference/Paper1131/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "1) The paper proposes ACT to utilize 2D pre-trained models as teachers for cross-modality knowledge transfer by masked point modeling. This can alleviate the data desert problem in 3D by using rich semantics learned from 2D images. \n2) ACT consists of three steps: 1. Adapt the pre-trained 2D/language models into 3D teachers via DAE and prompting on ShapeNet. 2. Regard this transferred model as distillation teachers to pre-train a Point-MAE. 3. Fine-tune the pre-trained Point-MAE on specific downstream tasks, e.g., classification, segmentation.\n3) ACT achieves favorable performance on various downstream benchmarks. The related discussion solves part of the confusions.",
            "strength_and_weaknesses": "Strength:\n1) The problem ACT aims to solve is practical and reasonable: insufficient 3D data can not afford large-scale pre-training like 2D and languages. The motivation is also clearly illustrated in the introduction that pre-trained models from other modalities with cross-modal teaching might be helpful.\n2) Writing and figures are presented with high qualities. It is easy to follow and understand the proposed multi-step method.\n3) The idea to construct cross-modal teachers by intermediate antoencoding and prompting is interesting.\n\nWeakness:\n1) The first concern is the too much complicated pipeline. As summarized above, for a downstream task, the pre-training requires two steps of pre-training (actually three steps if 2D's is included), which is time/space-consuming. The reviewer is suspicious about utilization values of ACT. The author could give some pre-training efficiency comparison with existing 3D self-supervised methods.\n2) It is unclear whether the 2D models indeed helps 3D understanding. Although the authors try to explain it in the discussion section, it still remains black-box. \n\n>**(a)** As shown in Figure 2 (b), using BERT-B pre-trained by languages as the teacher seems to surpass all of the vision models except ViT-B. If this results indicate that a good pre-trained transformer as initialization is already working for ACT, no matter it is a 2D model or not? \n\n>**(b)** In Table 8, the paper only utilizes Point-BERT to replace 2D teachers. Considering Point-BERT itself underperforms the newer PointMLP, MaskPoint and Point-MAE, using Point-BERT for ablation is not convincing to verify the effectiveness of ACT's 2D teachers. What happens if using Point-MAE here? Will the simple 3D teacher be better than 2D? \n\n>**(c)** The reviewer fails to understand the analysis about Table 10, e.g. why different positional encodings reveal 2D model understands 3D?\n\n3) The performance of ACT is not strong enough. \n\n>**(a)** Why ACT performs a little worse than Point-MAE on ModelNet40 (93.7 vs 93.8)? Given that ACT completely inherits network architectures of Point-MAE, if this means the simple 3D MIM in Point-MAE is already better than ACT's complicated two-step pre-training on synthetic data? \n\n>**(b)** On ShapeNetPart, ACT can hardly bring enhancement over Point-MAE (86.14 vs 86.1).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written with some novelty concerning the construction of cross-modal teachers, but fail to clarity its main contribution: why 2D helps 3D?",
            "summary_of_the_review": "The reviewer expects authors to respond to above questions in the weakness, which largely matter the contribution of this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1131/Reviewer_Dx8R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1131/Reviewer_Dx8R"
        ]
    }
]