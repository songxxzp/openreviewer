[
    {
        "id": "ZX95pnR59hm",
        "original": null,
        "number": 1,
        "cdate": 1666746947457,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666746947457,
        "tmdate": 1666746947457,
        "tddate": null,
        "forum": "-htnolWDLvP",
        "replyto": "-htnolWDLvP",
        "invitation": "ICLR.cc/2023/Conference/Paper5494/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes Classwise Separability Discriminant (CSD), an unlearnable strategy to transfer the unlearnable effects (perturbations to avoid unauthorized data usage) to other training settings and datasets by enhancing the linear separability. Experiments showcase transferability of the proposed unlearnable examples across training settings and datasets",
            "strength_and_weaknesses": "Strengths\n-the paper proposes training-wise and data-wise transferability for unlearnable effects using contrastive learning\n- the proposed methods uses linear separability in addition to similarity between augmentations \n-experiments are extensive and detailed and show the improvement in data-wise and training-wise transferability \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and novel. The experimental settings are detailed for reproducibility\n\n",
            "summary_of_the_review": "The paper addresses an important problem of improving data-wise and training-wise transferability of unlearnable examples. The proposed approach is novel and detailed results show it outperforms its counterparts.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5494/Reviewer_sZJu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5494/Reviewer_sZJu"
        ]
    },
    {
        "id": "jNyvmb03f88",
        "original": null,
        "number": 2,
        "cdate": 1667045764224,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667045764224,
        "tmdate": 1670370309891,
        "tddate": null,
        "forum": "-htnolWDLvP",
        "replyto": "-htnolWDLvP",
        "invitation": "ICLR.cc/2023/Conference/Paper5494/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an unlearnable strategy to prevent third parties from training on the data without authorization. The unlearnable strategy is based on class-wise separability discriminant that seeks to transfer better to other training settings and datasets, in contrast to existing methods that only work on a target training setting and a target dataset. Experiments on digit datasets suggest the feasibility of generating unlearnable examples across training settings and datasets.",
            "strength_and_weaknesses": "- The paper systematically establishes the limitations of existing supervised unlearnable methods, and then proposes a simple strategy to make the unlearnable examples transfer to unsupervised and other datasets. \n\n- The proposed strategy is simple, and adds a class-wise separability discriminant loss to the contrastive loss for learning a transferable representation.\n\n- Interpolation data-wise transferability strategy is not convincing. Just linearly interpolating class samples or creating new class categories is not going to be representative of the real-world situations. Further experimentation is required to establish the claims to this end.\n\n- Comparison with supervised contrastive learning that makes use of samples within the class categories as positives, and other class categories as negatives would be useful, as supervised contrastive learning yields representations that are similar to the ones desired from class separability discriminant. \n\n- It is also not clear to me the rationale of the paper. If the goal is to avoid unauthorized use of the data, the data could be encrypted, or the security authentication protocol can be defined accordingly. Adding a noise to make it linearly separable seems difficult for challenging datasets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. It identifies the limitation of supervised learnable examples and combines it with an unsupervised representation approach to make it data-wise and training-wise transferable. Novelty of the paper is very incremental, and the intended use case is somewhat clear. Experimental set-up is sound and results are reproducible.",
            "summary_of_the_review": "The paper presents an application of representation learning to dataset protection and security. It identifies the limitation of supervised unlearnable examples in transferring to a new situation, and presents a remedy to make unlearnable examples transferable. More experiments on use cases and challenging datasets will improve the quality of the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5494/Reviewer_5F6K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5494/Reviewer_5F6K"
        ]
    },
    {
        "id": "Pe_-oejaW3",
        "original": null,
        "number": 3,
        "cdate": 1667283055390,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667283055390,
        "tmdate": 1669942249662,
        "tddate": null,
        "forum": "-htnolWDLvP",
        "replyto": "-htnolWDLvP",
        "invitation": "ICLR.cc/2023/Conference/Paper5494/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed an unlearnable strategy, TUE. It aims to simultaneously enjoy training-wise transferability and data-wise transferability by enhancing linear separability. Experimental results show that the proposed method shows advantages in both transferability settings.\n",
            "strength_and_weaknesses": "Strength:\n1. This paper proposed a simple but effective unlearnable strategy that can preserve both training-wise transferability and data-wise transferability.\n2. The paper is easy to understand, the formula is concise, and the algorithm description is clear. \n3. The performance gain seems to be significant.\n4. The visualization of unlearnable examples helps understand the effectiveness of linear separability.\n\nWeaknesses:\n1. In section 3, the authors mentioned that this paper focuses on supervised unlearnable examples, which seems to contradict the training-wise transferability of the proposed method.\n2. The first term of equation 5 combines unsupervised loss explicitly, while the second term L_S combines supervised loss implicitly. What if we consider both unsupervised and supervised loss explicitly?\n3. It is not clear how much interpolation affects data-wise transferability.\n4. What are the backbones of EMN and SN in Table 5 and Table 6?\n5. No ablation experiments on the two proposed losses.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\nThe algorithm of this paper is clarity and easy to understand.\nHowever, something is puzzling in experimental results.\n\nQuality\nThis paper provides an effective strategy and some meaningful conclusions.\n\nNovelty\nThe contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n",
            "summary_of_the_review": "Overall, this work presents a simple, yet seemingly effective unlearnable strategy, and the paper is well written and easy to understand. But there are some deficiencies in the experiment, so I would give score 6 for now.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5494/Reviewer_esp8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5494/Reviewer_esp8"
        ]
    },
    {
        "id": "r-cNiOGEsd",
        "original": null,
        "number": 4,
        "cdate": 1667813870579,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667813870579,
        "tmdate": 1670527476178,
        "tddate": null,
        "forum": "-htnolWDLvP",
        "replyto": "-htnolWDLvP",
        "invitation": "ICLR.cc/2023/Conference/Paper5494/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a strategy to address a shortcoming of prior 'unlearnable' strategies which aim to prevent training on unauthorized, namely the lack of transferability across training settings and datasets. Typical strategies add perturbations generated for a specific training setting and target dataset, and so typically do not transfer. This paper proposes a strategy called Transferable Unlearnable Examples (TUE) based on Classwise Separability Discriminant (CSD) to enhance linear separability of perturbation classes to improve the transferability of unlearnable examples. This method is an extension of prior contrastive learning algorithms to also balance linear separability, with interpolation for coverage of missing classes. Experiments show its improvements over other recent methods in most cases, particularly transferability. ",
            "strength_and_weaknesses": "Strengths:\n- Experiments on three datasets support that the proposed method generally improves both unlearnability and improves unlearnability transfer compared to recent methods (by a large margin in some cases, but certainly more consistently across different settings). \n\nWeaknesses: \n- The motivation is somewhat unclear at points. Paper clarity and organization could be improved. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The proposed method is described relatively clearly, but the organization of the paper could be improved. \n    - The text is occasionally redundant, and some points are mentioned multiple times before sufficiently defined or described. Adding forward references in the introduction or related work to sections later may help improve the overall organization. \n    - It can be difficult to find some model and training details and would be difficult to reproduce some experiments with the details provided (Section 3.2.1 or the compared methods, for example). \n- The motivation is not completely clear to me; the experiments seem to aim preventing learnability in general, but the stated application is preventing training on protected user data, which would rarely be the full set of available training data. It is not clear to me that the proposed method would be effective in such a case where the data is only partly obscured. The unsupervised case is also stated as left to future work on page 2 but is given significant focus. \n- It is not clear to me how the interpolations for new classes are validated in Section 5.3 beyond overall transfer performance.\n- Why does UCL have no protection in supervised settings? While the other two baselines are discussed in Section 3, this is not; it would be nice to add a short description. \n- Minor presentation notes: the space has been overly compressed around tables and figures (particularly captions); the labels in Figure 1 are too small to read; there are occasional typos (Unlearnbale, unsuperivsed, unlearnbility); \\citet and \\citep are often flipped. ",
            "summary_of_the_review": "This paper presents an effective strategy to promoting transferable unlearnability in image classification settings. While the performance improvements from the method are encouraging, particularly for transfer, the writing of the paper itself could be improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5494/Reviewer_fZkw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5494/Reviewer_fZkw"
        ]
    }
]