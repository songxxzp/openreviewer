[
    {
        "id": "sLJZQgYDIv",
        "original": null,
        "number": 1,
        "cdate": 1666448790313,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666448790313,
        "tmdate": 1666448790313,
        "tddate": null,
        "forum": "a2-aoqmeYM4",
        "replyto": "a2-aoqmeYM4",
        "invitation": "ICLR.cc/2023/Conference/Paper903/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study general variational algorithms, which generalize Stein variational gradient descent (SVGD) in functional space. They generalize the functional space in minimizing the KL divergence step. They apply the algorithms in Bayesian neural networks and ensemble gradient boosting. The numerical experiments demonstrate the efficiency of their algorithms.  ",
            "strength_and_weaknesses": "Weakness: Please do not use blue and red colors on the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The paper presents an approach to generalize the Stein variational derivative. However, I did not see any motivations or analytical examples for the proposed methods. What is the quality of this method, at least in some simple examples? ",
            "summary_of_the_review": "This paper provides a method to approximate the Stein variational gradient in general settings. However, there is no analytical or motivation examples to support the proposed method.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper903/Reviewer_9cu5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper903/Reviewer_9cu5"
        ]
    },
    {
        "id": "rA8aSoCN4m7",
        "original": null,
        "number": 2,
        "cdate": 1666632892625,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632892625,
        "tmdate": 1669516735276,
        "tddate": null,
        "forum": "a2-aoqmeYM4",
        "replyto": "a2-aoqmeYM4",
        "invitation": "ICLR.cc/2023/Conference/Paper903/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to perform Stein Variational Gradient Descent directly in the function space of neural networks or gradient-boosted models, thus offering two new methods. It derives theoretically how these approaches can be approximated in practice and shows empirically how they perform on simple benchmark regression tasks.",
            "strength_and_weaknesses": "Strengths:\n- The problem of function-space inference is well-motivated.\n- The paper is clearly written.\n- The theoretical derivations seem to be correct.\n\nWeaknesses:\n- The related work is incompletely acknowledged.\n- The experiments seem rather inconclusive.\n\nComments:\n- The proposed SFVNN seems (at least superficially) very similar to [1] and to some lesser degree to [2]. Other functional BNN approaches that seem worth mentioning are [3] and [4].\n- The paper relies a lot on the KL divergence estimator from Sun et al. However, [5] has shown that the KL divergence is generally infinite between many different function-space distributions and that the estimator is thus ill-defined. Could the authors comment on that?\n- In the presented regression experiments, the proposed method only outperforms the baselines on a few datasets. Could the authors comment on why it doesn't work well on the others?\n- Could the authors comment on the runtime of the proposed method? I would naively think that it would probably be slower than the baselines.\n- Since NNs are not necessarily much better on these regression tasks than, e.g., GPs, I think an image classification task would be a more suited experiment to motivate the method, similar to e.g. [1,2].\n- Since [2] have shown that their method works better than the functional SVGD [1], it seems like that might also outperform the proposed method and should be a baseline in the experiment.\n- The notation is sometimes hard to parse (up to 4 stacked subscripts), so maybe that could be made lighter.\n\n[1] https://arxiv.org/abs/2106.10760\n\n[2] https://arxiv.org/abs/2106.11642\n\n[3] https://arxiv.org/abs/2008.08400\n\n[4] https://hudsonchen.github.io/papers/Tractable_Function_Space_Variational_Inference_in_Bayesian_Neural_Networks.pdf\n\n[5] https://arxiv.org/abs/2011.09421",
            "clarity,_quality,_novelty_and_reproducibility": "The description of the method is clear and it is of high theoretical quality. However, the empirical quality seems limited and the novelty is questionable, especially since many closely related works are not discussed. The experiments seem reproducible.",
            "summary_of_the_review": "Overall, the method seems theoretically well-motivated, but the weak experiments and insufficient discussion of related work currently hinder me from recommending acceptance. If the authors could provide stronger experiments with relevant baselines on realistic data, I would be willing to change my assessment.\n\nUPDATE: I have increased my score thanks to the changes made during the rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper903/Reviewer_bTr3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper903/Reviewer_bTr3"
        ]
    },
    {
        "id": "aJc2qX6oiD",
        "original": null,
        "number": 3,
        "cdate": 1666685130918,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685130918,
        "tmdate": 1666685263171,
        "tddate": null,
        "forum": "a2-aoqmeYM4",
        "replyto": "a2-aoqmeYM4",
        "invitation": "ICLR.cc/2023/Conference/Paper903/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies function-space BNN inference, and formulated function-space variants of the Stein variational gradient descent algorithm.  The algorithm is evaluated on synthetic and tabular regression data.",
            "strength_and_weaknesses": "### Strengths\n\nThis work attempts to provide a rigorous formulation of function-space, SVGD-like algorithms.  This is a sensible problem, as similar particle-based VI approaches have shown promises in the past, including for BNN inference where their function-space analogues have been studied in less rigorous ways.  A rigorous formulation would allow the principled use of such algorithms, and may also identify suboptimal constructions in past algorithms.\n\n### Weaknesses\n\nMy main concerns are the following:\n\n1. The current manuscript did not discuss previous work adequately.  It is not obvious from a first reading what efforts have been made to construct function-space SVGD algorithms: this work only cited Wang et al (2019), and neglected the works of D'Angelo et al (2021a; 2021b) which studied similar issues.  Moreover, there is no comparison between the update rules derived in this work and those in past works; and there is no empirical comparison either.  This leaves it very unclear how new derivations in this work have practical implications.\n\n2. The presentation of the technical contents is quite confusing, which makes it difficult to understand or verify the claims.  Gradients were presented without specifying the inner product structure they are defined with.  The same gradient notation can be used to refer to both L2 and RKHS gradients (e.g., Eq. 3 and the discussion below).  Typos such as in Eq. 8 further complicated reading (Eq. 8 refers to the $\\mathcal{H}$-Wasserstein gradient in a space of distribution over parameters, not \"functions\" which do not have to exist there).  It takes a lot of guesswork -- and familiarity with the background -- to understand what the authors really meant, and it should not have happened in a work that attempts to present a rigorous formulation for ideas that already exist (in part, and in various forms) in the past.\n\nThese two issues make it difficult to evaluate the manuscript in its present form, although I'm willing to go through it again once they are clarified.\n\n3. A less important issue, which should nonetheless be made clear, is that all discussions in this work only applies to the full-batch training setting, where the sampled \"measurement points\" (Sun et al, 2019) always include the entire training set.  This is an inherent limitation (Burt et al, 2020) of the framework of Sun et al (2019), and the readers should be made aware about it.\n\n### References\n\n* Wang, Z., Ren, T., Zhu, J., & Zhang, B. (2018). Function Space Particle Optimization for Bayesian Neural Networks. In International Conference on Learning Representations.\n* D'Angelo, Francesco, and Vincent Fortuin. \"Repulsive deep ensembles are bayesian.\" Advances in Neural Information Processing Systems 34 (2021): 3451-3465.\n* D'Angelo, Francesco, Vincent Fortuin, and Florian Wenzel. \"On stein variational neural network ensembles.\" arXiv preprint arXiv:2106.10760 (2021).\n* Burt, D. R., Ober, S. W., Garriga-Alonso, A., & van der Wilk, M. (2020). Understanding variational inference in function-space. arXiv preprint arXiv:2011.09421.",
            "clarity,_quality,_novelty_and_reproducibility": "See above for comments on clarity and novelty.",
            "summary_of_the_review": "While this work tackles an important problem and appears to have interesting contributions, the clarity issues make it difficult to judge its merits.\n\n(My ratings below reflect the uncertainty due to the clarity issues, and I'm willing to update them after clarifications.)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper903/Reviewer_qRMB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper903/Reviewer_qRMB"
        ]
    },
    {
        "id": "N20hko5mA1P",
        "original": null,
        "number": 4,
        "cdate": 1666707981512,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666707981512,
        "tmdate": 1666707981512,
        "tddate": null,
        "forum": "a2-aoqmeYM4",
        "replyto": "a2-aoqmeYM4",
        "invitation": "ICLR.cc/2023/Conference/Paper903/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to perform approximate Bayesian inference in function spaces with an emphasis on applications to Bayesian neural networks and gradient boosting. The proposed approach extends Stein variational gradient descent (Liu & Wang, 2016) to function spaces by applying it to minimise the KL divergence with respect to a posterior stochastic process. The approach is made tractable by approximations propagating the functional gradients to the parameter space of the models being learnt. Experiments learning BNNs and gradient boosting models on benchmark datasets are presented showing performance improvements with respect to baselines.",
            "strength_and_weaknesses": "### Strengths\n* Compared to previous work (Sun et al. 2019; Wang et al., 2019), the proposed approach more naturally translates SVGD to function spaces by starting its derivation from functional gradients.\n* Theoretical results are provided on the validity of the functional gradient estimators.\n* Experimental results show performance improvements against previous function-space variational approach FVBNN.\n\n### Weaknesses\n* Related work on SVGD could mention some recent work on other forms of SVGD, such as second-order methods, matrix-valued kernels, and its convergence analysis.\n* Related work on BNNs only mentions variational inference approaches, while Markov chain Monte Carlo methods, such as stochastic gradient Hamiltonian Monte Carlo, have also shown some success on inference for BNNs, though usually in problems of low dimensionality.\n* Background on gradient boosting could be expanded.\n* What is most concerning to me is that practical performance gains seem marginal when considering the results in Table 1.They are mostly within the +/- 1 std. deviation margin.\n* There is no discussion on the results in Table 1. For example, why did the BNN (using Bayes by backprop) baseline perform significantly better than the functional-gradient methods in a few of the benchmarks? And importantly, what would be a few reasons for the performance of the proposed SFVNN and the FVBNN baseline to be so similar in most cases?\n* In the appendix, there were no baselines for the comparisons with SFVGB.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well written, despite a few typos. The contribution seems novel. There's no mention of whether code would be released for reproducibility purposes.\n\nMinor clarity issues:\n* $\\mathcal{I}_b$ could have an explicit definition.\n* The integral in the denominator in Eq. 7 should be over $\\boldsymbol{\\theta}$, not $\\mathbf{x}y$.\n* A few parts of the first paragraph in Sec. 2.1 are missing the space between the period and the beginning of the next sentence.",
            "summary_of_the_review": "The paper's contributions seem to be mostly on the formulation of a new method with theoretical support, but with only marginal practical performance gains in experimental evaluations, which cast doubts regarding its potential impact.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper903/Reviewer_NXDp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper903/Reviewer_NXDp"
        ]
    }
]