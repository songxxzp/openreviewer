[
    {
        "id": "g4da3T2lsK",
        "original": null,
        "number": 1,
        "cdate": 1666632674500,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632674500,
        "tmdate": 1666632674500,
        "tddate": null,
        "forum": "GcM7qfl5zY",
        "replyto": "GcM7qfl5zY",
        "invitation": "ICLR.cc/2023/Conference/Paper1210/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper is the first work on automating the architecture design of graph Transformer, which is rarely studied in graph Transformer research. The contributions lie in solving the two challenges specifically related to graph-structured data during the design of neural architecture search.",
            "strength_and_weaknesses": "Strengths:\n1. The novel idea of automating the design procedure of graph transformer.\n2. Utilizing the neural architecture search for graph Transformers tends to become popular and practically important in graph representation learning community.\n3. The proposed model is solid and the solution is appropriate for solving the challenges.\n4. The paper is well written and the related work eases the understanding of neural architecture search and graph Transformer literature.\n\nWeaknesses:\n1. Fig. 1 is presented without explanation. Detailed explanations are expected to be provided. \n2. For the architecture search and training step, the authors are encouraged to further discuss the difference from the proposed main ideas from the previous works on neural architecture search.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, with good quality and novelty. Reproducibility will be guaranteed if the authors promise to release the code.\n",
            "summary_of_the_review": "This work proposes a neural architecture search based framework, Automated Graph Transformer (AutoGT), for automating the architecture design of graph Transformers. Tailored for non-euclidean graph data, the search space of AutoGT takes both the graph encoding strategy and Transformer architecture into considerations. Furthermore, the performance estimation strategy of AutoGT is designed to be encoding-aware through joint optimization of the encoding strategy and graph Transformer architectures. Experiments over several graph datasets with different sizes demonstrate the effectiveness of the proposed AutoGT. I think this paper investigates an interesting problem of good importance and I recommend to accept.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1210/Reviewer_iKrv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1210/Reviewer_iKrv"
        ]
    },
    {
        "id": "MgzktF4mZeH",
        "original": null,
        "number": 2,
        "cdate": 1666686338372,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666686338372,
        "tmdate": 1666686338372,
        "tddate": null,
        "forum": "GcM7qfl5zY",
        "replyto": "GcM7qfl5zY",
        "invitation": "ICLR.cc/2023/Conference/Paper1210/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper investigates the interesting problem of automated graph transformer through neural architecture search. The authors first propose a unified graph Transformer formulation that can represent most of the state-of-the-art graph transformer architectures. Then, an encoding-aware architecture searching and supernet training strategy are proposed. Experimental results over several graph datasets including the large-scale OGB datasets show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "### Strengths:\n+ The proposal of automatically searching the optimal graph transformer architecture is important in the community of machine learning.\n+ Constructing a unified graph Transformer formulation by combining several typical transformer architectures and graph encoding strategies seems to be a novel contribution.\n+ Taking graph-specific properties into the automated graph transformer architecture search process is helpful and can distinguish this paper from other approaches such as autoformer.\n+ Experiments are conducted over several widely used and benchmark datasets.\n\n### Weaknesses:\n- The major focus in this work is the complex relations between transformer architectures and the graph encodings in transformer layer. The authors are suggested to further discuss why this problem is important and challenging, and how the authors address the problem specifically. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written with comprehensive experimental evaluations over several well-used datasets. The idea proposed in this paper is novel. The paper is overall of good quality.",
            "summary_of_the_review": "This paper studies an interesting problem of automating the neural architecture design of graph transformer. The authors raise the challenges in the investigated problem, followed by the proposal of technical solid solution. Extensive experimental results validate the effectiveness of the proposed solution. I think this paper tends to be interesting to the research community and recommend acceptance for this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1210/Reviewer_ueHs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1210/Reviewer_ueHs"
        ]
    },
    {
        "id": "dNfBjyqNgAr",
        "original": null,
        "number": 3,
        "cdate": 1667367486034,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667367486034,
        "tmdate": 1668630551913,
        "tddate": null,
        "forum": "GcM7qfl5zY",
        "replyto": "GcM7qfl5zY",
        "invitation": "ICLR.cc/2023/Conference/Paper1210/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Existing graph transformers have succeeded in different applications but require cumbersome architecture designs and tuning by experienced engineers. This paper proposed a novel search framework for graph transformers, automatically searching within the designed unified search space for optimal architecture designs. The proposed method is compatible with most of the existing state-of-art graph transformers designs without introducing a substantial amount of computation cost. The experimental results show proposed strategy results in better designs over hand-crafted baselines. ",
            "strength_and_weaknesses": "Strength \n1. Well-explained basic transformer and graph encoding strategy, which show excellent intuition of the search space design. \n2. The proposed framework is compatible with more existing graph encoding strategies, which allows a broader comparison between graph transformer designs. \n3. Tab5 shows an ablation study using a single supernet vs. additional supernets. The ablation study shows the improvement of performance by splitting a supernet. Such a design brings extra performance without extra parameters. \n\nWeakness:\n1. Section 3 explained the proposed AutoGT well in text. However, neither Fig 1 nor Fig 2 are self-explanatory. I could not understand their relations with the proposed method without reading the main paragraphs. The small fonts in the figures made it more challenging.\n2. Figure 2 was never mentioned in the text, which makes it tricky to understand which section it belongs to. \n3. Table 3 tries to show the importance of the proposed evolution search by comparing it to simple random selection and existing hand-crafted options. The results have large deviations. How can we ensure the proposed method results in consistently high-quality architecture designs? \n4. The proposed method aims to automatically provide graph transformer designs. However, AutoGT introduces extra hyper-parameters such as the number of supernet sizes and the search space's design. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Consistent quality\n\nTable 3 tries to show the importance of the proposed evolution search by comparing it to simple random selection and existing hand-crafted options. Excellent experiment design! However, given the large deviation from the results, how can we ensure the proposed method results in consistently high-quality architecture designs? Without such a property, engineers will have concerns about deploying such a model to real-world applications. \n\n\nUsing eight subnets\n\nThe ablation study shows the performance boost from splitting the supernet into eight subnets. I would like to know how this behavior scales. Can you have better/worse results by having 2,4,16,32,64 subnets? You don't have to run all these setups, and some intuitive explanation serves my curiosity. \n\n\nHow to tune AutoGT\n\nSince the proposed method aims to automatically provide graph transformer designs, AutoGT also has some hyper-parameters, for example, the number of supernet size and the search space. Can you justify why this makes the design process easier? How can you make sure the search space includes the right amount of options, not too many, not too few? \n",
            "summary_of_the_review": "This paper proposed AutoGT, which can automatically search the design space, which is compatible with most of the existing state-of-art graph transformers designs, without introducing a substantial amount of computation cost. The experimental results show proposed strategy results in better designs over hand-crafted baselines. However, I have concerns about the ease of use since AutoGT introduces extra hyperparameters that need to be tuned. And the auto designs from AutoGT have less consistent performance. \n\nI recommend 6-marginally above the acceptance threshold. \n\n\n----- After hearing back from authors 11/16 -----\n\nThanks for the explanation and new results. They cleared all of my concerns. Changing to \"8: accept, good paper \"",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1210/Reviewer_C87i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1210/Reviewer_C87i"
        ]
    }
]