[
    {
        "id": "7SGDpBXqu8",
        "original": null,
        "number": 1,
        "cdate": 1666584842135,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584842135,
        "tmdate": 1666584842135,
        "tddate": null,
        "forum": "T5ADm9PHGeJ",
        "replyto": "T5ADm9PHGeJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4711/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors proposed a differentiable NAS approach that searches for efficient CNN models on GPUs. The key idea is similar to existing differentiable NAS methods: modeling NAS as a bi-level optimization problem where both weight and architectural parameters are updated during SuperNet training. The paper proposes to dynamically shrink the search space during the procedure of super network training. After super network training, operators with the largest architectural parameter is kept for each searchable block, which is also the standard procedure in differentiable NAS.",
            "strength_and_weaknesses": "Strengths:\n- The authors carry out very extensive experiments on ImageNet and COCO, achieving impressive accuracy results.\n- The proposed method itself is easy to understand.\n\nWeaknesses:\n- The writing of this paper needs to be improved. It seems to me that this paper is a tech report rather than a research paper. For citations in the paper, please make sure that the whole sentence can read. For example,\n\n> recent research has focused on partial training Falkner et al. (2018); Li et al. (2020a); Luo et al. (2018), performing network morphism ...\n\nin p1 does not seem to be a readable sentence. The authors should also consider to replace $*$ with $\\times$ in equation in the end of page 3.\n\n- Despite good results, the method itself has very limited novelty. Modeling the NAS problem as bi-level optimization, using Gumble softmax to convert non-differentiable HPO problems to differentiable optimization problems, and adding a latency term in the loss function are all very well-recognized techniques in the NAS community. The design space used in this paper is also highly inspired by EfficientNetV2. One can also easily find the idea of gradually shrinking the NAS design space in [AtomNAS](https://arxiv.org/pdf/1912.09640.pdf) of ICLR'20. \n- The comparison in Figure 4 is not fair. The method is clearly inspired by EfficientNetV2, but it compares itself with an apparently weaker baseline, EfficientDet (backed by EfficientNetV1). \n- The authors never made a head-to-head comparison against one-shot NAS (e.g. Single Path One Shot NAS in ECCV'20) or other differentiable NAS methods in the same search space, which makes it very hard for me to judge the merit of this paper against existing approaches.",
            "clarity,_quality,_novelty_and_reproducibility": "- The method is easy to understand, but the experiment details are not clear.\n- The experiment results are quite good, but this does not mean that the experiment quality of this paper is high. I believe most of the comparisons are not fair in this paper.\n- Novelty is fair.\n- Reproducibility is not guaranteed since the authors did not release the code and did not claim that code will be released.",
            "summary_of_the_review": "Despite good results, this paper fails to make fair comparisons with existing methods and has very limited novelty. Without a major reformat, I cannot see the possibility of recommending acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4711/Reviewer_Xpgv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4711/Reviewer_Xpgv"
        ]
    },
    {
        "id": "6vCX5pdZYcM",
        "original": null,
        "number": 2,
        "cdate": 1666693759387,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666693759387,
        "tmdate": 1666694031579,
        "tddate": null,
        "forum": "T5ADm9PHGeJ",
        "replyto": "T5ADm9PHGeJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4711/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies a new approach with the purpose of improving the efficiency of differentiable neural architecture search (DNAS). The authors introduce Prunode, a stochastic bi-path building block that enjoys O(1) memory and computation complexity during the search. Given this advantage, Prunode allows a much larger search space than conventional NAS algorithms. To further reduce the searching computation, the authors also propose to prune the blocks within the stochastic layer that are less likely to be chosen, as well as unnecessary layers. Experimental results show that PRUNET establishes the new state-of-the-art Pareto frontier w.r.t latency and accuracy on ImageNet.",
            "strength_and_weaknesses": "Strengths:\n\n- The authors propose to prune the search space for efficient neural architecture search. The pruning is conducted on different network angularities: inner block level, inter block level and layer level. \n\n- Experimental results show that PRUNET achieves new SOTA w.r.t the inference latency and classification accuracy on ImageNet. \n\n- The authors also provide thorough ablation studies over the hyperparameters of the algorithm.\n\n\nWeakness:\n\n- A major concern with this paper is the heuristics that make the procedure overcomplicated. There are also a number of associated hyper-parameters, e.g., constant $c$, max_distance, momentum in Section 3.2.1, $e_{\\textrm{warmup}}$, $e_{\\textrm{warmup}}$, $t_{\\textrm{initial}}$ and $t_{\\textrm{final}}$ in Section 3.2.2, and $\\lambda$, $\\phi$ in Section 3.2.3. Although authors provide some ablations, these hyperparameters make the approach hard to tune in practice.\n\n- Algo 1 is hard to follow. More explanations can be provided.  It is also not clear to me in what way can the task loss affect the mask training, since their updates are based on heuristic rules.\n\n- It seems not new to have O(1) memory and computation complexity. It is common for popular NAS methods (e.g., ProxylessNAS [1]) to sample one path from the cell that achieves the same complexity.\n\nDetailed comments:\n\n- How do you construct the latency table? Are the latency values measured based on TensorRT conversion? Besides, for baselines such as EfficientNet, do you apply the same TensorRT conversion before measuring their latencies?\n\n- It seems the algorithm cannot regularize the model size explicitly. Thus picking different sizes of PRUNET architectures from Table 6 can be tricky.\n\n- \"small parameter values mean a negative impact on accuracy but a positive impact on latency, and large parameter values mean a positive impact on accuracy but negative impact on latency\": what do you refer by \"small parameter value\"? Is it the architecture weight or the network parameter?\n\n\n[1] Cai H, Zhu L, Han S. Proxylessnas: Direct neural architecture search on target task and hardware[J]. arXiv preprint arXiv:1812.00332, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is a bit hard to read. The naming of variables seems to be inconsistent, and to some extent, causal (e.g., those in Algo 1).  \n\n- Quality and Novelty: please see the above. \n\n- Reproducibility: It can be hard to reproduce as many hyperparameters exist, which may require domain-expertise for careful tuning.",
            "summary_of_the_review": "The paper studies the important problem of efficient neural architecture search, and experimental results are thorough and solid. I can see the efforts from authors to improve searching efficiency by various practical designs. Yet, my major concern is that too many heuristics involved in the proposed approach that make it hard to implement in practice. The presentation can also be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4711/Reviewer_GWQr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4711/Reviewer_GWQr"
        ]
    },
    {
        "id": "pf8_-rPZiH",
        "original": null,
        "number": 3,
        "cdate": 1666762595040,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666762595040,
        "tmdate": 1666762595040,
        "tddate": null,
        "forum": "T5ADm9PHGeJ",
        "replyto": "T5ADm9PHGeJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4711/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper expands the current supernet training by introducing 3 components:\n\n1. Prunenode: although the name is a bit fancy, the concept is straightforward: it learns the expansion ratio of inverted residual blocks.\n2. Pruning blocks with stochastic layers: it kicks out the low likelihood blocks from the search space.\n3. Pruning unnecessary stochastic layers: this tries to search over the number of supernet layers that skips certain layers.\n",
            "strength_and_weaknesses": "Strength:\n\n1. the idea in this paper is pretty straightforward\n2. the results look promising but not necessarily a solid improvement.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The writing of this paper needs to be significantly improved. The current presentation is far from the bar of NeurIPS.\n2. The novelty of this paper seems very limited; more or less an engineering study.\n3. The application of proposed methods seems to be very limited.\n4. Code is missing.\n",
            "summary_of_the_review": "1. novelty: This is my main concern: a. The three claimed new components: i) searching for extension layers, ii) searching for blocks in an IRB, iii) search for the number of layers in a network are all previously discussed in Once-For-All[1].\n\n2. The curation of search space borrows from GPUNet. It will help a lot if the authors can clarify the contribution of this paper to the scientific community.\n\n[1] Cai, Han, et al. \"Once-for-all: Train one network and specialize it for efficient deployment.\" arXiv preprint arXiv:1908.09791 (2019).\n\n3. What's the performance if without starting from a good network?\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4711/Reviewer_qypu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4711/Reviewer_qypu"
        ]
    },
    {
        "id": "DC7_Flz7Vjq",
        "original": null,
        "number": 4,
        "cdate": 1666833767397,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666833767397,
        "tmdate": 1666833767397,
        "tddate": null,
        "forum": "T5ADm9PHGeJ",
        "replyto": "T5ADm9PHGeJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4711/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a multi-objective neural architecture search that optimizer for both accuracy and latency. They use GPU-Net1 as the skeleton and search for kernel size and the expansion ratio of the intermediate Fused-IRB and IRB layers. They use masked differentiable architecture search, where the output of a layer is the weighted sum of all the outputs of the blocks. The weights of these blocks are computed using gumbel-softmax of the architecture weights of all the blocks in layer i.\n   The input and output dimensions of all the blocks in a layer are the equal. To search for the expansion ratio, rather than have 1 block for each possible configuration, they use masks similar to FBNetV2. To this end, they create two copies of a block and apply small mask on one and a large mask on another. The small mask masks half of the channels to begin with and the larger mask does not mask any of the channels.  As the search progress, the small mask and the larger mask are updated such that the distance between them reduces and finally becomes close to 0. If the architecture weights of the block with small mask is higher, then both the masks shrink. Similarly, if the architecture weights of the block with the larger mask is higher, then both the masks are increased. Finally when the small mask and the larger mask are less than granularity away, the final candidate has channels between the smaller and the larger one.\n   During the search, they also prune the blocks if their weights are lesser than the threshold. As is the case with other pruning based NAS methods, the networks weights are trained for the first X epochs. After that the bilevel optimization of network weights and architecture weights begins. The threshold keeps increasing as the search progresses.",
            "strength_and_weaknesses": "Strength:\n1. They are able to find networks with higher accuracy and lower latency than all their baselines for both image classification and object detection.\n2. They use a novel search space - GPUNet which is better suited for building architectures with high accuracy and low latency.\n\nWeakness/ Questions:\n 1. Are we using masking for searching for kernels too?\n 2. The algorithm is not very straightforward to understand. \n3. How is this masking algorithm better than the masking technique used in FBNet V2? If we use the masking technique of FBNet V2 with the same search space and setup as this paper, how would it fare? FBNet V2 requires only 1 block per layer while searching for channels. So it seems to be occupying lesser memory.\n\nQuestions about algorithm 1\n1. What does progress variable denote? Is it the ratio of number of epochs done to number of epochs left?\n2. Why are we setting the architecture weights to 0 if s > 0? \n3. f weights are always non-negative, then update variable will also be non-negative. When will s become negative?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper can be written more clearly. They described the setup clearly so the experiments can be reproduced. The novelty of the work is limited.",
            "summary_of_the_review": "The algorithm1 is not clearly written. It is not evident why this is better than the masking technique in FBNet V2.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4711/Reviewer_Jf9e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4711/Reviewer_Jf9e"
        ]
    }
]