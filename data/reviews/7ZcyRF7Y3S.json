[
    {
        "id": "2d7cKrZ1aq",
        "original": null,
        "number": 1,
        "cdate": 1666638692108,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638692108,
        "tmdate": 1666639728308,
        "tddate": null,
        "forum": "7ZcyRF7Y3S",
        "replyto": "7ZcyRF7Y3S",
        "invitation": "ICLR.cc/2023/Conference/Paper5055/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present technical results showing that disentangled representations improve generalization if base predictors are sparse (they only use a limited number of features from the representation). Building on this, the authors propose an algorithm that learns disentangled representations (by encouraging base-predictor sparsity) and show its effectiveness on semi-synthetic data and on meta-learning. ",
            "strength_and_weaknesses": "The series of technical results is definitely interesting: the authors start from the assumption that only few features of the representation are useful for prediction. Building on the assumption, they provide insight on why learning disentangled representations is beneficial to downstream performance. Finally, an algorithm is proposed to capitalize on the gained insight (and is made practical, by operating in the dual space, for the meta-learning case).\nThe results would appear to be adequately backed by relevant semi-synthetic experiments and on meta-learning.\n\nIt would be nice, as an add-on, if the authors could provide evidence that the proposed approach is beneficial in a larger variety of contexts. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, as it makes the content accessible even for someone who is not deeply familiar with the topic, as myself. Technical results and assumptions are adequately motivated and explained, and relevant experiments are presented to support the claims.\nThe authors provide code for reproducibility.",
            "summary_of_the_review": "The authors present a series of technical and empirical results that explain the practical benefit of disentangled representations if the base predictors are sparse. I am not an expert of the research area, but this appears to be an interesting and insightful paper. It would be nice to see the usefulness of the proposed approach on additional benchmarks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5055/Reviewer_5SbL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5055/Reviewer_5SbL"
        ]
    },
    {
        "id": "aoJJehZ0Gt",
        "original": null,
        "number": 2,
        "cdate": 1666982079342,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666982079342,
        "tmdate": 1667110322773,
        "tddate": null,
        "forum": "7ZcyRF7Y3S",
        "replyto": "7ZcyRF7Y3S",
        "invitation": "ICLR.cc/2023/Conference/Paper5055/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors prove a new identifiability result that provides conditions under which maximally sparse base-predictors yield disentangled representations. Based on this theoretical result, they propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem. The authors also explore a meta-learning version of the algorithm based on group Lasso multi-class SVM base predictors. ",
            "strength_and_weaknesses": "Strength:\nThe authors developed theoretical studies and results with extensive mathematical proof. The theory in the paper is relatively novel. \n\nWeakness:\n1. The experiments on disentanglement are insufficient to support the claims.  The authors need to visually show the disentanglement effects regarding different latent factors.  Otherwise, we cannot say the learned representations are meaningful. \n\n2. Only one disentanglement metric was used in the paper, the author could use multiple disentanglement scores to validate the results. \n\n3. The author could present additional experimental results on how disentanglement benefits prediction or other tasks, and it could make the paper stronger. \n\n4. The notations of different variables are not easy to follow. The author could list them in a table to improve readability. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The notations and the technical sections are not easy to follow.  ",
            "summary_of_the_review": "The theory developed in the paper is relatively novel. The paper could be stronger with additional experimental results on disentanglement. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5055/Reviewer_iXc7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5055/Reviewer_iXc7"
        ]
    },
    {
        "id": "FeV78ti1Lny",
        "original": null,
        "number": 3,
        "cdate": 1667109747928,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667109747928,
        "tmdate": 1667109747928,
        "tddate": null,
        "forum": "7ZcyRF7Y3S",
        "replyto": "7ZcyRF7Y3S",
        "invitation": "ICLR.cc/2023/Conference/Paper5055/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study how disentangled representations can be beneficial for downstream tasks under certain assumptions. The authors come up with Theorem 1, which states that leveraging multi-task learning can naturally learn disentangled representation when regularizing the base-predictors to achieve maximal-level sparsity.\u00a0The authors propose to solve the problem of spare multi-task learning via solving its relaxed problem.\u00a0The authors show empirical evidence on few-shot learning to support their theorem and to validate the effectiveness of their method.\u00a0",
            "strength_and_weaknesses": "Strength:\n\n-- The paper formalizes the sparse task assumption (I.e., most tasks do not require all features), and introduces a novel identifiability result. The authors validate that disentanglement can be achieved by combining with sparse base-predictors, and can improve generalization, via both empirical evidence and the theories (under certain assumptions).\n\n-- The theoretical analysis shed light on better understanding disentangled representations.\n\n\nOverall, The authors could consider conducting experiments on more tasks and more datasets. I did realize some potential weaknesses on motivation and experiments:\n\n-- Do the authors want to show disentanglement helps with generalization, or sparsity regularization helps with generalization? The authors can consider comparing with the disentanglement methods that do not combine with sparse base-predictors. \n\n-- The authors propose Theorem 1 (and other theoretical results) under certain assumptions. I\u2019m not sure how realistic those assumptions are. According to Figure one, the disentangled methods would work well under two assumptions \u2014 The ground-truth coefficient matrix (See equation 2) should be very sparse, and there should be very few training samples. Without the two conditions, combining sparsity and disentanglement do not show clear benefits.\n\n-- It seems most theorems and propositions are based on MLE, while the meta learning part switches to SVM. \nI don\u2019t know how to understand the significance of the benefits of the group-sparse SVM method (when compared with SVM). For example, lambda/lambda_max = 0.01 yields the best performance (according to Figure 3 and Table 1 in the Appendix). In most configurations, the group-sparse solutions do not seem to outperform the vanilla SVM. In the configuration of 0.01, the benefit in terms of Meta-test is also not so big.",
            "clarity,_quality,_novelty_and_reproducibility": "I don\u2019t have specific concerns on the clarity. The authors wrote a well-organized draft, and reminded readers via using a lot of hyper-links. ",
            "summary_of_the_review": "Overall, I think this is an interesting paper which aims to thoroughly understand the relationship between disentanglement, sparsity and generalization. The authors tried to formalize a few intuitions, propose a few theoretical results and prove them under the dome of formalized assumptions. I think the paper shed some light on better understanding disentangled representations. It\u2019s unclear to me how strong the theoretical results are, how good the methods would be on large realistic datasets.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5055/Reviewer_GZww"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5055/Reviewer_GZww"
        ]
    },
    {
        "id": "KkoE_EhiK1",
        "original": null,
        "number": 4,
        "cdate": 1667575416863,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667575416863,
        "tmdate": 1667575416863,
        "tddate": null,
        "forum": "7ZcyRF7Y3S",
        "replyto": "7ZcyRF7Y3S",
        "invitation": "ICLR.cc/2023/Conference/Paper5055/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the disentangled representation learning. In particular, authors provide the evidence on the improved generalization when the disentangled representations  coupled with sparse base-predictors are learned. The paper then presents a theoretical result on identifiability condition wonder with maximally sparse base-predictors that lead to the desired disentangled representations. Relying on the presented theory, representation that maximizes sparsity is sought, in a hope of eventual disentanglement, in the context of the multi-task learning. Such sparsity maximization however not being trivial, authors propose a sparsity-promoting bi-level optimization paradigm for learning. Later, a connection between bi-lavel optimization and the meta-learning problems is established.  \n",
            "strength_and_weaknesses": "++The paper is well written and motivated. The addressed problem is a problem of sufficient interest. \n\n++The theoretical consideration of the paper is praiseworthy. Specifically, identifiability result  of Theorem 1 is particularly interesting, intuitive, and convincing. \n\n++The paper also proposes bi-level optimization and meta-learning based approaches, which are empirically shown to be effective in Figure 1,  with fewer samples for the Disentangled-Lasso method.\n\n++Supplementary material with additional theoretical and experimental analysis is helpful.\n\n--The reported experiments are rather toy-like settings. It is not clear when the proposed method is expected to be useful. For example, results reported in Table.1 (supplementary material) shows that the proposed method does not compete well against MetaOptNet (although not being directly comparable).\n\n--The meta-learning formulation of Section 3.2 is not easy to follow. The section lacks intuitive explanation and the jump form equation (7) to Proposition (3) is not well explained.\n\n--The paper relies a lot on the supplementary material. This in itself is not a big problem, given the nature of the addressed problem. However, I believe that the paper can be made more intuitive with the help of graphic illustrations and by bringing the B.2 Discussion of Assumption in the main paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please, refer to the strengths above.",
            "summary_of_the_review": "Overall, the paper makes a good case for the addressed problems and provides convincing and theoretical insights. The paper can benefit from more intuitive explanations (considering a broader audience) and the experiments on more realistic datasets.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerning ethical issues as far as can be seen.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5055/Reviewer_wTbQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5055/Reviewer_wTbQ"
        ]
    }
]