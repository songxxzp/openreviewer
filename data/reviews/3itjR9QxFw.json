[
    {
        "id": "maqQGj4gi_",
        "original": null,
        "number": 1,
        "cdate": 1666024931238,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666024931238,
        "tmdate": 1666024931238,
        "tddate": null,
        "forum": "3itjR9QxFw",
        "replyto": "3itjR9QxFw",
        "invitation": "ICLR.cc/2023/Conference/Paper1460/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The main idea of this paper is to convert data to bit, and then model these bits using continuous diffusion models. This paper proposes two additional techniques: the self-conditioning to improve the x0 prediction, and the asymmetric time to improve the sampling speed. There are experiments on both images and texts, which show the effectiveness of this method.",
            "strength_and_weaknesses": "Strength:\nThe author shows that modeling bits is a simple and effective method. This makes it possible to directly use continuous diffusion models to model discrete data such as texts. Although the self-conditioning technique takes some additional time, it looks very useful to improve the generation performance. This method works for both images and texts.\n\n\nWeakness:\n\n1. In Figure 6 (a), \"Asymmetric Time Intervals\" only improves the sample quality when the number of steps is small. When the number of steps is large, a \"time difference\" of zero performs the best. It needs to tune the \"time difference\" hyperparameter carefully given a specific number of steps. So I wonder would it be more convenient to directly use a fast sampling method, such as [1, 2], for a good sample quality with a small number steps?\n\n2. In Figure 6 (b), using a large number of steps for \"UNIT8 (RAND)\" leads to a worse performance. While the author empirically fixes this issue in Appendix, it would be better to provide some formal or intuitive explanation.\n\n3. Since the text is also categorical data, like the \"UNIT8 (RAND)\", would the text also meets the same problem that a large number of steps lead to a worse performance?\n\n[1] Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models\n\n[2] DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and readable. The work is original.",
            "summary_of_the_review": "The method is simple and looks effective under some cases, and meanwhile I have some concerns on the \"Asymmetric Time Intervals\" and the sample quality for categorical data. My recommadation is marginally above the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1460/Reviewer_YYSB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1460/Reviewer_YYSB"
        ]
    },
    {
        "id": "42iPSNgm4pC",
        "original": null,
        "number": 2,
        "cdate": 1666530059870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666530059870,
        "tmdate": 1666530059870,
        "tddate": null,
        "forum": "3itjR9QxFw",
        "replyto": "3itjR9QxFw",
        "invitation": "ICLR.cc/2023/Conference/Paper1460/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel idea of generating images in bits level. The proposed method is shown to be effective on selected dataset. The selected datasets are not simple and includes complex dataset such as COCO(in contrast, some of the works only report results on CIFAR and ImageNet, which are simpler than COCO). The two proposed tricks: 1. self-conditioning and 2 asymmetric time intervals are simple yet shown to be effective also. Overall, I like the paper.",
            "strength_and_weaknesses": "Strength:\n1. The paper is well written and easy to follow.\n2. The proposed methods are simple yet effective and supported by the experiments.\n3. The idea behind the paper is novel: modeling generative tasks in bits level with diffusion models.\n4. The proposed algorithm is well presented and the is reproducible.\n\nWeakness:\n1. The proposed algorithm is not verified on larger dataset such as LAION-5B wit larger models. It is hard to judge if the proposed method still work when scaled to large dataset. \n2. The two proposed tricks are working well on DDPM sampler. However, recently, a variety of efficient samplers are proposed such as DPM-Solver and PLMS sampler. Does the proposed tricks work on those efficient samplers?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. The paper is presenting an idea that the generation tasks can be modeled in bits level with diffusion models. However, the proposed method is not verified on large scale making it unclear on the impacts of those tricks in large scale.",
            "summary_of_the_review": "Overall, I like the idea behind the paper. Although the proposed methods is not verified in large scale, it shows some promising results on complex tasks already (COCO dataset).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1460/Reviewer_j3eh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1460/Reviewer_j3eh"
        ]
    },
    {
        "id": "yZ21-dZ458Z",
        "original": null,
        "number": 3,
        "cdate": 1666582693142,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582693142,
        "tmdate": 1666582693142,
        "tddate": null,
        "forum": "3itjR9QxFw",
        "replyto": "3itjR9QxFw",
        "invitation": "ICLR.cc/2023/Conference/Paper1460/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the simple yet effective idea of using continuous diffusion models to generate discrete data by representing them as binary data and modeling these binary data as real numbers. The authors also propose two techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to improvement in sample quality.",
            "strength_and_weaknesses": "Strengths:\n* The idea of treating discrete data as continuous bits, is simple yet effective.\n* The proposed self-conditioning and asymmetric time intervals techniques seem to improve the performance significantly and are also applicable to other continuous diffusion models.\n\nWeaknesses:\n* The authors should discuss we the experiments have been over small size images (only 64 by 64). If the computational cost is the main concern, it should be delineated if the expectation is that the same performance will be observed for higher resolutions or not.\n* Harder constraint or regularization to enforce the binary outputs has not been adopted based on the assumption that the model expressivity will take care of it. However, conducting experiments to show additional regularization won't benefit the performance will be valuable.",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is well-written with clear exposition of ideas.\n* Enough details are provided to enable reproducibility.",
            "summary_of_the_review": "Overall, the main idea of this paper is novel and simple. The experiments show the good performance of the analog bits idea, and this work paves the way for future works on generating discrete data using continuous diffusion models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1460/Reviewer_h3v1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1460/Reviewer_h3v1"
        ]
    },
    {
        "id": "NfshG4ZFtr",
        "original": null,
        "number": 4,
        "cdate": 1666643815473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643815473,
        "tmdate": 1669242873835,
        "tddate": null,
        "forum": "3itjR9QxFw",
        "replyto": "3itjR9QxFw",
        "invitation": "ICLR.cc/2023/Conference/Paper1460/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors of this paper introduce a method to use diffusion models to generate discrete data. The crux of the method rests on the observation that discrete data can be represented with binary bits and binary bits can be uses as real numbers to train a continuous diffusion model; the output at the end of the diffusion process can be rounded to obtain binary output bits.",
            "strength_and_weaknesses": "Strengths:\n* The authors introduce two techniques - self-conditioning and asymmetric time intervals. Both of these seem useful for both continuous and discrete diffusion models. Other concurrent work has also shown that using self-conditioning improves results of diffusion models.\n* Figure 7 shows that the generated bits are concentrated on two modes without adding explicit constraints and this validates the authors' hypothesis that modeling binary bits is easy. \n* For image captioning, the AnalogBits achieves the same performance as auto-regressive models with just 10 diffusion steps!\n\nWeaknesses:\n* Discrete diffusion models are especially suitable for NLP tasks given the discrete nature of text. Despite, proposing a method for discrete data, the authors only presents results on one NLP task (image captioning). Additional experimental results on other NLP texts would help strengthen this paper.\n* For encoding NLP tokens as analog bits, each token is mapped to a random set of 15 analog bits. As a results, there is no semantic meaning in the new representation (similar tokens don't have similar bit representations). The authors show that using UINT8 is much better when compared to using random bits for discrete image generation. Exploring ways to represent semantic meaning in bit representations of tokens would be an interesting extension to the presented work.\n* Gray codes perform worse than UINT8 and no explanation is give as to why this is the case. ",
            "clarity,_quality,_novelty_and_reproducibility": "The method is clearly explained and the necessary ablations have been conducted. ",
            "summary_of_the_review": "Diffusion models have shown strong performance in continuous domains. However, there are many domains that are not continuous (like text) and adapting diffusion models to discrete settings will aid in exploring whether state-of-the-art results can be improved by using diffusion models for discrete domains. The method in this paper also converts discrete domain data to a \"continuous representation\" and therefore additional work on continuous diffusion models can easily be ported and used for the discrete setting. Furthermore, self-conditioning and asymmetric time intervals are useful general techniques that will be of interest to the broader diffusion model community. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1460/Reviewer_ECCX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1460/Reviewer_ECCX"
        ]
    }
]