[
    {
        "id": "cO0twSNKy3",
        "original": null,
        "number": 1,
        "cdate": 1666355537954,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666355537954,
        "tmdate": 1666355537954,
        "tddate": null,
        "forum": "7PURWDjJCf3",
        "replyto": "7PURWDjJCf3",
        "invitation": "ICLR.cc/2023/Conference/Paper130/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to learn slimmable neural networks with contrastive self-supervised learning without labels. To further improve the performance of sub-networks, this paper suggests using (i) slow-start training for sub-networks, (ii) online distillation, and (iii) loss reweighting. This increases the gradient norm of the main parameters, leading to performance improvements in the main network. This paper demonstrates that the main and sub-networks outperform existing two-stage distillation approaches.\n",
            "strength_and_weaknesses": "Strengths\n- I think the main strength of this paper is that the proposed method outperforms other distillation approaches.\n\nWeaknesses\n- I feel the lack of methodological novelty. The idea of training slimmable neural networks is not new, and online distillation was already used in [1]. I also think other techniques are just engineering, and they have often been utilized in other literature: e.g., loss reweighting is a common strategy for multi-task learning.\n- Although this paper obtains some gains from the proposed techniques, the gains are marginal, and most of the gains come from using a large number of training epochs or MoCo-v3, not the techniques.\n- The training cost is linearly increasing with respect to the number of sub-networks. This could be critical since self-supervised learning is often time-consuming.\n- The motivation, the gradient imbalance, is really critical? I think the motivation is somewhat weak: the gradient imbalance can occur in any neural network layer because there exist dominant and minor neurons in any layer (e.g., think about singular value decomposition of weight matrix). Training sub-networks can be considered as determining the order of neuron importance. Note that the result in Fig (3a) is due to the random order of the neurons.\n  - Could you provide the distribution of singular values of the full weight matrix instead of gradient norms? I think it would be better to understand how training slimmable networks affect the weight matrix.\n- This paper is hard to follow: there are many confusing notations and descriptions.\n  - It would be better to use colors in Figure 1. In addition, it would be better to include the results of the proposed method in Figure 1 for readers.\n  - In general, notations should be defined before using them. For example, there is no description of \u03be when using the parameter.\n  - Suggest to use $\\Theta_{w_i}=\\\\{\\theta_{w_1},\\ldots,\\theta_{w_i\\}\\\\}$ instead of writing $\\Theta_{w_j}\\subset\\Theta_{w_i}$ if $w_j<w_i$.\n  - In Eq (1), why $\\xi_1$ instead of $\\xi_i$? There is no explanation for this.\n  - In the self-supervised learning literature, the first MLP is often referred to as projection and the second MLP as prediction. I recommend following the conventional terminologies. For example, SlimCLR-MoCo-v2 should use projection instead of prediction. The current usage causes some confusion.\n  - What is $\\theta_{1.0\\setminus0.25}$?\n  - What are the main and minor parameters? And why are they main and minor?\n\n[1] Yu & Huang, Universally Slimmable Networks and Improved Training Techniques, 2019\n",
            "clarity,_quality,_novelty_and_reproducibility": "The detailed comments are described in the previous section. In summary,\n- Clarity :: This proposed idea is clear, but its description is hard to follow in general.\n- Quality :: The empirical results are somewhat strong, but it is not clear where the performance gains come from (proposed techniques or MoCo-v3).\n- Novelty :: This paper lacks methodological novelty.\n- Reproducibility :: This paper describes the implementation details well.\n",
            "summary_of_the_review": "While learning slimmable neural networks is interesting, I feel the lack of methodological novelty and the weak motivation about gradient imbalance. In addition, this paper is hard to follow with confused notations. Hence I vote for rejection.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper130/Reviewer_5mjb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper130/Reviewer_5mjb"
        ]
    },
    {
        "id": "rq9Y9b-h7_",
        "original": null,
        "number": 2,
        "cdate": 1666500920715,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666500920715,
        "tmdate": 1666843174448,
        "tddate": null,
        "forum": "7PURWDjJCf3",
        "replyto": "7PURWDjJCf3",
        "invitation": "ICLR.cc/2023/Conference/Paper130/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies slimmable networks under the contrastive self-supervised learning (SlimCLR) setting. It proposes some strategies to solve the interference between weight-sharing sub-networks during training, including slow start training of sub-networks, online distillation, and loss re-weighting according to model sizes, and a switchable linear probe layer. SlimCLR is evaluated on ImageNet based on ResNet-50 using MoCo v2 and v3 frameworks.",
            "strength_and_weaknesses": "### Strength\n\nThis paper is a good study of the effect of slimmable networks for contrastive self-supervised learning. The empirical analysis has some contribution to the community.\n\n### Weakness\n\n**1: The technical novelty is limited and some details are confusing.**\n\nSlimmable networks are a special case of widely studied one-shot NAS (e.g., [R2, R3, R4, R5, R6]), which only considers the width dimension (see discussion in OFA [R2]). There are many techniques to deal with interference among subnetworks. Specifically,\n* \u201cSlow start\u201d belongs to progressive training in one-shot NAS. For example, OFA proposes a \u201cProgressive Shrinking\u201d strategy, which starts with training the largest sub-network and then progressively fine-tunes the network to support smaller sub-networks by gradually adding them into the sampling space.\n\n * \u201cOnline distillation\u201d was originally proposed in US-Nets [Yu et al., 2019b]. Apart from the inplace distillation, it also proposes the sandwich rule.\n\n* \u201closs reweights\u201d aims to assign larger weights for sub-networks with large widths. However, it violates the training objective of slimmable networks. The objective is to make each supported sub-network maintain the same level of accuracy as independently training a network with the same architectural configuration, rather than only training an accurate large \u201csupernet\u201d. This is evidenced in Table 2 (e), where adding loss reweighting makes R-50(0.25) perform worse, so what\u2019s the meaning there?\n\n**2:  Another concern is what are the fundamental differences between self-supervised and supervised training for slimmable networks?** This is not clear to me as all the training techniques used are common practices in supervised training.\n\n**3: What\u2019s the relationship between unsupervised NAS (e.g., [R4, R5]), including the contrastive self-supervised one (e.g., [R6])?** \n\n4: In Page 8, the authors study 4 possible cases of loss reweighting and show the results in Table 2e. However, I find case (3) archives the best performance for most widths but the paper uses case (1) by default in Eq. (5). I disagree with the author's explanation that \u201cTo ensure the performance of the smallest network, we adopt the reweighting manner (1) in practice\u201d as all sub-networks with different widths should be equally important. Otherwise, what\u2019s the meaning of slimmable networks there? \n\t\t\t\t\n**5: The paper lacks mathematical modeling for the gradient divergence issue which leads to the optimization difficulty claimed by the authors.** I think there are only four possible widths and it is not difficult to analyze the gradient magnitude and directions using SGD with maths formulations. Also, some theoretical analysis on convergence is expected, even assuming a linear neural network is fine [R1].\n\n**6: The experiments are far from enough to justify the effectiveness of the proposed method.**\n\n* 6.1:  The results are merely based on the ResNet-50 backbone. However, I would like to see more ResNet backbones such as R-101 and R-152. More importantly, experiments on Vision Transformers, such as ViT-B in MoCo v3, must be included in the experiments.  \n\n* 6.2:  The paper only evaluates the representation quality using linear probing. However, it must evaluate transfer learning performance which is the standard practice in self-supervised learning (e.g., in MoCo v3). For example, experiments on dataset transfer and downstream tasks such as dense segmentation and detection on COCO and ADE20k are needed.  \n\n* 6.3: How about training the whole network (width 1.0) first then using network pruning (e.g., [R7]) to obtain small networks (width 0.25, 0.5, 0.75)? As this strategy can avoid the interference issue during training. \n\n* 6.4: It lacks comparisons with methods dealing with sub-network interference, such as switchable BN [Yu et al., 2019], sandwich rules [Yu et al., 2019b] and many others.\n\n**7:  The discussions and references in related work are far from enough.** There are few discussions with single-shot NAS and unsupervised NAS methods. In addition, as I point out in the technical novelty part, the differences and advantages with the related work must be discussed. \n\n**8:  Writing also needs to be improved.** \n\n* 8.1: What is the definition of the \u201cmain parameters\u201d in the introduction? \n\n* 8.2: In Sec. 3.2, \u201c..., where $L$ is the loss function\u201d. It should be defined in Eq. (1) where it first appears.\n\n* 8.3: Many grammar issues. I only point out a few. \u201cSlimmable neworks\u201d in Sec. 2; \u201cserver performance degradation\u201d in Sec. 3.2. \n\n9: In Sec. 3.2, authors argue that the two ratios in Figure 3 should be large enough. \u201cIn Figure 3f, ..., are larger than 1.0 by a clear margin\u201d. It does provide a clear concept of how large is good enough. In my opinion, it also depends on the network architectures and self-supervised learning frameworks. So Figure 3 may not be statistically significant. \n\n\n**References:**\n\n[R1]: \u201cOn the optimization of Deep Networks: Implicit Acceleration by Overparameterization\u201d, ICML 2018\n\n[R2]: \u201cONCE FOR ALL: TRAIN ONE NETWORK AND SPECIALIZE IT FOR EFFICIENT DEPLOYMENT\u201d, ICLR 2020\n\n[R3]: \u201cBigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models\u201d, ECCV 2020\n\n[R4]: \u201cAre Labels Necessary for Neural Architecture Search?\u201d, ECCV 2020\n\n[R5]: \u201cDoes Unsupervised Architecture Representation Learning Help Neural Architecture Search?\u201d, NeurIPS 2020\n\n[R6]: \u201cContrastive Self-supervised Neural Architecture Search\u201d, Arxiv 2021\n\n[R7]: \u201cResrep: Lossless cnn pruning via decoupling remembering and forgetting\u201d, CVPR 2022",
            "clarity,_quality,_novelty_and_reproducibility": "From the technical part, the paper lacks novelty and I don\u2019t see techniques customized to train slimmable networks in the self-supervised setting. From the experimental perspective, it lacks essential empirical studies. Overall, the paper is clearly below the acceptance threshold.",
            "summary_of_the_review": "Please refer to the above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper130/Reviewer_1XPr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper130/Reviewer_1XPr"
        ]
    },
    {
        "id": "k4atcdIRCkI",
        "original": null,
        "number": 3,
        "cdate": 1666667895509,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667895509,
        "tmdate": 1666667895509,
        "tddate": null,
        "forum": "7PURWDjJCf3",
        "replyto": "7PURWDjJCf3",
        "invitation": "ICLR.cc/2023/Conference/Paper130/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Motivated by the performance gap between self-supervised learning and its counter supervised model, this work provides a one-stage self-supervised small model pretraining protocol. The slimmable network idea is used to get the representations of one augmentation $x_1$. Weights are shared between the set of slimmable encoders (the small one shared with the larger one). All the outputs from slimmable encoders are used as anchors in InfoNCE. The sum of loss from different anchors is calculated eventually. However, this naive implementation will cause gradient imbalance. The work proposed updating the full encoder first, knowledge distillation from the full encoder to the sub-network, and loss reweighting to mitigate the imbalance issue.  ",
            "strength_and_weaknesses": "An interesting work and clear motivation. \n\nHowever, I am not convinced about the gradient imbalance in learning slimmable networks. The measurement used in the paper seems not sufficient to me. Imagine that for each iteration the network weights are updated in a cyclic way, i.e., weights at (1,2,3) are updated in the first iteration, then (4,5,6), (7,8,9), (10,11,12) .... In this way the gradient norm ratio will be low as well, but the network can still be trained reasonably. Another question is since the paper mentioned Res18 in the motivation, is there a comparison of R-18 compared with R-18 distilled by R-50 teacher? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clear, and well-written. But the novelty seems limited to me, with no theory, and the hypothesis seems not convincing. Reproducibility should be ok.",
            "summary_of_the_review": "Overall this is an interesting work even though the idea of slammable is not new and no theory is provided. I hope the author could address my questions in the rebuttal. I\u2019m wondering whether a similar range of performance drops will happen on R-18 like on R-50. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper130/Reviewer_enLj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper130/Reviewer_enLj"
        ]
    },
    {
        "id": "t5d7dWxrim",
        "original": null,
        "number": 4,
        "cdate": 1667460640877,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667460640877,
        "tmdate": 1667460640877,
        "tddate": null,
        "forum": "7PURWDjJCf3",
        "replyto": "7PURWDjJCf3",
        "invitation": "ICLR.cc/2023/Conference/Paper130/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a slimmable contrastive self-supervised learning framework for building small models with SimCLR. It applied the well-known slimmable network and solved the gradient imbalance problem in the training by slow start training, online distillation, and loss reweighting, etc.\n",
            "strength_and_weaknesses": "1. The proposed approach achieved balanced performance between multiple architecture sizes and outperformed the other small SSL models.\n2. The paper is well written and easy to follow.\n\nWeakness\n1. The novelty is limited as this paper simply applies the slimmable architecture with some minor improvements on the training loss.\n2. Given the paper target on training a small SSL model, it would be interesting to compare the performance with the small distilled model in terms of accuracy, model size, FLOPs. The current evaluation is not sufficient to justify the motivation.\n3. The evaluation can be improved by evaluating with other downstream datasets and tasks.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is well written and easy to read.\n\n2. The novelty is limited since the paper mainly applied the well-studied slimmable network.\n\n3. The proposed approach is reproducible.\n",
            "summary_of_the_review": "The paper studied an interesting topic and solved it by applying an existing approach with some minor improvements on the training loss. The evaluation is not sufficient to justify the motivation and contributions.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper130/Reviewer_yYC4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper130/Reviewer_yYC4"
        ]
    }
]