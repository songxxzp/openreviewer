[
    {
        "id": "MTJgWzXQUFb",
        "original": null,
        "number": 1,
        "cdate": 1666374004622,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666374004622,
        "tmdate": 1670939999712,
        "tddate": null,
        "forum": "SNONkz5zEUF",
        "replyto": "SNONkz5zEUF",
        "invitation": "ICLR.cc/2023/Conference/Paper5781/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work establishes the convergence bound of split learning (SL) for non-convex objectives with non-iid data. The convergence results provide some insight on the parameter tuning and potential benefits/limitations of split learning, compared with standard federated learning (FL, local SGD). The authors also conduct empirical comparison between spilt learning and federated learning with respect to data heterogeneity, which provides a deeper understanding between them.",
            "strength_and_weaknesses": "Strengths:\n- The analysis seems solid and correct, though I didn't go through all the details in the appendix.\n- To my knowledge, the convergence result is new, which complements the theoretical study of split learning.\n\nWeaknesses:\n- The main takeaway of this work is that SL is faster than FL and generalizes similar to FL on mildly non-IID data, while FL prevails on highly non-IID data. This conclusion does not seem surprising to me since previous works (e.g., Gao et al. (2020; 2021) cited in the submission) already made such observations.\n- For the theoretical results, since split learning runs sequentially for its local updates, it is basically minibatch SGD with biased updates. Thus, as long as the accumulated error (client drift) can be bounded, establishing its convergence seems straightforward to me. I hope the authors can discuss the technical difficulties on deriving the convergence bound.  \n- The discussion (or guidance for hyperparameter selection) for the theorem is inadequate. For example, in section 4.1, \"one needs to meticulously design the value of K for the best performance\", \"large value of K forces SL prone to converge to local optima\", \"to reduce the communication overhead without hurting the convergence rate\" all require further discussion, such as to analyze the communicated bits of split learning. The Effect of $\\eta_g$ seems interesting, but it is not thoroughly studied. It seems that we can choose an arbitrarily large $\\eta_g$?\n- The presentation also needs to be improved (see below). \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity can be improved, for example,\n- The notations in Eq. 2 are not well defined until Algorithm 1, it is better to reorder the contents to ensure a clearer flow of reading. \n- In section 4.2, the paragraph \"Client drift in SL is upper bounded, as shown....\" does not have a clear flow of ideas. \n- $\\eta_{SL}$ and $\\eta_{FL}$ are not clearly defined. ",
            "summary_of_the_review": "This work provides some new analysis of SL on non-iid data and compare it with FL, which is only marginally interesting from my perspective. It is not clear what are the technical difficulties of proving the theorems, and the discussion of the theorems is inadequate. The overall presentation also needs to be improved.\n\n---------------------------------------------------\nAfter rebuttal: Thank the authors for the detailed responses and revision, I really appreciate the effort. I have also checked other reviews and responses. The revision greatly improved the clarity of this paper. However, I still think the novelty of this paper is below the bar as there seems to be no strong or really exciting result. I keep my score but will not be upset if this work gets accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5781/Reviewer_4bP6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5781/Reviewer_4bP6"
        ]
    },
    {
        "id": "aDl-P6LXkLL",
        "original": null,
        "number": 2,
        "cdate": 1666455465134,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666455465134,
        "tmdate": 1666498563855,
        "tddate": null,
        "forum": "SNONkz5zEUF",
        "replyto": "SNONkz5zEUF",
        "invitation": "ICLR.cc/2023/Conference/Paper5781/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyzes the convergence rate of split learning theoretically. With constant step sizes in both inner and outer loops, the outer loop complexity is $T^{-1/2}$ with additional terms depending on noise and dissimilarity. Then it compares the rate between SL and FL and presents discussions/explanations.",
            "strength_and_weaknesses": "The topic is of great importance for theoretical analysis of optimization in distributed machine learning. I believe the proof is correct and solid. The experiments are concrete.\n\nSome questions:\n- I suggest a table for notations, and highlight the super/sub-scripts of $x$. \n- Eq (2) is correct but a bit misleading for readers. Although there is a sum over $k$, one cannot get all $x_i^{r,k}$ over $k$ at the same time because the $k$ stands for iterations, so the $x_i^{r,k}$ comes sequentially like in Algo 1 box. When I saw a sum, I felt the terms can somehow be parallelized but actually not, so it would be great to clarify in a footnote, or always write as in Algo 1. \n- Assumption 3: Could you explain why \u201cIn the IID case, $B = 1, G = 0$\u201d? I suspect (e.g. in stochastic case) $E\\|\\nabla f_i\\|^2 = \\|\\nabla f\\|^2 + Cov(\\nabla f)$, was it wrong? Could you use a few simple examples in appendix calculating $B,G$, like linear functions, $f(x) = ax$ and $f_i(x) = a_i(x)$, and so on?\n- Step size: you choose $T=NKR$, where is $B$? In Thm.1, the third term in $\\min$ depends on $B$. \n- Rate: Is the rate outer loop complexity, and the total gradient calculation $K$ times that number? Could you compare it with SGD, and GD if you simply calculate all stochastic gradients together and average them? I didn\u2019t mean SL has to beat them, but it would be helpful and interesting to compare. If the rate is not as good, it would be great to explain the reason. Is it due to the fact that you cannot shuffle or randomize the individual/local gradients? Are you aware of any lower bound of RHS of Cor. 1 and how suboptimal the result is from that optimal rate?\n- Constant step size: if the step size depends on $T$ and error depends on $T$, would it be better to choose a larger error and a small $T$ at the beginning, i.e. use a large step size at the beginning, then refine the error, increase $T$ and decrease the step size? In SGD one can use the decreasing step size.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the work is novel and important. I like the presentation and clarity of the paper. In Sec.3, the paper gives one sentence summary of techniques of each paper. Later, following each theorem, the paper presents plenty of discussions. \nI suggest adding a table of notations, and a table of main techniques (the iteration expression) and rates of the prior works in appendix. The SplitFedv2/3/SFLG can be explained more clearly. ",
            "summary_of_the_review": "I think the paper is important and solid, and the clarity is great. With a few more revision regarding my questions above, I believe it's ready for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5781/Reviewer_434u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5781/Reviewer_434u"
        ]
    },
    {
        "id": "TzB24RXMlGb",
        "original": null,
        "number": 3,
        "cdate": 1666641434887,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641434887,
        "tmdate": 1671012194861,
        "tddate": null,
        "forum": "SNONkz5zEUF",
        "replyto": "SNONkz5zEUF",
        "invitation": "ICLR.cc/2023/Conference/Paper5781/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper derives the convergence guarantee of SL for non-convex objectives on non-IID data.  It compares also SL against FL theoretically and empirically,",
            "strength_and_weaknesses": "See Below",
            "clarity,_quality,_novelty_and_reproducibility": "See Below",
            "summary_of_the_review": "-In Algo 1. I have a problem understanding exactly what is the communication round here, because in step 8 and 10 there is communication between clients and server. You don't count that as a communication round?\n\nTo have the local update in step 12 (local model update) according to algo 1, communication is needed between the server and the client, so what is local here since there is a communication between server and client?...\n\nIt seems that there are \"local\" communication rounds which are denoted by k=0.1,...K and there are \"global\" communication rounds which are r=0.1...R can you clarify what this is, because for me the communication happens for both iterations k and r? am i missing something?\n\n-In step 12 you put, client-side and server-side updates, again I'm confused here, how the server can access the local gradients to do this local update.\n\n- In step 3, since in SL things work in parallel, why do you need to sample a subset of clients only?\n\n-Table 1: in local updates for SL, is it a typo of indices there x_{i}^{r,k+1} = x_{i-1}^{r,K}- ... or am I missing something again?\n\n-if my understanding is correct, x^r of FL is the same as x^{r,K} of SL. Then from Table 1, SL and FL and producing the same iterations if they use the same learning rates and batches? if my understanding is wrong, can you tell what prevents SL from using x^r in local updates.\n\n-The reported complexity bounds between SL and FL are the same up to the multiplicative constant N.\nThe reason is, I think, the choice of learning rates. \nIf you change the lr of SL by the lr of FL over N you get the same bounds for FL and SL!\n\n-Does K depend on i? you have K=En_i/b.\n\n\n------after rebuttal----\n\nAfter discussions with authors and rebuttal with other reviewers. The authors addressed some of my concerns. I still think that the complexity results given in the first version of the paper, SL and FL have exactly the same complexity bounds by choosing the right learning rates. See my comments and discussions on this. I decided to increase my rating of the paper to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5781/Reviewer_SLb6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5781/Reviewer_SLb6"
        ]
    }
]