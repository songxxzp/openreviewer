[
    {
        "id": "3Y_yYqZApFo",
        "original": null,
        "number": 1,
        "cdate": 1666570027803,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666570027803,
        "tmdate": 1666570161756,
        "tddate": null,
        "forum": "CTX5JcDaUX9",
        "replyto": "CTX5JcDaUX9",
        "invitation": "ICLR.cc/2023/Conference/Paper6057/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the paper, the authors propose P2C, a framework to synthesize the classification and preference learning tasks. The authors argue that\u00a0capturing the preference information from the annotators during the data labeling process would lead to better text classification task performance. They propose two methods to collect annotator's preference information: (1) Implicit preference extraction on existing annotation records; (2) Explicit preference collection of annotators by online query and they formulate preference learning as a supervised learning problem.",
            "strength_and_weaknesses": "Strengths\n1. The authors formulate a preference learning task as a classification task and propose a framework to simultaneously solve text classification tasks and preference learning tasks. These formulations are clean and make the difference between the various setup clear.\n2. Improved performance (with test accuracy) compared to baseline methods.\n3. The author has done a lot of experiments to verify the importance of each module in the P2C framework.\n\nWeaknesses:\n1. The description in parts of the paper needs to be improved. \n- The abstract part mentions \"we propose a new multi-task learning framework\", the introduction part mentions \"we propose a new multi-task learning method\", and the conclusion part mentions \"we propose a novel multi-task learning object\". It is not a reasonable way to introduce P2C in three ways.\n- The information expressed in Eq. 5 is that the preference learning task is a binary classification task, but in the authors\u2019 setting, the preference learning task is a triple classification task. That is, Eq. 5 does not express an equally preferable case (i.e., = 0.5).\n2. For datasets without any remaining annotation records, the proposed framework requires huge human resources (labels for preference learning tasks) to construct labels for preference learning tasks. Is this measure of improving model performance at a huge human resource desirable?",
            "clarity,_quality,_novelty_and_reproducibility": "In terms of clarity, the logic of this article is well organized, but some expressions need to be corrected.\nIn terms of novelty, P2C is not the first method to utilize the annotators\u2019 disagreement from the annotation records.\nIn terms of reproducibility, since some of the labels required for model training (i.e., labels for preference learning task) need to be manually labeled, it requires huge human resources to reproduce this work from zero. Some of the experimental results provided by the author cannot be reproduced if the data provided by the author (if any) is used directly instead of labeling the data, such as in Fig. 5(b).\nIn terms of quality, this paper proposes a novel MTL framework, however, in some cases (i.e., datasets without any remaining annotation records), it needs to take a huge human resource to achieve performance improvement.",
            "summary_of_the_review": "The MTL design to combine main tasks (e.g., text classification task in the paper) and other useful tasks (e.g., preference learning task in the paper) is well motivated. However, reproduction in some cases requires huge human resources.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6057/Reviewer_cBxH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6057/Reviewer_cBxH"
        ]
    },
    {
        "id": "N1fup78Tmmm",
        "original": null,
        "number": 2,
        "cdate": 1666659501704,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659501704,
        "tmdate": 1666660111386,
        "tddate": null,
        "forum": "CTX5JcDaUX9",
        "replyto": "CTX5JcDaUX9",
        "invitation": "ICLR.cc/2023/Conference/Paper6057/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a novel framework that utilizes the annotation disagreements in human-annotated benchmarks and human-annotated preference to improve the model training in scenarios where annotation records are provided. ",
            "strength_and_weaknesses": "Strength:\n  - The idea is simple yet effective.\n  - The paper is well-motivated and well-written.\n  - The empirical results are convincing.\n  - Besides simply using annotation records, the authors also employ crowd workers to provide specific preference annotation. And they show using the preference labels further improves the model performance compared to simply extracting preferences from annotation records, making the logic and motivation sound and self-explained.\n  - The paper illustrates the effectiveness of pair-wise human preference to better guide the model, which may motivate better data annotation in the future.\n\nWeakness:\n  - Since only a few datasets provide annotation record and collecting human annotation is very expensive, the application scenario of this model is very limited at this time.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good.\n\nQuality: Good.\n\nNovelty: Good.\n\nReproducibility: Good.",
            "summary_of_the_review": "This paper proposes a simple yet effective way to utilize human annotation records and incorporate human preference to guide the model training, which may motivate better data annotation and data usage in the future. I would recommend an acceptance for this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6057/Reviewer_TEZ4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6057/Reviewer_TEZ4"
        ]
    },
    {
        "id": "TnSZQT0PvM",
        "original": null,
        "number": 3,
        "cdate": 1667352069576,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667352069576,
        "tmdate": 1667352069576,
        "tddate": null,
        "forum": "CTX5JcDaUX9",
        "replyto": "CTX5JcDaUX9",
        "invitation": "ICLR.cc/2023/Conference/Paper6057/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a multi-task learning framework called  prefer-to-classify (P2C), which is based on pair-wise preference learning. Specifically, their work is focused on handling NLP tasks. The paper explains the overall approach, and the process of selecting informative pairs, and how the preferences are collected from human annotators using Amazon Turk.\nThey have shown experimentally  that their approach is able to outperform on baselines - they have used 7 base lines across 6 datasets. ",
            "strength_and_weaknesses": "Overall, the paper is well written, and the authors have explained their approach and provided experimental results to show their work. \n\nHowever, there are few weaknesses. \nThere are similar approaches in the literature, studied under various titles such as active learning, learning to rank and even preference learning. The approach sounds interesting, but not necessarily novel. The authors have compared their results against 7 approaches, but it is not clear which baseline is the state-of-art approach. Further, referencing approaches from active learning and learning to rank would greatly help demonstrating the effectiveness of the work.\n\nWhile the authors have given examples for the DynaSent-R1 dataset, it is not clear what kind of questions/ preferences were obtained for other datasets. Since they are subjective in nature, it will be good to understand the nature of responses obtained on others as well. On the positive side, the results are encouraging. \n\nFinally, in terms of relevance to the conference: the paper is more of an active learning or preference learning paper, and there is less focus on representations themselves. Though the authors have used a model build over RoBERTa-base, their approach could probably work on one-hot encoded vectors too. ",
            "clarity,_quality,_novelty_and_reproducibility": "As stated earlier, the paper is easy to follow and the authors have explained the problem setting, their solution and provided experimental results. \nIn terms of novelty, it is hard to judge whether the framework itself is novel since several preference learning that perform pair-wise comparisons exist. \nThe authors have provided details about implementing the approach and for using the datasets - which help with reproducibility. However, it is not clear if the questions they asked and the responses they obtained will be identical if another person attempted to reproduce the results. ",
            "summary_of_the_review": "The authors present a multi-task learning framework called  prefer-to-classify (P2C), for handling NLP tasks. They have shown experimentally  that their approach is able to outperform on baselines - they have used 7 base lines across 6 datasets. \nThere are some drawbacks in terms of comparison baselines as discussed in the weakness section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6057/Reviewer_M4mz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6057/Reviewer_M4mz"
        ]
    }
]