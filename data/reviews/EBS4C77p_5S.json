[
    {
        "id": "n2OLgJ7g5c",
        "original": null,
        "number": 1,
        "cdate": 1666483753220,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666483753220,
        "tmdate": 1666484036238,
        "tddate": null,
        "forum": "EBS4C77p_5S",
        "replyto": "EBS4C77p_5S",
        "invitation": "ICLR.cc/2023/Conference/Paper4063/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on how to solve four strongly related tasks in sign language translation (SLT) with a unified framework. They propose SLTUNET, a simple unified neural model designed to support multiple SLT-related tasks jointly. By discriminating between inputs from different modalities, different tasks can be trained on the same network and thus narrowing the gap between modalities. In addition, the paper also introduces machine translation tasks on this basis to solve the problem of small scale of sign language translation datasets. The model is tested on two popular SLT datasets, and it can achieve good experimental results without using pre-trained large models, proving its effectiveness.",
            "strength_and_weaknesses": "Strength\uff1a\n1. The paper combines the related tasks of sign language translation in a clever way. It is only necessary to perform different processing on different modalities in the input layer, and then a unified network can be used for subsequent training. The method is simple and effective and improves performance without significantly increasing model parameters.\n2. This paper has done a lot of ablation experiments, and the experimental details shown in Table 2 can enable readers to understand the improvement brought by each method and enhance the reproducibility of the method.\n\nWeakness:\n1. There is a little description of the method in this paper. There is only an illustration chart, and no details of the model are shown. It is not stated how the different modal inputs are fused at each iteration of the model.\n2. The experimental part of the paper does not highlight its method. The main innovation of this paper is to use a unified framework for related tasks of sign language translation and expect that multi-task training can improve the performance of the model. However, there is only a small amount of content about multi-task training in the experimental part of the paper.\n3. The performance metrics in the paper are confusing. The article discusses the common performance metrics of machine translation and uses two different metrics, tokenized Bleu (B@4) and detokenized Bleu (sBLEU). However, the values of these two indicators in the experiment of the paper are exactly the same, which is Unreasonable unless a unified Tokenizer is used, namely tokenize13a. This needs to be explained in more detail in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is somewhat confusing in the description of the method and the description of the performance metrics.\nQuality: Overall quality is average. The article proposes a simple framework with certain innovations. But there is no focus on key issues in the experimental part, and there are some presentation issues. \nNovelty: The revised paper has better innovation. The unified framework proposed in this paper for sign language translation learning is relatively novel, and for the first time, machine translation data is added to assist with sign language training.\nReproducibility: The paper has high reproducibility. The method of the paper is concise, and there are detailed experimental details.",
            "summary_of_the_review": "The paper has good innovation, has done some original work, and the proposed method is concise and has good reproducibility. But the clarity of the paper is not high, especially regarding the index part. This is related to the specific performance and the effectiveness of the method and must be explained in detail. Until then, I was skeptical about the paper. If you can well answer the above questions, I am apt to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4063/Reviewer_LzzH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4063/Reviewer_LzzH"
        ]
    },
    {
        "id": "ZhJ47FIQch",
        "original": null,
        "number": 2,
        "cdate": 1666513870449,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513870449,
        "tmdate": 1666514026038,
        "tddate": null,
        "forum": "EBS4C77p_5S",
        "replyto": "EBS4C77p_5S",
        "invitation": "ICLR.cc/2023/Conference/Paper4063/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a unified sign language translation framework that translates sign videos to natural texts. To better learn visual and text representations, the authors propose to use multi-task objectives to borrow knowledge from external datasets, like machine translation datasets. It uses a shared encoder and a shared decoder to handle gloss-to-text translation, text-to-text translation, sign-to-gloss translation, and sign-to-text translation. In addition to multi-task objectives, the authors also conduct a detailed ablation study to see how different experiment settings contribute to the final performance, like BPE dropout. Experiments show that the final architecture achieves comparable results with state-of-the-art baselines with large pre-trained networks as initialization. ",
            "strength_and_weaknesses": "Strength:\n\n1. The performance is promising. The final model does not rely on any pre-trained networks but achieves comparable performance with models with pre-trained networks as initialization. \n\n2. The multi-task objective is intuitive. Generally speaking, the size of datasets for sign language-to-text translation is limited compared with text-to-text translation datasets. Borrowing knowledge from other tasks and external task is a practical solution. \n\n3. The authors also conduct a comprehensive ablation study to see the effects of model settings to the final performance. \n\n\nWeaknesses:\n\n1. Table 2 shows that the multi-task objective improves results from 24 to 25.23, but the settings of other architecture details increase performance from  25.23 to 27.87.  Although experiment highlights are state-of-the-art performance, the contribution of the proposed multi-task objective is limited.  \n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n\nEq.2  and Eq3. can be written in a better format. For example, L(X, Y |tag) generally refers to modeling joint probability (X,Y) given tag. According to the equation description, this equation is modeling the conditional probability of Y given X and tag. The authors can re-formulate Eq.2 and Eq.3 to a conditional probability format (e.g, L(Y|X, tag)) for better clarification. ",
            "summary_of_the_review": "The papers show promising results with only sign-to-text data and limited external data. The paper is well-written and easy to follow. The idea is simple and intuitive. In addition, the detailed ablation study shows how the specific settings contribute to the final performance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4063/Reviewer_x36u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4063/Reviewer_x36u"
        ]
    },
    {
        "id": "EIIqncX6I0I",
        "original": null,
        "number": 3,
        "cdate": 1666714275514,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714275514,
        "tmdate": 1666714275514,
        "tddate": null,
        "forum": "EBS4C77p_5S",
        "replyto": "EBS4C77p_5S",
        "invitation": "ICLR.cc/2023/Conference/Paper4063/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This article proposed a unified model for sign language translation, in which multiple tasks are simulated jointly for enhancing the performance of sign language translation. ",
            "strength_and_weaknesses": "Strengths:\nThe proposed approach allows multiple tasks (e.g., such as sign-to-gloss, gloss-to-text, and sign-to-text translation) related to sign language translation to be simulated jointly for enhancing the performance of sign language translation. \n\nWeaknesses:\n\n1. The existing experiments were insufficient to verify how multiple tasks interact to improve the performance of sign language translation.\n\n2. In End-to-End: Sign2Text in Table 4, the proposed approach just was comparable to the baseline VL-Transfer model in terms of ROUGE and B@4 on the Test set of PHOENIX-2014T.\n\n3. One of the claimed contributions is that the set of optimization technologies for SLTUNET was proposed to improve the trade-off between model capacity and regularization. It was confusing.\n\nQuestions:\n\n1. When there is an imbalance between video and gloss input, what is the result?\n\n2. Author claimed that the DGS Corpus includes larger\nscale, richer topics, and more significant challenges than existing datasets. How to evaluate them?\n\n3. When the proposed approach was applied to multiple tasks, how about the learning curving of CTC and MLE losses?\n\n4. If there replaced the shared encoder or decoder with a separate encoder or decoder, what about the performance?\n\n5. In Figure 1 (1), What do the different colors of these modules denote? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is basically clear, except for the set of optimization technologies. Overall, this current submission just focuses on demonstrating performance on standard datasets and lacks in-depth experimental analysis of how multiple tasks interact with each other.\n",
            "summary_of_the_review": "Please see review in the previous sections.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4063/Reviewer_FxDU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4063/Reviewer_FxDU"
        ]
    },
    {
        "id": "fZMdZzUhqo",
        "original": null,
        "number": 4,
        "cdate": 1666745561759,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666745561759,
        "tmdate": 1666745561759,
        "tddate": null,
        "forum": "EBS4C77p_5S",
        "replyto": "EBS4C77p_5S",
        "invitation": "ICLR.cc/2023/Conference/Paper4063/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles a challenging problem of translating sign language to natural language. The problem's challenging nature arises from the data scarcity and the modality gap between video and text. The authors approach the problem from the perspective of learning a joint latent space for video and text, guided by multiple loss functions including CTC and MLE for both gloss and text. These approaches are verified by a thorough examination of ablation studies of model capacity and regularization over challenging datasets PHOENIX-2014T and CSL-Daily. The authors also propose an end-to-end translation protocol for SLT by using an even more challenging dataset of the Public DGS corpus that covers broader domains and more open vocabularies. ",
            "strength_and_weaknesses": "Strengths\n- The proposed model is clearly defined and the ablation studies are carefully structured in Table 2. The detailed discussion of these results is also appreciated.\n\nWeaknesses\n- Design the protocol for Public DGS for an end-to-end SLT setup: It seems that the protocol involved splitting the dataset into train, test, and dev sets. Was there more to that process? Furthermore, it would also be useful to include the details of how these splits were conducted. Do speakers overlap among the splits? If yes, does the speaker overlap matter? How does the speaker's identity affect sign language translation? What is the distribution of text tokens and gloss vocabulary across the splits?\n- The back translation losses are not useful for SLT (i.e. impact of Text2Gloss in Section 5.2.2), but Zhou et. al. 2021 showed that it was quite useful. That seems contradictory and some more discussion on this would be useful for the readers.\n- The choice of visual backbone could impact the final performance. Why was the SMKD model chosen? Chen et. al. 2022 also uses a visual backbone model which is S3D (i.e. different from the one used in this paper). Chen et. al. 2022 also use CTC losses for training the model. As their model have some level of similarity to the proposed model, it would be useful to explicitly talk about the differences.\n    - Especially, as Chen et. al. 2022 is also similar to the proposed model in terms of performance, it would have been insightful to see its performance on the DGS3-T baseline in Table 6.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is generally clear. Especially, the ablation studies are quite clear due to the numbering convention.\n- Novelty of the paper relies on the exhaustive study of SLT with multiple loss functions, model depth/width, and regularizations. From the point of view of experiments, this paper does provide insights into different components of an SLT model\n- In the current state, it looks mostly reproducible. That being said, the inclusion of a codebase would be useful.",
            "summary_of_the_review": "The detailed study of the model design choices are appreciated and could be useful for the SLT community. Although, some empirical comparisons on the new protocol of the DGS3-T dataset are missing which are crucial for understanding the complete utility of the proposed model.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4063/Reviewer_JikT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4063/Reviewer_JikT"
        ]
    }
]