[
    {
        "id": "DLQA-AiKFN",
        "original": null,
        "number": 1,
        "cdate": 1666596867620,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666596867620,
        "tmdate": 1666596867620,
        "tddate": null,
        "forum": "PINRbk7h01",
        "replyto": "PINRbk7h01",
        "invitation": "ICLR.cc/2023/Conference/Paper3229/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the convergence of gradient descent on overparameterized neural networks with smooth activation functions. The authors show that the Hessian of the neural network has a small spectral norm within a special ball of the initialization, and this ball is allowed to be significantly larger than those allowed by prior works, such as those that use the NTK framework. \n\nThe authors then show that the square loss is strongly convex within a restricted set around the initialization (under certain assumptions). This along with the spectral norm bound (which implies smoothness for the loss function) gives a geometric convergence rate for gradient descent.",
            "strength_and_weaknesses": "_Strengths_:\n- This paper considerably improves the results in Liu et. al. with polynomial factors wherever the prior work had exponential factors.\n\n- The use of RSC matches empirical observations that the Hessian of neural networks have many flat directions with eigenvalues $=0$.\n\n- The gradient descent iterates are allowed to be in a much larger ball when compared to prior work.\n\n_Weaknesses_:\n- The flow of the paper is a little confusing. The authors first show an upper bound on the spectral norm of the Hessian (in an appropriate ball around the initialization), and then in the second paragraph of page 2, say that this will be used to show Restricted Strong Convexity. This doesn't make much sense when phrased this way, since RSC would require a lower bound on the smallest eigenvalue of the Hessian within the restricted set. My understanding of the paper is that the Hessian spectral bound has nothing to do with RSC, and is instead used for showing smoothness of the loss function, as in Theorem 5.2. \n\n- In Theorem 4.1, the role of $\\sigma_1$ is not clear. What stops you from choosing an extremely small $\\sigma_1$? It seems like this would give a good value for $\\gamma$. Assumption 2 requires it to bounded away from 0, but its significance in Theorem 4.1 is unclear.\n\n- I'm a little concerned about the connections between the Lipschitz properties of the network (via the Lipschitz activations in Assumption 1) and the Restricted Strong Convexity properties. These two are conflicting properties, and the choice of $\\rho, \\rho_1$ would dictate whether the Lipschitz bound in Assumption 1 can be satisfied. Any clarification on this point would be appreciated.\n\n- The geometric convergence in Theorem 5.3 should also come with the disclaimer that the loss cannot be made arbitrarily small. In order for A4.1 to hold, the average gradient needs to be sufficiently large, which implies that the algorithm cannot be arbitrarily close to the minimum (in which case the gradient would be arbitrarily close to zero). THis would also contradict the claim in Remark 5.4, where the authors say that there exist parameters that can drive the loss to an arbitrary $\\epsilon$.\n\n- Assumption A4.1 should appear before Theorem 5.1. Currently it is buried in the last sentence of Theorem 5.1.\n\n_Clarifications_:\n\n- This is not a complaint, but perhaps the choice of $\\sigma_0$ can be compared with common initialization schemes, such as, Xavier initialization.\n\n- The line below Assumption 3 says that cross-entropy is strongly convex. Is this true? (the loss in logistic regression for e.g. is certainly not strongly convex) If your results in Theorem 5.1 are for the square loss, why mention cross-entropy at all?\n\n_Minor comments_:\n- Para above lemma 4.1; \"to established\" -> \"to establish\"",
            "clarity,_quality,_novelty_and_reproducibility": "_Clarity_:\nThe paper has some issues with clarity (as pointed out in the weaknesses section above), but overall I found it clear enough.\n\n_Originality_:\nThis paper uses a considerably finer analysis than Liu et al, and obtain interesting results that are strictly better than the prior work. The work also establishes poly-time guarantees without the NTK assumption.\n\n",
            "summary_of_the_review": "This result gives a poly-time (almost) geometric convergence for gradient descent on deep neural nets with smooth activations, and does not rely on the NTK framework.\n\nThe reasons for my score are:\n1. the GD iterates are allowed to lie in a significantly larger set than prior work.\n2. the paper has the added benefit that it closely resembles empirical observations -- it is well known that Hessians of neural nets have many directions in which the eigenvalues are zero, and the use of RSC implies that these directions can be accounted for.\n3. the analysis is much sharper than prior work in Liu et al, and obtains poly factors wherever prior work obtains exponential factors.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3229/Reviewer_GhrZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3229/Reviewer_GhrZ"
        ]
    },
    {
        "id": "_THLzeuUjJw",
        "original": null,
        "number": 2,
        "cdate": 1666641018210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641018210,
        "tmdate": 1666641018210,
        "tddate": null,
        "forum": "PINRbk7h01",
        "replyto": "PINRbk7h01",
        "invitation": "ICLR.cc/2023/Conference/Paper3229/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper provides another sufficient condition besides NTK to ensure geometric convergence of deep learning.",
            "strength_and_weaknesses": "The paper provides a good litterature review. It is well written and easy to follow. However, it is a dense paper with lots of theoretical proof to check. The ICLR review period is too short to allow a careful theoretical review. It would be better as a journal paper.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "It is clear and has good writting quality.",
            "summary_of_the_review": "This is seemingly a good paper, but too heavy to review for a ML conference with so short review periods. Hence, you should not consider my recommendation as meaningful.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3229/Reviewer_uzLy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3229/Reviewer_uzLy"
        ]
    },
    {
        "id": "1qI2B6ZqAG-",
        "original": null,
        "number": 3,
        "cdate": 1666678255378,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678255378,
        "tmdate": 1666678255378,
        "tddate": null,
        "forum": "PINRbk7h01",
        "replyto": "PINRbk7h01",
        "invitation": "ICLR.cc/2023/Conference/Paper3229/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper makes two contributions for single-output deep learning models with smooth activations:\n1) Upper bounds the spectral norm of the model's Hessian over a layer-wise spectral norm, which is larger than the euclidean ball that was studied in previous work. Hence, the result holds with relaxed conditions on how close it is to initialization.\n2) proves a restricted strong convexity property along the path of GD iterates that avoid directions orthogonal to the average gradient and that stay close to initialization with respect to the layer-wise spectral norm.\n\n",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is generally well-written. The authors make particular effort to make the presentation less 'dry' with several remarks on intuitions and interpretations of their results\n- The contributions are made clear and it is clarified what the novelties are compared to previous works\n- Thm 1 improves on a previous result by (Liu et al.)\n- Thm 5.2 provides an alternative a criterion for linear convergence that is different compared to the now common uniform minimum eigenvalue bound of the NTK\n\nWeaknesses: \n\n- The two contributions of the paper are not well connected. Have I missed something on this? Perhaps the authors can clarify how the content of Sec. 4 informs that of Sec. 5\n- How is Lemma 5.1 different from what is already shown in Karimi et al. that RSC=>RPL other than here you are talking about restricted PL?\n- The result on RSC requires (essentially) two assumptions. \n(a)The first is that GD travels a path in which iterates are not orthogonal to the average gradient. While it is mentioned, that this is something observed in practice, no further evidence is given. Also, it would be useful to add some intuition on why this is needed. \n(b) The second needed condition is that the squared norm of the average gradient is greater than c/\\sqrt{m}, where m is the network's width. Remark 5.5 is nice, but it would strengthen the argument if the authors could show an example of concrete architecture where (20) holds but (19) does not.\n- In Remark 5.6 they authors mention: \"our perspective is to view the NTK and RSC as two different sufficient conditions ...\". This remark is somewhat contradictory to the flavor of comment in the rest of the paper, e.g. Remark 5.5 also earlier that the layer-wise spectral ball does not require to be in NTK regime\n- In the experiments: what does it mean, step-size is chosen appropriately to keep training in NTK regimes?\n- minor: above Thm 1, cross-entropy is not strongly convex\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is generally well-written. The authors make particular effort to make the presentation less 'dry' with several remarks on intuitions and interpretations of their results\n- The contributions are made clear and it is clarified what the novelties are compared to previous works",
            "summary_of_the_review": "Disclaimer: I am not super familiar with the details of some of the closely related works, but the authors appear to have done a good job in comparing their results to them and clarifying the novelties in a way that is convincing. Also, given the reviewing load, I was unable to go through the proofs. Because of these hesitations, I recommend 'marginal accept' for now, but I am willing to improve my score further down in the discussion period.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3229/Reviewer_oVua"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3229/Reviewer_oVua"
        ]
    },
    {
        "id": "BHunqILRWE",
        "original": null,
        "number": 4,
        "cdate": 1667500975038,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667500975038,
        "tmdate": 1667500975038,
        "tddate": null,
        "forum": "PINRbk7h01",
        "replyto": "PINRbk7h01",
        "invitation": "ICLR.cc/2023/Conference/Paper3229/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the paper, the authors analyze the convergence of gradient descent (GD) on training a deep learning model with smooth activation function. The paper is presented for fully connected neural network with linear last layer, output dimension of 1, and equal layer width for all layers. Under reasonable assumptions on the loss function, activation function, randomness of the initialization, etc., the authors show that GD will converge (geometrically) to the minimizer in the neighborhood around the initialization. The analysis establishes an upper bound on the Hessian of the loss function in the neighborhood and then using Restricted Strong Convexity of the loss function to argue for convergence to the minimizer. The approach for showing convergence differs from NTK approaches which are generally restricted to wide networks.\n",
            "strength_and_weaknesses": "Overall the paper provides a new approach to analyze the behavior of gradient descent when training deep learning models. Below I highlight my concerns and include some minor comments.\n\n**Weakness**\n\n-  The analysis presented is for NN with same width for all layers and with the output dimension of 1. The authors point to another paper for extension to multidimensional output. It would be beneficial to provide a remark, at the least, on the central steps required for extending the result. \n-  Another restriction to the network is the activation function. Does the analysis extend to networks with smooth activation in last layer? Does it extend to non-smooth activation function?\n- In my understanding, the result presented is for convergence to a neighborhood near the initialization. The main point made in the paper is that the networks does not have to be as wide for convergence to hold, unlike for NTK approaches. A comparison between the two methods on the required width size is missing. \n- The experimental result presented is very limited. Following the results presented in the paper, the average gradient satisfies $\\bar{g}_t = \\Omega(\\frac{L}{\\sqrt{m}})$. However, figure 1b shows that the norm of the average gradient grows with network width. What accounts for this difference in theoretical vs experimental results.\n- In the abstract, the authors state \"...norm of the Hessian of such models...\". This should be reworded to indicate that Hessian is of the loss function.\n- In page 2, the authors state the set $Q_k^t$ without an explanation of what the set is (or referencing to where it is defined).\n- In assumption 2, the initialization for $v_0$ is stated in a roundabout way. I believe it is simply sampled uniformly at random from a unit sphere.\n- In Theorem 4.1, the bound on the maximum of the Hessian should either depend on $n$ or the bound holds for all $x_i$. This should be made clear.\n- In Definition 5.2, missing inner product in $\\cos(\\theta -\\theta_t, \\bar{g}_t)$.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\nThe paper is well written and the main paper provides sufficient clarity on the main proof ideas. However some terms are used without properly introducing them.\n\n\n**Novelty and quality**\nThe paper provides (and according to the authors, they are the first to do so) an convergence analysis that uses the Restricted Strong Convexity argument in the context of deep learning models. However the scope of the paper is limited to particular neural networks (see above comment) and not enough comparison is provided between their result and NTK. \n",
            "summary_of_the_review": "The stated problem is interesting and the authors provide an insight on the behavior of gradient descent for neural networks without large width. Overall, the paper is easy to read but contains some unclear parts.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3229/Reviewer_Qo6j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3229/Reviewer_Qo6j"
        ]
    }
]