[
    {
        "id": "0hr89HvSPU",
        "original": null,
        "number": 1,
        "cdate": 1666244355172,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666244355172,
        "tmdate": 1666244355172,
        "tddate": null,
        "forum": "Js4vqB4bWVh",
        "replyto": "Js4vqB4bWVh",
        "invitation": "ICLR.cc/2023/Conference/Paper1805/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper formulates a problem of decentralized online bandit optimization on directed graphs, in which the graph structure dictates the order of the players and how players observe the actions of other players. Moreover, all players can observe a joint bandit reward, which is a linear combination of the reward of each clique in the graph. To solve this problem, the authors make an extension of an existing single-player multi-armed bandits algorithm and established upper bounds for the joint pseudo-regret.",
            "strength_and_weaknesses": "1) The proposed problem of decentralized online bandit optimization on directed graphs seems to be interesting.\n2) The proposed algorithm is equipped with upper bounds on the joint pseudo-regret and its performance is partially verified by experimental results.\n\n#Weaknesses\n1) According to Algorithm 1, each player can observe the joint bandit reward, which may be unrealistic in the decentralized setting. \n2) As in Eq. (1), the authors consider a joint bandit reward, in which each clique reward has a weight. However, according to Theorem 3, the upper bound of the proposed algorithm actually does not depend on the weight. There seem to be two possible explanations. First, weight is not important. Second, the proposed algorithm cannot reflect the importance of weight. \n3) According to the analysis of this paper, it seems that it is not difficult to extend the algorithm and theory of Zimmert & Seldin (2021) into the problem in this paper, which limits the novelty of this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper should be improved, as explained below.\n1) The authors propose a new bandit setting, but do not clearly compare it against related existing bandit settings such as bandits with graph-structured feedback (Cesa-Bianchi et al., 2021). It is hard to justify what additional challenges the new setting brings.\n2) In the Introduction of this paper, the concept of equilibrium has been mentioned many times. However, this paper only studies the regret bounds and does not study the equilibrium.\n3) It seems that Step 8 of Algorithm 1 is implemented for all $j$, each of which will generate a strategy $\\pi_i(t)$. It is not clear $\\pi_i(t)$ generated by which $j$ is used by the player $i$ in Step 9 of Algorithm 1.\n4) In Step 11 of Algorithm 1, the authors update the cumulative loss $L_{j,k}$. However, according to this update rule, it seems that $L_{j,k}$ for different $j$ have the same value. It is not clear why this is sufficient for running different bandit algorithms for all $j$.\n\nBecause of the writing problems in Algorithm 1, it is hard to strictly follow the proofs in this paper. For the experimental results, since the authors provide the source code, I believe it is reproducible.\n\nMoreover, the novelty of this paper is limited because it seems that it is not difficult to extend the algorithm and theory of Zimmert & Seldin (2021) to the problem in this paper. \n\nOverall, the quality of this paper may be below the bar of ICLR.\n",
            "summary_of_the_review": "The main contributions of this paper include a new bandit setting and a corresponding algorithm with pseudo-regret bounds. However, the writing of this paper should be improved, and the novelty of this paper seems to be limited. So, I tend to reject this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1805/Reviewer_fbii"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1805/Reviewer_fbii"
        ]
    },
    {
        "id": "KE-Fm_Jdwnx",
        "original": null,
        "number": 2,
        "cdate": 1666611239332,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611239332,
        "tmdate": 1666611456982,
        "tddate": null,
        "forum": "Js4vqB4bWVh",
        "replyto": "Js4vqB4bWVh",
        "invitation": "ICLR.cc/2023/Conference/Paper1805/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies decentralized bandits on a DAG. In this model, the players perform actions consecutively and observe the actions of the preceding players. The paper proposes a novel algorithm for this model and achieves sublinear regret for both stochastic and adversarial rewards.\n",
            "strength_and_weaknesses": "Strength:\n\nThe studied model is novel. The proposed algorithm achieves sublinear regrets.\n\nWeaknesses:\n\n1. The motivation of the studied model is not clear. Why do players have to execute actions consecutively? Why does the model consider the linear combination of the clique-rewards rather than all players' rewards? I suggest authors provide some real applications for the proposed model.\n2. I'm confused about the definition of the independent clique in the DAG. I do not find the corresponding definition in (Koller & Friedman 2009, Ch.2). To increase the paper's readability, I suggest the authors provide the independent clique's definition.\n3. I'm also confused about inequality (5). Notice that Theorem 1 requires the player to play T consecutive rounds. However, the rounds in the set $T_a$ are not consecutive. Thus (5) cannot be obtained by Theorem 1 directly. Am I right?\n4. Tsallis-INF can achieve log(T) regret for stochastic rewards, but the proposed methods do not achieve log(T) regret in the stochastic setting.\n5. The technical contribution is not strong enough. Most analyses are straightforward and the results are not surprising.\n6. The authors only perform experiments on the graph with one clique. It would be better if the authors could provide numerical results on various connection graphs.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing should be improved as mentioned in the weaknesses. Both technical and empirical novelties are limited. Overall, the quality of this paper is a bit below the bar.",
            "summary_of_the_review": "Though the studied model is novel, many details should be clarified (see weaknesses). Also, both theoretical and empirical contributions are a bit below the bar.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1805/Reviewer_8PKf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1805/Reviewer_8PKf"
        ]
    },
    {
        "id": "CwgG0t-NO5",
        "original": null,
        "number": 3,
        "cdate": 1666672872921,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672872921,
        "tmdate": 1666672872921,
        "tddate": null,
        "forum": "Js4vqB4bWVh",
        "replyto": "Js4vqB4bWVh",
        "invitation": "ICLR.cc/2023/Conference/Paper1805/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the regret bound of multi-player online bandit problems with a leader-follower (directed graph) hierarchical structure. To be specific, there are n players in total and a directed graph G, and for any two players with j having an out-edge to i, player i will observe the action of player j and take actions after player j. The reward is taken according to the joint actions taken all the players and can be either stochastic or adversarial. \nWith the above problem, the authors propose an algorithm, and proved the upper bound for single player and multi-players. For the single-player case, the regret upper bound is O(\\sqrt{AT}) where A is the number of actions of this player and T is the time horizon, which I believe is optimal (up to a constant factor) as the regret bound's order is the same as the standard MAB problem. For multiple-player cases (n > 1), the regret is of order O(\\sqrt{T\\sum{A_i}}) where A_i is the action space (number of joint actions) of the i-th clique. \nFinally the authors did some numerical simulations for the algorithm, and the results look consistent with the theory.",
            "strength_and_weaknesses": "Strengths: The graph-structure bandit problem studied in this paper is interesting and meaningful, and I am convinced that it is a missing part of the literature and can found potential applications in the real-world systems. This paper gives an algorithm with reasonable regret upper bounds, which I think is optimal or near optimal (ignoring constant factors). \n\nWeakness: It is not sure whether the algorithm proposed in this paper is close to the lower bound. It will be much better if there can be analysis on the lower bound. Also, the numerical simulations are of small scales, but I think many real-world scenarios (such as communication networks) should have much larger scales. It will strengthen the paper a lot if there can be more large-scaled simulations.  \n\nMinor: Also wonder if the algorithm proposed in this paper can work only using local information. The step observing the reward seems to be costly in terms of communication in a large scale. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is clear and easy to follow. It uses well denoted mathematics with good clarity.\nQuality: This paper's results are trustworthy\nNovelty: This paper studies a novel problem and proposes good solutions to the problem and the solutions are not that straightforward. \nReproducibility: I think the results (both theoretical and numerical) in this paper are reproducible. ",
            "summary_of_the_review": "This paper studies an interesting and novel problem, and proposes an algorithm with good regret performance. The algorithm looks not trivial and the regret results are well supported by mathematical proofs. So I tend to vote an accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1805/Reviewer_ibWi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1805/Reviewer_ibWi"
        ]
    },
    {
        "id": "p0uq7d6A8Wd",
        "original": null,
        "number": 4,
        "cdate": 1666725047589,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725047589,
        "tmdate": 1666725047589,
        "tddate": null,
        "forum": "Js4vqB4bWVh",
        "replyto": "Js4vqB4bWVh",
        "invitation": "ICLR.cc/2023/Conference/Paper1805/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers a decentralized multiplayer game with a leader-follower hierarchy described by a directed acyclic graph. It proposes a cooperative learning algorithm and proves that the algorithm achieves sub-linear joint pseudo-regret for both adversarial and stochastic bandit rewards. ",
            "strength_and_weaknesses": "+ The paper considers a novel decentralized online bandit optimization setting, motivated by cooperative Stackelberg game. \n+ The paper is well written. The results are novel and interesting.\n\n-  The possible applications for the proposed setting is a bit vague. \n-  Although a few theorems are presented, Theorem 4 is the most general one which covers the previous ones.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the idea is novel. ",
            "summary_of_the_review": "The paper considers a decentralized multiplayer game with a leader-follower hierarchy described by a directed acyclic graph. It proposes a cooperative learning algorithm and proves that the algorithm achieves sub-linear joint pseudo-regret for both adversarial and stochastic bandit rewards. The paper is well written. The results are novel and interesting. The possible applications for the proposed setting is a bit vague. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1805/Reviewer_uQZP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1805/Reviewer_uQZP"
        ]
    }
]