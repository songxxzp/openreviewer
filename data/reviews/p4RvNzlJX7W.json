[
    {
        "id": "O5QBnNXpNk",
        "original": null,
        "number": 1,
        "cdate": 1666380750850,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666380750850,
        "tmdate": 1666380750850,
        "tddate": null,
        "forum": "p4RvNzlJX7W",
        "replyto": "p4RvNzlJX7W",
        "invitation": "ICLR.cc/2023/Conference/Paper4227/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents the convergence analysis for SGD and averaged SGD with fixed step size, showing that iterates converge to the vicinity of the optimizer of a smoothed loss function. Compared to prior work, the author improved existing results with a less strict assumption on the step size, and provide a convergence result for the averaged SGD, showing a better rate over vanilla SGD.",
            "strength_and_weaknesses": "Strength: This paper is well written, and it is easy to follow. The theoretical results are novel and the discussions/remarks are very helpful. While the results seem incremental over some existing work on the implicit bias of SGD, there is still a significant contribution, showing an improved rate using averaged SGD.\n\nWeakness: The author mentioned before Theorem 1, \"different proof techniques\" than (Kleinber et al.), maybe you can elaborate?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and novel.",
            "summary_of_the_review": "Good paper studies a very relevant topic in the theory of machine learning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4227/Reviewer_vTYE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4227/Reviewer_vTYE"
        ]
    },
    {
        "id": "UZaD9T8lO3",
        "original": null,
        "number": 2,
        "cdate": 1666424241460,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666424241460,
        "tmdate": 1666533699532,
        "tddate": null,
        "forum": "p4RvNzlJX7W",
        "replyto": "p4RvNzlJX7W",
        "invitation": "ICLR.cc/2023/Conference/Paper4227/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The manuscript studies the optimization and generalization trade-off caused by stochastic noise, step size, implicit bias, and sharpness of the minimum; and more importantly, why an averaged SGD with a large step size can improve the trade-offs and converge to a flat region more stably than SGD.",
            "strength_and_weaknesses": "## Strength\n* The paper is well-structured and well-written (in high quality), with a clear related work section. The reviewer was enjoying reading most part of the manuscript.\n* The studied question is of great importance to the optimization community.\n* Extensive numerical results are included in the manuscript in terms of various neural architectures and learning rate schedules; besides, some visualization results are very insightful. \n\n## Weaknesses\n1. The manuscript argues that SGD using the step size $\\eta$ converges to a distance $O(\\sqrt{\\eta})$ from the solution, whereas the averaged SGD using the same step size converges to a distance to $O(\\eta)$. First of all, the reviewer is unclear on how to conclude the $O(\\sqrt{\\eta})$ and $O(\\eta)$ from Theorem 1 and Theorem 2; and authors are required to elaborate on them. More importantly, compared to Theorem 1, Theorem 2 relies on the additional Assumption 3, and thus could derive a tighter rate; however, it does not make much sense to compare two results under different assumptions, and even highlight it as a key contribution.\n2. If the reviewer understands the manuscript correctly, the current convergence theory in the submission cannot capture the training dynamic caused by using a larger step size as the averaged SGD only runs normal SGD and takes the average in the end. Understanding this would be a key step in interpreting the implicit bias in SGD, however, the current draft cannot fill this gap.\n3. The choice of hyper-parameters used in the manuscript seems a bit random and no justification can be found. For example, weight decay with a coefficient of 0.05 was used but the reviewer believes many SOTA training recipes use 1e-4 or 5e-4; Similar comment can be applied to the choice of decay factor as well as the milestones for three cases.",
            "clarity,_quality,_novelty_and_reproducibility": "check comments above",
            "summary_of_the_review": "In general, the manuscript is well-written, and the main concerns of the reviewer are\n* unfair comparison of theoretical/numerical results\n* unsupported experimental setups.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4227/Reviewer_fQ6K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4227/Reviewer_fQ6K"
        ]
    },
    {
        "id": "VsE8OYFLwO",
        "original": null,
        "number": 3,
        "cdate": 1666821406115,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666821406115,
        "tmdate": 1670812728617,
        "tddate": null,
        "forum": "p4RvNzlJX7W",
        "replyto": "p4RvNzlJX7W",
        "invitation": "ICLR.cc/2023/Conference/Paper4227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to understand how averaging the parameters of SGD (averaged SGD) leads to a more stable convergence to flat regions. \nThe authors take the alternative view of SGD from Kleinberg et al. (2018), which tracks $v = w - \\eta \\nabla f(v)$ instead of $w$ itself. Then SGD wrt $w$ can be viewed as a stochastic semi-gradient method wrt $v$ on a smoothed loss, where the smoothening factor depends on eta. Under one-point convexity at the minimizer of the smoothed loss (and also various regularity conditions), they show that both SGD and averaged SGD oscillate around the minimizer, but the distance to the minimizer from the final parameter of averaged SGD is smaller than that from SGD ($O(\\eta)$ v.s. $O(\\sqrt{\\eta})$).",
            "strength_and_weaknesses": "### Strength\n\n1. The phenomenon that averaged SGD leads to a more stable convergence to flat regions is interesting to study, and may deepen the current understanding of the training dynamics in the late training phase.\n2. Clear convergence results for both SGD and averaged SGD.\n3. The theoretical result indeed provides a clear separation between SGD and averaged SGD in terms of the distance upper bounds to the minimizer of the smoothed loss.\n\n\n### Weakness\n\n**Major Concern.** The major concern I have is whether the current result indeed shows that larger LR leads to a flatter minimizer. If not, then people may want to use SGD with a smaller LR so that the distance to the minimizer is smaller, and then SGD may beat averaged SGD. Basically, the authors should check whether the minimizer $v_*(\\eta)$ as a function of $\\eta$ can move a lot when $\\eta$ goes from a big value $\\eta$ to a small value $\\eta'$.\n\nHowever, the distance between $v_*(\\eta)$ and $v_*(\\eta')$ should be bounded by $O(\\eta^2)$. The reason is that we can apply the implicit function theorem for solving the equation $\\nabla F(v) = 0$, which gives us a curve $v_*(\\eta)$ wrt $\\eta$. Note that the derivative wrt $\\eta$ is $-(\\nabla^2 F(v_*))^{-1} \\left(\\frac{\\eta^2}{2} \\nabla \\mathrm{Tr}(\\nabla^2 f(v_*) \\mathbb{E}[\\epsilon' \\epsilon'^\\top]) + O(\\eta^3)\\right)$. According to A4 and A5, this should be of order $O(\\eta^2)$.\n\nNote that the theorem for SGD shows a distance bound $O(\\eta)$. So for SGD with a much smaller LR $\\eta'$, the distance from the final parameter to the minimizer of the smoothed loss defined with $\\eta$ would be $O(\\eta^2) + O(\\sqrt{\\eta'}) = O(\\eta^2)$. But for averaged SGD with LR $\\eta$, it is $O(\\eta)$, which is actually much higher.\n\nThe above argument would suggest that averaged SGD with large LR does not encourage flatness at all.\n\n**Minor concerns.**\n1. I understand that one-point convexity makes the analysis possible, but the authors should give some concrete examples of one-point convex functions, and discuss how general this notion is. It should also be stated clearly whether one-point convexity holds in the experiments or not.\n2. Comparing the distance upper bounds for SGD and averaged SGD is reasonable, but this does not mean SGD cannot get much closer to the minimizer because the bound may not be tight. I would recommend the authors prove a lower bound in the SGD case to make the separation more concrete.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. The assumptions and theorems are stated very clearly.\n\nThe convergence analysis for SGD in the alternative view in this paper is new and the result is stronger than Kleinberg et al. (2018).",
            "summary_of_the_review": "I would like to hear the authors' comments on my major concern because I could be wrong. If $v_*$ can indeed change a lot for different learning rates, I would be rather happy to raise my score. Otherwise, I have to vote for rejection, but I encourage the authors to come up with another argument to motivate their convergence analysis and resubmit.\n\n=================\n\nAfter reading the rebuttal, I found the 1-dim example given by the author helpful. This simple example shows that the theory indeed predicts the averaged SGD converges to a flatter solution than SGD, so I have raised my score from 3 to 5.\n\nOverall, I'm not against accepting this paper because the convergence analysis still looks good to me. But I feel that the paper is at the borderline due to the following concerns:\n1. As I pointed out in the initial review, this paper compares the upper bounds for SGD and averaged SGD (even in the 1-dim example, only the upper bounds are compared). A more solid theoretical result should give a lower bound for SGD to ensure that it doesn't converge to flat minimizes.\n2. Though I appreciate the 1-dim example for its simplicity, it is unclear how general it is. It would be better if the authors could have generalized the example to a broad function class. Alternatively, the authors could also make up the generality issue in theory by measuring the empirical loss landscape to see if it is similar to the 1-dim example in some sense. For this reason, I believe that this paper still has much room for improvement.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4227/Reviewer_Q2SC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4227/Reviewer_Q2SC"
        ]
    },
    {
        "id": "JRpTFYAylgv",
        "original": null,
        "number": 4,
        "cdate": 1667111524490,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667111524490,
        "tmdate": 1670807905361,
        "tddate": null,
        "forum": "p4RvNzlJX7W",
        "replyto": "p4RvNzlJX7W",
        "invitation": "ICLR.cc/2023/Conference/Paper4227/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to explain why stochastic weight averaging (SWA, Izmailov et al., 2018) helps generalization. The paper adopts the framework of Kleinberg et al., 2018 and shows that SGD implicitly minimizes a regularized objective, where the regularizer depends on both Hessian of the loss and the covariance matrix of the stochastic gradient, under the assumption that the regularized loss is one-point convex. The authors show that the regularization strength is proportional to $O(\\eta^2)$ where $\\eta$ is the learning rate and SGD can reach a solution which is $O(\\sqrt{\\eta})$-close to the minimizer of the regularizers loss. The authors further explain the benefit of stochastic weight averaging by showing that the averaged iterate better optimizes the regularized loss, that is, SWA can get $O(\\eta)$-close to the minimizer. ",
            "strength_and_weaknesses": "##Pros:\n\n1. The paper is well-written and easy to understand. The proofs of the main theorems (Theorem 1 and 2) seem correct to me.\n\n2. The framework (Kleinberg et al., 2018) is originally developed for understanding optimization under assumptions weaker than convexity, that is, one-point convexity. The adoption of their framework towards understanding the implicit bias of SGD and SWA is quite novel, as well as the bias-optimization tradeoff view. \n\n3. The authors successfully explain the efficacy of SWA using the bias-optimization tradeoff, that is, though SWA implicitly penalizes the same objective as SGD does, SWA optimizes the objective better than SGD.\n\n\n\n##Cons:\n\n1. Though the theory proposed in this paper looks self-consistent, the authors don't really justify whether their theory is relevant to practice, like either directly testing their proposed regularizer using other optimizers so there is no optimization issue, or developing algorithms with better generalization using their insight. Technically speaking, it is not clear to me whether there exists a suitable learning rate, such that the optimization-bias tradeoff gives non-trivial improvement compared to the empirical minimizer of the original training loss.\n\n\tTo be more specific, under Assumption 2, it is fairly easy to see any stationary point $w^*$ of the original loss $f$ satisfies that $\\eta^2 \\nabla_{w^*} {Tr}(\\nabla^2 f(w^*) \\Sigma(w^*)) \\ge c\\|v^*-w^* \\|$ , where $\\Sigma(w^*)$ is the covariance matrix of noise in Eq. (4). By Assumption (3), $\\nabla^3 f(w^*)$ is of constant magnitude, which implies that $\\|w^*-v^* \\| = O(\\eta^2)$.  Immediately we see the optimization guarantee given by both Theorem 1 and 2 are vacuous in terms of the implicit bias. This is because we know even getting $O(\\eta^2)$-close (like $w^*$) to the minimizer of the regularized loss $v^*$ is not enough ($w^*$ can be understood by the solution found SGD with tiny LR)\n\t\n\tThat being said, I don't think the view presented in this paper is wrong. It is still valuable. It could be analysis are not tight or assumptions are too strong. But I do think the authors should address the above issue before publishing this work, which can be viewed as a sanity check to the theory.\n\t\n2. Regarding the relevance to the deep learning practice, another thing I think the authors should reconcile is why in practice typically the learning rate of SGD needs to be annealed and the generalization is still good. \n\n3. The details of the setting for experiments are missing, e.g. Figure 4. Why does ASGD have higher training loss than SGD in Figure 4? Does the same thing happen for Figure 3 (training loss is not reported there) Intuitively by averaging you should get a smaller training loss, e.g., think about SGD on a quadratic loss with isotropic gaussian noise.\n\n**Typos**: \n\n1.page 17. \"here we prove Theorem 1 which ..\". should be Theorem 2.  \n2.page 17. All $\\nabla R$ should be $R$.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to understand. The proofs of the main theorems (Theorem 1 and 2) seem correct. The adoption of the framework (Kleinberg et al., 2018) framework toward understanding the implicit bias of SGD and SWA is quite novel. More details about experiments are needed.",
            "summary_of_the_review": "This paper gives a novel understanding of the implicit bias of SGD, which explains the efficacy of stochastic weight averaging. However, I am not sure how this understanding is related to practice as the optimization guarantee presented in the main theorems is quite weak, not enough to separate SGD and SWA from ERM even. Thus now I tend to reject this paper, but I am willing to increase the score if the authors can address my concerns satisfactorily. \n(A good example is Damian et al.,2021, where they showed that the minimizer of the original loss doesn't meet their guarantee for label noise SGD on the regularized loss)\n\n- Damian, Alex, Tengyu Ma, and Jason D. Lee. \"Label noise sgd provably prefers flat global minimizers.\" Advances in Neural Information Processing Systems 34 (2021): 27449-27461.\n\n================================================\n\nUpdate after rebuttal: I've read the revision and rebuttal by the authors as well as reviews by other reviewers. My concern that \"whether there exists a suitable learning rate, such that the optimization-bias tradeoff gives non-trivial improvement compared to the empirical minimizer of the original training loss\" is not well-addressed by the authors and I decide to keep my score. \n\nThe new example in appendix C given by the authors in the revision shows when $\\delta$ is sufficiently small, SGD and average SGD will converge to the flat local minimizer (which is also the global minimizer of original loss $f$). This is just an optimization result because any algorithm that finds global minimizer will also find the flat minimizer in this case. To make it a result of the implicit bias of SGD, the author should demonstrate there is a case where the sharp local minimizer is indeed a global minimizer but still gradient noise biases SGD to the flatter area, even if the loss there could be slightly higher.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4227/Reviewer_EP51"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4227/Reviewer_EP51"
        ]
    }
]