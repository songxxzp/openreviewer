[
    {
        "id": "06ZjgXVE4Ph",
        "original": null,
        "number": 1,
        "cdate": 1666651405395,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651405395,
        "tmdate": 1670448292726,
        "tddate": null,
        "forum": "462z-gLgSht",
        "replyto": "462z-gLgSht",
        "invitation": "ICLR.cc/2023/Conference/Paper3318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper connects the DCI framework [1] for disentanglement to identifiability and proposes an extension of the framework that incorporates two new measures of representation quality which are explicitness (E) and size (S).",
            "strength_and_weaknesses": "### Strengths\n1) Overall, the writing of the paper is good \n\n### Weaknesses\n1) The novelty and significance of the work are low as it is mainly developed on an existing disentanglement metric which is DCI. \n2) The DCI metric was proposed long ago and has several drawbacks such as i) only supporting continuous factors, ii) using a not-well-normalized importance matrix R, \u2026  as discussed in a more recent work by Do & Tran ICLR-2020 [1] (Appendix A.8). The current work inherits all these drawbacks from DCI. It even complicates things by introducing 2 new minor metrics, while in my opinion, it should address the limitations of DCI and propose a simpler yet better metric for disentanglement.\n3) Lack of discussion of several related works about metrics for disentanglement [1, 2].\n4) Linking DCI to identifiability is straightforward and can be proven quite easily. Thus, Proposition 3.3 in the paper does not sound interesting to me. Besides, the DCI only achieves identifiability in the extreme case when D=C=1. But in practice, we rarely achieve such perfect values of D and C. Normally, D, and C will be between 0 and 1. How can the authors quantify the \u201camount\u201d of identifiability in this case? I think answering this question is more worthwhile than introducing 2 complementary minor metrics.\n5) The explicitness metric seems complex and ad-hoc to me. Since the capacity Cap(.) is defined differently for different kinds of models (e.g., Cap(.) is the maximum tree depth for a random forest while it is the number of neurons in neural networks), explicitness will be different if different models are used, which is not desirable. There is no consistent strategy to choose the \u201cprobe capacities\u201d $\\kappa\\_1$, \u2026, $\\kappa\\_T$. In addition, we need to train a new model for each \u201cprobe capacity\u201d which can be very costly if the model size or the number of probe capacities is large.  The suitable base loss $\\ell^b_j$ and the suitable lowest loss $\\ell^{*}_j$ are not well defined and not consistent between models. I don\u2019t understand what $\\mathbb{E}[z_j]$ really means for $\\ell^b_j$. Moreover, the loss $\\ell^{t, c}$ for each probe capacity $\\kappa_t$ can be noisy and incorrect, which greatly affects the accuracy of the metric. The explicitness E=1 means that AULCC=0, which is almost unlikely to happen.\n6) I don\u2019t see any point in using Size (S) as a metric.\n7) The results in Table 2 are very hard to interpret as I don\u2019t understand which representations are the best. Besides, there is no clear evaluation target for the representations. Looking at Table 2, how can we know the two proposed metrics E and S are correct and reasonable?\n\n[1] Theory and Evaluation Metrics for Learning Disentangled Representations, Kien Do and Truyen Tran, ICLR-2020.\n\n[2] Weakly Supervised Disentanglement with Guarantees, Shu et al., ICLR-2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please check my review above",
            "summary_of_the_review": "This paper is well written. However, its novelty and significance are limited. It seems to complicate things by introducing two minor, ad-hoc metrics besides DCI instead of designing better, more elegant metrics. Thus, I think the paper is below the acceptance bar of ICLR.\n\n### Post rebuttal\n******************************************\nI would like to thank the authors for your comments. I think most of my concerns are well addressed by the authors. Thus, I raise my score to 6 though I still worry about the soundness and the correctness of the metric for practical use.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3318/Reviewer_xBj7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3318/Reviewer_xBj7"
        ]
    },
    {
        "id": "xj0RnYvk2Sj",
        "original": null,
        "number": 2,
        "cdate": 1666662737575,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662737575,
        "tmdate": 1669233446584,
        "tddate": null,
        "forum": "462z-gLgSht",
        "replyto": "462z-gLgSht",
        "invitation": "ICLR.cc/2023/Conference/Paper3318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work draws connections between the DCI metrics, commonly used in the deep learning literature on disentanglement, and notions of representation identifiability, commonly used in the literature on independent component analysis (ICA). More precisely, Corollary 3.5 provides conditions under which D=C=1 and K=L implies equivalence between c and z, up to permutation and element-wise reparameterization. In addition, the DCI metrics are extended by adding \"explicitness\" (E) and \"size\" (S). Basically, the former measures how a given probe class can predict the latent variables with various capacity levels and the latter is simply the ratio of the dimensionality of the ground-truth latent vector over the dimensionality of the learned representation. These novel metrics are then evaluated on various learned representation and dataset.",
            "strength_and_weaknesses": "Strengths:\n- I believe both the deep learning community and the (more theoretically oriented) nonlinear ICA community will benefit from the connections made in this work.\n- I very much agree with the authors that one should take into account the capacity of a probe when evaluating a representation and that exploring different classes of probes with different capacities gives a more complete picture of the learned representation.\n- The \"size\" metric, although very simple and obvious, is important if one wants to make fair comparisons.\n- The paper is clearly written and easy to follow.\n- The experiments are clearly presented and sufficient for the argument.\n\nWeaknesses:\n- Mathematical precision is sometimes lacking. See my comment below, especially about Corollary 3.4.\n- As I said, I agree with the author that one should take into account the capacity of a probe when evaluating a representation and that exploring different classes of probes with different capacities gives a more complete picture of the learned representation. However, I am a bit skeptical that the proposed explicitness score (E) brings more value than just transparently reporting the Informativeness score (I) for a couple of different probes with different capacities (i.e. the loss-capacity curves). E is basically a less transparent way of communicating this information. Moreover, Figure 4 shows that the ranking can change when simple rescaling are applied to the capacity measures. I find this worrying.\n- I do not understand why the DCI metrics are restricted to R matrices with columns that sum to one (Definition 2.1). This is counterintuitive to me, since one could have a learned representation c that is completely useless to predict one of the ground-truth latent variable z_j, but since the columns of R must sum to one, it is impossible for the corresponding column (R_{.,j} to be filled with zeros (as it should, to represent the fact that the representation is useless to predict that ground-truth factor). ",
            "clarity,_quality,_novelty_and_reproducibility": "Lack of mathematical rigor:\n- Corollary 3.4: In general, it is not clear that |W| qualifies as a valid choice of R (definition 2.1), since its columns might not sum to one. This means one cannot apply Prop. 3.3, which leverages the fact that the columns of R sum to one. I guess this could be fixed by redefining R by normalizing its entries to make sure its columns sum to one. But one has to be careful when doing that, since, a priori, |W| might have a column filled with zeros (this possibility isn\u2019t excluded by the assumptions of the theorem, but that would imply some z_i = 0), which would prevent such a normalization. Also the proof mentions W^{-T}, but a priori it is not guaranteed that W is invertible (maybe this could be added to the assumption? That would also prevent one of the columns to be filled with zeros, thus allowing normalization). These subtleties must be addressed.\n- Corollary 3.5: Please add the assumption that f is differentiable, since the statement refers to its partial derivative.\n- (Not really about rigor, but a related point) I believe Corollary 3.5 would benefit from an example of R that satisfies this property (and nonlinear f). The points I raised about Corollary 3.4 shows that constructing a valid R can present subtleties. The following remark mentions the Gini importance for Random forest, but, as the authors acknowledge, isn\u2019t invertible.\n\nClarity:\n- As I said, this paper is overall well written and clarity is not an issue.\n- In introduction: It is unclear what is meant by \"a uniformly-mixed version thereof\" (point (iii)). I find the explanation confusing and imprecise: \"each ci containing the same amount of information about each zj\". A solution would be to refer to the definition in Section 6.1. \n- Section 4.1: \u201cFor example, we may choose \u03ba_T to be large enough for all representations to achieve their lowest loss and, for random forest f\u2019s, we may choose \u03ba_1 = 1\u2026\u201d Does k_1 = 1 mean tree depth = 1? Say it explicitly here.\n\nNovelty/Originality: \n- Although the results proved are close to being trivial, these connections were not clearly presented before, which makes this work novel.\n- The originality is somewhere between low and average. But this is still a valuable contribution.\n\nSuggestions for improvement:\n- Could Corollary 3.4 be seen as a special case of Corollary 3.5? For instance, could one present only Corollary 3.5 and give the linear case as an example?\n",
            "summary_of_the_review": "I believe this paper makes valuable contributions, but (i) Corollary 3.4 contains a mistake (although I am optimistic that the issue can be resolved) and (ii) I do not see the added value in using E instead of simply reporting the loss-capacity curves more transparently. Mainly for these reasons, I recommend weak rejection for now, but I am very open to increasing it to a weak accept once the problem in Corollary 3.4 is fixed.\n\n--- Post-rebuttal update ---\nI have updated my score to 6, see my comment below.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3318/Reviewer_wQrr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3318/Reviewer_wQrr"
        ]
    },
    {
        "id": "qOnE-hJmohg",
        "original": null,
        "number": 3,
        "cdate": 1666895879231,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666895879231,
        "tmdate": 1670573803248,
        "tddate": null,
        "forum": "462z-gLgSht",
        "replyto": "462z-gLgSht",
        "invitation": "ICLR.cc/2023/Conference/Paper3318/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an extended Disentanglement Framework to evaluate the disentanglement in representation learning. Specifically, the paper first links the disentanglement to independent component analysis, then propose two new measures of representation quality: explicitness (E) and size (S). The paper uses MPI3D and Cars3D datasets to evaluate the method.\n",
            "strength_and_weaknesses": "**Strength**\n\n1 The new measurement of explicitness (E) (easy-to-use) and size (E) is interesting for disentanglement evaluation.\n\n2 The paper writing is clear and easy to follow.\n\n**Weaknesses**\n\n1 The traditional DCI framework may already be considered explicitness(E) and size(S). For instance, to evaluate the disentanglement (D) of different representation methods, you may need to use a fixed capacity of probing (f), and the latent size should also be fixed. DCI and ES may be entangled with each other. For instance, if you change the capacity of probing or the latent size, then the DCI evaluation also changes correspondingly. The reviewer still needs clarification on the motivation for considering explicitness(E) and size(S) as extra evaluation.\n\n2 Intuitively, explicitness(E) and size(S) may be highly related to the given dataset. The different capacity requirements in the 3rd paragraph may be due to the input modality difference. Given a fixed dataset, the evaluation of disentanglement should provide enough capacity and training time which is powerful enough to achieve the DCI evaluation. If the capacity of probing needs to be evaluated, then the training time, cost, and learning rate may also be considered because they may influence the final value of DCI.",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "The reviewer thinks the disentanglement evaluation should consider relatively orthogonal and important evaluation metrics. Motivation and necessity are the primary concerns. \n\n\n\n--------------------------------------------------------\nThanks for the author's feedback! After reading the feedback, some concerns are addressed, e.g., the motivation for measuring explicitness(E) and size(S). While the reviewer's second question was not well addressed, \"If the capacity of probing needs to be evaluated, then the training time, cost, and learning rate may also be considered because they may influence the final value of DCI.\". After reading other reviewers' comments, the reviewer has similar concerns about the proposed method's novelty (technical contribution) compared to DCI and practical usefulness. Therefore, the reviewer will keep the original score and vote for borderline.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3318/Reviewer_WGrh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3318/Reviewer_WGrh"
        ]
    },
    {
        "id": "jWgCBKHfQDd",
        "original": null,
        "number": 4,
        "cdate": 1666991355530,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666991355530,
        "tmdate": 1670257380187,
        "tddate": null,
        "forum": "462z-gLgSht",
        "replyto": "462z-gLgSht",
        "invitation": "ICLR.cc/2023/Conference/Paper3318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces two quantifiers of disentanglement (measures of how good a latent representation is, measurable when we have the ground truth latents), complementary to Eastwood & Williams' DCI, Disentanglement, Completeness, and Informativeness. The first is _explicitness_, which is related to a (normalized) area under the curve of informativeness as a function of capacity, and illustrates how accessible the true latents are from a code. The second is _size_, which simply is the ratio of the dimensionality of the code to the dimensionality of the true latents.",
            "strength_and_weaknesses": "### Strengths\n- The paper is certainly well written\n- The work touches on an important open problem, how do we evaluate disentanglement?\n- The proposed ideas are intuitively appealing\n\n### Weaknesses\n- The main proposed quantity, explicitness, relies heavily on other quantities for which it is still an open problem how to measure\n- The empirical validation is a bit lacking, what are these measures predictive of? What are they a model of?\n\n\n### Comments\n\n> Note that $I_j$ depends on the capacity of $f_j$, as depicted in Fig. 1.\n\nShouldn't D and C also depend on the capacity of $f$ through $R$?\n\n> Corollary 3.5, [..] with $f$ an invertible nonlinear function\n\nThis seems to be a really strong requirement, which greatly reduces the class of functions that the results of section 3 can give us intuition, or \"theoretical insights\", about. I wonder if I am missing something deeper here?\n\n\n> Larger representations are often more informative. When dim$(c)<$dim$(z)$.\n\nI know that this is just meant to be an intuitive passage, but I'm not sure this is the right threshold for dim$(c)$. Perhaps appealing to compression-based generalization theory here might make more sense, if the number of bits one can encode in $c$ is smaller than the number of bits of information in $z$, then there is necessarily degradation/lossy compression. Some phase transition occurs at that threshold, although as the authors already argue, regardless of the threshold more capacity is generally accepted to be more informative.\n\n\n> larger representations (ImgNet-pretr, raw data) tend to be more explicit than smaller ones (VAE, $\\beta$-VAE)\n\nThere's an experiment that's missing here, that might make this claim much easier to validate. What happens to a [$\\beta$-]VAE as $L$ increases, i.e. S decreases?\n\nThe downside of asking this question is that, as the authors point out, changing $L$ changes the architecture of the model, which inherently changes the explicitness. Are two models with slightly different architectures comparable? Would it even make sense to plot E as a function of $L$? \n\nAs Figure 4 shows, even the choice of scaling on the AULCC affects the ranking of methods for which we have a poor measure of capacity, MLPs. This work leans heavily on a choice of quantification of capacity, which is still an open problem for DNNs. \n\nIt's obvious from recent research on generalization that we cannot hope to get interesting quantities that speak to generalization and disentanglement without taking into account (effective) capacity and data. This is why what is proposed is quite appealing intuitively. But I wonder what this quantity currently predicts, i.e. what it is a model of, what other quantities it correlates with and/or predicts.\n\nIn the face of this brittleness, one way to reassure ourselves is to gather more data. Perhaps such a plot as I'm proposing above would reveal some interesting (if noisy) patterns.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite clear. This precise idea is novel as far as I know, but it of course builds upon and expands previous ideas on disentanglement. I'm fairly confident I could reproduce this work.",
            "summary_of_the_review": "I'm leaning reject but still quite on the fence.\n\nThis paper provides a nice intuition and provides a decent amount of justification for it, but where the work is lacking is in showing where it leads. The authors propose a quantity, but it's not too clear how useful or predictive it is, nor if it has some empirical regularities which make it desirable in practice (if I wrote a paper on a new kind of VAE tomorrow, would I want to use this? I'm not convinced).\n\nUpdate: The authors have addressed most of my concerns, and going through the updated version of the paper, I think it is now a much better work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3318/Reviewer_R81A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3318/Reviewer_R81A"
        ]
    }
]