[
    {
        "id": "i47-j5z2ri8",
        "original": null,
        "number": 1,
        "cdate": 1666497265771,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666497265771,
        "tmdate": 1666497333412,
        "tddate": null,
        "forum": "rimcq1oIFeR",
        "replyto": "rimcq1oIFeR",
        "invitation": "ICLR.cc/2023/Conference/Paper5557/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a benchmark that evaluates video prediction models using the performance of downstream control tasks to complement traditional perceptual similarity or pixel-wise metrics. Specifically, the benchmark exposes a simple forward dynamics interface for the video prediction model to use it in a model-based planning method, visual foresight. The benchmark uses the robosuite tabletop environment and robodesk environment, where 11 robot manipulation tasks are designed in total. The paper evaluated three video prediction methods including variational RNN- and diffusion-based models.",
            "strength_and_weaknesses": "**Strength:**\n\n- The paper studies an important problem, i.e., the evaluation of generative models, which is hard since there are no predefined metrics. The paper provides an example of a better evaluation of generative video prediction models using downstream control tasks.\n- The paper proposes a good case study that shows popular metrics such as LPIPS and MSE may not reflect the true performance of downstream control tasks, which justifies the need for the proposed method.\n- The benchmark features a simple plug-and-play interface that makes it easy to use and general for almost all video prediction methods.\n\n**Weakness:**\n\n- As a benchmark paper, the number of baselines is too few. Only three methods are evaluated while there are many more video prediction methods that could be evaluated given the simple and general interface.\n- The performance is reported with 2 runs only (in Figure 3), which might not be enough to accurately capture the performance and variance.\n- Only a specific planning method with specific hyperparameters is used for evaluation, which might not be indicative, since different video prediction (dynamics) models may require different algorithms or hyperparameters to perform the best.\n- The ensemble experiment is weak since it was only showcased for 3 tasks where the improvement is only significant for one task while the ensemble actually performs worse for another task.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-motivated and easy to understand. The accompanying website provides a nice overview of the paper the benchmark.\n- The paper definitely proposes a novel benchmark, which is also important to complement existing video prediction evaluation.\n- The paper should be easy to reproduce since the authors promise to release the code soon.",
            "summary_of_the_review": "This paper addresses an important problem, i.e., improving generative video prediction evaluation with downstream control tasks. It proposes an easy-to-use benchmark for this setting. While there are several concerns such as the number of baselines, I currently slightly lean toward acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5557/Reviewer_wekY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5557/Reviewer_wekY"
        ]
    },
    {
        "id": "jndZrKznpm5",
        "original": null,
        "number": 2,
        "cdate": 1666527121751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666527121751,
        "tmdate": 1669183323081,
        "tddate": null,
        "forum": "rimcq1oIFeR",
        "replyto": "rimcq1oIFeR",
        "invitation": "ICLR.cc/2023/Conference/Paper5557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a benchmark for evaluating vision-based dynamics models. The paper shows that common prediction metrics (typically revolving around predicted image quality and not physical plausibility or causal interaction effects) may not necessarily correlate with control performance, and thus proposes an evaluation approach structured around a number of tasks in the robodesk and robsuite environments. For this the controller is standardised (MPPI) and abstracted away to allow SOTA chasers to focus directly on the dynamics modelling aspects of the task.",
            "strength_and_weaknesses": "Strengths:\n\nI think the empirical results showing that perceptual video predictive quality metrics do not correlate well with control success is an interesting finding that will likely be of value to others.\n\nI agree that abstracting away control will result in significant performance increases in dynamics modelling and short term progress in this area.\n\nWeaknesses:\n\nThe proposed benchmark abstracts controller details from the dynamics model in the name of simplicity and ease of use, but I am not convinced that this is a good decision when it comes to longer term progress in our field. I also think the benchmark itself is vulnerable to a number of the criticisms this paper makes about evaluating video prediction in isolation of the underlying control tasks. I have a strong suspicion that the same article could be written about this framework, with results showing that different dynamics models perform differently with different control strategies and planners (eg. CEM, MPPI, trajectory optimisation etc.) and arguing that we need a new benchmark to avoid this. It's likely that this framework will result in a set of good dynamics models for use with MPPI, but not with other approaches.\n\nI am not convinced this will advance control performance, which may well benefit from more tight coupling between dynamics modelling and control rather than less. For this reason, I would be more inclined to use robosuite and robodesk directly, and value the ability to explore multiple control, planning and dynamics modelling approaches. I think this benchmark needs more controller options (beyond sampling based approaches) and to ideally enable the use of gradient based approaches that better leverage the learned dynamics models.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed benchmark is clearly described and motivated for. The benchmark extends existing settings that are relatively well established.",
            "summary_of_the_review": "I like the analysis of video prediction vs control, but am unconvinced that the proposed new benchmark is of significant value to advancing dynamics modelling for control. I think the benchmark needs more controller options (beyond sampling) and to allow gradient propagation to be really useful.\n\n----- Post rebuttal updates ----\n\nThe authors have added additional controllers to the benchmark, addressing the primary concern I had with this work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5557/Reviewer_5Vyr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5557/Reviewer_5Vyr"
        ]
    },
    {
        "id": "wKepTZmLLI",
        "original": null,
        "number": 3,
        "cdate": 1666712960108,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666712960108,
        "tmdate": 1666712960108,
        "tddate": null,
        "forum": "rimcq1oIFeR",
        "replyto": "rimcq1oIFeR",
        "invitation": "ICLR.cc/2023/Conference/Paper5557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a new benchmark for evaluating video prediction approaches. The authors remark that the metrics used for evaluating video prediction quality, based on pixel accuracy, can be poor guides to the quality of the prediction. In some cases, an apparently accurate prediction may lead to failure in, eg, robotic planning. The authors propose to address this with a new benchmark evaluating video prediction from the perspective of 11 simulated robotic tasks, across two simulators. \nState-of-the-art video predictions methods are compared using those tasks. ",
            "strength_and_weaknesses": "### Strengths\n- The rationale for the paper is very good, and it is addressing a relevant question: How to find better metrics for video prediction (or image generation in general) is an important question for the field, and robotics tasks are a good choice. \n- The availability of such a benchmark is likely to stimulate discussion and research in the video prediction field. \n- The experiments are well-designed and thorough. \n- The paper is generally clear and pleasant to read. \n\n### Weaknesses\n- The choice of tasks for the benchmark is of very limited variety. Out of the 11 tasks: \n\t- 4 are pushing objects of different (but convex) shapes\n\t- 3 are pushing buttons of different colour\n\t- 2 are pushing off blocks\n\t- and two are opening - admittedly the most interesting ones. \n\tThis is a limitation as it is not clear that most of those tasks are really the best chosen to evaluate motion prediction in videos, and could very well fail to measure large errors in the prediction. More varied manipulation (stacking?) would be interesting. \n- The number of methods evaluated in the benchmark is limited, although if the benchmark is provided to the community this is not a very crucial issue. \n- The analysis of the result is limited, and provides only limited insights in the validity of the initial hypothesis. It would have been good, for example, to have a plot of pixel accuracy vs simulation accuracy of the different methods to assess whether the classical metrics are good proxy or not. Also, which tasks out of the 11 are correlated to which would also be an interesting consideration. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear and well-written. The benchmark and methods are well described and the experiments could be replicated. The authors provide the source code for the benchmark, therefore making sure that the community can use it. \nThe novelty of the paper is sort of secondary here: Although the authors are not the first to note that performance in robotic tasks is important for prediction, they provide, to my knowledge, the first benchmark of this sort. Therefore, I believe that this is a valuable contribution (despite my criticism earlier). ",
            "summary_of_the_review": "This is an interesting paper, that provides a useful and timely resource to the community to possibly help provide a broader understanding on the quality and the limitations of video prediction approaches. Despite my concerns that the chosen robotic tasks are too limited in variety, this is clearly a step in the right direction and would be very useful to the field. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5557/Reviewer_LW5N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5557/Reviewer_LW5N"
        ]
    },
    {
        "id": "dvrXgMYTt2",
        "original": null,
        "number": 4,
        "cdate": 1667509109447,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667509109447,
        "tmdate": 1667509109447,
        "tddate": null,
        "forum": "rimcq1oIFeR",
        "replyto": "rimcq1oIFeR",
        "invitation": "ICLR.cc/2023/Conference/Paper5557/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The papers presents a benchmark to evaluate if video prediction\nquality metrics correlates with control/ robotics manipulation.\nAs expected good perceptual quality of prediction does not\nnecessarily indicates better control capabilities.\n\nThey compare diverse sota methods on the new benchmark\nusing different model sizes, data quantity etc.",
            "strength_and_weaknesses": "\n### Strengths \n\n1. The paper guides the planning literature that maybe they shouldn't be using \nreconstruction and perception losses when predicting future moves.\n2. The paper is well written and the results are clear. \n\n\n### Weaknesses\n\n1. I feel the main weakness is the potential significance of the results\nis limited as I feel people tend to know that reconstruction is not ideal.\nI think the paper would be better if the authors actually found any \nmetric that seems to correlate more ? ",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nVery clear presentation.\n\n### Quality\n\nGeneral good quality of presentation and general material.\n\n\n### Novelty\n\nThe novelty consists on the benchmark with can have a very practical use.\n\n### Reproducibility\n\nThe paper misses code, but if the authors provide code the benchmark should be fully reproducible.\nThe training details are well explained on supplementary material.",
            "summary_of_the_review": "My recommendation is for weak acceptance.\nI think the benchmark is useful and give new insights about planning methods.\nI am however, not so familiar with the literature and I might be wrong about\nthe significance of this proposal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5557/Reviewer_fMXK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5557/Reviewer_fMXK"
        ]
    }
]