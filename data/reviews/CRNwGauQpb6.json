[
    {
        "id": "6XJnrBxNzaf",
        "original": null,
        "number": 1,
        "cdate": 1666102921937,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666102921937,
        "tmdate": 1670243714454,
        "tddate": null,
        "forum": "CRNwGauQpb6",
        "replyto": "CRNwGauQpb6",
        "invitation": "ICLR.cc/2023/Conference/Paper1150/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new knowledge distillation method called NORM, based on the idea of many-to-one representation matching. The method follows a two-stage distillation process using a linear Feature Transform (FT) module. The FT module has two layers: the first expands the number of channels to N times those from the teacher, and the second shrinks the number of channels back to the original student feature space. The expanded representation is forced to approximate the teacher representation N times. Experiments are conducted on the CIFAR100 and ImageNet datasets with various teacher-student pairs.",
            "strength_and_weaknesses": "Strengths:\n\nS1) Knowledge distillation is an important and active topic in the community, which is addressed in this work.\n\nS2) The paper is generally well written and easy to follow.\n\nS3) The method is very simple and easy to understand.\n\nS4) The experimental evaluation is significant, considering many different network architectures and several baselines. Ablations are performed to understand which parts of the system matter more.\n\nWeaknesses:\n\nW1) The main concern I have with this paper is that we don\u2019t really understand why the proposed method works. Simply create multiple heads with the exact same shape and distill the teacher into each of them simultaneously. Why is this supposed to be better than simply having one head here? I do recognize that the authors provide many experiments indicating that this replication of heads apparently really helps, which to me feels surprising.\n\nIn general, I think there should be more discussion on this. For example, what if all N heads are initialized with the exact same random weights? If this is the case, I believe that would degenerate into the single-head (N=1) case? What types of initialization are needed here? Maybe we need higher initialization variance in order to capture different optimization routes?\n\nW2) A question: can you please clarify if the networks are using global average pooling? Or if the fully connected layer from the classifier is applied on top of the final feature map directly? Can you please clarify additionally if the baseline methods you are comparing to also use global average pooling, or if in their cases the FC layer is applied on top of the final feature map directly? I wanted to make sure everything is comparable in this respect, and the gains we see are not coming from the removal of the global pooling, for example.\n\nMinor details:\n\nM1) Section 3.1 does not concern new contributions from this paper. I suggest the authors revise its title to \u201cBackground: One-to-one representation matching\u201d, to emphasize the fact that background is presented.\n\nM2) In several parts of the manuscript the word \u201ctyped\u201d is used incorrectly, eg \u201csame typed\u201d, \u201cdifferent typed\u201d. In most cases it should be \u201ctype\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well written and very easy to understand. Everything is very clear.\n\nReproducibility: the authors promise to release code for this work. Additionally, given that the method is very simple, I don\u2019t see any potential problems with reproducibility.\n\nNovelty: the method builds on top of standard techniques in the area of feature distillation, and the main contribution is a simple modification to the traditional two-stage distillation framework. There is some novelty, although it is not so strong.\n\nQuality: even though the method is simple and not so novel, the paper is very well written and the experiments quite comprehensive. The execution quality is generally high, but one aspect that is lacking is the justification of the method, as I discuss in detail in the \u201cWeaknesses\u201d section above.",
            "summary_of_the_review": "The paper tackles an important problem, proposing a modification to the standard two-stage distillation paradigm. Experiments are comprehensive and seem to show improved performance in common benchmarks, but the method lacks justification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1150/Reviewer_iHQs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1150/Reviewer_iHQs"
        ]
    },
    {
        "id": "7GX4lALU26q",
        "original": null,
        "number": 2,
        "cdate": 1666350529997,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666350529997,
        "tmdate": 1669236877846,
        "tddate": null,
        "forum": "CRNwGauQpb6",
        "replyto": "CRNwGauQpb6",
        "invitation": "ICLR.cc/2023/Conference/Paper1150/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an architectural trick for two-stage knowledge-distillation from a large to a small network. It expands the student feature tensor of the last layer from C to NC channels, applies distillation from the teacher feature tensor to each of the N sub-tensors, then contracts back to the original feature dimensions, C. Both mappings are linear and at inference they are merged into a single CxC layer.",
            "strength_and_weaknesses": "Strengths:\n- Well written\n- Mostly clear\n- Somewhat novel\n\nWeaknesses:\n- Too narrow experimental setting\n- Not particularly effective\n\nThe paper is well written and the method is mostly clear except for the distillation loss.\n\nIn particular, the default is Euclidean distance between features (3), referred to as NORM. In the experiments, the \"vanilla logits based distillation\" is additionally used, referred to as NORM+KD, citing the original distillation paper (Hinton et al. 2015).\n\nBut the original distillation loss in (Hinton et al. 2015) is cross entropy with soft targets given by the teacher, whereas the Euclidean distance between logits is only discussed as an approximation when the temperature is high. So the \"vanilla logits based distillation\" is not clear. It should be given formally as a second option below (3). This is very important given that NORM+KD is the best performing option. It cannot remain undefined.\n\nThe names NORM and NORM+KD are also not appropriate, because it is implied that NORM does not include distillation; which is not true as (3) is a form of distillation. It is more appropriate to use NORM+A and NORM+B for some A and B.\n\nThe novelty appears to be ok as I am not aware of such trick for knowledge distillation. However, a similar trick exists of introducing a number of classifiers by Littwin and Wolf 2015, \"The Multiverse Loss for Robust Transfer Learning.\" The differences are that we have N classifiers rather than N subtensors, C classes rather than C channels, cross-entropy rather than distillation loss and an additional constraint that the classifiers are orthogonal. But structurally, the ideas are very similar. Some discussion is in order.\n\nAnother very related idea is to match N features of the student to N features of the teacher by referring to dense features of different image patches. This has been explored in self-distillation settings for self-supervision, e.g.\n(Dense CL) Wang et al. 2021. Dense Contrastive Learning for Self-Supervised Visual Pre-Training.\n(iBOT) Zhou et al. 2022. iBOT: Image BERT Pre-Training with Online Tokenizer.\nSuch dense matching is an alternative way \"to introduce many parallel knowledge transfer routes.\" A discussion is in order. A comparison would be a great addition.\n\nThe paper includes extensive experiments on several teacher-student model pairs and a lot of ablation studies. However:\n- In Table 1, NORM+KD beats the competition only in two columns where many or all of the competitors are missing. In Table 2, it is best in more columns but again many of the competitors are missing. I do not find these results conclusive.\n- More importantly, two-stage knowledge-distillation from a larger to a smaller network for supervised classification is too narrow. For this architectural modification to be properly validated, I would suggest extending to online self-distillation, self-supervised and semi-supervised settings, continual learning, vision transformers and other tasks like object detection. It would be particularly interesting to combine with methods like BYOL, DINO, iBOT, Dense CL etc., which use self-distillation.\n\nAn ablation of a \"reverse\" idea in addition to \"Contracting teacher representation\" would be to match 1 representation of the student to N representations of the teacher. N-to-N could also be explored.\n\nAppendix A.1 appears to be an overkill. The composition of linear layers is obviously linear.\n\nMy final rating will depend on how many additional experiments can be done and how effective NORM will be shown to be.",
            "clarity,_quality,_novelty_and_reproducibility": "As detailed above, the paper is well written and the method pretty clear except for the distillation loss. The idea is somewhat novel.",
            "summary_of_the_review": "This is an interesting idea. The distillation loss is unclear but I am sure this can be corrected. However, the experimental setting is too narrow and the method does not appear to be very effective.\n\n**Update**: The rebuttal addresses my concerns to an extent and although further improvement is possible, I raise my recommendation from 5 to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1150/Reviewer_xShr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1150/Reviewer_xShr"
        ]
    },
    {
        "id": "9uWsr9_HYwF",
        "original": null,
        "number": 3,
        "cdate": 1666623007221,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623007221,
        "tmdate": 1669715943533,
        "tddate": null,
        "forum": "CRNwGauQpb6",
        "replyto": "CRNwGauQpb6",
        "invitation": "ICLR.cc/2023/Conference/Paper1150/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work present NORM, a  two-stage feature distillation method. It relies on a linear feature transform module that is inserted after the last convolutional layer of the student network to enable a many-to-one representation matching mechanism conditioned on a single teacher-student layer pair via feature expansion, splitting and group-wise mimicking. Experiments are peformed on CIFAR-100 and ImageNet  to show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Pros:\n\n+ This work put forward a plug-and-use module that is easy to deploy.\n\n+ The method is experimentally compared with many State-of-the-Art methods on ImageNet and CIFAR-100.\n\n+ The paper is in general well-written.\n\n\nCons:\n\n- The method seems to be limited to the convolutional architecture.\n\n- The idea of creating multiple heads with the exact same shape and distill the teacher into each of them simultaneously seems intuitive and I wonder why it is benefitial for improving the distillation performance.\n\n- Experiments are only performed on image classification datasets, will it still work on other tasks such as image segmentation?\n\n- In Equation 3, $d(,)$ is undefined. I guess it is a distance function yet the authors still need to make it clarified.\n\n- It is not clear to me how to find the desired channel dimension $NC_t$ for different teacher-student pairs.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is generally well-written and easy to follow. The methodology part lacks some details for reproduction (e.g., eq. 3, $NC_t$). The authors also promised that the code would be released later. ",
            "summary_of_the_review": "The paper is in general well written. The authors put forward a plug-and-use many-to-one presentation learning module to improve the single teacher-student distillation performance. Experiments under different variants on CIFAR100 and ImageNet are implemented to evaluate the effectiveness of the proposed method. However, some details are missing for reproduction. The method seems also limited to convolutional networks and Image classification tasks. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1150/Reviewer_d7Sn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1150/Reviewer_d7Sn"
        ]
    },
    {
        "id": "VUjTAvH69T",
        "original": null,
        "number": 4,
        "cdate": 1667268432547,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667268432547,
        "tmdate": 1667268432547,
        "tddate": null,
        "forum": "CRNwGauQpb6",
        "replyto": "CRNwGauQpb6",
        "invitation": "ICLR.cc/2023/Conference/Paper1150/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for feature distillation between a teacher model and a student model. It aims to improve the student model's accuracy by utilizing knowledge from the more capable teacher model. \n\nThe method adds a linear residual module after the last convolution layer of the student model. The module first transforms the student features to N times the number of channels for the teacher model. Then the expanded student features are grouped into N equal-width groups. Each group is compared with the teacher's features for computing the distillation loss. The distillation loss is optimized in conjunction with the task(classification) loss to learn the student model parameters. \n\nThe method is examined on multiple classification benchmarks with different teacher/student pairs. It achieves competitive accuracy with other baseline methods. The authors provide an analysis of the effectiveness of the proposed module through a comparative study. ",
            "strength_and_weaknesses": "Strength:\n\n+ The paper is well written with a clear description of the method and experiments. I also like that the method is simple and seems straightforward to implement based on the provided information. \n\n+ The experiment results seem strong compared with other baseline methods. These methods are up to date. \n\n+ The idea of having N representation matching objectives is interesting. The fact that different transform groups differ only in initialization but together improve accuracy is very intriguing. \n\nWeakness/suggestions:\n\n+ Though effective, there seems to be no obvious motivation to construct multiple transforms at first.  I would like to see some discussion or theoretical analysis on where the effectiveness comes from or what it suggests for the community of model distillation. Because a transform in each group can be considered as an independently initialized tiny model in an ensemble,  could it be possible that it has any connection with ensembling or mixture-of-expert methods? This is more of a question not explored than a weakness, but having this discussion may greatly amplify the impact of this work. \n\n+ Most experiments are conducted on classification tasks. It may be beneficial to show the results in other tasks, such as segmentation and object detection, where distillation is also widely used. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written with satisfying clarity and quality. I find the idea novel and the results promising, though providing some justification for the effectiveness of the method would further strengthen the work.\n\nOn reproducibility, I did not find code submission. But I believe a normal Ph.D. student would have sufficient information to implement the method based on the information provided. ",
            "summary_of_the_review": "Overall I feel the submission is of good quality and has satisfying results. My initial recommendation is to accept this paper. A few points as listed in the weakness section describe potential improvements that could be made before publication. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1150/Reviewer_Xp75"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1150/Reviewer_Xp75"
        ]
    }
]