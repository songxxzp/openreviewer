[
    {
        "id": "mifzX5O7hXv",
        "original": null,
        "number": 1,
        "cdate": 1666497858064,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666497858064,
        "tmdate": 1669162031322,
        "tddate": null,
        "forum": "Jifob4dSh99",
        "replyto": "Jifob4dSh99",
        "invitation": "ICLR.cc/2023/Conference/Paper4172/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the sample complexity and error bound for MAML applied on linear meta-learning problem. While the features are iid and the tasks apply a subset of features, one can learn the useful features and apply them for downstream learning. The overfitting would not hurt generalization.",
            "strength_and_weaknesses": "Although the model is only linear case, I think the result is important for a first step of understanding the MAML. The proof is solid. Questions in detail:\n- In Theorem 1, if $n_t$ is small, $m$ or $n_v$ is large, then the RHS is smaller than zero. Does it mean, if $b_{w_0}$ and $b_w^{ideal}$ are both positive, then RHS of Thm. 1 is always positive? It is still a bit confusing. Intuitively, we need a few data for test phase, but typically as long as it\u2019s larger than $s$ the number of features, the error should be small if you can identify the useful features in previous stages. But I did not see the explicit dependence on $s$. More importantly, why do more training tasks and validation samples hurt the probability, shouldn\u2019t more data mean better learning performance?\n- I think the above issue also appears in Prop. 4-6.  \n- Eq(2), should it be $X^{r^T}$?\n- Eq(4) and the text below: $\\hat w$ was defined in Eq(3), is it switched to the parameter that minimizes validation error? If so, it would be better to use another notation. \n- Could you have a corollary for the case $w_s^0 = 0$ and a discussion? I'm interested in separating the role of mean and covariance in the bound.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The results are clearly presented and further explained. I believe the results are novel. For clarity, I suggest\n- Add a notation table since there are three stages (training - validation - test) and involve a lot of super/sub-scripts. \n- Add the experiments that show experimental values and curves on theoretical errors, and check if they match, like Figure 3 in \u200b\u200b\u201dProvable Benefits of Overparameterization in Model Compression: From Double Descent to Pruning Neural Networks\u201d.\n- Intro: I suggest not using bulk citation like \u201cBelkin et al., 2018; 2019; Bartlett et al., 2020; Hastie et al., 2019; Muthukumar et al., 2019; Ju et al., 2020; Mei & Montanari, 2019\u201d. Please give a brief review of each paper individually so that one can get the message and be convinced that the authors are really aware of what those papers do.",
            "summary_of_the_review": "I believe the paper is solid in theory and justifies an important problem. However I have a few confusions regarding the insight of the results, and whether they are too restrictive in the scenario that the RHS of the probability bound is larger than zero. I would like to see authors' comment and consider adding on the score.\n\n=============== Update ===============\n\nScore changed to 6 after first round response.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4172/Reviewer_uCi7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4172/Reviewer_uCi7"
        ]
    },
    {
        "id": "MymNwkTIR5D",
        "original": null,
        "number": 2,
        "cdate": 1666576693586,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576693586,
        "tmdate": 1666576693586,
        "tddate": null,
        "forum": "Jifob4dSh99",
        "replyto": "Jifob4dSh99",
        "invitation": "ICLR.cc/2023/Conference/Paper4172/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work examines the generalization error and model error of single-step MAML in the mixed linear regression task. Unlike prior works, this work explicitly considers optimization in the over-parameterized regime, and derives model-error bounds in the non-asymptotic regime. Through theoretical derivations, the authors suggest that overparameterization tends to be beneficial, especially when task diversity and difficulty is high.",
            "strength_and_weaknesses": "\nOverall, the results proved in this paper are significant for demonstrating the benefits of overparameterized meta-learners, though the typical asterisks associated with the mixed-linear regression task and Gaussian features persist. The paper feels complete, but I find two factors that leave me wanting: \n\n1. Bernacchia(2021) leverages NTK theory to extend their analysis to non-gaussian and non-linear settings. Could similar techniques be used to extend your results to infinite-width settings? A road map highlighting results and propositions required to bridge this gap could contribute greatly to a discussion section.\n\n2. In showing the tightness of the bound in theorem 1, can you provide some numerical examples under some concrete sets of system parameters, and present it through visualizations? The tightness of the bounds in propositions 4 and 5 are quite difficult to contextualize as is. Space can certainly be found to present such figures, if not in at least in the appendix?\n\n3. In your setting, an assumption seems to be that the true features are a subset of the chosen features? Can you comment on what would happen if you incorporate a \u201cmisspecification\u201d term into the analysis, i.e. where some true features are omitted?\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "While some additional experimental verifications (or rather illustrations) would improve the absorbability of this work, I believe that this work makes solid theoretical contributions to our understanding of meta-learning, and therefore I would recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4172/Reviewer_Akh7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4172/Reviewer_Akh7"
        ]
    },
    {
        "id": "ni1xVRVGQNa",
        "original": null,
        "number": 3,
        "cdate": 1666583892869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583892869,
        "tmdate": 1670012608450,
        "tddate": null,
        "forum": "Jifob4dSh99",
        "replyto": "Jifob4dSh99",
        "invitation": "ICLR.cc/2023/Conference/Paper4172/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper gives the test risk guarantee on the minimum $\\ell_2$ norm zero-loss solution (via combination of Lemma 1 and Theorem 1), which is where overparameterized model converges to via SGD. The bound, when simplified, leads to several interesting observations such as double descent via task diversity. Authors also provide results that support the relative tightness of the provided bound.",
            "strength_and_weaknesses": "__Strength__\n- The technical quality of the result is clear, and the proof seems to be correct (but I could be wrong, as did not have enough time to go over the full derivation line-by-line, especially Appendix G).\n- The writing is clear overall, stating the assumptions and the setting in a rigorous manner.\n- The paper gives a very detailed discussion on the implication of the bound.\n\n__Weakness__\n- An explicit head-to-head comparison with the prior work is much needed. Although the paper already gives some discussions regarding the prior work in the introduction (and against Bernacchia (2021) in p6), it is still not super clear to me why Theorem 1 is an improvement over the previous results other than Bernacchia's. I am curious how the bound compares with Chen et al. (2022), especially because the criticism is that their bound is \"not in an explicit form (due to unknown quantities)\" and \"tightness of the bound is unclear.\" Regarding the former criticism, I think the same comment can be given to Theorem 1, as it contains the terms regarding the true model parameter $\\mathbf{w}_0$ and other constants from assumptions (which are usually not explicitly known but should be estimated). Perhaps including a big table for comparison, similar to that appearing in Chen et al., would be a good idea.\n\n- Related to the previous point, this paper considers a slightly different framework than previous works. Namely, the framework has a additional parameter $s$ to denote the true degree of freedom as opposed to the nominal dimensionality. I am curious how the provided result compares to the prior results when $s = p$ (please correct me if I missed something).\n\n- Tightness of the bound is part quite difficult to follow. Exactly how good is the approximation (e.g., the ratio between the upper and lower bound)? I recommend using cruder and simpler approximations to the weights regarding $p, m, n_v$ for the sake of clarity.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, especially considering that it is math-heavy, and is of high technical quality. The novelty of the problem considered and the proof technique is somewhat limited, but this should not be a reason for rejection.",
            "summary_of_the_review": "The contribution of the paper is not very clear in comparison with the prior work; a head-to-head comparison is much needed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4172/Reviewer_LapN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4172/Reviewer_LapN"
        ]
    },
    {
        "id": "H_SRhy0JkE",
        "original": null,
        "number": 4,
        "cdate": 1666635125631,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635125631,
        "tmdate": 1670034871304,
        "tddate": null,
        "forum": "Jifob4dSh99",
        "replyto": "Jifob4dSh99",
        "invitation": "ICLR.cc/2023/Conference/Paper4172/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studied benign overfitting and double descent phenomenon in Meta-learning. Their analysis is based on linear regression with Gaussian features. In particular, they characterize the descent curve of the model error for the overfitted min l2-norm solution and show the phenomenon would be quite different compared to the single task (non-meta learning) setting. ",
            "strength_and_weaknesses": "It is interesting to check whether benign overfitting and double descent phenomena exist in Meta-learning. The theoretical result is supported by comprehensive and technically strong proof. However, more evidence is needed to see whether such a finding also exists in a real application. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-ordered, and the message delivered is clear. I didn't see the supplementary material, so I am not sure whether the experiments are reproducible.",
            "summary_of_the_review": "This paper studied benign overfitting and double descent phenomenon in Meta-learning. I think this paper is well written, but currently, I can only give a weak acceptance because I have the following concerns, \n\nI am very interested in the messages delivered in section 3.2. I am convinced that such phenomena exist in the synthetic data distribution, but do they also exist for NNs in real applications?  Some experiments for real data or relevant literature may help a lot. \n\nOne of the most significant double descent phenomena for a single task is for epochs (iterations). What will happen for Meta-learning? The authors may want to add some comments.\n\nIn this paper, the authors studied the train-validation split method. What will happen for the train-train split method? Will the main results change, or remain the same?\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4172/Reviewer_3kzv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4172/Reviewer_3kzv"
        ]
    }
]