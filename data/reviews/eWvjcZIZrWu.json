[
    {
        "id": "BssZAU6QTg",
        "original": null,
        "number": 1,
        "cdate": 1666376910209,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666376910209,
        "tmdate": 1666376910209,
        "tddate": null,
        "forum": "eWvjcZIZrWu",
        "replyto": "eWvjcZIZrWu",
        "invitation": "ICLR.cc/2023/Conference/Paper4343/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes enhancing SVGD using importance weights which scales the SVGD update directions by $(\\pi / \\rho_t)^\\beta$. The resulting $\\beta$-SVGD is then shown to have an exponential convergence rate in terms of 2-Renyi divergence under a Stein Poincare inequality assumption when $\\beta = 1$ and another descent lemma is proved for $\\beta \\in (-1, 0)$ under quite a few assumptions. \n",
            "strength_and_weaknesses": "## Strengths:\n* Applying importance reweighting to the popular SVGD is a meaningful task that is well-motivated.\n* The resulting $\\beta$-SVGD algorithm is simple and clean.\n* Theoretical analyses demonstrate nice convergence properties of $\\beta$-SVGD compared to SVGD.\n\n\n## Weaknesses:\n* It is unclear to me what the optimal $\\beta$ is and the theories in Section 3 and Section 4 seem to suggest the opposite of each other. In Theorem 2 we take $\\beta = 1$ to obtain exponential convergence under the Stein Poincare inequality, while in Proposition 1 we need $\\beta \\in (-1, 0)$.\n* The selling point of the main Theorem 1 is that with $\\beta \\neq 0$ there is no dependence on $D_{KL}(\\rho_0 || \\pi)$. However, I do not consider (14) to be a strong result due to the minimization over $t \\in [0,T]$ on the left side. As Korba et al. (2020) commented on their Proposition 3 (which is straightforward from their (12)), \"the convergence of $I_{Stein}(\\mu_t | \\pi)$ itself can be arbitrarily slow\" unless with more assumptions. \n* It is also unclear to me whether the proofs contain novel techniques or are simple adaptations of existing techniques, especially given a large number of assumptions. As mentioned in the paragraph before Remark 1, the importance weights $(\\pi / \\rho_t)^\\beta$ is to cancel exactly the inverse term appearing in the Wasserstein gradient of $(\\beta+1)$-Renyi divergence. Are there other insights in the proofs other than this kind of cancellation which seems by design?\n* The algorithm is not very practical in my opinion, due to the presence of $\\pi/\\rho_t$ in the importance weights. In many applications (e.g. Bayesian inference with lots of samples), $\\pi$ can be numerically insignificant whereas only $\\log \\pi$ is numerically reliable. Furthermore, the authors claim we can ignore the normalizing constant in $\\pi$, yet this constant can change the step size drastically and must be addressed (this is not a problem of SVGD which only uses the score). Additionally, as already observed by the authors, $\\rho_t$ can be too small when using kernel density estimation and can cause overflow --- a rather ad-hoc clipping trick is adopted but I doubt its effectiveness. A more practical reweighting scheme is by Xu et al. [1] and it should be compared to.\n* The biggest weakness of the paper, to me, is that the paper has not demonstrated the effectiveness of the algorithm in any applications, despite the simple form of the algorithm. An extremely simple 1-D example is done in Figure 1 and in the appendix, and even then the proposed method does not seem to work well (in Figure 1 the green line is quite off from the target blue line). The authors wrote \"more complicated models on high dimension space are out of our interest and ability\" in the appendix, but such a statement is irresponsible. At the very least the authors should test on multivariate Gaussians in dimension 2 and above, and compute metrics (energy distance or Wasserstein distance) to evaluate the sample quality.\n\n\n[1] Xu, L., Korba, A. &amp; Slepcev, D.. (2022). Accurate Quantization of Measures via Interacting Particle-based Optimization. <i>Proceedings of the 39th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 162:24576-24595 Available from https://proceedings.mlr.press/v162/xu22d.html.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and is easy to read. I do not find source code but the amount of experiments is also very minimal.\n",
            "summary_of_the_review": "Overall I think the proposed algorithm is well-motivated but its effectiveness has not been demonstrated, both in theory and in practice. Hence I'm leaning toward rejection.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4343/Reviewer_Nor5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4343/Reviewer_Nor5"
        ]
    },
    {
        "id": "E136vvJS0zd",
        "original": null,
        "number": 2,
        "cdate": 1666484235886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666484235886,
        "tmdate": 1669228241130,
        "tddate": null,
        "forum": "eWvjcZIZrWu",
        "replyto": "eWvjcZIZrWu",
        "invitation": "ICLR.cc/2023/Conference/Paper4343/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies a generalization of Stein variational gradient descent by a particular preconditioner function (weight function). The objective is the same as the KL divergence. By assumptions, they show the convergence result. They also demonstrate the effectiveness of the method in simple numerical examples.  ",
            "strength_and_weaknesses": "Strength: The preconditioner function (importance weights) is an interesting idea. \n\nWeaknesses: \n\n1. Please do not use the red color in the abstract for the KL divergence. \n\n2. The authors miss much important literature in modified Wasserstein gradient flows. Please discuss them in the literature. \n\n(1) A. Garbuno-Inigo, F. Hoffman, W. Li, A. Stuart, Interacting Langevin Diffusions: Gradient Structure And Ensemble Kalman Sampler, 2019. \n(2) W.C. Li, L.X. Ying. Hessian transport gradient flows. Research in the Mathematical Sciences, 2019.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper is in the middle. Under the assumption, many convergence results can be formulated directly. \nI think the interesting part is either the analytical or numerical implementation of the methods. More numerical results are excepted. ",
            "summary_of_the_review": "The paper is overall written well with clear mathematics. However, much-related literature and many machine learning-related numerical examples are missing. I strongly suggest authors revise the paper accordingly. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4343/Reviewer_xAMu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4343/Reviewer_xAMu"
        ]
    },
    {
        "id": "N9bbKdw-Ip",
        "original": null,
        "number": 3,
        "cdate": 1666494191021,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666494191021,
        "tmdate": 1666494191021,
        "tddate": null,
        "forum": "eWvjcZIZrWu",
        "replyto": "eWvjcZIZrWu",
        "invitation": "ICLR.cc/2023/Conference/Paper4343/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThe paper proposes and studies an algorithm, $\\beta$-SVGD, which modifies the SVGD by multiplying a factor $-(\\frac{\\pi}{\\rho_t})^\\beta$ to the vector field. The main result is that when $\\beta\\in(-1,0)$, the convergence of the Stein Fisher information does not depend on the target $\\pi$ or the initialization $\\rho_0$. ",
            "strength_and_weaknesses": "**Strength**\n\n- The paper proves the convergence of the proposed algorithm.\n\n**Weakness**\n- In order to compute the weight $\\pi/\\rho_t$, the paper proposes to use a kernel density regression to estimate $\\rho_t$. However, the number of samples needed by kernel density regression grows exponentially with dimension. So the algorithm won't apply in high dimensions.\n\n- The paper only has toy examples (1-dimensional 2-Gaussian mixture) in the experiments. SVGD is expected to work for much more challenging problems, not for such simple tasks.\n\n- The mathematical assumptions needed for the proof are too strong. For example, for Assumption 4 to hold for some uniform bound M, it essentially requires that $\\pi$ and $\\rho_0$ cannot be very different.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly. \n\nSome notations are confusing. \n- Equation 19: $ \\left(\\frac{\\rho_n}{\\pi}\\right)^\\beta(x)\\left(\\frac{\\pi}{\\rho_n}\\right)^\\beta \\wedge M_{\\rho_n}(\\delta) $ should be $\\left(\\frac{\\rho_n}{\\pi}\\right)^\\beta(x) \\left[ \\left(\\frac{\\pi}{\\rho_n}\\right)^\\beta \\wedge M_{\\rho_n}(\\delta) \\right] $.\n\n- Assumption 5 $\\nabla\\left( \\frac{\\pi}{\\rho_n} \\right)^\\beta$: it is unclear whether the power is outside or inside the gradient operator.",
            "summary_of_the_review": "The paper proposes $\\beta$-SVGD but is unable to show that the algorithm works in higher dimensions or more complex problems. \nThe convergence rate proof depends on unrealistic assumptions. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4343/Reviewer_1o9E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4343/Reviewer_1o9E"
        ]
    }
]