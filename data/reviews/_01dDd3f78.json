[
    {
        "id": "kTgdWT2s8lt",
        "original": null,
        "number": 1,
        "cdate": 1666290301871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666290301871,
        "tmdate": 1666290301871,
        "tddate": null,
        "forum": "_01dDd3f78",
        "replyto": "_01dDd3f78",
        "invitation": "ICLR.cc/2023/Conference/Paper3263/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a novel concept-based explanation method: Concept Gradient (CG). In contrast to previous work, they use non-linear concept functions and show how the standard Concept Activation Vector (CAV) approach can be generalized to this setting.\nExperimental results show superior performance of CG over CAV.",
            "strength_and_weaknesses": "### Strengths\nThe idea is convincing and the theoretical motivation seems sound. \nI do appreciate the toy examples that highlight where CAV might fail although it does not seem relevant in practice.\nExperiments span different data sets and classifiers and a comparison to CAV shows significantly better performance of the proposed CG method, especially for global explanations.\n\n### Weaknesses\nThe local concept recalls in figure 1 do not look that convincing for CAV or CG. Table 4 could include CAV values for comparison. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe paper is relatively well written, although there are quite a few typos/articles missing which reduces the readability a bit and could be easily avoided.\nFor example\n- in the abstract: and performed case study on a $ \\rightarrow $ and performed a case study on a\n- in 1: though there are an infinite number of functions exist $\\rightarrow$ though there are an infinite number of functions \n- in 3.4: we can directly constraint c within the $ \\rightarrow $ we can directly constrain c within the\n- in 3.4: Despite infinite number of choices $ \\rightarrow $ Despite the/an infinite number of choices\n\nMotivation and approach are clearly outlined.\n\n### Quality\nTheoretical motivation, derivations, and experiments are all of good quality.\n\n### Novelty\nThe authors extend an already existing method and show its superior mathematical motivation and better experimental results.\n\n### Reproducibility\nDetails for experiments are stated in the appendix but there is no code being published.",
            "summary_of_the_review": "Overall I think the paper presents a nice contribution and I would lean toward publication at ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3263/Reviewer_txu2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3263/Reviewer_txu2"
        ]
    },
    {
        "id": "96t3UOix9_G",
        "original": null,
        "number": 2,
        "cdate": 1666344198443,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666344198443,
        "tmdate": 1666344198443,
        "tddate": null,
        "forum": "_01dDd3f78",
        "replyto": "_01dDd3f78",
        "invitation": "ICLR.cc/2023/Conference/Paper3263/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a concept-based explanation called Concept Gradient (CG), which is a non-linear extension of Concept Activation Vector (CAV).\nThe idea of CG is as follows.\nSuppose the outcome of the model $f$ is given as $y = f(x)$.\nSimilarity, there is another model $g$ predicting the concepts $c = g(x)$.\nThe authors then proposed to use the sensitivity of the outcome with respect to the concept change $\\frac{\\partial y}{\\partial c}$ as the contribution of the concept $c$ to the model's outcome $y$ for the input $x$.\nTo compute $\\frac{\\partial y}{\\partial c}$, the authors introduced chain rule $\\frac{\\partial y}{\\partial c} = \\frac{\\partial x}{\\partial c} \\frac{\\partial y}{\\partial x} \\approx \\left(\\frac{\\partial c}{\\partial x}\\right)^\\dagger \\frac{\\partial y}{\\partial x}$ where $\\dagger$ denotes pseudo-inverse.\nIn particular, the authors reported that treating each individual concept separately induced better results.\nIn such a case, we have $\\frac{\\partial y}{\\partial c} \\approx \\frac{1}{\\left\\|\\frac{\\partial c}{\\partial x}\\right\\|^2} \\frac{\\partial c}{\\partial x}^\\top \\frac{\\partial y}{\\partial x}$.\nIn the experiments, the authors reported that CG is better at identifying true relevant concepts compared to CAV.",
            "strength_and_weaknesses": "### Strength\nThe proposed score $\\frac{1}{\\left\\|\\frac{\\partial c}{\\partial x}\\right\\|^2} \\frac{\\partial c}{\\partial x}^\\top \\frac{\\partial y}{\\partial x}$ seems to be a reasonable one for quantifying the correlation between the model's outcome and the concept.\nEssentially, $\\frac{\\partial c}{\\partial x}^\\top \\frac{\\partial y}{\\partial x}$ is away from zero when both $y = f(x)$ and $c = g(x)$ changes simultaneously for moving $x \\to x + \\Delta x$ for a certain choice of $\\Delta x$.\nThis is intuitive in a sense that moving $x$ to a direction changing $y$ also changes the strength of the concept $c$ simultaneously, indicating that $y$ and $c$ are correlated.\n\n### Weakness\nThe essential weakness of the paper is on the justification of the score $\\frac{1}{\\left\\|\\frac{\\partial c}{\\partial x}\\right\\|^2} \\frac{\\partial c}{\\partial x}^\\top \\frac{\\partial y}{\\partial x}$.\nIn the paper, the authors first adopted the sensitivity $\\frac{\\partial y}{\\partial c}$ as the ideal ground truth, and then derived its estimate $\\frac{1}{\\left\\|\\frac{\\partial c}{\\partial x}\\right\\|^2} \\frac{\\partial c}{\\partial x}^\\top \\frac{\\partial y}{\\partial x}$ by using pseudo-inverse.\nThis justification is questionable in two ways.\nFirst, $\\frac{\\partial y}{\\partial c}$ is not well-defined in general unless $g$ has inverse mapping.\nSecond, the choice of pseudo-inverse has a large degree of freedom.\nIn the paper, the authors adopted the pseudo-inverse with the minimum norm, i.e., Moore-Penrose inverse.\nIt is not very clear to me whether these questionable steps are required to justify the score $\\frac{1}{\\left\\|\\frac{\\partial c}{\\partial x}\\right\\|^2} \\frac{\\partial c}{\\partial x}^\\top \\frac{\\partial y}{\\partial x}$.\nIndeed, as the authors reported in the results, treating multiple concepts together leads to inferior performance.\nThis finding implies that the form of the inner product $\\frac{\\partial c}{\\partial x}^\\top \\frac{\\partial y}{\\partial x}$ will be a more appropriate expression than $\\left(\\frac{\\partial c}{\\partial x}\\right)^\\dagger \\frac{\\partial y}{\\partial x}$ obtained from these questionable steps.\nAs I pointed out in Strength, it would be more natural to interpret the score as a (scaled) correlation between $f$ and $g$.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe paper is written clearly and the main idea is easy to follow.\n\n### Quality, Novelty\nThe proposed score based on the inner product $\\frac{\\partial c}{\\partial x}^\\top \\frac{\\partial y}{\\partial x}$ will be the main novelty of the paper.\nHowever, its justification based on chain rule and pseudo-inverse is questionable.\nTo me, there is a more natural way to interpret the score as a (scaled) correlation between $f$ and $g$.\n\n### Reproducibility\nThe authors provided the experimental details in Appendix.",
            "summary_of_the_review": "I think the proposed score based on the inner product $\\frac{\\partial c}{\\partial x}^\\top \\frac{\\partial y}{\\partial x}$ is novel and will be effective in practice.\nHowever, its justification is questionable.\nThere may be some other ways to justify the score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3263/Reviewer_UPnS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3263/Reviewer_UPnS"
        ]
    },
    {
        "id": "fHxcTPePQ6i",
        "original": null,
        "number": 3,
        "cdate": 1666689199008,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689199008,
        "tmdate": 1666689199008,
        "tddate": null,
        "forum": "_01dDd3f78",
        "replyto": "_01dDd3f78",
        "invitation": "ICLR.cc/2023/Conference/Paper3263/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The work extends the CAV approach for concept-based explainability to\ndeal with concepts that cannot be expressed as linear combinations of\ninput (or latent) features. \n",
            "strength_and_weaknesses": "PROs\n\n- The work addresses a clear limitation of the CAV approach, that severely affects its practical applicability.\n\n- The derivation is sound\n\n_ The experimental evaluation is extensive and convincingly supports the advantage of the approach wrt to CAV.\n\nCONs\n\n- A critique one could make to this line of research is that if\n  concepts are available, one should aim at directly enforcing models\n  to use them (the concept-based learning models) rather than testing\n  their alignment post-hoc. \n\n- The complete version of the approach, jointly computing concept\n  gradient for all concepts, seems to provide worse results in terms\n  of alignment with concept importance as assessed by humans. To me\n  this again hints at a limitation of this research line (see the\n  summary of the review).",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clearly written, with well explained concepts and clarifying examples. \n\nThere is a minor inaccuracy in the notation, as at the end of page 3, Vc represents both g(x) and its gradient.",
            "summary_of_the_review": "Overall, I think this is a reasonable contribution to the XAI field,\nfixing a clear deficiency of existing solutions for concept-based\npost-hoc explanations.  My main concern is on the limitations of\nconcept-based post-hoc explanation itself. However I do not think this\nis a reason to disregard this research line and this work in\nparticular.  Rather, I would like the authors to discuss in more\ndetail the pros and cons of post-hoc interpretation with respect to\nconcept-based models. The authors did briefly mention this in the\nrelated work section, but I think a deeper analysis is needed. For\ninstance, the suboptimal performance of concept gradient computed\njointly over concepts hints at the lack of orthogonality /\ndisentanglement between \"concepts\" learned by the network, something\nthat the research on concept-based models is actively addressing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3263/Reviewer_zoZR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3263/Reviewer_zoZR"
        ]
    },
    {
        "id": "IQnhmgHLGRu",
        "original": null,
        "number": 4,
        "cdate": 1666748758498,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666748758498,
        "tmdate": 1666748758498,
        "tddate": null,
        "forum": "_01dDd3f78",
        "replyto": "_01dDd3f78",
        "invitation": "ICLR.cc/2023/Conference/Paper3263/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper propose a method for explaining intermediate activations of a neural network using concepts. In a nutshell, f(x) and g(x) are the networks that take activations x from an intermediate layer as input and respectively predict class and concept. The relevance of a concept c to a class y is measured using the proposed Concept Gradient (CG) score, which is based on the gradient of f and g with respect to x. CG score is calculated as normalized dot product of the two gradients. The proposed method is compared with TCAV (Kim 2018) on two datasets (CUB and AWA). The proposed method is also applied to a third dataset but the results are not compared with TCAV or any other method. ",
            "strength_and_weaknesses": "+The method is simple. \n+The provided formulations about gradient of class with respect to concept are insightful. Although, eventually it all reduces to a simple dot product of two gradient vectors. \n\n\n-Why finetuning the same network for g? Specially for deeper layers, g becomes very shallow and simple. Why should one not use a more complex model for g?\n\n-Concept accuracy in Table 1 and 2 are very close suggesting that a linear model is not as bad as one may think (0.72 vs 0.79 in Res50). Which layer is used for generating the results in Table 1 and 2? \n\n-TCAV does not simply use directional derivative which is analogous to the proposed local CG. Instead, they define TCAV_Q as \u201cthe fraction of k-class inputs whose l-layer activation vector was positively influenced by concept C\u201d. In contrast the proposed global CG is defined based on sum of sign(local CG). What is the advantage of this definition over the definition of TCAV_Q score?  \n\n-TCAV performs a statistical significance testing and uses only the results that pass this test (Kim 2018, section 3.5). Did you do this step? \n\n-When comparing to TCAV, did you rank the concepts according to TCAV_Q score from \u201crelative\u201d CAV (Kim 2018, section 3.6) which is the recommended way of using the method when assessing relative importance of multiple concepts? Please clarify these details in the paper.\n\n-Simply using gradient to measure feature importance is widely admitted to be misleading and that\u2019s why many gradient-based methods have been trying to generating more faithful saliency maps (Adebayo 2018). CG score is purely based on gradient. Does CG suffer from similar issues? \n See Adebayo , et al Sanity Checks for Saliency Maps, Neurips 2018.\n\n-In the experiments on the third dataset about mortality risk, have you had the results reviewed by a medical expert? Since I, and likely other reviewers, are unable to comment on the validity of these results, I was hoping you could provide TCAV_Q scores too. Having both CG and TCAV_Q scores side by side would enable us to at least see if there are any disagreement between the two method and if CG has any advantage over TCAV_Q on this dataset.\n\n- CG is defined to measure how small perturbations on each concept affect the label prediction. CG essentially is dot product of the two gradient vector (after a normalization which is shown not to be important). The dot product is a symmetric operation. So it also measures how small perturbations on a predicted class affect the concept prediction. Right? \n\nTypo: Pp 15 Appendix D para. 2 Fig ??\n",
            "clarity,_quality,_novelty_and_reproducibility": "For the most part, the methodology is clear. \nThe provided formulations about gradient of class with respect to concept is insightful and novel.\nI have some concerns that I hope the authors can address.",
            "summary_of_the_review": "I have some questions that I hope the authors can clarify.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3263/Reviewer_pw8j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3263/Reviewer_pw8j"
        ]
    }
]