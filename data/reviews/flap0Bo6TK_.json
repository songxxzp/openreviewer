[
    {
        "id": "ThWPxK-r4HP",
        "original": null,
        "number": 1,
        "cdate": 1666551383585,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666551383585,
        "tmdate": 1666551383585,
        "tddate": null,
        "forum": "flap0Bo6TK_",
        "replyto": "flap0Bo6TK_",
        "invitation": "ICLR.cc/2023/Conference/Paper3331/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper derives a GP model as a GCN with layer widths increased to infinity. To make the prediction more scalable, a low-rank approximation approach is proposed. Experiments are conducted to compare with other baseline models and show the GP model\u2019s running time and scalability advantages.",
            "strength_and_weaknesses": "Strength:\n\n1. The designed GP covariance kernel is able to capture the geometry information of the graph.\n2. The runtime of the designed GP is much faster than standard GCN, especially using the low-rank version. Also, its performance on all datasets is better than GCN.\n3. For most of the operations in GNNs, operational counterparts for the GP exist. Therefore, the proposed GP model can be extended to many other GNNs.\n\nWeaknesses:\n\n1. It is interesting to see that GP versions of many SOTA GNNs achieve comparable performance with respect to their original models. However, this is only done in one dataset. It would be great if the authors can also have this comparison on other datasets in Table 4.\n\n\nQuestions:\n1. How is $d_0$ defined in $C^{(0)}$? \n2. For usual GPs, their hyperparameters (typically in the prior mean function and the covariance function) can be obtained by optimizing the log-marginal likelihood. In the proposed GP, how can these hyperparameters be optimized?\n3. Can the proposed GP model be extended to the GAT family?\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is written clearly.\n2. The code of the proposed model is not uploaded. Given the designed efficient implementation and the proposed GP model's potential influence in GNNs, it is encouraged to have the code public so that others can study or reproduce it.\n",
            "summary_of_the_review": "The idea of deriving a GP model for infinitely wide GCN is novel. Also, the GP model can be extended to a wide range of GNNs, which shows its generality.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3331/Reviewer_SNQW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3331/Reviewer_SNQW"
        ]
    },
    {
        "id": "CNkoR7KiRP",
        "original": null,
        "number": 2,
        "cdate": 1666596647504,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666596647504,
        "tmdate": 1666596647504,
        "tddate": null,
        "forum": "flap0Bo6TK_",
        "replyto": "flap0Bo6TK_",
        "invitation": "ICLR.cc/2023/Conference/Paper3331/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper defines GCN-inspired kernels by deriving GP as the limit of GCN. The authors present some theoretical results such as universality and develop a scalable posterior inference method based on low-rank approximation of covariance matrix. The effectiveness of the proposed method is demonstrated using multiple bench mark datasets.",
            "strength_and_weaknesses": "S1. The authors derive GCN-inspired kernels by defining GP as the limit of GCN.\n\nS2. The theoretical guarantees such as universality are thoroughly discussed.\n\nS3. A practical inference algorithm is also presented, whose effectiveness is verified on real-world datasets.\n\nW1. There is no discussion of important related studies, graph Gaussian processes [R1]. Can you please explain in detail the relationship with the proposed method? Please add to the baseline method if necessary.\n\n[R1] Yin Cheng Ng et. al., Bayesian Semi-supervised Learning with Graph Gaussian Processes, NeurIPS, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "- The manuscript is readable.\n- Technically sound.\n- The proposed formulation seems novel, but some related works are missing.",
            "summary_of_the_review": "Connection of GP and GCN is interesting. The proposed formulation is reasonable and the approximation method is beneficial. But discussion about related works (graph Gaussian processes) are missing. Then, my opinion is \"weak accept\" for now.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3331/Reviewer_okj8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3331/Reviewer_okj8"
        ]
    },
    {
        "id": "_OsrwFBmbM",
        "original": null,
        "number": 3,
        "cdate": 1666671639494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671639494,
        "tmdate": 1666708921764,
        "tddate": null,
        "forum": "flap0Bo6TK_",
        "replyto": "flap0Bo6TK_",
        "invitation": "ICLR.cc/2023/Conference/Paper3331/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The goal of this study is to attack the semi-supervised learning problem for graph-structured data. The paper focus on bridging the gap between GNNs and the limiting GPs by deriving the covariance kernel that incorporates the graph inductive bias as GNNs do.  More specifically, the paper has shown that the GP can be derived as a limit of the GCN when the layer widths tend to infinity and demonstrated the kernel universality and the limiting behavior in depth. In addition, the paper also develops an efficient computational algorithm for posterior inference on large-scale data.",
            "strength_and_weaknesses": "Overall, the paper is well motivated. The use of a real data set is a huge strength of the paper. In particular, I would also like to commend the authors on the crystal clear exposition and writing throughout the paper. The proposed method and theory are clearly presented and easy to follow. \n\nMy main concern is the originality of establishing connection between GCNs and GPs. I have seen a number of previous studies which reached a similar conclusion. For example:\n- https://arxiv.org/pdf/1711.00165.pdf\n- https://arxiv.org/pdf/2002.12168.pdf\n- https://proceedings.neurips.cc/paper/2019/hash/5e69fda38cda2060819766569fd93aa5-Abstract.html\n\nIt would be better for the authors to clarify the differences and highlight their main theoretical & methodological contributions compared to the existing studies. Also, the low rank approximation for scalable computation hinges on the Nystrom approximation of matrix K, which is widely-used in the GP literature. Therefore, in my opinion, the algorithmic contributions are incremental (or limited).\n\nGiven that I believe the authors would need to effectively address the above questions and the totality and generality of the contributions to be not significant enough to warrant publication at this stage.\nI hope the authors may find these high-level comments helpful in thinking about the next steps for their paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is cleary written and well presented. ",
            "summary_of_the_review": "See above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3331/Reviewer_XBR8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3331/Reviewer_XBR8"
        ]
    },
    {
        "id": "UEg9ZjM9We",
        "original": null,
        "number": 4,
        "cdate": 1666856369980,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666856369980,
        "tmdate": 1666856369980,
        "tddate": null,
        "forum": "flap0Bo6TK_",
        "replyto": "flap0Bo6TK_",
        "invitation": "ICLR.cc/2023/Conference/Paper3331/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the graph neural network structure is integrated into the GP framework to construct a kernel function with graph structure, and the low-rank approximation method is used to deduce the efficient training and inference method of GP, aiming to improve the performance and efficiency of graph structure data classification and regression.",
            "strength_and_weaknesses": "Strength:\n\n1. The idea is interesting and somewhat novel.\n2. The manuscript is clear and easy to read.\n3. The experiments validate the effectiveness of the proposed method.\n\nWeaknesses:\n\n1. Why does integrating graph neural network structure into GP framework improve the performance compared with GNN? In my opinion, one of the main advantages of GP is the use of kernel functions like RBF, which can approximate the nodes of infinite element network, and the kernel functions used here are still based on the inner product of mapping functions.\n\n2. The authors emphasize that the proposed method is suitable for semi-supervised scenarios, but does not give the reason and the specific algorithm of semi-supervised learning. If it is supervised learning, whether the proposed method still has the same advantage?\n\n3. Can the authors explain the trend of the curves in Figure 3?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well-organized. The techniques are solid and correct.",
            "summary_of_the_review": "A novel idea and implementation of combing GP and graph neural network.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3331/Reviewer_svk4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3331/Reviewer_svk4"
        ]
    }
]