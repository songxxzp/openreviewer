[
    {
        "id": "jKAKfiCKDd",
        "original": null,
        "number": 1,
        "cdate": 1665940486983,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665940486983,
        "tmdate": 1665940486983,
        "tddate": null,
        "forum": "ZTCxT2t2Ru",
        "replyto": "ZTCxT2t2Ru",
        "invitation": "ICLR.cc/2023/Conference/Paper3503/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents the first approach for retrieving code documentation for code generation as a way of addressing the fact that APIs are constantly changing. They present a general strategy entailing a retriever (sparse-BM25 or dense-large pretrained model) which first retrieves a set of relevant documents from a pool of documents (which does not need to be fixed). Then, the retrieved documents and concatenated with the NL intent to form the prompt that is fed into a generation model for code generation. They evaluate a number of different generation models, including T5, CodeT5, GPT-Neo, and Codex. For evaluation, they curate two different datasets, by ensuring disjoint training/validation/test splits, spanning two different programming languages: bash commands (tldr) and Python (re-splitting CoNaLa). Their results show that in general, their DocPrompting approach tends to achieve superior performance over simply prompting with the NL intent alone. \n\nContributions:\n- First paper to consider retrieving documentation for code generation\n- Two curated datasets that can be useful for future work\n- Extensive empirical experiments and ablation studies demonstrating the utility of their approach\n",
            "strength_and_weaknesses": "Strengths:\n- The motivation and approach is well-grounded\n- There are extensive results, spanning many different metrics. The Recall_unseen metric is particularly useful in demonstrating how much the prompting could help in a realistic scenario in which models are tested when new API changes are made on unseen functions.\n- The analysis in Section 6 is very thorough, providing possible explanations for a number of different observations that are made.\n\nWeaknesses:\n- Adding retrieval will likely increase latency. The authors do not discuss/report how this is affected. As the size of the document pool increases, it seems that model latency will also increase.\n- While the reason for splitting paragraphs into individual documents is explained in the Appendix, it seems odd to treat them as independent documents. For example, in Figure 2, the two bottom documents cannot standalone without the top one.\n- It is not clear how k is selected for the number of documents to retrieve. Is it always 3?\n- The annotation process for writing the test cases is not clear. In Appendix B, it is written that the two first authors did this. It is not clear whether each example was annotated by both of them and if so, what the agreement on that was.\n- Missing reference: https://arxiv.org/pdf/1808.09588.pdf\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: Most things are explained clearly either in the main paper or the appendix, except for the few points that are raised in the Weaknesses above.\n- Quality: Well-designed and experiments and well-written work.\n- Novelty: There is novelty in being the first paper to retrieve documentation for code generation and also in terms of the two newly curated datasets. However, in terms of technical novelty, already existing models and techniques are used for retrieval and generation.\n- Reproducibility: Model training details are provided in the Appendix, which should be sufficient for reproducing these results, once the datasets are made available.\n",
            "summary_of_the_review": "While the technical novelty may be limited, there is novelty in terms of application and benchmark creation which will be useful for future work. The experiments and empirical results are thorough. Overall, I feel that this is a good paper contributing useful ideas and artifacts to the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3503/Reviewer_k6bL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3503/Reviewer_k6bL"
        ]
    },
    {
        "id": "yDIFtSzPJ5G",
        "original": null,
        "number": 2,
        "cdate": 1665961882823,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665961882823,
        "tmdate": 1668647144371,
        "tddate": null,
        "forum": "ZTCxT2t2Ru",
        "replyto": "ZTCxT2t2Ru",
        "invitation": "ICLR.cc/2023/Conference/Paper3503/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper describes an approach which uses documentation to generate code conditioned on a natural-language (NL) intent. The approach is essentially a standard retrieval-conditioned generation strategy, but where the set of documents being retrieved are constructed based on documentation.\n\nThe paper describes a set of practical instantiations of this technique, exploring a variety of retrieval models (sparse heuristic-based retrievers, and \"dense\" learned retrieval models trained to recover \"correct\" docs), and a few variants of generator architectures based on transformers, with either fine-tuning or few-shot prompting. The paper also introduces two datasets: a new benchmark for NL->bash prediction based on the `tldr` docs, and a variant of the CoNaLa benchmark with a different test split combined with a new reference corpus of documentation based on `devdocs.io`.\n\nThe authors show that retrieving documentation improves performance across all model types, and also study the effects of retrieving documentation vs code, providing some ground-truth knowledge, and using different retriever strategies.",
            "strength_and_weaknesses": "Strengths:\n\n- Using documentation as an extra input to generating code is a good idea for ensuring models generalize to new types of functions, and this paper confirms that this generalization is indeed enabled by this approach.\n- The authors compare their approach with a variety of alternatives and find a consistent improvement across experiment settings.\n- The new datasets/benchmarks are interesting and seem like a reasonable way to evaluate generalization to unseen types of function.\n\nWeaknesses:\n\n- ~~The novelty is primarily empirical in nature, and~~ the technical approach is essentially an application of known techniques. *(update: After reflection and discussion with the authors, I admit that there is novelty in the problem formulation as well, which goes beyond the setting considered by previous work.)*\n- ~~The introduction makes bold claims about \"all existing code generation models\" which I do not think are justified.~~ *(update: The unsupported claims have been qualified in the new revision.)*\n- ~~The notion of \"ground truth docs\" is a bit confusing.~~ *(update: The paper has been reworded to make this more clear.)*\n- It's possible there is some function-knowledge leakage due to the use of pretrained code models for retrieval (perhaps the authors can clarify this) *(update: I am still curious whether this influences the results, but it is a bit orthogonal to the main point of the paper, since it would also affect baselines. The authors have added a brief discussion of this potential leakage to the appendix.)*\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Novelty\n\n### Overly-strong claims about existing models not being able to generalize *(addressed by new revision)*\nThe introduction makes very strong claims:\n- \"all existing code generation models either learn directly from input-output pairs provided as training data ... or learn the mapping between input and output implicitly from naturally occuring corpora\"\n- \"all these works assume that all libraries and function calls were seen in the training data; and that at test time, the trained model will need to generate only seen libraries and function calls\"\n- \"Thus, existing models inherently cannot generalize\"\n\nThese claims seem much too strong. Although there has been a lot of work in the setting the authors discuss, there have also been a number of works that do not fit neatly into this categorization, and that allow models to generate code using new functions and libraries. For instance:\n\n- [Hashimoto et al. (2018)](https://proceedings.neurips.cc/paper/2018/file/cd17d3ce3b64f227987cd92cd701cc58-Paper.pdf) and [Lu et al. (2017)](https://arxiv.org/pdf/1705.09042.pdf) learn to retrieve from a dataset of examples at inference time, which could be distinct from those used at training time. Some of the other works the authors cite also do something similar (e.g. Parvez et al. (2021) and Pasupat et al. (2021))\n- [Shrivastava et al. (2021)](https://arxiv.org/abs/2206.12839) show that code generation can be improved by conditioning on context extracted from other relevant files, which may include new functions\n-  [Wu et al. (2022)](https://arxiv.org/pdf/2203.08913.pdf) use an external memory to allow transformers to memorize new data at training time and recall it later, and show that this allows it to predict newly-defined functions at test time.\n\n### Novelty of the approach and findings\nI am not aware of prior work which specifically retrieves documentation to generate code from natural language, although there has been prior work on both *retrieving relevant documentation* (e.g. section 5.6 of [this survey](https://dl.acm.org/doi/pdf/10.1145/3212695)) and on *retrieving existing code to generate code* (e.g. [Hashimoto et al. (2018)](https://proceedings.neurips.cc/paper/2018/file/cd17d3ce3b64f227987cd92cd701cc58-Paper.pdf)). \nAs such, the empirical results provided by the authors do provide new evidence that documentation helps for code generation across a number of model architectures and tasks (as one might expect).\nAdditionally, the tldr task, the resplit of CoNaLa, and the documentation pool from DevDocs are new contributions of this work, and might be useful for evaluating improved documentation-retrieval models in the future.\n\nBeyond this, the technique itself appears to be essentially the same as prior work: they use standard retrieval techniques based on existing algorithms and pre-trained models, and their generation models are also models that have been used for code or text generation before.\n\n## Clarity\nOverall, the paper is clearly written.\n\nOne thing I thought could be improved: Section 3.1 starts from the assumption that we are given triplets $(n, c, \\mathcal{D}_n^*)$ where $n$ is a natural-language intent, $c$ is the target code, and $\\mathcal{D}_n^*$ is the \"ground truth docs\" for $n$. I didn't think this was very well explained, since it's not obvious why $n$ would have ground truth documentation associated with it. From the rest of the paper and the appendix, it appears that $\\mathcal{D}_n^*$ is actually selected based on a heuristic that uses $c$, not $n$. I think it would be better to explicitly describe this process in section 3.1, e.g. by stating that we are given a pair $(n, c)$ and then we infer a set of reference docs $\\mathcal{D}_n^*$ from $c$. (I also wonder whether \"ground truth docs\" is the best terminology to use, since it's still being inferred heuristically and not manually observed or labeled.)  *(update: this has been addressed by the new revision.)*\n\nOther minor suggestions:  *(update: addressed by the new revision.)*\n\n- Section 4.2 states that ground-truth docs are constructed by \"matching the function name with its documentation\". However, there may be multiple function names in an example, which makes this statement ambiguous.\n- I found page 7 hard to read, since it has a table at the top and a figure in the middle, with ordinary text interleaved around it.\n- Figure 3 has overlapping text, and the axes are not labelled (I assume the x-axis is supposed to be $k$?)\n\n## Quality\nThe dataset setup and experiments, which make up the bulk of the contribution, appear to be high quality overall. The  tldr dataset is an interesting source of documentation for bash code, and re-splitting the CoNaLa dataset based on use of new functions is a good idea for assessing generalization ability.\n\nExperimentally, the authors do a good job of evaluating DocPrompting across a variety of metrics and for a variety of base models. They also conduct a series of interesting ablations and extensions, including using oracle access to the appropriate documentation, comparing to example-based prompting, and investigating different retrievers.\n\nOne potential concern is that the strongest results on CoNaLa are given by CodeT5, but CodeT5 was pretrained on a dataset of code that might include some of the \"held-out\" Python functions described in 4.2. This might be a source of knowledge leakage, since CodeT5-based retrievers might just pick documentation that includes functions that accomplish the task by simply memorizing the functions. It would be helpful to analyze whether this leakage might have occurred, and if so, how much of an effect it might have on the results in Tables 3 and 4. (Am I right in assuming all of the results in Table 3 used a CodeT5-based retriever?)  *(update: Authors have added some discussion of this in the appendix.)*\n\nAdditionally, I wasn't convinced by the argument in section 6.1. If I understand correctly, this section compares n-gram overlap between the natural language $n$ and code $c$, and also n-gram overlap between the \"ground-truth\" docs $\\mathcal{D}_n^*$ (along with $n$) and the code $c$. But, from the appendix, $\\mathcal{D}_n^*$ is selected exactly by finding the documentation with the highest string overlap with $c$! So it seems obvious that $\\mathcal{D}_n^*$ and $c$ would have high n-gram overlap, regardless of how much documentation does or doesn't \"ease the mapping between NL and code\". (A better experiment might be to look at n-gram overlap between $c$ and the *retrieved* examples from the model, using only information from $n$ instead of the oracle. It would also be interesting to look at overlap between the NL description and the documentation to make sure the gap is actually being \"bridged\" on both sides.)  *(update: The authors have taken my suggestion here and presented results using the retrieved documents.)*\n\n## Reproducibility\nThe technique is fairly straightforward, and the authors release their new benchmarks and datasets, so it seems likely this work will be reproducible.",
            "summary_of_the_review": "*Original review:* Using documentation as additional context for generating code is a sensible idea, and it is good to have a thorough empirical confirmation that this does in fact yield higher accuracy in the NL -> code setting. Thus, although the novelty of the technique itself is limited, and the findings aren't particularly surprising, I tend to lean toward acceptance (assuming the issues I mention regarding strength of claims, clarity, dataset leakage, and n-gram overlap can be resolved).\n\n*Updated review:* The authors have resolved my concerns, and upon reflection I do think that the problem formulation is interesting regardless of the novelty in the technique itself. I have increased my score from 6 to 8.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3503/Reviewer_6cUe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3503/Reviewer_6cUe"
        ]
    },
    {
        "id": "njHPw3iKxt1",
        "original": null,
        "number": 3,
        "cdate": 1666620656530,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620656530,
        "tmdate": 1666620656530,
        "tddate": null,
        "forum": "ZTCxT2t2Ru",
        "replyto": "ZTCxT2t2Ru",
        "invitation": "ICLR.cc/2023/Conference/Paper3503/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose DocPrompting for the problem of generating code from natural language instructions. DocPrompting supplies the model with in-context documentation and uses retrieval to find relevant documentation for a given input. The authors experiment with using BM25 and training dense encoders for retrieving documentation, and find consistent improvements with DocPrompting. The authors focus on evaluation settings where functions in the test set are not seen during training, and also introduce the new code generation dataset \u201ctldr\u201d.",
            "strength_and_weaknesses": "Strengths:\n* The paper has a clear motivation and is easy to follow.\n* The idea of document prompting is tested across a range of models and retrievers with promising results. The authors also compare DocPrompting to a baseline with in-context examples (ExPrompting) instead of in-context documentation (DocPrompting).\n* The n-gram analysis provides a simple account of how DocPrompting may bring benefits.\n\nWeaknesses:\n* I am unsure how fair the evaluation setting with unseen functions is for the baseline models, especially since the ExPrompting baseline cannot retrieve examples of unseen functions from the training set.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is of high quality and the paper is well written. The idea of retrieving documentation for prompting seems interesting and novel to me.",
            "summary_of_the_review": "This paper proposes the simple and intuitive idea of incorporating documentation into code generation tasks and finds that it brings empirical benefits, especially when generalizing to unseen functions. The paper is well executed and will likely lead to future work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3503/Reviewer_145E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3503/Reviewer_145E"
        ]
    },
    {
        "id": "Ng06Gpx119-",
        "original": null,
        "number": 4,
        "cdate": 1666625618849,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625618849,
        "tmdate": 1666625618849,
        "tddate": null,
        "forum": "ZTCxT2t2Ru",
        "replyto": "ZTCxT2t2Ru",
        "invitation": "ICLR.cc/2023/Conference/Paper3503/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper observes that existing models can only generate seen libraries and function calls at test time. However, new functions and libraries are introduced all the time, and even a seen function call can have arguments that were not used in the training data. To address such unseen functions in code generation, the authors propose a simple and effective approach for code generation by retrieving the relevant documentation, dubbed DocPrompting. The extensive experiments demonstrate that DocPrompting consistently improves NL->code models in two tasks, in two PLs, and across multiple strong base models. DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to 6.9% exact match, and Codex by 6.78 charBLEU score.",
            "strength_and_weaknesses": "Strengths:\n1. The idea of leveraging the newly added manuals and documentation to generate code containing unseen and unused functions and libraries is interesting and novel. The work demonstrates an exciting endeavor direction to generate high-quality code.\n2. The main results from Tables 1 & 3 demonstrate that the improvement of the proposed DocPrompting is significant across various pre-trained models. Conducting experiments on more than one benchmark and thorough ablations make the conclusion of this work convincing.\n3. The presentation of this work is excellent. The rich use of diagrams and tables makes this paper easy to understand.\n\nWeaknesses: \n1. Although the usage of documentation for code generation is novel, it is somewhat simple from a technical point of view.\n2. I examined the supplementary material provided by the authors and only found the data files. It would be better if there were codes to prove its reproducibility.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the Strength And Weaknesses",
            "summary_of_the_review": "In my opinion, the proposed DocPrompting method is interesting and novel, and the experimental results successfully demonstrated the effectiveness of the proposed method. Considering the technical simplicity and reproducibility, I give a weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3503/Reviewer_xC2U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3503/Reviewer_xC2U"
        ]
    }
]