[
    {
        "id": "MkR2_iN4swk",
        "original": null,
        "number": 1,
        "cdate": 1665993543615,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665993543615,
        "tmdate": 1668999105896,
        "tddate": null,
        "forum": "07tc5kKRIo",
        "replyto": "07tc5kKRIo",
        "invitation": "ICLR.cc/2023/Conference/Paper42/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies class imbalance and proposes that we should model the imbalance based on the semantic scale per class instead of the number of samples per class. Following this, this paper conducts a series of empirical studies to verify this idea and proposes a new method to handle class imbalance. Empirical results demonstrate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n1. The problem of class imbalance is important for practical applications.\n2. It is interesting and reasonable to rethink class imbalance beyond just the sample number per class.\n3. The formulated semantic scale is new to me.\n\nWeaknesses:\n1. The designed DSB loss relies not only on the semantic scale but also the inter-class interference. Therefore, it is unclear how much performance gains are from the semantic scale and how much is from the inter-class interference. Since the main innovation is the semantic scale, it is better to show its empirical effectiveness.\n2. In Sec 4.1, this paper argues that \"all methods that rebalance the loss function or adjust the sampling rate based on the sample number can be improved with the semantic scale, since both are natural measures and they are not model-dependent.\" This argument is not clear enough. First of all, the evaluation of the semantic scale and the inter-class interference are based on model features, so they are model-dependent. Moreover, in my view, combining the proposed reweighting loss and existing class re-balancing is not that easy, since they are competing with each other in reversing imbalance to some degree. Therefore, the combination pipeline should be carefully designed and the hyper-parameters should be carefully tuned; otherwise, the model performance may not be improved and even become worse.\n3. There are many re-weighting long-tailed approaches that do not rely on the number of samples, like Uncertainty-based margin learning [A], LOCE [B] and Domain balancing [C]. Please discuss some of them if you have not done so and also compare the proposed method with them to show superiority.\n[A] Striking the right balance with uncertainty. In CVPR 2019.\n[B] Exploring classification equilibrium in long-tailed object detection. In ICCV 2021.\n[C] Domain balancing: Face recognition on long-tailed domains. In CVPR 2020.\n4. Can the proposed semantic scale be used to guide re-sampling for long-tailed data? Whether is it better guidance than the sample number per class for class-balanced re-sampling?\n5. The performance of BS [39] on ImageNet-LT is not that bad. Many recent studies have verified this. Please recheck your implementation in the experiment.\n6. In Section 5.2, can you order the semantic scale of different classes, and group them into three groups? I wonder how many accuracies of different groups are improved. Whether are the classes with small semantic scales improved more?",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is easy to follow. \n- The quality is overall good but there are some issues should be resolved. \n- The novelty is fair. \n- The reproducibility: the source code is expected to be released.",
            "summary_of_the_review": "This paper is overall good, but there are several problems that need to be resolved. \n\n*******************\nPost-rebuttal: Thanks for the response and modifications. The quality of this paper is further improved now. However,  I still think the proposed method is a mix of different ideas, considering that inter-class interference is not that important. Empirical analysis and suggestions regarding inter-class interference are good, but it is not that elegant when designing them into one method. Even so, I appreciate the efforts of the authors and increase my rate to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper42/Reviewer_bNtG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper42/Reviewer_bNtG"
        ]
    },
    {
        "id": "wu7teKUhB3",
        "original": null,
        "number": 2,
        "cdate": 1666180248264,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666180248264,
        "tmdate": 1669900008727,
        "tddate": null,
        "forum": "07tc5kKRIo",
        "replyto": "07tc5kKRIo",
        "invitation": "ICLR.cc/2023/Conference/Paper42/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies long-tailed learning in the semantic level, which goes deeper beyond the conventional quantity bias. Specially, the authors explore to explain multiple phenomena by defining the semantic level imbalance and propose a measure by volume of manifold to reweight the learning objective, which has been demonstrate to improve the performance effectively.",
            "strength_and_weaknesses": "In summary, there are some positive points regarding this work, summarized as follows,\n\n(1) The topic is relatively novel and meaningful in the perspective of long-tailed learning, since the quantity bias is not the essential factor to decide the performance of imbalance learning. \n\n(2) The proposed measure for imbalance degree is effective, which is demonstrated by a range of validation on uniform datasets and long-tailed datasets. Specially, the trend of the curve is approximately similar to the final performance, which is heuristic to future extensions.\n\n(3) The proposed semantic-scale-based balanced loss has been demonstrated useful to a range of previous methods to improve the performance in both class-imbalanced and class-balanced cases.\n\nHowever, there are also some clear flaws in terms of the current submission, which needs to be further improved.\n\n(1) It is confusing that the authors discuss both the marginal effect and semantic-scale imbalance. Especially, the marginal effect is not the unique characteristic of imbalance learning, and also exists in the absolutely balanced learning. When the samples are sufficient to make the model reach the optimal, more samples might not bring gains. \n\n(2) In terms of the semantic-scale-based imbalance learning, it is the approximately same topic with the recent work termed as the generalized long-tailed learning [1], which takes into the fine-grained pattern imbalancedness into account. The authors missed this work and did not discuss their difference as well as the corresponding comparison in the experimental part. Especially, they have released a benchmark in the github repository, which can be used in this work.\n\n(3) The evaluation in the quantity-uniform datasets like CIFAR10 and CIFAR100 is ill-posed, since both the training and the test are IID. The improvement is not convincing to attribute to the imbalance learning, and conversely it is more possible to be explained as the effect of the hard negative mining by the re-weighting. Here, the authors need to carefully discuss the improvement in the IID instead of the Non-IID case about the training and the test. \n\n(4) There are some concerns about the experiments, since it seems that the authors do not demonstrate the model can achieve the better improvement on the basis of the state-of-the-art method like PaCO and LA, which have adjusted the model according to the quantity imbalance. Note that, RIDE is a multi-branch ensemble model, and it is meaningless to show the improvement on RIDE achieves the state-of-the-art, since PaCO can also integrate this mechanism to achieve better performance. It will be better to conduct some experiments to compared LA or PaCO and your method in the same backbone, which can show the gap of the improvement between the explicit quantity bias and the implicit semantic-scale bias.  Again, it is better to have some comparison in the benchmark of [1].\n\n[1] Invariant Feature Learning for Generalized Long-Tailed Classification. ECCV 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The topic is relative novel, since the work from the same topic is just conducted in the same year and in a few months ago.\n\n[Posthoc Response] \nThe code of this work can be open to promote the further explorations in this direction.",
            "summary_of_the_review": "In summary, the current work has both pros and cons. I advise the authors to carefully consider the advices in the review and I can consider to raise the score if the concerns have been well solved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper42/Reviewer_DeDZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper42/Reviewer_DeDZ"
        ]
    },
    {
        "id": "yGItNI61cCd",
        "original": null,
        "number": 3,
        "cdate": 1666279783776,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666279783776,
        "tmdate": 1669993441475,
        "tddate": null,
        "forum": "07tc5kKRIo",
        "replyto": "07tc5kKRIo",
        "invitation": "ICLR.cc/2023/Conference/Paper42/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new concept semantic scale, which conceptually captures the richness or diversity of the feature space, e.g., \u201cBird\u201d features should be richer than \u201cSwan\u201d features. This quantity is quantified by the volume of the learned feature space, proportional to the determinant of feature covariance matrix. Using this new concept, the paper attempts to explain 1) the change of performance improvement w.r.t the number of training data and 2) implicit semantic bias in classification when classes are balanced. Finally, based on the discrepancy in semantic scales, the paper proposes a dynamically weighted loss function to show improvement on classification under balanced and imbalanced settings. ",
            "strength_and_weaknesses": "Pros:\n\n1, The concept of semantic scale is novel and well-motivated mathematically. \n\n2, The usage of sphere packing method to control the scale of semantic scale is a very neat idea.  \n\nCons: \n\n**1, Strong correlation with model performance weakens the contribution of the proposed loss function.**  The paper highlights the correlation between semantic scale and model performance. There exits method such as hard-class mining [1] and recall loss [2] that rely on the model performance to re-weight losses. This strong correlation makes one wonder why a loss function weighted by semantic scale would be better than one weighted by performance. \n\n**2, Adding inter-class interference seems ad-hoc and obscures the contribution of semantic scale.** In sec 3.4.1, an inter-class interference term is added to the semantic scale metric. This combination seems ad-hoc. Even though the performance of the combined metric seems to work well, it becomes less clear how much contribution the vanilla semantic scale has. \n\n**3, Marginal effects of semantic scale and model performance w.r.t the amount of data represent correlation not causation.**  In the sec.3.3, the authors observe a strong correlation between the marginal effects of semantic scale and model performance w.r.t the amount of data. This is an interesting correlation, however, which cannot be used to explain why classification gain is marginal when data are sufficient or why performance drops quickly when data are scarce, as claimed in the abstract. Because the observation only establishes a correlation not a causation between the two events. \n\n**4, Semantic scale is model-dependent.** Even though the authors claim that semantic scale is a natural measure that is model-dependent, it is calculated from the output (feature space) of a trained model and therefore should be model-dependent. The same dataset, with the same class statistics, will lead to different semantic scales if models are different. \n\n[1] A. Shrivastava, A. Gupta, and R. Girshick, \u201cTraining region-based object detectors with online hard example mining,\u201d in Proceedings\nof the IEEE conference on computer vision and pattern recognition, 2016, pp. 761\u2013769.\n\n[2] Tian, Junjiao, et al. \"Striking the Right Balance: Recall Loss for Semantic Segmentation.\" 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively clear in writing and has extensive experiments. The method is novel. Values of hyperparameters are given for reproducibility. However, no code is provided. ",
            "summary_of_the_review": "The paper presents a novel idea to quantify implicit bias in classification. However, questions regarding claims and some less well explained components in the method make the overall message less convincing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper42/Reviewer_kLsB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper42/Reviewer_kLsB"
        ]
    },
    {
        "id": "Q-onsSes0_",
        "original": null,
        "number": 4,
        "cdate": 1666342586675,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666342586675,
        "tmdate": 1666342586675,
        "tddate": null,
        "forum": "07tc5kKRIo",
        "replyto": "07tc5kKRIo",
        "invitation": "ICLR.cc/2023/Conference/Paper42/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors study the problem of imbalance in classification problems. To be specific, they propose a measure to quantify the `semantic scale' of classes and use this measure as a means to analyze the imbalance among classes. Moreover, they use this measure to mitigate the effect of imbalance in various classification datasets. They report significant improvements over the baseline methods.\n\nI have reviewed an earlier version of the paper for ICML2022 and I see that the authors have significantly improved their paper since then (e.g. experiments on ImageNet, iNaturalist, MNIST & MNIST-LT). Thank you for taking into account my earlier comments.",
            "strength_and_weaknesses": "Strengths:\n- I am very fond of the idea of measuring imbalance and using such measures instead of counting the number of samples in classes. This is a very promising research direction.\n\n- I am very fond of the idea of extending the mechanism of Cui et al. [10] to develop a measure that better reflects imbalance among classes.\n\n- I am very fond of the analysis and the insight provided by the paper, before comparing it with other methods.\n\nWeaknesses:\n\nAlthough I am very fond of the approach and the ideas, I believe that the paper has significant limitations and is not ready for publication yet. Here are the major issues that I've identified, the minor problems are listed further below:\n\n1. First of all, the overall story of the paper can be improved: \n\n1.1. The paper starts with a very convincing example of semantic scale (birds and swans), motivating the overall approach of introducing a measure for it, then it does not provide any insight or analysis about whether or not the proposed measure is really capturing it: The analyses in Section 3 do not provide any information about the class labels nor a discussion about them.\n\n1.2 Although the motivation and the story of the paper is toward imbalance in classification problems, in the experimental analysis section, the paper adopts a deep metric learning setup and compares the proposed approach against the SOTA method of that domain. Although an analysis in a deep metric learning setup is not irrelevant, the story of the paper necessitates performing an extensive comparison with the SOTA of imbalance mitigation strategies.\n\n2. I recommend comparing the proposed scale against other measures of hardness / difficulty. I find the lack of such an analysis to be a significant limitation of the paper.\n\n\nTypos and Unclear Bits:\n- \"As the information of data increases\" => What's meant is unclear.\n- Section 3.2, 5th line: Error in notation. The covariance matrix for vector z_i is being defined, yet the summation runs over index i. I recommend using j in the summation just to avoid confusion.\n- Eq. 1: Did you want to write \"Vol(z_i)\" instead, as implied by the previous sentence? Or you should update the sentence.\n- \"Considering that real-world metrics ... scales. We\" => \"scales, we\". Otherwise, the sentence \"Considering that...\" is incomplete.\n- Figure 2 is way too small and not readable. \n- \"which indicates that the quantitative measurement of semantic scale is reasonable. In addition, the growth rate of the semantic scale varies across classes, which is determined by the grain size of the class itself. It leads to different semantic scales even if all classes have the same number of samples.\" => From Figure 2, what we see is that (i) semantic scale saturates and (ii) it is different for different classes. I would describe these results as \"as expected\" and consider \"reasonable\" a strong word.\n- Figure 2 right column: Changing dataset size requires re-tuning the hyper-parameters. Without re-tuning the hyper-parameters, it is expected that the performance will drop when data is subsampled. It is not clear whether this has been performed and whether the results in Figure 2 are reliable.\n- \"the centers of all classes are C\" => It is not clear in what space these centers are.\n- \"C\" is used in Section 3.3 and 3.4 for two different things. It is better to use different symbols. There is a similar problem with N (there might be other symbols).\n- Table 1: It would have been nicer to add E_n to the table.\n- Section 3.4.1: It would be nice to visualize or provide insight about the calculated weights.\n- Section 5.1 and \"We create long-tailed CIFAR-100 and long-tailed Cars196 for training using the first 60 and 98 classes of CIFAR-100 and Cars196, respectively, and the test sets are the remaining classes.\" => The authors should cite the literature for the setup.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is now very clear. The evaluation is more extensive and convincing. I find the approach & contribution very original.",
            "summary_of_the_review": "Fond of the paper. Novel approach, good analyses & insights, strong results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper42/Reviewer_MKbH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper42/Reviewer_MKbH"
        ]
    }
]