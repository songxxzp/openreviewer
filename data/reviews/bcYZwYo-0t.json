[
    {
        "id": "DLvsqtaesR",
        "original": null,
        "number": 1,
        "cdate": 1666118671870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666118671870,
        "tmdate": 1669829783643,
        "tddate": null,
        "forum": "bcYZwYo-0t",
        "replyto": "bcYZwYo-0t",
        "invitation": "ICLR.cc/2023/Conference/Paper2710/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the effect of stochastic representations in long-tail recognition scenarios and designs a self-distillation method to adapt stochastic representations to long-tail learning. ",
            "strength_and_weaknesses": "I think this paper is very well-written and well-organized. The preliminary experiments on the effect of stochastic representation (Stochastic Weight Average, SWA) on decoupled long-tail learning methods are detailed and intuitive. They start with testing directly applying stochastic representations to single stage and two stage training and acquired results that promote further development of SWA on two-stage/decouple methods. The idea of using Gussian SWA (SWAG) as a measurement of prediction certainty, although not new, but directly connected to the intuition that tail class predictions are usually less certain. A robust certainty measurement is always a challenge in long-tail recognition, and I think using SWAG is an interesting proposition. The discussion and explanation of the limitations of direct usage of SWAG are clear, because it needs multiple runs to get a stochastic distribution. And this leads to the proposition of the proposed self-distillation method, which has multiple teachers during training for a SWAG distribution, and has a student, that learns from the stochastic teachers. Again, this multi-teacher single student framework is not new (e.g., mean teachers in semi-supervised learning). But it aligns well with the challenges mentioned in direct usage of SWAG. The experiments are extensive. Even the state-of-the-art multi-expert methods like  RIDE are compared. However, the improvements, especially the improvements over multi-expert methods, are limited (<1% on average). The biggest drawback of this method is that it is relatively too complicated (multiple teach networks need to be trained for SWAG to work), and, thus, not compatible with existing multi-expert models, which leads to limited improvements. The proposed multi-teacher single student framework itself is already a multi-expertish model. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written, code is not yet available. ",
            "summary_of_the_review": "This paper overall is a good contribution to the community for its detailed experiments on the effect of stochastic representations on long-tailed learning, and the idea of using stochastic representations as a measurement of confidence. However, the proposed method is relatively complicated and yields limited improvements over state-of-the-art methods. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2710/Reviewer_eyhi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2710/Reviewer_eyhi"
        ]
    },
    {
        "id": "0Shtq3W6ZL",
        "original": null,
        "number": 2,
        "cdate": 1666411608314,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666411608314,
        "tmdate": 1666411968291,
        "tddate": null,
        "forum": "bcYZwYo-0t",
        "replyto": "bcYZwYo-0t",
        "invitation": "ICLR.cc/2023/Conference/Paper2710/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper aims to solve the fundamental problem --- long-tailed recognition. It analyzes SWA/SWAG with long-tailed data. \n           \n\uff081) Classifier re-training can make better use of SWA for long-tailed data.    \n  (2) The positive correlation between NLL and dispersion is observed. Based on this phenomenon, a self-distillation strategy is developed to reduce predictive variance.   \n  (3) Experiments on ImageNet-LT and iNaturalist 2018 show some improvements.    \n",
            "strength_and_weaknesses": "Strength    \n\n(1) The paper writes clear and is easy to follow.    \n\n(2) The method should be general and is expected to work well with previous methods.   \n\nWeakness  \n\n(1) This paper use SWA to enhance the representation ability of deep models and a distillation method is proposed to reduce predictive variance. However, comparisons with most related work are missed.     \n      I. The state-of-the-art methods, like PaCo [1].     \n      II. distillation-based methods, like CBD [2].     \n\n\n[1] Parametric Contrastive Learning. ICCV 2021.     \n[2] Class-Balanced Distillation for Long-Tailed Visual Recognition. BMVC 2021.     \n\n(2) About the distillation objective.  Why do the teacher prediction probabilities obey the Dirichlet distribution?    \n\n(3) As shown in Figure2, as the number of stochastic representations increases, the changes of ECE and NLL are limited.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The code is provided in the supplementary file.",
            "summary_of_the_review": "The paper investigates SWA/SWAG for long-tailed data. However, some important comparisons are missed.    \nIf the concerns are addressed, I'm very glad to raise my score.    ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Some discussion is added.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2710/Reviewer_DCKB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2710/Reviewer_DCKB"
        ]
    },
    {
        "id": "yBHA2KD7mS0",
        "original": null,
        "number": 3,
        "cdate": 1666591111829,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591111829,
        "tmdate": 1666591111829,
        "tddate": null,
        "forum": "bcYZwYo-0t",
        "replyto": "bcYZwYo-0t",
        "invitation": "ICLR.cc/2023/Conference/Paper2710/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on improving features to be more generalizable to achieve better performance in Long-Tailed Recognition (LTR). In particular, the paper adopts Stochastic Weight Averaging (SWA) for improving the generalization of deep neural networks. Moreover, the paper proposes a classifier re-training algorithm based on the learned representation. The algorithm uses a Gaussian perturbed SWA, and a self-distillation strategy that can harness the diverse stochastic representations based on uncertainty estimates to build more robust classifiers. Experiments on two benchmarks show that the proposed method improves upon previous methods in terms of both prediction accuracy and uncertainty estimation.",
            "strength_and_weaknesses": "Strength\n- Studying how generalized features improves LTR is interesting.\n- Studying uncertainty estimation in the context of LTR is interesting.\n\n\nBelow are some weaknesses.\n\n- While \"the success of decoupling naturally motivates obtaining more informative representations from which the classifier re-training can benefit\", using SWA seems like less well-motivated. Instead, this motivation should suggest learn more generalized features using more data sources (e.g., external data) and more general loss functions (e.g., contrastive learning). I would like the authors to discuss this further, otherwise the paper just seems incremental especially when it states \"to the best of our knowledge, it has never been explored for long-tailed classification problems.\", and in the conclusion \"to the best of our knowledge, this is the first attempt to introduce SWA into long-tailed learning.\"\n\n- While the paper is motivated to learn \"generalizable\" features, a recent work [R1] shows that learning a backbone which has \"balanced-norm\" filters significantly boosts LTR performance. Moreover, [R1] simply tunes weight decay to regularize backbone's norms and achieves the state-of-the-art. This is an open questions -- whether to learn \"balanced backbone\" or more \"generalized backbone\" to achieve better LTR performance? Can authors discuss and show insights?\n\n[R1] Alshammari, et al., \"Long-Tailed Recognition via Weight Balancing\", CVPR 2022\n\n- While it is interesting to study uncertainty estimation in the context of LTR, the paper does not sufficiently analyze the results. For example, it is unclear how good uncertainty estimation for tail classes vs. head classes. Authors are encouraged to dive into this part and provide insightful discussions.\n\ntypos\n- Figure 2 caption: \"asdfsadf\".",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, novelty and reproducibility are okay. The novelty is incremental as the paper seems to apply Stochastic Weight Averaging to improve the generalizability of learned features to improve long-tailed recognition performance. However, the performance improvement is marginal especially given the large computation cost. Code is provided but it is unclear whether running it reproduces the reported results. I'd suggest attaching something like jupyter notebook for better demonstration; authors should not expect reviewers to run their code smoothly.",
            "summary_of_the_review": "The paper focuses on using Stochastic Weight Averaging (SWA) to improve features used for long-tailed recognition (LTR). The motivation is to learn more generalizable features for LTR. SWA is an existing method that learns more general features and the paper claims that \"to the best of our knowledge, this is the first attempt to introduce SWA into long-tailed learning\". However, SWA seems weak as it achieves only marginal improvement given the large added computation burden. Therefore, the rating of the paper is \"5: marginally below the acceptance threshold\"",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics issues as I am aware.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2710/Reviewer_fqWB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2710/Reviewer_fqWB"
        ]
    },
    {
        "id": "fD5C6L5qD6",
        "original": null,
        "number": 4,
        "cdate": 1666610080596,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666610080596,
        "tmdate": 1670764847688,
        "tddate": null,
        "forum": "bcYZwYo-0t",
        "replyto": "bcYZwYo-0t",
        "invitation": "ICLR.cc/2023/Conference/Paper2710/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors study the problem of long-tailed visual classification. To be specific, they propose obtaining a stochastic model for representations through stochastic weight averaging (SWA) and retraining a robust classifier by distilling these representations.",
            "strength_and_weaknesses": "trengths:\n+ A novel approach to obtaining better representations / decision boundaries for long-tailed visual recognition.\n+ Positive results over the baselines.\n\nWeaknesses:\n\n1. The paper identifies the main source of imbalanced learning as the decision boundary. However, the recent view identifies the culprit as the representation learning stage and suggests that features of the under-represented class are somehow sub-optimal, which leads to worse decision boundaries for the under-represented class. I suggest the authors to tone down \"the main bottleneck is the decision boundary\" claim.\n\n2. Fig 2: It is not clear why the dispersion of models along the training trajectory should be correlated with NLL. \n\n3. The proposed approach is computationally too expensive. One can easily obtain a set of alternative representations through dropout &  perform distillation. Would this not be cheaper? A discussion on complexity is necessary. \n\n4. The experimental evaluation needs improvement.\n\n4.1. The SOTA comparison in Table 4 seems to have unfair aspects: In some cases, less training epochs are used with the compared baseline (e.g. the comparison against KCL) and more are used in some others (Liu et al. (2021)). \n\n4.2. The experimental evaluation is missing important comparisons with more recent approaches from 2022. No approach is compared against from 2022.\n\n4.3. The experimental evaluation is missing comparisons on CIFAR10LT & CIFAR100LT. This is important to get a better feeling about how the method performs in a small-scale problem (CIFAR10LT) and a middle/large-scale problem such as CIFAR100LT. In fact, CIFAR100LT is surprisingly challenging for some problems and you can obtain unexpected results.\n\n\nMinor comments:\n- \"Confirming that SWA can benefit to long-tailed classification\" => \"Confirming that SWA can provide benefit to long-tailed classification\".\n- Eq 2: \"x\" is forgotten after the dot.\n- \"parameter \u03b8\" => \"parameters \u03b8\".\n- \"parameter \u03d5\" => \"parameters \u03d5\".\n- Eq 4: A minor detail but this should include a step size.\n- Eq 6: \u0398\u2032 => not introduced.\n- Below Eq 9: \"predictive uncertainty of x,\" => Predictive or epistemic? They are not the same. Two lines above Eq 9, you said epistemic.\n- Figure 2: \"asdfsadf.\" => Agreed :)\n- Fig 3: ERM not introduced. ",
            "clarity,_quality,_novelty_and_reproducibility": "Generally clear. Good quality. Novel approach. Reproducible.",
            "summary_of_the_review": "Novel approach but there are concerns about the evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2710/Reviewer_1FXP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2710/Reviewer_1FXP"
        ]
    }
]