[
    {
        "id": "evVp_0hs12",
        "original": null,
        "number": 1,
        "cdate": 1666285327199,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666285327199,
        "tmdate": 1666285327199,
        "tddate": null,
        "forum": "hQwb-lbM6EL",
        "replyto": "hQwb-lbM6EL",
        "invitation": "ICLR.cc/2023/Conference/Paper2022/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes an approach to perform code infilling that supports multiple insertion points. The approach is by defining an infilling format that describes the missing code locations and the code to be infilled. The training approach is by the traditional autoregressive next token prediction loss. This paper indeed proposes a simple technique that seems to work reasonably well.",
            "strength_and_weaknesses": "Pros:\n\nThis paper shows that a simple method works. \n\nCons:\n\nThere is no baseline against the traditional left to right approach. That is, with a given amount of data / compute this work uses, what would be the performance if we use these resources for left to right completion alone? This is important because the amount of data that the paper uses as well as the training resources are slightly different from other papers, therefore, it would be good to see how much this amount of data/compute yield in terms of left to right completion. Note that this could be different from the left-to-right performance that is obtained from the infilling method proposed. That goal here would be to compare if this approach results in performance drop due to infilling or not, and if so how much. A similar work by Bavarian 2022 shows that with a similar approach but only with 1 infilling spot, there seems to be no degradation, but this work is sufficiently different that it is not clear if the results by Bavarian 2022 also applies here.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The approach proposed seems clearly presented. However, in the figure, I would not call these 'zero-shot' inference since the model is trained specifically to do such infilling, correct?\n\nQuality: Missing an important comparison, as mentioned above. However, there are a lot of important evaluations (single line, multi line etc, as well as entire scope + execution based evaluation). \n\nNovelty: Moderate, but has a good amount of impact.\n\nReproducibility: This work releases model weights, so it seems reproducible (even though the code release on the inference is slightly lacking).",
            "summary_of_the_review": "Nice contribution, but lacking thorough comparison to the normal baseline / how much performance drops compared to normal left to right approach. However, the multiple infilling spots is a plus.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2022/Reviewer_CKVi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2022/Reviewer_CKVi"
        ]
    },
    {
        "id": "gKGjCGaCmQ",
        "original": null,
        "number": 2,
        "cdate": 1666603509132,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603509132,
        "tmdate": 1666603509132,
        "tddate": null,
        "forum": "hQwb-lbM6EL",
        "replyto": "hQwb-lbM6EL",
        "invitation": "ICLR.cc/2023/Conference/Paper2022/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a code synthesis model that is trained to perform both autoregressive left-to-right generation as well as infilling via added mask tokens. The experimental results show improved performance on various infilling related code generation tasks.",
            "strength_and_weaknesses": "The paper is well written and easy to read. The approach described tackles fundamental challenges in autoregressive generation by training  models that are capable of filling the masked regions. This ability is necessary for tasks such return type prediction and doc string generation where the output depends on both left and right context. The experimental results are also encouraging as for various tasks shown, the model is able to perform better with less model size (e.g., compared to code-davinci-001).\n\nThe paper, however, does not discuss full code generation as it focuses primarily on infilling experiments, so its competitiveness on more challenging tasks is not known. With the ability of infill and the original motivation that \"Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined.\", I'd be very interested to see how the model performs on full code generation as well as devising an ability to detect and re-write portions of the code when solving those tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear.\n\nI think the algorithmic novelty is predictable in a sense that masking and filling mask regions have been used in other context, but it is interesting to see its results in code infilling tasks.\n\nThe paper uses custom dataset generation steps, so without exact details of those, it would be challenging to reproduce the paper.",
            "summary_of_the_review": "Despite not evaluating on full code generation task, the paper has enough experimental results to support itself for infilling tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2022/Reviewer_bKjD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2022/Reviewer_bKjD"
        ]
    },
    {
        "id": "nQ3roIgNaV",
        "original": null,
        "number": 3,
        "cdate": 1666683489195,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683489195,
        "tmdate": 1666683489195,
        "tddate": null,
        "forum": "hQwb-lbM6EL",
        "replyto": "hQwb-lbM6EL",
        "invitation": "ICLR.cc/2023/Conference/Paper2022/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a mask infilling method to train language model for program synthesis with the ability of code infilling. The method randomly selects a span and replace it with a mask token, and place the span after the sequence as the target. The proposed method is novel, however the masking method is also used in the literature of T5 pretraining. Experiments show that the mask infilling does not degrade left-to-right performance, while significantly improve insertion capability.",
            "strength_and_weaknesses": "Strength:\n- The paper is well-written. The method is well explained with examples. Experiment results are extensive with analysis.\n- The proposed method is a simple but effective masking objective. Zero-shot performance demonstrates the effectiveness for infilling line completion task, as well as other related prediction tasks.\n\n\nWeakness:\n- From the main text, I cannot infer whether the training is done 100% with mask infilling or there are some probability that normal left-to-right data is used for training without any masking. If it is 100% mask infilling, it seems surprising to me that left-to-right is not affected at all. I would expect normal left-to-right data exist in training, like the concurrent work fill-in-the-middle (FIM) training (Bavarian et al., 2022).\n- Some training details seem missing, for example, how many training tokens and number of iterations the model has gone through during the training process.\n- Though there is an <EOM> token indicating the end of generation, there is possibility that the generated code after <MASK:0> will contain overlapping content as the actual right context. Did you see any such issue exist? How do you resolve such issue?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-writtern and is of high quality. The proposed method is novel. And I expect that the results should be easy to reproduce.",
            "summary_of_the_review": "Overall, I think this is a good paper with novel method proposed and well-performed experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2022/Reviewer_7zUC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2022/Reviewer_7zUC"
        ]
    },
    {
        "id": "xKRUnqBJkc",
        "original": null,
        "number": 4,
        "cdate": 1666714323077,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714323077,
        "tmdate": 1666714323077,
        "tddate": null,
        "forum": "hQwb-lbM6EL",
        "replyto": "hQwb-lbM6EL",
        "invitation": "ICLR.cc/2023/Conference/Paper2022/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The current work presents InCoder, an alternative approach to general CLM or seq-to-seq code generation where the training regime is changed to allow code generation in arbitrary subsections of the code.\nExperimentation seems appropriate, including HumanEval, DocString generation, return type prediction and variable name prediction showing model capabilities.\n",
            "strength_and_weaknesses": "Strengths \nA really good paper\nThe idea is clever, simple and straightforward. It provides a functionality that general GPT-like or full transformer models don\u2019t.\nExperimentation seems appropriate. Authors make a great effort not sticking with standard benchmarks which might not be the ideal scenario for InCoder\n\nWeaknesses\nThe training regime comes with a cost and the Left to right generation, specifically on Table 11\n",
            "clarity,_quality,_novelty_and_reproducibility": "Table 11 might have included CODEX-2.5B (in addition to the 12B). The performance of Codex 2.5B in HumanEval surpass InCoder, but as stated in the weaknesses section it is expected given the change in the training regime\n",
            "summary_of_the_review": "A good idea, well implemented and wrapped up in a quality paper\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2022/Reviewer_PWMt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2022/Reviewer_PWMt"
        ]
    }
]