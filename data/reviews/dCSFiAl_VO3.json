[
    {
        "id": "NVVoA3wFS4M",
        "original": null,
        "number": 1,
        "cdate": 1666227238491,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666227238491,
        "tmdate": 1666651630953,
        "tddate": null,
        "forum": "dCSFiAl_VO3",
        "replyto": "dCSFiAl_VO3",
        "invitation": "ICLR.cc/2023/Conference/Paper4639/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the $k$-means and $k$-median problem in the learning-augmented setting, where we assumed to access a predictor that provides information about the label of each point with a $(1 - \\alpha)$-precision, and the approximation factor of the algorithm is measured in terms of $\\alpha$. For both problems, the authors propose an algorithm that gets a better approximation. The experiments also demonstrate the advantage of these algorithms.",
            "strength_and_weaknesses": "Strength\n\n1. The technical contribution is solid. Both of the algorithms achieve a better guarantee than the previous algorithms.\n2. The experimental results show that the performance of the proposed algorithm is better than the previous ones.\n\nWeakness\n\n1. The paper claims to preserve the time complexity of the previous approaches. However, when referring to the time complexity, the setting of this paper seems to be different than that in Ergun et al. If my understanding is correct, In Ergun et al, the time is including the time to assign each point to the closet center, which is $O(mdk)$ naively, so the authors show that even include this step, the time complexity is still $O(md \\log m)$. However, for this paper, the time seems to not include this step but only output the $k$ centers. One question is that can the technique used in Algorithm 3 of Ergun et al. be applied in this paper? \n2. The paper considers the time complexity of the different algorithms. However, in the experiment section, the authors seems not to list the result of the runtimes of different algorithms.",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to the last question. ",
            "summary_of_the_review": "Overall I think it is an interesting paper. However,  the authors should clarify the issues in the time complexity(see strength and weakness).  Also, the comparison of the runtime in the experiment will make this paper more complete. Therefore, my current rating is 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4639/Reviewer_6m4b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4639/Reviewer_6m4b"
        ]
    },
    {
        "id": "xxHxaCB-2i0",
        "original": null,
        "number": 2,
        "cdate": 1666271057681,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666271057681,
        "tmdate": 1666271231394,
        "tddate": null,
        "forum": "dCSFiAl_VO3",
        "replyto": "dCSFiAl_VO3",
        "invitation": "ICLR.cc/2023/Conference/Paper4639/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper can be regarded as a follow-up of (Ergun et al. 2021) which is published in ICLR2022. The main contribution of this submission is to answer the question: \"Is it possible to design a $k$-means and a $k$-medians algorithm that achieve (1 + $\\alpha$)-approximate clustering when the predictor is not very accurate?\". Moreover, the proposed two algorithms (Algorithm 1 and Algorithm 2) also have improved bounds on the clustering cost, while preserving the time complexity of the previous approaches and removing the requirement on a lower bound on the size of each predicted cluster.\n\n[Ergun et al. 2021] Jon Ergun, Zhili Feng, Sandeep Silwal, David P. Woodruff, and Samson Zhou. Learning-augmented\nk-means clustering.  In: ICLR, 2022.",
            "strength_and_weaknesses": "Strength:\nThis submission follows the setting in (Ergun et al. 2021), where we are given a data set in $d$-dimensional Euclidean space, and a label for each data point given by a predictor indicating what subsets of points should be clustered together.  The proposed two algorithms (Algorithm 1 and Algorithm 2) have improved bounds on the clustering cost, while preserving the time complexity of the previous approaches and removing the requirement on a lower bound on the size of each predicted cluster.\n\nWeaknesses:\nThe presentation quality can be improved to make it easier to be understood by readers. For example, there is not a conclusion section in this submission. Besides, there are some typos and grammatical problems.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation quality can be improved to make it easier to be understood by readers.\n\nBesides, it seems that the main reference (Ergun et al. 2021) has been published in ICLR2022:\nhttps://openreview.net/forum?id=X8cLTHexYyY\n\nIt is better to update the reference list to their newest status.",
            "summary_of_the_review": "This submission follows the setting in (Ergun et al. 2021), where we are given a data set in $d$-dimensional Euclidean space, and a label for each data point given by a predictor indicating what subsets of points should be clustered together. I am not familiar with the topic of this submission, and I am open to hear opinions from other reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4639/Reviewer_HcAi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4639/Reviewer_HcAi"
        ]
    },
    {
        "id": "Mzr6GUbCdL8",
        "original": null,
        "number": 3,
        "cdate": 1666727044679,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727044679,
        "tmdate": 1666727044679,
        "tddate": null,
        "forum": "dCSFiAl_VO3",
        "replyto": "dCSFiAl_VO3",
        "invitation": "ICLR.cc/2023/Conference/Paper4639/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies approximation algorithms for k-means and k-median clustering in the presence of \"advice\" in the form of an oracle that approximately provides the optimal clustering. The main assumption is that the algorithm has access to a clustering that agrees with an optimal clustering with parameter \"\\alpha\". The parameter ensures that for every cluster, the provided clustering has a good overlap with the optimal.\n\nUnder this assumption, the authors improve upon prior work in the same model, and show a (1+O(\\alpha)) approximation guarantee. Specifically, they improve prior work in terms of the dependence on \\alpha, as well as the range of \\alpha to which the algorithm applies. \n\nFor k-means, the authors use a \"coordinate wise\" algorithm that keeps track of a good subset of points (the potential \"agreement\" with OPT) in each cluster, and show that this must yield a good approximation ratio. For k-median the high level idea is similar, but the algorithm is randomized and it uses as a subroutine an approximate median algorithm from previous work. Both the algorithms are quite clean and practical. ",
            "strength_and_weaknesses": "The main strengths of the paper are: \n- They analyze simple and practical algorithms for the two problems and show an approximation ratio that improves upon prior works\n- The range of \\alpha is considerably improved compared to prior work.\n\nHowever, the paper has a few weak aspects:\n- The first is wrt the motivation: the model isn't necessarily about \"learning augmented\" algorithms. It is much more closely related to clustering with noisy labels, which has been extensively studied (including in some recent works, e.g., in COLT 22). \n- Secondly, from an algorithmic point of view, the paper is somewhat on the weak side: most of the techniques are relatively standard and it's hard to get excited about. While the overall result is clean, it is not especially surprising. Some matching lower bounds, or surprising trade-offs, or perhaps improved results with the \"errors\" are random, might be interesting to know about.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite well written and was easy to follow. The algorithm and analysis were clean. As outlined above, the algorithm and analysis are somewhat along known lines, but the exact results are of course novel.\n\nReproducibility: N/A (theory paper)\n",
            "summary_of_the_review": "In summary, the paper presents simple, practical algorithms for k-means and k-median clustering when the algorithm has access to a \"noisy version\" of the optimal clustering. The novelty in terms of algorithm/techniques is somewhat limited though, which is why I rate it as mildly above acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4639/Reviewer_JvjK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4639/Reviewer_JvjK"
        ]
    }
]