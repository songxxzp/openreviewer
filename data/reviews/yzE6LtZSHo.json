[
    {
        "id": "vVQ4xsJjEx",
        "original": null,
        "number": 1,
        "cdate": 1666178585722,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666178585722,
        "tmdate": 1666178585722,
        "tddate": null,
        "forum": "yzE6LtZSHo",
        "replyto": "yzE6LtZSHo",
        "invitation": "ICLR.cc/2023/Conference/Paper3141/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes the imputation strategy called ReMasker for filling in missing values in tabular data. The idea is based on masking out randomly selected values and training the neural network to reconstruct them. As a neural network, the authors use Transformer, which better reflects the correlation between attributes. The evaluation was performed on typical tabular datasets and compared with SOTA imputation techniques. Additionally, the authors justify that REMASKER learns representations of tabular data, which are insensitive to missing values.",
            "strength_and_weaknesses": "The idea of the proposed method is very simple, which is its great advantage. However, I have a feeling that similar ideas (masking out an additional set of values and optimizing the loss for their reconstruction) appeared in previous works, even if the authors did not sell it as a \"new imputation method\". When missing values do not appear in training, then an analogical approach was implemented in ContextEncoder (https://arxiv.org/abs/1604.07379). Since the authors deal with inpainting, the training data does not contain missing values. More importantly, re-masking approach was used as an ingredient in MisConv approach, see page 12 (paragraph \"Training of DMFA on incomplete data\") in https://arxiv.org/pdf/2110.14010.pdf. Instead of reconstruction loss, they apply negative log-likelihood since they deal with probability distributions. I think that there are more techniques with the analogical idea.\n\nNevertheless, I see some important contributions in the paper. This is the first paper where I see the application of the Transformer model in the case of missing data imputation. Even if the general strategy of re-masking was known, the authors show that the application of Transformed allows for obtaining significant improvement over SOTA. The authors could focus more on this aspect than on promoting a re-masking strategy.\n\nI appreciate the very good evaluation presented in the paper, which includes three missing scenarios, and various missing ratios. Moreover, the authors compared their method with a large number of recent imputation techniques showing very good results of ReMasker.\n\nI wonder if the authors verified what happens if we replace the Transformer model with e.g. fully connected neural network. In other words, I would like to know how much we gain by using the general strategy of re-masking with an arbitrary model and how much of the improvement is gained using the Transformer model itself.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is nicely written and the idea is clearly explained. My only suggestion is to improve the description of Transformer model applied in ReMasker. The description provided on page 4 would be insufficient for me to re-implement the proposed approach. I suggest the authors make clear how the Transformed is implemented in this case because this is the most important part of the paper for me.",
            "summary_of_the_review": "To summarize, I like the paper and I think it is a valuable contribution. In particular, the application of the Transformer is a major contribution for me. The authors were able to improve the SOTA in the imputation task. However, I have a feeling the idea of re-masking appeared before. In consequence, this is a new instantiation of the previous idea. This is a borderline paper for me and I am willing to increase my score after the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3141/Reviewer_Hron"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3141/Reviewer_Hron"
        ]
    },
    {
        "id": "oWkw8T6nbyg",
        "original": null,
        "number": 2,
        "cdate": 1666214954608,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666214954608,
        "tmdate": 1666657663201,
        "tddate": null,
        "forum": "yzE6LtZSHo",
        "replyto": "yzE6LtZSHo",
        "invitation": "ICLR.cc/2023/Conference/Paper3141/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "- The authors proposed a simple but effective imputation framework for tabular data.\n- The proposed framework additionally introduces the missingness to the incomplete data and using the reconstruction loss on the introduced missing components and unmasked components to train.\n- The proposed method achieves comparable results in comparison to SOTA (HyperImpute).",
            "strength_and_weaknesses": "Strength:\n- The methodology is simple but the performance is quite impressive.\n- The authors provided extensive experiments to empirically verified the claims. \n- The authors utilize the transformer algorithm for tabular data imputation which seems like a proper model.\n\nWeakness:\n- It is unclear how the MCAR based remasking can be generalized to impute missing components with MAR and MNAR settings.\n- The performances are comparable with SOTA and not consistently better than SOTA. But it seems like the authors claim that it consistently outperforms SOTA.",
            "clarity,_quality,_novelty_and_reproducibility": "- The idea is clear and the paper is easy to understand.\n- It is hard to say that the idea is novel. Mask autoencoding is widely utilized in many areas and using mask autoencoders for imputation is quite straightforward.\n- It seems like the paper can be somewhat easily reproducible.",
            "summary_of_the_review": "1. Complete data availability\n- Many of the imputation methods do not need complete data for missing data imputation.\n- Therefore, it would be better to tone down this contribution.\n\n2. Re-masking with MCAR setting\n- Authors introduce the re-masking with MCAR setting.\n- In other words, m' is uniformly sampled.\u00a0\n- In that case, it is unclear how this kind of remasking and reconstructing those remasked components can be generalized to the MAR or MNAR settings.\n- More specifically, the model is learning to estimate the missing components with MCAR settings via remasking and reconstruction. How this learning process can be generalized to impute the missing values in MAR and MNAR settings.\n\n3. SOTA performances?\n- It seems like the performance of the proposed method is comparable with HyperImpute.\n- However, it is hard to say which one is better.\u00a0\n- In that point of view, it is hard to support the claims in the abstract that \"ReMasker consistently \"outperforms\" SOTA\".\n\n4. Categorical data\n- In tabular data learning, handling categorical data is critical.\n- In this work, how to impute the categorical data?\u00a0\n- What kind of embedding did the authors use for handling categorical data?\n\n5. Hyper-parameters\n- It seems like some of the hyper-parameters are important.\n- For instance, the missing ratio of remask would be a critical hyper-parameter.\n- Also, the reconstruction loss weights between remask and unmask can be another important hyper-parameter.\n- It would be good to add additional sensitivity analyses on those hyper-parameters.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3141/Reviewer_CAJ5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3141/Reviewer_CAJ5"
        ]
    },
    {
        "id": "O065jk4_ln",
        "original": null,
        "number": 3,
        "cdate": 1666509910764,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666509910764,
        "tmdate": 1666509910764,
        "tddate": null,
        "forum": "yzE6LtZSHo",
        "replyto": "yzE6LtZSHo",
        "invitation": "ICLR.cc/2023/Conference/Paper3141/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a remask method which masks out some more data under the original missing data and reconstruct them. The main contribution is this remask part.",
            "strength_and_weaknesses": "Strength:\n1. The authors use many datasets and include many baselines (12 datasets and make a comparison with 13 imputation methods).\n2. The authors does ablation study and has some discussion trying to explain why their method works.\n\nWeakness:\n1. The method does not seem to be novel. This is the main weakness. The \"remask\" method basically masks out some extra non-missing features and predict them. But this kind of technique is widely used in imputation method in real-world dataset when there is missingness. Researchers usually mask out extra non-missing features to create datasets for training when there is already missingness in real-world data. So I am not sure what extra novelty the paper provides.\n2. The authors only consider synthetic dataset where the authors create missingness from some pre-specified missingness patterns on data where. The authors should consider real-world dataset where there is already missingess. \n3. The discussion on learning representation invariant to missingness pattern is vague. All the methods try to learn representation invariant to missingness patterns. The authors' analysis does not provide extra intuition. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear. Figure 1 is nice-looking and clearly explains the method. The novelty is limited.",
            "summary_of_the_review": "The main reason I choose to reject is that the novelty of the paper is limited. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3141/Reviewer_Vv4x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3141/Reviewer_Vv4x"
        ]
    },
    {
        "id": "WW0Y7a4QFU",
        "original": null,
        "number": 4,
        "cdate": 1666660364224,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660364224,
        "tmdate": 1666676895180,
        "tddate": null,
        "forum": "yzE6LtZSHo",
        "replyto": "yzE6LtZSHo",
        "invitation": "ICLR.cc/2023/Conference/Paper3141/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose ReMasker, an adaptation of the masked autoencoder (MAE, for vision tasks) for tabular data imputation.  Additionally, ReMasker included transformer blocks to the autoencoder and both the encoder+decoder during training.  Missing at random (MAR) is simulated for a large number (12) of datasets, and the utility of imputation using ReMasker is evaluated for different metrics, as well as in contrast to a large number of SOTA imputation methods.  ReMasker regularly outperforms the other baseline methods.  Additional studies include the effects of the network architecture (i.e., depth and width) on overall performance, the masking ratio hyperparameter, and the combination of ReMasker with the ensemble imputer HyperImpute.",
            "strength_and_weaknesses": "Strengths:\n-ReMasker is a great extension of MAE to tabular data imputation\n\n-ReMasker works well across different downstream performance metrics (i.e., RMSE for regression and AUAOC for classification), outperforming many widely used imputation schemes (e.g., MissForest and MICE).\n\n-The ReMasker architecture is pretty lightweight, making it a reasonable approach to large-scale tabular datasets\n\nWeaknesses:\n-Important model design choices are not explained in any depth.  E.g., \"Unlike conventional MAE, the REMASKER decoder is used in both fitting and imputation phases.\" <- Why?  Without explanations for the differences in models between MAE and ReMasker, an evaluation must be conducted to explain these decisions.\n\n-The exact model details could be better explained.  E.g., \"Note that the encoder is only applied to the observed values: in the\n fitting phase, it operates on the observed values after re-masking\" <- This statement is inconsistent with the paper; based on Figure 1,\n it is operating on missing and masked values, which are not observed.  Please clarify\n\n-More work is needed to show that ReMasker theoretically learns missing-invariant representation of tabular data (in Section 5).  The current section is unconvincing\n\n-Only MAR is considered in the main text.  A discussion of the MCAR and MNAR results included in the appendix would greatly strengthen the paper.\n\n-The ablation study is limited; it is currently an evaluation of network-architecture hyperparameters.  An actual ablation study would consider the role of transformers in ReMasker (vs without in the original MAE architecture) and the exclusion of the decoder during training (as is originally done in the MAE for vision).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n--------\n-Aside from a few points (see weaknesses), the paper is well written and easy to follow.\n\nQuality\n--------\n-The paper contains significant contributions for ML applications suffering from missing data.\n\nNovelty\n---------\n-Leveraging MAE, ReMasker is a novel contribution for tabular data imputation.  The inclusion of a transformer is also novel, but an ablation study or discussion of why this (and inclusion of the decoder during training) is currently lacking in the paper.\n\nReproducibility\n-----------------\nResults are not reproducible, no code was included.",
            "summary_of_the_review": "Results are impressive and the use of MAE for tabular data imputation is intuitive.  However, the results are not currently reproducible and the paper could clarify more points (detailed throughout the review).  I currently lean towards accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3141/Reviewer_socG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3141/Reviewer_socG"
        ]
    }
]