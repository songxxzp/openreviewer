[
    {
        "id": "w2NWXNbONtt",
        "original": null,
        "number": 1,
        "cdate": 1666674937976,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674937976,
        "tmdate": 1666674937976,
        "tddate": null,
        "forum": "5-3YJbVPp6m",
        "replyto": "5-3YJbVPp6m",
        "invitation": "ICLR.cc/2023/Conference/Paper2317/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper discussed the transfer learning in relatively relaxed conditions. Particularly, the paper assumes (1) no label overlapping space between source and target tasks, (2) no source dataset access when training the target task, and (3) the inconsistent model architectures.\n\nTo tackle the transfer learning problem in above assumptions, the authors propose the pseudo pre-training (PP) and pseudo semi-supervised learning (P-SSL). \n\nDuring PP stage, the target classifier is pre-trained on the pseudo data generation from source generative model but the final classification layer will be replaced later because the output is in source label space. \n\nDuring P-SSL, the target data will be first fed into pre-trained target classifier and output a pseudo soft label in source space. The pseudo soft label will be fed into source generative model to produce a pseudo data. This pseudo data will be used for unsupervised training. For unsupervised training, the paper adopted UDA loss rather than the cross entropy loss based on the original label. \n\nThe experiments compare the proposed method with scratch training and knowledge distillation.",
            "strength_and_weaknesses": "Strength:\n1. The proposed transfer learning in the three assumptions is difficult to learn but the authors make it work.\n2. The author found directly using the original label for the pseudo data in P-SSL didn't work and UDA as the unsupervised loss can achieve better performance. This observation is important and may help other related research works.\n\nWeaknesses:\n1. I'm afraid the main contribution is UDA. Because UDA loss is used, the author can directly apply UDA loss on the original target data $x_t$ rather than $x_{s\\leftarrow t}$. In this way, the pseudo data construction is basically discarded. Although the authors indicate there is less prior work in this area and compare with two simple baselines, I think this is also an important and simple baseline. \n2. I think a more rigorous experiment setup is needed or should be added. For example, in sec 4.6, both the source and target dataset are vehicle or cars related. According the proposed transfer learning with three assumptions, the source data can be vehicle related, and the target dataset could be animal related.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall writing is clear and easy to follow. \n\nThe only thing confused me is Eq (3). In Eq (3), $y_{s\\leftarrow t}$ is generated from $C_{s}^{A_t}$. \nHowever, in Line 4 of Algorithm 1, $y_{s\\leftarrow t}$ is generated from $\\hat{C}_{s}^{A_t}$. \n\nAdditionally, in the last paragraph of Page 4, the last layer of $C_{s}^{A_s}$ is swapped. \nBut in Line 2 of Algorithm 1, the last layer of $C_{s}^{A_t}$ is swapped.\n\nThis work didn't provide source code but I believe it should be reproducible.",
            "summary_of_the_review": "In general, the paper raises an important and difficult topic of transfer learning, and proposes a pseudo data based framework to tackle this problem. However, the experiments can not fully verify the proposed method. My initial review is marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2317/Reviewer_kCqh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2317/Reviewer_kCqh"
        ]
    },
    {
        "id": "3JFNe9q7to",
        "original": null,
        "number": 2,
        "cdate": 1666932137065,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666932137065,
        "tmdate": 1666932137065,
        "tddate": null,
        "forum": "5-3YJbVPp6m",
        "replyto": "5-3YJbVPp6m",
        "invitation": "ICLR.cc/2023/Conference/Paper2317/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors utilize the pre-trained conditional generation model as a data augmentation technique to facilitate transfer learning. Extensive experience done prove the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\nBased on my knowledge, the authors are the first to apply the pre-trained GAN to transfer learning. It's a novel. The authors carefully design two scenarios with and without source data. In the meantime, the authors conduct extensive experiments on classification and detection. \n\nWriting: \n\nThe introduction and the description of the main idea are clear, however, the symbols are a little misleading. Though the reviewer can guess the general meaning, it will be better to make it clear.\n1. In equation (2) do supervised loss and unsupervised loss share similar weight parameters?  Where is $\\theta$ on the right side of equation (5)? \n2. Does \\hat(C_t) and C_t share similar parameters? How does $\\tau$ select? Does $C_t$ also have a temperature? \n\n\nFigure:\n- Why does C_s^{A_t} has different color in Figure 1(a)(b)? It will be better to describe the color meaning in caption part. \n- Is the complete PP algorithm used in experiments is the combination of Figure 1(a) and Figure 2? My understanding is that we can train $A_s$ in Figure 1(a) and fix it in Figure 2 for transfer learning. If my understanding is correct, it will be better to merge Figure 1/2 and use dotted lines (or anything else) to differentiate different training stages.\n\n\nQuestions:\n1. Method\nThe architecture inconsistency part in Table 1 is interesting, but I have some questions about the second point: \"no source dataset access\".\nDid you fine-tune GAN using the source dataset? I understand the case that the source dataset is available. However, when the source dataset is not available and GAN never sees a similar source dataset before, does the algorithm still work? If not, the description of \"no source dataset access\" is inaccurate, under which case, we can hardly match the space of the source label and the label space (condition) of GAN, not to mention generate augmentations for transfer learning.\n\n2. Experiments\n- How to select the custom architecture in Table 4? Does each method select its own architecture independently using the validation set or are they selected by \"ours\"? What's the meaning of \"from (2,2,2,2) to (2,2,2,10)\". What's the performance with Architecture RN18(4,4,4,4)? \n- It seems that PP contributes the most in Table 6  but harms the performance in Table 9, any comments on this? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please check the above section about the discussion on clarity, quality and novelty.\n\nNot sure about the reproducibility. Some details of the experiments are missed.\nAre the authors going to release the code when the paper get accepted? ",
            "summary_of_the_review": "The idea is novel. But I do think some expression and explanation could be improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2317/Reviewer_MbsB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2317/Reviewer_MbsB"
        ]
    },
    {
        "id": "fT-CNC6pm4L",
        "original": null,
        "number": 3,
        "cdate": 1666992392608,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666992392608,
        "tmdate": 1666992392608,
        "tddate": null,
        "forum": "5-3YJbVPp6m",
        "replyto": "5-3YJbVPp6m",
        "invitation": "ICLR.cc/2023/Conference/Paper2317/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Transfer learning presents different types of problems like non-availability of source data (privacy, costs, etc), or the difference in source and target labels or the target task is different. In this light, the paper proposes to use conditional generative models (CGM) in a two stage proposed method which involves pseudo pretraining (PP) on target architecture on synthesized datasets (CGM),  and then using pseudo semi-supervised learning (P-SSL) on the target data and pseudo samples to tackle the task.  \n\nThe results show that the approach conditioned on the CGM does provide considerable and stable improvements, specifically as compared to scratch training or knowledge distillation. \n\nAdditionally, the empirical experiments reveal that source and target datasets do not necessarily need to be close, and still achieve a good performance gain.    ",
            "strength_and_weaknesses": "**Strength**\n\n1. PP combined with P-SSL gives a strong experimental evidence that one can use standard Generative models and their corresponding classifiers in the context of transfer learning. \n\n2. The proposed method seems stable under different target architectures and datasets, as evidenced by various empirical experiments. \n\n3. The proposed method is demonstrated for classification as well as object detection tasks, which makes it more generic.  \n\n**Weakness**\n\nThe generated samples are then used to learn an encoder representation which is then used to train a P-SSL. Why not use the encoder of generative models to train P-SSL directly, instead of training it on synthetic samples?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear and is not difficult to follow. The motivations are often re-stated multiple times which makes it a bit confusing. I would request the authors to make this aspect clear-er. \n\nThe work quality is quite good. The paper supports its claims with empirical studies mentioned in the appendices.  \n\nThere is enough details in the paper to reproduce the results. \n\n**Typo** \n\nFig. 1 caption, (a) \u2026\u2019sampled source label ys\u2019.\n\n",
            "summary_of_the_review": "The paper demonstrates, through various experiments, the validity of their claims satisfactorily. For me, the work is quite good and gives a lot of take-aways for further research. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2317/Reviewer_JCMy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2317/Reviewer_JCMy"
        ]
    },
    {
        "id": "kEQkgW1AuAG",
        "original": null,
        "number": 4,
        "cdate": 1667933633248,
        "mdate": 1667933633248,
        "ddate": null,
        "tcdate": 1667933633248,
        "tmdate": 1667933633248,
        "tddate": null,
        "forum": "5-3YJbVPp6m",
        "replyto": "5-3YJbVPp6m",
        "invitation": "ICLR.cc/2023/Conference/Paper2317/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper identifies the key assumptions in transfer learning, addressing an interesting problem that how to solve it when some assumptions does not hold from a systematic perspective. This paper proposes a two-way step, first by pseudo pre-training by using a powerful generative model to generate source data set, then using P-SSL, to fine tune the model combined with unsupervised learning. The extensive empirical exps shows that this approach works \"when source dataset is unaccessible\".",
            "strength_and_weaknesses": "Strength:\nThis paper is well written, easy to follow and conducts a lot of experiments from scratch, demonstrating the effectiveness of the proposed approach.\n\nWeaknesses:\nHowever, if thinking it carefully, this paper still didn't really solve the problem when condition 2/3 is missing, and condition 1 is trivial.\nfor condition 1, it is trivial by definition because nowadays most transfer learning tasks assume that source/target class labels are different, that's why pre-training and fine-tuning are so popular.\nfor condition 2, the authors claimed that source dataset is usually accessible, so they propose a solution that using synthetic data from a generative model G_s with uniform label distribution label_s will be good, however, the exp set up for source dataset is ImageNet/CompCars(in sec 4.6), but this paper is using BigGAN as G_s, which is already powerful enough to represent the missing source dataset. So the question is, what if BigGAN is not available? this paper is just playing a game that basically replacing an unaccessible source dataset with a powerful generative model, So in this regard, this paper never works with condition 2 missing. \nthen for condition 3, it is obvious that if source dataset (for pretraining) is given, then we can just pretrain it on architecture A_t, and finetune  A_t on target dataset. So in other words, condition 3 is just a byproduct of condition 2, and this paper is just following the same pre-train -> finetune workflow.\nIn addition, what is the semantic meaning of using an extra unsupervised loss based on [Xie et al., 2020], what will the performance be if without such loss? such ablation study is missing.\n\nIt will be more interesting to see that, in this exp set up, if ImageNet is not available as source dataset, can we resort to other less powerful source data set? e.g, using dogs pics to pretrain model and then finetune a task to recognize cars? Not just using a powerful generative model, which could be used to generate a powerful and representative source data set. The problem of how to find a good source data set in this experimental setup is quite trivial, e.g, like privacy concern mentioned in the introduction is interesting but never discussed.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity 3/4\nQuality 3/4\nNovelty 1/4\nReproducibility 3/4",
            "summary_of_the_review": "This paper tries to solve the problem when pretraining dataset is not accessible, but instead using a powerful generative model as a replacement.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2317/Reviewer_FAEQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2317/Reviewer_FAEQ"
        ]
    }
]