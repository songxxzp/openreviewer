[
    {
        "id": "JdYYwNFJPtp",
        "original": null,
        "number": 1,
        "cdate": 1666502621737,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666502621737,
        "tmdate": 1666635185710,
        "tddate": null,
        "forum": "mAazgkPutZ",
        "replyto": "mAazgkPutZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1930/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies various adaptive training methods and schedules in online continual learning.  ",
            "strength_and_weaknesses": "Strength\nStudying different training methods is interesting in online continual learning. \n\nWeaknesses\nMy understanding of information retention is to avoid catastrophic forgetting. Why not use the standard terminology?\n\nOnline continual learning is a non-stop process, I don\u2019t understand what you mean by convergence. In standard online continual learning, whenever a small batch of data from the stream is collected it is trained in one iteration. New tasks of completely different distributions (with new class labels) come constantly. In such a non-stationary data setting, it is unclear how you know when the convergency should happen.  \n\nYou wrote \u201cWe allow 80 training iterations per time step,\u201d but that is not the standard online continual learning setting. My understanding is that you are doing online learning rather than online continual learning. In online continual learning, it is assumed that the system sees the data only once. Once a small batch of training data comes it is immediately trained in one iteration. Some classes as a task may come early and some classes may come later in a sequence of tasks. So, the distribution change at the task switch is drastic and the time of each such switch is unknown. I don't think that you are working under this setting, but I am not familiar with the datasets that you are using. In the standard online continual learning setting, it is unclear how your adaptive training can be applied. \n\nAgain, I think you are working on online/incremental learning rather than online continual learning. Then, you should compare with methods in online learning literature. I'm not working in that field, so I cannot give you any references off the top of my head. But I did read some papers in the area a while ago. \n\nSince your data is not commonly used in online continual learning, it is hard for me to see how your method performs against other methods. There are many online continual learning benchmark datasets. I think you should compare with those existing online continual learning baselines using the existing benchmarks. I am surprised that you did not compare with any existing online continual learning baselines, but only various variants of training methods. If no comparison, I won\u2019t know that your information retention is better. For example, some recent systems are (you can find a lot of other baselines in these papers): \n1. Mai et al. Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning. CVPR workshop\u20192021.\n2. Bang et al. Rainbow memory: Continual learning with a memory of diverse samples. CVPR\u20192021.\n3. Guo et al. Online Continual Learning through Mutual Information Maximization, ICML\u20192022. \n\nIf you are working under a different setting, you should have compared with the existing setting and stated the differences. The existing online continual learning systems should be easily adapted for your setting by simply learning one task of all classes. Actually, in the case, existing online learning methods may do better.\n\nIt is unclear whether you are using your adaptive training schedule at each time step or in the time span of the whole training process. My guess it is the latter. \n\nThe figures in Figure 1 are too small to see. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Mostly clear\nNo code uploaded. ",
            "summary_of_the_review": "I think the paper is studying online learning rather than online continual learning. It didn't compare with any existing online learning or online continual learning methods. The adaptive learning methods and schedules proposed in the paper are not  new as they have been used in other areas. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1930/Reviewer_u5yA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1930/Reviewer_u5yA"
        ]
    },
    {
        "id": "Tbzvg0j44S",
        "original": null,
        "number": 2,
        "cdate": 1666615624266,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666615624266,
        "tmdate": 1666615624266,
        "tddate": null,
        "forum": "mAazgkPutZ",
        "replyto": "mAazgkPutZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1930/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studied the problem of improving information retention in online continual learning (OCL), the convergence of SGD in OCL are theoretical analyzed. Besides, an Adaptive Moving Average (AMA) Optimizer and a Moving-Average-based Learning Rate Schedule (MALR) to optimize the pure replay objective online are proposed. Experiment on several large scale continual learning benchmarks demonstrated the effectiveness of AMA+MALR.",
            "strength_and_weaknesses": "Strength: \nThis paper considers an interesting problem, it is clearly presented and readable. The results of numerous experimental results are convincing. The proposed method is easy to implement. A wide range of related studies are discussed.\n\nWeaknesses: \n1.The typesetting in some places is out of order, such as equations (23), (25), and (31).\n2. In Page 4, \"A4 bounds the degree of non-stationarity between consecutive iterations\", why this assumption holds?\n3. This author should add more description about the contribution of this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Excellent\nQuality: Good\nNovelty: Good\nReproducibility: Good",
            "summary_of_the_review": "This paper is well written and studies an interesting problem, I think it is marginally above the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1930/Reviewer_Ndan"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1930/Reviewer_Ndan"
        ]
    },
    {
        "id": "tLtnr-vDCU",
        "original": null,
        "number": 3,
        "cdate": 1666683016954,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683016954,
        "tmdate": 1666683016954,
        "tddate": null,
        "forum": "mAazgkPutZ",
        "replyto": "mAazgkPutZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1930/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nIn Online continual Learning scenario, where one needs to deal with non-iid data stream, information retention can be a problem, even using unlimited replay buffer. In this study, applying naive Stochastic Gradient Descent and using constant or decreasing learning rate has been partly associated to the information loss in long term. The authors put forward using heuristics based on adaptive moving average to improve the optimization process, and also propose heuristics for learning rate scheduling. ",
            "strength_and_weaknesses": "Indeed Online continual learning is a challenging scenario and the authors opted for an interesting research topic. \n\nIn my opinion this manuscript could be reorganized in a way that it is more to the point and explain how it is differentiated to the other relevant approaches in the past.   \nIt would be interesting to discuss standard Adam and RMSProp and compare with the proposed heuristics in terms of time and performance. \nAlthough it is mentioned that the authors used a population based search to find the best adaptive point within a convex hull for \\gamma_k, I think making the effort to include the proof could indeed improve the work.\nThe overhead of the proposed approach seems to be high, how would you justify it in an online continual learning scenario. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The manuscript can be improved in terms of quality. \nThe proposed heuristics seem interesting but might not be enough novel to be published in this conference. ",
            "summary_of_the_review": "In my opinion this study needs more work to be organized and to get into the shape to be published.   ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1930/Reviewer_axbU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1930/Reviewer_axbU"
        ]
    },
    {
        "id": "KYVxDK3juDu",
        "original": null,
        "number": 4,
        "cdate": 1667400072881,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667400072881,
        "tmdate": 1667400072881,
        "tddate": null,
        "forum": "mAazgkPutZ",
        "replyto": "mAazgkPutZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1930/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces three main results:\n\n- a theoretical analysis of SGD under distribution shift to highlight limitations of current OCL approaches\n- a new OCL algorithm based on moving averages (AMA)\n- an associated heuristic to tune the learning rate also based on moving averages (MALR)\n\nother smaller contributions are a population-based approach to tune the EMA weights and empirical evaluation on multiple datasets",
            "strength_and_weaknesses": "The strength of the papers are mainly in the simplicity of the resulting algorithm, and in the thorough exposition of the result (attempt at theoretically grounding the approach, multiple ablations, etc...)\n\nThe main weaknesses are:\n\na) the algorithm analysis is only for unlimited memory SGD. The final algorithm proposed is finite-memory and employs EMA. This is an enormous disconnect, and makes Thm. 1 more of a help to exposition in the paper than as a contribution. Section A.2 only gives a very brief and informal idea of how parts of this gap can be filled. The authors should either report a theoretical result more aligned with what the algorithm is actually doing, or clarify that Thm. 1 is mostly for illustrative purposes.\n\nb) The final algorithm seems simple but hides its complexity behind several design choice. A partial list of values to tune includes K_W, K_V, K_M, delta, and how the train/validation splits should be chosen in practice both for AMA and MALR. The conditions C1-3 also introduce extra tunable parameters, without much discussion (e.g. what happens without C3?). Only a fraction of these values are ablated in the appendix. Giving advice on how to improve this is hard, as even the obvious thing (extensive ablation study) might change greatly from dataset to dataset.\n\nc) The only baseline is SGD+RWP. This is enough to justify that the approach is superior to SGD+RWP, but many other approaches for OCL exist in the literature with varying degrees of complexity. Even a quick search yields several off-the-shelf tools to set up some baselines (e.g. https://github.com/ContinualAI/continual-learning-baselines) and at least a few of them should be included to gauge how much AMA-MALR can learn on these datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively clear, with good explainations of the reasoning behind the algorithm choices. The main improvements from a clarity perspective would be a better listing of all the tunable parameters and their effect, and more explainations on the gaps between the theoretical inspiration and the final algorithm.\n\nThe final algorithm is definetly novel, mostly as a new combination of existing techniques from optimization. The theoretical analysis is a straightforward extension of (Ghadimi & Lan, 2013) and (Besbes et al., 2015).\n\nThe paper is overall good quality, but falls slightly short since the final performance is not really compared with competitive baselines, and the theoretical analysis is quite far from the algorithmic part.\n\nDespite the large numbers of hyperparameters, the authors make a good job in taking care of reporting them in the paper to help with reproducibility.  ",
            "summary_of_the_review": "Overall the paper falls a bit short of the acceptance threshold due to the lack of focus in the contribution, where the authors try to provide results from three sides (theoretical, algorithmic and experimental) but only achieve small gains in each. In particular:\n- The theoretical contribution is a straightforward extension of existing results, and only act as a guideline in designing the algorithm as it cannot be applied to it. This can be fixed by a more realistic analysis of AMA+MALR.\n- The algorithmic contribution is novel and based only on familiar and well established tools, but its impact is reduced by the large number of hyperparameters involved in the actual implementation.\n- The experimental results are good, but not rigorously evaluated against strong baselines and might be greatly influenced by the large number of hyperparameters.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1930/Reviewer_mh9H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1930/Reviewer_mh9H"
        ]
    }
]