[
    {
        "id": "H3XI3a5Fidp",
        "original": null,
        "number": 1,
        "cdate": 1665845023531,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665845023531,
        "tmdate": 1668953968682,
        "tddate": null,
        "forum": "ik91mY-2GN",
        "replyto": "ik91mY-2GN",
        "invitation": "ICLR.cc/2023/Conference/Paper1931/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a method for learning diffusion models over neural fields. More specifically, the authors perform diffuson directly on coordinate (such as pixel locations) and feature (such as RGB values) pairs, allowing the model to handle a wide variety of data that can be expressed as a neural field. The authors evaluate their model on small image datasets, 3D shapes and spherical images, obtaining favorable results compared to recent baselines.",
            "strength_and_weaknesses": "\n**Strengths**\n\n- The paper tackles an interesting and timely problem. With the recent surge of interest both in neural fields and diffusion models, I believe this paper will be of interest to the ICLR community\n- The method is fairly novel. Previous works on learning distributions of neural fields have used a two stage approach (embedding neural fields into a latent vector and then learning a distribution over these) which has limitations discussed by the authors. Learning directly on coordinate and feature pairs has been considered in the context of GANs before with GASP, but the results in this paper are considerably stronger\n- The experimental results are generally thorough and interesting. The model performs favorably compared to several recent baselines\n- There is a good discussion of related work and the paper is quite well situated in the literature\n- I like the ablation comparing each element of DDPM and the proposed model in the appendix. It's interesting to see what \"makes the model work\"\n- It is nice to see how seamlessly the model works on spherical data\n\n**Weaknesses**\n\n- While most of the model is fairly well explained, it was not very clear to me exactly how context and query pairs were treated differently until reading the appendix (e.g. in Algorithm 1 and Figure 3, context and query points are treated exactly the same way). I think the paper would benefit from a clearer explanation of this as it is quite crucial to how the model works. It could for example be useful to have a diagram of the score network either in the main text or appendix to clarify this as it's currently not very clear\n- There are no qualitative or quantitative comparisons for the spherical data experiments. As both functa and GASP consider generative modeling on spherical data it would be useful to have these as baselines to compare to\n- The model appears to be computationally expensive which I believe is quite important and deserves to be discussed in more detail in the main text. The two stage process used by GEM and functa for example makes training a generative model fairly cheap, as the model is only fed a relatively small latent vector. Further, the authors only train on fairly small images (up to 64x64) and still require 8 A100 GPUs to train, which is quite heavy. While it is okay for the model to have this weakness, it should be more openly acknowledged in the main text\n- The results do not match SOTA compared to specialized models on images for example. However, as learning generative models on neural fields is a fairly new problem, I don't believe it is very crucial for these models to outperform highly optimized convolutional models yet. This is therefore only a minor weakness in my opinion\n- The proposed model is quite closely related to neural processes (also modeling distributions of fields and using a context/query pair setup). I think the paper would benefit from a more thorough discussion of neural processes in the related work. While I realise there are space constraints, I don't think the extended discussion of GANs is necessary, so this could be replaced with a discussion of neural processes\n- There is some confusion about the naming of baselines. I believe a better naming convention would be functa for Dupont 2022a and GASP for Dupont 2022b. I'm not sure exactly why FDN was chosen as the name for functa. Further, it seems that for some reason in the GEM paper, the authors refer to GASP as FDN, whereas in this paper the authors refer to functa as FDN, leading to even more confusion (I am not sure where FDN comes from as it does not seem to be mentioned in either of Dupont 2022a/b). In addition, in the 3D experiments, the baseline is incorrectly referred to as FDN (Dupont 2022a), when it is actually GASP (Dupont 2022b). In short, I think it would be better to call Dupont 2022a functa (or something similar and more intuitive). In addition, the name of the baseline in the 3D geometry experiments is incorrect and should be changed to GASP (as it's referred to in the rest of the paper).",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nWhile most of the paper is generally clear and the figures are nice, the architecture of the score network and the treatment of context and query pairs could be further clarified.\n\n**Quality**\n\nThe authors present an interesting method with a nice experimental evaluation and a good discussion of related work. As such I believe this is a quite high quality paper.\n\n**Novelty**\n\nWhile generative modeling of neural fields has been explored before, the authors provide a fairly novel method for learning diffusion models directly on neural fields (without an intermediate stage of mapping the fields to a latent space).\n\n**Reproducibility**\n\nThe details and explanations provided in the paper seem to be sufficient for reproducing the results in the paper. In addition, the appendix contains detailed tables of hyperparameters for all experiments.",
            "summary_of_the_review": "Overall I think this is a fairly strong paper which will be of interest to the ICLR community. The model is timely and the experimental results are quite convincing. While there are some issues around clarity and a few other minor weaknesses, I still believe the strengths outweigh the weaknesses and so I recommend a weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1931/Reviewer_6arD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1931/Reviewer_6arD"
        ]
    },
    {
        "id": "ozLCPWv97k",
        "original": null,
        "number": 2,
        "cdate": 1666580017450,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580017450,
        "tmdate": 1669059078646,
        "tddate": null,
        "forum": "ik91mY-2GN",
        "replyto": "ik91mY-2GN",
        "invitation": "ICLR.cc/2023/Conference/Paper1931/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a diffusion model for an explicit neural fields. They extend the regular diffusion model training and sampling, but instead of assuming an explicit domain for the signal, like 2D grid for images, their model is general enough to work for all kinds of domain through neural field representation. That is possible because the diffusion model is learnt over those fields, $f : \\mathcal{M} \\to \\mathcal{Y}$. Another key feature of the paper is that same score model architecture applies to all kinds of data like images, 3D geometric data or data on $\\mathbb{S}^2$. This is achieved by using transformer as the score function. ",
            "strength_and_weaknesses": "The field of representing data as neural field has recently seen popularity. This work is new iin diffusion models in the sense that it is probably the first to work with field representation of data, which is a positive aspect of the paper. Also, this paper uses the transformer architecture as a score function, which allows it to propose a common architecture for all kinds of data. This is a good contribution and might encourage more use of transformer architecture in the future. \n\nHowever, the technical contribution of the paper seems to be limited. Mainly using coordinate signal pair and having a transformer architecture that can process such a paired information appropriately is the key idea. While training, the authors use context and query pairs. I believe a bit of more analysis, perhaps theoretical, on how to choose these context and query pairs in better way could strengthen the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear, and it has some novel ideas since it extends the diffusion model to neural field representation of data. It seems the contribution is a bit insufficient (see above comments). Paper seems to have sufficient information for reproducibility. I hope code will be made available upon acceptance.",
            "summary_of_the_review": "The paper is easy to follow and extend the diffusion model to the field representation of data. The use of transformer architecture allows the  common architecture for data of different types. However, the contribution seems to be insufficient. So, I am slightly on the negative side.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1931/Reviewer_E4qq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1931/Reviewer_E4qq"
        ]
    },
    {
        "id": "jTuTvV4Boe9",
        "original": null,
        "number": 3,
        "cdate": 1666671137939,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671137939,
        "tmdate": 1666671137939,
        "tddate": null,
        "forum": "ik91mY-2GN",
        "replyto": "ik91mY-2GN",
        "invitation": "ICLR.cc/2023/Conference/Paper1931/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper adapts a generative diffusion approach to neural fields, allowing simple adaptation to different data domains. A diffusion process is modeled on both context and query pairs and training is done with a score network that incorporates both sets of pairs. This framework is one of a few domain-agnostic approaches, and is the only diffusion-based method which does not require a latent vector to specify the field. \n\nA PerceiverIO architecture is used for the score network, and experiments are done with image data, volumetric data, and spherical data. The method performs well when compared to other domain-agnostic approaches, on both quantitative and qualitative comparisons.",
            "strength_and_weaknesses": "Strengths:\n- The approach is the first diffusion-based method that does not use latent vectors to parameterize fields, and hence does not require a step to train for the distributions over latent vectors. \n- The method seems to perform fairly well compared to other domain-agnostic models.\n\nWeaknesses:\n- The clarity of their model explanation with context and query pairs could be improved, I feel.\n- The PerceiverIO framework could also use some explanation, even if brief, given that it's a key part of their method.",
            "clarity,_quality,_novelty_and_reproducibility": "The work was mostly clear in my opinion, but lacked details on their main conceptual contributions, in my opinion. Some questions that lingered after a reading:\n- How does the diffusion model operate differently than the classical DDPM, given the need for some sort of interplay between the context and query pairs? If one reads the Ho et al. paper, there is significantly more detail outlining the exact model and motivations behind the training and sampling procedures. Mostly, I'd like more detail on Equation (6) and how the dependency between context and query pairs is reflected in the loss terms and resulting algorithms.\n- In line 7 of Algorithm 1, should $\\epsilon_Q$ be $\\epsilon_q$?\n- What is the structure of the PerceiverIO, and is it necessary for good performance of the approach, or could another architecture have been used? Some of the explanations from the appendices should be part of the main text, I believe.",
            "summary_of_the_review": "The work provides an interesting diffusion-based approach for generating neural fields. The clarity could use some work, but the empirical results are compelling, and I feel it is worth publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1931/Reviewer_NtLL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1931/Reviewer_NtLL"
        ]
    },
    {
        "id": "qqF-SFYUCkO",
        "original": null,
        "number": 4,
        "cdate": 1666794191041,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794191041,
        "tmdate": 1666794236680,
        "tddate": null,
        "forum": "ik91mY-2GN",
        "replyto": "ik91mY-2GN",
        "invitation": "ICLR.cc/2023/Conference/Paper1931/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an extension of Diffusion Probabilistic Models using Fields to model data.\nThe basic idea is to model data as functions (fields) e.g. images are functions from $R^2$ to $R^3$, a map between pixel coordinates and color pixel value.\nThis approach is motivated by the need to unify the generative model architecture to cover different modality, thus avoiding the need to construct specific architecture for score functions $\\epsilon_\\theta$ which are the denoisers used in the generation process.\nFields are parameterized using pairs of coordinates in source and target domains and evaluated in a similar manner using pairs of query pairs of coordinates.\n\nResults are compelling considering that the Field Model is domain agnostic.\n\nIn the end the method is using a Transformer (Perceiver) to remap coordinates (implementing the field) (Appendix B)\n",
            "strength_and_weaknesses": "Strenghts\n\n- The method is elegant in principle and domain agnostic yet obtains good performance\n\nWeaknesses \n\n- It is not clear how the gradient at step 7 is computed. $epsilon_Q$ is not defined.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Main implementation details are in the Appendix B. Clarifying that the field is implemented as a transformer early on would benefit the reader understanding of the approach.",
            "summary_of_the_review": "Overall the paper is interesting and results are compelling. It has to be noted that the approach claims to be general with respect to the data in input but I feel that the main aspect is that a very flexible and powerful architecture (PerceiverIO) is used as a score function thus enabling this feature. The whole modelling of data as fields seems less relevant than this implementation detail.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1931/Reviewer_joBr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1931/Reviewer_joBr"
        ]
    }
]