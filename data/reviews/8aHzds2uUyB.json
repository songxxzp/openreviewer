[
    {
        "id": "s8wG_fiWKp",
        "original": null,
        "number": 1,
        "cdate": 1666501809269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666501809269,
        "tmdate": 1666501809269,
        "tddate": null,
        "forum": "8aHzds2uUyB",
        "replyto": "8aHzds2uUyB",
        "invitation": "ICLR.cc/2023/Conference/Paper5151/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a library (open-sourced) called RL4LMs, which is for optimizing generation models using RL. The library is compatible with HuggingFace. Second, the paper comes up with a GRUE (general reinforced-language understanding evaluation) benchmark, which consists of six language generation tasks (under the RL framework) containing IMDB text continuation (reward is related to sentiment), a CommonGEN generative common sense task, CNN/DailyMail summarization, ToTTo (data-to-text generation), a machine translation task using a small IWSLT dataset, and NarrativeQA (question answering). The paper also proposes the NLPO algorithm (natural language policy optimization); the difference between NLPO and PPO is in green font in Algorithm 1. Essentially, the policy which we collect trajectories from is an old copy of the current policy. Moreover, initialization is different from PPO.\n",
            "strength_and_weaknesses": "I think the toolkit will be a nice contribution to the research community. It will also encourage more people to dig deeper into the RL for NLP field. \n\nIt\u2019s great that the software contains multiple RL algorithms.\n\nThe paper is clearly written \u2013 it\u2019s effective in getting points across to readers.\n\nHuman annotation is involved, which is great. \n\nConcerns below:\n\nThe selection of tasks isn\u2019t totally satisfactory to me. Generating a positive continuation of movie reviews is too artificial and unchallenging. I think the benchmark (given there\u2019s the word \u201cgeneral\u201d in it) should consider real use cases of RL for NLP; for example: generating better dialogue responses where the reward is a human-preference-based metric or a toxicity-based metric. Moreover, IWSLT17 English-German is also quite artificial and IWSLT-trained systems will never be put into production or real use. Additionally, most of the tasks have a correct answer (or the space of correct generations is quite small), but there are many text generation tasks where the correct space of generations is very large, like in dialogue or story generation. I think these tasks should be taken into consideration as well.\n\nOne concern I have about the NLPO algorithm is that if the masked policy is an old copy, then the algorithm is not on-policy anymore. Then, the policy gradient derivation (e.g., see the Berkeley deep RL course policy gradient derivation, or any other policy gradient derivation) would not hold anymore. How do the authors justify the fact that we\u2019re not able to do the policy gradient proof anymore (please correct me if I am wrong)? Or why does this periodic synchronization trick work? I would hope for a summary of either a theoretical analysis or empirical analysis. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Neutral comment: The Figure 2 plots initially looked weird to me (e.g., why would there be a line connecting two unrelated tasks), but later I realized it\u2019s a clear way to demonstrate the aggregate performance (because we can look at the area). \n",
            "summary_of_the_review": "Toolkit -- nice contribution. Selection of task for the general benchmark -- not satisfactory to me. The NLPO algorithm is clever, but it needs more justification. In general, I'm leaning toward accepting because of the software and the encouraging empirical results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5151/Reviewer_a1G9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5151/Reviewer_a1G9"
        ]
    },
    {
        "id": "4LxL40awo1",
        "original": null,
        "number": 2,
        "cdate": 1666637499292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637499292,
        "tmdate": 1666637499292,
        "tddate": null,
        "forum": "8aHzds2uUyB",
        "replyto": "8aHzds2uUyB",
        "invitation": "ICLR.cc/2023/Conference/Paper5151/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors introduce a new library for benchmarking RL methods on NLP tasks, along with a modified version of PPO for on-policy optimization. Results demonstrate that the proposed NLPO algorithm outperforms the PPO baseline according to human assessment.",
            "strength_and_weaknesses": "Strength:\n\u2022 This paper builds a cross-task diverse evaluation benchmark containing 6 tasks.\n\u2022 For the proposed NLPO, human evaluation and automatic evaluation show it outperforms the PPO baseline\n\n\nWeakness:\n\u2022 With the mask prediction policy update, the gain based on human evaluation seems to be marginal except on IMDB task metrics, and CNN naturalness metric\n\u2022 The proposed NLPO in this paper seems to be similar to [1] ? \n\u2022 In experiments, there no comparison to traditional structure learning methods like [2]\n\n[1] Donati, A.M., Quispe, G., Ollion, C., Corff, S.L., Strub, F., & Pietquin, O. (2021). Learning Natural Language Generation from Scratch. ArXiv, abs/2109.09371.\n\n[2] Shu, R., Yoo, K., & Ha, J. (2021). Reward Optimization for Neural Machine Translation with Learned Metrics. ArXiv, abs/2104.07541.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has a clear writing and builds a library for benchmarking on a range of tasks. There is no concern on clarity.",
            "summary_of_the_review": "I would like to evaluate the contribution of this paper focused on the proposed NLPO method. I will lean towards an acceptation if the proposed method is indeed novel, results in state-of-the-art performance and the improvement is significant enough comparing to PPO.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5151/Reviewer_pVj7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5151/Reviewer_pVj7"
        ]
    },
    {
        "id": "0FjLw98tDvX",
        "original": null,
        "number": 3,
        "cdate": 1666668709783,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668709783,
        "tmdate": 1666668709783,
        "tddate": null,
        "forum": "8aHzds2uUyB",
        "replyto": "8aHzds2uUyB",
        "invitation": "ICLR.cc/2023/Conference/Paper5151/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work aims to explore whether reinforcement learning (RL) can be a useful part of NLP generative models. Although RL methods have been used fairly often, there have been disagreements among experts about whether it provides meaningful gains, especially in light of training difficulties (eg enormous action spaces of 30k+). In this work, the authors develop an open source library (6 tasks, 15 metrics, 3 learning algorithms), create a new state-of-the-art RL learning algorithm, create the first RL-NLP leaderboard, and use MTurk to annotate 4 of those 6 datasets with human-validated labels. It finds that tasks involving heavy novelty (ie \"zero-shot performance\") do seem to benefit from training with RL.",
            "strength_and_weaknesses": "STRENGTHS\n- This work is very comprehensive. It makes public a large library of many tasks, rewards/metrics, and learning algorithms.\n- The learning algorithm (NLPO) outperforms the previous state of the art (PPO)\n- In addition to the above work, further validating 6 of the datasets with MTurk is impressively thorough\n\nWEAKNESSES\n- The work could be more clear on pages 4 and 5 when describing on-policy actor-critic algorithms (3.3) and NLPO (4).\n\nMINOR\n- There appears to be a typo on page 8 \"2 out of 4 tasks tasks\". The word \"tasks\" is repeated.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: For the most part, the writing in this paper is very clear.\n\nQuality: Although RL is outside my area of expertise, the work appears to be high-quality. It implements 15 metrics/rewards and compares against 6 datasets, which is difficult to engineer.\n\nNovelty: Based on a wide survey of RL-NLP lit, the authors \"hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.\" Based on this insight, they develop a new learning algorithm (NLPO) which improves upon this shortcoming and outperforms the existing standard (PPO, as well as presumably REINFORCE, which it claims is strictly inferior to PPO based on previous work)\n\nReproducibility: By making their code (and datasets) into a public library, this work is very reproducible. Train/test splits of the datasets included in GRUE follow the splits of the original papers.",
            "summary_of_the_review": "This work is very comprehensive: the authors develop an open source library (6 tasks, 15 metrics, 3 learning algorithms), create a new state-of-the-art RL learning algorithm, create the first RL-NLP leaderboard, and use MTurk to annotate 4 of those 6 datasets with human-validated labels. It finds that tasks involving heavy novelty (ie \"zero-shot performance\") do seem to benefit from training with RL. By making their code into a public benchmark, they provide a very helpful starting point for the community to develop & compare future work against. Finally, they provide a good lit review of many (though not every) instance of RL in NLP, commenting on its shortcomings to date.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5151/Reviewer_H8br"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5151/Reviewer_H8br"
        ]
    },
    {
        "id": "G2YiJCQtIm",
        "original": null,
        "number": 4,
        "cdate": 1666841797766,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666841797766,
        "tmdate": 1666841797766,
        "tddate": null,
        "forum": "8aHzds2uUyB",
        "replyto": "8aHzds2uUyB",
        "invitation": "ICLR.cc/2023/Conference/Paper5151/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies RL on language models, and makes 3 contributions:\n1) RL4LMs, a modular library for optimizing language generators with RL.\n2) GRUE, a benchmark of 6 language generation tasks with reward functions.\n3) NLPO, a new algorithm which improves on PPO for RL on LMs. ",
            "strength_and_weaknesses": "Strengths:\n+ The lack of open-source benchmarks and tasks for RL on language models is indeed a huge problem. This paper does the community a huge service by providing this library + benchmarks. \n+ The paper is clearly written and easy to follow.\n+ The paper is quite thorough. I appreciate the many ablations, and the study of human agreement with automatic metrics.\n+ The NLPO algorithm is simple, clearly explained, and does seem to provide an improvement over PPO. It remains to be seen if this will transfer to other, more complicated tasks + larger scale. (it does involve an extra copy of the policy weights, which has memory downsides, especially at large scale, so it's unclear if it will be feasible / worth the tradeoff). Though overall I think this is a small part of the contribution.\n+ The paper has a useful section on important implementation details.\n\nWeaknesses: \n- personal nit: I don't think CNN/DM is a very good summarization dataset (though it is very standard). \n- I think the biggest weakness of this benchmark / tasks is that it relies on simple automatic metrics (eg BLEU, sentiment score). This makes sense of course, since it makes it tractable to evaluate. But I suspect that, once people start optimizing these metrics directly, whatever metric correlation with human judgments exists will disappear. I think it's important to consider how to incorporate human data collection into this process. (This would obviously be a very big endeavor, so I don't think it's within scope for this paper, but I think is a useful future direction.)  One option is to hold regular competitions (eg at a conference) where you collect human data which you use to evaluate models, and then open-source that data for further training (similar things have been done in the context of dialog models). Basically, I think it will require quite a bit of effort to ensure this benchmark remains useful over time, and doesn't saturate like every other NLP benchmark.\n\nSmall notes:\n- \"We find that using RL to learn from scalar reward feedback is can be more\"\n> remove \"is\"?\n\n- \"high-quality estimates..\"\n> two periods\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nAll of these are strong, particularly reproducibility. Novelty is a bit lower, but that is less important for a paper like this.\n\n\nQuestions:\n- I'm very curious why the PPO + NLPO models do so badly without starting from the supervised init. Any ideas? (Never mind, I see this is addressed in 5.2). \n\n",
            "summary_of_the_review": "My personal belief is that fine-tuning language models to optimize specific metrics that are not perplexity (particularly human preferences) is going to be a huge field, perhaps one of the most impactful in ML. This paper clearly advances the accessibility of research on this problem. Thus, I think this paper may become an enormous contribution to the field. I'd be excited for the datasets and tasks to be updated regularly. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5151/Reviewer_zDTr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5151/Reviewer_zDTr"
        ]
    }
]