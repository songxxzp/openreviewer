[
    {
        "id": "X2O602RiR6t",
        "original": null,
        "number": 1,
        "cdate": 1666365331173,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666365331173,
        "tmdate": 1667639878767,
        "tddate": null,
        "forum": "Hu4r-dedqR0",
        "replyto": "Hu4r-dedqR0",
        "invitation": "ICLR.cc/2023/Conference/Paper3321/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a phrase-level NLI model that outperforms previous baselines for NLI on SNLI dataset. The proposed model also has improved interpretability thanks to the phrase-level similarity matrix. ",
            "strength_and_weaknesses": "Strength:\n1. The proposed model achieves significant improvement in sentence level accuracy when compared to the previous reasoning model baselines, namely NNL and STP, on the SNLI dataset.\n2. Good amount of ablation studies were performed to understand the contributions of each component (e.g. phrase chunking, phrase alignment, fine-tuning etc) .\n3. The model is interpretable to some degree in that it can utilize the phrase-level similarity matrix to generate phrase pairs that contribute to the inference results.\n\nWeakness:\n1. Even with the sophisticated design, the model performance still lacks behind a non ad-hoc transformer model (Transformer model in Table 2). While the added inductive-bias does add more interpretability, this seems to suggest that training universal large LM is still the best performant choice, as demonstrated by various big LM papers.  \n2. The proposed phrasal alignment only works when the dataset are well curated and don't use synonyms, I wonder how much it will be applicable in the wild. Specifically, 1) How well does sentence BERT produced embeddings deal with synonyms and different phrasing of the same content? Is there any analysis on this. 2) Real world NL texts are not as concise as in SNLI, there can be added redudant adjectives, disfluencies,  reversed sentence structure. I wonder if alignment will fail for some of these cases.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with clear explanations and helpful illustrations. Quality is good and there is some novelty. Codes are available but I didn't check if it is runnable.",
            "summary_of_the_review": "The paper proposes a NLI model that has improved explainability. Evaluation is through, even though results are still not on-par with general purpose transformer models. The proposed method also may have trouble for uncurated data.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3321/Reviewer_vyjc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3321/Reviewer_vyjc"
        ]
    },
    {
        "id": "atEe6BMmyE6",
        "original": null,
        "number": 2,
        "cdate": 1666717265025,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717265025,
        "tmdate": 1668151807115,
        "tddate": null,
        "forum": "Hu4r-dedqR0",
        "replyto": "Hu4r-dedqR0",
        "invitation": "ICLR.cc/2023/Conference/Paper3321/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new method for sentence-level natural language inference, which uses intermediate phrase-level entailment annotations to reach to a sentence-level conclusion. First, content phrases are extracted based on a set of heuristics. Secondly, an alignment is found between phrases from the premise and phrases from the hypothesis according to their similarity given by a pretrained text-embedding model. Third, a classification layer predicts class probabilities for each of the alignment pairs. This classification layer is not trained directly from alignment-pair labels, but instead, the predicted probabilities are combined into sentence-level entailment class probabilities, for which labels are available. Hence, phrase-level classification is trained implicitly in a weakly supervised way through sentence-level training.\nThe model is evaluated on three main tasks: Sentence-level entailment, phrase-level entailment (for which a dataset is created), and explanation generation.\nWhile the proposed model achieves competitive results at the sentence-level, it is the only model capable of phrase-level entailment. For explanation generation, it is demonstrated that inclusion of phrase-level annotations obtained from the model helps to generate better explanations.\n",
            "strength_and_weaknesses": "Strengths:\n* many contributions: a model capable of phrase-level entailment classification, a dataset for evaluation, ablations, multiple use cases\n* the paper is well written\n* significant interest to the NLP community\n\nWeaknesses:\n**Update:** The following weaknesses have been resolved during rebuttal.\n* some ablations are missing or too unclear to fully support all claims:\n    * You claim that EPR achieves a good balance between sentence-level and phrase-level performance. However, it is not clear in how far the provided non-reasoning baselines are comparable, as they are trained with different underlying pretrained models. If I understand correctly, you are missing a controlled experiment where you use the same underlying LM as your model but finetune it in the \"conventional\" way, i.e., by plugging a heuristic matching feature layer on top of a siamese encoder, or by training it BERT-style.\n    * What is the purpose of your baseline called \"Sentence label Training Phrases\"? In my understanding it is proposed to show that directly using the sentence-level labels for phrase-level predictions is not suitable. But it doesn't appear to be a controlled experiment, because it uses a different underlying language model? Moreover, how do you obtain sentence-level classification for this model if it is only trained on phrase-level classification?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper proposes a new task, including an annotated dataset, and a novel, reasonable method for it. The originality is therefore high. Most claims are well supported by the evidence. The provided code and description of experimental setup should lead to good reproducibility. The paper is generally well written, the proposed method is clear. Some minor points:\n* captions go above tables according to ICLR guidelines\n* non-reasoning baselines are not described, their inclusion is not motivated. Differences in experimental results are not discussed. If you don't discuss something, why include it in the first place?\n* while the model by Mahabadi et al. (2020) is substantially different from the proposed model, it also uses fuzzy logic and similar sentence-level classification ideas. The differences should probably be discussed in the related work section, rather than introducing the reference in the middle of the experimental section.\n* you are missing a report of whether your data annotation process was approved by an ethical review board",
            "summary_of_the_review": "The paper makes a number of valuable contributions and has no major weaknesses. I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3321/Reviewer_72Kh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3321/Reviewer_72Kh"
        ]
    },
    {
        "id": "VylQNsPZgI",
        "original": null,
        "number": 3,
        "cdate": 1667478887559,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667478887559,
        "tmdate": 1667478887559,
        "tddate": null,
        "forum": "Hu4r-dedqR0",
        "replyto": "Hu4r-dedqR0",
        "invitation": "ICLR.cc/2023/Conference/Paper3321/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of the explainability of NLI tasks. To address the problem, this paper presents a phrase-based reasoning approach by first detecting the phrases in the given two sentences and aligning those phrases between the sentences. Then, the model is trained to predict the NLI label for the aligned phrases and induce the sentence label by fuzzy logic formulas. With such settings, the model is able to interpret the relationships between the phrases, thus contributing to the explainability of the NLI model. ",
            "strength_and_weaknesses": "Strength:\n\n1. The setting of phrase-level NLI is interesting, and is intuitively beneficial for improving the interpretability of neural models.\n\n2. The technical design is sound, and the effectiveness is verified on several datasets.\n\n3. The annotated phrasal logical labels and designed metrics would be useful resources to the community.\n\nWeakness:\n\n1. The presentation of the experimental part is not quite smooth. Both SNLI and e-SNLI datasets are used in the experiments, but it is not clear which datasets the results in Tables 2-3 are based on. A clear indication in the table captions would be better. Besides, as SNLI is mentioned as the main dataset used in the experiments (Section 4.1), my curiosity is raised about how the model will perform in the standard setting of SNLI (i.e., only considering the sentence-level accuracy). I went through the experiments but did find such results. In my understanding, the technique is supposed to be not only useful for the explanation but also generally effective in the standard settings of NLI. \n\n2. Definition of \"semantic units\" is controversial. Extracting the phrases is the foundation part of this work. According to the details in Appendix A.1, the phrases are extracted based on POS tags. It would be controversial to call those POS-based phrases as \"semantic units\". In the research line of NLI, there are studies that use semantic roles, which are more likely to be \"semantic units\". Clarification about this definition would be beneficial.\n\nIn addition, as implied in Section 4.2, using different \"semantic units\" influences the results dramatically. It would also be helpful to see an ablation study by comparing different kinds of language units (e.g., semantic role labels, fact units, etc.), as those units have been more commonly used as fine-grained units in existing studies.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the clarity is good in general but some details in the experiment could be more clear.\n\nQuality: This work is of good quality.\n\nNovelty: The idea is novel in the scope of NLI.\n\nReproducibility: The method is clearly described and the authors claim that the source codes will be open.",
            "summary_of_the_review": "Overall, this work is interesting in the scope of the NLI field. But the technique is heavily customized to the NLI task, it might have less impact to general audiences. The presentation could also be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3321/Reviewer_K5wq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3321/Reviewer_K5wq"
        ]
    },
    {
        "id": "uzscxuYOb_",
        "original": null,
        "number": 4,
        "cdate": 1667506981512,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667506981512,
        "tmdate": 1668031092257,
        "tddate": null,
        "forum": "Hu4r-dedqR0",
        "replyto": "Hu4r-dedqR0",
        "invitation": "ICLR.cc/2023/Conference/Paper3321/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an interpretable method for Natural Language Inference: first, the premise and hypothesis sentences are splitted into \u201cphrases\u201d (or sub-sentences) with a custom heuristic method, then a pre-trained sentence-BERT is used with cosine-similarity to align phrases from the premise to phrases from the hypothesis, then a Transformer network predicts a NLI label (Entailment / Contradiction / Neutral) for each pair of aligned phrases. Finally, a set of fuzzy logical rules combine those predictions to infer the overall sentence pair NLI label.\n\nExperiments show that the phrase alignments and their NLI labels agree with human judgment. However it seems like the overall NLI prediction is not beating a simpler Transformer baseline.\n",
            "strength_and_weaknesses": "**Strengths**:\n\nThis work proposes a novel technique to do NLI at a sub-sentence level. It is interesting to analyze this behavior in order to better understand how the model behaves. \n\nIn addition, this work proposes a heuristic approach relying on SpaCy tagger to create the sub-sentence chunks.\n\nEventually, the set of fuzzy logical rules is also a contribution of this work.\n\n**question** :  can the authors report how the quality of the phrase definition influences the performance of the sentence-BERT alignment which in turn influences the overall NLI accuracy?\n\n**Weaknesses**:\n\nFrom table 2 it looks like the proposed method is not better than the Transformer baseline on the overall sentence accuracy. It should be mentioned that this work proposes an interpretable way of performing the task of NLI by essentially doing NLI on sub-phrases but this is at the cost of a weaker overall performance.\n\nIt is not clear if this work proposes a new task (sub-phrases NLI) or a new method for doing NLI. In both cases it is not clearly shown that predicting \u201cphrases\u201d NLI labels is beneficial for the NLI task. Maybe the set of fuzzy logical rules are not optimally designed for the model to take advantage of the \u201cphrases\u201d labels? It is very possible that large models rely on other statistical features not captured by our human judgment of what may or may not help performing NLI.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The statement of \u201caddressing logical reasoning for the NLI task\u201d is not justified as logical reasoning for the NLI task is not properly defined. It is not clear what the goal of this work is, what the problem is.\n\nIt is not clear what the term \u201cphrasal reasoning\u201d refers to. It seems like it is simply the task of NLI on sub-sequences. It should be further motivated why this is considered reasoning.\n\nQuestion: where in Figure 2.c is the Fuzzy Logic step? Is the \u201cnormalize\u201d box doing the fuzzy logic? I would make the paper more clear to include that step in the Figure 2.c.\n",
            "summary_of_the_review": "This paper represents a lot of work but its motivation is not clearly presented. It is interesting to analyze sub-phrases NLI alignment but results do not show a strong improvement over a transformer baseline on the NLI task. The motivation of this work should be clearly mentioned at the beginning of the paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3321/Reviewer_VdSJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3321/Reviewer_VdSJ"
        ]
    }
]