[
    {
        "id": "ZWUQQjSB-HQ",
        "original": null,
        "number": 1,
        "cdate": 1666105897366,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666105897366,
        "tmdate": 1669549933022,
        "tddate": null,
        "forum": "TTLLGx3eet",
        "replyto": "TTLLGx3eet",
        "invitation": "ICLR.cc/2023/Conference/Paper3403/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a sequential algorithm for feature selection, where a feature is selected in each step. The intuition behind the idea is to minimize the selection of redundant features. A theoretical study is also presented, matching the proposed algorithm with other state-of-the-art methods like OMP or Sequential LASSO. The experimental results show promising accuracy scores when compared against other incremental Feature Selection methods.",
            "strength_and_weaknesses": "Strengths:\n* The idea is easy to implement\n* The theoretical explanation is interesting\n* The spatial complexity of the algorithm is low\n\nWeaknesses:\n* The idea is somehow trivial\n* The algorithm time complexity is extremely high\n* The experimental results only cover other incremental Feature Selection methods.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Reproducibility:\n* The algorithm is easy to reproduce. However, some hyper-parameters appears to be missing. The value for $\\lambda$ in the sequential LASSO is not explained. Some information regarding the training procedure (number of epochs, optimizer, ...) is also missing.\n\nNovelty:\n* The use of a softmax mask attached to the input was previously discussed in [1]. It is worth mentioning the authors reduce the matrix mask presented in [1] to a binary vector, but it increases its computational cost. Other approaches like DFS [2] or SFS [3] provide a similar structure, using only one extra vector that multiplies the input data.\n\nClarity:\n* The algorithm is easy to understand and implement. The theoretical explanation is also interesting, adding extra value to the contribution.\n\nQuality:\n* The experimental section is poor. The authors only compared their algorithm against other incremental FS algorithms. Although it is a good sign to establish similar or better results in this specific approach, there exists multiple FS algorithms with better results than the ones provided in the paper. There is a lack of comparison against similar FS architectures like CAE [1], DFS[2] or SFS[3].\n\n* The use of the softmax function over the mask can be difficult when the number of features is very high. It could case the mask initialization to have values extremely close to zero. This could led to a difficult mask training, as the gradient over those vector has also close to zero values. I suggest the authors to maybe include a temperature parameter to their softmax function.\n\n\n\n[1] Bal\u0131n, M. F., Abid, A., & Zou, J. (2019, May). Concrete autoencoders: Differentiable feature selection and reconstruction. In International conference on machine learning (pp. 444-453). PMLR.\n\n[2] Zou, Q., Ni, L., Zhang, T., & Wang, Q. (2015). Deep learning based feature selection for remote sensing scene classification. IEEE Geoscience and Remote Sensing Letters, 12(11), 2321-2325.\n\n[3] Cancela, B., Bol\u00f3n-Canedo, V., Alonso-Betanzos, A., & Gama, J. (2020). A scalable saliency-based feature selection method with instance-level information. Knowledge-Based Systems, 192, 105326.",
            "summary_of_the_review": "Overall, I think it is an interesting solution, but more work has to be done before publishing it. More comparisons against other FS algorithms have to be performed. Besides that, I think the novel contribution of this algorithm is questionable, as the solution is extremely similar to the mentioned OMP and sequential LASSO.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3403/Reviewer_vzVg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3403/Reviewer_vzVg"
        ]
    },
    {
        "id": "OZggcxrQt0",
        "original": null,
        "number": 2,
        "cdate": 1666560310702,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560310702,
        "tmdate": 1666560310702,
        "tddate": null,
        "forum": "TTLLGx3eet",
        "replyto": "TTLLGx3eet",
        "invitation": "ICLR.cc/2023/Conference/Paper3403/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work introduces the Sequential Attention algorithm for supervised feature selection. This algorithm is based on an efficient implementation of greedy forward selection and uses attention weights at each step as a proxy for marginal feature importance. \nThe authors provide theoretical insights into the Sequential Attention algorithm for linear regression models by showing that its regularized linear Sequential Attention model is equivalent to sequential LASSO and Orthogonal Matching Pursuit, and thus inherits all of their provable guarantees. Their theoretical and empirical analyses provide new explanations towards the effectiveness of attention and its connections to overparameterization.",
            "strength_and_weaknesses": "Strength\n- The paper is well written and organized.\n- The paper proposes efficient sequential feature selection methods with attention, which reduces the complexity of greedy forward based feature selection.\n- The authors provide provable guarantees for Sequential Attention for least squares linear regression by analyzing its variation called regularized linear Sequential Attention. They show the connection between Sequential LASSO and Orthogonal Matching Pursuit. \n\nWeakness\n- The authors claimed a lot of merits for the proposed algorithms, such as \u201cthis technique reduces the overhead of our algorithm, removes the need to tune unnecessary hyperparameters, works directly with any model architecture,\u2026achieves state-of-the-art feature selection results for neural networks on standard benchmarks\u201d. However, the experiments seem not quite complete to support the claim. For example, there is no experiment demonstrating the approach works on all model architectures.\n- The advantages over existing methods are not clear or significant. The results in Figure 3 on DNNs seems to suggest there is limited advantage over SL, GL or OMP for these datasets. \n- The scale of the experiment on DNN is quite small and limited. The experiments on neural networks are done with only a one-layer neural network with hidden width 67 and ReLU activation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and written. The novelty of the paper mainly lies in the theoretical analysis of the connection between the sequential attention and existing approaches. The algorithm is clearly described and should be reproducible.",
            "summary_of_the_review": "The novelty mainly lies in theoretical analysis. The theoretical and experiments both suggest the proposed approach is similar to the existing approach and the benefits are not clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3403/Reviewer_FwQ2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3403/Reviewer_FwQ2"
        ]
    },
    {
        "id": "ppFLJoS_SyN",
        "original": null,
        "number": 3,
        "cdate": 1666659672444,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659672444,
        "tmdate": 1669075140772,
        "tddate": null,
        "forum": "TTLLGx3eet",
        "replyto": "TTLLGx3eet",
        "invitation": "ICLR.cc/2023/Conference/Paper3403/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to perform feature selection by learning to weigh the input features with probabilities given by a softmax transformation. Given the similarity to attention methods, the method is called Sequential Attention. It consists of running the model with the reminiscent non-selected features and then greedly selecting the features associated with the highest softmax weight. The authors provide a large body of theoretical guarantees, linking an adapted (regularized) version of Sequential Attention to the classical Sequential LASSO algorithm. Results on 7 datasets showcase the predictive performance of the proposed approach.",
            "strength_and_weaknesses": "Things that I liked in this paper:\n- The paper has a strong math foundation and excels at theoretical contributions. \n- The proposed method outperforms related approaches despite being very simple.\n- On top of this, this paper shows empirically and theoretically that \"attention\" is, in fact, a suitable approach to selecting a good subset of features.\n\nHowever, I think the paper can be improved, especially regarding clarity. The paper focuses significantly on the theoretical aspect, while the experimental section is very short, with several important experiments left in the appendix. For example, it was not until section 4.2 that I realized that the previous results were achieved using the standard Sequential Attention shown in Algorithm 1. Finally, I believe that Algorithm 1, in its current form, is very inefficient since it requires training at least $k$ models. \n",
            "clarity,_quality,_novelty_and_reproducibility": "My biggest concern in this paper is regarding clarity. The experimental details are often unspecified or hard to understand. Here are some examples:\n\n- $\\bar{S}$ is not defined anywhere (even though it becomes evident from the paper what it means)\n- Some tables are referenced as figures (e.g., figure 7)\n- What do the shadowed regions represent in Figure 4?\n- It is hard to understand the correlation plots in Figure 14. What was the dataset? Was the model trained until the end? Can you provide more details about this?\n- How was the method implemented efficiently (as stated in the abstract)? What are the running speed and memory consumption? How do they compare to related approaches?\n- It is unclear how the \"empirical success of attention-based feature selection is primarily due to the explicit overparameterization\". The additional parameters are only $d$, a tiny number, even for the experimented datasets. \n- The examples of selected features are very uninformative (probably because of the dataset). Displaying independent pixels is hard to interpret, so it is unclear if the subset of selected features is meaningful.",
            "summary_of_the_review": "Overall this paper provides a great theoretical contribution to attention-based feature selection. It also presents solid empirical results on several datasets, outperforming related approaches. However, I believe it has room for improvement, especially regarding the clarity of the experimental setup.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3403/Reviewer_tvep"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3403/Reviewer_tvep"
        ]
    },
    {
        "id": "bFNa1wJ73Lz",
        "original": null,
        "number": 4,
        "cdate": 1666688388023,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688388023,
        "tmdate": 1666688388023,
        "tddate": null,
        "forum": "TTLLGx3eet",
        "replyto": "TTLLGx3eet",
        "invitation": "ICLR.cc/2023/Conference/Paper3403/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is proposing a forward feature selection algorithm with the use of attention mechanism for assessing the feature relevance of currently not selected features. Since attention mechanism consider all unselected features at once, there is computational saving, compared to the common greedy forward selection algorithm. A specific instance of the Sequential attention feature selection framework, was shown to be equivalent to Orthogonal Matching Pursuit, thus inheriting theoretical guarantees on the quality of approximation. ",
            "strength_and_weaknesses": "Strengths\nWhile equivalence of Sequential LASSO and OMP was shown under several assumptions before, this work shows the equivalence holds for more general settings. \nClever use of attention to achieve computational savings by evaluating unselected features all at once, at every step (until the prespecified number of features are selected).\n\nWeaknesses\nEmpirical evaluation is not as extensive, however it is performed against relevant feature selection like competitor attention-based feature selection, as well as sequential LASSO and OMP (with neural networks).\nIt is encouraging that the regularized linear Sequential attention version had indistinguishable performance from the softmax based Sequential attention one, on benchmark data, but it is still not \u201cbridging the gap\u201d, but just leaves a chance that it might enjoy theoretical guarantees too. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clearly written study, the methodology seems fairly novel.\n\nMinor comments\nRelated work could have mentioned another relevant feature selection approach MRMR - Maximum Relevance Minimum Redundancy. This instance of greedy forward selection algorithm tends to select a subset of features having the most correlation with the target, but the least correlation wrt already selected features.\nSection 4.2 there is redundant \u201cAlgorithm Algorithm 1\u201d\n",
            "summary_of_the_review": "Based on the perceived novelty and theoretical results, this paper seems like it could be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3403/Reviewer_ABqs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3403/Reviewer_ABqs"
        ]
    }
]