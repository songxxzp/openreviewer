[
    {
        "id": "axhzKk1ng9g",
        "original": null,
        "number": 1,
        "cdate": 1666465231070,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666465231070,
        "tmdate": 1669560279030,
        "tddate": null,
        "forum": "MLJ5TF5FtXH",
        "replyto": "MLJ5TF5FtXH",
        "invitation": "ICLR.cc/2023/Conference/Paper129/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "[REVISED]\n\nTask: The paper considers the relatively unexplored task of making multi-label text classification interpretable by learning a model that induces implicit labeled spans given only sentence-level labels as supervision.\n\nModel: The paper assumes a \"backbone\" model for unsupervised parsing that gives an embedding $e_{i,j}$ for each span $(i,j)$. A conditional label distribution $P(l|n_{i,j})$ is defined at every considered node $n_{i,j}$ based on $e_{i,j}$. In experiments, the embeddings are augmented to incorporate top-down information.\n\nTraining: The model is trained by optimizing $L + L^t$ where $L$ is the usual loss for the backbone model and $L^t$ is the new loss. The new loss fixes the predicted tree from the backbone model $t$ (which changes dynamically during training), then computes the likelihood of $t$ yielding labels that are consistent with the ground-truth labels. Naively, this requires summing over all possible label configurations of $t$ which is exponential in the label size. The paper makes conditional independence assumptions to derive a dynamic programming to approximate this loss. \n\nExperiments: The paper considers multiple single-/multi-label text classification datasets (SST-2, ATIS, etc.). The paper shows that (1) the proposed model works as well as conventional models and (2) the model recovers gold spans and labels in slot-filling/NER tasks.\n\n[OLD REVIEW]\n\nThe paper proposes using a structured language model for interpretable classification. The method is based on Fast-R2D2. It proposes a variant of the inside algorithm to compute marginal probabilities of subtrees with gold labels and adds the resulting classification loss to the original objective of Fast-R2D2. Experiments on single- and multi-label classification datasets show that the method is competitive in performance but can yield meaningful trees.",
            "strength_and_weaknesses": "[REVISED]\n\nSTRENGTHS\n- The proposed problem is relatively unexplored and interesting. \n- The proposed algorithm for computing the label-consistent loss conditioned on a MAP tree is technically novel.\n- There is a clear practical benefit. The model works as well as conventional classification models but also induces labeled spans.\n\nWEAKNESSES\n- The paper is written *very* confusingly, which is the reason I (and the reviewer Vws4) found it almost impossible to judge the quality of the submission. It's not just one problem but several, which makes the reader wonder \n   - What is the model? What parameters are being optimized? The model is never precisely defined but kind of sketched on top of Fast-R2D2. The reader doesn't necessarily know about Fast-R2D2 and has to wonder if this specific to Fast-R2D2 or similar models. \n   - What is the new loss? Again despite many equations that technically define the loss, the objective is simply defined in $P(\\hat{t}^{[\\mathcal{Y}(\\hat{t})=\\mathcal{T}]} |t)$ and I found the current explanation both verbose and confusing. My main feedback is to really transform the model and training section. Precisely define the model and the objective. The detailed derivation should follow only after the model/objective is crystal clear. I think I've parsed the paper more correctly at this point, but this should never be the burden of the reader. \n- I stand by my one previous weakness, which is: the case for the usefulness of label trees needs to be made with more systematic studies instead of qualitative analyses only.\n\n\n[OLD REVIEW]\n\nSTRENGTHS\n- The important problem of developing interpretable neuro-symbolic models is considered.\n\nWEAKNESSES\n- The method is somewhat straightforward and relies on an existing structured language model. \n- The performance results are underwhelming. Table 1 shows that the method attains little or very slight gains over the Fast-R2D2 baselines. \n- The case for the usefulness of label trees needs to be made with more systematic studies instead of qualitative analyses only, it seems. ",
            "clarity,_quality,_novelty_and_reproducibility": "[REVISED]\n\nUpon revision, I've come to appreciate the novelty in problem formulation and model/loss formulation of the submission. However, the current lack of clarity of the paper makes it difficult to appreciate these contributions.  \n\n[OLD REVIEW]\n\nI find the details in Section 3.2 not useful. It's better to state the algorithm clearly and move the details to an appendix.",
            "summary_of_the_review": "[REVISED]\n\nThe paper considers a relatively unexplored problem of interpretable text classification and a new model and training scheme. However, the paper is quite vague and confusing in communicating the model and the training scheme, which makes it very difficult to appreciate these contributions. Even after revising and appreciating the submission much more than before, I think the paper needs substantial restructuring before publication.\n\n\n[OLD REVIEW]\n\nThe paper proposes using a structured language model for interpretable classification, but the method is not surprising and relies on existing structured LMs. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper129/Reviewer_AUue"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper129/Reviewer_AUue"
        ]
    },
    {
        "id": "vqT0lExwrld",
        "original": null,
        "number": 2,
        "cdate": 1666667612381,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667612381,
        "tmdate": 1666667612381,
        "tddate": null,
        "forum": "MLJ5TF5FtXH",
        "replyto": "MLJ5TF5FtXH",
        "invitation": "ICLR.cc/2023/Conference/Paper129/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a Symbolic-Neural (SN) interpretable model for text classification that uses a structured language model Fast-R2D2 as the backbone and during training, maximizes the probability summation of all potential trees whose extracted labels are consistent with a gold label set via dynamic programming with linear complexity.  Given a sentence and parse tree, the model generates a labeled tree which explain constituent spans in the sentence.  The authors conduct experiments on 6 datasets and compare different representation architectures ( uninterpretable dense Sentence Representation and their interpretable Symbolic Neural constituent representation ) showing their method matches or surpasses dense non-interpretable methods.  Additionally, they conduct a quantitative analysis comparing the interpretability of SN to Integrated Gradients and Multi-instance learning by designing a constituent-level attribution task to see how close the results learned by the models in an unsupervised fashion are to human-annotated results.",
            "strength_and_weaknesses": "**Strengths**  \nThe empirical results of their method are impressive and pretty thorough and the analysis of explainability from a quantitive standpoint is also interesting. \n\n**Weaknesses**   \nThis paper is too dense and not self contained ( ie, references to charts, chart encoders for example, etc ).  \nrequired me going to other cited papers to be clear on certain aspects of the paper.\n\nSection 3.2 and in particular page 4 are very dense and hard to follow initially as is and really could be improved by having a table for variables and a more fully guided example ( another figure in the style of Fig 2 ) \n\nAdditionally, I think conclusion does a better job of motivating and describing the paper than the intro does, so you may want to take parts of that to help guide the reader, in particular when it comes to understanding when supervision is needed vs not.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty, Quality and Reproducibility ( code provided ) are solid.  Clarity and describing motivation, for people less knowledgable of the prior work ( particularly in regard to structured LMs and Symbolic Neural modeling ) could be improved.",
            "summary_of_the_review": "The empirical results of their method are impressive and thorough as is the analysis of explainability from a quantitive standpoint. My biggest qualms is with clarity of writing and keeping the paper self contained.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper129/Reviewer_1uUN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper129/Reviewer_1uUN"
        ]
    },
    {
        "id": "954ftL2-Cc",
        "original": null,
        "number": 3,
        "cdate": 1667561679952,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667561679952,
        "tmdate": 1669796854904,
        "tddate": null,
        "forum": "MLJ5TF5FtXH",
        "replyto": "MLJ5TF5FtXH",
        "invitation": "ICLR.cc/2023/Conference/Paper129/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper describes a method of training a classifier based on a pretrained syntax tree extractor called a structured LM. The structured LM produces contextualized representations of each node through a learned composition function applied to the node's children. They then apply an MLP and softmax to produce label probabilities for each node. The authors also define a yield function to gather task-relevant node labels based on the tree structure and node labeling.\n\nHowever, I don't understand how the optimization objective is supposed to work. The authors mention Tau as representing a \"gold label set\" associated with a single example, suggesting Tau is supposed to be a set of gold labels on the level of individual nodes / words. However, the authors apply their method to datasets that (to my knowledge) don't have such fine-grained labels, such as CoLA.\n\nThe authors say the objective is to \"maximize the probability summation of {t_hat | yield(t_hat) = Tau}\". I do not understand what this means. What's the probability we're summing up, and how is it computed? Later text mentions how to compute per-node label probabilities, but those seem like different things than what's being summed here. In fact, I don't even understand how there's anything here to sum over. It seems like t_hat is a specific set of possible labels for a specific parse tree, and Tau is either another set of labels for that specific parse tree, or a single label for the entire sentence. Either way, I don't see how there's a space to sum over.\n\nWhat does it mean for t_hat to be conditioned on yield(t_hat) = Tau? I'd assume it means that we're restricting the space from which we're drawing t_hat to only include elements whose yield is equal to Tau. However, the previous text just called t_hat one of the possible label trees associated with the best syntax tree of a sentence, with no indication that t_hat is at all restricted. ",
            "strength_and_weaknesses": "Strengths:\n- Focuses on interpretability from the start, rather than slapping some ad-hoc method on at the end.\n- Provides class predictions at each node of the syntax tree. You can see the overall prediction evolving as you walk up the tree, which is neat.\n- I appreciate the authors' effort to provide a fair comparison of their approach by training their own BERT model trained with comparable resources.\n\nWeaknesses:\n- The authors do not go into much detail regarding the training process of their BERT model, and they do not include the standard pretrained BERT among their comparisons. This makes it difficult to judge the performance of the method relative to the alternatives that are available to a potential user. It would also have been nice to see comparisons of the amount of compute required to train a Fast-R2D2 model as compared to a BERT model.\n- I think (though am not entirely sure) that the authors evaluate their method's interpretability by comparing the degree to which highly attributed words in a classified input overlap with human rationales for that input. I think this way of evaluating a method's interpretability is very questionable for two reasons:\n    1. Interpretability methods should highlight a model's true basis for making a classification. That basis might be very different from human rationales. The degree of rationale overlap depends on *both* the degree to which the model's rationales overlap with human rationale and the degree to which the highlighted text represents the model's true rationale. \n    2. The quality of baselines for rationale overlap metrics is quite erratic, and can vary between different tasks, datasets and models. Integrated gradients and attention scores may be poorly suited to the task at hand. In particular, attention scores can do very poorly as explanation unless scaled by the magnitude of the associated value vectors. See (https://arxiv.org/abs/2004.10102). Failing to apply this correction leads to attention-based attribution scores seeming to highly weight CLS and punctuation tokens, which may be degrading the performance of the author's attention-based baselines. Additionally, the authors do not mention what they used as the baseline for integrated gradients, which (https://arxiv.org/abs/2111.07367) find can have a significant influence on the method's performance.\n\n    - I'd suggest the authors consider another approach to demonstrating the interpretability advantage of their method. E.g., (https://arxiv.org/abs/2111.07367) \"poison\" a classification dataset with synthetic shortcuts, train classifiers on the poisoned data, and then test if a given interpretability method can pick up on the shortcut.\n\n    - If the above approach represents too much of a burden, an alternative would be to include a wider range of attribution baselines such as:\n        - Integrated gradients (preferably with the [MASK] token as the baseline)\n        - Attention (while scaling by the attention value vector)\n        - L2 norm of the gradients\n        - Sum( gradient x input embeddings )\n        - ||Max( gradient x input embeddings, 0 )||, a method called NormGrad selective. See (https://arxiv.org/abs/2004.02866)\n    - Note that the above alternative only addresses the weakness of using integrated gradients / attention as baselines, not the deeper conceptual issue of rationale overlap being a poor way to evaluate interpretability, regardless of the baselines used for comparison.\n- The method seems to require a class of language model different from those currently in common use. This requires potential users to either train their own structured language model (expensive) or to use a publicly released structured language model, of which there seem to be very few.\n    - It might be helpful if the authors could demonstrate that their finetuning method works on a publicly released structured language model, which is what potential users would realistically be starting from themselves.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- I found the paper very difficult to understand. Much of it is probably my own unfamiliarity with structured language modeling as a subfield. However, there are a number of strange turns of phrase (like \"layer stacking model\") and areas that could use clearer explanations. Particular points of confusion for me included:\n    * The description of the pretraining process for Fast-R2D2. It contained pieces of jargon I was unfamiliar with, such as \"chart-based en-coder\" and generally seemed too short to fully describe the pretraining. Maybe cut it entirely and just reference the original paper?\n    * The description of the interpretability metrics, which I had to read several times before I felt like I understood it. This made me particularly concerned about the clarity of the rest of the paper because interpretability is one of my focus areas. It pushed me more towards thinking that my difficulty understanding other parts of the paper stems from genuine clarity issues, rather than my own unfamiliarity. \n    * The description of the training objective (see summary)\n    * The yield function (see below)\n\n- There are a number of times where the text detours into fairly technical points without providing context. E.g., when discussing the yield function, there's a detour \"For simplicity, we don\u2019t discuss nesting cases in this paper, so there is only one unique non-terminal label...\". There's no indication of what a \"nesting case\" is, why we might encounter one, or why they'd lead to multiple non-terminal labels.\n\nNovelty:\n- The primary novel contribution of this work seems to be the optimization objective the authors use to teach structured language models to perform classification at multiple levels of the parse tree. Unfortunately, I couldn't understand how the optimization objective is supposed to work, so I can't really evaluate its significance. I *think*, but am not sure, that the paper's optimization objective allows for classifiers to be trained to predict span level labels without requiring access to span level gold labels. If that's true, then it would be quite the interesting contribution.\n\n\nReproducibility:\n- Provides the code used for training models / running experiments, which is good.\n- However, that code is pretty poorly documented. E.g., no list of library requirements.\n- I also do not understand why the authors trained their own version of the Fast-R2D2 model. A public version is available here (https://github.com/alipay/StructuredLM_RTDT). Training a new model seems to just make reproducing this work more difficult. Perhaps the public model was not available at the time of the paper's writing? ",
            "summary_of_the_review": "I tentatively recommend rejecting this paper. My top issues with this paper are its lack of clarity (particularly regarding the optimization objective, which is its primary contribution) and the weakness of its experimental protocol. \n\nLack of clarity:\n- I was unable to follow key aspects of the paper.\n- There seem to be many opportunities to improve the explanation of the paper's technical contribution, as I highlighted in the summary section.\n\nExperimental weakness: \n- I described my concerns with the interpretability experiment in the weaknesses section.\n- I am concerned about the lack of detail provided in the training process of the BERT baseline, and the lack of comparison to the publicly available BERT model, as mentioned in the weaknesses section.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper129/Reviewer_Vws4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper129/Reviewer_Vws4"
        ]
    }
]