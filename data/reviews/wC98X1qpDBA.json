[
    {
        "id": "fpTtsmPCHx",
        "original": null,
        "number": 1,
        "cdate": 1666247593910,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666247593910,
        "tmdate": 1670753570355,
        "tddate": null,
        "forum": "wC98X1qpDBA",
        "replyto": "wC98X1qpDBA",
        "invitation": "ICLR.cc/2023/Conference/Paper783/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the setting of unsupervised domain generalization, where the multiple source domains do not have label supervision and the target domain is unseen. Based on the previous work (i.e., DiMAE) that applies the masked-reconstruction-modeling as a self-supervised task, this paper further proposes two more objective functions: the cycle reconstruction loss and domain contrastive loss. The first one loss transforms the generated image back to original image domain and the second one loss performs a contrastive learning across the features from different decoders. The authors have conducted experiments on the DomainNet and PACS and achieves some improvements over the previous methods.",
            "strength_and_weaknesses": "Issues regarding the fair experimental comparisons.\n+ To the reviewer\u2019s knowledge, some previous works such as DiMAE use a supervised pre-trained model while this paper adapts an unsupervised pre-trained model on ImageNet. To this end, could the authors explain how much will the different pre-trained models affect the final performance? Meanwhile, pre-trained models using different pretraining recipes (e.g., number of training epochs) are also different starting points for downstream tasks. Overall, the reviewer hopes the authors to make sure the experimental comparisons are fair in terms of the pre-trained model.\n***\nIssues regarding the methods.\n+ Could the authors clarify the extra training overhead they have introduced in their framework? Because more decoders and forward passes are appended upon previous approach.\n+ As claimed in the paper that, the domain contrastive loss encourages more diverse domain-specific styles to be learned in the decoder. The reviewer is interested that why such constraints should be placed in the early layer of the decoder since the higher layer closer to the pixel space might be more responsible to encoder style information? \n+ The unsupervised domain generalization is a new setting in the field. The reviewer believes that reporting the performance in the unsupervised source domain is also of significance. Could the authors provide such information?\n",
            "clarity,_quality,_novelty_and_reproducibility": "+ The paper is well-written and easy to follow.\n+ If compared with previous work DiMAE, the novelty of this paper is somehow limited. Based on the previous reconstruction loss, this paper proposes to further include a cycle-consistent one. The similar story has repeated frequently in the field.\n+ Since the unsupervised domain generalization setting is relatively new, the reviewer finds that most of the published methods do not have open implementations. To this end, the reviewer is not sure whether the experiments are conducted in a fair environment and whether there are some tricks hinder the reproduction. \n",
            "summary_of_the_review": "This paper proposes to use a cycle-reconstruction loss and domain contrastive loss in the unsupervised domain generalization. Consistent performance gains are detected on two widely-used benchmarks. Currently, the reviewer\u2019s concerns are two-fold. On the one hand, the authors should make sure that the experimental comparisons are fair and reproducible. On the other hand, the two new losses are kind of limited in the terms of novelty, which might be lower than the bar to be accepted to a venue like ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper783/Reviewer_7ZLc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper783/Reviewer_7ZLc"
        ]
    },
    {
        "id": "rt2hJlEnGY",
        "original": null,
        "number": 2,
        "cdate": 1666585993938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585993938,
        "tmdate": 1666585993938,
        "tddate": null,
        "forum": "wC98X1qpDBA",
        "replyto": "wC98X1qpDBA",
        "invitation": "ICLR.cc/2023/Conference/Paper783/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to use cycle consistency to help learn domain-invariant features for unsupervised domain generalization.",
            "strength_and_weaknesses": "Pro:\n\n1.  Previous method DiVAE only considers the reconstruction loss ||y_m-x_m||. The proposed loss considers how to utilize the information of y_n and use cycle consistency to further help the task, i.e., adding a backward transformation process for other domain labels $n\\neq m$.\n\n2. The paper also notices that the generated images in other domains $$y_n$$ may be trivial such that all images are of the same style. To encourage diverse domain information, they propose a domain contrastive loss.\n\n3. The results are promising and the paper is well-written.\n\nCons:\n\n1.  Since the model adds a backward process compared to DiVAE, I'm wondering how much training time is increased. It would be good to have a running time comparison.\n\nComments:\n\nThe proposed domain contrastive loss encourages the outputs to have different domain styles but there is no supervised loss to encourage it to be exactly the domain style i. Is it possible that the outputs of different domain-specific decoders are out of the given domains and harm the performance? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and novel.",
            "summary_of_the_review": "The paper proposes to use cycle consistency to help learn the domain-invariant features and the results are promising.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper783/Reviewer_BvPn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper783/Reviewer_BvPn"
        ]
    },
    {
        "id": "B4zyon7nYMm",
        "original": null,
        "number": 3,
        "cdate": 1666846731330,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666846731330,
        "tmdate": 1667192266761,
        "tddate": null,
        "forum": "wC98X1qpDBA",
        "replyto": "wC98X1qpDBA",
        "invitation": "ICLR.cc/2023/Conference/Paper783/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to address unsupervised domain generalization via a proposed cycle-consistent masked autoencoder, which includes a CycleGAN, MaskAutoEncoder, and contrastive learning. The authors claim that the poposed method can significantly improve state-of-the-art performances, which are demonstrated by the experimental results to some extent.",
            "strength_and_weaknesses": "# Strength\n\n1. The motivation of introducing CycleGAN, MaskAutoEncoder, and contrastive learning sounds reasonable.\n\n2. The experimental results look solid.\n\n3. The writing is good, and the structure is easy to follow.\n\n# Weaknesses\n\n1. The novelty of this paper seems marginal. The main modules of the proposed method, including CycleGAN, MaskAutoEncoder, and contrastive learning, are all existing famous works. It seems that the authors simply combine these works into UDA.\n\n2. The visualization results in Figure 5 are unsatisfied, especially the sketch ones. It is highly suggested to release more visualization results to demonstrate the effectiveness of the reconstrcution process.\n\n3. I notice that the authors use Vit-small as the backbone network. Do all the other comparison methods use this network? If not, it is suggested to compare the backbones of different methods, including training time, model size, and inference speed. \n\n4. The proposed method needs to be pre-trained for 1000 epochs, which may easy to cause overfitting. It is suggested to add a set of experiments to fully prove the generalization of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality, clarity and originality of the work are good.",
            "summary_of_the_review": "Please see [Strength And Weaknesses].",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper783/Reviewer_QvTm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper783/Reviewer_QvTm"
        ]
    }
]