[
    {
        "id": "IWzbo0Mv8Hr",
        "original": null,
        "number": 1,
        "cdate": 1666595695864,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666595695864,
        "tmdate": 1666595695864,
        "tddate": null,
        "forum": "_4F4CDK9Mo",
        "replyto": "_4F4CDK9Mo",
        "invitation": "ICLR.cc/2023/Conference/Paper3217/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an out-of-distribution (OOD) detection method for text generation based on the concept of information measure. More specifically, the author considers two measurements, the Renyi divergence, and the Fisher-Rao distance under both the no-reference and the reference scenarios. The empirical results show improvements with the proposed method in both dialogue generation and machine translation.",
            "strength_and_weaknesses": "Strength:\n\n1. The author adopts the OOD into the scenarios of text generation, which is a new task that lacks exploration. \n\n2. The paper is clearly stated and easy to follow.\n\nWeaknesses:\n\n1. Some details of the proposed method lack theoretical justification. In equation (5), the author takes the average of the soft-distribution probabilities as a representation of the overall sentence distribution. This averaging step can lose a lot of sequential information of the input x. I am not convinced that the mean of the next-token prediction probabilities can represent the distribution information of the whole sentence in language modeling. In contrast, the token-level information measurements in equation (3) seem more reasonable to me. Why cannot the author define a token-level information measurement for the reference scenario, which might be consistent with the no-reference case?\n\n2. The sequential likelihood method is not compared for the with-reference scenario. With the reference, one can fine-tune the generator to obtain a better sequential likelihood estimation for text. The estimated likelihood can be also regarded as a score of occurrence for input sequences. Is there any specific reason why the author did not compare with this simple baseline?",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is good. The quality and novelty of the paper are fair. The experimental parts seem reproducible.",
            "summary_of_the_review": "The paper designed several out-of-distribution detection methods for text generation based on information measurements, which is a good exploration of the OOD problem in text generation. However, some method details require more theoretical justification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3217/Reviewer_JtYo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3217/Reviewer_JtYo"
        ]
    },
    {
        "id": "v8lv60ZX5C",
        "original": null,
        "number": 2,
        "cdate": 1666626757997,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626757997,
        "tmdate": 1666626757997,
        "tddate": null,
        "forum": "_4F4CDK9Mo",
        "replyto": "_4F4CDK9Mo",
        "invitation": "ICLR.cc/2023/Conference/Paper3217/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors have provided an out of distribution detector, RAINPROOF and an out of distribution performance benchmark, LOFTER. RAINPROOF is an unsupervised model that is able to detect whether a reference sample is in or out of distribution when compared to the training distribution, without hampering the overall performance of the system.\n\nThey find the similarity score between two samples and apply a threshold to classify it as in or out of distribution. During inference, a soft distribution is extracted and used to compute an anomaly score, which is thresholded to perform the in or out of distribution classification.\n\nThe metrics used were AUROC and FPR and the authors report better performance on domain and dialog shifts. In language shifts, the performance is mixed, as the baselines perform better for , which is the data-driven notion normality.",
            "strength_and_weaknesses": "1. A major strength is that detecting the distribution does not hamper the performance of the model and is task independent.\n\n2. It is fully unsupervised, thus there is no need for annotated data.\n\n3. For similarity calculation, it would be good to provide results on how well the model does when the common similarity metrics like cosine and dot product is used.\n\n4. It would be good to see the performance of other reference distribution for u.\n\n5. Why is F1 score not considered along with AUROC.",
            "clarity,_quality,_novelty_and_reproducibility": "The work targets distribution detection for generation tasks, an area that has not been explored much as most previous work has focused on classification tasks. The ideas are novel and have been explained properly in the paper. More implementation details would help in the reproducible aspect.",
            "summary_of_the_review": "There are some typos in the paper. Particularly on line 10 of the abstract (ODD -> OOD) and section 3.3 has some grammatical errors and typos:\n1. In the with reference scenario -> In the reference scenario\n2. weakness of eq 3 is to imposes -> weakness of eq 3 is that it imposes\n3. under s1, our OOD detecor -> our OOD detector\n4. the last step consists in -> the last step consists of\n\nSome experimental results on the choice of similarity metric and reference distribution would help to understand the current selections.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3217/Reviewer_BFEH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3217/Reviewer_BFEH"
        ]
    },
    {
        "id": "zqMgIG7H5JJ",
        "original": null,
        "number": 3,
        "cdate": 1666938057087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666938057087,
        "tmdate": 1666938521650,
        "tddate": null,
        "forum": "_4F4CDK9Mo",
        "replyto": "_4F4CDK9Mo",
        "invitation": "ICLR.cc/2023/Conference/Paper3217/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper explored OOD detection in text. Different from previous OOD work, this paper more focuses on realistic data shifts and domain change. To measure the out-of-domain score, the authors introduce renting divergence and fisher-ran distance. ",
            "strength_and_weaknesses": "\nLack of baselines\nI think the experiment lacks an important baseline, the perplexity (PPL), since this work is based on large pretrained language model. The language model is originally designed to maximize the $p(x) = p(x_1)p(x_2)...p(x_n)$, which is a great anomaly detector itself. \n\nExperiments\n- Have you ever test your model on some other classical OOD tasks?\n\nProblem\n- Could the author give more explanation on no-referene scenario? Take mutual information, a special case of Renyi divergence as an example, I(P, U) = H(P) + H(U) because U and P are independent variables. H(U) is a constant variable for any distribution.  As the results, the entropy is used as the novelty score. \n- Did you fine-tune the language-model with in-domain data?\n\n\nPerformance\n- From the reviewers' experience on OOD in text, the performance could be much better using a simple plug-in structure. (The reviewer understood that the authors want to avoid the extra plug-in structure, just curious if better results can be achieved) \n",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation is clear. \nThe quality could be further improved. \nNovelty is limited.\n\nSuggestion:\n- In section 4.2, it's unnecessary to add an arrow after every AUROC and FPR. \n- The notation part is distributed in two places, 2.1/2.2. Similarly, the related work is also appeared in two places 2.3/3.1. It will be easier to read if related work are together and important notations are gathered.",
            "summary_of_the_review": "I have some doubts on the methods. The experiment part should compare with more baselines on some common OOD datasets. Currently, nearly all the datasets are synthesized by authors and there is no demo in supplementary. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3217/Reviewer_PSuw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3217/Reviewer_PSuw"
        ]
    }
]