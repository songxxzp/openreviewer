[
    {
        "id": "NR4-CcFim0",
        "original": null,
        "number": 1,
        "cdate": 1666365846931,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666365846931,
        "tmdate": 1669665996541,
        "tddate": null,
        "forum": "b3itJyarLM0",
        "replyto": "b3itJyarLM0",
        "invitation": "ICLR.cc/2023/Conference/Paper4900/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies distributed (synchronous) algorithms for monotone variational\ninequality (VI) optimization, generalizing convex minimization, saddle-point\nproblems, and games. Specifically, it proposes a quantized, general\nextra-gradient framework called \"Q-GenX\" that unifies previous methods.  This\napproach uses unbiased and adaptive compression techniques for communicating\nlocal stochastic gradients, and it uses an adaptive learning rate rule for\nwhich the authors prove convergence guarantees: $O(1/T)$ under relative nosie\nand $O(1/\\sqrt{T})$ in the presence of absolute noise. For this VI problem,\nthey show that increasing the number of processes can accelerate convergence.\nLastly, they train GANs on multiple GPUs in the appendix to support their\ntheoretical results.\n\n",
            "strength_and_weaknesses": "**Strengths:**\n- This paper is very well written and provides a comprehensive survey of\n  related works.\n- The Q-GenX algorithm is indeed practical, and the authors provide strong\n  theoretical guarantees about its performance.\n\n**Weaknesses:**\n- The exact contributions of this work over previous methods (aside from the\n  unification aspect) should be made more clear. The discussion after Theorem 5\n  talks about more processors leading to faster convergence, but how does this\n  convergence rate compare to existing methods for different slices of the VI\n  problem (e.g., convex, saddle-points)?\n- Given the nature of this paper, there should be some experiments in the main\n  body. Some GAN experiments are discussed in Appendix H, but it seems\n  incomplete.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is extremely clear, but the contributions of this work relative to\nothers could be more explicit. The experiments in the appendix are minimal but\nappear to be reproducible.",
            "summary_of_the_review": "This work is polished and well-organized, but the comparison to prior works and\nexperiments to support the Q-GenX algorithm seem to be lacking. The algorithm\nis clean and practical, and come with strong theoretical guarantees. This paper\ncould be accepted to ICLR, but it would benefit from addressing the weaknesses\npointed out above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4900/Reviewer_PUDg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4900/Reviewer_PUDg"
        ]
    },
    {
        "id": "D0T8rICacm_",
        "original": null,
        "number": 2,
        "cdate": 1666767472376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666767472376,
        "tmdate": 1666767472376,
        "tddate": null,
        "forum": "b3itJyarLM0",
        "replyto": "b3itJyarLM0",
        "invitation": "ICLR.cc/2023/Conference/Paper4900/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers a monotone variational inequality to be solved in a Federated learning setup, with a central server and multiple clients. They develop a version of the extra gradient (EG) algorithm in this setting where the bottleneck is the communication: In their algorithm, communications between the central server and the clients are compressed.\n\nThe resulting algorithm is a version of EG with compression and they show convergence rates in terms of the restricted gap function under different assumptions on the noise in the stochastic \"gradient\". The stronger assumption on the noise ensures zero noise at the optimum and therefore recover the deterministic rate of EG. Besides, the authors compute the communication cost and show reduction in number of bits communicated at each round.",
            "strength_and_weaknesses": "Strength\n\n- The compression part of the paper is precise and specific. The compression operators are described and the number of bits communicating is computed. I am not sure, but these calculations seem to be new. So there is also a contribution here, additional to the optimization part (analysis of EG) \n\n- The optimization part (EG algo) seems equally solid. Convergence is in restricted gap and recover the deterministic result as a special case.\n\n-The assumptions on the noise cover several types of stochastic gradients\n\n-The meta algorithm actually covers several algorithms for monotone VI\n\n\nWeaknesses\n\n-In many applications of ML, monotonicity does not hold (Monotonicity is like convexity). This paper deals only with the monotone case",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality:\n\nOverall the paper is clearly written. The paper is technical and it is possible that I misunderstood part of it. However, the authors did a good job explaining all the material.\n\nSection 3.3. (optimization of the quantization levels) could be clearer, as well as the difference between encoding and quantization.\n\nNovelty:\n\nThis papers seems to be the first to consider communication efficient variant of EG. The rates are reasonable and recover known results as special case. \n\nNo reproducibility issues.",
            "summary_of_the_review": "A solid work that considers an idealized setting (monotonicity) but provides precise and competitive guarantees for both the computation complexity and the communication complexity of the proposed algorithm.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4900/Reviewer_4SD9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4900/Reviewer_4SD9"
        ]
    },
    {
        "id": "TYS9Cm23Urb",
        "original": null,
        "number": 3,
        "cdate": 1666939965459,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666939965459,
        "tmdate": 1666939965459,
        "tddate": null,
        "forum": "b3itJyarLM0",
        "replyto": "b3itJyarLM0",
        "invitation": "ICLR.cc/2023/Conference/Paper4900/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on co-coercive variational inequalities and proposes a quantized generalized extra-gradient (Q-GenX) for solving these problems in multi-GPU settings where multiple processors/workers/clients have access to local stochastic dual vectors. In terms of theory, the paper proves $O(1/T)$ convergence of Q-GenX under relative noise and $O(1/\\sqrt{T})$ under absolute noise. ",
            "strength_and_weaknesses": "The paper has interesting results, however, it has several issues in terms of presentation that do not allow me to suggest acceptance at this stage. Let me provide more details below. \n\n1. The paper claims that it solves general monotone problems while truly focusing on a much smaller class of VIPs for which the analysis is typically much easier. This is the class of co-coercive VIPs. \n\n2. On top of the co-coercivity condition, the authors mentioned \"Moreover, in order to avoid trivialities, we make the following mild assumption\" to refer to assumption 1. However, Assumption 1 assumes that the problem has a unique solution as well. Thus it is far from general monotone or even general co-coercive. \n\n3. In the expression of the co-coercive condition the dual norm is used without further explanation. Note that the standard co-coercivity holds for euclidean norms. \n\n4. For co-coercive problems assuming bounded variance, bounded noise or relative noise (growth condition) are unnecessary and quite strong assumptions. Please check the results of paper [1] where these conditions have not been used. \nThe authors spend half a page providing examples where Ass. 2 and 3 are satisfied which is good for pedagogical reasons but the examples are standard for anyone familiar with noise assumptions in stochastic algorithms. This space could have been used for the numerical experiment (which is not part of the main paper)\n\n5. Proposition 1 is a well-known result. As presented now it gives the impression that this is a contribution of the authors. The authors mention: \"Mathematically speaking, we have the following proposition:\" \n\n6. Presentation of Section 3 is confusing as it involves quantities that appear later in the paper. What is $Q_{\\ell_t}$ what is $V_{k,t}$, etc. A good idea for clarity will be to introduce a notation paragraph at the beginning of that section 3. Note that definition 1 only appears in section 3.2 while it should have been presented before section 3.1\n\n7. In section 4 a proof sketch is provided in the main paper which takes half a page. This space could have been used for the experiments which as it is mentioned in several parts of this work is one of its main contributions. At the moment there are no experiments in the main paper. \n\n8. Main Issue: \nEven if one checks the experiments in the appendix (not a requirement for the reviewer) the paper has no comparison with closely related works that use compression on top of distributed algorithms for solving VIPs. The authors mentioned the works of Beznosikov et al. (2021); Kovalev et al. (2022) and say that they are concurrent works. This is not true. Beznosikov et al. (2021) for example, is 1-year-old paper. \n\nIn the numerical evaluation, a proper comparison with algorithms from the above papers should be presented. \n\nA highly related paper, that also focuses on co-coercive VIPs, and to the best of my knowledge has state-of-the-art performance in terms of distributed methods with compressors for VIPs is paper [2]. This is also non-concurrent work as it appears some time ago. A formal comparison with the distributed methods with compressed communications presented in that work is highly recommended.  \n\nMissing references:\n\n[1] Loizou, Nicolas, Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas, and Simon Lacoste-Julien. \"Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity.\" Advances in Neural Information Processing Systems 34 (2021): 19095-19108.\n\n[2] Beznosikov, Aleksandr, Eduard Gorbunov, Hugo Berard, and Nicolas Loizou. \"Stochastic gradient descent-ascent: Unified theory and new efficient methods.\" arXiv preprint arXiv:2202.07262 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the above review for further details.",
            "summary_of_the_review": "As I mentioned in my review the idea of the paper is interesting. However, there are many issues in terms of presentation and numerical evaluation. \n\nIn my opinion, a fair score for the current version of the paper is \"3: reject, not good enough\" or \"5: marginally below the acceptance threshold\". \n\nAt this point, I give a score of 5 to this work. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4900/Reviewer_Qv34"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4900/Reviewer_Qv34"
        ]
    }
]