[
    {
        "id": "XxPX9vcpSr",
        "original": null,
        "number": 1,
        "cdate": 1666636760246,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636760246,
        "tmdate": 1666636760246,
        "tddate": null,
        "forum": "a70lGJ-rwy",
        "replyto": "a70lGJ-rwy",
        "invitation": "ICLR.cc/2023/Conference/Paper4802/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a theoretical analysis of backdoor attacks from the perspective of neural tangent kernels (NTK). It uncovers that NTK is more prone to backdoor attacks than other kernels, e.g., the Laplace kernel. A new poisoning method, NTBA, is proposed based on the analysis. NTBA optimizes the poisoning examples, producing stronger backdoors than previous methods. Experiments are performed on subclasses of CIFAR-10 and ImageNet with WideResNet-34 and ConvNeXt with periodic and patch trigger attacks, showing that NTBA only requires few poisoned examples.",
            "strength_and_weaknesses": "Strengths:\n- Novel analysis of poisoning attacks linked to neural tangent kernels.\n- Novel poisoning attack strategy, that is principled and based on theoretical analysis, resulting in stronger poisoning capacity.\n- Comprehensive state-of-the-art analysis on poisoning and backdoor attacks and defenses (unfortunately pushed to the appendix).\n- The code for experiments was provided.\n\nWeaknesses:\n- It is not fully clear to me what are the practical implications and applicability of the analysis and NTBA method.\n- A more extensive discussion of assumptions and limitations of the proposed approach would be welcome.\n\nSee more details below.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty:\n- The work in the paper seems novel.\n- However, neural tangent generalization attacks [Yuan&Wu, 2021] seem closely related to the present work. While the paper cites this reference, it is unclear how they are related.\n\nQuality:\n- The paper seems to lack a section stating explicitly what assumptions are being made about the threat model and attacker / defender capabilities.\n  * E.g., Appendix (A.1) states that \"the attacker has information about the network\u2019s architecture and training data\", which seems like a strong assumption. This should of course not prevent one from deriving novel theoretical analyses, but should be addressed from the beginning in the threat model.\n  * A commentary on the realism of these assumptions would also be welcome.\n- Addressing technical or principled limitations of the method in one place would help.\n  * E.g., we learn that batch normalization cannot be applied due to an implementation limitation.\n  * The conclusion briefly mentions the lack of scalability of the method.\n\nClarity:\n- The paper is well-written, but sometimes a bit hard to follow, due to main important parts of the paper being deferred to the appendix. While I appreciate the effort that has gone into fitting this amount of content into the ICLR format, my impression is that clarity has been impacted.\n- It would help to mention at the beginning of the ablation study (Sec 2.1) that the number of method components (mainly 1, 2, 3) is the one introduced at the beginning of Sec. 2.\n\nReproducibility:\n- The paper mainly contains small experiments. The code for reproducing them has been provided.\n\nReferences:\n* [Yuan&Wu, 2021] Yuan, Chia-Hung and Wu, Shan-Hung. Neural Tangent Generalization Attacks. ICML 2021.",
            "summary_of_the_review": "Paper with novel and thorough analysis of backdoor attacks, which uncovers new insights into the vulnerability of neural networks to poisoning. Principled, effective attack method developed based on the theoretical analysis.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4802/Reviewer_qvbc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4802/Reviewer_qvbc"
        ]
    },
    {
        "id": "mS5sTgK6qyl",
        "original": null,
        "number": 2,
        "cdate": 1666678483522,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678483522,
        "tmdate": 1668848331340,
        "tddate": null,
        "forum": "a70lGJ-rwy",
        "replyto": "a70lGJ-rwy",
        "invitation": "ICLR.cc/2023/Conference/Paper4802/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to reduce the number of poisoned samples (i.e., few-shot) in backdoor attacks. The paper formulates backdoor-attacks as a bi-level optimization problem, which is approximated with a NTK objective to trace training dynamics. The paper also proposes a greedy algorithm to gradually reduce the number of poisoned samples. Experiment results show: (1) the proposed method makes attacks more efficient; (2) it is enough to know partial training data; (3) the attacks on other kernels; (4) deeper understanding of backdoors from the perspective of NTK.",
            "strength_and_weaknesses": "Strengths:\n(1) The idea of using NTK to approximate the training dynamics is interesting in backdoor attacks.\n(2) The paper provides deeper analysis of backdoor attacks from the NTK perspective.\n\nWeakness:\n(1) The introduction of the proposed method is not self-contained. Too many details are given to the theoretical analysis, while it is actually not very clear to me how exactly the attack is conducted. The three items in Page 3 are far from being enough. I do not think it is a good idea to put these contents in Appendix.\n(2) I have doubts whether \"few-shot backdoor attack\" it is a valid problem. I do not think human beings will check each training sample. Automated detection methods will be applied, where the poisoned set size is not important (anyway the detector will go through each of the samples).\n(3) Experiments: First, I feel Section 3.2 is not necessary. Having 100% of training samples or 50% does not matter that much. Second, in Sec 4, what does \"strong attack\" or \"attack strength\" mean? Does it refer to the backdoor sample with more clear trigger pattern or more stealthy pattern? Third, a flaw of experiment is that no defense method is applied to evaluate the effectiveness of the proposed backdoor attack method.",
            "clarity,_quality,_novelty_and_reproducibility": "The technical quality and novelty of the papre is good.\nThe presentation clarity is OK, but could be improved.\nThe codes for implementation are provided.",
            "summary_of_the_review": "Technically speaking, the paper tackles the challenge of tracing model training process with NTK methods, which is used for more efficient backdoor attacks. Some interesting analysis is conducted. However, there are several major problems for this paper. First, I am not fully convinced by the motivation of \"few-shot backdoor attacks\". Second, the experiment design has some flaws, especially the defense method based evaluation is missing. Third, the paper presentation is not self-contained. Based on these reasons, I think the paper is slightly below the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4802/Reviewer_iX6N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4802/Reviewer_iX6N"
        ]
    },
    {
        "id": "OFdEHo_LPl",
        "original": null,
        "number": 3,
        "cdate": 1666810290783,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666810290783,
        "tmdate": 1669750646693,
        "tddate": null,
        "forum": "a70lGJ-rwy",
        "replyto": "a70lGJ-rwy",
        "invitation": "ICLR.cc/2023/Conference/Paper4802/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new backdoor attack that exploits neural tangent kernels to optimize the perturbations of the poisons introduced in the training set in a more efficient way. The attack comprises of three elements: modeling the training dynamics with the neural tangent kernel, a greedy initialization strategy to select the initial set of poisons and optimizing the perturbations of the poisons. The empirical results provided by the authors show that the proposed attack allows to achieve a high success rate with a few number of poisoning points in the training set when compared to a baseline. ",
            "strength_and_weaknesses": "Strengths:\n+ The use of neural tangent kernels for solving the bilevel optimization problem to learn the poisons perturbations looks interesting and can help to improve poisoning attacks relying on this methodology. \n+ The authors show how the combination of the three steps in the attack is necessary to achieve a high success rate. \n\nWeaknesses: \n+ The experiments just compare the performance of the proposed attack against a baseline, but it does not compare its performance against other state-of-the-art attacks to provide a more comprehensive view of its benefits and limitations. For instance, the authors could compare with Turner et al. 2019, which also optimize the values of the poisons. Similarly, different strategies for solving bilevel optimization-based attacks could be compared to analyze the possible benefits in the use of the neural tangent kernels. \n+ The paper is difficult to follow as it often refers to information in the appendix that is important to understand the paper. In this sense, the authors, perhaps, abuse on the use of the appendices and could rethink of a better organization of the paper to make it more readable. \n+ Similar to the previous point, the authors do not provide many details about the attack in Section 2. For instance, the authors just refer to the appendices to clarify the 3 main steps in the attack. On the other side, the derivation of the attack by using equation (2) is not very well justified and detailed: For instance, does this attack provide an exact or an approximate solution to the bilevel optimization problem? How does this differ from other approximate techniques to solve this type of problems? \n+ The attack is not tested against existing defenses. This can be a key point to validate the usefulness of the attack. In this sense, the attack can be very strong compared to the baseline when attacking undefended systems. However, the strength of the attack could make it more detectable. Thus, it is necessary to explore its effectiveness against existing defenses. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing style is good, but the organization of the paper can be improved and the use of the appendices is not very adequate. Important information that is key to understand the paper is often described in the appendices and the flow to read some of the sections in the main paper is not very good. \nIn terms of novelty, the authors propose to use neural tangent kernels to craft a backdoor poisoning attack relying on a formulation based on bilevel optimization. This technique has been used in similar settings (e.g. meta-learning), but its application to poisoning/backdoor attacks is novel. However, there is a lack of discussion and comparison with existing state-of-the-art attacks using similar formulations to validate the method proposed by the authors. \nIn terms of reproducibility, although the authors provide some information about the experimental settings at the beginning of Section 3. Throughout the paper it is not really clear the settings used for all the experiments. For instance, the ablation study in Section 2.1 does not say anything about datasets or models used for the experiments. The way it is presented, the information provided in that section cannot be reproduced and the results cannot be properly assessed. \n",
            "summary_of_the_review": "The idea of using neural tangent kernels is interesting and I think that the paper has potential and I really encourage the authors to follow this research direction. However, I think that the paper requires more work on the following points:\n1) The experimental evaluation: it would be necessary to compared to other state-of-the-art attacks, especially those relying on approximate techniques to solve bilevel optimization problems. Similarly, it is necessary to analyze the behavior of the attack against existing defenses and analyze if there is a trade-off between attack effectiveness and detectability. \n2) The organization of the paper (see my previous comments). \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4802/Reviewer_yTWS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4802/Reviewer_yTWS"
        ]
    }
]