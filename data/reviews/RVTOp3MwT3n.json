[
    {
        "id": "0VDtybl10AO",
        "original": null,
        "number": 1,
        "cdate": 1665841504223,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665841504223,
        "tmdate": 1666353553206,
        "tddate": null,
        "forum": "RVTOp3MwT3n",
        "replyto": "RVTOp3MwT3n",
        "invitation": "ICLR.cc/2023/Conference/Paper86/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "DivDis tackles the problem of underspecification by generating diverse hypotheses and then disambiguating to choose the best one. Diversification is performed by training multiple heads to yield different predictions on an unlabeled target distribution. Disambiguation is performed by first actively querying to obtain labels for a small target set and then selecting the head with the best accuracy on this set.",
            "strength_and_weaknesses": "**Strengths (S):**\n\n[S1] The paper addresses a very important issue where the training data alone may not uniquely specify the best solution for the target distribution. This is a real issue for the current DNNs, since they are limited to producing a single hypothesis, which may or may not be the best way to perform the task. The work provides a principled way to uncover multiple plausible hypotheses and select the best one given a target distribution.\n\n[S2] The process of training diverse heads on unlabeled target distribution is logical. The experiments with the toy dataset and completely/partially correlated spurious factors clearly demonstrate the ability to learn different functions and the ability to choose the best one. However, I think the paper needs to clarify the points mentioned in the weaknesses section.\n\n[S3] The results showing that the hyperparameters that work on held-out source data work for target distribution too is important. This eases hyperparameter tuning. \n\n**Weaknesses (W):**\n\n [W1] Is it realistic to expect to have access to unlabeled data that has the SAME distribution as the test data? Wouldn't it be more realistic that the unlabeled data is distributed different from both train and test? How would the approach handle that situation?\n\n[W2] For each dataset/experiment, I think the paper should clarify how the unlabeled, target distribution was constructed. Where do the samples come from (a separate validation set?)? Does the size of unlabeled target set matter to ensure diversity?\n\n[W2.1] For Camelyon17, are both unlabeled data and test data from the same hospital? \n\n[W3] Most experiments are limited to 2 heads, so it is unclear how well the method generalizes beyond that (for non-toy datasets). \n\n[W4] Moreover, the separate head approach assumes prior knowledge of the number of plausible hypotheses. I wonder what happens when there is a mismatch between the actual number of plausible hypotheses for the task and the number of heads in the model (and not just toy dataset).\n\n[W5] For the sake of completeness, I think the paper should include methods (JTT/gDRO) for MNIST-CIFAR data with various mixing ratios.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The writing is clear except for a few parts mentioned in the weaknesses section.\n- Since the research direction itself seems novel, the proposed approach is also novel.\n- The code is publicly available.",
            "summary_of_the_review": "Overall, I vote to accept the paper since it addresses an important shortcoming of most DNNs: the inability to consider multiple, plausible ways of performing the task, especially in the absence of a clear signal from the training set. The proposed approach is sound and logical. \n\nClarifying some of the points I mentioned in the weaknesses could strengthen the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper86/Reviewer_X9wJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper86/Reviewer_X9wJ"
        ]
    },
    {
        "id": "7cgq42fquQM",
        "original": null,
        "number": 2,
        "cdate": 1665940011679,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665940011679,
        "tmdate": 1665940069481,
        "tddate": null,
        "forum": "RVTOp3MwT3n",
        "replyto": "RVTOp3MwT3n",
        "invitation": "ICLR.cc/2023/Conference/Paper86/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a solution to underspecification. The authors show how to learn multiple models/hypotheses compatible with the training data (low source risk). They then propose multiple options to select the best one (using additional data such as OOD annotations).",
            "strength_and_weaknesses": "### Strengths\n- The paper studies a problem of great importance (OOD generalization) that is receiving increasing attention.\n- The paper proposes a solution that is complementary to other contributions proposed very recently (appropriated discussed by the authors). This paper encourages diversity across models in prediction space, whereas other solutions work in the space of features or input gradients.\n- The disambiguation stage is cleverly addressed with a variety of strategies, which allow comparing a complete method (i.e. including model selection, which some competing works do not address).\n- Experiments are performed on a variety of tasks and datasets.\n\n### Weaknesses/questions\n- W1: In the conclusion: \"An appealing property of DivDis is its automatic discovery of disentangled features, as demonstrated in Sec. 4.\" Which results does this refer to specifically?\n\n- W2: All experiments (except one on toy data) seem to use 2 heads. Is that right? What happened if the method is applied with >2 heads on the other datasets? The results in (Teney et al. 2021) seem to indicate that more heads are often better. The method in the two papers are different, but it would be interesting to investigate if/why (not) more heads could also help here.\n\n- W3: This paper includes a comparison with the method of (Teney et al. 2021) only on Waterbirds-CC, which seems unfair. I would have liked to see a comparison with the same CIFAR-MNIST (with 4 tiles) used by these other authors, since this dataset can only work with >= 4 heads.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and well written. I feel like all details are included to replicate this method.",
            "summary_of_the_review": "The paper studies an important problem, proposes an innovative solution, and demonstrate very interesting results on a range of datasets. I recommend it for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper86/Reviewer_kpaT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper86/Reviewer_kpaT"
        ]
    },
    {
        "id": "NNjx7oXNkW",
        "original": null,
        "number": 3,
        "cdate": 1666353360071,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666353360071,
        "tmdate": 1666353360071,
        "tddate": null,
        "forum": "RVTOp3MwT3n",
        "replyto": "RVTOp3MwT3n",
        "invitation": "ICLR.cc/2023/Conference/Paper86/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper adresses distribution shift between train and test samples. The authors leverage the fact that different models can perform equally on the train distribution while exhibiting diversity in their prediction on the test distribution. The diversity among trained model is enforced by explicitly requesting different predictions on unlabeled test data. More precisely, this is done by adding a loss term that minimizes mutual information between pairs of predictions from two models during their respective trainings. Another loss term avoids learning naive classifiers such as constant functions. \nAt inference time, one needs to select the appropriate model. This is done by querying the label of few data points for which disagreement among trained models is high and examining average accuracy on these points. \n\n",
            "strength_and_weaknesses": "Pros : \n- The paper is very clear and easy to understand.\n- The contributions are well framed and positioned\n- The contributions are supported by clear experimental evidence \n\nCons : \n- The methods required unveiling the class labels of a selection of test points. \n- The approach is limited to underspecified data otherwise diverse predictors cannot be trained on the source distribution.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear. Nice justifications and motivations are provided. \nThe contributions are fairly novel compared to concurrent approaches. \nA code snippet is provided for one aspect of the method. I haven't seen some information about a repo where the entire code can be found. The method is simply enough to be reproduced event without code sharing, but I recommend to share the code anyway.",
            "summary_of_the_review": "I really enjoyed reading this paper and I have only a few remarks. \n\nHow does the model scale to large number of heads ? I suppose the MI term will be required for each pair of heads. Could this be a bottleneck ?\n\nI was expecting a bit more experiments of the number of necessary test point label reveals necessary. If possible, it would be nice to extend this. \n\nSince the methods is useful for underspecified data how can we assess to match this use case when start with a new dataset ?\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper86/Reviewer_5h8Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper86/Reviewer_5h8Y"
        ]
    },
    {
        "id": "gezSAr3TxJ",
        "original": null,
        "number": 4,
        "cdate": 1666701909305,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701909305,
        "tmdate": 1668575953732,
        "tddate": null,
        "forum": "RVTOp3MwT3n",
        "replyto": "RVTOp3MwT3n",
        "invitation": "ICLR.cc/2023/Conference/Paper86/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the problem of ***underspecification*** of the dataset. To address this issue, this paper proposes a *two-stage* framework, Diversify and Disambuguate (DivDis), for learning from underspecified data. For the first stage (***I. diversity***), DivDis learns a diverse set of hypotheses that gain low source loss but make different predictions on target data. Then, in the second stage (***II. disambiguate***), DivDis aims to select one of the learned functions. Note that, the work uses additional information for the function selection. In the experiment, this work validates the effectiveness of DivDis on datasets with *a complete correlation* (the source distribution has a spuriously correlated attribute that can predict the label perfect accuracy).\n\n",
            "strength_and_weaknesses": "**[Strength]**\n\n+ This work is well written. The problem, method, and experiment are clearly presented. \n\n+ The related work is nicely organized.\n\n+ The proposed DivDis is straightforward and has a clear motivation. Learning multiple and diverse functions on the underspecified dataset is reasonable. Then, selecting the most suitable function is also straightforward.\n\n+ The three strategies in stage two of DivDis are good. This work clearly presents them. Active querying required as little as a single label is interesting to me. In addition, this work highlights in Section 3.3 that \"We emphasize that existing OOD methods tune hyperparameters using target set labels\" Please clarify how the existing methods use target labels and give some references or examples.  \n\n**[Weakness]**\n\n- While the framework is reasonable, its novelty is somewhat weak. First, existing methods already study the idea of learning diverse functions. For example, works [1,2] also study the underspecification issue and propose to learn multiple different functions. From this point, the novelty of this work is limited.\n\n- Following up on the above, the first stage of this work (i.e., diversity) uses a mutual information loss to learn diverse functions. In comparison, [1] uses a penalty on the alignment of their input gradients; [2] uses a local independence loss and an on-manifold loss. Then, a question is: which manner is more effective to learn multiple diverse functions? A discussion or experimental analysis would be helpful.\n\n- The comparison with [1,2] should be clearly reported in the experiment. I noticed that methods of [1,2] do not have a simple function selection while this work has (stage 2). So, this work can use the same selection manner of [1,2] for comparison.  \n\n    [1] Teney, Damien, et al. \"Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization.\" In CVPR, 2022.\n\n    [2] Teney, Damien, Maxime Peyrard, and Ehsan Abbasnejad. \"Predicting is not understanding: Recognizing and addressing underspecification in machine learning.\" In ECCV, 2022\n\n - Crucial details are missing: 1) according to Section B.2, this work only uses N=2 heads. Namely, this work only learns two functions. Why use N=2? What is the effect of N on learning? 2) when the dataset is more complex (e.g., collages dataset of [2]), using N=2 might not suitable. Thus, this work needs to study N. \n\n-  In domain adaptation, using the disagreement of two classifiers is beneficial for achieving high accuracy on the target data [3]. Table 3 compares several domain adaptation methods. It would be better if this work could include [3] in the table. \n\n--- **Post Rebuttal** ---\nThe authors have provided responses for the above questions. The rebuttal is helpful and addresses most of my concerns.\n",
            "clarity,_quality,_novelty_and_reproducibility": "--- **Post Rebuttal** ---\n- This work has ***good clarity***: it clearly presents the problem, method, related works, and experiment.\n- The proposed method (DivDis) is reasonable to me. The idea of learning multiple functions is already explored and studied for the same problem of underspecification. However, the new contributions based on this idea are convincing.\n- This work gives ***sufficient details for the reproducibility***",
            "summary_of_the_review": "--- **Post Rebuttal** ---\n\nThe proposed method (DivDis) is reasonable to me. The idea of learning multiple functions is already explored and studied for the same problem of underspecification. However, ***the new contributions based on this idea are convincing and sufficient***. The authors also discuss the effect of the number of heads (N), which corresponds to the number of functions. Therefore, I tend to accept this paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper86/Reviewer_jJSw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper86/Reviewer_jJSw"
        ]
    }
]