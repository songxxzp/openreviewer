[
    {
        "id": "cuar0BZL8X7",
        "original": null,
        "number": 1,
        "cdate": 1666133299703,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666133299703,
        "tmdate": 1669768912264,
        "tddate": null,
        "forum": "U9yFP90jU0",
        "replyto": "U9yFP90jU0",
        "invitation": "ICLR.cc/2023/Conference/Paper3187/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Well known problem by now is the non-iidness of the client data causes degradations in FL trained models. The paper proposes FedFA, federated feature augmentation to address feature shift in clients data. FedFA exploits instance based statistics to augment local features to curb the introduction of novel domains because of augmentations. The proposed approach sounds promising to advance FL.",
            "strength_and_weaknesses": "# Pros:\n\n- The paper is well-written, easy to understand and follow along.\n\n- The idea of treating multiple channels as multiple variates and measuring the statistical variances is really interesting since they will always remain normally distributed as they are measured over mean and variances already.\n\n- A very simple technique with no or negligible communication overheads.\n\n- Literature is covered adequately.\n\n# Cons:\n\n- The FL based hyper params are missing in the paper, such as the number of rounds, the number of clients participating in training, client/device selection if any, etc.\n\n- The scale of the benchmarks is small compared to CIFAR-10 or EMNIST, can you show the method works on one/both of these relatively large dataset.\n\n- Also, the number of samples per client seems to be uniform and the proposed statistical measures seem to work well for something like that. Can you experiment with an unequal number of samples per client? \n\n- The main advantage that I see with FEDFA are the communication overheads compared to mixup kind of augmentation approaches (of course privacy is there, this on top), why not asymptotically quantify the costs?The cost analysis that is currently presented is talking about some raw numbers, they will go away if it is \\BigO notation and the costs might be almost same as FedAvg.\n\n- It looks like there is an ablation on \\alpha and \\p done separately, but what is the best combination, can we call it (\\alpha, p) = (1, 0.5) or (1.,0, 1.0) or (0.5,0.5)?\n\n\n### Minor comments: \n\n- \u201cSince data coming different users \u2026 \u201d should be corrected\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper clearly explains the concepts and the presentation is easy to follow\n\nThe work has quality but it can be further improved by increasing the scale of the benchmarks.\n\nThe proposed approach is simple and novel in the realm of federated optimization algorithms which definitely contributes to the body of knowledge in the domain/literature.\n\nThe method is easy to reproduce and the results can also be tested, having access to the open sourced code can help further.",
            "summary_of_the_review": "Please refer to the strengths and weaknesses ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3187/Reviewer_q8Hi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3187/Reviewer_q8Hi"
        ]
    },
    {
        "id": "Kq2dJmNlJ4b",
        "original": null,
        "number": 2,
        "cdate": 1666337446237,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666337446237,
        "tmdate": 1669769866360,
        "tddate": null,
        "forum": "U9yFP90jU0",
        "replyto": "U9yFP90jU0",
        "invitation": "ICLR.cc/2023/Conference/Paper3187/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tries to address feature shift by modelling feature statistics among clients.  Specifically, the authors model feature statistic via a Gaussian distribution.  The mean of the Gaussian distribution denote the original statistic while the variance denote the augmentation scope. The authors designed a solution to determine an appropriate variance for the Gaussian distribution. In the solution, the Gaussian variance is estimated by not only the local data but also the feature statistics from all participating clients.  In the variance estimation, the authors designed an adaptive variance fusion strategy to assign different weights to different channels in the feature statistic space.\nAfter establishing the Gaussian distribution, the novel features can be synthesised by sampling from the Gaussian distribution.",
            "strength_and_weaknesses": "Strength:\n-- modeling the feature statistic via a Gaussian distribution and generating new features by sampling from the Gaussian distribution seems interesting.\n-- the client sharing statistic variances and adaptive variance fusion strategy are technically solid.\n\nWeakness:\nWhat concerns me is the writing of this work. I feel it is not clear enough and I feel a little hard to fully understand the technical details.\n-- I feel difficult to fully understand the technical details. For example, in Eq 6, there is j  while in Eq 7 there isn't. How to use the result calculated in Eq 6 for Eq 7? \n-- How to take advantage of the shared feature statistics are not clear to me. Specifically,  the relation between Eq. 5/6/7 is not clear to me.\n-- the formulation terms in this draft need refined. For example, in page 4, \\mu_m^k \\sim \\mathcal{N} (\\mu_m^k, ##).  ",
            "clarity,_quality,_novelty_and_reproducibility": "This work is of fair quality. I feel the writing is not clear enough and its technical details need to be clarified. The originality of the work is good.",
            "summary_of_the_review": "This work propose to model feature statistic via a Gaussian distribution. The parameters of the Gaussian distribution is estimated from the data from active clients. For the parameter estimation, the authors designed client sharing statistic variances and adaptive variance fusion strategies.  Once the parameters are estimated, new features are generated by sampling from the Gaussian distribution. Although the idea seems interesting, the writing of this work needs significant improvement. I feel hard to fully understand the technical details.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3187/Reviewer_7jRY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3187/Reviewer_7jRY"
        ]
    },
    {
        "id": "KatJXYY1om",
        "original": null,
        "number": 3,
        "cdate": 1666664394616,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664394616,
        "tmdate": 1666675261901,
        "tddate": null,
        "forum": "U9yFP90jU0",
        "replyto": "U9yFP90jU0",
        "invitation": "ICLR.cc/2023/Conference/Paper3187/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes federated feature augmentation to solve the data heterogeneity in federated learning. Each local model models its feature with Gaussian distribution, where the variance is the key to fulfill augmentation.\nThe experiments on image classification and medical image segmentation show effectiveness of the proposed work.  ",
            "strength_and_weaknesses": "Strength:\n1. The proposed method targets at the non-i.i.d. problem of FL, and the idea of feature level augmentation is interesting. \n2. The FedFA method is simple but effective. Empirical comparisons with classical FL methods demonstrate the effectiveness of the proposed feature augmentation method.\n\nWeakness:\n1. FedRobust and FedBN also targets at feature shift problem in FL, it is not clear why \"these algorithms may still suffer significant local dataset bias\". Could the authors provide in-depth theoretical analysis how the proposed method is different/superior compared with these two counterparts? Besides, the experiments should show comparisons with FedRobust and FedBN.\n2. I wonder if the method utilizes the statistics of features from all layers or specific layers. The paper lacks discussions and ablations regarding this. \n3. The experiments only focus on splitting local data with different data origin. I would expect results with other kinds of local data heterogeneity, such as large variances among local data size or different modalities among locals. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear. The experiments design is generally good. \nThe proposed method is simple yet novel.",
            "summary_of_the_review": "The paper proposes a novel feature augmentation method for non-i.i.d. federated leaning. \nMy main concern is the method only shows effectiveness with local data split by data origin. There should be more experiments for other kinds of local data heterogeneity (e.g. variant local data size). A typical FL setting is to split data with Dirichlet distribution. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3187/Reviewer_JqCN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3187/Reviewer_JqCN"
        ]
    },
    {
        "id": "bEELce5DzKs",
        "original": null,
        "number": 4,
        "cdate": 1667072166453,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667072166453,
        "tmdate": 1671011827893,
        "tddate": null,
        "forum": "U9yFP90jU0",
        "replyto": "U9yFP90jU0",
        "invitation": "ICLR.cc/2023/Conference/Paper3187/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper develops a robust federated learning algorithm to address feature shift in clients\u2019 samples, which can be caused by various factors, e.g., acquisition differences in medical imaging. The paper proposes FEDFA to tackle federated learning from a distinct perspective of federated feature augmentation. The paper models each feature statistic probabilistically via a Gaussian distribution, with the mean corresponding to the original statistic and the variance quantifying the augmentation scope, from which novel feature statistics can be drawn to fulfill augmentation.",
            "strength_and_weaknesses": "Strength:\n(1) The paper propose a federated learning algorithm from a novel perspective of feature augmentation.\n(2) The proposed approach is conceptually simple but seems to be effective.\n\n\nWeakness:\n(1) The most related baseline seems to be FEDMIX. However, the evaluated datasets and the adopted model architecture in this paper are different from the datasets in the original paper of FEDMIX. There is limited discussion of the rationale of how the datasets are selected and why they are different from the original paper of FEDMIX.\n(2) A key assumption of this paper is that the each feature statistic follows a multi-variate Gaussian distribution. I am not sure whether the assumption is valid, for the neural embeddings in general. The paper may discuss more details of how/when this assumption is satisfied in tasks relying on deep neural networks.\n(3) Previous works (e.g., FEDMIX) provide theoretical discussions. This paper may be further improved by providing theoretical discussions.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well motivated, and the proposed solution is conceptually simple and has good intuitions. The evaluations may be further improved by using the same datasets and model architecture in the original paper of FEDMIX.\n\nThe paper is clearly written, and it is very easy to read the paper.\n\nThe paper proposes a federated learning algorithm from a novel perspective of feature augmentation. Conceptually, it can bring additional advantages, compared to the SOTA work in this area (FEDMIX). However, the evaluations may be further improved by using the same datasets and model architecture in the original paper of FEDMIX.",
            "summary_of_the_review": "The paper proposes a federated learning algorithm from a novel perspective of feature augmentation. Conceptually, it can bring additional advantages, compared to the SOTA work in this area (FEDMIX). However, the evaluations may be further improved by using the same datasets and model architecture in the original paper of FEDMIX. A key assumption of this paper is that the each feature statistic follows a multi-variate Gaussian distribution. The paper may discuss more details of how/when this assumption is satisfied in tasks relying on deep neural networks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3187/Reviewer_uZsm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3187/Reviewer_uZsm"
        ]
    }
]