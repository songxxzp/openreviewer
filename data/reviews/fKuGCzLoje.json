[
    {
        "id": "66rV76OmprE",
        "original": null,
        "number": 1,
        "cdate": 1666303312307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666303312307,
        "tmdate": 1669057305551,
        "tddate": null,
        "forum": "fKuGCzLoje",
        "replyto": "fKuGCzLoje",
        "invitation": "ICLR.cc/2023/Conference/Paper2448/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the metrics (or abstractions) on the policy space for a given MDP. The paper defines three pseudo-metrics on the policy space: $d_\\pi(\\pi,\\pi\u2019)$ that measures the distance between the outputs of $\\pi$ and $\\pi\u2019$ given a state, $d_{P^\\pi}(\\pi,\\pi\u2019)$ that measures the distance between the distributions of next state given a state and policy $\\pi$ (and policy $\\pi\u2019$), and $d_{V^\\pi}(\\pi,\\pi\u2019)$ that measures the difference between the value functions of $\\pi,\\pi\u2019$. This paper also provides an algorithm that learns a representation of policies such that the l2 distance of the representations is approximately equal to a given metric. Empirically, the metrics on the policy space can be used to improve algorithms such as TRPO and DGES on grid world environments, and can help value generalization on off-policy evaluation tasks.",
            "strength_and_weaknesses": "### Strengths:\n\n- The question of studied in this paper is interesting and relevant to many deep RL algorithms. For example, TRPO iteratively optimize its policy in a small neighborhood, whose definition requires a metric on the policy space. Different metrics induces different algorithms and it\u2019s interesting to see how the choice of the metrics affects the performance.\n- The results for value generalization on off-policy evaluation tasks are interesting, and raises several questions to ask in future works: does the same conclusion hold for more complex environments? Is it possible to use this method to off-policy optimization tasks? Does the method improve over other baselines?\n\n### Weaknesses:\n\n- The contribution of this paper is somewhat limited. The theoretical results are rather  straight-forward, and the empirical results are limited to grid-world environments (for policy optimization) and simple Mujoco tasks (off-policy evaluation). For the policy optimization experiments, the results only demonstrate that, unsurprisingly, the performance of TRPO and DGES depends on the choice of metrics. For off-policy evaluation, this paper only tests two tasks from the Mujoco environment.\n\n- It is unclear to me whether a policy abstraction is necessary for online RL. Many algorithms for online policy optimization do not require a metric on the policy space, and whether TRPO with a good metric can outperform those algorithms remains a question.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written in general, but some important details are missing:\n- How is $d_{P^\\pi}$ and $d_{V^\\pi}$ computed in the experiments? Does the algorithm know the transition model of the underlying MDP? How is the value function computed? To some extent, computing the value function of every given policy is not much easier than solving the optimal policy of the MDP.\n",
            "summary_of_the_review": "My main concern is that the contribution of this paper is limited both theoretically and empirically. As a result, I recommend a rejection for the current version of this paper. \n\n==== after rebuttal ====\n\nI thank the authors for addressing my concerns regarding the relevance of policy abstraction in prior works. However, I am still not convinced that the contribution of this paper is significant enough. Therefore, I raised my score slightly, and encourage the authors to evaluate the policy abstraction algorithms systematically on benchmarking environments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2448/Reviewer_M8fJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2448/Reviewer_M8fJ"
        ]
    },
    {
        "id": "ozi69UgSphy",
        "original": null,
        "number": 2,
        "cdate": 1666601322832,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601322832,
        "tmdate": 1666601362800,
        "tddate": null,
        "forum": "fKuGCzLoje",
        "replyto": "fKuGCzLoje",
        "invitation": "ICLR.cc/2023/Conference/Paper2448/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on providing a theory and methodology on policy abstraction and representation to reduce the high complexity of policy space in the Markov Decision Process. To achieve this, they first proposed a unified policy abstraction theory and discussed three types of policy abstraction. Then they generalize these policy abstractions to three policy metrics that can quantify the distance of policies instead of only binary signals. Further, they proposed a policy representation learning method based on the policy metrics. Empirically, they conducted experiments in both policy optimization and evaluation problems, which demonstrates the effectiveness of the proposed representation method. ",
            "strength_and_weaknesses": "**Strengths**\n\n1. The problem this paper considers is very important, and this paper provides a new method to solve it.\n2. The literature review is sufficient. This paper has offered a detailed table of a taxonomy of prior policy abstractions under the policy abstraction theory in the Appendix. \n3. This work is well-motivated. To solve the challenge of large scale and high complexity of policy space, policy abstraction representation might be a very promising way. And this work provides a unified theory for this line of works. \n4. The empirical evaluation and theoretical results are of high quality. (1) Figure 1 is especially interesting and it clearly showcases the differences between the three policy metrics. (2) The definition of abstraction fineness is also interesting and might be a key property of policies. \n\n**Weaknesses**\n\n1. The empirical results are significant in some easy tasks and some specific algorithms. These results might be enough for this paper but the reviewer is still wondering whether there is a unified way to use policy abstraction to other algorithms that do not need policy metrics explicitly, such as PPO, for policy learning not policy evaluation. In other words, can $f_\\psi$ in Equation 1 be directly used for policy learning?\n\n2. This paper does not provide enough discussions of limitations. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. Although the three policy metrics are not very new, the proposed unified theory of policy abstraction is novel and it provides a good tool for designing more practical methods of policy abstraction. ",
            "summary_of_the_review": "This paper is a very complete work about policy abstraction and may inspire future related works. Although the optimality of the policy abstraction is not guaranteed or discussed, this paper still fill up the plank in both the theory and methodology of this sub-field. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2448/Reviewer_LtQT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2448/Reviewer_LtQT"
        ]
    },
    {
        "id": "p9d3wUURI9P",
        "original": null,
        "number": 3,
        "cdate": 1666657135847,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657135847,
        "tmdate": 1666671074890,
        "tddate": null,
        "forum": "fKuGCzLoje",
        "replyto": "fKuGCzLoje",
        "invitation": "ICLR.cc/2023/Conference/Paper2448/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper first highlights the observation that the policy space size can be prohibitive for policy search and off-policy evaluation when the  state-action space is large (e.g., high-dimensional continuous). Then, the paper aims to propose a unified theory of policy abstractions for determining equivalence between given pairs of policies analogously to the unifying state abstraction theory of Li et al. (2006). Three distinct policy abstractions are identified based on equality criteria for (i) action distributions, (ii) next-state distributions and (iii) expected value when the agent's behavior is conditioned on the respective policies. A theorem that attempts to establish a coarseness-based partial ordering between said abstractions (as well as two trivial extremal abstractions) is presented. The abstraction equivalence relations are relaxed to similarity metrics via an arbitrary statistical distance. Three GridWorld MDPs are considered to provide intuition on the differences between these similarity metrics. A policy representation learning approach is proposed: the approach minimizes an MSE error between policy embedding distance and an MMD-based realization of the target policy similarity distance. Empirical evaluations investigate the use of these metrics in the context of policy search on trust-region policy optimization (1st order) and diversity-guided evolutionary strategies (0th order) for GridWorld environments. Further empirical evaluations on Off-Policy Evaluation (OPE) performance are provided. These measure the ability to interpolate and extrapolate policy performance from policy parameters without interaction with the environment given prior policy-value pairs that are used to learn policy representations that respect policy distances in latent space.",
            "strength_and_weaknesses": "Weaknesses:\n\n1. [Major] I think that the writing of this paper could be significantly improved and it needs copy editing (frequent grammatical mistakes, awkward wording, etc.).\n\n2. [Major] Simply put, Thm. 3.1, which is the main theoretical result of the paper, is factually incorrect and therefore misleading as it stands. As discussed in the Appendix, as soon as the expected immediate reward depends on the action selection, $f_{P^{\\pi}}(\\pi_i) = f_{P^{\\pi}}(\\pi_j)$ does not imply $f_{V^{\\pi}}(\\pi_i) = f_{V^{\\pi}}(\\pi_j)$. The discussion in the Appendix downplays the importance of this case as a minority among interesting environments, but one can easily imagine, for instance, an Atari game (hardly a minority) where taking different actions in a given state yields different rewards despite transitioning the environment to the same next state. For a theorem to be mathematically correct, its surrounding conditions and assumptions must be listed explicitly. These cannot be deferred to the Appendix. The appeal of a simpler expression cannot take precedence over facts.\n\n3. [Major] The formulations of the *-irrelevance abstractions and metrics are fairly trivial and should be readily apparent to a savvy RL audience, as are the coarseness relations among them that do hold true. For instance, it's quite obvious that $f_\\pi \\succeq f_{P_\\pi}$ since $\\pi_1(a|s) = \\pi_2(a|s), \\forall a \\in A \\Rightarrow P_{\\pi_1}(s'|s) = P_{\\pi_2}(s'|s)$.\n\n4. [Major] I would have expected significantly more discussion on the choice of the state distribution $p$ over which the expectation of the distance is taken to compare policies. In Fig. 1, the doorway environment is used to dismiss $d_\\pi$ as a metric that fails to show the difference, but as noted in the paper itself, this is only the case because a uniform $p$ is used. Then, isn't this a failure to choose the right $p$ for computing $d_\\pi$ rather than a failure of $d_\\pi$ itself?\n\n5. [Major] It's not clear to me whether increased stochasticity in Fig. 1 indeed makes \"for a better evaluation\" and what \"better\" means. For example, in the case of $d_{V^\\pi}$ the evaluation no longer makes sense as the value of both policies decay rapidly with increasing stochasticity since the probability of reaching the goal decays. Given both value functions $V^{\\pi_1}$ and $V^{\\pi_2}$ decay, it's not at all surprising that their difference would as well. Note that this can also be remedied by choosing $p$ to be concentrated around states adjacent to the goal.\n\n6. [Major] The empirical evaluations on policy optimization only cover GridWorld environments. This would have been acceptable for a more theoretical paper, but the theory here is fairly simple so I would have expected evaluation on a larger number of more serious benchmarks (e.g., 3D continuous control). Furthermore, the shaded confidence areas in Fig. 2, especially 2a, show significant overlap between various approaches, which make it difficult to draw meaningful conclusions or be convinced of the claims made in the paper. Even if we take the mean curves, none of the policy metric learning approaches seem to significantly improve over the vanilla TRPO approach (which uses the maximum TV distance to measure policy distance). This result is once again deferred to the Appendix Fig. 4, but must be included in Fig. 2 in the main text in my opinion.\n\n7. [Minor] Both, IDP and LLC are listed as MuJoCo environments, but I believe only IDP is a MuJoCo environment. The LunarLanderContinuous-v2 environment should be a simpler 2D environment from Brockman et al. (2016) to the best of my knowledge.\n\n8. [Minor] It would have been nice if the paper engaged more with the literature on state abstractions and state similarity metrics.\n\n9. [Minor] Near the end of Sec. 3.2, it is noted that a common choice for $D$ could be the KL divergence, but this is inconsistent with Def. 4, which requires that $D$ is a (pseudo-)metric. KL divergence is neither a metric nor a pseudo-metric, but a _divergence_. \n\nStrengths:\n\n- I appreciated the importance of studying policy abstractions and the attempt at providing a unified theory. \n- I think the paper includes some interesting ideas and has a good set of initial experiments, which would make for a good publication after a major revision with better writing, more extensive evaluation and more precise claims & mathematical statements.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: while the writing is quite sloppy, interestingly, this does not take much away from clarity. However, at various points in the paper important results are pushed into the Appendix and their implications are omitted in the main text (see Weaknesses above), which does take away from clarity.\n- Quality: I found the overall quality of the paper to be below average due to reasons listed under Weaknesses above.\n- Novelty: I did not find the taxonomy of policy abstractions / similarity metrics proposed by the paper to be original or deeply insightful. ",
            "summary_of_the_review": "My current recommendation for the paper is rejection. I am not convinced of the significance and validity of neither empirical nor theoretical results to a satisfactory extent, and further believe that a major revision is required to improve the writing and organization. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2448/Reviewer_DwZy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2448/Reviewer_DwZy"
        ]
    },
    {
        "id": "txf1lXhwG7",
        "original": null,
        "number": 4,
        "cdate": 1667432355794,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667432355794,
        "tmdate": 1667432355794,
        "tddate": null,
        "forum": "fKuGCzLoje",
        "replyto": "fKuGCzLoje",
        "invitation": "ICLR.cc/2023/Conference/Paper2448/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a theoretical framework to categorize policy abstractions based on what properties are preserved. The main contribution is the proposal of irrelevance-based definitions based on policy action distribution, state transition distribution and state values. The authors then propose corresponding pseudo-metrics that permit for more approximate notions of irrelevance. The theoretical contributions conclude by showing that proposed categories of abstraction can be ordered using a concept of abstraction fineness. The authors show some illustrative empirical results in the case where the proposed metrics and their relevant distributions can be computed and compared exactly.\n\nFor the purpose of learning policy representations, the authors then propose a scalable approximate method for optimizing these metrics based on optimizing an alignment loss where the metrics are approximated using maximum mean discrepancy, a RKHS based approach. Empirical results are provided for a policy optimization task using TRPO and a diversity-guided evolution approach comparing the behavior of when simultaneously learning and constraining with, in the case of TRPO, the policy representation. The final empirical contributions compare the metrics and learned representation's ability to generalize state-values in the context of off-line policy evaluation.",
            "strength_and_weaknesses": "High-level comments\n===============\n\nThe propose categorization of policy abstractions are interesting. Seeing as they are inspired by similar efforts for state abstraction and, more generally, by existing formalism of abstraction, I would expect these definitions to be provide useful categorizations future work can build off of. The accompanying theorem further strengths the theoretical contributions and my only complaint is that I which there was more.\n\nMy biggest issue is that many of the experiments don't seem to provide much insight on the properties of these abstractions. The empirical contributions are probably the weakest aspect of this paper. Overall, these contributions mostly involve retroactively interpreting and attributing meaning to the results of the experiment through the lens of the proposed abstractions, rather than focusing on more targeted experiments which provide insight on the benefits and limitations of these abstractions, e.g., with domains designed to highlight stark differences. When scaling to complex settings with approximations and many design decisions, it becomes difficult to know if some of the patterns we see are due to interpretation the author's provide or if it is just a result of the particular combination of choices and domain.\n\nDetailed comments and questions\n==============\n\nWhy is definition 4.3 based on distributions over returns while definition 3.3 is defined on expected returns, i.e., state values?\n\nWhat discount factors are used? Notably, in Figure 1?\n\nSec 4.3 was hard to follow and took me a fair bit of effort/time to understand what the authors were doing. It feels like this can be explained a bit better. Maybe making equations more explicit or a diagram would help?\n\nIn addition to targeted experiments, experiments that vary fewer variables, e.g., small methods but different environment, can also provide some evidence if trends are robust. However, the empirical results found in Figure 2 vary both domain and the optimization method possibly conflating any trends.\n\nAm I mistaken in saying that there is no original TRPO (which uses max KL-divergence as proxy for total variation if I recall correctly) as baseline?\n\nWhy does the performance of several of the TRPO variants decreasing?\n\nFigure 2, why plot only 1/2 standard deviation?\n\nSec. 6, many of the comparisons seem to make strong conclusion from what seems like insignificant differences in results. In some instances, differences are well below the standard deviation. I don't think much can be said from these results, especially in the \"strong generalization\" case which uses only 5 seeds.\n\n\nMinor nitpicks\n===========\n\nParagraph 2, sec 4.2, should probably use Gretton et al. 2012 as main reference.\n\nEq. 3, might be worth mentioning that the biased estimate was used instead of the unbiased one.\n\nlast para, sec 4.3, \"policy buffer\" doesn't tell me where they came from. I would recommend saying something like \"past observed policies\" or abstract it away more explicitly, e.g., \"policy samples from some given dataset of policies\".\n\nFigure 2 (and several other places), \"std\" what? Standard error or standard deviation? This should be stated fully.\n\nFigure 2, caption probably should mention that the two subplots show results for different environments (i.e., shouldn't be directly compared).\n\nTable 2, it might be worth discussing training error when commenting on generalization. Do cases with the best test errors also have the best train error? The current table makes this tedious to deduce. The test and train error could be reported instead, using highlighting to show better gap. Alternatively, just discussing the train error in the text would do.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and ideas are well structured. The contributions are novel, to the best of my knowledge. The empirical contributions are likely reproducible, with at most a few minor details omitted.",
            "summary_of_the_review": "The base idea of this work is very interesting but isn't carefully examined. The theoretical results plus the illustrative experiment might not be sufficient on it's own, as is. The other contributions, the approximate policy representation learning approach and accompanying experimental results, don't seem particularly insightful and, in some cases, significant. For this reason, I'm on the fence as to whether this is ready for publication but I'm open to discussion.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2448/Reviewer_5gmd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2448/Reviewer_5gmd"
        ]
    }
]