[
    {
        "id": "wNCUu7fZQl",
        "original": null,
        "number": 1,
        "cdate": 1666422189324,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666422189324,
        "tmdate": 1666422189324,
        "tddate": null,
        "forum": "wKPmPBHSnT6",
        "replyto": "wKPmPBHSnT6",
        "invitation": "ICLR.cc/2023/Conference/Paper6087/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new message passing method for GNN. It orders the message passing into the node representation with specific blocks of neurons targeted for message passing within specific hops. Extensive experiments show the effectiveness of the proposed method on homophily and heterophily data. It also alleviates the over-smoothing problem. ",
            "strength_and_weaknesses": "The strength of this paper includes:\n1. The paper is well-motivated. \n2. The experiments are extensive.\n3. The analysis is sufficient.\n The weaknesses are:\n1. The experimental results are not impressive. The improvement is not significant.\n2. The comparison methods are not STOA. Some recent works are ignored.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper develops a new method for message passing. It aligns the hierarchy of the rooted-tree of a central node with the ordered neurons. The motivation is clear and the paper is well-organized. The experiments are also comprehensive to evaluate the effect of each component. \n",
            "summary_of_the_review": "My worry about this paper is on the experimental part. Most of the comparison methods are published in or before 2020. Only GGCN, which was published in 2021, is compared. Much progress has been achieved in 2021 and 2022. Therefore, it is fairer to compare some of them. Furthermore, the improvement is also not significant in most cases.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6087/Reviewer_gn7g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6087/Reviewer_gn7g"
        ]
    },
    {
        "id": "EPfWzplm0Rk",
        "original": null,
        "number": 2,
        "cdate": 1666617544709,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617544709,
        "tmdate": 1666617544709,
        "tddate": null,
        "forum": "wKPmPBHSnT6",
        "replyto": "wKPmPBHSnT6",
        "invitation": "ICLR.cc/2023/Conference/Paper6087/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposed one ordered GNNs to mix the features calculated from different hops. This method can achieve the SOTA performance on both homophily and heterophily graphs, and could prevent the over-smoothing problem.",
            "strength_and_weaknesses": "### Strengths:\nIt is interesting to introduce the ordered neurons into graph representation learning, and it is one straightforward solution for the over-smoothing problem. The experimental results demonstrate the effectiveness in preventing this problem.\n\n### Weakness:\n\n1. The motivation for separating the different hops is not clear. As mentioned in the introduction, what are the advantages of \u201cmodel the information exactly at some orders\u201d? \n\n2. The evaluations on the node-wise split points are lacking. Designing the personalized operation for each node helps to improve the model performance [1,2]. It seems that different nodes have different split points as mentioned in Section 2.3. This aspect is not evaluated in the experiments, either by comparing with some node-wise GNNs or showing the different split points. Besides, there lacks a detailed discussion about the ordered neuron methods to verify the contributions of this paper.\n\n3. The information extraction on the ego features. The self-loop-free graphs are utilized in this paper. How to incorporate the node features from itself? \n\n### Refs:\n[1] Graph Neural Networks with Node-wise Architecture. KDD 2022\n\n[2] Policy-GNN: Aggregation Optimization for Graph Neural Networks. KDD 2020\n",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation of the proposed method is kind of unclear and lack some evaluations.",
            "summary_of_the_review": "It is interesting to employ the ordered neurons in the graph representation learning and the experiments can demonstrate the effectiveness. However, the proposed method is not well motivated and some baselines are lacked. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6087/Reviewer_44qL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6087/Reviewer_44qL"
        ]
    },
    {
        "id": "uBOOmldJOP",
        "original": null,
        "number": 3,
        "cdate": 1667427611376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667427611376,
        "tmdate": 1667427611376,
        "tddate": null,
        "forum": "wKPmPBHSnT6",
        "replyto": "wKPmPBHSnT6",
        "invitation": "ICLR.cc/2023/Conference/Paper6087/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an ordering mechanism for the messages passing in GNN for node representation learning. In particular they are focused on the combining stage in the GNN procedure. The ordering mechanism is achieved by the rooted-tree hierarchy and a curated gating function. which can iteratively preserve the ego information and gradually absorb k-hop neighboring features. The authors claimed to address the problem of \u201cover-smoothing\u201d and \u201cheterophily\u201d. The experiment results show the performance boost in both settings above. And the large scale dataset experiment demonstrated the scalability of the proposed method.",
            "strength_and_weaknesses": "Strength:\n1. Intuitive and effective design of the gating function to iteratively update the feature while preserving the ego node info by inductive bias using ordered neurons.\n2. Sufficient number of experiment in both small and large scale dataset.\n3. Good presentation, easy to follow.\n\nTo be clear:\n1. For the experiment result in the Squirrel dataset, the performance surpass the baselines by a quite large margin. I'm wondering if there is an underline reason for such improvement.\n2. Fig 3 shows some baselines which previously designed to deal with the over-smoothing problem are actually pretty good. Even though there overall performance doesn't beat the proposed Ordered GNN. A good question is that if the number of layers is further increased, what would happen? In another word, is such carefully designed gating procedure actually needed for solving the over-smoothing problem?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear. The experiment result is convincing. But, the source code is not released which prevents me from the judgment of the reproducibility.\n\nAlthough the ordered neuron trick was introduced in prior work, the author adapted it to literately preserving the ego info which is still considered as a good contribution in terms of novelty.",
            "summary_of_the_review": "The proposed Ordered GNN approach is clear and the experiments and the ablation studies demonstrated the effectiveness of each part of the design. The algorithm is tested in both small and large scale dataset. I recommend for accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6087/Reviewer_rLLb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6087/Reviewer_rLLb"
        ]
    },
    {
        "id": "TTEbag9ntov",
        "original": null,
        "number": 4,
        "cdate": 1667661330734,
        "mdate": 1667661330734,
        "ddate": null,
        "tcdate": 1667661330734,
        "tmdate": 1667661330734,
        "tddate": null,
        "forum": "wKPmPBHSnT6",
        "replyto": "wKPmPBHSnT6",
        "invitation": "ICLR.cc/2023/Conference/Paper6087/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an Ordered GNN for handling the heterophily problem, which attempts to improve the updating process in the message passing scheme. The paper studies an important problem, i.e., how to extend current GNNs to heterophilic graphs.",
            "strength_and_weaknesses": "This paper presents an Ordered GNN for handling the heterophily problem, which attempts to improve the updating process in the message passing scheme. The paper studies an important problem, i.e., how to extend current GNNs to heterophilic graphs. Unfortunately, there are many confusions and presentation problems in this paper, which makes it hard to follow. Besides, I also have several concerns about its motivation, detailed method, and evaluations.\n\nComments\n1. I am afraid that there are many confusions and grammar errors in the paper, which makes it hard to follow. For example, in Sec. 2.2, I am wondering what \"one round of message passing\", \"the neurons used to represent the nodes\u2019 ego information\", \"the neurons used to represent message passing within Tao^{(k-1)} should be a subset of those for Tao^{(k)}\", and \"the delta between k-1 and k\" mean. Besides, the paper may benefit from revising certain presentations, e.g., \"its k-th order rooted tree\", \"And since \u2026\", \"Apparently here we have\u2026\", \". where Tao^{(0)} corresponds to\u2026\". I am afraid that the paper needs to be carefully revised.\n2. I am afraid that some utilized mathematic symbols lack formal definitions and thus induce unnecessary confusions. For example, Tao^{(k)}_{v} is defined as the \"k-th order rooted tree\" of node v. However, it is not clear what its subset operation is (as shown in Eq. (3)) and why \"Tao^{(0)}_v corresponds to the neurons used to represent the ego representations\".\n3. I am afraid that the reviews of related work in Sec. 1 are confusing. This paper focuses on two drawbacks of the message passing scheme, i.e., over-smoothing and heterophily. It is not clear which problems these reviewed literatures were designed to solve. It actually seems that this paper considers these two problems as the same problem.\n4. In Sec. 2.5, I am afraid that the intuitive analysis for Fig. 2 is not convincing. The paper states \"In Case 1, two root nodes share the same first-order neighborhood pattern, one GNN layer can capture this good heterophily and obtain good performance\". This analysis only considers node 1 and node 2. However, the children of these two nodes possess different situations which are not considered in this analysis. The analysis for Case 2 also has a similar problem.\n5. I am afraid that the results of the employed baselines in Table 2 are most borrowed from a preprint paper, which is not convincing. For example, I am just wondering why the results of GRP-GNN are much lower than its reported ones.\n6. I am just wondering how the proposed method can specifically solve the over-smoothing and heterophily problems.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to the Strength And Weaknesses above.",
            "summary_of_the_review": "The paper studies an important problem, i.e., how to extend current GNNs to heterophilic graphs. Unfortunately, there are many confusions and presentation problems in this paper, which makes it hard to follow. Besides, I also have several concerns about its motivation, detailed method, and evaluations.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6087/Reviewer_UBsM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6087/Reviewer_UBsM"
        ]
    }
]