[
    {
        "id": "wQdKPC-6Zt",
        "original": null,
        "number": 1,
        "cdate": 1666632888618,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632888618,
        "tmdate": 1666730825614,
        "tddate": null,
        "forum": "dSYkYNNZkV",
        "replyto": "dSYkYNNZkV",
        "invitation": "ICLR.cc/2023/Conference/Paper2283/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes an augmentation-free graph contrastive learning framework based on the assumption of the graph homophily assumption. Meanwhile, the authors use the Random Fourier Features to approximate the negative loss in contrastive learning and thereby reduce the computation cost to linear complexity. ",
            "strength_and_weaknesses": "The following are the pros of the paper: \n1. The paper proposes a new graph contrastive learning without relying on any hand-crafted graph augmentation techniques. According to the reported results, the proposed method achieves SOTA performances on many datasets. \n\n2. The paper design a surrogate loss to approximate the negative loss in contrastive learning and thus reduce the quadratic computation cost in linear complexity. Experiments are conducted to demonstrate the obvious complexity advantages of the proposed method over prior graph CL models with extensive negative sampling (GRACE etc.)\n\n3. Overall, the paper is well-organized and easy to read. \n\nThe following are some concerns and questions about the paper:\n1. Although the proposed model can achieve better computation efficiency compared with those GCL methods with extensive negative sampling, this advantage does not exist for those negative-sample-free methods, e.g., BGRL and CCA-SSG. Besides, the performance gain over the two baselines seems to be incremental, the authors are encouraged to clarify the contribution of the proposed methods over the prior negative-sample-free graph CL methods. \n\n2. The proposed augmentation-free mechanism is designed based on the graph homophily theory. According to the assumption and theoretical justification of the paper, the performance of the proposed method should be heavily dependent on the homophily ratio. However, the performance gain of the method seems to be even more significant on the heterophily graphs, which conflicts with the paper\u2019s assumption. It is very important to conduct more discussions and experiments to study this phenomenon because this is the most important claim of this paper. The authors are encouraged to provide more explanation on the phenomenon and testify to the correlation between its work mechanism and graph homophily. \n\n3. It is good to see the authors try to theoretically justify their claims in the paper, but the proof of theorem 1 is still confusing. For example, how do you derive Equation (14) from Equation 12 and how do you derive the relation between the homophily ratio and the node embeddings that minimize matrix factorization loss? The authors are encouraged to provide more detailed proof of their proposed theorem.\n \n4. The ablation study in the paper can be only considered as the parameter sensitivity analysis. A more comprehensive ablation study is necessary to evaluate the design of the paper, for example, what the performance and scalability of the proposed method will be with different positive and negative sampling strategies. Besides, the paper only compares the training time of the proposed methods and selected baselines, it would be better if the experiment about training space comparison is included.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper makes a good try to reduce the computation bottleneck of the current graph CL models. But the theory and claim in the paper are not well-supported.",
            "summary_of_the_review": "Overall, the idea is interesting and efficient, but the contribution of the idea over the prior negative -sample-free graph SSL method is limited. More importantly, the proposed claim is not well-supported, and the authors do not provide a very detailed analysis of the theorem in the paper.  Except for the performance comparison, the paper does not demonstrate enough experiments to support the claim and show the superiority of the proposed method over the baselines.  Additional theoretical analysis and experiments mentioned above are necessary to strengthen this work.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2283/Reviewer_BqrR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2283/Reviewer_BqrR"
        ]
    },
    {
        "id": "hU7N0nGgYv",
        "original": null,
        "number": 2,
        "cdate": 1666648943962,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648943962,
        "tmdate": 1666648943962,
        "tddate": null,
        "forum": "dSYkYNNZkV",
        "replyto": "dSYkYNNZkV",
        "invitation": "ICLR.cc/2023/Conference/Paper2283/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper \"Localized Graph Contrastive Learning\" suggests a contrastive learning method (Local-GCL) for graph data, based on the InfoNCE loss. In this setting, each graph node is associated with a feature vector, and the goal is to learn a meaningful representation of the graph nodes (representation quality can be measured using a linear classifier for known node labels). Local-GCL is based on two ideas: (1) use graph edges instead of data augmentations for positive pairs; (2) use random Fourier features to quickly compute negative part of the loss. The authors show that Local-GCL achieves SOTA performance on several benchmark datasets.",
            "strength_and_weaknesses": "Disclaimer: I am familiar with contrastive learning but am unfamiliar with the literature on graph representation learning / graph contrastive learning. So I cannot judge on the novelty compared to the existing literature. That said, the literature overview and the benchmarks shown in this paper look legit convincing, and the method does appear SOTA. Both novel ideas (for positive and negative pairs) are interesting. Therefore I believe this is a strong paper and a strong accept. \n\nI do not have any major issues. A number of things were not entirely clear to me, but it should be easy to edit them for clarity. \n\n* Section 2.3: it is not immediately clear how this section refers to the rest. I only understood it several pages later. Perhaps one introductory sentence could be added to explain the purpose of this section.\n\n* Equation 2 -- the InfoNCE loss usually has positive pair in the denominator as well, not only in the numerator (see e.g. SimCLR paper). Here you don't include the positive pair in the denominator. Why is that? Please comment.\n\n* Section 3: it was not entirely clear to me here what is the input to the network, and what are the i's in z_i. Later I understood that every node in the graph has an associated feature vector, and these features constitute the input neurons of the network. This is what you mean by \"node-level\", but I think it may be helpful to spell it out more clearly.\n\n* page 5: \"st\" -> \"set\"\n\n* Theorem 1 sounds like a really strong and important result, but inspection of Appendix A.1 suggests that it is a rather straightforward consequence of the results from two other papers. I think it would be prudent to cite both of them in the main text and to write something like \"The proof is a direct consequence of the results of \\citet{} and \\citet{}\".\n\n* Section 5.1: It would be great to have Table 4 in the main text but I understand that the space constraint may not allow it. But I would suggest to give some summary stats in the text of this section, perhaps mention that the number of nodes varied from 2k to 170k, etc. Maybe also give the range of the feature dimensionalities, etc.\n\n* Section 5.1: \"Implemention details\" paragraph could mention the network architecture. Was it a fully-connected network (MLP)? Something else?\n\n* Figure 2: I would suggest to connect all GRACE points with a solid green line.\n\n* Section 5.4: In SimCLR, the number of negative examples is determined by the mini-batch size. Is the same true in GRACE? \n\n* Related: I am actually not sure what if you used mini-batches for Local-GCL. What was the mini-batch size? Or did you use full-batch training instead of stochastic gradient descent? Your description on page 5 sounds as if you use *all* graph nodes as negative pairs, meaning that your M in Equation 2 is equal to the number of nodes. Is my understanding correct? If so, then does this mean that you did full-batch training? Otherwise how did you implement mini-batches?\n\n* Equation 8 holds for a non-graph contrastive learning models such as SimCLR. Does it mean that one could you user random Fourier features approach in SimCLR and related non-graph methods?\n\n* Section B.1 and also the main text: As I said, I am unfamiliar with the graph contrastive learning literature, but I am surprised to read that most existing methods usg some elaborate data augmentation schemes. Your approach (take graph edges as positive pairs) seems much simpler. Why has it not been adopted before?",
            "clarity,_quality,_novelty_and_reproducibility": "see above",
            "summary_of_the_review": "The paper suggests several novel ideas to graph contrastive learning and achieves SOTA results. Strong accept. However, disclaimer: I am unfamiliar with the literature on graph contrastive learning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2283/Reviewer_Nj2N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2283/Reviewer_Nj2N"
        ]
    },
    {
        "id": "5_lMBH0-OL",
        "original": null,
        "number": 3,
        "cdate": 1666670664423,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670664423,
        "tmdate": 1666670664423,
        "tddate": null,
        "forum": "dSYkYNNZkV",
        "replyto": "dSYkYNNZkV",
        "invitation": "ICLR.cc/2023/Conference/Paper2283/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a contrastive learning method for self-supervised node-level tasks in graph domain. It involves (1) sampling positive samples from the first order neighbourhoods and (2) kernelizing negative loss to reduce the training time and memory overheads.\nThe main contribution of this paper is presenting a simple yet efficient and intuitively reasonable idea regarding the use of first level neighbourhoods for selecting positive samples and achieving competitive performance with state-of-the-art methods. Some theoretical results are presented to support the adopted approach.\n",
            "strength_and_weaknesses": "Strengths:\nS1. The use of first order neighbourhoods for positive sampling is theoretically justified under the homophily assumption.\nS2. The proposed method is computationally efficient and experiments show that the approach can scale to larger graphs.\nS3. The paper is well organized and explains the approach very clearly.\n\nWeaknesses:\n\n1. The idea relies on homophily, however the performance improvement for heterophilic graphs (table 4) is more pronounced. It is not clear how this is possible and the paper provides no explanation apart from unsupported conjectures. The result tends to undermine the other experimental results for the homophilic, drawing into question whether the observed good performance is genuine and whether it arises for the reasons posed in the paper.\n\n2. Although the proposed approach is effective, the idea of using neighbours as positive samples in graph contrastive learning has been proposed before, albeit recently. The use of kernel approximations random Fourier features has been proposed in other (non-graph) self-supervised learning work. This diminishes the novelty and impact of the paper. \n\n\nQuestions and Comments\n\n1.\tNovelty: The idea of using the neighbour as a positive sample in contrastive learning has been proposed before (for example, in [R1, R2]). [R2] was only published in July, so may be consider concurrent work, but [R1] appeared in February. Although the definition of a \u201cneighbour\u201d in [R1] is more complicated (making it much less computationally efficient than the proposed technique), this does diminish the novelty of the proposed approach. \n\nIf it is accepted that graph contrastive learning using neighbours as positive samples is a known idea, then it\u2019s not clear to me that the paper makes a sufficient theoretical or experimental contribution. Each theorem, while interesting, is derived fairly straightforwardly from existing results. The experiments are conducted over multiple datasets and compare against numerous baselines, but the results for the graphs displaying heterophily are unexplained and a concern. The kernel approximations to make the computation efficient are a valuable contribution, although in the context of non-graph learning, the use of random Fourier features has been proposed in [R3]. The proposal to use structured orthogonal random features in this context is novel, to the best of my knowledge, and does result in a significant computational saving. \n\nCan the authors discuss the related work that I have identified, stressing the differences, and reinforcing the novelty of the contribution?\n\n2. Can the authors provide a better explanation for the performance in the heterophilic graph case?\n\n3. The paper does not report the results of any statistical significance tests. I would encourage the reporting of (bootstrapped) confidence intervals rather than 1 standard deviation. Single split results also give a false impression of the variability of the performance. Wilcoxon pair tests between the best and second-best techniques would provide more compelling evidence that there is a genuine performance difference.\n\n[R1] Zihan Lin, Changxin Tian, Yupeng Hou, Wayne Xin Zhao, \u201cImproving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning,\u201d in Proceedings of the ACM Web Conference (WWW), 2022.\n\n[R2] Sun, Z., Harit, A., Cristea, A. I., Yu, J., Shi, L., & Al Moubayed, N. \u201cContrastive Learning with Heterogeneous Graph Attention Networks on Short Text Classification,\u201d In Proc. International Joint Conference on Neural Networks (IJCNN), 2022.\n\n[R3] Li, Y., Pogodin, R., Sutherland, D. J., & Gretton, A. (2021). Self-supervised learning with kernel dependence maximization. Advances in Neural Information Processing Systems, 34, 15543-15556.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The method and background are explained well. The implementation details and experimental setting are also clear.\n\nQuality: The theoretical analysis is sound, and provides support for the adopted method. The experiments are convincing and appear to be well-executed, demonstrating the superior computational efficiency of the proposed method.\n\nNovelty: Although the idea is simple and efficient, overall the work is lacking novelty, as discussed above.\n\nReproducibility: The code and the shell file that can produce the results presented in paper are provided. Assuming these are made public, the reproducibility is sufficient.\n",
            "summary_of_the_review": "The paper proposes a simple technique for self-supervised learning for node classification. Theoretical results are provided to motivate the approach. The paper introduces a kernelization strategy to significantly reduce the computational burden. Although simple, the experiments demonstrate that the method can achieve SOTA performance. The main misgiving concerning the paper is the novelty and the impact, considering that a similar approach has already been proposed in other work. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2283/Reviewer_4oqi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2283/Reviewer_4oqi"
        ]
    },
    {
        "id": "2iA6xWkSjm",
        "original": null,
        "number": 4,
        "cdate": 1666716071798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666716071798,
        "tmdate": 1666716071798,
        "tddate": null,
        "forum": "dSYkYNNZkV",
        "replyto": "dSYkYNNZkV",
        "invitation": "ICLR.cc/2023/Conference/Paper2283/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces Local-GCL, a graph contrastive learning approach that tackles two of the most common issues of GCL models: the need for augmentation to generate positive pairs (which is non-trivial in the graph-learning setting) and the need for a high amount of negative pairs comparisons (whose quadratic complexity makes the problem intractable for large graphs). It does so by, respectively, creating positive pairs from first-order neighbors, and introducing an approximated contrastive loss computation that approximates the original loss with a much lower complexity (linear instead of quadratic). The authors provide theoretical justifications for their model and show empirically that it performs competitively to the state of the art while having a much smaller computational footprint.",
            "strength_and_weaknesses": "Strengths:\n- the method proposes a method to significantly reduce the computational complexity of the calculation of negative loss in GCL models\n- the experiments show results which are promising, especially when compared to other models' running times\n\nWeaknesses:\n- the paper is not always clear due to mistakes or ambiguous definitions\n- I have some doubts about the contribution and the soundness of the theoretical part of the paper (see below)\n- the experimental section still leaves some open questions (related to the contribution brought by the two different components and the performances on heterophilic graphs)",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper brings some interesting thoughts and should be quite reproducible, considering the code+instructions that will be provided in supplementary material.\n\nIn terms of clarity, there are a few issues due to mistakes in equations / definitions which I believe it would be important to address (in addition to typos, which only marginally reduce the understanding of the document):\n\n- Lemma 1 calls $\\Sigma$ the eigenvalues matrix $\\Lambda$.\n- Lemma 2 refers to $\\hat{y} = X \\rightarrow [r]$ while it should be $\\hat{y} = V \\rightarrow [r]$\n- Theorem 1: the dimensionality of $B^*$ is $k \\times c$, while it should be $d \\times c$\n- the RFF map is defined as being $\\phi : \\R^d \\rightarrow \\R^D$ but, while the $\\omega_i$ samples are just $D$, the dimensionality of $\\phi$ in Eq 1 seems $2D$ instead.\n- finally, I think using $\\phi$ to refer both to homophily ratio and to the RFF map might lead to some confusion\n\n\nSome additional issues relate to the soundness of the theoretical presentation. In my opinion, a few concepts require a better explaination that also takes into consideration assumptions and boundary conditions. For instance:\n\n- The simplification of Eq 9 that allows us to precalculate all the $\\phi(h_j)$ is possible because $\\phi(h_i)$ is fixed and can be taken out of the sum. This, I guess, is the reason why the negative loss is calculated on all nodes and not just the ones which do not belong to a given node's neighborhood. While the impact of this choice does not seem to be too important, I think it should be made explicit and thoroughly discussed (i.e. why it makes sense, when it might not work, how the error changes wrt the average degree vs graph size).\n\n- The proof of Theorem 1 relies on a set of assumptions which to me are not completely clear:\n\t- Lemma 1 builds above Theorem 5 from Balestriero and LeCun (2022) which specifically refers to the global minimiser of the SimCLR loss. How is Local-GCL's loss, which includes a negative loss component which is approximated, related to it? Is there a proof of its compatibility with the one of SimCLR?\n\n\t- Lemma 2 builds above Theorem B.3 from HaoChen et al. (2021) which explicitly introduces eigenvalues $\\lambda_1, \\dots, \\lambda_{k+1}$ as the smallest ones, while in the main body of the paper (just before Definition 1) they are defined as being in a descending order so the bigger $i$ is the smaller the eigenvalue (i.e. the first ones are the larger ones)\n\n\t- Correlation between dimensionality and performances is introduces without saying anything about boundary conditions. Nothing is said about what the maximum dimensionality is, and how it relates not just to the size of the graph but also its properties. For instance, if we have $k$ connected components, we cannot have $d > |V| - k$ because the smallest k eigenvalues would all be zero\n\n\nIn terms of novelty (and in the light of the previous issues) my impression is that, while the paper brings an interesting solution derived from previous works, in absence of a sound theory behind it the theoretical contribution would just look incremental.\n\nOn the experimental side:\n\n- it is not clear which of the two (orthogonal) model improvements contributes the most to  performances: intuitively, looking at GRACE's performances, I'd say it is the possibility of using all negative pairs in a more scalable way; getting positive pairs from the 1-hop neighbors, on the other hand, mainly allows us to get rid of augmentations. To verify this, I think it would be useful to have an ablation experiment showing how a model such as GRACE with approximate neg loss would perform. It looks like the main difference in GRACE is the presence of the intra-view negative pairs term, which I think could be computed as efficiently as the inter-view one. \n\n- In Table 1, the difference of the top two means in the Photo dataset is not really significant given the values of means and stds and the amount of experiments. While this does not make a difference in practice (I think the claim \"our approach is competitive with the SOA while being more scalable\" is correct), it would be fair to make both of the values bold as it cannot be statistically determined whether one is actually better than the other.\n\n- after the discussion on how positive pairs are generated, results on heterophilic graphs might come unexpected but, as the authors correctly say citing Ma et Al's work, this can happen e.g. due to the model learning to recognize different neigbor distributions. This claim, however, is unsupported by experiments which would allow us both to verify the reason of this behavior and learn (e.g. from counterexamples) when this model would work and when it would not.\n",
            "summary_of_the_review": "The paper aims at improving CGL by providing a model which is, at the same time, independent of graph augmentations and more scalable. Motivations are clear and relevant and the presentation of the related work is in my opinion sufficient to place this work in the proper context.\n\nWhile empirical results are in my opinion quite positive, the paper still leaves some open questions about the contributions brought individually by the two components implemented in the model. I also think some claims should be better supported, so that the theoretical contribution becomes more rigorous and less incremental.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2283/Reviewer_q5an"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2283/Reviewer_q5an"
        ]
    }
]