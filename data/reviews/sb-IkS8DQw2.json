[
    {
        "id": "7jfM7rwuMzr",
        "original": null,
        "number": 1,
        "cdate": 1666481324820,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666481324820,
        "tmdate": 1666481324820,
        "tddate": null,
        "forum": "sb-IkS8DQw2",
        "replyto": "sb-IkS8DQw2",
        "invitation": "ICLR.cc/2023/Conference/Paper2634/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a new way of doing Bayesian Meta Learning (BML) by fitting a full-covariance Gaussian Mixture Model (GMM) to approximate each Task Posterior (TP). TRNG-VI proposed by Arenz et al. (2022) was used to ensure efficient and robust optimization of the variational bound. The training complexity is similar to existing approaches, and does amortize inference over tasks and thus does not learn a set encoder architecture. Experiments on synthetic datasets showed the proposed GMM-NP method outperformed all baselines by a large margin over the whole range of context sizes, both in terms of LMLHD and MSE, in different test scenarios regarding task posterior inference, Bayesian optimization, dynamic modeling and image completion.",
            "strength_and_weaknesses": "1. What is the distribution of p(z), the prior used?\n\n2. Is there any convergence guarantee for Alg.1, if $\\phi$ is updated via TRNG and $\\theta$ via Adam?\n\n3. For Figure 1:\n- What is the definition of  z1 and z2 - amplitudes and phases?\n- \"Right panel: corresponding function samples from our model (blue lines)\" - is each line is a sample from q(z)?\n- The paper mentioned \"all samples pass through the red context example\". But from the plot it does not seem so. Is there any guarantee that each sampled functions will certainly pass through the points in the context data points?\n\n4. The paper mentioned \"sample a fixed set of auxiliary subtasks\" during training. What's the difference between tasks and sub-tasks?\n\n5. The paper mentioned the TRNG-VI require at most first-order gradients of $\\tilde{p}(z_l)$. Could you point out which variational update equation needs this gradient info?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: I can understand the overall ideas of the paper after some careful thinkings. It's a bit painfully structured, trying to explain both the idea itself and its difference with existing works. It could be beneficial to present the work itself first and then have a separate section to compare against existing works.\n\n**Quality**: The paper tests the performance against existing methods extensively, to justify the effectiveness of the different aspects of contributions made.\n\n**Novelty**: The application of TRNG variational inference method for GMM in BML is the main novelty point of this work. Although it does not sound super groundbreaking, this approach is very principled and led to much better performance than existing approach that relies on more tricks and/or specific model architectures to work.",
            "summary_of_the_review": "The GMM-NP approach proposed by this paper achieves much better results than existing approaches for BML, with less requirements in model architectures and objective approximations. The experiments illustrated the effect of each part of the design in a high quality. I would recommend an accept for it, to make people in the BML/NP area aware of the importance of accurate task posterior inference and work along this fundamental direction.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2634/Reviewer_7Kjw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2634/Reviewer_7Kjw"
        ]
    },
    {
        "id": "CUPGnYErqQ",
        "original": null,
        "number": 2,
        "cdate": 1666670775396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670775396,
        "tmdate": 1666670775396,
        "tddate": null,
        "forum": "sb-IkS8DQw2",
        "replyto": "sb-IkS8DQw2",
        "invitation": "ICLR.cc/2023/Conference/Paper2634/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for Bayesian meta-learning that aims to learn a conditional generative model f(x,z) such that variational inference on a latent z can model the posterior predictive distribution given context data from a new task. Specifically, the authors draw connections between their approach and the Neural Process (NP) model, and emphasize how performing non-amortized variational inference via trust-region natural gradient optimization of a GMM distribution on z can yield higher quality posterior predictive distributions than NP-style models, even those with more complex encoders.",
            "strength_and_weaknesses": "Strengths:\n- Overall, the paper was clear and easy to follow. The preliminaries introduced the intuition behind the core topics of the approach well. The description of VI was clear and made the exposition of the core contribution of this paper easy to understand.\n- The approach is simple yet effective, and serves to highlight the utility of more expressive task posteriors in a latent-space meta-learning context.\n- The experiments are thorough and demonstrate the capabilities of the architecture at representing an accurate task posterior both qualitatively, as well as quantitatively via the log-marginal likelihood metric on synthetic task distributions as well as the utility of accurate posterior modeling on downstream tasks such as bayesian optimization, dynamics modeling, and image completion. I also appreciated the ablations measuring ELBO looseness as a function of VI strategy and distribution expressivity to support the author's hypothesis that the quality of variational approximation correlated with downstream performance.\n\nWeaknesses:\n- As stated by the authors, a main limitation is the added computational complexity due to the non-amortized variational inference. While this leads to higher quality task posteriors, it brings   a natural tradeoff of prediction quality vs runtime. It would have been useful to see more details on the computational overhead in terms of wall clock speed -- how many iterations of VI are necessary for convergence on average? How does wall clock runtime compare to algorithms with more complex encoders, such as attention based NPs?\n- The method requires optimizing a separate task posterior not only for each task in the meta-dataset, but also for each context size size considered for each task, each requiring storing the set of GMM parameters. In contrast, an amortized inference approach would share parameters across tasks, but here the parameters would be the weights of an encoding network. How do the number of training parameters compare across methods?\nThe number of latent-dimensions differed between algorithms (and was selected in the hyperparameter optimization). However, this made it difficult to compare different approaches -- perhaps in a higher dimensional latent space, a unimodal distribution can encode similar information as in a multimodal distribution in a lower-dimensional latent space? The paper would be strengthened by either a comparison between methods with a fixed latent dimension, or a more in-depth discussion of any trends in the choice of latent-space dimension that emerge through the hyperparameter optimization.\n\nQuestions:\n- What data was used for hyperparameter optimization?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, easy to read, and the authors provide code for easy reproducibility. To my knowledge, the approach is novel and highlights the importance on the quality of variational approximation when working with variational inference models.",
            "summary_of_the_review": "Overall, I think this is a strong paper, presenting a simple, novel algorithm which achieves impressive results. The work is well presented, and the experimental evaluation is thorough, so I would recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2634/Reviewer_FbJt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2634/Reviewer_FbJt"
        ]
    },
    {
        "id": "fY84-ypA5Oa",
        "original": null,
        "number": 3,
        "cdate": 1666689968776,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689968776,
        "tmdate": 1666689968776,
        "tddate": null,
        "forum": "sb-IkS8DQw2",
        "replyto": "sb-IkS8DQw2",
        "invitation": "ICLR.cc/2023/Conference/Paper2634/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes GMM-NP, a novel BML algorithm inspired by the NP model architecture, which focuses on accurate task posterior inference. Despite its simplicity, GMM-NP outperforms the state-of-the-art on a range of experiments and demonstrates its applicability in practical settings",
            "strength_and_weaknesses": "Strengths: \n1. identifies the optimization suboptimal and solves by combining VI with natural gradients and trust regions\n2. strong experimental results which covers multiple settings and applications\nWeakness/Questions:\n1. As said in the paper, the additional overhead in the testing time can be parallelized. Can you give some empirical results to support it?\n2. How would your proposed method be used for Bayesian uncertainty learning models, especially those multi-modal, time-varying models for classification/regression?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity&quality: this paper is well-written \nnovelty: this paper solves a critical problem and has good novelty\nreproducibility: full experimental setting and release github code repo",
            "summary_of_the_review": "This paper is well-done for improving the optimization and inference of BNP. This paper can be improved by providing more empirical results and elaborations. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2634/Reviewer_sb3h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2634/Reviewer_sb3h"
        ]
    },
    {
        "id": "AMr7KArTor",
        "original": null,
        "number": 4,
        "cdate": 1666690200053,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690200053,
        "tmdate": 1666690404219,
        "tddate": null,
        "forum": "sb-IkS8DQw2",
        "replyto": "sb-IkS8DQw2",
        "invitation": "ICLR.cc/2023/Conference/Paper2634/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission modified the Neural Process model by replacing the Gaussian approximated posterior parameterized by encoder with a Gaussain mixture distribution parameterized by full-covariance matrix. For inference, the proposed method divide the VI into two alternating updating steps between variaitonal and model parameters. For variaitonal parameters, the method uses the existing trust region natural gradient based VI in [Arenz et al. 2022]; for model parameters, it uses the standard VI. The propsoed model and inference is validated on a range of experiments. \n\n",
            "strength_and_weaknesses": "Strength: The paper is generally easy to follow, and the derivation seems correct to me although I did not check it carefully. Replacing the Gaussian approximated posterior parameterized by an encoder with a Gaussain mixture distribution is an interesting idea. \n\nWeakness: The proposed inference method is inherited from the TRNG-VI in [Arenz et al. 2022] and the contribution is incremental. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The submission is easy to follow.\n\nQuality: All equations and derivations seems correct although I did not check it carefully. \n\nNovelty: The contribution is incremental. \n\nReproducibility: I did not check it. ",
            "summary_of_the_review": "From the technical perspective, the methodology in the submission is correct in my eyes (I did not check it carefully) and the experiments are rich and solid. Generally speaking, the submission is a good paper without obvious defects. However, from the model inference perspective, this work simply applied the method in \"A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models\" [Arenz et al. 2022] to modify the Neural process model; all contributions mentioned in this work, \"full covariance matrix\", \"Gaussian mixture distribution\", \"non-amortized\", \"trust region natural gradient descent\", are inherited from this previous work. Therefore, the incremental contribution dampens my enthusiasm to give a very high score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2634/Reviewer_tZEh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2634/Reviewer_tZEh"
        ]
    }
]