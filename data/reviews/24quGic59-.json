[
    {
        "id": "BSUVSUwM3u",
        "original": null,
        "number": 1,
        "cdate": 1666591311436,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591311436,
        "tmdate": 1666591311436,
        "tddate": null,
        "forum": "24quGic59-",
        "replyto": "24quGic59-",
        "invitation": "ICLR.cc/2023/Conference/Paper1872/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new method for improving the robustness of learning neural networks with respect to group correlations using the Gram matrix that is extracted from the output of different layers in a learned neural network.",
            "strength_and_weaknesses": "Strength:\n- This is an important problem.\n- The proposed method seems to be mostly competitive with the other baselines\n- Their proposed method does not need group labels for the validation set (although the paper does not provide any clear answer as to why this is the case)\nWeaknesses:\n- The proposed method is computationally much more expensive than the baselines. I strongly recommend the authors to give a more comprehensive comparison between their methods and the baselines with respect to the computation burden.\n- The fact that we know unsupervised clustering and two step training procedures with GDRO could result in better worst case performance has already been observed. As a result the overall results of the paper are really incremental. Unfortunately, the paper does not shed much light on why their method is superior or what are the limitations of their approaches? For example, at one point the authors mention that gram matrices seem to encode \"textures and color palette\" rather than \u201cstructure\u201d in the image. So, does this mean that the proposed method works for cases where group labels or spurious correlations relate to these? I think these are more important questions to answer rather than just showing that the method works better than very similar previously proposed methods on some datasets. \n- The proposed method, and specifically the hp tuning for it (for the first and second step) are not very well elaborated. This makes it very difficult to judge if their claims about fair comparisons are correct.\n- Why is it that the method does not require labeled validation? Could it be true that other methods could also be applied without labeled validation? If not, what is so different about the approach that enables that? In fact, on all datasets GRAMCLUST-cv is better than GRAMCLUST-orig which is a bit counter-intuitive. I suspect this might be due to some counter-intuitive hp tuning. Note that the explanations around the mismatch in the labels might be true for water-birds, but is not very applicable to CelebA or Coco dataset. \n- What is so special about VGG-19 and why should it be sued for Gram matrix? Would the result still hold if one uses other network architectures, e.g. resnet? other pre-trainings? This is so mysterious that all results are only presented with VGG-19 without any intuition/understanding.\n- The JTT numbers are much lower than what they should be (reported in other papers). I suspect it is due to footnote 3. But I am not sure why footnote 3 is applicable and fair. With those numbers I do not believe that gram matrix approaches can show much gain over the baselines. I also noted that the gram matrix performances have relatively larger std compared to the baselines, which might mean that the methods are not very stable and their performance depends on the random seeds.\n- Compared to meanvar, the proposed gram matrix solution is very computationally intensive with not significant gains over waterbirds and coco on p. \n- The definition of matching accuracy is not provided (at least for the sake of completeness). Also, could you provide more details on the correlation between matching accuracy and the final performance? \n- For figure 4 only the results for water-birds are provided while in that case the ground truth E=2 (which is the smallest possible number fo clusters). Could you please provide evidence on cases where the ground truth E is different than the minimum, e.g. Coco on P data, to better illustrate the trade-off?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: could be greatly improved (see above)\nNovelty: not novel\nReproducibility: The authors have provided the code but I did not go through the code. Based on the paper, there are questions around the baselines and HP tuning.",
            "summary_of_the_review": "The paper's contributions are marginal without adding much to the fundamental understanding of why the method works better than other methods (this part is questionable based on the lower than expected numbers on JTT), why it does not need validation group labels and what are the limitations of it (e.g. are there any spurious correlations that this method cannot capture)?",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1872/Reviewer_Nwqa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1872/Reviewer_Nwqa"
        ]
    },
    {
        "id": "u2SGrFAQxM6",
        "original": null,
        "number": 2,
        "cdate": 1666720621442,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666720621442,
        "tmdate": 1666720621442,
        "tddate": null,
        "forum": "24quGic59-",
        "replyto": "24quGic59-",
        "invitation": "ICLR.cc/2023/Conference/Paper1872/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "One of the challenges in training over-parameterized neural networks is that they may overfit to \"spurious features\", resulting in poor generalization performance on minority subgroups. A popular approach to alleviate such biases is the use of group distributionally robust optimization (DRO). However, prior DRO approaches required group labels to be available either in the training or validation set. This paper proposes a method to automatically discover the underlying subgroups, targeting practical use cases where the underlying spurious correlations are unknown. \n\nThe proposed method first trains an embedding model, and clusters the data points into groups based by computing (dot product) similarity between embeddings. Group DRO is then applied treating the identified clusters as subgroups. The proposed method uses no group label information, yet is competitive with methods that use group labelings in a validation set.\n\n",
            "strength_and_weaknesses": "Pros:\n- The idea of clustering to identify group information is a natural one to try out.\n- The paper shows that one can get competitive performance on common benchmarks even with no knowledge of the underlying groups, even against baselines that do make use of group label info in the validation set. This I think is a significant empirical contribution.\n\nCons:\n- Unfortunately, the idea lacks sufficient formalism. While it is intuitive that clustering the data based on learned embeddings could help discover subgroups, I think the paper would be more compelling if the authors are able to provide formal framework for characterizing spurious correlations and identifying scenarios where applying a simple k-means clustering to the training data would uncover the right subgroups.\n- Determining the right number of clusters remains an important challenge. The authors show robustness to the number of clusters on one of the datasets (do these trends hold for the others too?), but in general, I worry that mis-specifying the number of groups could adversely impact the subsequent DRO step. Since DRO optimizes for worst-case accuracy, doing so with a larger number of groups than needed may adversely impact average accuracy. Could you show a similar plot as Fig 4 for average accuracy too?\n\nOther questions:\n- In the experiments, some times GramClust-cv (no group labels) outperforms GramClust-orig (groups labels in val set). This happens not only in Waterbirds (where the authors point to possible noise in group annotations), but also in CelebA. Is there a reason you've identified for this?\n-  In your implementation of Group DRO from Sagawa et al., did you implement the per-group regularized updates in eq (5) in their paper (which I think is what they use in their experiments), or the online version in Algorithm 1 in their paper?\n\nAdditional related literature: Kirichenko et al. Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. ArXiv:2204.02937.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the high level idea is intuitive, but limited on novelty. The experimental results are strong and make for a good contribution. The paper is written well.",
            "summary_of_the_review": "Overall, intuitive idea, strong experiments, but limited novelty. The paper would benefit from some minimal theoretical justification.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1872/Reviewer_4Q7E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1872/Reviewer_4Q7E"
        ]
    },
    {
        "id": "QVDuQfx_y8w",
        "original": null,
        "number": 3,
        "cdate": 1666751341909,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666751341909,
        "tmdate": 1666751341909,
        "tddate": null,
        "forum": "24quGic59-",
        "replyto": "24quGic59-",
        "invitation": "ICLR.cc/2023/Conference/Paper1872/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Recently, it has been observed that although neural networks achieve very good accuracy on average over the dataset but they can have very low accuracy on certain subgroups of the distribution. There have been many works on trying to fix this gap. This paper proposes an algorithm to improve the accuracy on different subgroups of the distribution and show improved performance on many benchmark datasets as compared to many existing baseline methods. The benefit to their approach is they do not require information about the group labels and learn them using unsupervised clustering. The unsupervised clustering has been used previously but they propose to use clustering over the Gram matrices of the activations of the intermediate layers which are shown to capture the texture and style patterns of images and hence can capture the groups divisions well. They use these clusters to perform group distributionally robust optimization in the second stage. Moreover, they propose to use the learned clusters of the validation set for tuning the hyper parameters as well and hence, do not even require knowledge of these subgroups over the validation set.",
            "strength_and_weaknesses": "The idea of using Gram matrices which are supposed to capture the style and texture features of the images and hence, can capture the spurious features is a simple and interesting idea. \n\nThey propose to use the groups learned for even the validation set but I wanted to check with the authors that even with other previous methods, we can do the same thing? \n\nAlso, is there any guarantee that we will indeed learn the natural groups of the data. Are there any examples of the dataset where the groups contain divisions based on non-style and texture based features and hence, won\u2019t be captured well by the Gram matrices based clustering?  \n\nThe authors also compare their method to using last layer features for clustering. In their identifier model, they have also trained the model for only a few iterations so that the model only capture easy features. Did the authors use the same early stopped model for the last layer features while doing this experiment? It would be good to separate out the gains coming from early stopping vs. using Gram matrices for clustering. \n\nIt is quite interesting to see that increasing the number of clusters does not hurt their performance. I would imagine that increasing the number of clusters can maybe form some clusters which only contain hard data points and having low loss on them can hurt the overall performance. Do the authors have some intuition on what kind of clusters does this method find when the set the number of clusters to be large and why it does not hurt the performance?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well written and clear. The idea of using Gram matrices to do the clustering in the first state is novel and also leads to improved performance.",
            "summary_of_the_review": "I like the overall idea of the paper. This work also improves performance on many benchmark datasets. The only concern is whether the gain is coming from using Gram matrices or the early stopped model. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1872/Reviewer_9Mcs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1872/Reviewer_9Mcs"
        ]
    },
    {
        "id": "PajzuWDL4K",
        "original": null,
        "number": 4,
        "cdate": 1666879341359,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666879341359,
        "tmdate": 1666879341359,
        "tddate": null,
        "forum": "24quGic59-",
        "replyto": "24quGic59-",
        "invitation": "ICLR.cc/2023/Conference/Paper1872/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the author combines the existing gram matrices into the group robustness classification. The whole pipeline can be summarized into two steps: the author first adopts the clustering techniques to cluster the dataset samples into several groups with the gram matrices features; then,  cluster groups with the pseudo-group labels are integrated into the existing group robustness optimization framework.\nThe proposed method is validated on three datasets. The proposed method only can achieve higher test accuracy on worst-group (w-g) but not achieve the best performance on average (avg) test accuracy.",
            "strength_and_weaknesses": "Strength:\nThe proposed method can achieve best worst-group (w-g) test accuracy.\n\n\nWeaknesses:\nThe novelty is weak. The proposed method is just a combination of gram matrices and group robustness optimization essentially.\nThe proposed method can't achieve the best performance on average accuracy.\nThe proposed method is not validated on the large-scale dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good. Some examples are adopted to clarify the problem clearly.\n\nThe quality needs to be better. Some descriptions are not rigorous. \nFor example, the author mentioned that ``This results show empirically that our proposed approach, using Gram matrices of feature to discover pseudo-groups, which are then used for robust optimization and hyperparameter cross-validation, is very effective for group robustness. It also supports that Gram matrices are well suited to capture various types of dataset biases (background for Waterbirds, physical attribute in CelebA, multiple backgrounds in COCO-on-Places-224). \" \nThe Gram matrices are supposed to extract texture features. The higher accuracy can't prove the Gram matrices are well suited to capture various types of dataset biases (background for Waterbirds, physical attribute in CelebA, multiple backgrounds in COCO-on-Places-224). The connection between the function of Gram matrices between the background, physical attribute, and other biases should be verified with other experiments.\n\nThe novelty is weak.\nThe reproducibility is good. The author provides a detailed experiment setting in the manuscript.\n",
            "summary_of_the_review": "\nOverall, novelty is weak. The experiment results only can support part of the claims or the proposed method.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1872/Reviewer_nkU5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1872/Reviewer_nkU5"
        ]
    }
]