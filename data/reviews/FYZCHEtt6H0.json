[
    {
        "id": "v2sGBMAMmy",
        "original": null,
        "number": 1,
        "cdate": 1666684144119,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684144119,
        "tmdate": 1666684170352,
        "tddate": null,
        "forum": "FYZCHEtt6H0",
        "replyto": "FYZCHEtt6H0",
        "invitation": "ICLR.cc/2023/Conference/Paper931/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Combinations of evolutionary algorithm (EA) and deep reinforcement learning (DRL) have been investigated in the literature to take advantage of EA (exploration ability, robustness and stability) while maintaining the advantage of DRL (sample efficiency). However, in the previous work on the combination of EA and DRL, we observe that the performance of the combination is sometimes dominated by EA-only or DRL-only approaches, for example on Swimmer and Humanoid on MuJoCo environments.\n\nThis paper proposed a novel combination, namely Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re2). In the proposed approach, the policies of EA and RL agents are linear policies on a non-linear state-representation that is shared over all EA and RL agents. The state-representation is trained using a replay buffer storing all the experiences generated by EA and RL. Linear policies are trained independently by EA and RL. To improve the sample efficiency of EA part, the authors introduced a shared critic network taking the policy parameter as its input and use it as a surrogate function of the cumulative reward, leading to reduction of the number of interaction for policy evaluation in EA. Moreover, to make the crossover and mutation in EA part more sense, the authors introduced novel operators, named behavior-level mutation and crossover. \n\nNumerical experiments on MuJoCo environments show its efficiency as well as superior final performance over existing ERL variants and RL baselines and EA. ",
            "strength_and_weaknesses": "# Strength\n\n* Strong performance has been observed on MuJoCo environments in comparison with PPO, SAC, DDPG, TD3 (RL baselines), ERL, CERL, PDERL, CEM-RL (ERL baselines).\n* Overall, the paper is well-written. The algorithm and experimental details are provided sufficiently.\n\n# Weaknesses\n\n* From the description of the algorithm written in the main text, the introduction of the shared state representation appears to be aimed at improving search efficiency. However in the experiments, the final performance was improved on some environments. Is this because the training was not finished? A possible criticism is that as the training is not finished, the final performance of the proposed approach might be lower than the other approaches. A careful discussion is required.\n\n# Comment / Question\n\n* The proposed behavior-level operators seem to make sense. However, similar effect (considering the correlation between parameters) could be realized by just using CMA-ES or CEM. Why do you want to employ a GA-like approaches?",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned in the above section, this paper is generally clearly written. The approach is novel. The algorithmic and experimental details are provided.",
            "summary_of_the_review": "This paper improved the performance of ERL significantly as summarized in the strength and weaknesses section. Each algorithmic component have been carefully evaluated in experiment (in appendix). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper931/Reviewer_ouqq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper931/Reviewer_ouqq"
        ]
    },
    {
        "id": "wfxCAxftWSr",
        "original": null,
        "number": 2,
        "cdate": 1666795124347,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666795124347,
        "tmdate": 1666795124347,
        "tddate": null,
        "forum": "FYZCHEtt6H0",
        "replyto": "FYZCHEtt6H0",
        "invitation": "ICLR.cc/2023/Conference/Paper931/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper points out the problems of the previous ERL frameworks, i.e., the parameters of different policies are independent and the EA operators often fail. The paper then proposes a new ERL framework that splits the policy network into two parts, i.e., the shared nonlinear state representation and the independent linear policy representation, to overcome the former problem, and proposes the behavior-level EA operators to overcome the latter problem. Experimental results show that the proposed method performs better than other ERL, EA, and RL methods in all tasks.",
            "strength_and_weaknesses": "- It is a good and reasonable idea to split the policy into the shared state representation and the independent policy representation, which may be of independent interest.\n\n- The method performs significantly better than other methods in the experiments, and the ablation studies are extensive.\n\n- The paper is well-written and easy to follow.\n\n- The method may be further improved by taking full advantage of EAs, e.g., keeping the diversity of population.\n\nMinor problems: There are some confusing symbols in the paper, e.g., the symbols of the sets in Section 6.1.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The idea is novel. I believe that the experiments can be reproduced according to the details in the paper and appendix.",
            "summary_of_the_review": "This paper proposes a new ERL framework that splits the policy into two parts, and proposes the behavior-level EA operators. The idea is novel and reasonable. The paper conducts extensive experiments and ablation studies to show the performance of the framework and the effects of each part of the framework. Despite some limitations, it is overall a good work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper931/Reviewer_oDSW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper931/Reviewer_oDSW"
        ]
    },
    {
        "id": "w5lOBGuPx1",
        "original": null,
        "number": 3,
        "cdate": 1667470551760,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667470551760,
        "tmdate": 1667470551760,
        "tddate": null,
        "forum": "FYZCHEtt6H0",
        "replyto": "FYZCHEtt6H0",
        "invitation": "ICLR.cc/2023/Conference/Paper931/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a strategy for combining aspects of reinforcement learning (RL) and evolutionary algorithms (EA). A shared representation is learned for all actors (EA policies and an RL policy), with each EA actor maintaining only its own linear head. The EA policies are recombined using proposed behavior-level genetic operators that act on rows of the linear head, while the RL policy is updated by maximizing the Q function as usual. The authors show that EA agent performance can be estimated without full rollouts by using the learned value function. The proposed algorithm, dubbed ERL-Re$^2$, displays strong performance on various control tasks.",
            "strength_and_weaknesses": "Strengths\n* The core idea of utilizing a shared representation across agents and only applying EA to the last layer makes intuitive sense.\n* On all the tasks used in evaluation, ERL-Re$^2$ outperforms existing RL and EA algorithms.\n* The appendix includes extensive ablations investigating the effects of various hyperparameters and design choices. I appreciate that the authors have reported negative results for different ideas they tried; this may save a grad student some time later.\n\nWeaknesses\n* Since the algorithm combines elements of RL and EA, it has many hyperparameters to tune.\n* The performance gains resulting from the proposed modifications (behavior-level genetic operators and fitness evaluation metric) are not very large.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear. The writing could be improved in places, but it does not cause confusion. \n\nI do not feel that I am familiar enough with the EA literature to evaluate the novelty of the proposed behavior-level crossover and mutation.\n\nRegarding the fitness evaluation metric: I think \u201cnovel\u201d is a strong word, seeing as the $H$-step return estimate has been used in other works such as A3C and Rainbow. Also, it is not accurate to call it $H$-step \u201cTD\u201d as there is no temporal difference involved.\n\nThe paper includes hyperparameter settings and references to which specific codebases were used. Therefore, I\u2019m inclined to believe it can be reproduced.",
            "summary_of_the_review": "Overall, I think the paper is a useful contribution owing to its strong empirical results. The RL community would benefit from seeing that ideas from EA can be effectively combined with RL to improve performance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper931/Reviewer_vR7r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper931/Reviewer_vR7r"
        ]
    },
    {
        "id": "dQRIdxxOI7",
        "original": null,
        "number": 4,
        "cdate": 1667505579265,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667505579265,
        "tmdate": 1668867140035,
        "tddate": null,
        "forum": "FYZCHEtt6H0",
        "replyto": "FYZCHEtt6H0",
        "invitation": "ICLR.cc/2023/Conference/Paper931/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Following on from the direction proposed by ERL at NeurIPS 2018, this work seeks to combine evolution and off policy RL into a hybrid algorithm, achieving the best of both. This direction remains promising albeit a little saturated after several years, with many variants proposed. The novel component of this work is to maintain two separate policies which share a representation space, and evolve the evolution policy in the behavior space. Empirical results show gains in the MuJoCo suite for both TD3 and DDPG.",
            "strength_and_weaknesses": "### Strengths\n\n* The general direction of combining evolution and RL seems promising, and maybe under explored despite many similar works in the past few years. \n* This paper proposes a slightly different approach, keeping track of two separate policies, and some tricks to share data that may be useful for others.\n* The empirical gains are strong in MuJoCo and ablations seem convincing that the method is not overly sensitive.\n* It is good to see a limitations section even though it is 80% just discussing future work.\n\n### Weaknesses\n\n* This would have been a good paper in 2019, but right now the field has moved on so far from just showing gains on MuJoCo to provide a useful contribution. Since 2020 it has become common practice to use pixel based tasks, which is no longer expensive to run. Further, there has been a shift in focus towards generalization of agents (see Kirk et al 2021 for a survey), and tasks requiring more complex behaviors. Why does this matter? It is very hard to know if the insights in this work will translate to settings at the forefront of research, and thus the impact seems limited.\n* For a pure methods paper, which is solely based on experimental results, I would expect to see at least two distinct domains. Having both TD3 and DDPG doesn't seem additive since they are essentially the same algorithm with a few tricks. This seems like a bit of a duplication which superficially makes the method look more general.\n* The method seems to rely heavily on a linear behavior representation. How does this work if we move to a higher dimensional setting? We already know from Mania 2018 that a linear policy can solve MuJoCo, but how about something higher dimensional?\n* There is no intuitive analysis as to why this method outperforms where it does. Why is it so good on Swimmer? More broadly, what can we take away from this other than knowing it is great on MuJoCo from proprioceptive states with a 1M timestep budget?\n\nMinor Issues:\n* There are a few places where acronyms should be in the parentheses with the reference, e.g. (RL, Sutton 1998). The paper has it like (RL) (Sutton, 1998).\n* There are some grammatical mistakes for example \"Evolutionary Algorithm\" should be plural.\n* In the DDPG figure the Swimmer plot has the wrong title.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea has some novel components as far as I am aware.",
            "summary_of_the_review": "This paper presents a method for general RL tasks. Since the experiments are toy MuJoCo domains, with no analysis over how or why it has gains there, it is hard to know if it is useful for problems we actually care about in the RL community. I thus vote to reject the paper based on the breadth and depth of the experimental results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper931/Reviewer_eTSz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper931/Reviewer_eTSz"
        ]
    }
]