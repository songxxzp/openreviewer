[
    {
        "id": "-FUzlPU2l9x",
        "original": null,
        "number": 1,
        "cdate": 1666624924187,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624924187,
        "tmdate": 1666624924187,
        "tddate": null,
        "forum": "icmTV7mhxuQ",
        "replyto": "icmTV7mhxuQ",
        "invitation": "ICLR.cc/2023/Conference/Paper1349/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method for improving generalization in multi-agent games, where game manuals describing entity and environment dynamics are provided, and high-level goals are specified in natural language. The proposed method learns a policy for each entity, trained with reinforcement learning, with an auxiliary supervised loss on predicting the dynamics of other agents in the environment. They modify two existing similar environments to be multi-agent (MESSENGER and RTFM) and evaluate with several comparisons to other methods and ablations.",
            "strength_and_weaknesses": "Strengths:\n- The use of \"opponent\" modeling as an auxiliary loss is intriguing (although I don't know how conventional it is in multi-agent RL already).\n- Description of POSG is clear (Section 3)\n- Performance of proposed method significantly outperforms existing methods\n- Experiments are relatively thorough\n\nWeaknesses:\n- See below for clarity concerns.\n- I'd like more discussion on what is functionally different between the two environments that make both of them good to study.\n- I found it a bit confusing that EnDi enabling agents to individually model subgoals also helps the agents coordinate. By modeling subgoal division, are the agents explicitly predicting how all of the other agents will act next? Does this mean it's possible for two collaborative agents to fail to complete a subgoal under the \"assumption\" the other will complete it, e.g., if they are both equally far away from the entity relevant to that subgoal?\n\nSome questions:\n- The opponent modeling head actually predicts multiple actions, corresponding to all of the other agents, right?\n- In Eq. 1, does each agent have access to the observations of the other agents, or is it predicting others' actions based on its own partial observation of the world?\n- How were the stages for the curriculum learning chosen?\n- Supervised loss ablation (EnDi-sup) is equivalent to ablating the opponent modeling, right?\n- Why evaluate QMIX, rather than ablating language as much as possible in EnDi? E.g., by removing access to the manual representation z?",
            "clarity,_quality,_novelty_and_reproducibility": "Quality:\n\nExperiments are mostly thorough. I would have liked to see a bit of error analysis to see what kind of mistakes are still being made in this environment.\n\nClarity:\n\nI struggled to understand the task setting in a formal way. \n- Throughout the paper there was vague wording like \"entity level\" that made it difficult to understand exactly what was going on and what the challenges were. \n- I'd suggest formalizing the task setting in a similar way to how POSG was formalized; I found that part easy to read\n- I would suggest adding something formal about the game's incentives (it seemed like these games were both collaborative and competitive at the same time, maybe because you're learning a policy that's part of a team that's acting against another team?)\n- The representation of subgoals and subgoal division was also unclear; formalization would help here.\n- The word \"opponent\" modeling is confusing if some of the other entities are collaborating with one another, and aren't opponents.\n\nOriginality: I'm not very familiar with multi-agent RL work, so I am not sure how novel it is from that side of things. From language-based RL, the setting appears to be rather conventional, except for the multi-agent setting (which is the novelty of this paper).",
            "summary_of_the_review": "This paper was interesting, and I think the contribution is potentially useful, but I did find the description of the task itself to be relatively informal to the point of being somewhat difficult to understand the contributions. I am also not deeply familiar with multi-agent work, so cannot evaluate it thoroughly from that point of view.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1349/Reviewer_imbM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1349/Reviewer_imbM"
        ]
    },
    {
        "id": "lKRpZVbLFUV",
        "original": null,
        "number": 2,
        "cdate": 1666658058841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658058841,
        "tmdate": 1666658058841,
        "tddate": null,
        "forum": "icmTV7mhxuQ",
        "replyto": "icmTV7mhxuQ",
        "invitation": "ICLR.cc/2023/Conference/Paper1349/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies using natural language as instructions and manuals for multi-agent reinforcement learning (MARL). The problem is challenging because the agents need to ground the language and consider other agents\u2019 behavior at the same time. This paper proposed a language-grounding framework, which is referred to as \u2018entity divider (EnDi)\u2019, for MARL. Based on opponent modeling, the proposed approach enables agents to divide subgoals at entity level and act with the environment accordingly. The proposed EnDi is evaluated on multi-agent RTFM and multi-agent MESSENGER which are augmented from the popular RTFM and MESSENGER environment. The experimental results show that the proposed approach outperforms baselines and is able to generalize to unseen tasks. \n",
            "strength_and_weaknesses": "### Strength:  \n1. Language grounding for multi-agent settings is an important area which draws less attention. The reviewer thinks this paper took a first step toward this goal. \n\n2. The reviewer found the proposed sub-goal division, which based on opponent modeling, simple yet interesting. The experimental results also demonstrate its effectiveness on two MARL environments. \n\n3. The paper is well-written and easy to follow.  \n\n### Weakness / Questions:\n\n1. To get $X^{others}_{goal}$, joint position feature of all other agents ($l_{-i}$) is needed.  Isn\u2019t it a privileged knowledge that violates the partially observable assumption discussed in Section3 and the \u2018decentralized execution\u2019 of general multi-agent settings?\n\n2. Are the binary entity masks on the full grid? Or is it only on each agent\u2019s visual range (local observation)? In addition, what is the visual range of each agent in the experiments?\n\n3. It seems that the opponent modeling (Eq. 1) is a critical component for avoiding sub-goal conflict. However, opponent modeling itself is a challenging problem. It is unclear how the proposed method ensures the modeling results are good and thus lead to good subgoal division? In addition, Suppose agent A never occurs in another agent\u2019s, e.g. agent B, visual range. How could agent B model agent A\u2019s behavior correctly?  If the agent is not able to model others\u2019 behaviors accurately, how would it impact the quality of the binary entity mask?\n\n4. It is unclear how many agents each task. It seems that there are only two controlled agents in a task. The experimental section could be more convincing if there are more controlled agents in the tasks. \n\n5. In p. 8, the paper explains Fig. 3: \u201c it chooses to go down to the target monster, leaving the upper target monster to Agent1\u2026\u201d. Given the upper monster is left for agent 1, why is the upper monster assigned to the mask of agent 2?\n\n6. Is the proposed sub-goal approach applicable to environments that are not grid-world based?\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity:\nWhile some clarification is needed, the paper is generally well-written and easy to follow \n\n### Quality:\nThe claim is mostly supported by the experiments. However, more comprehensive experiments may be needed. Please see the weakness / questions section for details. \n\n### Novelty:\nThe paper formulated and addressed the important language grounding problem for MARL. The reviewer found the setting and subgoal division method interesting. However, the reviewer has some concern regarding the generality of the proposed approach.  Please see the weakness / questions section for details. \n\n### Reproducibility:\nThe description and the hyper-parameter aew clear. However, no code for the approach and the environment is provided. Without the code of the environment, it would be hard to reproduce the results.  \n",
            "summary_of_the_review": "In summary, the reviewer found the problem and the proposed approach interesting. However,  it is unclear if the proposed approach is applicable to more complex environments, e.g. more agents, non-grid-world tasks, and limited visual range. Further clarification and revision are needed. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1349/Reviewer_affK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1349/Reviewer_affK"
        ]
    },
    {
        "id": "DRyL2EozBf",
        "original": null,
        "number": 3,
        "cdate": 1666680674522,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680674522,
        "tmdate": 1666680674522,
        "tddate": null,
        "forum": "icmTV7mhxuQ",
        "replyto": "icmTV7mhxuQ",
        "invitation": "ICLR.cc/2023/Conference/Paper1349/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors address the challenges of grounding language in generalization for unseen dynamics in multi-agent settings. They propose a new framework for language grounding called entity divider (EnDi).\nEnDi enables agents to independently learn subgoal division at the entity level and act in the environment. This division is regularized by the opponent\nmodeling to avoid subgoal conflicts and promote coordinated strategies. \nThe authors also conducted experiments to show that EnDi outperformed existing language-based methods in all tasks by a large margin.",
            "strength_and_weaknesses": "Strength\nThe experiment shows that their proposal is working better than the baselines.\n\nWeakness\nThe generality of the proposed model in broader MARL studies is not apparent.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the abstract and the main body has room for improvement.\nThe quality of the paper looks good. Experiment settings are described clearly to a certain extent.",
            "summary_of_the_review": "In summary, this is an interesting paper that provides a new method. However, the description and style may have room for improvement. It is better to make the abstract and conclusion more detailed and explicit.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1349/Reviewer_dLxy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1349/Reviewer_dLxy"
        ]
    },
    {
        "id": "Ew-1KAnZ4d",
        "original": null,
        "number": 4,
        "cdate": 1667251298011,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667251298011,
        "tmdate": 1667349259068,
        "tddate": null,
        "forum": "icmTV7mhxuQ",
        "replyto": "icmTV7mhxuQ",
        "invitation": "ICLR.cc/2023/Conference/Paper1349/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses the problem of collaborative MARL with language for specifying instructions and environmental dynamics, focusing on the problem of distributing subgoals among multiple agents. The approach proposed in the paper builds upon the grounding architecture from Zhong et al, 2019 and Hanjie et al. 2021, with two main architectural changes meant to enable individual agents to focus on different subgoals: (a) complementary masking over observations, leading to different agents attending to different entities in the observation; and (b) modeling of other co-operating agents as an auxiliary objective. \n\nIn the experiments on multi-agent versions of Messenger (Hanjie et al, 2021) and RTFM (Zhong et al, 2019), the proposed architecture outperforms independently trained agents (without the modifications to the architecture), also demonstrating generalization to completely new monsters / modifiers and increased grid size. ",
            "strength_and_weaknesses": "I appreciated that the setting addressed in this paper is novel, in general, language-assisted RL is not well-studied and it's good to see more work in this area. The results are certainly promising, the proposed method strongly outperforms independently trained agents, with particularly interesting results in generalization to unseen entities / modifiers and grid-sizes. The proposed architecture overall makes sense. \n\nHowever, this work is not ready for publication due to a lack of comparison to standard MARL baselines, methodological issues, and a lack of clarity about the method (see Clarity section).  \n\nThe authors do not specify well which grounding-related problems are introduced by the addition of multi-agent dynamics to language-assisted RL beyond standard problems of MARL (i.e. problems related to incorporating language or attending to manual; instead of just increased non-stationarity and learning coordination strategies). The authors touch on this question in p2 of the Introduction, however, this needs to be elaborated on better. As it stands, the proposed method could be used in most MARL benchmarks and standard MARL methods (such as QMIX [1], MAPPO [2]) could be combined with architectures more suitable for grounding such as EMMA and text2pi when the task demands language grounding. So the authors could also examine the proposed architecture in standard non-language MARL tasks; but in either case, the baselines need to include standard MARL methods. While one of the experiments does contain a comparison to QMIX (Section 5.3), this is only in a language-free setting (i.e. using QMIX with a standard architecture that is not suitable for language grounding). If there is a reason why standard MARL methods can not be combined with grounding architectures, the authors need to elaborate on it in the paper. \n \nIf I'm interpreting this correctly, the results are reported over environment random seeds, but not training seeds. In section B.2 of the Appendix, the authors say that the validation games are used to select the highest-performing parameters instead of hyperparameters, with the reported results corresponding to this best-performing model on the random selections of test environments. This is very non-standard in RL for a variety of reasons [3], including not giving a sense of how stable the training is. The authors also do not seem to perform any hyperparameter search over either the proposed model or the baselines, which is a problem since the default baseline hyperparameters were optimized on another version of the environment. My suggestion is to perform a hyperparameter search, use the validation performance for hyperparameter selection, then report the results over both training and environment seeds for the best hyperparameters (but not the model). \n\nThe generalization results (Table 2 & 3) do not report baseline performance.\n\nLastly, there is only one experiment with more than 2 agents (it's only 3 agents, and the experiment is on the easiest version of one of the environments). The performance gap is much smaller there, with the proposed model barely outperforming the non-MARL baseline (why is this the case?). This sheds doubt on whether the proposed method works as well with more than 2 agents and in more challenging environments. \n\n[1] https://arxiv.org/abs/1803.11485\n[2] https://arxiv.org/abs/2103.01955\n[3] https://arxiv.org/abs/1709.06560",
            "clarity,_quality,_novelty_and_reproducibility": "To my knowledge, there is no prior work in multi-agent language-assisted RL, so the setting is novel. I am also unaware of similar approaches to subgoal assignments in MARL, though I am less familiar with this literature. \n\nI found the paper difficult to follow in places with some important details left unclear or insufficiently motivated. The following parts of the paper need to be clarified before the paper is ready for publication:\n-  the function Ground(.) that is mentioned across Section 4.1: is this always the same function, is there parameter sharing within or between agents? \n- what are the dimensions of mask $m_i$? does it mask just the entities, positions or also features of the entities? \n- in Section 4.1 subgoal division module, what exactly is the target distribution $p_e^*$ and how do you ensure it has a relatively low variance? Clarify that |e| refers to the number of entities (if I'm interpreting this correctly)\n- in Section 5.2 performance, is $|e_{i}|$ number of entities covered by mask of agent $i$? What is the form of these losses when there are more than 2 agents?  \n- which version of the RTFM task is shown in Figure 3?\n\nThe paper also needs a couple more passes to fix various grammatical and writing errors. \n\nIn addition, I'd suggest the following (less important) changes to improve clarity:\n- add examples from the task to p5 and p6 of introduction \n- explain what motivates the choice of curriculum in Section 5.1, how did you decide on this particular curriculum and how well do the models perform without it\n- the visualizations of architecture in Fig 2 and 5 are hard to understand on their own, the figure captions should be expanded to make the figures understandable without the referrals to various sections and prior work\n- the usage of the term opponent is confusing, given the agents are trained in a cooperative setting\n- separate the ablation experiments into another table (from Table 1); it is hard to keep track of different variants of the model\n- explain what is meant by rationality of the subgoal division (sec 4.1 and 5.2) (maybe a better term is complementary/uniform coverage of entities?) \n- explain what is the shaded in Fig 4\n- the term natural language typically refers to the language generated by humans, hence I'd refer to language in RTFM as simply templated language\n- Table 1-3: explain the table column names in the caption \n- in section 5.3 multi-agent RTFM, S5-Eval(new), clarify that all monsters and modifiers are novel\n- add references to the Appendix; there is often important clarifying information in appendix, but it is not referred to when applicable ",
            "summary_of_the_review": "The paper proposes several architectural changes to enable subgoal division in collaborative MARL with language for specifying instructions and environmental dynamics. The results with two agents are promising, with the proposed method significantly outperforming independently trained agents with a non-modified grounding architecture. However, the paper is not ready to be accepted due to a lack of comparison to standard MARL baselines, methodological issues, and a lack of clarity about the method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1349/Reviewer_xDyS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1349/Reviewer_xDyS"
        ]
    }
]