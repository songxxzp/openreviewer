[
    {
        "id": "s0OnrS1EbX",
        "original": null,
        "number": 1,
        "cdate": 1666206351180,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666206351180,
        "tmdate": 1666206351180,
        "tddate": null,
        "forum": "25VgHaPz0l4",
        "replyto": "25VgHaPz0l4",
        "invitation": "ICLR.cc/2023/Conference/Paper5186/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper \"Selection Collider Bias in Large Language Models\" explores a type of collider bias in which the shared effect variable is the selection process by which training data is generated. They show that the effect of this selection process can be highly pronounced in cases of textual ambiguity, leading to variables that are causally disconnected in the real word (e.g. gender vs year) exhibiting strong predicted correlations. They use this spurious correlation to define a novel uncertainty metric that can help indicate when a model is conditioning on non-causal variables. They also demonstrate the validity of their model formulation on a more general toy example. The authors also open-sourced their code with an easy-to-use UI so that others can easily experiment with it.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is well-organized and clearly written.\n\n- The paper combines techniques from the usually-disjoint domains of causal modeling and LLMs.\n\n- The paper takes pains to motivate the approach throughout its first several sections, both conceptually and mathematically.\n\nWeaknesses:\n\n- Referring to S as \"access\" is confusing, as this implies that it is an indicator of whether a given gender had access to the process that produced the dataset. However it's possible that a gender did have access, and yet the data was nonetheless highly gender biased, for example a corpus of harlequin novels written by female authors. It would therefore makes more sense to call this variable something like \"representation\", as what it actually expresses is the presence or absence of certain conditionally gendered data in the dataset.\n\n- Need more information about how the dataset used to perform the fine-tuning in Figure 2 was constructed. Was there a heuristic used to identify sentences in an existing corpus that have a particular gender-ambiguous structure? Or was the heuristic used to construct the sentences directly? If the latter, then how were the \"true\" labels known? All of this information should be described in more detail in Section 3.1.\n\n- The selection criteria of $(W \u00b1 G + N(0,1)) > 2\\alpha$ used as a cutoff in the toy example of Section 8 seems like it would be throwing away a considerable majority of the W, G values. The authors should state the fraction of sampled values that satisfy the selection cutoff, preferably with an associated figure.\n\n- The selection collider bias effect is presented as a general method, but (aside from the toy example) is presented solely in the context of gender biasing, to the point that one of the nodes is explicitly stated to be gender (G). The benefits of this approach would be clearer is it were demonstrated for other types of biases, for example bad weather being textually correlated with negative outcomes of all kinds, for causally unrelated reasons. For example \"it was a [dark and stormy/sunny and bright] morning when the S&P 500 [crashed/rallied]\".\n\n- Part of the stated contribution of the paper is the introduction of a new technique for modeling uncertainty, however this measure is not investigated in much detail. Some natural additional steps would have been to (1) compute its correlation with a measure of intra-profession gender representation over time, (2) compare it to the standard deviation over softmax probabilities rather than taking their most extreme differences, (3) use it to identify specific scenarios in which a human observer might think that the LLM is conditioning on causally meaningful information, but really is conditioning on acausal correlations.\n\n- Although the paper centers around a novel data biasing mechanism, it does not make any attempt to describe how one might go about alleviating this biasing, or detecting it a priori. (The proposed uncertainly measure is only useful if you already know where to look.)\n\nTypos:\n\n- Bottom of page 3: \"we could the use\"\n\n- Top of page 5: \"supports our supports our\"\n\n- Top of page 9: \"only holds underspecified models\"\n\n- Middle of page 11: opening-quote marks in list of countries are backwards\n\n- Figure 3: Pearson's r should be lower-case in the legend",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clearly written with minimal typos, the mathematical formulation is thoroughly motivated, and figures and equations are explained in detail.\n\nQuality: The paper is of a very high quality. This includes thorough explanations, mathematically grounded problems formulations, use of external data sources, and training large models.\n\nNovelty: It is unclear to this reviewer how novel of a contribution this work represents. The concepts of underspecification in machine learning models, and collider bias in graphical models, were already well-established concepts prior to this work. The novelty would be more obvious if this \"selection collider bias\" formulation led to a new correction or detection method, but that does not appear to be the case here. It is mentioned in the Discussion section that the correlations between gender and time/place are previously unreported, but this is used more as an example within the paper than a primary contribution.\n\nReproducibility: Reproducibility is helped by the authors open-sourcing their code and making available a nice UI for running similar experiments. However the paper fails to go into sufficient detail on the process they used to generate the data used for fine-tuning their models, or what the resulting data looked like, or what architectural changes were made to the models in order to train on this data. Even if the authors do not feel that this information is important enough to be present in the main body of the paper, it should at least appear in the appendix.",
            "summary_of_the_review": "Overall the paper is clear, well-written, and well-organized. However the primary contribution of the paper is a novel mathematical formulation of the already well-established phenomenon that if a model cannot predict a label by conditioning on causally-related words, then the model will try to predict the label using whatever non-causal word correlations are present, and these may be sampling artifacts. The authors' new proposed formalism for describing this phenomenon does not lead to a new method for diagnosing it a priori, nor does it lead to a new approach to alleviating it. Therefore this reviewer does not feel that this paper meets the bar for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5186/Reviewer_ukSe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5186/Reviewer_ukSe"
        ]
    },
    {
        "id": "BC_BOMoY_E8",
        "original": null,
        "number": 2,
        "cdate": 1666458859711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666458859711,
        "tmdate": 1666458859711,
        "tddate": null,
        "forum": "25VgHaPz0l4",
        "replyto": "25VgHaPz0l4",
        "invitation": "ICLR.cc/2023/Conference/Paper5186/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on what it refers to as \"selection collider bias\", using a causal inference framework to discuss spurious correlations in language data. The paper seems largely focused on formalizing notions of selection bias and spurious correlations through the language of causal inference, and presenting reasoning about factors involved in these phenomena. Concretely the paper offers a couple of experiments: the first involves fine-tuning LMs by \"masking common gender-identifying words for prediction\", and showing that both these fine-tuned models and normal pre-trained LMs show an interaction between gender pronoun preference and mentions of years or of countries. The second experiment involves augmenting the WinoGender dataset with prefixes mentioning a date, as well as explicit mentions of gender for one of the antecedent entities, and showing that there are correlations between the date and gender preferences for the pronoun, but only when the pronoun's antecedent is not explicitly specified. The third experiment runs a toy simulation to further demonstrate the relationship between data, label, and causal variables depending on underspecification and sampling. ",
            "strength_and_weaknesses": "Strengths: The paper involves the important topic of spurious correlations, and it frames that topic within an interesting causal inference framework. The paper identifies spurious correlations between model gender preference and gender-irrelevant cues like date and country.\n\nWeaknesses: The paper makes no effort to contextualize its contribution with respect to related work. The paper cites a bit of work while explaining and motivating the relevant concepts and formalisms that it uses, but there is no Related Work section to lay out how the paper's contribution relates to and is novel with respect to all of the other literature in this area.\n\nIt is not only the contextualization of the contribution that the paper doesn't make clear -- the paper doesn't really make a clear argument for what contribution it is making in the first place. The first symptom of this is that the introduction spends a lot of time defining \"selection collider bias\" and briefly arguing for its relevance, but makes no mention at all of what contribution the paper plans to make with respect to this selection collider bias concept. The paper then proceeds to present a number of theoretical discussions and empirical demonstrations, so in theory there could be good contributions that should simply be laid out more explicitly (in the introduction and elsewhere) -- but at the moment I don't think the paper is making any clear statement for what exactly it's trying to accomplish, and as I elaborate in the next paragraph, this contribution isn't clear from the content of the experiments/theoretical discussions either.\n\nExamining the experiments themselves to extract a potential contribution, it seems to me that the most concrete finding is that pre-trained LMs' preference for different pronoun genders is influenced by irrelevant cues like year and country mentions, but more so/exclusively so when actual relevant gender cues are not available/sufficient to inform the prediction. This characterization seems reasonably clear from the second experiment -- but the first experiment is harder to interpret, in large part because the specific details of the data and task used for fine-tuning are not actually provided, so it is difficult to know exactly what to make of the stronger correlations for the fine-tuned models. But the fact that the original LMs also show some spurious correction between year/country and gender preference is somewhat interesting. However, there are numerous issues surrounding this potentially interesting finding: 1) it's not clear whether it's a novel finding, since there is no comparison to related literature, 2) the paper does not provide any particular motivation for having chosen date and country as possible spurious correlating variables with gender, apart from speculating that they could show spurious correlations (wrt this, see also below), and 3) the paper puts no particular focus on this finding and makes no argument about its significance.\n\nIt seems likely that the authors want to focus more on a theoretical contribution that emerges from conceptualizing spurious correlations within the causal framework that they use. While I'm happy to be convinced otherwise on this, I'm also just not seeing any clear, novel insight that emerges from the theoretical framing or reasoning here -- mainly it seems just to be restating and reframing the existence of spurious correlations. Additionally, some of the theoretical framing strikes me as unclear and potentially problematic: the specific example I have in mind is the conceptualization of variable G in Section 3, which is described as corresponding to \"gender\", but it is not clear whether this refers to gender of the writer of the text or gender of the entity being referred to by a pronoun or pronouns in the text. Assumption of a causal relationship between G and Y seems to suggest G refers to gender of entities referred to by pronouns, but the discussion of a relationship between G and dataset access seems to suggest G refers to gender of writers of text. Obviously these two variables cannot be conflated, so the authors should clarify their thinking here.",
            "clarity,_quality,_novelty_and_reproducibility": "Aspects of the paper (definition of concepts in the introduction, for instance) are pretty clear, while others (relationship to existing work, some key methodological details, motivations, intended contributions, etc) are not. I don't think the methods are clear enough for sufficient reproducibility, and the lack of a Related Work discussion makes novelty also unclear. Overall quality is diminished by all of the issues I've discussed, and can be improved by addressing them.",
            "summary_of_the_review": "The paper focuses on an important overall problem, uses an interesting theoretical framing, and has at least one seemingly interesting empirical finding. However, it does not attempt to contextualize with respect to related literature, does not provide a clear statement (or demonstration) of what contribution it is making, and has additional issues with methodological and theoretical clarity.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5186/Reviewer_qdVm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5186/Reviewer_qdVm"
        ]
    },
    {
        "id": "Yq3npUcgi1",
        "original": null,
        "number": 3,
        "cdate": 1666612987922,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612987922,
        "tmdate": 1666612987922,
        "tddate": null,
        "forum": "25VgHaPz0l4",
        "replyto": "25VgHaPz0l4",
        "invitation": "ICLR.cc/2023/Conference/Paper5186/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explores the spurious correlation problem, a common problem in current LMs that learns dependency relations between two unconditional entities. The authors take \"date\", \"place\", and \"gender\" as examples and analyze the spurious correlation between gender and date (or place). The authors also provide an uncertainty metric to evaluate and identify spurious correlations. ",
            "strength_and_weaknesses": "Strength:\n\nIt is interesting to see how LMs learn spurious bias from causal mechanisms.  \n\n\n\n\n\nWeaknesses:\n\n[1]  There are important concepts lacking illustrative explanations. It makes the whole paper hard to read. \n",
            "clarity,_quality,_novelty_and_reproducibility": "\n[1] Definition of sample selection bias and selection collider bias.\n\nLines 5-6 in Intro. say \"sample selection bias occurs when some mechanism, observed or not, causes samples to be included or excluded from the dataset\".  Please give more explanation about sample selection bias.  From my view, it seems that \"samples to be included or excluded\" defines a full set where the probability is always 100%.  Given any mechanism, this event always happens. \n\nFigure 1 illustrates what is selection collider bias. Is there any example to explain selection collider bias? \n\n[2] Definition of underspecification \n\nSection 2 describes what is underspecification. Compared with robustness and over-fitting, underspecification is somehow not very common. It would be better to just give an illustrative example or illustrative definition. \n\n[3] Details of Figure 2\n\nAbout \"Now applying the above-mentioned Markov and faithfulness assumptions to the underspecified model in Figure 1(b), we can estimate the conditional probability of a gender-identifying word, Y, given gender-neutral text, X, in a LLM as shown in Equation (1) through Equation (4)\", could the author provide more details?\n\n[4] What is \" the lowest relative uncertainty\" in Table 2?\n\n[5] Details of uncertainty definition in Section 7.2 \n\n\n",
            "summary_of_the_review": "The paper focuses on an interesting problem that explores spurious correlation from a causal mechanism perspective. However, due to missing necessary explanations, it is hard for me to understand the details of the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5186/Reviewer_LK4n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5186/Reviewer_LK4n"
        ]
    }
]