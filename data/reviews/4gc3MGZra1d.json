[
    {
        "id": "FeiGsKdKH1",
        "original": null,
        "number": 1,
        "cdate": 1666337178994,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666337178994,
        "tmdate": 1666337178994,
        "tddate": null,
        "forum": "4gc3MGZra1d",
        "replyto": "4gc3MGZra1d",
        "invitation": "ICLR.cc/2023/Conference/Paper5879/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors identify cases which a particular method of graph representations of mixed integer linear programming (MILP) problems shall not distinguish feasible problems from infeasible problems using Weisfeiler-Lehman (WL) test. Authors characterize a subset of indistinguishable MILP problems as \"foldable\", which means there are nodes with the same color from WL test. As a treatment, authors suggest to add a random features to make all nodes distinguishable. Synthetic data experiments demonstrate that random features do provide representation power to separate feasible MILPs from infeasible ones.",
            "strength_and_weaknesses": "While GNNs have become a popular method of encoding MILP problems, there has been lack of clarity on whether these GNNs are actually representative enough for their successful applications to downstream tasks such as neural diving or branching. This paper provides a clear example which this method might fail, and provides an insightful characterization. I believe such a finding will help practitioners understand limitations of current approaches in applying GNNs to MILPs.\n\nAuthors also build upon well-established approaches from the literature: The suggested remedy of adding random features is simple, but well supported by recent literature on GNNs. \n\nThe contribution of the paper is mostly theoretical rather than empirical, and unfortunately the paper is not very clear about the limitation of the analysis. Most importantly, authors fail to mention that the particular bipartite graph representation they introduce is a simplification of what was actually introduced in Gasse et al. (2019). The paper almost reads like this representation is exactly what Gasse et al and follow-up papers like Nair et al are using, whereas in reality these papers have a number of engineered features and therefore the suggested \"unfoldability\" is much less likely to happen. I still find authors' analysis to be insightful- good analysis requires simplification. However, authors should've clarified they are making simplifications.\n\nI am also not sure adding random features is the right remedy to the problem. For sure, making every node color unique will allow WL test to always separate two graph instances. But on the other hand, when there are two equivalent MILP problems, WL test will now fail to recognize they are equivalent? Hence we seem to be losing statistical power from this \"remedy\", but this loss of statistical power is not characterized, not giving readers the sense of trade-off being made.\n\nAuthors could've provided much more insight on the practical usefulness of foldability if they used MILP problems from existing benchmarks. How many of MILP problems in the literature are actually foldable; is there any? If there are, how does that impact the performance of GNN-based feasibility classifiers and/or neural diving/branching models?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. Mathematical notations are consistent throughout the paper, and follow well-established conventions, which make it easy to follow the discussion without much effort.\n\nThe identification of the negative example which current graph representation fails is clever and insightful. Given GNNs have been quite popular for MILP applications, I am surprised such a negative result did not already exist. Hence I highly value the novelty of this work. Authors also built upon standard tools for understanding GNN representative power, which makes the method principled. Beyond the identification of the negative example & characterization with foldability, however, the rest of the analysis seems standard.\n\nThe paper is mostly theoretic, so reproducibility is not a big concern. I wasn't able to find code, but replicating results is likely low effort given the simplicity of experiments.\n\n",
            "summary_of_the_review": "Being clear about limitations of the analysis, and bridging the gap between the analysis & actual practice will strengthen the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5879/Reviewer_LV4F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5879/Reviewer_LV4F"
        ]
    },
    {
        "id": "_YLIcrsGbT5",
        "original": null,
        "number": 2,
        "cdate": 1666396199987,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666396199987,
        "tmdate": 1666396199987,
        "tddate": null,
        "forum": "4gc3MGZra1d",
        "replyto": "4gc3MGZra1d",
        "invitation": "ICLR.cc/2023/Conference/Paper5879/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors try to answer an interesting yet challenging question\uff1a\nIs GNN a good representation model for MILP and whether it can predict important properties, eg., feasibility, for MILP problems?\nThey construct a simple example showing that there exist two MILPs such that one of them is feasible while the other one is not, but no GNN can distinguish them.\nThis example illustrates the limitation of the representability of GNN for MILP problems.  \nThey further divide MILPs into two classes: foldable and unfoldable and show that GNN has strong enough representation power on unfoldable MILPS. \nFinally, they show that by appending random features, GNN can also have strong enough representation power on foldable MILPS. \n",
            "strength_and_weaknesses": "Strong points:\n1. Studying the representability of GNN for MILPs is a fundamental problem in this area\uff0c and this topic fits well with the conference's scope. \n2. The construction of the counterexample is neat, and this example clearly shows the limitation of the representability of GNN for MILP problems.  \n3. Proving GNN has strong enough representation power by appending random features provides some theoretical explanations for this widely known trick. \n\n\nWeak points:\n1. Since the definition of foldable MILP depends on Algorithm 1, which is used to measure the separation power of GNN, the resulting theorems (Thm 4.2 ~ Thm 4.4) seem to be straightforward.\n2. For the experiments, reporting training loss is not very meaningful from a practitioner's point of view. As you claim that adding random features is a good practical solution, can you also report the corresponding test loss?\n3. Although the three questions raised in the paper are important, they are not quite practical. It will be better if the authors can link the results of this paper to answer whether GNN is able to predict the branching strategy which has been empirically studied recently. ",
            "clarity,_quality,_novelty_and_reproducibility": "This is a well-organized paper and easy to follow.",
            "summary_of_the_review": "Overall, this is a good paper that tries to answer important questions on the topic of \"learning to optimize\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5879/Reviewer_mtYU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5879/Reviewer_mtYU"
        ]
    },
    {
        "id": "MzXEDrXuXb",
        "original": null,
        "number": 3,
        "cdate": 1666592787845,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666592787845,
        "tmdate": 1666592787845,
        "tddate": null,
        "forum": "4gc3MGZra1d",
        "replyto": "4gc3MGZra1d",
        "invitation": "ICLR.cc/2023/Conference/Paper5879/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a Graphical Neural Network (GNN) based method for the solution of Mixed-integer linear programming (MILP) optimization problem. The proposed method adds random features to the MILP based GNNs and prove is provided that this will provide a optimal solution for MILP. Details, from a reference,  are provided for the representation of an MILP into a weighted graph. GNN is built to represent the properties of the whole graph. Feasibility, Optimal objective value and Optimal solution mappings are provided along with the invariance and equivariance properties. A lot of theoretical description of mathematics is provided for the GNN based MILP solution. Feasibility through numerical experiments is  described. \n",
            "strength_and_weaknesses": "Strength:\n\t1. The proposed technique of addition of random features\n\t2. Mathematics and details of the MILP representation into GNN\n\t3. Reapplication of theorem 3.1. for MILP - graphs\n\t4. Theorems of section 4 & 5\n\t5. Numerical experiments to validate proposed theories\nWeaknesses:\n\t1. Typos e.g. \"Fugure 2\" on page 6\n\t2. It is not clear what is the contribution to the mathematics and theory and what is reapplication of existing work.\n\t3. Mainly the paper is about graph theory\n\t4. Although GNN is used in machine learning, the paper does seems to be doing machine learning\n\t5. No real world MILP scenarios are implemented or solved\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\tMost of the paper is clearly written but in some descriptions, contribution and reusing of others work is not clear. eg. Do the theorems of section 4 and 5 are contributions or reapplications?\nQuality and Novelty:\n\tThis paper is mostly about Graph theory and MILP and these are not in my area of expertise. Therefore, I am not able to judge the quality or the novelty of this paper. In my opinion this paper is not suitable for a machine learning conference. This paper should be submitted to a applied mathematics or graph conference.\nReproducibility: Seems reproducible as theorems proofs and details are provided in the appendix\n",
            "summary_of_the_review": "\t1. The paper seems sound mathematically and theoretically.\n\t2. Mostly the paper is about MILP and Graphs theory\n\t3. Other than GNN, no machine learning\n\t4. More suitable for graph or applied mathematics conference\n\t5. Not suitable for a machine learning conference\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5879/Reviewer_ekKz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5879/Reviewer_ekKz"
        ]
    },
    {
        "id": "ZG0ydwIq7Mj",
        "original": null,
        "number": 4,
        "cdate": 1666687038838,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687038838,
        "tmdate": 1669271714744,
        "tddate": null,
        "forum": "4gc3MGZra1d",
        "replyto": "4gc3MGZra1d",
        "invitation": "ICLR.cc/2023/Conference/Paper5879/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper study the representation power of GNN for mixed integer linear programming (MILP) problems. ",
            "strength_and_weaknesses": "See my comments below. ",
            "clarity,_quality,_novelty_and_reproducibility": "See my comments below. ",
            "summary_of_the_review": "This paper studies the representation power of GNN for mixed integer linear programming (MILP) problems.  The problem is important and fundamental. The analysis justifies the advantage of the random feature technique when solving MILP problems. The overall presentation is quite clear.  The proof combines the Stone-Weierstrass theorem (including its generalized version in Azizian and Lelarge\u201921) with domain knowledge of MILP.  The anaylsis is mostly sound and solid. However, I have the following questions. \n\n**Major questions:** several important proof steps are shown in a hand-waving style without rigorous derivation. For instance: \n\n1. Lemma A.8 is a very strong claim. I think it only holds for some specific graphs like bipartite graphs. Please write down the proof rigorously. The same suggestion also applies to the proof of Thm 5.1, page 16, the claim in the sentence \"Therefore, similar to Lemma A.8, one can see for any....\". Please write down the proof rigorously. \n\n2. For the proof of Thm 5.1, page 16,  please verify that for any function f in the function class of  GNN, f(G1) = f(G2) if G1 and G2 (which are both equibed with random features) are isomorphic.  Please write down the proof rigorously (if this claim is correct.)\n\n3.  Why is phi_feas continuous?\n\n4. phi_solu( ) is an one-to-many mapping. It is not a function. I don't understand how to approximate this mapping using Lusin' & Stone-Weisterass Theorem, which only apply to measureable & continuous functions. Please explain more. \n\nA follow-up question related to the above comments 1 and 2:\n\nRegarding the proof of Thm 5.1, page 16, the authors apply Thm A.7 after verifying  \"phi_feas(G, H ) =  phi_feas(Ghat, Hhat), for all (G, H,w )~ (Ghat,Hhat,what)\". However, I dont think we can apply  Thm A.7 like this because (G,H) and (G, H, w) lie in different spaces.  From my understanding, by applying Thm A.7 correctly, we can only get \"random-featured GNN\" can approximate \"phi_feas defined on random-featured  graphs\".  But it seems unclear what is the relation between \"phi_feas defined on random-featured  graphs\" and \"phi_feas defined on original  graphs\". Please verify.\n\nTo briefly summarize, the proof of Thm 5.1, page 16 is quite dense and hand-waving. **The rigorous proof is missing and I doubt the correctness of the results about random features.  I would like to raise the score if authors address these concerns.**\n\n\n\n**Other comments:**\n1. The paper steps forwards an important first step to establish the feasibility of learning MILP by GNNs. However, the problem is far from being concluded. For MILP problems, another equally important (if not more important) application of GNN is to learn the algorithmic configuration such as the branching strategy, which is not covered in the script. Please add more relevant discussions.  \n\n2. Correct me if wrong: from my understanding,  the original WL test paint all the nodes in a single graph with the same initial color.  Why do we allow different initial colors (across nodes) in the script? \n\n2. typo: page 6 lemma 3-2: Consider two MILP provlems ==> problems.\n\n3. I would suggest the authors double-check all the \\citep and \\citet properly. \n\n4. When encoding the MILP into a graph, how do you implement \"<=, >=\" and \"\\infty\" in the node feature of the graph?\n\n\n+++++++++++++++++++++**POST REBUTTAL** ++++++++++++++++++++++\n\nMany thanks for the detailed reply from the authors. Most of my concern is addressed.  I am also convinced that the script has many differences with  the LP paper  so the ethic concern is also addressed. I briefly summarize the contribution of the script as follows:\n\n1. Negative results on GNN for representing MILP\n2. A candidate solution called \u201crandom-featured GNN\u201c is provided.\n\nFor me, 1 is not surprising because it is well known that GNN has limited distinguishing power, so it is possible that GNN might fail to capture certain info on MILP graph. However, it is good to see a counter-example to explicitly point out the limitation of GNN for representing MILP.  In summary, although the result is not surprising, I think this part is still worth sharing with the community. \n\nAs for 2, I think the contribution is limited due to the following reasons.\n\n[R1] The analysis requires infinite-width random-featured GNN.  It seems unclear what is the advantage of random-featured GNN over MLP. Since infinite width MLP has universal approximation power, MLP can solve isomorphism problems and also it can represent MILP as well. Then the random-feature strategy seems not that important since you also require infinite width anyways.  \n\n[R2] The practical impact is unclear due to the toy experiments. Further, random-feature technique is not invented by the script, either. \n\n\nBy balancing the pros and cons, I raise my score to 6.\n\n\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5879/Reviewer_wHAc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5879/Reviewer_wHAc"
        ]
    }
]