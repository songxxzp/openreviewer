[
    {
        "id": "LofdUIUMAKq",
        "original": null,
        "number": 1,
        "cdate": 1665990514532,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665990514532,
        "tmdate": 1665990514532,
        "tddate": null,
        "forum": "guSxooOK9E",
        "replyto": "guSxooOK9E",
        "invitation": "ICLR.cc/2023/Conference/Paper4852/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the trade-off between robustness and test error in a setting in which (i) the teacher is a quadratic function, (ii) infinite training data are available, and (iii) a proportional scaling between the input dimension and the neural network width is assumed. The authors provide results for 3 different regimes: (i) two-layer networks trained via SGD (Theorem 3.1), (ii) random features (Theorem 4.1), and (iii) neural tangent approximation, with and without initialization (Theorem 5.1-5.2). The test error in such models was computed in previous work by Ghorbani et al. (2019) and the contribution of this paper consists in computing a certain metric of robustness/sensitivity using rather similar tools (coming from high-dimensional statistics). By doing so, the authors are able to exhibit an interesting balance between the generalization error and the robustness to perturbation. Finally, a lower bound is provided for a general non-linear teacher/student model (Theorem 6.1). This lower bound is a relatively simple consequence of the concentration of measure phenomenon. ",
            "strength_and_weaknesses": "Strengths:\n\n(1) The problem is well motivated and the particular measure of robustness/sensitivity is well discussed (in Appendix C).\n\n(2) The trade-off exhibited is, to the best of my knowledge, novel.\n\n(3) Even though the assumptions are sometimes strong (see the weakness (2)), it is always remarkable to have an exactly solvable model, leading to a closed-form expression for the robustness.\n\n\nWeaknesses:\n\n(1) From the technical standpoint, the paper is somewhat incremental, since the tools are essentially those of Ghorbani et al., (2019). If that's not the case, I would like to encourage the authors to highlight their technical innovations in the rebuttal.\n\n(2) The assumptions on the model are rather strong, although I understand that they are taken from Ghorbani et al., (2019). In particular: \n\n(a) Having a teacher model which is a quadratic function is rather restrictive.\n\n(b) I am actually wondering how crucial Condition 4.3(A) actually is. If I understand correctly, it basically ensures that the activation function is centered. In Appendix F.3, the effect of centering is discussed. Given this discussion, I wonder about what happens when Condition 4.3(A) is violated. Is the model less robust (as in the case of the neural tangent approximation with an initialization)?\n\n(3) The authors show that having an initialization term in the neural tangent approximation (and potentially not being able to fulfil Condition 4.3(A), see remark above) leads to less robustness. It is not clear to me whether this actually means a worse resistance to adversarial examples or whether this is just an effect of the particular setting taken into account (and, more specifically, of the particular metric for robustness being chosen). Can the authors comment on this point? \n\n(4) One of the key messages of the paper seems to be that the trade-off between generalization and robustness is of the form $\\tilde\\varepsilon_{\\rm test}+\\tilde\\varepsilon_{\\rm rob}=1$. I have three comments about this. \n\n(a) This same trade-off is proved for the three settings taken into account. Does this mean that training with SGD, doing random features or the neural tangent approximation makes no difference as far as robustness is concerned? This would be a bit strange.\n\n(b) As a matter of fact, if I understand correctly the paper, $\\tilde\\varepsilon_{\\rm test}+\\tilde\\varepsilon_{\\rm rob}=1$ holds for random features and neural tangent only in some corner cases ($\\rho\\to\\infty$ for random features and $\\beta=1$ for neural tangent). Can the authors comment on why, in these corner cases, one would expect that the trade-off is the same in different models?\n\n(c) Finally, can the authors comment on the trade-off when it does not have the form $\\tilde\\varepsilon_{\\rm test}+\\tilde\\varepsilon_{\\rm rob}=1$? Theorem 4.1 and 5.1 hold more in general and should allow to discuss a more general trade-off.\n\n(5) How does Theorem 6.1 relate to the body of literature that uses concentration of measure to show the inevitability of adversarial examples, see e.g. Mahloujifar et al., (2018)?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear. The trade-off between test-error and robustness is original. The technical tools are close to those of Ghorbani et al., (2019).\n\nThere are some typos, e.g., \"the network with $m$\" (end of the first paragraph of 'Related works') and \"For simplicity, simplicity of this experiment,\" (caption of Figure 2).",
            "summary_of_the_review": "Overall, the paper demonstrates an interesting and novel trade-off between test error and robustness. At the technical level, the analysis is a bit incremental, as it appears to resemble closely that of Ghorbani et al., (2019). My borderline (leaning positive) score is due to the several weaknesses I pointed out above, which can potentially be resolved (at least partially) after the rebuttal.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4852/Reviewer_7Nsv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4852/Reviewer_7Nsv"
        ]
    },
    {
        "id": "5kuF36ExUg",
        "original": null,
        "number": 2,
        "cdate": 1666180390296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666180390296,
        "tmdate": 1669389453715,
        "tddate": null,
        "forum": "guSxooOK9E",
        "replyto": "guSxooOK9E",
        "invitation": "ICLR.cc/2023/Conference/Paper4852/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the trade-off between robustness and test error for learning a quadratic target function. The models considered include two-layer quadratic nets, random feature models, and the neural tangent one. The sample size is assumed to be infinite and thus, the test error is essentially the approximation error. \n\nThe authors show that there exist a trade-off between approximation error and robustness of approximant: $robust(f) + error(f) = 1$. The authors argue that these theoretical findings can contribute to the explanation of why there exists a  trade-off between clean accuracy and adversarial robustness, a phenomenon observed in practice. ",
            "strength_and_weaknesses": "I do not know how to assess the results of this paper. I feel that it is **not even wrong** since the result has nothing to do with adversarial robustness. If I understand correctly, the theoretical result basically means that when $f-f^*$ is small, $||\\nabla f||$ is close to $||\\nabla f^*||$.  Here $f$ and $f*$ are our model and target function, respectively. In other words, the relative robustness approaches $1$ when the approximation error is decreasing toward zero. \n\nFirst, this result is kind of irrelevant, since we only need the approximant and the target function to satisfy a **coercive condition**: that the smallness of $||f-f^*||$ can imply the smallness of $||\\nabla f - \\nabla f^*||$. This can be seen in Section 6, where the authors actually prove the trade-off for any functions that satisfy the Poincare inequality. \n\nSecond, I think the result has nothing to do with the vulnerability of ML models. As the approximation error is small, we have $||\\nabla f||\\approx ||\\nabla f^*||$, which in fact suggests that our model is as robust as the ground truth. This is almost the best we can expect. The vulnerability issue should be the case that the learned model is much more vulnerable than the ground truth but the test error is small. This is obviously not the case studied in this work.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear but the interpretation of their result seems irrelevant (at least to me)",
            "summary_of_the_review": "This paper provides a precise characterization of the trade-off between robustness and approximation error for a few models. Unfortunately, I find the result is kind of irrelevant. Please correct me if I misunderstandard the result. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4852/Reviewer_d515"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4852/Reviewer_d515"
        ]
    },
    {
        "id": "y4ObolRcVV",
        "original": null,
        "number": 3,
        "cdate": 1666432591526,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666432591526,
        "tmdate": 1666432591526,
        "tddate": null,
        "forum": "guSxooOK9E",
        "replyto": "guSxooOK9E",
        "invitation": "ICLR.cc/2023/Conference/Paper4852/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyzes generalization and robustness properties (measured by gradient norm) of two-layer neural networks in different learning regimes under the high dimensional setting with Gaussian data. The tradeoff between them is theoretically given for such two-layer neural networks trained by SGD, the random features regime, and the NTK regime. Besides, a lower bound between the test error and the robustness over general data is given, which implies that the test error cannot be decreased without increasing the robustness error.\n",
            "strength_and_weaknesses": "**Pros:** \n\n1. In two-layer neural networks with polynomial activation functions trained by SGD, random features, the neural tangent kernel for two-layer networks, the tradeoff between the generalization (excess risk) and the robustness (gradient norm) is given.\n2.  a lower bound between the test error and the robustness over general data is given. \n\n**Cons:**\n\nI\u2019m familiar with this topic equipped with high dimensional statistics, and like this theoretical result on the trade-off between generalization and robustness. Nevertheless, I don\u2019t like the writing style: mixing with Ghorbani et al. (2019) throughout this paper, e.g., problem settings, generalization results, random features regime and neural tangent kernel regime. \nFor example, Section 4.2 has already been studied in Ghorbani et al. (2019) or can be easily obtained (e.g., Eq. 20).\n\nI strongly suggest the authors rewrite this paper to emphasize their main contribution on robustness. Besides, a table is needed to summarize the problem settings and results presented in Section 3, 4, 5. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Appart from the above mentioned issues, I have the following questions:\n\n1. There exists different definitions on over-parameterization, e.g., $m/d \\gg 1$ in (Allen-Zhu, 2018, https://arxiv.org/abs/1811.03962) that this paper used or $m/n \\gg 1$ in (Belkin et al. 2019). I suggest the authors clarify this.\n\n2. This work shows that over-parameterization hurts robustness, which is contrary to (Bubeck and Sellke, 2021) as well as other results on robustness. More discussion is needed.\n\n\n3. This work uses the gradient norm in a L2-integrable space in the sense of expectation as a robustness metric, called Dirichlet energy. In fact, this average-case robustness view also exists in [1] under different initializations as well as the over-parameterization. (authors are excused for this paper because this paper was recently posted). Besides, I\u2019m wondering that, the tradeoff between robustness and generalization in this paper can be extended to other robustness metrics, e.g., [1] and Lipschitz constant? \n\n[1]  Zhu, Z., Liu, F., Chrysos, G.G. and Cevher, V. Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization). NeurIPS 2022.\n\n4. It is unclear to me why the input dimension $d$ is used in the assumption $d || \\Gamma ||_{op} = O(1)$. \nNormally $ || \\Gamma ||_{op} = O(1)$ makes sense in RMT.\n\n5. It is unclear to me why the neural networks in Section 3 exhibit a form of feature learning? \n\n**Minor issues:**\n\nThe studied test error in this paper works in the approximation theory view, but I suggest the authors adopt the estimation/test error instead of approximation error, leading to extra confusion to readers.\n",
            "summary_of_the_review": "This paper give a rigious theoretical evidence on the trade-off between robustness and generalization in high dimensional settings under two-layer neural networks. I vote for acceptance of this paper but strongly suggest the authors re-organize this work for better presentation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4852/Reviewer_qTks"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4852/Reviewer_qTks"
        ]
    },
    {
        "id": "cevDHEi0ewG",
        "original": null,
        "number": 4,
        "cdate": 1666587590590,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666587590590,
        "tmdate": 1666587590590,
        "tddate": null,
        "forum": "guSxooOK9E",
        "replyto": "guSxooOK9E",
        "invitation": "ICLR.cc/2023/Conference/Paper4852/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the trade-off between the test accuracy and robustness of two-layer neural networks, specifically concerning regression. More precisely, they focus on using a 2-layer network to learn a quadratic form. \nTheoretically, it is shown that there is a clear trade-off between robustness and test accuracy for some variants of the above setup where the initialization is taken into account, as well as the neural tangent regime. \nThe simulations verify the findings for this specific setup.",
            "strength_and_weaknesses": "This paper studies the robustness of two-layer neural networks in different learning regimes and proves that there is a compromise between robustness and test accuracy. \n\nPros:\n- the paper is well structured and, (albeit dense) it is relatively clear \n- to my knowledge, understanding the tradeoff on this particular setup is novel and well motivated\n- it is particularly nice that the considered setup is simple so that it can be studied thoroughly.\n\nCons:\n- while I agree with the soundness of the robustness measure used, and I appreciate the additional details in the appendix, I am not sure to which extent the results of this paper (in the regression setting) can be generalized to the more standard classification setting.\n- there are missing comparisons with existing works (see below)\n\n## Missing comparisons with existing works\n- The work of Zhang et al. (TRADES) which focuses on the classification setting is also discussing the tradeoff between robustness and accuracy\n- Specifically for the regression case,  [Rothenh\u00e4usler et al., 2018.] rigorously show the trade-off between robustness and test accuracy in a linear setting; it is necessary to discuss the differences.  \n\n[Zhang et al, 2019] *Theoretically Principled Trade-off between Robustness and Accuracy* -- https://arxiv.org/abs/1901.08573\n[Rothenh\u00e4usler et al., 2018.] *Anchor regression: heterogeneous data meets causality* -- https://arxiv.org/abs/1801.06229",
            "clarity,_quality,_novelty_and_reproducibility": "\n- **Clarity**: the paper is written very clearly, and very technically precise. It is also relatively dense--see my suggestion below.\n- **Quality**: the paper rigorously considers various settings and has interesting insights that I learned \n- **Novelty**: to my knowledge, considering this setup for the robustness-generalization trade-off is novel, and moreover, the connections with initializations are also novel\n- **Reproducibility** : I did not see an appendix on implementation details, please clarify in your response if the code will be made public.\n\n\n\n### Minor points / Suggestions\n- Maybe this has been also done by previous works, but I personally find it confusing that the terms *teacher-student* are used here given that teacher-student is a different problem, whereas here the teacher is not a neural network.\n- In the abstract, it is not clear what is referred to by *lazy training*; either add a citation or describe it with a few words therein\n- While reading the paper, I was often wondering why is this relevant -- it would be very helpful to improve the flow of the reading: e.g. prepare the reader for what follows, why the answer that follows is relevant etc. I understand that there is a lack of space and it is a strict advantage of the paper that there are many contributions, but I would advise moving some less-important details in the appendix if needed (while summarizing only in an informal theorem for e.g.) in order to improve the reading flow.\n",
            "summary_of_the_review": "The compromise between test accuracy and robustness has been studied before in the regression and classification setting, but this work sheds light on more detailed discussions that include initialization and other regimes, specifically for regression given quadratic form. \nWhile broadly the question is relevant, it is not clear if these findings can be extended to the more relevant classification setting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4852/Reviewer_vJwo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4852/Reviewer_vJwo"
        ]
    }
]