[
    {
        "id": "QLwK5F6FHYy",
        "original": null,
        "number": 1,
        "cdate": 1666178177273,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666178177273,
        "tmdate": 1666178177273,
        "tddate": null,
        "forum": "NYtq-CsRP3H",
        "replyto": "NYtq-CsRP3H",
        "invitation": "ICLR.cc/2023/Conference/Paper362/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a novel feature ranking algorithm that consists in two parts: first, a mask generator network is used to determine the importance of each feature, in a similar way the Learning to Explain algorithm (L2X) is used. Later, the average of multiple runs (with different weight initializations) over the mask generator are obtained, which are finally introduced in a final training, combining the weights of the average mask generator with a new local one, which will establish the final ranking of each feature. The experimental results show a better consistency on how the important features are ranked, when compared with the state-of-the-art",
            "strength_and_weaknesses": "Strengths\n* The algorithm is easy to implement.\n* The consistency in the solution is remarkable.\n\nWeaknesses\n* The paper is very difficult to read and to follow. Some of the information included in the appendix should be accessible at the regular paper.\n* The results are not remarkable when compared against the state-of-the-art.\n* Some theoretical decisions need more clarification.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n* Although the idea is simple, the paper is somehow very difficult to read. Some of the paper acronyms are defined in the appendix section, which is really odd. I suggest the authors to rewrite the experimental section, attempting to give a better explanation about how and against who they are comparing their algorithm.\n* The architecture of the mask generator should be defined in advance. Is is also advisable to explain how the output of the mask generator is defined. If I understood it correctly, it is a sigmoid vector, which will be an advantage against higher complexity solutions like L2X. I also assume the feature importance is selected by ranking the sum of the masks of all samples. However, is it fair. In my opinion, using the mean position of the feature ranking of each sample should be better. I would like the authors to clarify why they are using the proposed solution.\n\nQuality\n* Intuitively, different NN initialization could lead to permutations in the convolutional layer channels [1], or even rotations over the dense networks [2]. As a consequence, I find the claim that the average mask generator lead to poor results is expected. However, I am intrigued about the solution provided by the authors. Does the weight averaging really helps the solution? An ablation study regarding this point should be included.\n\n* Although the consistency of the author's solution compared to the state-of-the-art is remarkable, it does not clearly affect the results, as techniques like L2X can provide a slightly better accuracy in Table 1. Does the authors have an explanation to this phenomenon. Besides that, since they are using a synthetic algorithm, it is easy to now, for each instance, which is the feature that triggers the correct output. Thus, I suggest the authors to include an extra experiment by providing how many times the feature with the highest importance matches the one that triggers the solution.\n\n* I would also like to see how the algorithms behave when the solution depends on a combination of multiple features. Is the algorithm capable of cope with it?\n\nNovelty\n* The idea is similar to the L2X algorithm, but introducing some modifications. The spatial complexity seems to be reduced, but the computational cost should be increased by the need of training multiple instances of the neural network.\n\nReproducibility\n* All the information needed to reproduce the results appears to be included in the Appendix section.\n\n\n[1] Ainsworth, S. K., Hayase, J., & Srinivasa, S. (2022). Git re-basin: Merging models modulo permutation symmetries. arXiv preprint arXiv:2209.04836.\n\n[2] Weiler, M., Hamprecht, F. A., & Storath, M. (2018). Learning steerable filters for rotation equivariant cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 849-858).",
            "summary_of_the_review": "Although I think it is an interesting paper, it is very difficult to read in its actual form. Furthermore, some decisions require an extra explanation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper362/Reviewer_iffa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper362/Reviewer_iffa"
        ]
    },
    {
        "id": "wGz-ruZk0W",
        "original": null,
        "number": 2,
        "cdate": 1666581575900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581575900,
        "tmdate": 1666581575900,
        "tddate": null,
        "forum": "NYtq-CsRP3H",
        "replyto": "NYtq-CsRP3H",
        "invitation": "ICLR.cc/2023/Conference/Paper362/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a new method to rank feature importance in a neural network. The authors propose to train multiple shallow networks and take the average of the weights for the final ranking. The proposed method is evaluated in several datasets to prove its accuracy and stability. ",
            "strength_and_weaknesses": "Strength\n- The paper motivates the problem well, and presents the idea clearly.\n- Extensive numerical studies are provided to support the authors' claims.\n\nWeaknesses \nOverall I feel this paper is too \"empirical\" while it's not clearly supported in the paper why current design could be the best choice. \n- There are several techniques borrowed from previous work (subsetting features, random noise added, etc), but without a clear justification on whether they are needed (did not seem to find in the experiments).  Naively, maybe just to compare with an average of normally trained networks without all the added tweaks? \n- The authors mention that in deeper architecture the weight averaging does not work well. This raises a question on how to choose the side of the shallow networks in practice? Empirically I think there might be a sweet area for the size of shallow networks, but how do we find it and balance between the size of the network and the efficiency? \n\nMinor Issues\n- I found the method description especially the terminology on mask, classifier and encoder are a bit confusing (Maybe it's just because I am not familiar with this type of presentations).\n- On the robustness vs correctness of the feature importance, I agree with the authors that it's indeed hard to know the latter but is out of the scope of this paper. There are recent work on trees related to this which are missed from the citation (e.g., https://proceedings.neurips.cc/paper/2019/hash/702cafa3bb4c9c86e4a3b6834b45aedd-Abstract.html,  https://dl.acm.org/doi/pdf/10.1145/3429445) \n",
            "clarity,_quality,_novelty_and_reproducibility": "I feel some part of the paper could be more clear (see Minor Issues above)but overall it's of good quality. \n\n",
            "summary_of_the_review": "The paper introduces a new method to rank feature importance in a neural network based on weights averaging from multiple shallow networks. Being a purely empirical paper, I feel some choice in the paper is not well motivated and lack supporting evidence. I believe with some revision it may be published in ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper362/Reviewer_brop"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper362/Reviewer_brop"
        ]
    },
    {
        "id": "GY3DHOO7cY",
        "original": null,
        "number": 3,
        "cdate": 1666645201315,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645201315,
        "tmdate": 1666645201315,
        "tddate": null,
        "forum": "NYtq-CsRP3H",
        "replyto": "NYtq-CsRP3H",
        "invitation": "ICLR.cc/2023/Conference/Paper362/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an approach to estimate feature importance. Multiple shallow mask models are trained with different random seeds. These resulting weights and then averaged to produce a global model that can be inferior in accuracy but better identify the global feature importance. ",
            "strength_and_weaknesses": "I found the proposed approach interesting but very ad hoc. There are quite a few components such as feature masks (local and global), feature subsets, feature noise, encoder that generates an embedding for each feature subset and finally a classifier. It is unclear which of these components are needed to improve feature importance stability and which are for classification performance. It is also unclear what authors mean by \"global\" feature ranking. In my understanding feature importance is typically model specific. Tabular datasets often contain subsets of highly correlated features and typically multiple such subsets can be used to get accurate models. Artifacts such as random seed influence which subset (or subsets) the model converges to during training. This is commonly observed in xgboost where, when optimized under the \"hist\" setting and random seed, the trees tend to pick a subset of the correlated features. Averaging together models is likely going to spread importance over correlated features and I don't quite see how this \"global\" importance is useful.",
            "clarity,_quality,_novelty_and_reproducibility": "I found parts of the paper difficult to read. Authors refer to definitions such as generate_subsets(X_M) which are important but not defined the main paper and one has to hunt through the supplementary material to find them. I think the main paper should be as self contained as possible instead of constantly referring to the supplementary. Also, the full submission is 37 pages long which is excessive for a 9 page conference paper, if such length is needed I would consider a longer journal submission.",
            "summary_of_the_review": "I think the proposed approach is interesting but the paper needs a considerable revision to both justify the goal and each of the proposed components including a detailed ablation study.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper362/Reviewer_ZHfL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper362/Reviewer_ZHfL"
        ]
    },
    {
        "id": "7RJFallmyJA",
        "original": null,
        "number": 4,
        "cdate": 1666705651659,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666705651659,
        "tmdate": 1666705651659,
        "tddate": null,
        "forum": "NYtq-CsRP3H",
        "replyto": "NYtq-CsRP3H",
        "invitation": "ICLR.cc/2023/Conference/Paper362/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a novel method based on parameter averaging to estimate accurate and robust feature importance in tabular data setting. The proposed method first initializes and trains multiple instances of a shallow network (referred as local masks) with different random seeds for a downstream task and then obtains a global mask model by averaging the parameters of local masks. The experimental results show that although the parameter averaging might result in a global model with higher loss, it still leads to the discovery of the ground-truth feature importance more consistently than an individual model does.",
            "strength_and_weaknesses": "Strength: Experimental results are good.\nWeaknesses: There is no experimental validation on real task.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is novel, but no practical application validation.",
            "summary_of_the_review": "The proposed method is novel, but no practical application validation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper362/Reviewer_G9vP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper362/Reviewer_G9vP"
        ]
    }
]