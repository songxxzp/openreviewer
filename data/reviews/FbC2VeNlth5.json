[
    {
        "id": "erGdU5i1ZFY",
        "original": null,
        "number": 1,
        "cdate": 1666714930136,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714930136,
        "tmdate": 1666714930136,
        "tddate": null,
        "forum": "FbC2VeNlth5",
        "replyto": "FbC2VeNlth5",
        "invitation": "ICLR.cc/2023/Conference/Paper4187/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a  derivcate construction mechanims for logical formulae. Results are very good.",
            "strength_and_weaknesses": "This paper presents a theory for obtaining derivatves of logic programs Several researches worked on this, Domingo s and colleagues had a relatively simple model for mlns, Problog used Derivatives from BDDS, Last we have graph Rule construction: iit is kind of reminiscent of Tensorlog\n\nResults look good (including variance).\n\nIsues: What is novel here? \n\nAre you constructing a probabilistic system or just a NN?\nPlease explain more in detail how this work evolves over previous packages/",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity,  Mostly clear\nQuality, Loks good, especially in reviewing related work.\nNovelty I would like a more convincing argument of that\nReproducibkitty :ok",
            "summary_of_the_review": "\nMaybe my fault: there\u015b a lot of good suff here, but it didn;t entirely convince me, esoecially when comparing  to related work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4187/Reviewer_mi4W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4187/Reviewer_mi4W"
        ]
    },
    {
        "id": "vSAgU003x03",
        "original": null,
        "number": 2,
        "cdate": 1666715553927,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666715553927,
        "tmdate": 1666715553927,
        "tddate": null,
        "forum": "FbC2VeNlth5",
        "replyto": "FbC2VeNlth5",
        "invitation": "ICLR.cc/2023/Conference/Paper4187/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a differentiable logic programming (DLP) framework, which relaxes logical operations (e.g., AND, OR) in a rule with numerical operations (e.g., multiplication, addition). DLP can learn logical rules and weights. The key idea is to (recursively) generate logical rules by following a chain-like pattern (specified in Equation 1) with conjunction, disjunction, and negation. Candidate logical rules are associated with weights, which are learned through minimized a proxy loss. The empirical evaluation shows that DLP achieves similar or slightly better performance compared to a number of baselines. ",
            "strength_and_weaknesses": "Strengths: \n- this paper uses many recent baselines and three different kinds of datasets in the empirical evaluations\n- many performance ablation studies have been presented\n\nWeaknesses:\n- the writing is not easy to follow, particularly, heavy use of (undefined/unexplained) notations makes the reading quite challenging. For instance, proposition 4.2 is especially hard to understand. \n- the idea of relaxing logical operations as numerical operations and attaching logical rules with weights is well-known\n- using differentiable learning to discover useful logical rules is also not new (see the survey paper R1)\n- the so-called generative definition (Equation 1) is essentially a simple template, which may be more generally viewed as grammar. Then the construction of networks (or rule generation process) is a simple instance of the well-known Syntax-guided synthesis (SyGuS) in the programming language community (see R2). \n- the empirical evaluation shows very little improvement. Particularly, around 1% or less improvement on three knowledge graph completion datasets (i.e., Kinship, FB15k-237, WN18RR). \n\n[R1] Luc De Raedt, Sebastijan Dumancic, Robin Manhaeve, Giuseppe Marra: From Statistical Relational to Neuro-Symbolic Artificial Intelligence. IJCAI 2020: 4943-4950\n\n[R2] \tRajeev Alur, Rastislav Bod\u00edk, Eric Dallal, Dana Fisman, Pranav Garg, Garvit Juniwal, Hadas Kress-Gazit, P. Madhusudan, Milo M. K. Martin, Mukund Raghothaman, Shambwaditya Saha, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, Abhishek Udupa: Syntax-Guided Synthesis. Dependable Software Systems Engineering 2015: 1-25\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of this paper needs significant improvement, especially regarding notations and their meanings. The ideas presented in this paper have been explored by many recent papers, thus the novelty is little. There are extensive comparisons with many baselines, which is great, however, the empirical improvement is not significant at all. ",
            "summary_of_the_review": "This paper studies an important and interesting challenge in AI -- learning logical rules and weights. The approach of using numerical relaxation and simple template (or so-called generative definition in the paper) to facilitate rule learning is well-known. It's difficult to see the actual novelty besides an obscure way of formalization. And, the practical significance is unclear given the current evaluations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4187/Reviewer_kdMZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4187/Reviewer_kdMZ"
        ]
    },
    {
        "id": "G9pdljPWAJm",
        "original": null,
        "number": 3,
        "cdate": 1667098093139,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667098093139,
        "tmdate": 1667098093139,
        "tddate": null,
        "forum": "FbC2VeNlth5",
        "replyto": "FbC2VeNlth5",
        "invitation": "ICLR.cc/2023/Conference/Paper4187/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a differentiable logic programming framework that aims to learn first-order logic structures and weights. This framework further supports probabilistic reasoning by performing probabilistic forward chaining. Some theoretical guarantees on error rate and solution optimality are shown. Empirical evaluations of the proposed framework are carried out in various tasks including inductive logic programming, systematic reasoning and knowledge graph completion.",
            "strength_and_weaknesses": "This work aims to bridge between differentiable programming and symbolic reasoning which has been an important research topic. However, here are a few concerns/confusions when I go through this work:\n\n- Figure 1 is very helpful for understanding the differentiable logic program. Still, it seems that the structure of the learned is not arbitrary since it depends on how the network is defined. Can the authors further illustrate how expressive are the learned logic rules and what are the limitations?\n- Also, I wonder how are the edges are chosen in the example in Figure 1. For example, why \\phi_2 choose the edge connected to Red but not the edge connected to Blue, is there some threshold for for the weights to decide which edge results in the learned logic rule? It is not clear in Sec. 3 or Sec. 4 how this is achieved.\n- For Prop. 4.2, it is unclear to me what these assumptions mean. Specifically, for the assumption (1), the notation is confusing that on the left hand side the x seems to be a variable while on the left hand side x seems to be taken over all possible assignments to the variables. For assumption (2), it is unclear to me what does \"remove all queries Q(x) = 1\" and \"proved true by previous ...\" mean. The authors might want to provide a formal description here to improve readability.\n- In Equation (7), what are the alpha's? Why do we need alpha and how are alpha chosen? It does not seem that they are learnable parameters.\n- The empirical evaluation is extensive. Still, it is not obvious that the proposed framework is more advantageous over RNNLogic+ in Table 3.\n\nMinor issues:\n- in Preliminary, the same notation \\mathcal{V} is used for bot the set of constants and also the set of variables.\n- at the first line of Equation (1), should it be exist y instead of exist z?\n- \\sigma in Equation (8) is not defined.",
            "clarity,_quality,_novelty_and_reproducibility": "Some of the technical details are unclear to me and the readability of this work needs to be further improved.\n",
            "summary_of_the_review": "My main concern is the readability of this work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4187/Reviewer_qAry"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4187/Reviewer_qAry"
        ]
    },
    {
        "id": "LcUeEGJkpw",
        "original": null,
        "number": 4,
        "cdate": 1667543767800,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667543767800,
        "tmdate": 1667543767800,
        "tddate": null,
        "forum": "FbC2VeNlth5",
        "replyto": "FbC2VeNlth5",
        "invitation": "ICLR.cc/2023/Conference/Paper4187/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method for an ILP-style rule-learning problem by using gradient descent over differentiable approximations to the various connectives in the language under consideration. The method is empirically tested on a suite of ILP tasks, the CLUTRR benchmark, and some knowledge graph completion tasks.",
            "strength_and_weaknesses": "The main strength is that the proposed method seems to do well on CLUTRR and relatively well on the knowledge graph completion tasks, compared to the handful of chosen baselines for each task.\n\nThere are some significant weaknesses, however. The main weakness is that the presentation of the method is essentially unreadable and disturbingly vague at places. \n\nTo begin with, the problem statement is unclear: the input data is described as both a set of FOL statements and as a set of triples. It is not clear to me if the FOL statements are supposed to be ground atoms (as they are in the example) or contain free variables as the notation suggests, or if they can be arbitrary FOL formulas. The tasks \"Recognition of Logical Patterns\" and \"Probabilistic Reasoning\" are only described in rather vague terms: what does it mean to \"explain how the answers are deduced?\" What does it mean to \"make decisions [some expression] based on the groundings of classifiers\"? \n\nIn Proposition 4.2, what is the \"precision on B\"? In the formulas there, for the sums indexed by x, what does x range over? What does it mean to \"remove all queries Q(x) = 1 and also proved true\"? What is the \"prediction model\" being constructed -- what does this notation with p_w mean? Apparently after learning such models, we \"remove or reduce the weights of data instances where Q(x) = phi_i(x) = 1\" -- what are these weights and which is it, remove or reduce them, and by how much?\n\nIn Equations 6 & 7, it seems like some strange vector notation is being used, but the dimensions don't seem to work and I'm not certain I know what is meant by raising the vector to the power alpha.\n\nIn the section on inference, I still don't know what x ranges over; are we looking at all possible groundings? The function \"sigma\" is not defined. Q_0 is also not defined. \n\nThe description of constructing the networks is vague. Equation 7 gives a correspondence between connectives and the differentiable approximations; how do we obtain \"output LPs\" from these definitions? What does it mean for \"another LP\" to be \"needed\"? We repeat the procedure \"a couple of times\" -- meaning two? or just some arbitrary constant number?\n\nIn the learning section, Proposition 4.3, how do you decide when a node should be fixed? What does it mean for the solution to be \"near-optimal\"?  Needless to say, I don't see why the vague construction procedure satisfies these constraints, in particular the constraint of having a tree topology on the unfixed nodes. Note that the network in Figure 1 definitely does not have a tree topology. Actually, this raises a concern, that to satisfy this requirement, the method might need to explicitly represent the tree of possible logical expressions. Needless to say, this would not seem to offer any computational advantage over traditional ILP methods.\n\nMoving on, in the experiments, there are a few fishy features. For one, the authors chose to write their numbers in boldface when there are other methods that seem to have essentially equivalent performance as far as I can tell. For some reason the authors give their method's numbers to three decimal places when the stated accuracy clearly only warrants two. Also, for some reason the knowledge graph completion tasks are only examined on subsets of the baselines for unknown reasons. \n\nUltimately, my main concern with the experiments is that the vagueness of the description of the method may be covering some kind of handcrafting of solutions for the various benchmark problems. At several points, e.g., the definition of the \"update\" function, the choice of structure, etc., the text suggests that there are many possible choices. If these choices are made by the experimenter individually per benchmark, it calls the validity of the results into question.",
            "clarity,_quality,_novelty_and_reproducibility": "As described above, the work is extremely unclear and I don't think I can reproduce it. It's difficult to pin down novelty without some clarity on what has been done; certainly differentiable approximations to logical operators have been proposed previously, so at that level of detail, the work is not novel.",
            "summary_of_the_review": "The presentation of the proposed method is too vague to be useful. At times, the presentation seems to deliberately avoid committing to details of the method. This also raises questions about the validity of the experiments, specifically whether or not a single, common algorithm is being evaluated.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4187/Reviewer_qPBm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4187/Reviewer_qPBm"
        ]
    }
]