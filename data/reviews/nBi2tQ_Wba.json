[
    {
        "id": "eXrs14AxUD",
        "original": null,
        "number": 1,
        "cdate": 1666618796585,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666618796585,
        "tmdate": 1666618796585,
        "tddate": null,
        "forum": "nBi2tQ_Wba",
        "replyto": "nBi2tQ_Wba",
        "invitation": "ICLR.cc/2023/Conference/Paper5273/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers two approaches typically used in comparing deep\nnets and brain recordings, linear regression and CKA, and explores\ntheir behavior when used to measure the similarity of different neural\nnetworks. This is used to highlight under which conditions (and in\nwhat sense) these methods allow to identify common patterns between\ninformation processing systems.",
            "strength_and_weaknesses": "## Strengths\n1. The idea of this paper will be of interest to the iclr\n   community. Regardless of the issues I have with some of the\n   conceptual framing of this paper (see below, point 1 in weaknesses), I\n   believe it is useful to get a rough sense of what one can expect even in a best case scenario, when using these similarity/alignment metrics.\n2. The range of networks analyzed seems appropriate and the results\n   are reported in a clear way.\n3. The paper conclusions are given in a balanced way, presenting both\n   a \"glass half full\" and a \"glass half empty\" perspective.\n\n## Weaknesses\n1. Several passages in the text seem to imply that comparing deep net\n   and brain representations of certain stimuli must be the same as\n   testing the hypothesis that information processing in the brain\n   follows the same patterns/motifs as the deep net. While this is\n   certainly true of some existing works in the literature, it is not\n   true of all of them. Indeed, one could appreciate having a good\n   model of neural activations even from a purely phenomenological\n   perspective. Just to make one example, in the Bashivan paper cited\n   among the references, the deep net allows the experimenters to\n   design stimuli that can drive target neurons very strongly. This\n   would still have value even if the model brought no insight\n   whatsoever about information processing in visual cortex, as it\n   would enable new kinds of experiments. Now, I understand that the\n   present paper focuses on system identification (so exactly the\n   \"hypothesis testing\" scenario), but it would be good to mention\n   somewhere that system identification is not the only possible\n   motivation behind modeling the ventral stream/brain circuits using\n   deep networks.\n2. The value of \u03bb (the regularization parameter for the linear\n   regression) is fixed at a hand-picked value, rather than selected\n   in a systematic way, for instance by cross-validation. Other values\n   of \u03bb are explored in the supplement. This seems like a suboptimal\n   way of performing a systems identification analysis. The results of\n   the paper would be strengthened if \u03bb was not picked by hand.\n3. I found it interesting to learn how identification reliability\n   increases with increasing population size. The main analyses were\n   done with populations of 3000 neurons, an arbitrary number. It is\n   possible that using a larger number (still compatible with the\n   upper limits of what is possible experimentally) would have yielded\n   somewhat different results. This is especially true for some of the\n   experiments that are already borderline, like the results in figure\n   2, where a lot of emphasis is given to the fact that 2 layers out\n   of 5 in one network out of five tested do not have the ground-truth\n   model as the best match. In this light, it would be good to warn\n   the reader up front, when specifying the size of the populations\n   used (page 3, top) that the results are dependent on this choice.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow. The research question is\nreasonably novel, and I couldn't find any inconsistency or\ntechnical/correctness issue. The experimental procedures are\nadequately described for reproducibility.\n\nOne minor issue I have found with the presentation quality is that the\nlabels and text on the plots is way too small. Please make sure that\nthe text is readable even when the document is visualized at its\n\"intended\" dimension (as if printed on regular paper).\n\n*Typos:*\n- page 3, paragraph below equation 2: \"principle components\" (should\n  be \"principal\").",
            "summary_of_the_review": "This paper provides some data about the behavior of common\nsimilarity/alignment measures used to compare deep nets and neural\nrecordings. Although descriptive in nature, it will provide a useful\nreference for a sizeable subcommunity at iclr.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5273/Reviewer_eR5e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5273/Reviewer_eR5e"
        ]
    },
    {
        "id": "8GnyzTVx-7J",
        "original": null,
        "number": 2,
        "cdate": 1666628510714,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628510714,
        "tmdate": 1666795678006,
        "tddate": null,
        "forum": "nBi2tQ_Wba",
        "replyto": "nBi2tQ_Wba",
        "invitation": "ICLR.cc/2023/Conference/Paper5273/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work takes a closer look at the prevalent tools in computational cognitive neuroscience today that are used to relate representations of deep learning models to representations in the brain. These tools are encoding models and a form of representational similarity analysis (CKA). The main question in this work goes along one of the several directions in this increasingly popular research area, which is to compare different deep learning architectures in terms of their alignment with brain recordings and conclude that the best performing one must be closer to the actual brain system. This specific work examines whether these comp neuro tools are indeed able to identify the correct target system, in the case that the target system is known. For this purpose, the work investigates the alignment (both in terms of regression scores and representational similarity) among various deep learning models. It reports that even though both encoding models and CKA are able to identify the correct system in most cases, the differences between the correct and incorrect source systems is small. Furthermore, the work also examines the effect of different types of images used for evaluating the alignment and finds that more naturalistic images lead to higher identifiability.",
            "strength_and_weaknesses": "Disclaimer: I reviewed a very similar version of this work at NeurIPS 2022. The authors have addressed some of my concerns in the current work but several of my major concerns remain and I will summarize these below, along with some new ones in this version.\n\nStrengths:\n- Investigates a problem that is becoming increasingly important as the popularity of the research area of relating representations in machines and brains grows\n- The experiment that examines the effect of different types of images used for evaluating the alignment is novel as far as I know, and important for this subfield.\n\nWeaknesses:\nMajor:\n1. The ridge regularization parameter is fixed at 1. This parameter needs to be optimized with nested cross-validation in order to get a fairer estimate of the generalization performance. This is important since one of the points the authors are trying to make is that CKA is (slightly) better than ridge regression at system identification, and also that the difference between the performances for the correct and incorrect systems is small.\n2. The takeaways from the work are not clear enough. The results suggest that system identification is possible in most cases with the current approaches in an ideal setting. How generalizable are these results to neural recordings? The authors can do more to investigate this question since it is the main motivation behind the work. In addition, the work tries to interpret the results pessimistically (though this is toned-down a lot from the previous version), but this further muddles the message. It is furthermore not clear whether any significance testing was done for the results (e.g. \"However, for some target layers in AlexNet and ResNet18, although the layer with the highest score may be the matching layer in the same architecture, linear regression scores for other source models do not show a significant decrease in predictivity\" is this statement based on a significance test?)\n3. All of the figures are illegible due to size, small fonts, and blur.\n\nMinor:\n- \"Following the procedure developed by previous works (Schrimpf et al 2020, Yasmins et al 2014, Conwell et al 2021)..\" The procedure for training encoding models for predicting brain recordings dates back much further than the cited works. See Kay et al. 2008 Nature (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3556484/), and Mitchell et al. 2008 Science (https://pubmed.ncbi.nlm.nih.gov/18511683/).\n\nSuggestion:\n- \"One potential interpretation of the result would be that different neural network architectures are equally good (or bad) models of the visual cortex. An alternative explanation would be that the method we use to compare models with the brain has limitations in identifying precise computational operations. To test this hypothesis,..\" -- Yet another alternative explanation (which I believe is actually more likely) is that even though different source models explain similar percent variance in the target model, this variance may be different from model to model (especially when comparing different layers of models). It is not uncommon nowadays to do a variance partitioning analysis (both for encoding models and for RSA/CKA) to better understand the contributions to predictive performance. One suggestion is to partition the explained variance in the target system by the different candidate source systems, which may provide more insight into why different source systems achieve good prediction performance for the same target system.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, and reproducibility are solid. \nThe approach is not inherently novel (it is close to Kornblith, Norouzi, Lee & Hinton (2019)), but the significance can be improved by performing additional analyses that better reveal the reasons for the limitations of the current methods.",
            "summary_of_the_review": "While this work investigates an important and timely problem, it does not yet go deep enough to warrant a top-tier ML publication in my opinion. There are several good directions that this work has started (e.g. investigate the effect of training data distribution of the source and possibly target domain) and I encourage the authors to further pursue them.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5273/Reviewer_ngXd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5273/Reviewer_ngXd"
        ]
    },
    {
        "id": "sQdnJa9lY9",
        "original": null,
        "number": 3,
        "cdate": 1666682807843,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682807843,
        "tmdate": 1666682807843,
        "tddate": null,
        "forum": "nBi2tQ_Wba",
        "replyto": "nBi2tQ_Wba",
        "invitation": "ICLR.cc/2023/Conference/Paper5273/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper challenges the interpretability of common neural recording analysis methods with deep artificial networks, by simulating such methods on artificial networks themselves. First, it shows that several architectures with totally different components are very similar in commonly used prediction measures when tested on a neural recording benchmark data set. Second, it shows that these common measures can not distinguish the ground truth even when it is an artificial neural network. Importantly, this simulation is done with a sample size of target neurons analogous to neuroscience experiments. Moreover, the authors show high variability in results when the stimulus type (family of the input images) changes. The authors briefly suggested two ways to overcome the interpretability issue, i.e. using more natural stimuli, and recording from more neurons. ",
            "strength_and_weaknesses": "Strength: This paper tackles an important problem in computational neuroscience and raises a very concerning issue regarding using deep networks for (neuro)science. It is clearly written and gives a nice overview of the literature on using neural networks for neuroscience. \n\nWeakness: \nMy main concern is that the authors did not really provide a solution (or proof that there is none) for the raised issue. It is also not clear that the issues arise because of the measurements themselves or the authors' specific statistical choices such as reporting the median for correlations. I think a more in-depth analysis is needed to flesh out issues of using artificial architectures in neural recording analysis. For example, I think it is necessary to show how the measures and conclusions change with the increase in neural sample size, especially because increasing neural recording size is suggested as one of the solutions. In addition, as mentioned in the paper there exist several measures in these types of analysis. Given that the experiments are all simulations on very common networks, I think trying other measures such as SVCCA and SVD is necessary (see \"SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability\" by Raghu et al, NeurIPS 2017 and \"insights on representational similarity in neural networks with canonical correlation\" by Morcos et al, NeurIPS 2018). \nFinally, recurrent networks should be examined too especially because of feedback connections in the visual cortex. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the architecture parameters are fully reported. While the paper has some novel aspects, this type of work has been already done, and new contributions need more extensive examinations (e.g. see the references above). \n\n",
            "summary_of_the_review": "The paper is clear and tackles an interesting problem but needs further experiments given the history of this type of work in the DL community. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5273/Reviewer_RWEG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5273/Reviewer_RWEG"
        ]
    }
]