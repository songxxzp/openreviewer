[
    {
        "id": "Ge707hdbOZj",
        "original": null,
        "number": 1,
        "cdate": 1666459345035,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666459345035,
        "tmdate": 1666459345035,
        "tddate": null,
        "forum": "HUsh1c7p0gc",
        "replyto": "HUsh1c7p0gc",
        "invitation": "ICLR.cc/2023/Conference/Paper1365/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This submission proposed a new plug-in module which decomposes the 3D vision attention in video modeling. The proposed module, T2D, decomposes a self-attention among 3D spaces (spatial: XY and temporal: T) into three 2D space attention: XY, XT and YT. The proposed idea is very similar to what has been shown in R(2+1)D model and TimesFormer. The performance has been validated on different datasets including Kinetics 400, SomethingSomething v2, etc.",
            "strength_and_weaknesses": "Strength:\n1. The proposed method has been validated on different datasets and the experiments are very comprehensive to validate the performance of T2D\n\nWeakness\n1. The idea of decomposing 3D attention into different planes of 2D attention is not novel, it has been proposed and experimented in R(2+1)D (cnn based architecture) and TimeSformer (transformer based architecture). The decomposition of T2D is very similar to axial decomposition in TimeSformer, which undermines the novelty of this proposed module\n\n2. The performance on different datasets shows the introduction of T2D only leads to very marginal gain. For example, on Kinetics 400 and SSv2 datasets, T2D compared to regular 2D+1D, the performance gain is 0.4 and 0.3 respectively. Comparing proposed method with current SoTA method, when using pretrained CLIP-L ViT, the proposed method only achieved 0.0 and 0.3 in top1 and top3 accuracy, under similar GFLOPS cost.",
            "clarity,_quality,_novelty_and_reproducibility": "This proposed method original and writing is good. However, the novelty is very marginal.",
            "summary_of_the_review": "Based on the reviews in weakness part, I am prone to reject the submission.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1365/Reviewer_LYJp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1365/Reviewer_LYJp"
        ]
    },
    {
        "id": "wflxs1Ggw1",
        "original": null,
        "number": 2,
        "cdate": 1666586720896,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666586720896,
        "tmdate": 1666586720896,
        "tddate": null,
        "forum": "HUsh1c7p0gc",
        "replyto": "HUsh1c7p0gc",
        "invitation": "ICLR.cc/2023/Conference/Paper1365/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes triple 2D decomposition (T2D) of a 3D vision transformer for spatiotemporal feature learning. It is achieved by decomposing the 3D representation into three 2D representations, e.g. XY, YT, XT. The isolated self-attention operation on three 2D representations improves the model performance on widely-used datasets. \n\nDespite the performance improvements, this paper has the following weakness and shortcomings, which lead me to give a weak rejection.\n1.\tThe decomposition operation is a bit too simple, and the improvement is not obvious from the results of the ablation study in Table2 compared with 2D+1D.\n2.\tI doubt the core difference between XT+YT in T2D and +1D in 2D+1D. The former will introduce more computation but bring little benefit.\n3.\tThe decomposition operation does not bring efficient inference speed, nor does it greatly reduce the complexity. \n4.\tThe ablation study on the combination of XY+XT, XY+YT, XY+XT+YT, etc. is missing. I think XY+XT or XY+YT contains enough temporal information for feature extraction. \n5.\tI think this decomposition operation applies to any vision network, either CNN or transformer. As the results in Table5, the performance improvement comes more from the network structure and pre-training, rather than the proposed method.\n",
            "strength_and_weaknesses": "Please see summary",
            "clarity,_quality,_novelty_and_reproducibility": "Please see summary",
            "summary_of_the_review": "Please see summary",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1365/Reviewer_LefE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1365/Reviewer_LefE"
        ]
    },
    {
        "id": "a-LtRP-0nL",
        "original": null,
        "number": 3,
        "cdate": 1666661037311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661037311,
        "tmdate": 1666661037311,
        "tddate": null,
        "forum": "HUsh1c7p0gc",
        "replyto": "HUsh1c7p0gc",
        "invitation": "ICLR.cc/2023/Conference/Paper1365/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed to decompose video data (3D, space 2D + time 1D) into 3 data planes. Each of the data plane is only 2D. \n\nThis allows the T2D model to create tokens in 3 2D planes separately. ViT 3D self-attention is decomposed along each plane. Compared with a single 3D self-attention, using the three 2D self-attention is less computationally expensive. The weights of the ViT's self-attentions are tied for the two 2D planes w/ time dimension.\n\n\n\n",
            "strength_and_weaknesses": "Strength\n- Decomposing ViT's 3D self-attention along three 2D plane is novel. It reduced computation complexity. This exact decomposition has not been explored before (see Fig 3), though similar to the \"2D + 1D attention\", and the \"Axis attention\".\n- The authors show this simple model design can achieve competitive evaluation accuracy on video action recognition datasest, if not better.\n    -- higher or competitive performance on Kinetics-400 and Something-Something-v2 benchmarks \n    -- significantly improves SOTA accuracy on Diving-48, Gym99, and Gym288\n- comprehensive ablation studies\n\nWeakness\n- It's not entirely clear by reading the abstract, the author decomposes the self-attention in 3D ViT. I suggest revise the abstract to make it clearer. Or consider move the Fig 2 to the first page.\n- The improvement from 2D + 1D to T2D is not that significant (Table 2). Which is as expected - somewhat incremental.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is well written and easy to follow. The method is novel and the experiments are comprehensive. Results are good -- improved upon SoTA.",
            "summary_of_the_review": "The authors proposed an atomic and novel decomposition of ViT 3D self-attention along each 2D planes (XY, XT, and YT). The authors conducted extensive experiments to show this approach has advantage over existing approaches. The authors tuned the method well, so that it improved upon SoTA accuracy on a few popular action recognition datasets.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1365/Reviewer_NtS5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1365/Reviewer_NtS5"
        ]
    },
    {
        "id": "9D93G7amTqk",
        "original": null,
        "number": 4,
        "cdate": 1667462237917,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667462237917,
        "tmdate": 1667462237917,
        "tddate": null,
        "forum": "HUsh1c7p0gc",
        "replyto": "HUsh1c7p0gc",
        "invitation": "ICLR.cc/2023/Conference/Paper1365/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the action recognition problem from 3D video input. The core idea of the manuscript is to propose a triple 2D decomposition (T2D) for 3D vision transformer, which groups 3D video tokens along XY,XT and TY axis for self-attention computation. To address the memory cost issue in stander transformer models, the authors propose to extract spatial and spatiotemporal features from the 2D slices and fuse the attention feature for the final prediction. Extensive experiments on multiple datasets demonstrate that the proposed method is able to improve the recognition accuracy while greatly reducing the computation complexity. ",
            "strength_and_weaknesses": "What's good:\n1) First, the motivation of this paper is clear and intuitively effective. The idea of learning attention features by tri-plane modeling is novel and interesting.\n2) The idea of triple 2D decomposition seems general and can be extended to related tasks.\n3) Fig. 3 is really helpful for understanding the decomposition.\n4) The proposed method shows better quantitative scores than the previous approach.\n\nTo be improved:\n1) For Eq. 1, what does \"+\" means? Sum or concatenate?\n2\uff09Lacking the reference, discussion on previous triplane representation, and tensor decomposition methods. For example EG3D, TensoRF.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writing should be improved\nNovelty: Yes as commented above\nReproducibility: not sure",
            "summary_of_the_review": "The proposed triple 2D decomposition for action recognition from videos is novel and interesting to me, but the paper lacks a detailed discussion on the branch of decomposition work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1365/Reviewer_Z88y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1365/Reviewer_Z88y"
        ]
    }
]