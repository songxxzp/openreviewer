[
    {
        "id": "smp2_Kf_bgA",
        "original": null,
        "number": 1,
        "cdate": 1666547742054,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666547742054,
        "tmdate": 1666547742054,
        "tddate": null,
        "forum": "lXMlDL78Alx",
        "replyto": "lXMlDL78Alx",
        "invitation": "ICLR.cc/2023/Conference/Paper4363/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a novel mechanism of attention to train neural networks to identify the coupling parameters of complex dynamical system under small perturbations. The problem belongs to the space of dynamical systems with strongly dissipative elements with small coupling strength. The idea is inspired by Schriber, 2000 to use transfer entropy to measure uncertainty in the causal relationships between variables. The results shown large improvements respect to other methods. And the attention variables shown in Fig. 3 appear to capture well the state-space volumes where external perturbations from other neurons/elements are expected to have larger impact. When the neurons are near the resting state perturbations are quickly erased, while exiting the saddles through the unstable separatrix amplifies the perturbation effect and the signal-to-noise ratio is expected to be more favorable. So, the method makes quite a lot of sense to me.\n\nMy recommendation to the authors is to present the limitations of this type of methodologies right after the contributions in the introduction. So, a new reader can quickly understand where and how this methodology can be applied. One of the limitations is the degradation of performance as the degree increases. It\u2019s certainly problem dependent but it can be useful to know in what problem one can potentially apply this methodology. Another limitation is for dynamical systems with strong perturbations when, for example, synchronization phenomena dominate the system dynamics. The authors also show in the appendix that the method is quite sensitive to noise, some something should be written in this regard. It also appears to me that there\u2019s quite a lot of effort setting up the right balance between the learning rates and the attention variable such that the cross-entropy loss leads to good generalization. Having guardrails on how to set the balance between the a-values and the cross-entropy loss would facilitate extending this method to other contexts. \n\nAnother more specific comment that I wish the authors could have addressed is the intended consequences of the mechanism of attention in the context of stable and unstable (saddles) fixed points in dissipative dynamical systems. The fixed points and their separatrices form the skeleton of trajectories and the saddles are the escape routes away of a resting state. Moreover, the neuron models the authors can have bistability and another neuron can flip the child one from resting state to the limit cycle, for example. Once in the limit cycle there will be very tiny modifications on the frequency of oscillations, but the pulse that switched the neuron away from the resting state is short lived. So, if one understands the bifurcations of the individual neuron dynamics, then we can infer where in the state-space external perturbations will lead to the largest information gains and, perhaps, we do not need the attention mechanism.\n  \nMinor comments:\nIn the appendix, injecting noise in a dissipative dynamical system is not equivalent as adding 10% noise to the time series. The noise gets integrated through any SDE solver. It leads to compression of the noise in the highly dissipative regimes and gets amplified nearby the unstable saddles. So, it does work in a similar way as the attention mechanism (or works against it), because it can also be used to identify the state-space volumes where the entropy gets amplified. \n\n\nIntro: Replace the rhetorical question: \u201cWhy are information from parent ..?\u201d by a direct sentence.\n\nATE is commonly used as average treatment effect in classic causal effect estimations. So, it may lead to confusion when people from other fields read this paper in the context of causality.\n\n\n\n",
            "strength_and_weaknesses": "My recommendation to the authors is to present the limitations of this type of methodologies right after the contributions in the introduction. So, a new reader can quickly understand where and how this methodology can be applied. One of the limitations is the degradation of performance as the degree increases. It\u2019s certainly problem dependent but it can be useful to know in what problem one can potentially apply this methodology. Another limitation is for dynamical systems with strong perturbations when, for example, synchronization phenomena dominate the system dynamics. The authors also show in the appendix that the method is quite sensitive to noise, some something should be written in this regard. It also appears to me that there\u2019s quite a lot of effort setting up the right balance between the learning rates and the attention variable such that the cross-entropy loss leads to good generalization. Having guardrails on how to set the balance between the a-values and the cross-entropy loss would facilitate extending this method to other contexts. ",
            "clarity,_quality,_novelty_and_reproducibility": "Good written paper.",
            "summary_of_the_review": "Novel contribution well supported by plenty of experiments and extensive appendix.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4363/Reviewer_Ube5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4363/Reviewer_Ube5"
        ]
    },
    {
        "id": "OqyE5TB5m0",
        "original": null,
        "number": 2,
        "cdate": 1666645949203,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645949203,
        "tmdate": 1666645949203,
        "tddate": null,
        "forum": "lXMlDL78Alx",
        "replyto": "lXMlDL78Alx",
        "invitation": "ICLR.cc/2023/Conference/Paper4363/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a causal attention model to identify the causal effects in the time series data. This model is trained by maximizing the Attention-extended Transfer Entropy. Beyond the causal attention model, a binary classification module is introduced to mitigate the negative effect of noises. The experiments on both synthetic and real datasets show good performance.\n",
            "strength_and_weaknesses": "Strength:\n1) Identifying causal relations of time-series data plays an important role in AI applications, such as science and human-robot interaction.\n2) Compared with the original methods using transfer entropy to detect causal-effect relations, this paper learns a reweighted transfer entropy to refine the construction.\n3)  This method shows great performance in the experiments. \nWeaknesses:\n1) \"Causal attention\" is not accurate. This paper applies a reweighted transfer entropy to promote the construction but does not learn an attention model for causal structure prediction. It is suggested to modify the title and claims in the paper. For example, detect causal effects in time-series data with reweighted/attentive transfer entropy.\n2) What is the advantage of reweighting? The insight into using reweight is not clear. Why add a few parameters to reweight can improve the performance dreamily? \n3) Is the model identifiable? If yes, it is better to give corresponding proof and assumptions.\n4) Could the author provide a detailed ablation study? It seems to be important to identify which component is the key to performance improvement.\n5) Could this method be used for i.i.d data? If not, the boundary of the method should be clarified. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:  The method part (the deriving) is clear. But the insight into the method is unclear. Besides the title and some claims should be refined.\n\nNovelty.  The difference between the proposed method and the original TE method is clear.   The performance improvement is significant with this simple modification. But more details should be explained, such as why this method works well.\n\nReproducibility: code is attached. ",
            "summary_of_the_review": "Overall, after balancing the positive and negative points, I think this paper needs to be further polished since many details need to be explained.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4363/Reviewer_pNYJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4363/Reviewer_pNYJ"
        ]
    },
    {
        "id": "JRiPfPEdvQF",
        "original": null,
        "number": 3,
        "cdate": 1666683221047,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683221047,
        "tmdate": 1666683221047,
        "tddate": null,
        "forum": "lXMlDL78Alx",
        "replyto": "lXMlDL78Alx",
        "invitation": "ICLR.cc/2023/Conference/Paper4363/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies a cyclic causal model where values of every variable $X_i$ at time step t are decided by a differential equation $\\frac{d x_i}{d x}  = g(x_i) + \\sum_{j} B_{ij}f(x_i, x_j)$. The learner's goal is to recover the structural functions $f$ and $g$ and the coefficients $B_{ij}$ from the observational data. The authors propose a new loss function for this learning task, called attention-extended transfer entropy, which encourages certain conditional dependence in the model. Finally, simulations were performed on both synthetic and dynamical models to validate the proposed approach. ",
            "strength_and_weaknesses": "This paper may have some interesting ideas. The definition of attention-extended transfer entropy (ATE) seems intriguing. However, due to the lack of clarity, it is unclear how this paper contributes to the existing literature and how novel the proposed method is. I will elaborate on the following.\n\nFirst, the authors claim that \"our task is to infer causal relationships between observed variables based on time series data and reconstruct the causal network connecting large numbers of these variables.\" However, this inference task is never formally defined. What is the performance measure of the learning task? How does one measure the quality of the reconstructed causal networks? Should we measure the L1 / L2 distance of the learned parameters with the actual parameters of the underlying model, or should we measure of the divergence between simulated and observed samples? Unfortunately, I tried to read through the paper but could not find answers.\n\nSecond, the paper describes the proposed algorithm. However, it does not perform any analysis of the algorithm's theoretical guarantee, concentration properties, and sample complexity. It is unclear how the proposed algorithm improves the existing baseline. To support the proposed method, the authors have to resort to empirical evaluation.\n\nAs for the experiments, I appreciate the authors' efforts in including various synthetic and dynamical models. Unfortunately, the clarity issue remains. For instance, the authors state \"compared with the baselines, our method usually substantially improves\nreconstruction performance on both synthetic and real causal networks, as shown in Figure 4.\" Again, it is unclear how the reconstruction performance is measured here. Without such information, it is hard to evaluate and compare the proposed method with the existing baseline.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see my response above.",
            "summary_of_the_review": "This paper studies a cyclic causal model where values of every variable $X_i$ at time step t are decided by a differential equation $\\frac{d x_i}{d x}  = g(x_i) + \\sum_{j} B_{ij}f(x_i, x_j)$. The learner's goal is to recover the structural functions $f$ and $g$ and the coefficients $B_{ij}$ from the observational data. The authors propose a new loss function for this learning task, called attention-extended transfer entropy, which encourages certain conditional dependence in the model. However, this paper provides little theoretical guarantee for the proposed method. Due to the lack of clarity, it is unclear how this paper contributes to the existing literature and how novel the proposed method is. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4363/Reviewer_F8Hs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4363/Reviewer_F8Hs"
        ]
    }
]