[
    {
        "id": "MrteuxDYd7",
        "original": null,
        "number": 1,
        "cdate": 1666356637155,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666356637155,
        "tmdate": 1666356637155,
        "tddate": null,
        "forum": "HehY2ZX2Cz",
        "replyto": "HehY2ZX2Cz",
        "invitation": "ICLR.cc/2023/Conference/Paper589/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies variations on the Frechet Inception Distance $d_{FID}$.\nFirst, the authors show how we can implement between two datasets by using\nmore numerically stable eigenvalue subroutines instead of a full\nmatrix square root method. This leverages the fact that the covariance matrices\nfor the two sets of samples is PSD. This observation leads to practical running\ntime improvements. Second, the authors then propose the \"Sorted eigenvalue\ncomparison\" distance, which is motivated by FID when the two covariance matrices\nare diagonalized by the same orthgonal matrix $Q$. The remainder of this work\nexplore the implications of using $d_{Eig}$ more broadly.",
            "strength_and_weaknesses": "**Strengths:**\n- The Frechet inception distance (FID) is widely used.\n  Heusel et al., (NeurIPS 2017) proposed using FID for GANS, which has kicked off a massive amount of subsequent research on distances between distributions that are useful in practice.\n- The derivation of $d_{Eig}$ as an alternative to $d_{FID}$ is a nice contribution and could be studied more deeply.\n\n**Weaknesses:**\n- When introducing FID, it would also be valuable to draw connections to the\n  Wasserstein distance-based definition (and equivalently couplings).\n- The runtime reductions of 25% and 90% need more context (e.g., does this come\n  from an improve constant factor in the running time, or is it an artifact of\n  your data/implementation/machine?).\n- A more detailed discussion about the differences between `scipy.linalg.sqrtm`\n  and `np.linalg.eigvals` under the hood is needed? Don't both compute the Schur decomposition?\n- Paper organization: Introduce all notation in a single section. Splitting\n  between the beginning of Section 2 and Section 3 is non-standard. I also\n  recommend putting all empirical results at the end of the paper.\n- Figure 2a is a good experiment, but I think these plots would be easier to\n  parse if the dimension increased from left to right, and if the number of\n  samples on the x-axis increased from left to right (i.e., reverse everything).\n  Moreover, I think language like \"Approximation quality\" or \"Numerical stability\"\n  would be more helpful than \"Oracle comparison\".\n- [page 6] The paper could benefit from a couple more sentences between Theorem\n  10 about why the stability condition in Bao et al. (2015) is reasonable.\n- The paper could benefit from fewer experiments, where each\n  experiment has a little more substance and deeper theoretical backing (or\n  connections to experiments in other related works).\n\n**Suggestions:**\n- [page 1] The notation $\\lambda^{(1)}_{j}$ would better differentiate $S_1$ and $S_2$.\n- [page 2] Suggestion: \"the proposed $d_{FID}$ section\" --> use \\Cref{} to give actual section.\n- [page 2] It would be useful to explicitly say $S_{i} \\in \\mathbb{R}^{p \\times p}$ to\n  make the orientation of your vectors clear.\n- [page 3] suggestion: Add citation for the uniqueness property in Theorem 2.\n- [page 3] suggestion: Add proof of Lemma 4 to the appendix for completeness.\n- [page 4] The notation $\\lesssim O(\\epsilon)$ is redundant/imprecise.\n- [page 5] suggestion: Format the parenthesis in Eq (5) so that they're at\n  least as tall as the sqrt symbols.\n- [page 8] All of the tables should use tabular environments instead of Numbers\n  screenshots.\n- [page 9] suggestion: Discuss related works as last section of the\n  introduction.\n- [page 9] typo: \"as an example(Fremond et al.,...)\"",
            "clarity,_quality,_novelty_and_reproducibility": "The ideas in this work are presented clearly. The writing and overall quality\nof the argument for (1) why the proposed method for computing $d_{FID}$ via\neigenvalues is universally beneficial and (2) the use of their new $d_{Eig}$\ndistance could be strengthened both with better experiments and more\ntheoretical justification. The experiments and results are reproducible.",
            "summary_of_the_review": "This work studies a clean mathematical distances and begins to explore (1) how\nit can be computed faster in practice and (2) variants that also fast to\ncompute while being theoretically sound. I do not recommend that it be accepted\nto ICLR 2023 given the overall quality of the paper, arguments, and experiments.\nThe work is interesting, but it needs to be developed quite a bit further\nbefore being accepted to a conference of this caliber.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper589/Reviewer_6Rmw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper589/Reviewer_6Rmw"
        ]
    },
    {
        "id": "cNtmdUxgEiM",
        "original": null,
        "number": 2,
        "cdate": 1666545640311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666545640311,
        "tmdate": 1666545640311,
        "tddate": null,
        "forum": "HehY2ZX2Cz",
        "replyto": "HehY2ZX2Cz",
        "invitation": "ICLR.cc/2023/Conference/Paper589/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents two formulas to compute the Fr\u00e9chet Inception Distance (FID) between real/generated datasets. \nThe authors provide some worst-case error analysis coupled with practical details and eigenvalue sorting. Numerical \nexperiments demonstrate the effectiveness of the proposed schemes. ",
            "strength_and_weaknesses": "Before I post my summary, I want to suggest the authors to flip the orientation of the x-axis. It took me several minutes until I realize that the x-axis increases along the left direction. Normally, you would expect the number of samples to increase along the right direction, and the opposite is really confusing.\n\nThe main strength of the paper is the experimental evaluation where the authors demonstrate benefits of the eigenvalue-based formulation. In particular, d_{eig} can perform well with fewer data samples than d_{fid}. Nonetheless, my main concern is that a lot of the things discussed in the paper are straightforward. For example, there is no need to compute the eigenvalues of S_1S_2 using a non-symmetric eigenvalue solver (this is claimed as a drawback). This seems to be one of the main reasons why the proposed d_{eig} does better; but when S_1 is SPD, the pencil (S2,S_1^{-1}) is also SPD, and there is no need to use a non-symmetric eigenvalue solver. \nMoreover, computing the square root of a matrix product is asymptotically equivalent to that of an eigenvalue decomposition, although I do agree that computing eigenvalues seems cheaper indeed; especially if someone uses the definition of d_{fid} in (1) which requires way too many floating-point operations. Section 2.2.2 holds when the two covariance matrices commute. Is this happening in practice? If not, the definitions in 2.2.2 become heuristics. Are these heuristics the ones reported in the experiments? I can tell that this seems to be the case in Section 3.2, in which case the authors do not compute the exact FID. \n\nOverall, I think the paper is well-written and clear. While I support the idea, many of the modifications the authors present are relatively incremental (for example, (2) is well known in the literature).  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Look good.",
            "summary_of_the_review": "See my answer to \"Strength and weaknesses\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper589/Reviewer_7tys"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper589/Reviewer_7tys"
        ]
    },
    {
        "id": "roB7I-zXjm",
        "original": null,
        "number": 3,
        "cdate": 1666649622923,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649622923,
        "tmdate": 1666649622923,
        "tddate": null,
        "forum": "HehY2ZX2Cz",
        "replyto": "HehY2ZX2Cz",
        "invitation": "ICLR.cc/2023/Conference/Paper589/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study two similiarty distances for distribution shift, $d_{FID}$ (Frechet Inception Distance) and the prposed $d_{Eig}$ (Sorted Eigenvalue Comparison). The authors first showed by slightly modifying the $d_{FID}$ algorithm, the computational time for $d_{FID}$ can be improved. Then, the authors provide better upper bounds for this method. Finally, inspired by this algorithm, the authors propose $d_{Eig}$ and justify its stability by eigenvalue rigidity.",
            "strength_and_weaknesses": "Strength: $d_{Eig}$ is much cheaper to compute than $d_{FID}$.\nWeakness:\n1. The trick to accelerate $d_{FID}$ seems not hard.\n2. Comparing to upper bounds. (See Summary of Review)\n3. $d_{FIG}$ in experiment isn't more stable and it lacks of some desired properties.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and original. The paper doesn't seem novel. The experiments are sound.",
            "summary_of_the_review": "The paper is clear to follow, but however the main contribution of the paper is unclear. The first improvement trick seems to be not that hard, I found it hard to believe that no one has ever implemented in such way to save computational time--it would be more convincing that some well used library has been using $np.trace(sqrtm(S_1S_2))$ as an evidence. The second point: by comparing to upper bounds, it is not enough to say that one algorithm is better than the other; you need a lower bound or PAC-type of result to justify that. Finally, despite arguing $d_{Eig}$ is more stable alternative than $d_{FID}$, the empirical studies didn't show this. (Say in Fig 3, if you look at the relative error in the table, $d_{FID}$ has the smallest errors)\n\nI agree with the authors that $d_{Eig}$ seems to be computationally cheaper, but however it seems to lack the desired mathematical properties in evaluation. As the contribution of the paper isn't clear for me, I tend to reject the paper for ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper589/Reviewer_RbUs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper589/Reviewer_RbUs"
        ]
    },
    {
        "id": "myGMeiMx2Hp",
        "original": null,
        "number": 4,
        "cdate": 1666857807379,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666857807379,
        "tmdate": 1666857807379,
        "tddate": null,
        "forum": "HehY2ZX2Cz",
        "replyto": "HehY2ZX2Cz",
        "invitation": "ICLR.cc/2023/Conference/Paper589/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed an improved Fr\u00e9chet Inception Distance estimator (d_FID), as well as a sorted eigenvalue distance estimator (d_Eig), between two sample covariance matrices (SCMs). \nComputational error bounds were established for these two methods, with a huge saving in the running time.\nThe authors then provided a few (statistical) arguments from random matrix theory to characterize the behavior of the largest eigenvalues of SCMs in Section 3.1.\nThe proposed d_Eig is then applied, as a simple alternative to d_FID, in Section 3.2, as the scores of GANs.\n",
            "strength_and_weaknesses": "**Strength**: the paper is in general clearly written. And the problem under study is of significance.\n\n**Weaknesses**: some efforts are needed to better highlight the contribution of this paper. See my detailed comments below.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: the paper is in general clearly written.\n\n**Quality and Novelty**: the contribution seems to be of limited interest, some efforts are needed.\n\n**Reproducibility**: no code for proof, but those may not be necessary? In any case, that depends on the precise contribution of this work.",
            "summary_of_the_review": "Detailed comments: \n\n* some derivations in Section 2 are somewhat elementary. These are not enough to be the major contribution of the paper.\n* Section 2.2.1: error bound of eigvals(): in the first sentence of this paragraph, s_j is not yet defined? or perhaps I missed something\uff1f\n* Section 2.2.2 considers a very special case: I am wondering if there exists any practical situation where such a special case holds.\n* Theorem 10 holds under the stability condition (Bao et al., 2015, Condition 1.1 (iii)) which should be stated explicitly. Also, it remains unclear whether Theorem 10 is a novel result.\n* I do not understand why it is of interest to consider the \"spike\" behavior in Sec 3.1, and in which context.\n* some numerical results, e.g., in Figure 2(c) and in Figure 3 are hardly visible: Not sure if they should be put in the appendix.\n* in Section 3.2, the concrete application in GAN is considered. However, it remains unclear how the proposed improvement is important from a GAN perspective.\n* No proof is given: this makes me feel that the results presented in the paper are all existing results. And no code is released for the numerical experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper589/Reviewer_28R5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper589/Reviewer_28R5"
        ]
    }
]