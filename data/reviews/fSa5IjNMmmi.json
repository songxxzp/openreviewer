[
    {
        "id": "Uo4hGWlgQz",
        "original": null,
        "number": 1,
        "cdate": 1666504434818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666504434818,
        "tmdate": 1666504434818,
        "tddate": null,
        "forum": "fSa5IjNMmmi",
        "replyto": "fSa5IjNMmmi",
        "invitation": "ICLR.cc/2023/Conference/Paper4265/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new method leveraging equivariant deep learning to approximate hypervolume calculation in multi-objective optimization (DeepHV). The authors first introduce important concepts in multi-objective optimization, such as problem formulation and the Pareto optimality. The authors then describe current hypervolume calculation methods (WFG, HBDA, FPRAS, HV-Net) and their scaling limitations in terms of number of objectives (M) and number of data points (N) and how hypervolume are used or avoided in current multi-objective optimization methods, such as multi-objective Bayesian optimization and multi-objective evolutionary algorithms. Following the introduction, the authors formally define the hypervolume calculation and it's important symmetric properties (permutation invariance and G-equivariance) which are then used to propose a G-equivariant variation of a fully-connected neural network layer that make up the building block of DeepHV. The Deep HV network used in the experiments includes five layers of G-equivariant layers proposed by the authors.\n\nFor their experiments, the authors generate $\\sim$ 1M training data points by sampling solutions for different dimension of objectives (3-10) that are then used to train the DeepHV approximator. The first set of experiments presented show that DeepHV achieves a lower MAPE compared to HVNet and generally performs better with a greater set of parameters. Subsequent experiments show runtime benefits of DeepHV compared to open-source hypervolume calculations in Pymoo and Botorch. The final set of experiments of experiments involve using DeepHV in active multi-objective EA and BO settings. In the multi-objective EA settings, DeepHV performs better than NSGA2 but fails to beat SMS-EMOA with a traditional Monte-Carlo calculator across various multi-objective optimization benchmarks. For multi-objective BO, the results are mixed with traditional methods sometimes outperforming DeepHV methods and vice versa.",
            "strength_and_weaknesses": "**Strengths**\n\n* The paper proposes a novel methods in a relevant challenge in multi-objective optimization and show promising results for making hypervolume calculations computationally feasible in large-scale search with high numbers of objectives and data points.\n* The paper proposes a new G-equivariant layer based on first principles and provides results supporting their method compared to current methods (HVNet).\n* The paper shows experiments with DeepHV used in modern multi-objective optimization methods.\n\n**Weaknesses**\n\n* The paper lacks an ablation on the equivariant layers to clearly show what benefits they provide. It might be possible that the HVNet performance provides that ablation in an indirect sense, so it would be good for the authors to clarify this.\n* The generalization claim of the authors related to DeepHV does not appear to well-tested. Right now it seems that DeepHV trains on data for the same problems that it is applied on and one could argue that it somewhat generalizes to the distribution of hypervolume in those design problems. It would be nice to explore this notion further as greater generalization would provide a stronger case for DeepHV.\n* SMS-EMOA with Monte-Carlo estimation still outperforms DeepHV in terms of task performance. It might be interesting to see if the compute cost of SMS-EMOA is higher compared to the DeepHV methods. That might showcase an interesting trade-off between the methods worth exploring.\n\n**Additional Questions**\n\n* Could you provide an intuitive explanation for the mathematics in Section 3.1. This would complement the current section and proof in the appendix (which would be good to reference in the main text).\n* What is the importance of generating data only non-dominated solutions? Wouldn't hypervolume calculations be equally valid with dominated solutions?\n* It would nice to see to potentially have a measure of compute cost on the x-axis instead of function evaluations. Assuming you can do that analysis, does it show that DeepHV consistently saves on compute? If so, in what cases?\n* Any intuition why training with multiple dimensions of objectives does not help the 3-objective case in Table 2?\n* What is the primary source of noise in the multi-objective BO settings?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe paper is well-written with good clarity explaining all relevant pieces.\n\n**Quality**\n\nThe method description and experiments generally show good quality. I think answers to some of the above questions could further strengthen the quality.\n\n**Novelty**\n\nThe paper proposes a novel method for a relevant problem in multi-objective optimization.\n\n**Reproducibility**\n\nThe authors provide clear description of the method and a reproducibility statement. Assuming the code gets released, as indicated in the reproducibility statement, other should be able to reproduce the results.",
            "summary_of_the_review": "Overall, the paper is well-written and proposes a novel in a relevant problem settings which provide good reasons to argue for acceptance. Nevertheless, I think the paper can still be improved by addressing current weaknesses, such as providing a more detailed description related to generalization and compute costs of DeepHV compared to other methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4265/Reviewer_9RfZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4265/Reviewer_9RfZ"
        ]
    },
    {
        "id": "VilI2xkqdv",
        "original": null,
        "number": 2,
        "cdate": 1666602634990,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666602634990,
        "tmdate": 1670384682641,
        "tddate": null,
        "forum": "fSa5IjNMmmi",
        "replyto": "fSa5IjNMmmi",
        "invitation": "ICLR.cc/2023/Conference/Paper4265/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method to approximate the hypervolume function with a deep neural network The network is built by using specialized layers to incorporate some symmetry properties of the hypervolume function, such as permutation and scale equivariance. ",
            "strength_and_weaknesses": "Strengths\n1.\tA new deep neural network-based approximation with permutation and scale equivariance is proposed for the hypervolume function.\n2.\tThe organization is clear and easy to follow.\n\nWeaknesses\n1.\tThe advantages of the permutation and scale equivariance in the hypervolume approximation need more discussion.\n2.\tThe effectiveness of the proposed method needs more evaluation. First, when comparing the effectiveness of training a model on each objective case separately with that on all objective cases simultaneously, how is the experiment set to ensure fairness? Second, it is reported that the boost in performance of DeepHV compared to HV-Net is likely due to the incorporation of the symmetry properties of DeepHV. How? Third, the results shown in Section 5.2.1 do not appear to be consistent with that in Section 4.2. For example, SMS-EMOA that uses exact computation mostly achieves the best results, yet the approximation models (e.g., DeepHV-256-all) that are shown to achieve higher accuracy in Section 4 rarely show better results in Section 5. This raises the question of whether the metrics used in Section 4 are appropriate. Fourth, the experiments conducted in Section 5 should include more state-of-the-art (approximated) hypervolume-based methods.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1.\tThe advantages of the proposed method need more discussion to improve the quality of the work. \n2.\tA diagram is suggested to show the network architecture used in this paper.\n",
            "summary_of_the_review": "This paper proposes a new deep neural network-based hypervolume approximation method that is equipped with some symmetry properties. More discussions and more evaluations are needed to verify the advantages of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4265/Reviewer_DvS5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4265/Reviewer_DvS5"
        ]
    },
    {
        "id": "IaRlbejvsK",
        "original": null,
        "number": 3,
        "cdate": 1666604062747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604062747,
        "tmdate": 1666604062747,
        "tddate": null,
        "forum": "fSa5IjNMmmi",
        "replyto": "fSa5IjNMmmi",
        "invitation": "ICLR.cc/2023/Conference/Paper4265/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "        This paper proposes a deep neural network to approximate the computation of the hyper-volume, which becomes expensive for a large number of objectives or points. The network has a special architecture to take into account the particularities of the hyper-volume computations such as invariances to permutations. The model is compared with state-of-the-art approaches, showing lower prediction error. It also compares favorably to the exact computation and other approximations in the context of evolutionary algorithms and Bayesian optimization. The computation speed is better also for a larger number of objectives.\n",
            "strength_and_weaknesses": "Strengths:\n\n        - Well written paper.\n\n        - Original architecture proposed to compute the hyper-volume.\n\n        - Extensive experiments evaluating performance and computational time.\n\nWeaknesses:\n\n        - It seems only synthetic problems are considered in the experiments.\n\n        - The authors claim in the discussion that their approach outperforms MC approximations in terms of performance. However, the experiments only seem to indicate that the proposed approach is faster. MC approximations seem to perform better.\n\n        - No error curves are given in Table 1 and 2. However, since the test set is large. E.g. 100k points it may not be a problem.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "        The paper is clearly written and difficult concepts are clearly explained.\n",
            "summary_of_the_review": "Overall I think that this is an interesting paper that proposes a new method that will receive the attention of the community. The paper is well written and the experimental section is strong, with experiments that clearly show the benefits of the propose approach. My only concern is that only synthetic experiments seem to have been carried out.\n\nMinor:\n\n        There are other methods that for multi-objective BO that do not use the hyper-volume directly, but that need to\n        solve a multi-objective problem. For example, see:\n\n        Predictive entropy search for multi-objective Bayesian optimization\n        D Hern\u00e1ndez-Lobato, J Hernandez-Lobato, A Shah, R Adams\n        International conference on machine learning, 1492-1501\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4265/Reviewer_cbGc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4265/Reviewer_cbGc"
        ]
    },
    {
        "id": "KmAk1m_lSM",
        "original": null,
        "number": 4,
        "cdate": 1666609323402,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609323402,
        "tmdate": 1669038464633,
        "tddate": null,
        "forum": "fSa5IjNMmmi",
        "replyto": "fSa5IjNMmmi",
        "invitation": "ICLR.cc/2023/Conference/Paper4265/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes DeepHV, a deep neural network based model, to approximate hypervolume for multi-objective optimization. The idea of model-based hypervolume approximation has been proposed in the previous work (HVNet). This work's contribution is to design a more powerful equivariant layer to further exploit hypervolume's scale-equivariant and permutation invariant properties for better modeling. Experimental results show that DeepHV can better approximate the hypervolume, and can be well combined with EA and BO methods for multi-objective optimization. ",
            "strength_and_weaknesses": "**Strengths:**\n\n+ This paper is generally well-organized and easy to follow.\n\n+ Efficiently calculating the (approximate) hypervolume, especially for problems with many objectives (e.g., >= 5), is an important while challenging problem for multi-objective optimization. This work is a timely contribution to an important research topic for multi-objective optimization.\n\n+ The proposed model can achieve good performance for hypervolume approximation, and can be easily used in the current EA/BO methods for multi-objective optimization.\n\n**Weaknesses:**\n\n**1. Novelty and Contribution** \n\nThe original idea of model-based hypervolume approximation, problem formulation (e.g., problem definition and training data generation), and model framework are already proposed in the previous work HV-Net [1]. This work's main contribution is the new equivariant layer for better approximation performance, which is a natural extension from the equivariant layer proposed in [2]. Although it is glad to see the proposed model can achieve better performance (as listed in the strengths), the contribution is not very novel given the previous works.\n\nIn addition, the close relation to the previous work HV-Net should be clearly discussed and emphasized early in this paper. I did believe this work has proposed the idea of model-based hypervolume approximation until the end of page 2. The claimed contribution in the abstract, \"To overcome these restrictions, we propose to\napproximate the hypervolume function with a deep neural network, which we call DeepHV\", is also a bit misleading. \n\n**2. Unclear Comparison with HV-Net**\n\nSince the main contribution is the improvement over HV-Net, their difference should be carefully discussed in detail, which is missing in the current paper.\n\n*a) Symmetry Properties* \n\nFor example, this work claims DeepHV can \"leverage additional symmetry properties of hypervolume compared to HV-Net\". It seems that both DeepHV and HV-Net have the permutation invariant property, while DeepHV additionally has the scale-equivariance of the objective values. Do the additional symmetry properties mean the scale-equivariance to objective values? Why and how this additional property can lead to better Hypervolume approximation? \n\nIn addition, it could be beneficial to have an ablation study and detailed analysis for the layer design (21) to clearly show the effect of a) no permutation invariance, no scale equivariance; b) only permutation invariance, but no scale equivariance; c) no permutation invariance, but only scale equivariance; d) both permutation invarince, and scale equivariance for hypervolume approximation. Is HV-Net corresponding to the case of b), and the proposed DeepHV is the case of d)?\n\nCould a simple normalization of the objective values (e.g., all to [0,1]) significantly improve the approximation performance of HV-Net? It seems that the related order of HV could be more important than the approximation accuracy for optimization. \n\n*b) Comparision Setting*\n\nSome comparison settings with HV-Net might need further clarification.\n\nThe DeepHV model is both trained and tested with the MAPE loss, while the HV-Net is trained with the log MSE loss and tested on the MAPE loss. Will it lead to unfair comparison?\n\nFor DeepHV model, the training data is split into train-validation-test sets, so the hyperparameters and settings can be carefully fine-tuned on the validation set. It seems that the HV-Net is directly trained on the whole dataset without fine-tune. Will it lead to unfair comparison?\n\n**3. Test Problems and Reported Results**\n\nIt is glad to see DeepHV can have promising performance with both EA and BO approaches to solve multi-objective optimization problems. However, the current reported results are not solid enough to truly show DeepHV's advantages.\n\nThis work uses the DTLZ benchmarks for most experiments, and only one real-world problem is reported at the very end of the Appendix (e.g., page 18). The DTLZ benchmarks have a regular shape of PS, which might be very different from real-world problems. It could be much better to report the results on the real-world problems suite in [3] with 2-9 objectives, which is now widely used in both the EA and BO community.\n\nWhy only the results of DTLZ 1,2,5,7 are reported? How about DTLZ 3, 4, 6?\n\nFor all experiments, please report the value of the log hypervolume difference (e.g., as in the qEHVI paper [4]) rather than the hypervolume value. The found Pareto set with \"similar\" hypervolume value could perform very differently and have quite different values of log hypervolume difference. \n\n**4. Experiments with EA**\n\nFor all EA experiments, only 1,000 function evaluation is far from the ideal setting. In the standard EA setting, all algorithms should be run with a large number of evaluations (e.g., 100,000) such that the found solutions can converge to the true Pareto front. Will the approximation nature of DeepHV make it hard to find the true Pareto solutions, and lead to poor performance in the standard EA setting?\n\nFor the baseline algorithm, it is well-known that the domination-based algorithm such as NSGA-II will perform poorly on problems with more than 3 objectives. Outperforming NSGA-II is not enough to show the advantage of DeepHV for many-objective optimization. NSGA-III [5] could be a much more suitable baseline for comparison (which is also available in Pymoo).\n\nIt is also interesting to know the wall-clock runtime of these algorithms for problems with different numbers of objectives.\n\n**5. Experiments with BO**\n\nFor all BO experiments, the important results of UCB-Exact HV and UCB-MC HV are missing. Will UCB-DeepHV have similar performances with these baselines?  \n\nOn the other hand, since UCB-DeepHV with different model sizes perform similarly to each other, will HV-Net also have similar BO performance?  \n\nIt is interesting to know the performance of qEHVI with UCB acquisition function as in UCB-DeepHV.\n\n**6. The Opening Story**\n\nI personally enjoy reading the opening story in the introduction for multi-objective optimization. However, reading this paper while driving could be dangerous and indeed against the law in many countries. It is better to replace it with a harmless story.\n\n**Other Comments**\n\n1. In Figure 2, it is interesting to know the computation time on GPU for DeepHV and the exact hypervolume calculation in BoTorch.\n\n2. If the goal of hypervolume approximation is to be used with EA or BO for optimization, it seems directly approximating the hypervolume contribution (HVC) could be a more reasonable choice. There is a concurrent work on HVC approximation (HVC-Net) [6], where a comparison is not needed, but a brief discussion could be beneficial.\n\n[1] HV-Net: Hypervolume Approximation based on DeepSets. IEEE Transactions on Evolutionary Computation 2022.\n\n[2] Deep models of interactions across sets. ICML 2018.\n\n[3] An easy-to-use real-world multi-objective optimization problem suite. Applied Soft Computing 2020.\n\n[4] Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization. NeurIPS 2020.\n\n[5] An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I: solving problems with box constraints. IEEE Transactions on Evolutionary Computation 2014.\n\n[6] HVC-Net: Deep Learning Based Hypervolume Contribution Approximation. PPSN 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** This paper is generally well-organized and easy to follow, but many important details and discussions are missing as discussed in the weaknesses above.\n\n**Quality:** The overall quality is good, but the concerns listed in the weaknesses section should be carefully addressed.\n\n**Novelty:** The original idea of model-based hypervolume approximation, problem formulation (e.g., problem definition and training data generation), and model framework are already proposed in the previous work HV-Net [1]. Therefore, the novelty of this work is somehow limited.  \n\n**Reproducibility:** It seems the proposed model can be easily reproduced, but many additional experiments are required to truly analyze the pros and cons of the proposed model.  ",
            "summary_of_the_review": "This work is a timely contribution to an important research problem for multi-objective optimization. However, due to the concerns on novelty, unclear comparison with closely-related work, and experimental settings, I cannot vote to accept the current submission.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4265/Reviewer_favf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4265/Reviewer_favf"
        ]
    }
]