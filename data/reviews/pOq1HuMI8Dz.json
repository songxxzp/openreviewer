[
    {
        "id": "fs2EPBucou",
        "original": null,
        "number": 1,
        "cdate": 1665860667021,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665860667021,
        "tmdate": 1665860667021,
        "tddate": null,
        "forum": "pOq1HuMI8Dz",
        "replyto": "pOq1HuMI8Dz",
        "invitation": "ICLR.cc/2023/Conference/Paper2758/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the use of the multiple-input multiple-output (MIMO) paradigm for vision transformers. The authors propose to train a ViT by using two images A and B at a time \u2013 the two images are tokenized and the tokens are \u201cmixed\u201d by randomly selecting tokens from image A or B. The mixed tokens are then passed through the ViT, and a proposed \u201csource attribution\u201d layer at the top allows the network to predict the classes of both images A and B. Experimentally, the authors show that 1) their network beats previous small networks reported in the literature on small-scale datasets (table 1), 2) the proposed MIMO architecture gives improvements across datasets and architectures (table 2 and 3) and 3) that the proposed MIMO architecture beats previous MIMO architectures for ViTs (table 6).\n\n",
            "strength_and_weaknesses": "Strengths:\n\n* The proposed method generally gives solid gains in known benchmarks.\n* The authors give clear ablation experiments which show that their methods are better than other MIMO methods for VITs (table 6).\n\nWeaknesses:\n\n* The novelty is low.\n* Some results do not control for the training recipe, e.g. numbers in table 1 might have vastly different hyperparameters. \n* It is not clear if the method improves SOTA models.\n* The paper mostly focuses on small-scale datasets. It is known that VITs work best for large-scale datasets.\n* The clarity when explaining the proposed architecture could be improved.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is not clearly explained, and the figures do not really help. Most captions are only a sentence or so \u2013 see e.g. Figure 3. This makes the figures hard to interpret. I would encourage the authors to add some PyTorch/NumPy/Jax pseudo-code for the network architecture. \n\nFor table 1, the hyperparameters might be so different that it is hard to compare the methods. The number of parameters can also be misleading since many authors change the resolution when training networks \u2013 and higher resolution typically leads to better scores.\n\nI would also like to ask, are all numbers in table 3 from you? Do the methods in table 3 all use the same hyperparameters? If not, the different performances might simply come from different training recipes instead of the MIMO architecture. I would encourage the authors to use the original code from the VIT authors and then make sure that the proposed method can bring clear improvements over the official code. I am not convinced that the proposed method would actually improve SOTA methods. The best result would be to improve the SOTA number for e.g. Vit-base on imagenet. \n\nMixing two input images on the token level seems like a simple adaptation of cut mix. Could you add some ablation experiments that test how the method compares against cut mix and if the improvements from the two methods are orthogonal?\n\n",
            "summary_of_the_review": "The authors present a natural extension of the MIMO framework for ViTs and show how this can improve performance for various vision datasets. While the empirical results are positive, it is not clear that all factors are controlled for and that the proposed method would actually improve SOTA models. Furthermore, the novelty is relatively low and the proposed method is not explained clearly.\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2758/Reviewer_sVL6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2758/Reviewer_sVL6"
        ]
    },
    {
        "id": "L4lnY5FLjp",
        "original": null,
        "number": 2,
        "cdate": 1666679080324,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679080324,
        "tmdate": 1666679080324,
        "tddate": null,
        "forum": "pOq1HuMI8Dz",
        "replyto": "pOq1HuMI8Dz",
        "invitation": "ICLR.cc/2023/Conference/Paper2758/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes to bring MIMO for CNNs to vision transformers. Specifically, it designs a source attributing module at last layer to separate the input source and perform classification for different input. Experiments are mainly conducted on small-scale datasets, CIFAR, Imagenet-100 to show its effectiveness. \n",
            "strength_and_weaknesses": " Strength:\n\t\tMany small-scale datasets are used for validating the results. And the paper proposes one working example of MIMO for vision transformers. \n\n\tWeaknesses:\n\t\tThe paper introduces one way of bring MIMO to vision transformers, which is quite straightforward and novelty-limited, \nSource attribution is a direct analogy of using different input projection in existing MIMO.  But there exist many other natural designs that can potentially work for vision transformers. The study space in the paper is not enough to show this proposed way is better.  i.e. important baselines are missing. For example, use different patch embedding for different inputs (similar to different project for inputs in MixMo paper). \n\t\tAgain, this specific design of late source embedding is not thoroughly studied. Early source embedding with carefully designs comparison is missing. Or middle source embedding, etc, this design should be carefully studied.\n\t\tExperiments are mainly conducted on small-scale datasets, ability to adapt to other large-scale datasets and large models remain unknown. Table 3 shows CaiT S-24 and M-24 results are 82.3 and 82.7. However, table 3 in CaiT paper shows s-24, m-24 results are 82.7 and 83.4 respectively. Can authors check the numbers here? And provides some large model (CaIT-M, or other large popular ViT variants) results would be better.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good, novelty is limited as the method is a direct adaption of existing MIMO to vision transformers. Reproducibility is good as authors provides many implementation details. ",
            "summary_of_the_review": "\nOverall the paper is a direct adaption of MIMO to vision transformers. However, the design space is not thoroughly studied, and current results are only on small-scale dataset and small models, raising the concern regarding if current design is optimal and can adapt to other large models. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2758/Reviewer_jCdb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2758/Reviewer_jCdb"
        ]
    },
    {
        "id": "fq4IMMTxJ3",
        "original": null,
        "number": 3,
        "cdate": 1666709721053,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666709721053,
        "tmdate": 1666709721053,
        "tddate": null,
        "forum": "pOq1HuMI8Dz",
        "replyto": "pOq1HuMI8Dz",
        "invitation": "ICLR.cc/2023/Conference/Paper2758/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a transformer-based MIMO framework, called MixViT. At training time, two subsets of tokens from two images are taken as input and the model is tasked to predict labels for the two input images. At test time, the same input is passed to the model multiple times with different sourced tokens. The features in the earlier layers are shared while two separate encoders, called source attribution, are introduced in the later layer for the input-dependent feature extraction. In this way, the model is expected to achieve feature sharing and separation for multi-prediction. Improved results are obtained on image classification tasks compared with the single input counterparts and baselines. ",
            "strength_and_weaknesses": "Strength\n- Consistent improvements are achieved on the task of image classification.\n\nWeaknesses\n- The technical contribution seems limited. The input mixing for training is simply adopted from CutMix, and the source attribution appears to be quite straightforward. \n- The model needs to run multiple times for the inference, causing a significant increase in the computational cost.\n- The model is claimed to be multi-input and multi-output. However, it only supports two inputs and outputs, and the model needs to be changed for a different number of inputs, and it appears to be non-trivial to extend this model for more than two inputs and outputs. Hence, the model is not scalable and flexible for a different number of inputs and outputs.\n- As this is an architecture paper, it would be more convincing to also validate the effectiveness on other tasks like segmentation and detection, for which the model seems not directly applicable.",
            "clarity,_quality,_novelty_and_reproducibility": "- The motivation is clear and the method is technically sound, but the technical contribution is somewhat limited as discussed above.\n- The writing is generally clear.",
            "summary_of_the_review": "Overall, I think the proposed method sounds interesting and good performance is also achieved. However, the technical contribution is somewhat limited as discussed above, and the weaknesses overwhelm the strength. Hence, I lean to recommend rejection based on its current shape.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2758/Reviewer_GaDC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2758/Reviewer_GaDC"
        ]
    },
    {
        "id": "X3bt8-IffuW",
        "original": null,
        "number": 4,
        "cdate": 1666777319199,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666777319199,
        "tmdate": 1666777319199,
        "tddate": null,
        "forum": "pOq1HuMI8Dz",
        "replyto": "pOq1HuMI8Dz",
        "invitation": "ICLR.cc/2023/Conference/Paper2758/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Similar to MIMO in wireless communication, multi-input multi-output training improves network performance by optimizing multiple subnetworks simultaneously. Previous MIMO network architecture uses CNN. This paper proposes MixViT, the first MIMO framework for vision transformers which takes advantage of ViTs\u2019 inherent mechanisms to share features between subnetworks. Unlike MIMO CNN, MixViT only separates subnetworks in the last layers leveraging a source attribution that ties tokens to specific subnetworks. The paper shows that MixViT can have significant gains across multiple architectures (ConViT, CaiT) and datasets (CIFAR, TinyImageNet, ImageNet-100, and ImageNet-1k).",
            "strength_and_weaknesses": "Strength\n1. This paper proposes MixViT, the first MIMO framework using vision transformers which takes advantage of ViTs\u2019 inherent mechanisms to share features between subnetworks. \n\n2. The paper shows that MixViT can have significant gains across multiple architectures (ConViT, CaiT) and datasets (CIFAR, TinyImageNet, ImageNet-100, and ImageNet-1k).\n\nWeaknesses\n1. It is not clear how MIMO training helps representation learning. The paper lacks analysis on this. Does MIMO training improve hard example classification accuracy? Does it better cluster images into different clusters? Does most of the gains come from input mixing such as CutMix?\n\n2. The evaluation is on relatively small datasets. The paper should evaluate on larger datasets such as ImageNet.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper uses certain phrases without clear definition, e.g. in \"MixViT modifies the traditional MIMO structure to take full advantage of ViTs\u2019 propensity to mutualize features between subnetworks while still retaining the advantage of training distinct predictions\", it is not clear exactly what mutualize features mean. Can you show an example?\n\nIt seems MIMO networks using vision transformer is new. \n",
            "summary_of_the_review": "MIMO networks with vision transformer seems to novel. However, the paper does not provide deeper analysis on how it helps representation learning, besides the data augmentation benefits from input mixing, e.g. CutMix. The evaluation is also inadequate as it does not provide results on large datasets such as ImageNet.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2758/Reviewer_6pnV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2758/Reviewer_6pnV"
        ]
    }
]