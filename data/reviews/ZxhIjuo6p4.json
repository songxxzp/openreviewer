[
    {
        "id": "SQ2m5diPvd",
        "original": null,
        "number": 1,
        "cdate": 1666629327751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629327751,
        "tmdate": 1666630754709,
        "tddate": null,
        "forum": "ZxhIjuo6p4",
        "replyto": "ZxhIjuo6p4",
        "invitation": "ICLR.cc/2023/Conference/Paper4094/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The study proposes a model-based decentralized policy optimization for MARL to overcome the non-stationary issues arising from the concurrent policy learning of multiple agents. Specifically, the current study proposes modeling individual agents\u2019 state transition model and reward function while taking into account other agents\u2019 policies into a latent variable. The modeled individual dynamic and reward functions along with the latent factor encoder are used to train an individual decision-making policy. Furthermore, to take into account the nonstationary aspect of other agents\u2019 policies, the current study uses a latent variable prediction function. ",
            "strength_and_weaknesses": "1. This study explains in detail why decentralized MARL learning is difficult and explains in detail and theoretically the tasks that must be preceded in order to overcome the difficulties.\n\n2. The performance bound of decentralized MARL learning was analyzed very strictly by correlating the performance of the prediction model.\n\n3. The mathematical analysis is rigorous and detailed, but the description of the structure of the model used and the learning method is relatively week. In particular, reasons and justifications for using a specific model structure and choosing a learning method are not sufficiently provided.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n<Methodology>\n\n1. The description of the structure of the encoder model that estimates the latent variable is poor. In particular, it would be nice if you authors explain what kind of distribution is used to model the latent variable and what architecture was used.\n\n2. I am wondering if the authors have tried to use a variational learning approach to learn the encoder model capturing the latent factor. The variational approach can possibly prevent the overfitting of the latent variable function to certain datasets. In addition, the variational recurrent model can be possibly used to model both the latent factors and their temporal variation. This approach can be beneficial in that one can impose proper inductive biases regularizing the temporal behavior of latent factors, i.e., temporal consistency. \n\n3. In Equation 3, it is strange to learn a transition model based on a latent variable and a reward function as one objective function. There must be a difference in scale between the elements constituting the objective function, but I wonder if the learning is going well.\n\n4. How to compare the proposed approach with the other decentralized MARL approach modeling opponent behaviors and using them to optimize individual policy? \n\n5. I am curious about the mechanism of cooperation. Although agents share the same reward, there is the chance that individually selected action does not necessarily induce cooperation.\n\n\n<Experiments>\n\nIt may be required to include an additional baseline. For example, MARL with an opponent modeling approach. \n",
            "summary_of_the_review": "The mathematical analysis is rigorous and detailed, but the description of the structure of the model used and the learning method is relatively week.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4094/Reviewer_p6F5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4094/Reviewer_p6F5"
        ]
    },
    {
        "id": "kI20s2iXtxx",
        "original": null,
        "number": 2,
        "cdate": 1667149800723,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667149800723,
        "tmdate": 1667149800723,
        "tddate": null,
        "forum": "ZxhIjuo6p4",
        "replyto": "ZxhIjuo6p4",
        "invitation": "ICLR.cc/2023/Conference/Paper4094/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The main contribution of this paper is to propose MDPO, a model-based decentralized policy optimization (MDPO), which incorporates a latent variable function to help construct the transition and reward function from an individual perspective.  The theoretical monotonic improvement guarantee is given and experimental results show the superiority of the proposed method.\n",
            "strength_and_weaknesses": "Strength\uff1a\n\u2013 The paper is clearly motivated and well structured.\n\u2013 Theories and proofs are provided to support the algorithm.\n\nWeakness: \n\u2013 The authors made good theory contributions. But how the theory would guide the design of the algorithm and how the experiments show this relation are not clear to me. \t\t\n\u2013 the experiments can be enhanced by comparing to recent model-based RL methods. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The reproducibility is difficult to evaluate as code is not provided. The optimization part is also not clear. See summary of the review below.\n",
            "summary_of_the_review": "The problem considered in this paper is clearly motivated. I find the methodology lacks motivation and the relation between the theoretical results and algorithm design (sec 3.5) is not clearly addressed. \n\nSome detailed comments are listed below.\n\n\u2013 Can authors explain why the first two inequalities in sec 3 hold?\n\n\u2013 In sec 3.1, the authors propose to maximize likelihood of the dynamic and reward prediction model. How is $P_\\theta\\left(o^{\\prime} \\mid o, a, z\\right)$ implemented? Do you use MLP or gaussian distribution? Similarly, how do you implement $\\left(R_\\phi\\left(o, a, o^{\\prime}, z\\right)\\right.$?\n\n\u2013 There is another work on decentralized model-based policy optimization (DMPO). The setting of DMPO can be equal to this work by setting the number of neighbors as zero. \nCan the authors include this method as a baseline? \t\t\n\t\t\t\t\t\nYali Du, Chengdong Ma, Yuchen Liu, Runji Lin, Hao Dong, Jun Wang, and Yaodong Yang. Fully decentralized model-based policy optimization for networked systems. arXiv preprint arXiv:2207.06559, 2022. \n\n\u2013 Line 9 of algorithm 1, how is $\\psi_{w_{j+1}}$ updated?\n\t\t\n\u2013 The authors mentioned that MDPO is a decentralization method, but in the process of learning hidden variables, it is actually necessary to obtain the information of other agents. How could the algorithm be decentralized in this sense?\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4094/Reviewer_DZj6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4094/Reviewer_DZj6"
        ]
    },
    {
        "id": "0ZQQImJ81Cm",
        "original": null,
        "number": 3,
        "cdate": 1667532912054,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667532912054,
        "tmdate": 1667532912054,
        "tddate": null,
        "forum": "ZxhIjuo6p4",
        "replyto": "ZxhIjuo6p4",
        "invitation": "ICLR.cc/2023/Conference/Paper4094/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new approach (MDPO) to model-based decentralized reinforcement learning by leveraging a latent variable function $\\psi(z|o)$ to help learn the observation transition $P_i(o_i'|o_i,a_i,z_i)$ and reward $R_i(o_i,a_i,z_i)$ models. \n\nThe main problem this paper tries to address is that the assumptions of TRPO may not hold in a decentralized MARL because multiple agents update their policies simultaneously, making a significant difference between the new and old joint policies. MDPO uses the environment model to bridge the gap between $\\rho^{\\pi^\\text{new}}$ and $\\rho^{\\pi^\\text{old}}$.\n\nThis paper also provides the proposed algorithm's theoretical performance bounds (Theorem 2, Theorem 3), which theoretically contribute to the monotonic policy improvement.",
            "strength_and_weaknesses": "Strength:\n\n1. MDPO learns a latent variable to help distinguish different transitions resulting from varying unobservable information of the full state and other agents' policies. This novel approach makes the observation transition $P_i(o_i'|o_i,a_i,z_i)$ and reward $R_i(o_i,a_i,z_i)$ models more stable during training.\n2. This paper also provides theoretical performance bounds (Theorem 2, Theorem 3) for the proposed algorithm, which theoretically contributes to the monotonic policy improvement.\n\nWeaknesses:\n\n1. It needs to be clarified how MDPO solves the main problem of decentralized MARL. It seems that the main reason that makes $\\|\\rho^{\\pi^\\text{new}}-\\rho^{\\pi^\\text{old}}\\|>\\|\\rho^{\\pi^\\text{model}}-\\rho^{\\pi^\\text{old}}\\|$ hold is that the $\\psi^n_w=(1-\\alpha)\\psi^{n-1}_w+\\alpha\\psi^n$ in Theorem 1, where $\\psi^n$ is the true latent variable function. MDBP does not update $\\psi$ directly to the true value, but uses a $\\alpha$-tradeoff between the true value and the old value. Then does it mean that MDBP solves this problem by slightly updating each agent's policy like TRPO? As this paper says, such a slight update will lead to much slower convergence, especially in fully decentralized MARL. This is a key point about whether MDPO truly solves the nonstationary problem caused by the simultaneous update of policies (which also makes the assumption in TRPO not hold).\n2. The experimental part of this paper needs to be more convincing. The baseline algorithm is only IPPO, and complex multi-agent environments such as The StarCraft Multi-Agent Challenge or Google Research Football are missing.",
            "clarity,_quality,_novelty_and_reproducibility": "Good",
            "summary_of_the_review": "This paper introduces a latent variable to help learn the observation transition $P_i(o_i'|o_i,a_i,z_i)$ and reward $R_i(o_i,a_i,z_i)$ models. This paper proposes a new decentralized MARL algorithm, MDRL, to solve the nonstationary problem in a model-based way. However, I am negative about this paper based on the weak points above.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4094/Reviewer_WQD2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4094/Reviewer_WQD2"
        ]
    }
]