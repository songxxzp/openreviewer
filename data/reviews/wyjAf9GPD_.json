[
    {
        "id": "76WPm7TxW0",
        "original": null,
        "number": 1,
        "cdate": 1666264225410,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666264225410,
        "tmdate": 1666264225410,
        "tddate": null,
        "forum": "wyjAf9GPD_",
        "replyto": "wyjAf9GPD_",
        "invitation": "ICLR.cc/2023/Conference/Paper5337/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper propose using Koopman operator to speed up the optimization of hybrid quantum computing algorithm. The motivation is gradient step in quantum computer takes many forward evaluations (linear in number of parameters). Using Koopman operator I the classical computing sense, to predict future optimal quantum machine learning parameters. then pick the best, optimal time, and then do more predictions from there.\n",
            "strength_and_weaknesses": "The motivation of the paper is good. It is one plausible solution to an important problem. The method of Koopman operator learning using sliding window and neural network to learn dictionary, can potentially be use for optimising other classical computing algorithm. its advantage is most in quantum computing.\n\nI have not thought through thoroughly if using the sliding window approach is theoretically sound. Certainly did not sit down, write the required equations and convince myself this is a theoretically sound approach. Loss function of Eq. (8) (or similar) can be constructed in many ways, sliding window way etc, seems arbitrary to me. I understand several other works in the area of Koopman operator also uses similar approach of constructing some measurable / observable and then do optimization. In the limit of infinite observables, one can agree that the trained Koopman operator can represent the underlying dynamics accurately. There is also an issue of how to invert the \"g\" function to get back the underlying \"x\" (Eq. (1)). In this paper, it seems g is the identity function.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity of the paper is good.\n\nThe main novelty should be finding a good application in quantum computing. Sliding window method offers some novelty, but to my ignorant, I do not know of any other similar approach.\n",
            "summary_of_the_review": "Good motivation for this work. Not absolutely sure if the equations are theoretically sound due to my ignorant.\nMy sense is this paper is in between (6) Marginally above acceptance threshold and (8) accept, good paper.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5337/Reviewer_zjTr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5337/Reviewer_zjTr"
        ]
    },
    {
        "id": "APfUgZxg7d",
        "original": null,
        "number": 2,
        "cdate": 1666587676548,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666587676548,
        "tmdate": 1666587676548,
        "tddate": null,
        "forum": "wyjAf9GPD_",
        "replyto": "wyjAf9GPD_",
        "invitation": "ICLR.cc/2023/Conference/Paper5337/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to model the variational quantum eigensolver optimization as a dynamical system, and approximate its iterative updates via Koopman-based approaches. In particular, the authors propose a sliding window DMD method, and a neural DMD approach. The authors evaluate their approach on quantum machine learning tasks, and compare it to standard DMD. ",
            "strength_and_weaknesses": "In my opinion, the strongest point of the paper is the application domain of quantum machine learning (QML) and the incorporation of Koopman-based methodologies in it. Having said that, it should be noted that the technical problem solved in this paper is essentially the ability to approximate complex dynamics with Koopman methods, and the relation to QML is only by application.\n\nUnfortunately, there are several weak points. First, the same observation and application was already considered in Dogra et al. (and other papers, some of which cited in this paper). In this context, the main difference between this paper and the paper of Dogra et al. is in the evaluation on QML tasks vs. vision tasks. While this difference may be significant in certain applications and algorithms, I do not believe this is the case here, as they essentially optimize a standard neural network, similar to those considered in Dogra et al.\n\nSecond, the proposed approaches to approximate the Koopman operator generally ignore many similar if not identical ideas in the literature. Both sliding windows with time delay coordinates and neural DMD have been heavily studied before. Properly discussing these approaches and positioning the proposed method in this context is essential.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is very clear and can be probably reproduced. Based on the discussion above, I believe the novelty of the paper is somewhat limited.",
            "summary_of_the_review": "In summary, while the application domain of this paper is interesting and refreshing, it is essentially a different application of an existing paper. Moreover, the proposed methods are similar or equivalent to existing work in the field. I suggest the authors to conduct a proper literature search in Koopman and DMD communities (see e.g., Hankel DMD and Koopman Autoencoder), and to position their contribution accordingly. Due to these reasons, I think the paper should not be published in its current form.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5337/Reviewer_TgmW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5337/Reviewer_TgmW"
        ]
    },
    {
        "id": "Hv2O_3DyvY",
        "original": null,
        "number": 3,
        "cdate": 1666731200065,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731200065,
        "tmdate": 1666731200065,
        "tddate": null,
        "forum": "wyjAf9GPD_",
        "replyto": "wyjAf9GPD_",
        "invitation": "ICLR.cc/2023/Conference/Paper5337/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The author has discussed the issue of backpropagation and obtaining the gradient of a quantum computer. The gradient computation during backpropagation in a quantum computer is not similar to a classical computer because the complexity increase with the number of parameters and measurements. The author discusses the Koopman operator theory for predicting nonlinear dynamics with the natural gradient method in quantum optimization to address this issue. The author proposed a data-driven approach using the Koopman operator to accelerate quantum optimization and Quantum machine learning. The author proposed Sliding window dynamic mode decomposition (DMD) and Neural DMD for efficiently updating parameters in quantum computers. The author shows that the method can predict gradient dynamics and accelerate the quantum variational eigensolver in quantum optimization and quantum ML.  \n \n",
            "strength_and_weaknesses": "Strength:\n\nThe paper discusses an essential problem in the literature.\n  \nAlso, the author provides an excellent solution to tackle the problem, but the analysis and intuition part is very poorly represented in this paper.\n  \nThe paper is easy to follow and understand. The authors introduce several quantum machine learning strategies, advantages, and weaknesses. Based on this, the motivation of this paper is reasonable.\n\n\nWeaknesses:\n\nThe author has discussed accelerating quantum mechanics and quantum optimization algorithms compared to classical operators. Did the author quantify or qualify this using some experiment? \n\nHow did the Koopman operator help to accelerate the gradient calculation in quantum computers compared to classical computers? Can the author provide a brief explanation for this?\n\nIn section5.2, the author has discussed the sequence of the quantum gate. Is this similar to layers? Can the author discuss the objective of each block in Figure 4a (like the objective of block \"ansatz\" and its input and output)? Similarly need to discuss other blocks. Can the author describe all the blocks and their functionality in the figure-1 for MNIST dataset example (Like, for an image sample)? The author should describe the complete algorithm and discuss which part is classic machine learning and which is quantum machine learning.\n\nAlong with MNIST, the author also discuss Figure 1 with the time series task. Did the author report any comparative results on time series?\n\nThe author should report the performance and analysis of results on the MNIST dataset.\n\nThe author can add a small preliminary section that describes the steps for the quantum ML algorithm (Training and Inference)  \n\nThe experiment section is not designed correctly. The author should compare the results with SOTA and do an ablation analysis on the Koopman operation. The analysis section of the paper needs to improve with more analysis results.\n\nTake home: Koopman operator  for stability and robustness and \nMNIST on QML\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to the weaknesses section.",
            "summary_of_the_review": "The overall writing quality is ok, and the proposed method is simple and beneficial. However, the experiment comparison lacks justification, and the technical contribution is plain.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5337/Reviewer_XcoP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5337/Reviewer_XcoP"
        ]
    },
    {
        "id": "CNbYxjDPB5c",
        "original": null,
        "number": 4,
        "cdate": 1666770609711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666770609711,
        "tmdate": 1666770609711,
        "tddate": null,
        "forum": "wyjAf9GPD_",
        "replyto": "wyjAf9GPD_",
        "invitation": "ICLR.cc/2023/Conference/Paper5337/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present two algorithms for quantum optimization and machine learning based on the Koopman operator. They test their algorithm with simulations and a small hardware experiment.",
            "strength_and_weaknesses": "+ the Koopam operator optimization method may turn out to be interesting for learning\n- there is hardly any strong evidence in this work of the performance of the methods\n- the experiments are trivial (3 qubits) and no conclusions can be drawn from them\n- no theoretical reasons why this methods should outperform other gradient-free optimization methods are given",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well written. The ide is a simple extension of classical work to the quantum case. The results are reproducible",
            "summary_of_the_review": "The paper has an interesting idea of using Koopman operator theory as a way to avoid gradient calculations in training quantum neural networks. More work and evidence is needed for the impact of this methods",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5337/Reviewer_5VYL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5337/Reviewer_5VYL"
        ]
    }
]