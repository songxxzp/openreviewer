[
    {
        "id": "BL4GZJUw6yJ",
        "original": null,
        "number": 1,
        "cdate": 1666335634842,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666335634842,
        "tmdate": 1666399110614,
        "tddate": null,
        "forum": "_NlE9YiyXKb",
        "replyto": "_NlE9YiyXKb",
        "invitation": "ICLR.cc/2023/Conference/Paper1972/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tries to improve adversarial robustness via a new perspective: tessellated 2D convolutional network as a divide and conquer defence. The input image is first divided into several non-overlapping patches (regular or irregular), sent to parallel branches, and then the features from different patches are aggregated inside the network. The underlying idea is that such a network architecture leads to implicit feature transformation, cropping, masking and thus improve model robustness. Results on Fashion-MNIST and Cifar-10 validate the effectiveness of the proposed model to some extent.",
            "strength_and_weaknesses": "Strength: The idea of design a new architecture that allows the model to do implicit feature transformation to defend against adversarial attack  is novel. \n\nWeakness: \n1. The writing is poor. There are seven paragraphs in the Introduction section that focus on the background information of adversarial attack, and only four paragraphs talking about the proposed method. Many information in the first seven paragraphs seem irrelevant to the main idea in this paper. Reading the introduction is boring and tedious. Also, I fail to find any description regarding to the evaluation behavior of tesellating input images. It seems to me the two non-uniform tessellation methods are random operations, and how to keep the model predictions deterministic during evaluation is still a mystery based on this paper. Plus, does all the patches share the same model weight? Or there is a separate branch for each one of them? In the latter case, the model's computational cost is multipled.\n2. Lack of citations. This paper makes claims without any citation. For example, the last sentence in the first paragraph 'Architectural changes in the network topology is a promising means of achieving adversarial robustness ' is one without any citation. \n3. Lack of (experimental) proofs. The motivation of this paper is a hypothesis: 'modification of the network structure leading to implicit feature transformation, cropping, masking, and distillation may result in improved robustness'. To verify this hypothesis, the authors should at least show some experiments demonstrating that explicit image-level cropping or masking or other transformations will help the model defend against adversarial attack, and embedding such a prior knowledge into the architecture design is a similar but more efficient implementation. However, I fail to see any such experiment. \n4. Poor experiment designs. This paper introduces a new metric called weighted accuracy (accw) without enough justification. This paper only experiments on very simple CNNs. The results in Table 2 show that a modern architecture like ResNet can easily beat the proposed T-2D-CNN in terms of both clean accuracy and robustness, let alone lots of more powerful networks have been developed after ResNet. Furthermore, the images in the two used datasets in this paper are of small resolutions (28 or 32). What's the effect of the proposed T-2D-CNN on larger images, like 224x224?",
            "clarity,_quality,_novelty_and_reproducibility": "The work is novel and original. The writing is not clear. The quality is not so good.",
            "summary_of_the_review": "In general, I think this work proposes a interesting idea, but fail to prove its effectiveness. There is still a lot to improve. Maybe this work can be published at another conference after polishing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1972/Reviewer_n7Ya"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1972/Reviewer_n7Ya"
        ]
    },
    {
        "id": "CkrHsB1UEa7",
        "original": null,
        "number": 2,
        "cdate": 1666547881946,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666547881946,
        "tmdate": 1666547881946,
        "tddate": null,
        "forum": "_NlE9YiyXKb",
        "replyto": "_NlE9YiyXKb",
        "invitation": "ICLR.cc/2023/Conference/Paper1972/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper provides a novel CNN architecture that process input images as non-overlapping regions or tiles. The paper claims that the partition reduces the input dimension and diversifies the model architecture, which may provide effective defense against adversarial attack.",
            "strength_and_weaknesses": "## Strength\n1. The paper proposes a novel model architecture that utilize diverse partition of the input image. To my knowledge this haven't been explored in previous work\n2. The paper is overall well written and easy to follow\n3. The proposed method shows natural improvement on blackbox robustness over regular model without the need of adversarial training\n\n## Weakness\n1. The experiment provided is inadequate to fully support the claim on improved blackbox robustness. All attacks provided in the paper are generated with a regular ResNet model, which naturally will have better transferability on a regular model than the proposed model. To fully verify the robustness of the proposed architecture experiment should be conducted on transfer attack generated against an independently trained TRN model with the same partition configuration. This is also a valid threat under blackbox setting, as only model architecture is known by the attacker.\n2. The proposed method is only tested against naive model, without any other baseline blackbox robustness improvement methods\n3. The attack effectiveness in Table 2 is doubtful. Even on the regular model the attack is not effective, especially for the FAB-T and SQUARE attack, which is unexpected. More information is needed on how the attack is implemented, an how is the convergence behavior in the generation of the attacks\n4. The paper mainly uses the $\\Delta$acc as the metric, which does not make sense. Given the tradeoff between clean accuracy and robustness, we want the robust model to have a better accuracy under attack than the clean model, not just a smaller accuracy drop. Otherwise a randome guess model will always have the best $\\Delta$acc = 0 under no matter what attack",
            "clarity,_quality,_novelty_and_reproducibility": "The paper proposes a novel method. The method is well motavated the clearly explained. The quality of the paper can be improved with more solid experimental settings (see weakness for detail).",
            "summary_of_the_review": "In summary, the paper propose an novel and intereting model architecture against blackbox adversarial attack. However, the experiment results are inadequate to support the claim that the proposed method is effective. Thus I would recommend rejection for now.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1972/Reviewer_KrRX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1972/Reviewer_KrRX"
        ]
    },
    {
        "id": "rcCFtvcCoju",
        "original": null,
        "number": 3,
        "cdate": 1666650404777,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650404777,
        "tmdate": 1666650404777,
        "tddate": null,
        "forum": "_NlE9YiyXKb",
        "replyto": "_NlE9YiyXKb",
        "invitation": "ICLR.cc/2023/Conference/Paper1972/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work propose a training pipeline to help defend against adversarial robustness. Specifically, an input image is partitioned into a number of non-overlapping sub-rectangles. Then each sub-rectangle is process by a shared branch of base neural network and further merged together to make a prediction. They show that this new training pipeline suffers less from adversarial attacks.",
            "strength_and_weaknesses": "Strength:\n\n1. This paper is well-written and easy to follow.\n\n2. The proposed method to tessellate neural networks is simple and clear.\n\nWeakness:\n\n1. The proposed method suffers too much from in-distribution accuracy drop. Based on the results shown in Table 2, the proposed new method significantly reduces classification accuracy on the clean images. For example, on CIFAR-100, the standard ResNet can achieve around 77% accuracy whereas the proposed method can only achieve 50%. The significant in-distribution accuracy drop makes this method less practical. In addition, the low in-distribution accuracy makes the evaluation metric: difference between clean accuracy and adversarial accuracy is less reasonable. \n\n2. The proposed method does not have inspiring motivations and lack of sufficient contributions to the robustness research community. For example, in the Section 2.2.2, the authors discussed \"Why should tessellated networks be robust against adversarial attacks\" and mentioned that \"this offers an implicit ensemble\". Since ensemble does not harm in-distribution accuracy, I am not fully convinced by using this new training pipeline instead of simple ensemble if they share similar motivations.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written but lacks novelty to make sufficient contributions to the research community.",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1972/Reviewer_nZEg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1972/Reviewer_nZEg"
        ]
    },
    {
        "id": "CitPdgSnn_a",
        "original": null,
        "number": 4,
        "cdate": 1666690627243,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690627243,
        "tmdate": 1666690627243,
        "tddate": null,
        "forum": "_NlE9YiyXKb",
        "replyto": "_NlE9YiyXKb",
        "invitation": "ICLR.cc/2023/Conference/Paper1972/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel divide-and-conquer based approach of tessellating a base network architecture. Each tessellated network involves a separate set of learnable parameters. Experiments show this method is more robust against black-box attacks.",
            "strength_and_weaknesses": "Strength\n\n1. The paper is clear, and I had no trouble understanding it.\n2. The idea of this paper is simple and intuitive. \n\nWeaknesses\n\n1. There are no results on white-box attacks which are generated from tessellated network. The results on black-box attacks only show that this approach reduces the transferability of black-box adversarial attacks, rather than improving the adversarial robustness. Moreover, this is not consistent with your proposal that the ensemble of diverse network structures have been shown to exhibit robustness.\n2. This paper lacks more theoretical analysis of the tessellated network.\n3. This paper lacks related work on tessellation in images and in NN.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear. In this regard, the method may be novel.",
            "summary_of_the_review": "This work presents an architecture with simple and intuitive idea, but there are no results on white-box attacks which are generated from tessellated network. The contributions to adversarial robustness are only marginally significant.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1972/Reviewer_z2SF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1972/Reviewer_z2SF"
        ]
    }
]