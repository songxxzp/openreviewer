[
    {
        "id": "LLU45OETnGf",
        "original": null,
        "number": 1,
        "cdate": 1666730682168,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666730682168,
        "tmdate": 1666730682168,
        "tddate": null,
        "forum": "PfHk0P9lgMy",
        "replyto": "PfHk0P9lgMy",
        "invitation": "ICLR.cc/2023/Conference/Paper996/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper looks at maxout networks which are randomly initialized with parameters drawn from a Gaussian distribution. It then looks at the directional derivative (i.e., the input-output Jacobian times a fixed vector). The authors derive a variety of statistics for this derivative (bounds, distribution, expected moments). Based on their findings, they suggest a good parameter initialization for wide maxout networks. They experimentally validate that their proposed parameter initialization works well.",
            "strength_and_weaknesses": "Large parts of this paper are outside of my domain of expertise, so I am unable to judge certain aspects confidently.\n\nThis looks like a strong paper. The theoretical results seem extensions to existing proofs for ReLU networks (e.g., Hanin), but the extensions seem non-trivial. The theoretical results are validated thoroughly using numerical experiments, and it is nice to see that concrete and simple recommendations are made for practitioners in section 4.",
            "clarity,_quality,_novelty_and_reproducibility": "A minor comment: I appreciate the rigour of the paper, but at times the amount of notation makes the paper hard to read. I would encourage authors to occasionally repeat the definition of a variable again, so that the reader isn't expected to remember each variable for the entire text.\n\nFor example, the text introduces $\\mathcal{M}$ in passing just before equation 2. The rest of the text heavily uses this variable, but never mentions it by name again. In particular, this makes it hard for a practitioner who skipped to section 4 to understand what is going on.\n\nAll in all the paper is well-written and structured though. The appendix is very thorough, and all the details are provided to check the proofs and reproduce the experimental results.",
            "summary_of_the_review": "Outside of my domain of expertise, but looks like a well-written paper with a significant technical contribution (although quite directly building on previous work done for ReLU units).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper996/Reviewer_3PQc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper996/Reviewer_3PQc"
        ]
    },
    {
        "id": "RmhpOVobs3a",
        "original": null,
        "number": 2,
        "cdate": 1666747303067,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666747303067,
        "tmdate": 1666747303067,
        "tddate": null,
        "forum": "PfHk0P9lgMy",
        "replyto": "PfHk0P9lgMy",
        "invitation": "ICLR.cc/2023/Conference/Paper996/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper derives multiple theoretical insights into the properties of maxout neural networks at initialization. In particular, they derive results for the distribution of Jacobian-vector product norms, number of linear regions, NTK, and curve length distortion. They also provide recommendations for initialization of maxout networks based on their analysis, leading to significantly improved performance when trained with SGD and Adam compared to naive He initialization.",
            "strength_and_weaknesses": "## Strengths\n\nThe paper provides a serious study with multiple results on various properties of the maxout networks, including some practically relevant recommendations. The results of the analysis are presented relatively clearly, and the implications are discussed when possible.\n\nThe analysis also suggests an initialization strategy that the authors demonstrate works better than naive initialization.\n\n## Weaknesses\n\n**W1**: I don't fully understand why the terms in Eq. (1) are decomposed the way they are, and why the authors focus on the analysis of the term $\\|J_{\\mathcal{N}}(x^{(l)}) u\\|$ and $\\|x^{(l)}\\| / n_l$.\n\n**W2**: As the authors mention, the particular initialization strategy derived was already proposed for maxout networks in [1]. \n\n**W3**: The significance of the empirical results is not obvious, as the authors remove batch normalization and consequently do not achieve particularly outstanding results for the respective benchmarks.\n\n**Discalimer.** I am not an expert in this area, and not familiar with other works. The paper is very technical and I cannot judge the significance of most of the derived theoretical results. Please assign a low weight to my review.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity (W1)\n\nI have a few questions about the discussion of Eq. (1) on page 3. First, what is the motivation behind the decomposition $\\langle \\mathcal{L}(\\mathcal{N}(x)), J_\\mathcal{N}(x_i)^{(l)}\\rangle = C(x, W) ||J_\\mathcal{N}(x^{(l)}) u ||$? You use $u = e_i$, so $||J_\\mathcal{N}(x^{(l)}) u || = ||J_\\mathcal{N}(x_i)^{(l)}||$, correct? Then, the term $C(x, W)$ is given by $\\langle \\mathcal{L}(\\mathcal{N}(x)), J_\\mathcal{N}(x_i)^{(l)}\\rangle / ||J_\\mathcal{N}(x_i)^{(l)}||$. In other words, you multiply and divide by $||J_\\mathcal{N}(x^{(l)}) u ||$, but then this term is the center of your analysis. \n\nThroughout the paper, you seem to assume that the term $||J_\\mathcal{N}(x^{(l)}) u ||$ is the most important term in the Eq. (1). \nWhy is this justified, given that it was introduced artificially? Why can we ignore $C(x, W)$? \n\nThen, you say that the other term of interest is $A^(l) = ||x^{(l)}||^2 / n_l$. Where did this term come from? Where does it appear in Eq. (1)?\n\n## Novelty and Significance (W2)\n\nIn terms of novelty, my understanding is that related results have been described for ReLU networks, but the authors generalize and extend these results to maxout networks. The main practical suggestion of the paper is the initialization strategy, which was also proposed in [1] already, but this work provides theoretical justification for this choice.\n\nIn terms of significance, it is not obvious to me how significant the results are, given the limited adoption of maxout networks (although I am not an expert here). However, theoretically, the paper generalizes and extends the results of ReLU network analysis, which is valuable.\n\n## Experiments (W3)\n\nThe experiments show promising results, although the immediate practical significance is limited. The authors admit that they do not aim to achieve state of the art results, and instead aim to cleanly test the effect of the proposed regularization strategy. It is unclear if the initialization would have a similar effect if the models were using batch normalization.\n\n",
            "summary_of_the_review": "Overall, this is an interesting and dense paper deriving multiple new results for maxout networks. I don't have the background to fully appreciate the contributions of the paper, as I indicated above. I am currently voting for a weak accept, but with a low confidence score.\n\n## References\n\n[1] [_On the Expected Complexity of Maxout Networks_](https://arxiv.org/abs/2107.00379);\nHanna Tseran, Guido Mont\u00fafar",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper996/Reviewer_RjF3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper996/Reviewer_RjF3"
        ]
    },
    {
        "id": "15qZBas0Nl-",
        "original": null,
        "number": 3,
        "cdate": 1667012444938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667012444938,
        "tmdate": 1668793558022,
        "tddate": null,
        "forum": "PfHk0P9lgMy",
        "replyto": "PfHk0P9lgMy",
        "invitation": "ICLR.cc/2023/Conference/Paper996/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the paper, authors analyze the gradients of a maxout network with respect to inputs and parameters. In consequence, the authors improve the training of deep maxout networks in the case of fully-connected and convolutional networks.",
            "strength_and_weaknesses": "1. The introduction paragraph, \"Maxout networks,\" is not clear.\n2. The introduction section is long and describes simple concepts, like Parameter initialization, and does not specify details of Maxout networks.\n3. The contributions of the paper are long and need to be more convincing. After reading the introduction, it is challenging to specify the paper's main result.\n4. Also, in section 2, the paragraph Architecture needs to be clarified. Some figures may help.\n5. Theoretical results from section 3 Results are interesting but nontrivial to follow. Authors try to show too many things in very short lines.\n6. The paper is longer than 9 pages.\n7. The experimental section confirms the results from the theoretical part. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty and quality are on very good level. Unfortunately  the paper is very hard and difficult to read.",
            "summary_of_the_review": "Most of the results from the paper are interesting, but the article's presentation and construction of the article caused the paper to be very hard and difficult to read. The paper is better suited to a journal, where we do not have a limit of pages and can describe everything clearly. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper996/Reviewer_Fz1h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper996/Reviewer_Fz1h"
        ]
    },
    {
        "id": "HK5C-2o9Jvi",
        "original": null,
        "number": 4,
        "cdate": 1667326493742,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667326493742,
        "tmdate": 1671423504407,
        "tddate": null,
        "forum": "PfHk0P9lgMy",
        "replyto": "PfHk0P9lgMy",
        "invitation": "ICLR.cc/2023/Conference/Paper996/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors study theoretical questions regarding the training of randomly initialized fully-connected neural networks with **maxout** nonlinear activations. In particular, they derive the moments of the input-output Jacobian of the network, which motivates a parameter initialization strategy that could avoid the exploding and vanishing gradient problem in wide networks. They validate their results by experiments, and also derive some other theoretical properties, such as bounds on the expected number of linear regions, the expected curve length distortion, and neural tangent kernels.",
            "strength_and_weaknesses": "Strength:\n- Intuitively, maxout networks are more difficult to analyze. Therefore compared with the analysis of ReLU network, it is a step forward.\n- The paper is well-organized and the non-technical part is easy to follow.\n\nWeakness:\n- It seems that all the questions (Jacobian moments, proper initialization scale, number of linear regions, curve length distortion, NTK) and techniques to answer them have appeared in previous work. The originality of the contributions and the theoretical impact would both be limited.\n- Maxout networks (especially the fully-connected version) are not widely used in practice, to the best of my knowledge. Therefore the practical interest of this work would be limited.\n\nSome comments on the experiment section:\n- [1] reports a simple linear kernel achieved 1.2% test error on MNIST, which is better that all the fully-connected network performance reported in this paper.\n- As a rank-5 maxout network is 5 times as large as a ReLU network with the same architecture, I wonder if their results are directly comparable. Maybe a fair comparison should try to equal the number of parameters of different networks or else indicate their different sizes.\n\n[1] Understanding Deep Learning (Still) Requires Rethinking Generalization. https://dl.acm.org/doi/pdf/10.1145/3446776",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, has good clarity, and the authors provide proofs for their theorems. The theoretical results in the paper are original, to the best of my knowledge.\n\nThe authors have not provided source code of their experiments to reproduce their results, though they provide a detailed description of the experiment configurations in the appendix.\n\n\u2014\u2014\u2014\u2014\u2014\n\nUpdate:\n\nI thank the authors for their clarification on the anonymous code repository to reproduce their experiments, and appreciate their contribution to the open science community.",
            "summary_of_the_review": "I think the paper contains original theoretical results which contribute to people's understanding of maxout networks. However, the theoretical and practical impact of this work may be limited, and the technical questions and proof techniques have appeared in previous work. Therefore, I tend to argue the paper is marginally below the bar of the NeurIPS venue.\n\n\u2014\u2014\u2014\u2014\u2014\n\nUpdate:\n\nAfter reading the authors\u2019 rebuttal, I would like to keep my suggestion that the paper is marginally below NeurIPS\u2019s standard, due to my personal judgment on its potential impact, and I am raising my confidence score from 2 to 4.\n\nThat said, I very much appreciate the authors\u2019 discussion and clarification during the rebuttal phase.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper996/Reviewer_pCnC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper996/Reviewer_pCnC"
        ]
    },
    {
        "id": "r16HF8iKNZ6",
        "original": null,
        "number": 5,
        "cdate": 1667462540386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667462540386,
        "tmdate": 1667462540386,
        "tddate": null,
        "forum": "PfHk0P9lgMy",
        "replyto": "PfHk0P9lgMy",
        "invitation": "ICLR.cc/2023/Conference/Paper996/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the gradients of a maxout network with respect to inputs and parameters. Based on their results, they also obtain refined bounds on the expected number of linear regions, results on the expected curve length distortion, and results on the NTK. With their theoretical understanding, they justify a parameter initialization strategy that avoid vanishing and exploding gradients in maxout networks. They also empirically verify the success of their initialization strategy on multiple datasets.",
            "strength_and_weaknesses": "Pros:\n1. Interesting theoretical studies from various perspectives, including bounds for directional derivative, moments, distribution of input-output Jacobian. \n2. Good implications that refines previous bounds on the expected number of linear regions, and new results on length distortion and the NTK.\n3. Justification for the parameter initialization strategy and hyperparameter selection are reasonable.\n\nCons:\n1. Empirical ablation studies are needed to verify the selection strategy of hyperparameters. The authors compute a range of combinations of K and c given $c = 1 / M$. It would be great to test if the selected combination works better than others. For example, you can fix K=5 and vary a range of c to test the performance.\n2. Is there any difference between the proposed initialization and the original one proposed by Tseran & Mont\u00fafar (2021)? It would good to see if the original strategy can be improved by the new theoretical understanding.\n3. About max-pooling initialization, why initialize some layers as $K \\times m^2$? Some motivations are needed since the authors report it achieves better performance.\n4. More baseline strategies and architectures are needed to verify the effectiveness of the proposed initialization strategy. Currently the authors use VGG and He Initialization as a comparison, but He Init is designed for ResNet and it could be more effective under ResNet. So it would be great to compare maxout and Relu under ResNet, which is more like an apple-to-apple comparison. In addition, since the authors aim for a batch normalization-free network setting, comparing the proposed strategy with recent initialization methods targeting on this setting would be more fair, such as Fixup, Rezero.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good\nQuality: Medium\nNovelty: Medium\nReplicability: codes are provided ",
            "summary_of_the_review": "Overall I believe the paper is slightly above the acceptance level, and I would like to raise my score if the authors address the above concerns.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper996/Reviewer_uU8d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper996/Reviewer_uU8d"
        ]
    }
]