[
    {
        "id": "RLuZj7QYpI",
        "original": null,
        "number": 1,
        "cdate": 1666314684649,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666314684649,
        "tmdate": 1666423034964,
        "tddate": null,
        "forum": "MdSGM9PEQ7",
        "replyto": "MdSGM9PEQ7",
        "invitation": "ICLR.cc/2023/Conference/Paper640/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This article shows that exponential moving averages have several weaknesses, namely momentum and Lookahead optimizers. The authors then propose modifications corresponding to these weaknesses and propose a framework, Admeta, to analyze optimizers. Theoretical results are provided to show the convergence and the reasons for such modifications. Empirical results on various tasks are provided to demonstrate that the proposed method outperforms the baseline optimizer and the currently proposed optimizer.\n\nIn the implementation, the authors propose new variants of Adam and SGD using the DEMA method and a dynamic look-ahead strategy to avoid the potential negative effects of traditionally used EMA and look-ahead optimizers. The paper also provides evidence that both implementations converge in the convex and non-convex cases.",
            "strength_and_weaknesses": "Strength:\n1. The paper provides a point of view that is often overlooked by researchers.\n2. This paper provides experimental results on various tasks and datasets to demonstrate the advantages of the proposed method.\n3. The paper is easy to read.\n\nWeaknesses:\n\n1. The proof in the paper, although valid, is not sufficient to demonstrate the advantages of the proposed method, as it only guarantees convergence, which is the most basic requirement of an optimizer in deep learning.\n\n2. The paper can be improved by adding more theoretical explanations. What's more, the paper could add more and larger datasets.\n\n3. The improvements of the proposed method on some tasks are small in the experiments, so error bars are needed to verify that these improvements are not due to randomness. However, this paper does not provide error bars for the empirical results.\n\n4. AdaBelief's report in CIFAR seems bad. However, in the original paper by Adabelief ([1]), it achieves state-of-the-art results, albeit with a different backbone model. I don't know if there is a problem with the parameter settings of AdaBelief.\n\n5. SGDM in PyramidNet tried a learning rate of 0.5. Why not try it in ResNet110?",
            "clarity,_quality,_novelty_and_reproducibility": "The article is clearly written and presents an enlightening method. I think the attached code can be reproduced.",
            "summary_of_the_review": "The paper is well written and easy to read. To clearly explain these methods, the authors combine text, formulas, and figures. For this, I thank the author for his hard work. I also believe that the Admeta framework provides an impressive perspective on the future work of the optimizer. Experiments demonstrate its good results in many tasks. All in all, it's an interesting piece of work that I'm sure would be of interest to researchers in related fields.\n\n[1] AdaBelief optimizer: adjust step size based on observed gradients",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper640/Reviewer_kSFP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper640/Reviewer_kSFP"
        ]
    },
    {
        "id": "RK0Opiw_Gn1",
        "original": null,
        "number": 2,
        "cdate": 1666530197732,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666530197732,
        "tmdate": 1666530197732,
        "tddate": null,
        "forum": "MdSGM9PEQ7",
        "replyto": "MdSGM9PEQ7",
        "invitation": "ICLR.cc/2023/Conference/Paper640/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a framework for stochastic optimizers named Admeta. Based on SGD and R-Adam, the authors provide two implementations, AdmetaS and AdmetaR. \n\nSpecifically, the author contributes a new gradient decent algorithm framework in the style of SGD and Adam optimizer. First, the author identifies the problem of the commonly used EMA method: the increasing lag time and overshoot problem. Then the author identifies the problem of the Lookahead optimizer: a slow stepsize around the early stage of training. The paper is well written.\n",
            "strength_and_weaknesses": "Pros:\n- The authors identify the problem of the traditional used EMA and Lookahead method and give improvements to modify them.\n- The introduction of the method is straightforward and easy to follow and the motivation is very interesting which comes from stock markets.\n- It is simple but effective. I am positively surprised by the results achieved in some tasks brought by the modifications. \n\nConcerns:\n- I notice that in the CIFAR-100 task trained on PyramidNet, the results of R-Adam, Ranger, and Adabelief are even worse than AdamW. It is very interesting. What hyper-parameters do you use? More discussion of this phenomenon is welcomed.\n- Can you explain why SGD-family achieves awful results in Finetune task? Additional experiments are needed to explore this point. How will AdmetaS perform in Finetune tasks? Will all SGD family optimizers behave like this? What if the model chooses the Deep RNN/CNN architecture instead of the Deep Transformer architecture? \n- The authors simply changed EMA to a DEMA variant and turn lookahead into a dynamic one. I think innovation is a bit incremental.\n- There are some problems in the organization of the paper. Many important parts have been placed in the appendix, which increases the difficulty for readers.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- The introduction is straightforward and the method is stated clearly.\n\nQuality:\n-This work is OK.\n\nNovelty:\n- The innovation is a bit incremental.\n\nReproducibility:\n-Checked",
            "summary_of_the_review": "I believe that the paper has a lot of potentials but requires some additional discussion. My main concern lies in the results and innovation of the proposed optimizer. In general, this paper is worth publishing if concerns can be well addressed in the rebuttal stage.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper640/Reviewer_merB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper640/Reviewer_merB"
        ]
    },
    {
        "id": "0y6hfWHTPzD",
        "original": null,
        "number": 3,
        "cdate": 1666535368327,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535368327,
        "tmdate": 1666535368327,
        "tddate": null,
        "forum": "MdSGM9PEQ7",
        "replyto": "MdSGM9PEQ7",
        "invitation": "ICLR.cc/2023/Conference/Paper640/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors give a clear introduction to current optimizers in deep neural networks and propose a new framework for improvement, which divides the optimizer into two parts, forward-looking and backward-looking. \n\nTo replace the traditional EMA method used in backward-looking part, a variant of double exponential moving average (DEMA) is used. The authors demonstrate its advantage over EMA by analyzing the structure of the formula and attaching a comparison figure, which visually presents the convergence trend in the appendix.\n\nAnd to further improve the Lookahead optimizer, an asymptotic weight is used, which achieves faster training than constant lookahead around the early stage and maintains the advantage of better convergence around the final stage. The authors also provide two alternatives to compute the asymptotic weight.\n",
            "strength_and_weaknesses": "Strengths:\n\nThe ideas are reasonable, and this work gives a novel view in the optimizer field.\n\nThe authors conduct several experiments in Image Classification, Natural Language Understanding and Audio Classification, and experiments show the effectiveness of the proposed method. For all but a few tasks, this method shows advantages over other recent optimizers.\n\nThis paper follows a good logic and is easy to understand.\n\nConcerns:\n\nOn the other hand, I also have some concerns and questions toward this paper:\n\n (1) The theoretical explanation for the advantage of DEMA is insufficient. For example, the authors mention that the past gradients in EMA follow a fixed proportionality which is incompatible with actual optimization. Why?\n\n (2) In Image Classification, why the improvement for ResNet-110 is larger than PyramidNet?\n\n (3) The authors introduce a bias term in the DEMA method, but omit it in the rest of the paper and point out that this does not make difference. Why?\n\n(4) The choice of $\\mu$ and $\\kappa$ seems strange. What is the reason for constructing such a function? \n\n(5) How do you choose the hyper-parameter of the Admeta optimizer? It seems that the hyper-parameters chosen for each task are different.  \n\n (6) There are some deficiencies in the theoretical analysis, although I understand the space is limited. If more theoretical analysis is added, it will be more convincing and interesting. \n\n (7) Other works on optimizers like [1] tend to give an interval on experiment results in the appendix. I suggest that the results need to be tested for statistical significance.\n\n(8) Although training on large datasets is not necessary for the optimizer, if Admeta can be used in the large-scale dataset ImageNet or BERT pre training stage, the advantages of the proposed optimizer will be better demonstrated.\n\n(9) Though the related work is organized well and summarized into SGD Family, Adam Family, Stochastic Second Order Family and Other Optimizers, the authors should clearly explain the relation between the proposed method and related works.\n\n[1] Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well organized and clear.\n\nQuality: See contents above.\n\nNovelty: This article is innovative as a conference paper.\n\nReproducibility: The paper has released the code for reproducing.\n",
            "summary_of_the_review": "The paper proposes a novel framework for optimizer. It seems that this work is quite original and the experimental results are relatively good. The main weaknesses are that some points are not made clear and extended experiments are needed to demonstrate the effectiveness of the proposed method. In general, the paper is well-written and comprehensible, and the idea is impressive.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper640/Reviewer_dVGC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper640/Reviewer_dVGC"
        ]
    },
    {
        "id": "vRRrvZN_Pzn",
        "original": null,
        "number": 4,
        "cdate": 1666680908094,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680908094,
        "tmdate": 1666680908094,
        "tddate": null,
        "forum": "MdSGM9PEQ7",
        "replyto": "MdSGM9PEQ7",
        "invitation": "ICLR.cc/2023/Conference/Paper640/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors found that using a variant of DEMA to replace the EMA used in past optimizers and using a dynamic lookahead optimizer can achieve a better result. Motivated by that, the author proposed a novel optimizer framework Admeta, and implement it on SGD and Radam. Empirical results demonstrate the superior performance of Admeta compared to other optimizers.",
            "strength_and_weaknesses": "The strengths of the paper include an interesting framework to analyze the optimizer with modifications to each part, and a nice visalization figure show the comparison between DEMA and EMA. In my views, the limitations of the paper are that some results are not so good, especially in some relatively large models or datasets, such as MNLI with large BERT model. (But it should not be a big problem for the optimizer.) \nMy detailed comments are listed as below.\n\n1) Theorems 1-4 provide convergence proof on AdmetaR and AdmetaS, the two implementation versions of Admeta based on RAdam and SGD respectively. Before the main proof in the appendix, the authors make many assumptions and omit many things and argue that this argument can be attended to these cases. For example, the authors omit the forward-looking part in the convergence proof.  Are these assumptions and neglections reasonable? More discussion should be put in this part.\n\n2) As in some related works, the author should do some convergence rate analysis of AdmetaR in the convex and nonconvex settings to demonstrate its high convergence speed?\n\n3) The authors should introduce the hyperparameters tuning with more details since it is very important for the application of this optimizer in other tasks.\n\n4) In the ablation study part, I notice that AdmetaS \u2013 DEMA achieves worse result than SGDM, which may indicate that DEMA is not always a good method compared to EMA unless combined with a dynamic method. What may be the reason for this? The author needs to explain more.\n\n5) Separating optimizers into forward-looking and backward-looking parts is not a novel view. In fact, it is already used in Ranger ([1]). What is the difference of Admeta compared to Ranger optimizer. \n\n\n6) I notice that authors use SGD with Nesterov momentum in the experiment. It would be more interesting to see Nadam, Adam with Nesterov momentum in the comparisons. How about incorporating Nesterov\u2019s approach in DEMA?\n\n[1] Less Wright. Ranger - a synergistic optimizer. https://github.com/lessw2020/ Ranger-Deep-Learning-Optimizer, 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear for readers to follow with a nice quality and interesting novelty. The reproducibility is good.",
            "summary_of_the_review": "This work has introduced a new optimizer with modifications on two parts. Despite some deficiencies in theory, the paper is well written.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper640/Reviewer_RS9p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper640/Reviewer_RS9p"
        ]
    }
]