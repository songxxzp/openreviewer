[
    {
        "id": "kVfw3RcrYXp",
        "original": null,
        "number": 1,
        "cdate": 1666544603863,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666544603863,
        "tmdate": 1666544603863,
        "tddate": null,
        "forum": "IDSXUFQeZO5",
        "replyto": "IDSXUFQeZO5",
        "invitation": "ICLR.cc/2023/Conference/Paper1939/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes NeuralPCG, which learns a preconditioner using neural networks to improve the solution procedure of \nthe linear solver for the resulting system of linear equations from discretizing partial differential equations (PDEs). The authors\ntested the learned preconditioner for two- and three-dimensional simple PDEs.",
            "strength_and_weaknesses": "Strength: \n\nThe paper is easy to read and it is straightforward to catch the basic idea of the paper.\n\nWeaknesses:\n\n1. Learning preconditioners using neural networks for improving solving systems of linear equations has been widely studied. However,\n*the entire existing works are completely ignored by the authors*. I actually doubt if this paper has any novelty over existing works. By typing \"Learning preconditioner\" on the google search engine, we immediately get many related papers, e.g., the first two papers are: https://arxiv.org/abs/1906.06925 and https://arxiv.org/abs/2010.02866. Indeed, the study in the first paper is much more solid than the paper under review. Look at figure 5 of the the paper https://arxiv.org/abs/1906.06925, you will see the existing algebraic multi-grid (AMG) preconditioner is much better than the learned ones, Jacobi. and etc. In the paper under review, the authors never contrast the learned preconditioner with the existing state-of-the-art.\n\n\n2. The authors claimed that \"Classic numerical solvers provide unparalleled accuracy but often require extensive computation time.  Machine learning solvers are significantly faster but lack convergence and accuracy guarantees. We present Neural-Network- Preconditioned Conjugate Gradient, or NeuralPCG, a novel linear second-order PDE solver that combines the benefits of classic iterative solvers and machine learning approaches. Our key observation is that both neural-network PDE solvers and classic preconditioners excel at obtaining fast but inexact solutions. NeuralPCG proposes to use neural network models to precondition PDE systems in classic iterative solvers.\" --- I wonder if there is any convergence and accuracy guarantee of PDE solvers using the learned preconditioner? If not, then the bottleneck of existing learning-based PDE solvers is also a bottleneck of the proposed approach.'\n\n\n3. The authors stated that \"While this line of methods typically outperforms classical solvers in speed by a large margin, it struggles with converging into a highly precise solution (e.g., 1e \u2212 12). A lack of theoretical analysis on  convergence and accuracy inhibits neural-network PDE solvers\u2019 applications in mechanical engineering, structure  analysis, and aerodynamics, where precise PDE solutions have a higher priority than fast yet inexact results.\" --- My concern is whether we really need 1e-12 accuracy in real applications. Also, if we need such an accurate solution, do learning-based PDE solvers really faster than classical PDE solvers?\n\n\n4. The authors stated that \"NeuralPCG generalizes to physics parameters or meshes with moderate differences from the training data and is robust to outliers by design.\" --- Does the learned preconditioner generalizes to PDEs of different types and problems at different scales? AMG can be applied to problems of different sizes and even different types. I would like the authors to make some comparisons of their NeuralPCG with AMG and other state-of-the-art preconditioners.\n\n\n5. Does NeuralPCG works well when to the linear PDE when A is x-dependent?\n\n\n6. I wonder if NeuralPCG works for solving high-frequency wave equations?\n\n\n7. The authors may compare the method with some fast methods for solving PDEs, e.g., fast Fourier transformer and fast multipole method. Rather than consider the method that is applicable to any type of PDEs, which is a highly unfair comparison as the learned \npreconditioner is problem-dependent.\n\n\n8. Does the learned preconditioner reduce the condition number? I want to see some numerical evidence of reducing \ncondition number of the underlying problems.\n\n\n9. Does over-smoothing of graph neural network affect the performance of the learned preconditioner? The computational cost of \ngraph neural networks can be non-negligible when the system of linear equations is massive.\n\n\n10. In equation (6), the learned matrix L can be dense, significantly reducing the scalability of the PDE solver. This can be\na significant bottleneck of NeuralPCG. \n\n\n11. I think the authors should report the computational time, including the time for learning the preconditioner. In particular, for large-scale problems. It seems to me that NeuralPCG is much more expensive than existing SOTA algorithms. E.g. algebraic multi-grid.\n\n\n12. The baseline preconditioning methods are very weak. In practice, we use much more efficient ones.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: seems fine.\n\n\nQuality: not high.\n\n\nNovelty: very limited.\n\nReproducibility: Perhaps extremely difficult. The source code is not provided.",
            "summary_of_the_review": "The contribution of this paper is very limited. The paper ignores the entire existing research on learning preconditioners for solving systems of linear equations. Indeed, many existing works on deep learning of preconditioners are more solid in algorithm and validation than NeuralPCG. The experimental results are not convincing either.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1939/Reviewer_G2sH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1939/Reviewer_G2sH"
        ]
    },
    {
        "id": "3eeLUo7xgV",
        "original": null,
        "number": 2,
        "cdate": 1666622636339,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622636339,
        "tmdate": 1670708111048,
        "tddate": null,
        "forum": "IDSXUFQeZO5",
        "replyto": "IDSXUFQeZO5",
        "invitation": "ICLR.cc/2023/Conference/Paper1939/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a so-called Neural-Network-Preconditioned Conjugate Gradient (NeuralPCG) method to solve partial differential equations. This method uses graph neural network to model the preconditioner of iterative solvers for PDE. It is noted that the NeuralPCG can solve PDE with a satisfying convergence condition and works faster than classic PDE solver. Several classic examples, including passion equation and heat conducted equation, are solved via the NeuralPCG, the comparing results with classic numerical PCG method show the generality and efficiency of the proposed method.",
            "strength_and_weaknesses": "Strength: \nThe NeuralPCG combine the advantage of classic iterative solver and the machine learning based solver. It can have a good accurate solutions compared with neural network based PDE solver and work faster than classic solver.\n\nWeaknesses: \nThe approach is now limited to linear PDEs with symmetric-positive-definite matrix, and the preconditioner\u2019s sparsity is constrained to be the lower triangular sparsity of A.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and the overall quality is good.\n\nThe investigation on deep learning based preconditioner of PDE solver can date back to many years ago, some more similar paper can be found, e.g. \"Machine Learning-Aided Numerical Linear Algebra: Convolutional Neural Networks for the Efficient Preconditioner Generation\" and \u201cDeep Learning of Preconditioners for Conjugate Gradient Solvers in Urban Water Related Problems\u201d. This paper does not clarify differences from other similar papers, except the type of neural network used.",
            "summary_of_the_review": "This is a well-written and easy to read paper. However, it did not review enough on the investigations of deep learning based precondition method. Some comments are given as follows:\n\nComment 1. Maybe the contour plot of Fig.1 and Fig.3 can have a bar of labeled field.\n\nComment 2: This paper shows the performance of NeuralPCG method based on the Conjungate Gradient method, can this paper give some discussions on the performance of general preconditioning method based on deep learning method?\n\nComment3: This paper only shows a little reviews on the investigations of deep learning based preconditioner method and does not clarify the distinguished difference of this paper with other similar paper.\n\nComment4: The accuracy of classic finite element method often shows dependence of the numbers of mesh, does this method also show this dependency?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1939/Reviewer_94zp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1939/Reviewer_94zp"
        ]
    },
    {
        "id": "Bkp2KN1FN_",
        "original": null,
        "number": 3,
        "cdate": 1667593390102,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667593390102,
        "tmdate": 1667593390102,
        "tddate": null,
        "forum": "IDSXUFQeZO5",
        "replyto": "IDSXUFQeZO5",
        "invitation": "ICLR.cc/2023/Conference/Paper1939/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "The authors presented NeuralPCG, a hybrid ML-numeric PDE solver. While prior ML PDE solvers\noften do not have precision guarantees, their approach attains arbitrary precision (up to floating-point\nerror) by construction.",
            "strength_and_weaknesses": "Strength:  The authors propose a novel and efficient NeuralPCG  solver for linear second-order PDEs and present a framework for studying the performance of preconditioners in classic iterative solvers.\n\nWeaknesses: The paper is well written and easy to follow with clear comparison to the literature.  The significance and novelty of this paper are limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow with clear comparison to the literature. ",
            "summary_of_the_review": "The significance and novelty of this paper are limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1939/Reviewer_DNx8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1939/Reviewer_DNx8"
        ]
    }
]