[
    {
        "id": "s3sorT12L3",
        "original": null,
        "number": 1,
        "cdate": 1666282975997,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666282975997,
        "tmdate": 1666283087011,
        "tddate": null,
        "forum": "1FVv8PS8LYW",
        "replyto": "1FVv8PS8LYW",
        "invitation": "ICLR.cc/2023/Conference/Paper2270/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper formulates the fairness setting with a bi-level optimization on finding good group ratios. Some approximation techniques are proposed to overcome the difficulty of bi-level optimization.\n",
            "strength_and_weaknesses": "Strength\n\n+ This paper formulate the fairness setting with a bi-level optimization on finding good group ratios.\n\nWeaknesses\n\n- \"\u03bby,z is the ratio for group z in class y, \u00b5y,z is the ratio for real data in the (y, z)-class, and \u03bb and \u00b5 are the sets of all \u03bby,z and \u00b5y,z\". It is reasonable to adjust \u03bb due to different generated data qualities, but why is adjustment of \u00b5 beneficial (not using all available real data)? simply due to class imbalance? For different generated data qualities, the ideal case is to weed out generated data with low qualities? Is this tried in the proposed algorithm? Group ratio is a quite coarse measure.\n\n- One limitation of this paper is that it applies only when synthetic data samples are generated.\n\n- If FairBatch has a theoretical guarantee to find the optimal group ratios, why does \"Dr-Fairness achieves much higher accuracy with similar or better fairness than FairBatch\" if they converge to similar group ratios?\n\n- Experiment results are better in CelebA than ImageNet People Subtree. So will more complex data hurt Dr Fairness even more? Any insight?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThis paper is well organized.\n\nQuality\n\nThe paper is technically solid in general.\n\nNovelty\n\nThe novelty of this paper is reasonable as a new fairness setting is formulated with a bi-level optimization on finding good group ratios.\n\nReproducibility\n\nGood.",
            "summary_of_the_review": "This paper formulates a new fairness setting with a bi-level optimization on finding good group ratios. However, to overcome the difficulty of bi-level optimization approximation techniques are proposed, which may hurt the quality of found group ratios.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2270/Reviewer_SsCr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2270/Reviewer_SsCr"
        ]
    },
    {
        "id": "b5nVCGa0Vg",
        "original": null,
        "number": 2,
        "cdate": 1666497111566,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666497111566,
        "tmdate": 1666497111566,
        "tddate": null,
        "forum": "1FVv8PS8LYW",
        "replyto": "1FVv8PS8LYW",
        "invitation": "ICLR.cc/2023/Conference/Paper2270/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies machine learning fairness in visual recognition. The unifying idea of this paper is to use both real and generated data to improve fairness via adaptive sampling. The iterative procedure adaptively adjusts data ratios between real and generated data based on feedback from a classifier. The authors cast this problem into a bi-level optimization and accelerate the solving process with some tricks. Experiments on image data demonstrate the effectiveness.",
            "strength_and_weaknesses": "Pros:\n\n1. The high-level idea makes sense to me. The authors want to precisely sample data from different groups and generated data to achieve better fairness and lower accuracy cost. Sample data more carefully should work for this problem because all biases inherently come from data.\n\n2. The paper is technically sound.\n\n3. The experiments are comprehensive and solid. Compared to FairBatch, the proposed method shows a better fairness as well as good tradeoffs between fairness and accuracy.\n\nCons:\n\n1. The authors give limited explanations on 'generated data.' I would query what kinds of data are 'generated data' for algorithmic fairness, and what is the distributional assumption of such kinds of data.\n\n2. Some clarifications are needed in the current version. For example, the statement 'In particular, proper usage of generated data can reduce the accuracy degradation caused by over-sampling minority groups from real data' lacks corresponding explanations.\n\n3. The motivation for using generated data can be further elaborated. In my opinion, the proposed method can also be adapted to reweighing training data only without using any external data. Why there are different treatments between the original data and generated data in this case?\n\n4. Since this paper works on data resampling and reweighing, probably the authors want to focus on similar techniques in the related work part. I list three relevant works on data reweighing for authors' reference [1,2,3].\n\n[1] Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification, WWW'18\n[2] Identifying and Correcting Label Bias in Machine Learning, AISTATS\u201820\n[3] Achieving Fairness at No Utility Cost via Data Reweighing, ICML'22",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is well-written and the clarity is good.\n\n2. The proposed techniques are somehow novel.\n\n3. Good reproducibility.",
            "summary_of_the_review": "In sum, I think the paper brings some interesting ideas to this field. However, I hold some concerns about the generated data part and the later usage. I am looking forward to the authors' response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2270/Reviewer_geXT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2270/Reviewer_geXT"
        ]
    },
    {
        "id": "8lp9oA2iRa",
        "original": null,
        "number": 3,
        "cdate": 1666639466794,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639466794,
        "tmdate": 1668784139738,
        "tddate": null,
        "forum": "1FVv8PS8LYW",
        "replyto": "1FVv8PS8LYW",
        "invitation": "ICLR.cc/2023/Conference/Paper2270/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied the unbalancing problem in data generation based fairness, i.e, the generated data should be balanced or adjusted to control the prediction/fair performance with raw data. Based on this idea, an bi-level optimization (through implicit function) is proposed to learn the balancing coefficient $\\lambda$, $\\mu$. Empirical results on CelebA/ImageNet demonstrated its effectiveness. \n",
            "strength_and_weaknesses": "### Pros\n- This paper considered a meaningful problem in data-augmentation based fairness, i.e, rebalancing the training loss of generated and real data. Furthermore a bi-level objective is proposed to automatically balance the sensitive attribute and label balancing weights. The idea sounds reasonable and empirically evaluations are conducted. \n\n### Cons:\n- This paper focused on a very narrow improvement in technical fair learning without proper motivation in social aspects. Thus I would say the paper scope and significance are quite modest. \n- The bi-level objective is not well-motivated. The framework could also be realized by DRO (distribution robust optimization).  \n- Lack of clear discussion on previous work.\n\nOverall I would say, this paper has merits, while they are rather modest in terms of technical part and understanding fairness.\n\n### Comments on cons\n\n1. [Paper scope] This paper focused on the specific aspect in data augmentation based fairness. I.e, data generation to improve unfair predictions. However, data generation through generative models is generally difficult. Thus I would be a bit unsure about the importance of fair generative models in data-augmented fair learning or general fair learning. \n2. [Technical motivation] The key contribution is to introduce the label and sensitive attribute reweighted loss to balance different factors. While I think this framework could also be achieved by the DRO framework by properly adjusting the weights of different groups/labels. Why not DRO? Is it necessarily to formulate the problem as a bi-level objective?\n3. [Related work/Technical novelty] The bi-level objective in fair learning or data-augmentation are not entirely new. Discussions on related work seem quite lacking. For example, fair-mixup [1] as a typical data augmentation approach. As for the bi-level in learning fairness is not new, for example fair-path [2] indeed introduced bi-level objective and implicit function to learn fair representation. \n4. [Impact on real-world fairness] This paper focused on a nich technical point in learning the balancing coefficient. However, this paper does not consider the real-world fairness impact. Please take note that algorithmic fairness is not merely a method to improve state-of-the-art. An important aspect is to understand the social impact of proposed fair analysis. Unfortunately, this part is completely missing in the paper, making the contribution in fairness (non technical) quite modest. \n5. [Experiments, Impact on real-world fairness] The experiments are fine to validate the improved performance on benchmark but are not well-motivated to support the real-world fairness. We could always have better results on some benchmarks while the real-world discrimination still is not addressed. \n6. [Technical correctness] In Sec 2.1 and Sec 2.2 the loss constraint is not equivalent to enforce EO. They could be equivalent only in certains specific assumptions such as the loss is calibrated. This part is not correct for me. \n7. [Claims on achieving fair and accuracy] In many parts, this paper claimed the fair and accuracy could be both improved, this is quite a big claim. In fact, this paper did not provide rigorous justifications on why/when/how these could satisfy. I would think there is an over-claim of the contribution.  \n\nReference \n[1] Fair mixup: Fairness via interpolation. ICLR 2021\n[2] Fair Representation Learning through Implicit Path Alignment. ICML 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general this paper is clearly written and easy to follow. The reproducibility seems high. \n",
            "summary_of_the_review": "[ I would recommend that *the paper is generally fine, but not good enough* (4/10). While it seems that no such a score is provided. Thus I would currently recommend 3.]\n\nThis paper studied the unbalancing problem in data generation based fairness, i.e, the generated data should be balanced or adjusted to control the prediction/fair performance with raw data. Based on this idea, an bi-level optimization (through implicit function) is proposed. \n\nThis paper has merits, while they are rather modest in technical parts (see 1-3, 6 in detailed comments) and understanding fairness (see 4-5, 7 in detailed comments).\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2270/Reviewer_ZWfo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2270/Reviewer_ZWfo"
        ]
    },
    {
        "id": "HxNz-FptVyb",
        "original": null,
        "number": 4,
        "cdate": 1666663250904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663250904,
        "tmdate": 1666663250904,
        "tddate": null,
        "forum": "1FVv8PS8LYW",
        "replyto": "1FVv8PS8LYW",
        "invitation": "ICLR.cc/2023/Conference/Paper2270/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This submission develops a novel adaptive sampling approach that leverages both real and generated data for fairness. The authors design a bilevel optimization that finds the optimal data sampling ratios among groups and between real and generated data while training a model. The proposed method exhibits desired trade-off between accuracy and fairness on the CelebA and ImageNet People Subtree datasets.",
            "strength_and_weaknesses": "Strength: Different from Fairbatch, which finds the optimal group ratio for fairness only using real data, Dr-Fairness optimizes  not only group ratio but also sampling ratios between real and generated data. Therefore, Dr-Fairness can enjoy the potential of both real and generated data. It design a bilevel optimization to perform adaptive sampling.\n\n\nWeakness: The main novelty of the method lies in extending Fairbatch to Dr-Fairness which seems marginal. Specifically, Dr-Fairness adopts a conditional generator to geerate samples and adopt the to-be-learned sampling ration for real and generated data, which is similar to the group ration. The authors stated that they propose a simple approximation. I am not sure which part is new given the implicit function theorem and its corresponding work. Besides, this work needs more auxiliary information from pre-trained generator. And, from the experiments on ImageNet People Subtree test set, we find that the performance of Dr-Fairness is highly dependent on the quality of the generated samples. Also, it is unclear why the authors claimed that Dr-Fairness achieves the state-of-the-art accuracy and fairness, which is not.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow. It considers an interesting problem in fairness, and has originality. But the proposed method relys on an additional generator  which might be not easy to reproduce, especially for image datasets that are difficult to generate.\n\n\n\n",
            "summary_of_the_review": "Although the proposed method is reasonable and achieves desired trade-off between acurracy and fairness, the proposed method only combines existing components. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2270/Reviewer_niP4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2270/Reviewer_niP4"
        ]
    }
]