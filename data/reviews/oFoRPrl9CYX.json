[
    {
        "id": "LLP-2yGtGJm",
        "original": null,
        "number": 1,
        "cdate": 1666536322928,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666536322928,
        "tmdate": 1669645042756,
        "tddate": null,
        "forum": "oFoRPrl9CYX",
        "replyto": "oFoRPrl9CYX",
        "invitation": "ICLR.cc/2023/Conference/Paper3159/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "**Update after rebuttal**\nThe authors added an important control experiment (fluid in-polarity) which shows that the main improvements result from transferring a favorable initial set of weights, whereas actually freezing the polarity (which is *the* main NI inspiration) has a a marginal effect (and it is conceivable that this effect mainly results from very early layers). This makes the empirical results in the paper stronger, but significantly weakens the link to the NI inspiration (after all, as the authors point out, polarity change in biological brains is rare). The authors chose to largely keep the emphasis on NI and reformulate one, two sentences (where explicit claims were made) rather than shortening the discussion to a single paragraph in the discussion - this is a rhetoric trick to save work with re-writing, but essentially leaves most of my criticism unaddressed. Overall, the authors rebuttal was quite deflective and many of the issues I raised were not addressed or only minimally addressed (see detailed comments to author feedback). I therefore remain with my final verdict, that at its core the paper explores an interesting phenomenon in transfer learning (a combination of good initialization and a regularizing effect of fixing polarity); the connections to NI are misleading and contradictory (if freezing polarity is so important, how can the fluid-in-polarity results be explained; in fact it is currently unclear what fraction of weights changes polarity in that setting). From a purely transfer learning perspective it is unclear whether the observed effects also hold in a regime where models are practically useful (currently results where the proposed scheme performs well are in the low accuracy regime); limiting the impact of the work to practitioners aiming to improve classifiers' performance beyond the very low data regime. Given the additional control experiments and some of the small fixes I would increase my score by one point (from 3 to 4), but the current scoring system only allows a 5 as the next score and I personally think that the paper in its current form is not borderline. I remain open to updating my final verdict based on the reviewer discussion of course.\n\n\nThe paper investigates the effect of fixing the signs of weights (polarity) in an artificial neural network during training by using an oracle (expert knowledge or pre-trained network). This idea is inspired by the fact that \u201cneuronal connections in the brain rarely see polarity switch\u201d. As the paper shows on some simple datasets, if signs of weights are set correctly (and not randomly) and held fixed during training, faster / more sample-efficient learning is possible compared to a network that does not use the oracle knowledge (i.e. a standard neural network). Some theoretical insight is provided via the argument that fixed polarity leads to a reduction over the functions representable by the network, and thus a reduced search space over functions via exhaustive search. The second set of experiments shows that transfer learning in the very low data regime with pre-trained ImageNet weights to FashionMNIST and CIFAR-10 works slightly (but statistically significantly) better when only transferring weight polarity but not magnitude. \n\nDisclaimer: I have reviewed a previous version of this manuscript, and am happy to see that the authors have taken into account some suggestions raised by the reviewers, but have chosen to ignore some other suggestions. My review might thus be somewhat repetitive in some places (I have not recycled anything from my previous review though).\n",
            "strength_and_weaknesses": "**Main contributions, Impact**\n1) Empirical analysis of the gains of freezing weight polarity during training using oracle knowledge. To the best of my knowledge this is novel. Results confirm the expectation that additional knowledge helps reduce the required number of training examples / increases sample efficiency by providing additional biases. While the result is interesting, its practicality is very limited since this oracle knowledge is often unavailable.\n\n2) Empirical analysis of using pre-trained signs instead of pre-trained weights for adapting from ImageNet to FashionMNIST/CIFAR-10. In the very low data-regime, where the model still has very large validation errors, there is some small, but consistent improvement (Fig. A.3 gives a better sense of the scale). This could potentially be interesting, but is currently of very limited impact due to the following two issues: (i) effect currently only holds in a regime where the model performs very poorly and disappears for large enough sample sizes from the transfer dataset; (ii) current experiments cannot rule out that we simply observe a regularizer that prevents overfitting to the small dataset instead of transferring useful knowledge (the missing control experiment for this is Freeze RAND-polarity).\n\n3) Simple theoretical analysis that fixing the polarity of weights reduces the cardinality of the space of possible representable functions. This trivially leads to a reduced search time under exhaustive search. Impact: marginal - the result is very simple to construct, and its relevance for training a network via SGD is unclear (SGD is different from exhaustive search and a reduction in cardinality of the hypothesis space does not trivially translate into faster and more sample efficient SGD training).\n\n**Strengths**\n * Investigation of an effect not studied before (to the best of my knowledge)\n * Most of the paper is written clearly\n * Statistical significance analysis of some results\n\n**Weaknesses**\n * Practical relevance currently seems minor: in many cases oracle knowledge around the correct polarity is not available, and the current transfer learning results are not very convincing (only small improvements in a regime where the performance of the classifier is quite bad).\n * Theoretical analysis has no link to training neural networks via SGD.\n * Connections and claims w.r.t. natural intelligence, learning in biological brains, and neuroscience are overstated, vague, and of insufficient scholarly standard. What\u2019s needed as a minimum is a technical discussion of learning in dynamic models of neural processing (spiking neurons or other models involving short- and long-term neuronal **dynamics**); talking about the role of excitatory and inhibitory synapses by referring to the sign of weights in an extremely simplified rate-based model only allows for extremely simple statements - the paper makes claims far beyond these. This would be tolerable if toned down and limited to a small paragraph in the discussion, but currently occupies roughly a third of the paper throughout all sections.\n\n**Improvements**\n\n1) Remove the claims and analogies w.r.t. biological brains and how this paper uncovers the main reason why evolutionary optimization has led to fixed polarity of \u201cneuronal connections\u201d. It is ok to use this fact about biological neural networks as an inspiration (one, two sentences in the intro) and then have one paragraph of discussion, but none of the current discussion is actually necessary for the technical part concerning artificial neural networks and SGD. If the amount of space devoted to this is kept, then the scholarly standard needs to be raised significantly. As a starting point I strongly suggest reading up on *spiking neuron models* and various types of *short-term plasticity*, as well as neuro-computational models using circuits of spiking neurons and population dynamics. Many basic textbooks in computational neuroscience cover the topic - I can recommend \u201cSpiking Neuron Models: Single Neurons, Populations, Plasticity\u201d by Gerstner and Kistler 2002. To defend against this criticism raised during the last round of reviews the paper says \u201cAs our goal is to see the pure effect of fixing weight polarity, we did not adopt any bio-plausible learning algorithms as they may introduce confounding factors.\u201d - this is fine and reasonable, as long as **NO** claims are made w.r.t. biological learning or brains. If such claims are made they need to be discussed in the context of biologically plausible neuron models and learning algorithms.\n\n\n\n2) Important questions w.r.t. the neuroscientific connections currently not answered in the paper:\n  * Biological neural networks are much more dynamic than artificial neural networks (= simple rate based models). Neurotransmitters, short-term plasticity, electrochemical depletion, all the way to the dynamic recruitment of whole neuronal circuits and populations can play a huge role in the function of the brain for rapid adaptation (few show learning) and transfer learning - and different excitatory and inhibitory mechanisms play a significant role, and have been well studied, in each of these. The paper currently only says \u201cneuronal connections in the brain\u201d - what exactly does that refer to (synapses, neurons, populations?), and what exact excitatory and inhibitory mechanisms is the paper trying to explain?\n  * The experiments in the paper use artificial neural networks (= simple rate based models) trained via SGD. Most neuroscientists would claim that results obtained from these have very limited bearing on explaining the function of biological neural networks and biological learning. Why do the experiments in this paper have biological relevance, and what exactly is that relevance?\n  * The one neuroscientific reference provided in the paper (Spitzer 2017), except for the discussion, seems quite irrelevant - it is about the (rare?) phenomenon of \u2018neurotransmitter switching\u2019 where \u201cNeurotransmitter receptors on postsynaptic cells change to match the identity of the newly expressed neurotransmitter.\u201d [from the paper\u2019s abstract]. While the phenomenon is potentially associated with certain neural diseases and addiction, it is not hypothesized to have a connection with few-shot learning or transfer learning. Why choose this as the one reference?\n  * \u201cpost-development, neuronal connections in the brain rarely see polarity switch\u201d. What developmental stage is exactly meant here? What \u201cconnections\u201d are meant? Be precise, and add citations.\n* \u201cWhat then makes it so that our brains may have willingly chosen to give up on a vast portion of their representation capacity? Our answer is: to learn more quickly.\u201d I disagree with this answer. The widely agreed upon reason (to the best of my knowledge) is that biological (spiking!) neurons communicate activations via increases in firing rate; without inhibitory connections stable dynamics in circuits of such neurons are impossible (activations, and thus firing rates would reinforce each other without bounds, which quickly drives neurons beyond the biologically feasible operating regime; there are hard physical limitations for maximum firing rates of neurons and neural populations - inhibitory mechanisms are necessary to allow any computation at all). Please clarify.\n\n3) Figure 2 and 3 also need to show Liquid (IN-)polarity, where the correct polarity (but not the weight magnitudes for Fig 3) are used to initialize the network, but then the polarity is free to change during training. This answers whether it is important to freeze the polarity (as proposed in the paper), or whether it\u2019s sufficient to only initialize it correctly.\n\n4) Fig. 3 is missing the Freeze RAND-polarity experiment, where the correct proportion of polarities is frozen but not necessarily for the correct connections. This allows ruling out that  freezing polarity simply provides additional regularization but does not lead to any transfer of information from the pre-trained network. \n\n5) Perform the transfer learning analysis in Fig. 3 layer-wise (i.e. transfer polarity for the first layer only, then the first two layers, etc). One hypothesis to explain the results in the paper is that the transfer of Gabor-like filters in very early layers explains a large fraction of the observed performance gains. Sign-information might indeed be virtually all that\u2019s needed for Gabor-like filters. Transferring whole weights for the final classification layer might cause most of the observed decrease in performance for Liquid IN-weight.\n\n6) Ideally repeat Fig. 3 with a validation set and early stopping instead of a fixed set of epochs, to rule out effects of overfitting on the very small datasets.\n\n\n**Minor comments**\n\nA) Change in notation from \u2018Fluid\u2019 (Fig 2) to \u2018Liquid\u2019 (Fig 3).\n\nB) \u201cIn the literature of bio-plausible artificial neural networks (ANNs), the most related work is on Dale\u2019s principle: a single unit\u2019s output weights are exclusively excitatory or inhibitory (Dale, 1935).\u201d With all due respect, but the literature on biologically plausible neural networks was at best at its infancy in 1935 - if this is the closest reference to this work, I would see this as a strong sign of concern in terms of biological plausibility (not a sign of groundbreaking novelty of the current work). \n\nC) \u201cOur angle is completely novel in the neuroscience literature.\u201d This is too strong; there is a vast body of literature on the relationship between excitatory and exhibitory circuits/neurons/synapses/neurotransmitters and their role e.g. in decision-making and short-term adaptation and motor control. While the core idea of fixed polarity might be an interesting hypothesis to explore seriously from a modern neural dynamics perspective, the current work fails to bridge this gap.\n\nD) \u201cAs discussed in Sec 2, such an approach intrinsically put Frozen-Net in a disadvantageous position\u201d. This is not entirely correct: the net is in a disadvantageous position in terms of representing any possible function - if the polarity is set correctly the net is in a highly advantageous position since the search-problem has been greatly reduced (under exhaustive search).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing in the paper is generally clear, except for the vagueness around many of the claims w.r.t. natural intelligence and biological neural networks.\n\nQuality:\n * Neuroscientific motivation, connections and learnings - poor.\n * Experiments: some important ablations missing to solidify the main claims and rule out alternative hypotheses, but multiple repetitions of runs and stat. significance checks - good.\n * Theory - ok but not very relevant (because of limitation to exhaustive search) - poor.\n * Discussion of related work: very long and raises many connections, overall rather superficial though and could be strengthened by being more precise - ok.\n\nNovelty: the main technical idea and experiments in the paper are novel to the best of my knowledge.\n\nReproducibility: Have not checked carefully but some minor experimental details might be missing; maybe worth doing another pass on this, but nothing of major concern.   \n",
            "summary_of_the_review": "Overall I think the paper has an interesting technical core (investigating the effect of frozen weight polarity, provided by an oracle / pre-trained net). There are some alternative hypotheses that need to be ruled out via control experiments (see improvements) and the results are currently a bit limited in terms of practical relevance. Particularly, the transfer learning results would be very interesting if observed in more challenging settings and in a regime where the classifier actually performs reasonably well. The theory does not add much to the paper unless it is extended to include SGD, which currently does not seem trivial. In my opinion, the neuroscientific/neuroevolutionary speculation currently lowers the overall quality of the paper - I suggest removing it completely from the current work, since it is not necessary and does not add to the technical part on artificial neural networks trained via SGD. As the paper currently stands I recommend rejection - even if the neuroscientific motivation is removed, the results (though promising) are not quite above the threshold for an ICLR publication (but would make an interesting workshop contribution). Having said that, I think compared to the version I previously reviewed, the paper has made good progress in the right direction.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3159/Reviewer_8Q8m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3159/Reviewer_8Q8m"
        ]
    },
    {
        "id": "8tyrk9aGLV",
        "original": null,
        "number": 2,
        "cdate": 1666634434649,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634434649,
        "tmdate": 1666634434649,
        "tddate": null,
        "forum": "oFoRPrl9CYX",
        "replyto": "oFoRPrl9CYX",
        "invitation": "ICLR.cc/2023/Conference/Paper3159/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper argues that in biological systems the weights\u2019 polarities (signs) remain fixed, and only magnitudes (synaptic plasticity) change over time, leading to faster learning. Inspired by this argument, the authors propose to fix the polarity of weights in the DNNs to learn and transfer faster. Towards this the authors propose a learning algorithm, called Freeze-SGD, whereby given a predefined polarity of all the weights in the network, the weights\u2019 signs are kept fixed throughout training and only magnitude is updated. The authors argue that proper task-based configuration of the polarity before training is important for faster learning. The authors further argue that transferring polarity instead of the weight patterns (magnitude + sign) is a more effective and compressed form of knowledge transfer. The experiments are conducted on classification tasks using a synthetic 5-XOR and image CIFAR-10 dataset.",
            "strength_and_weaknesses": "**Strengths**\n\n1. While I am not an expert in biological neural networks literature, it seems the fact that the weight signs don\u2019t change throughout learning is an important observation and should be exploited more in the artificial neural networks.  \n\n\n**Weaknesses** \n\n1. **Experiments**: I feel that the claims made by the authors, especially on ImageNet $\\mapsto$ CIFAR10 transfer learning setup, are not well justified by the experiments. \n\n* **Hyper-parameter setup**: The authors use a fixed learning rate 0.001 and number of epochs 100 for all the setups? How are these hyper-parameters determined? Could it be the case that this particular learning rate setting favors the fixed polarity baseline more?\n\n\n* **Sec 3, ImageNet pre-training for polarity determination**: Obtaining weight polarities from the ImageNet trained model, and then training it on CIFAR10, and comparing it with randomly initialized model on CIFAR-10 seems unsatisfying. It is well-known in the over-parameterized network\u2019s theory that the SGD trained network remains close to the initialization, so already by transferring the polarity from ImageNet to CIFAR-10, you start with a model with a good knowledge of the task making the overall comparison unfair. \n\n* **Sec 4: Transferring polarity is better than transferring weights is not well-justified from the experiments**: It can be seen from Fig 3, that in fact transferring the weights and freezing them works better than transferring polarity in terms of statistical efficiency. Similarly, from A.3 it can be seen that for different thresholds of the validation accuracy different methods work better making it difficult to see whether there is a clear pattern where one method is consistently better. In fact, the authors themselves say that, given their experiments, \u201c... polarity configuration is an effective medium, if not superior to weights pattern\u201d. If it is not consistently superior I am not sure then what\u2019s the point of the experiments and how can the authors conclude that polarity is a more effective and faster transfer mechanism. By the way, it could still very well be the case that in all these experiments the hyper-parameter settings are sub-optimal for the weight transfer baselines (as mentioned above). \n\n\n* Overall, I feel that the authors only reported their experiments on two rather small settings, one of which was in fact a toyish synthetic setting. Even on CIFAR10, the experiments were not convincing enough to establish the points made by the authors. \n\n\n2. **Writing**: It seems to me that the paper was written with a neuro-science audience in mind whereas most of the ICLR audience are ML researchers. Since the arguments made in the paper are not very difficult to follow so it may not make much of a difference. Nevertheless, it would be good to mold the paper along ML lines. For example, define the setup properly \u2013 metrics, train/ eval protocol etc, define what is a \u201cSuccess rate\u201d. Maybe even define what is the meaning of \u201cpolarity\u201d. Overall, make paper more self-contained.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing needs more clarity. Please refer to the strengths and weaknesses section above for details.",
            "summary_of_the_review": "Please refer to the strengths and weaknesses section above for details.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3159/Reviewer_afAt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3159/Reviewer_afAt"
        ]
    },
    {
        "id": "kScxilFLoRb",
        "original": null,
        "number": 3,
        "cdate": 1666687463284,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687463284,
        "tmdate": 1667238036432,
        "tddate": null,
        "forum": "oFoRPrl9CYX",
        "replyto": "oFoRPrl9CYX",
        "invitation": "ICLR.cc/2023/Conference/Paper3159/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Connections between neurons in the brain are almost universally exclusively excitatory or inhibitory, which seemingly reduces the representational capacity of biological networks. Inspired by this observation, and the assumption that the brain has been optimized throughout evolution, the authors propose that by freezing the sign of connections between neurons may facilitate faster and more data-efficient learning. ",
            "strength_and_weaknesses": "Strengths\n- Interesting and important direction of research with potential improvements for AI and insights for both AI and neuroscience.\n- If the brain is following this strategy it is a very interesting theory for why the brain has the restrictions it does. \n\nWeaknesses\n- The main contribution of this work is unclear to me. There seems to be two separate issues that are not adequately separated in the text and experiments: (1) the initialisation of a network with a task-specific pattern of signs that results in better learning (2) the freezing of signs during learning. The experiments lack the controls of \u201cFluid sufficient-Polarity\u201d, and \u201cFluid IN-polarirty\u201d which would be critical to understanding this. My suspicion is that (1) mainly underlies the results. \n- The networks do not reflect the type of polarity restrictions as found in the brain (all of the outgoing connections from a neuron are of one polarity). Though the author\u2019s acknowledge this in the discussion, it is not clear to me how this is work is a bridge. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This work is of high quality and seems novel. Although the paper is generally well written, the authors could make it clearer. In particular, the methods and implementation details could be clearer and more organized. What it means for a function to be representable could be defined.  \n\n",
            "summary_of_the_review": "While this is interesting and important work the main contribution is unclear to me - is freezing important or just choosing signs apriori. The connection to the biological networks seems lacking and should be developed in order to make the claim that this is the strategy the brain is employing. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3159/Reviewer_R5EG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3159/Reviewer_R5EG"
        ]
    },
    {
        "id": "X3Bx3AbZwy",
        "original": null,
        "number": 4,
        "cdate": 1666724570296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724570296,
        "tmdate": 1666724570296,
        "tddate": null,
        "forum": "oFoRPrl9CYX",
        "replyto": "oFoRPrl9CYX",
        "invitation": "ICLR.cc/2023/Conference/Paper3159/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explores the computational features that result from a neural network with weights of fixed polarity. The authors demonstrate through proofs in constrained settings, and experiments in both constrained and more general settings, that networks with fixed and appropriately initialized weight polarity learn faster than randomly initialized networks. They also demonstrate that for randomly initialized networks of sufficient size, constraining the polarity of the weights in the update function does not compromise learning.\n\nThe paper proposes  a learning algorithm which is a variant on stochastic gradient descent but which does not permit polarity changes in the weights (Freeze-SGD). The paper compares networks trained under this algorithm (frozen nets) with networks trained under regular SGD, including networks that have previously had some pretraining. They present a theoretical trade-off between network representational capacity and learning speed, as a function of the network\u2019s polarity being fixed.\n\nIn experiment 1 they consider single-layer networks on the 5D-XOR problem (the first 2 dims are classic XOR and the final 3 dims are noise). They compare learning rates of 3 networks to consider the influence of two factors: fixed weight polarities and well-chosen polarity patterns. The first net is initialised with the weights set to have the correct polarity combination for the task (a priori knowledge) and which uses a polarity-frozen update rule the authors propose. The second network uses this same update rule but is initialised with random small weights (presumably of the same scale), and the final network uses random weights and vanilla SGD. The network which had both the correct % polarity and used the polarity-preserving update rule learned the fastest.\n\nIn experiment 2 they consider image classification with AlexNet on the datasets Fashion-MNIST and CIFAR-10, and consider the same three initialisation and update conditions.  \n\nIn experiment 3 they compare initialising the network with ImageNet-trained weights vs Imagenet-trained polarity %s, to assess whether transfer between tasks is more effective when transferring the weights magnitudes and polarities or simply the  polarity alone (both under the polarity-preserving update rule). This yielded a very surprising result that polarity alone resulted in faster learning on the new tasks. \n\nThe paper then calculates the probability that a randomly initialised network with a fixed polarity update rule will never be able to learn the XOR task. The experimental results closely match the theoretical results.\n",
            "strength_and_weaknesses": "*Strengths:*\n- A very interesting idea of broad and general interest to both the computational neuroscience community and the ML community.\n- The authors explore the idea of fixed weight polarity is some depth, discussing the theoretical benefits and costs of fixed-polarity networks (costs: limits on representational capacity, pros: learning speed).\n- Nicely designed experiments in both toy and more realistic settings (however see comment below on an important way they can be improved).\n- Beautifully written.\n\n*Weaknesses:*\n- In all experiments, but most crucially in experiments 1 and 2: The authors should do the final control experiment in which they initialize the network with the same weights as Freeze-sufficient polarity (set to be the correct % polarity) but without the frozen learning algorithm. This will truly isolate the two factors of (1) initially setting the weights % with the correct polarity, and (2) keeping that weight polarity throughout training. This yields a fully factorized experimental design which is tidier for isolating the two contributing factors and their interactions. It may be that initializing the network with the correct polarities but without constraining these polarities via the update rule, is sufficient for the network to gain the same benefits. This experiment would tell us these sufficiency conditions.\n\n- I couldnt see whether in each experiment the initial weights were set with the same generative process for the different experimental conditions (and so have the same magnitude distribution). To ensure that different weight magnitude distributions (which are known to affect learning rates) are not playing a role in these results, the Freeze- in polarity vs Freeze Rand, vs Fluid weights should all start with the same magnitude distribution, despite the differences in polarity. For these results to be interpretable this must be true - and so I assume this is the case - but I could not see this mentioned in the text (perhaps I missed it) and it\u2019s quite an important feature to highlight.\n\n- Could the authors comment on whether randomly initialised networks trained with SGD learn polarity early on in training? I am aware that weight magnitudes are typically learned very early on in training but am not sure whether polarity is also a commonly learned early feature of weights with SGD. If this is known, it would help to complete the picture and could be added to the discussion.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper communicated its ideas very clearly and these ideas (to my knowledge) appear very novel. There is one point on weight initialisation that the authors should clarify for reproducibility and interpretability to ensure this paper has as much impact as it has the potential to have. ",
            "summary_of_the_review": "This paper explores the computational features that result from a neural network with weights of fixed polarity. The authors demonstrate through proofs in constrained settings, and experiments in both constrained and more general settings, that networks with fixed and appropriately initialized weight polarity learn faster than randomly initialized networks, and that for randomly initialized networks of sufficient size, constraining the polarity of the weights in the update function does not compromise learning. In my opinion the findings are novel and very interesting, but subject to a few caveats that need confirming.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3159/Reviewer_JBKK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3159/Reviewer_JBKK"
        ]
    }
]