[
    {
        "id": "BaIjFThLcqc",
        "original": null,
        "number": 1,
        "cdate": 1666321432691,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666321432691,
        "tmdate": 1669328261195,
        "tddate": null,
        "forum": "uR6x8Be7o_M",
        "replyto": "uR6x8Be7o_M",
        "invitation": "ICLR.cc/2023/Conference/Paper6060/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a framework based on Slot Attention (for object-centric representation learning) and Transformer (for reasoning) to tackle the task of RAVEN's Progression Matrices. The input to the network is composed of 8 images as the context, and 8 images as candidate answers to be filled in as the ninth image. These images are generated synthetically, composed of geometric shapes of primitive shapes, sizes, colors, etc. The encoder module encodes each image into a set of slots, each of which corresponds to an \"object\" in the image and uses a transformer to rank all answers based on the slot-based representations.",
            "strength_and_weaknesses": "The proposed framework is straightforward and is well described in the paper. I personally very much like the simplicity of the proposed method. Tables 1 and 2 seem to be comprehensive lists of recent works on these tasks.\n\nDespite the fact that I like the overall idea of the paper (it is simple and improves over the baselines). However, there are still a few weaknesses in the current version of the paper.\n\nThe first question is at the conceptual level. So this paper claims that \u201csuggesting that an inductive bias for object-centric processing is a key component of visual reasoning\u201d in the intro. I think this is one of the most important contribution of the paper (testing object-centric inductive bias in visual reasoning). However, this inductive bias has been tested many times in similar contexts, including the ALOE paper this paper cited. Therefore, this paper has a relatively low scientific contribution.\n\nThe second comment comes with my first one. If we look at the model performances, this paper does not achieve state-of-the-art performance on the testing tasks. I understand that this is because of the simplicity of the framework / not using any problem-specific inductive biases. But this makes the contributions poorly supported. One idea is to test the proposed framework on other tasks.\n\nRegarding the non-SOTA performance on these benchmarks, I am wondering if this is because the transformer architecture is not designed for perform \"reasoning\" on this task. Specifically, if you look at the SCL paper from Wu et al, a major claim they are trying to make is that RPM-style tasks require reasoning about \"the meta-level relationship\" between concepts: understanding \"progression,\" no matter whether it is applied to the axis of object colors or object shapes. However, Transofrmers (and other GNNs) only have object-level parameter sharing, but not factor-level parameter sharing. I wonder if this could be the reason why it underperforms several baselines.\n\nThere are a few comments about the experimental design:\n\n1. Have you tried to train SlotAttention alone and just fix the encoding while training your transformer?\n2. What's the conclusitve message from Section 4.6? Should we always use a high lamdba?\n\nA few writing suggestions:\n\n1. I think the title is a bit too broad. You probably want to be a little more specific.\n2. The first paragraph in the intro does not have any supportive papers.\n3. Given the length of the paper, I think the authors can expand their description of the Slot Attention module a bit more. This will help make the paper more self-contained.\n4. Even though your model is not the state-of-the-art performer, you should still highlight the best-performing model in the tables.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presentation is clear. The experimental designs and results are reasonable.",
            "summary_of_the_review": "While overall I very much like the simplicity of the proposed framework, there are a few weaknesses that have been listed. I think the current version of the paper is not ready for publish at ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6060/Reviewer_7pmp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6060/Reviewer_7pmp"
        ]
    },
    {
        "id": "V3lmi6chTtN",
        "original": null,
        "number": 2,
        "cdate": 1666653436614,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653436614,
        "tmdate": 1669152936822,
        "tddate": null,
        "forum": "uR6x8Be7o_M",
        "replyto": "uR6x8Be7o_M",
        "invitation": "ICLR.cc/2023/Conference/Paper6060/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper tackles RPM (Raven\u2019s Progressive Matrices) problems where a set of context images are given and a continuation image is supposed to be selected from a given set of image choices. The images are governed by abstract rules which needs to be inferred by the method, which is claimed to parallel human reasoning capabilities. The authors propose a method which largely builds on the slot attention method of Locatello et al. On two RPM benchmarks (PGM, I-Raven), the authors show competitive performance compared to prior methods.",
            "strength_and_weaknesses": "Pros\n* Paper is easy to follow\n* Hyperparameter choices are discussed\n* Ablations show impact of each component\n* Helpful visualization of the trained slot representations\n\nCons\n* Scope of method is limited\n* Limited technical novelty as the method is largestly based on prior work\n* Experimental results are mixed and hard to contextualize due to lack of details about baselines\n* Technical details are not described in detail",
            "clarity,_quality,_novelty_and_reproducibility": "Slot attention is based on prior work (Locatello et al.). This makes it seem like this work is an application of an existing method to this problem. The authors need to clarify the main technical contributions of this paper. \n\nTechnical details (section 3.2, 3.3) are only verbally described. Although I can get the gist of the method, the specific details need to be discussed with better clarity (e.g. mathematical expressions). \n\nThe reconstruction loss is vaguely described as the slot decoder in Locatello et al. Paper needs to include these details to be self contained. \n\nIt is hard to spot the best methods in the result tables, which makes it difficult to gauge the relative performance advantage of the proposed method. \n\nThere needs to be a detailed discussion about the baselines providing intuition about each method being compared against.",
            "summary_of_the_review": "Raising my score to 6 post-rebuttal.\n\nThis paper builds on an existing approach designed for object centric processing and applies it to abstract reasoning problems involving structured image patterns. Although the problem and approach are interesting, the novel technical contributions of this work are unclear and the paper further has issues in the experimental setup (see detailed comments). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6060/Reviewer_4JwD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6060/Reviewer_4JwD"
        ]
    },
    {
        "id": "SpIt9rrQ1R",
        "original": null,
        "number": 3,
        "cdate": 1666734886075,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666734886075,
        "tmdate": 1666734886075,
        "tddate": null,
        "forum": "uR6x8Be7o_M",
        "replyto": "uR6x8Be7o_M",
        "invitation": "ICLR.cc/2023/Conference/Paper6060/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the problem of training deep networks to solve visual reasoning problems. It studies PGM and I-Ravens benchmarks, which pose challenging visual reasoning problems for humans, and explores if an end-to-end deep network can solve them.\n\nThe proposed method uses slot attention to individually encode each of the prompt images in an object-centric way (ideally separating the shapes in each prompt image) and combines them in another transformer to make the final prediction. The final training objective is reconstruction on each of the individual prompt images from the object-centric representation, and a classification objective for the task.\n\nOverall the proposed method performs comparably to the best models on the benchmarks, while claiming to be more general. ",
            "strength_and_weaknesses": "*Strengths*\n\n- The paper studies an interesting problem in training deep networks for challenging visual reasoning tests given to humans.\n- Rather than having an overly hand-crafted solution that is not general, the proposed approach does seem to maintain good generality while still performing well on the task. \n- The paper is written clearly.\n- Ablations suggest all parts of the proposed model are important.\n\n*Weaknesses*\n\n- I'm not very familiar with this domain, so it is a bit difficult to evaluate the experiments. For example, it does seem like another approach SCL outperforms the method on both benchmarks. However, the authors claim that their proposed architecture is more general than that of SCL. The paper should do a better job of explaining exactly what ways the baselines which outperform their approach are less general, right now the claim is not well supported. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall clarity, quality, and novelty seem strong. ",
            "summary_of_the_review": "The paper does seem to propose a novel approach for learning difficult visual reasoning tasks without too much hand-crafted structure. Results seem decent but are a bit hard to evaluate without detailed knowledge of the baselines, which perform better but the authors claim are less general. Overall I would defer to other reviewers who have more knowledge of this line of work. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6060/Reviewer_hgxS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6060/Reviewer_hgxS"
        ]
    },
    {
        "id": "KORDrmLdo42",
        "original": null,
        "number": 4,
        "cdate": 1667593142818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667593142818,
        "tmdate": 1670206277557,
        "tddate": null,
        "forum": "uR6x8Be7o_M",
        "replyto": "uR6x8Be7o_M",
        "invitation": "ICLR.cc/2023/Conference/Paper6060/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work finds a simple model (called STSN) consisting of slot attention and transformer modules. Without incorporating inductive biases specific to the RPM problem format, this model still displays near-SOTA performance on the I-RAVEN and PGM datasets. Such performances suggest the importance of object-centric processing within visual reasoning.",
            "strength_and_weaknesses": "Pros:\n1. It is valuable to show that object-centric models using modern modules (slot attention + transformer) can perform well on the RAVEN and PGM datasets.\n2. It is informative to have ablation studies to show the improvement of each module in Table 3.\n\nCons:\n1. It is unclear what contributions this work presents. The slot attention and transformer already exist, and this work piles them up.\n2. This work claims STSN is a general-purpose architecture but do not show its performance over other visual reasoning tasks other than RPM tasks (like CLEVR [1]).\n3. The model size is large compared to SCL. It is unclear whether the larger model size is a cost for general-purpose. Can you do ablation study over the size of the model (like the size of hidden dimension or the number of layers of Transformer)?\n\nMinors:\n* Abstract: \"This work has ...  \", a bit confusing which work this part refers to, your work or the recent work in last sentence? (Also in Intro: \"Much of this work ...\")\n* Sec 3.3 cnadidate -> candidate\n* Figures 4 to 6 are not fit well into the page.\n* The last page of the references needs re-format.\n\n[1] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., & Girshick, R. (2017). Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2901-2910).",
            "clarity,_quality,_novelty_and_reproducibility": "The model architecture is clear overall, but the concrete contributions need to be emphasized.\n\nThe novelty is low, the network architecture is a patching up of already exist models (slot attention, transformer, TCN) and does not achieve new SOTA. The importance of object-centric processing within visual reasoning is already well-known.\n\nThe results seem reproducible given the details in the text, but open-source would much increase reproducibility (or indicate the open source upon acceptance).",
            "summary_of_the_review": "The contributions are not clear. As the model does not achieve new SOTA on both the I-RAVEN and PGM datasets, if the contribution goes to a general-purpose visual reasoning model, it should show its performance in other reasoning tasks that are related to objects.\n\n---\nUpdate (2022/11/22): raise from 3 to 5 after reading the rebuttal, comment below.\n\n---\nUpdate (2022/12/04): raise from 5 to 6. I hope the authors continue improving their paper (about clarity and rigorousness) and include the points they addressed in the rebuttal.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6060/Reviewer_j7eh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6060/Reviewer_j7eh"
        ]
    }
]