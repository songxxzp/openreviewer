[
    {
        "id": "vU4ydV0c1OZ",
        "original": null,
        "number": 1,
        "cdate": 1666174969087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666174969087,
        "tmdate": 1666176241353,
        "tddate": null,
        "forum": "qs2YCziX2o-",
        "replyto": "qs2YCziX2o-",
        "invitation": "ICLR.cc/2023/Conference/Paper3810/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies dimensionality reduction techniques for estimating the kernel distance up to a (1+eps)-factor. First, the paper shows one can sample the random Fourier features to preserve $\\Vert\\phi(x) - \\phi(y)\\Vert_2$, where $\\phi$ is the feature map such that $\\langle \\phi(x),\\phi(y)\\rangle = K(x,y)$ for some shift-invariant and analytic kernel $K$ and then applies to kernel k-means clustering. Then the paper shows a lower bound on the target dimension, which depends on the \u2018condition number\u2019 of the point set. The earlier upper bound requires the kernel function to be analytic at the origin, which excludes the Laplacian kernel. To remedy for this, the paper uses the fact that an $\\ell_1$ metric space can be embedded into a metric space of metric $\\Vert x-y\\Vert_2^2$, thus turning the dimensionality reduction for a Laplacian kernel into that for a Gaussian kernel, for which the earlier random Fourier feature sampling scheme applies. ",
            "strength_and_weaknesses": "Strengths: Giving dimensionality reduction results for distances induced by shift-invariant kernels, where the kernel is analytic or a Laplacian kernel. The idea is simple (which is good) and some parts of the proof are nontrivial.\n\nWeaknesses: It is not entirely convincing that the method for the Laplacian kernel is a bigger framework for other kernels. The bound $\\max\\{\\epsilon^{-1}\\log^3(1/\\delta), \\epsilon^{-2}\\log(1/\\delta)\\}$ looks less satisfying.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well-written in the sense that it describes the motivation and the ideas/techniques well. The overall quality looks good, though I do not have time to verify all the proofs. The main technical innovation is the upper bound in Lemma 3.1. The proof is kind of straightforward, though not trivial.\n\nComment on the problem background: It says that the feature map $\\phi$ is from $\\mathbb{R}^d$ to a general Hilbert space $H$, on the other hand the distance is defined to be the $\\ell_2$ norm. Does this mean that the Hilbert space $H$ must be embeddable into $\\ell_2$? It seems that the distance does not need to be $\\ell_2$ norm, it just needs to be induced by the inner product on $H$. \n\nMinor points:\n- Page 2, Line -5: it would be good to elaborate on \"nearly matches\"\n- Page 3, Line 8: \u201cCauchy\u2019s evaluation formula\u201d -> \u201cCauchy\u2019s integral formula for multiple-variable functions\u201d\nLine 3 below Theorem 3.1: i.i.d. sampled -> i.i.d. samples\n- Fact 3.1, last bullet point: no need to state $\\operatorname{dist}_{\\phi}(x,y)^2$ as this can be inferred without difficulty from the first bullet point.\n- Fact 3.1: need citation or a short proof. \n- Proof of Theorem 3.1, the series of inequalities in the middle of the page: the integral $\\int_t^\\infty 4^{-\\sqrt{\\alpha/8}} d\\alpha$ can be calculated directly that $\\int_t^\\infty e^{-c\\sqrt{\\alpha}}d\\alpha = \\frac{2}{c^2}e^{-c\\sqrt{t}}(1 + c\\sqrt{t})$. What is written there is just a change of variable but then you still need an upper bound. \n- Proof of Theorem 3.1: \u201cBy our choice of $t$ and $\\delta$, \u2026 $\\leq \\delta l$, denote $\\sigma\u2019^2$ \u2026, so \u2026\u201d -> \u201cBy our choice of $t$ and $delta$, \u2026 $\\leq \\delta l$. Let $\\sigma\u2019^2$ \u2026, then \u2026\u201d\n- Proof of Theorem 3.1, last line: \u201ccombine it together\u201d -> \u201ccombine them together\u201d. It would be good to be explicit about what intermediate results are combined here.\n- Theorem 4.1: please rephrase the theorem statement. \u201cFor every \u2026, every \u2026, denoting\u2026, there exists\u2026\u201d do not read very well. It might be better to say \u201cSuppose that \u2026\u201d\n- Page 7, line 1 after the proof of Theorem 4.1: right hand -> right-hand\n- Page 7, line -7:\u201d$\\geq\\Omega(1)$\u201d -> \u201c$ = \\Omega(1)$\u201d\n- Page 7, Line -2: insert a comma after $O(1)$\n- Page 8: please use different line styles - colours cannot be distinguished when printed black and white.\n- Proof of Lemma 3.1: I feel there may be some small mistakes or gaps in the proof. For instance, $g_k(x)/i!$ seems to be $g_i(x)/i!$? What happens for the terms of $i < 2k$? Why do they vanish? In particular I don't see why Lemma A.2 means that they'll vanish because there is no limiting process here? Perhaps it should be an inequality here that $g_k(x)$ is dominated by the remainder term. \n- Section E.1: Do NOT use $\\ell_2^2$ as this typically means a two-dimensional $\\ell_2$ space.\n",
            "summary_of_the_review": "This is a solid and neat paper. I do enjoy reading it and would recommend accepting it. But I am not sure if ICLR is the correct place. Theoretical conferences may be more suitable, such as AISTATS, ICALP, ESA, RANDOM, ...",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3810/Reviewer_pJCZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3810/Reviewer_pJCZ"
        ]
    },
    {
        "id": "pTdHxXDK-y-",
        "original": null,
        "number": 2,
        "cdate": 1666555154763,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555154763,
        "tmdate": 1666555154763,
        "tddate": null,
        "forum": "qs2YCziX2o-",
        "replyto": "qs2YCziX2o-",
        "invitation": "ICLR.cc/2023/Conference/Paper3810/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies when Random Fourier Features (RFF) is able to create a Johnson-Lindenstrauss-like result for preserving the distances between the feature mappings of vectors given a shift-invariant kernel. Prior work emphasized different metrics, like getting an additive error guarantee on the pairwise distances, instead of more JL-like relative error guarantees. This paper really focuses on relative error guarantees.\n\nThree core results are shown:\n1. Suppose a kernel is both shift-invariant and analytic near zero (i.e. kernel is very smooth when it's two input are equal). Then, RFF give a relative error guarantee with a standard $O(\\frac{1}{\\varepsilon^2})$ sample complexity for fixed failure probability. It has a polylog dependence on failure probability where the additive error guarantees have just a log dependence, I believe.\n1. Suppose a kernel is shift-invariant but not analytic near zero, like the Laplacian kernel $K(\\vec x, \\vec y) = e^{-\\|\\|\\vec x - \\vec y\\|\\|_1}$. Then, RFF for all such kernels cannot give relative error guarantees in general. If we assume the input vectors cannot be arbitrarily close to each other, then relative error guarantees become possible. Roughly speaking, two vectors being arbitrarily close to each other corresponds to approaching the non-analytic part of the shift-invariant kernel, which makes relative error estimation hard.\n1. For the Laplacian kernel, a new algorithm is proposed that achieves relative error pairwise distance embedding, but not via RFF. This algorithm relies on embedding an $\\ell_1$ space into a very very high dimensional $\\ell_2$ space, runs RFF in some way on that $\\ell_2$ space, and uses pseudo-random generators to make this computationally efficient.",
            "strength_and_weaknesses": "The paper has some really cool results in it. The first 5 pages were a really nice outline of new ideas in thinking about when and why RFF can embed pairwise distances within relative error. The first two major results, at I outlined them in the last box, are really cool and clean. Despite the issues the paper has, which I will discuss momentarily, I think those two results are strong enough to accept the paper.\n\nThere's a nice logical flow the the proofs in the paper, so my review will follow that flow.\n\n## 1. RFF gets relative error for analytic shift-invariant kernels\n\nThe proof is a nice and intuitive statement, complete with a good sample complexity. The high level proof proceeds in three parts: use complex analysis tools to say that being analytic implies a good moment bound on each RFF feature; use being analytic to prove that the kernel has restricted strong convexity about 0; and combine these results in a fairly standard concentration argument. The burden of the proof lies in the first claim -- the moment bound -- which is proven in the appendix and I did not review in detail. Despite this, the message is pretty clean -- the proof clearly and strongly uses being analytic to create geometry that makes RFF give good accuracy near zero.\n\nI suspect there is a very nice corollary to this relative error result, where an epsilon-net argument can give a whole subspace embedding via RFF; proving something akin to the results in [Avron et al, 2017] but with standard RFF sampling and with the regularization parameter set to zero. I'd like to know if the authors ever considered such a guarantee?\n\nI don't know why the authors chose to prove the concentration argument underlying this result on page 6 of the paper. I checked the details, and I have a minor technical concern I'll ask later, but overall it doesn't feel very enlightening to read. Consider making a short paragraph describing the proof, pushing the details to the appendix, and recovering a bunch of space that could be used to describe the third result in the body of the paper (I'll elaborate what I mean by this later).\n\nEither way, first result it cool and clear. All in favor.\n\n## 2. RFF is incapable of relative error embedding for non-analytic kernels\n\nThe key proof under this claim is really simple, and just follows from the asymptotic Gaussianity of individual the RFF features (formalized via the Berry-Eissen theorem). The authors prove that all shift invariant kernels have failure probability that depends on the ratio of the standard deviation of an RFF feature to the mean value of an RFF feature, formally denoted by $s_k$ in Theorem 4.1 on page 7. When we allow two input vectors to get arbitrarily close to each other, this ratio is well controlled for analytic kernels, but may be arbitrarily bad for non-analytic kernels.\n\nSomewhat frustratingly, in Remark 4.1 at the bottom of page 7, the authors intuitively argue why this ratio is bad for Laplacian kernels and good for analytic kernels, but do not include a formal proof. Their description makes it sound pretty easy to prove, but it really should be formalized in the appendix. A concern I have is about the hardness of _other_ non-analytic kernels. Without that proof in the appendix, it's not immediately clear to me how many other non-analytic kernels will suffer the same hardness condition. Can we state an condition on (e.g.) the Laurent coefficients of a kernel that implies RFF is incapable of giving relative error?\n\nEither way, it's a pretty intuitive big picture, but is lacking detail in the formalization.\n\n## 3. Pseudorandom generators and fancier ways to make JL-style guarantees for the Laplacian kernel\n\nThis result is not really described within the body of the paper. It's discussed for 5 paragraphs in the introduction (\"Going beyond RFF\" on page 3). But that's it. The rest is fully described in 4 pages of the appendix. I don't have spare time to review that unfortunately, so I actually have no idea how it really works or how correct it seems. 5 paragraphs of introduction ain't a lot. I'd say the proof on page 6 of the paper should be appendicized. Maybe even cut out the experiments entirely (though I realize other ICLR reviewers may not agree with this). Then the authors could fit the third contribution within the body of the paper.\n\nI can't judge how correct it is, but it would be cool if it was correct, I guess?\n\n## Smaller Notes and Judgements\n\nSection 3.2 on page 7 is a short blurb about improved sample complexity for kernel clustering. The result is neat, I guess, but is not well contextualized within the world of prior work. They also call their metric an $\\ell_p$-objective, where the norm really is $\\|\\|\\cdot\\|\\|_2^p$, which would not usually be called an $\\ell_p$ norm. Pick a new name for it, maybe call it an $\\ell_2^p$-objective?\n\nSimilarly, I know that prior work on RFF like algorithms have cared about certain kinds of relative error guarantees, like in [Avron et al, 2017]. While these are cited in the paper, the results here are not well contrasted against the prior work. This is clear in Section 1.2, which is basically a block of citations without any discussion of similarity. Why does their notions of relative error subspace embedding not extend to the authors' setting? (Formally, make a subspace embedding via RFF or RFF-LS for a 2x2 kernel matrix, and see if that subspace embedding guarantee implies a relative error pairwise guarantee like what the authors are aiming for; I can clarify if this description isn't clear).\n\nIn the experiments, the authors use SVD as an algorithm to attempt to preserve pairwise distances. Why is this a benchmark? Do people really use low-rank approximation algorithms to preserve pairwise distances? This feels intuitively doomed to fail. Also, if you run an algorithm for 20 iterations, could you include measures of confidence intervals in the plot? I'm personally a fan of plotting the medians with 25th and 75th quantiles, but anything that gives a sense of typical variation would be nice.\n\nTheorem 1.4 isn't clearly phrased. The theorem clearly creates a mapping $\\pi$ which has dimension $D$ which does not depend on the precision $\\frac{\\Delta}{\\rho}$, but the first sentence after the theorem clearly sets the dimension depends on $\\log \\frac\\Delta\\rho$.\n\n## Summary\n\nThe paper has cool results, but is somewhat lacking in some parts of the presentation. The first results is clear and compelling. The second is compelling but a bit less clear, and the third result isn't clear at all since it's not in the body of the paper.\n\nNevertheless, the clear and compelling parts are absolutely strong enough to merit publication.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is overall clear and nice to read.\n\nThe reproducibility, for a theoretical paper, has some proofs that I'd like to see elaborated, but is overall in good condition.\n\nI didn't review the contribution of the paper that's completely buried in the appendix.\n\nThe results are novel, though I'd like a better comparison against the prior work.\n\n## Typos and small notes\n\n1. Consider using a different typeface for vectors versus scalars. Not vital by any means, but I think it'd help legibility a bit.\n1. [Proof of theorem 3.1, page 6, the line that starts \"Assume $\\delta \\leq$\"] I think the actual constants are mixed up here a bit. I think it's $t = 32 \\log^2 (2D^2/\\delta)$ and $k=\\log(2D^2/\\delta)$\n1. [Proof of theorem 3.1, page 6, the line that starts \"For simplicity denote\"] Consider putting brackets in the subscript of the indicator function. I think that $1_{[|X_i| \\geq t l]} tl$ reads better than $1_{|X_i| \\geq t l} tl$. Also, consider using `\\ell` instead of `l` in latex: $\\ell$ instead of $l$.\n1. [Proof of theorem 3.1, page 6, the line that starts \"By Lemma 3.1 for every\"] I got the rightmost probability being $(\\frac 4l)^{-\\sqrt{alpha}/8}$ instead of $4^{-\\sqrt\\alpha / 8}$. I could have messed up my algebra fwiw.\n1. [Proof of theorem 3.1, page 6, the math immediately below \"By Lemma 3.1 for every\"] You should write out how this inequality on the right hand side appears. I assume it's some linear combination of indicator functions, but I didn't figure out how to recover this bound.\n1. [Proof of theorem 3.1, page 6, the math immediately above \"By out choice of $t$\"] What is the rate of this integral? What happens to it? It seems to shrink in $t$, but how does that tradeoff with the $O(\\frac{t\\delta}{D^2})$ term next to it?\n1. [Remark 4.1, bottom of page 7] At the start of the remark, $D = \\Theta(...)$ has a mis-typed parenthesis",
            "summary_of_the_review": "## Summary\n\nThe paper has cool results, but is somewhat lacking in some parts of the presentation. The first results is clear and compelling. The second is compelling but a bit less clear, and the third result isn't clear at all since it's not in the body of the paper.\n\nNevertheless, the clear and compelling parts are absolutely strong enough to merit publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3810/Reviewer_vUht"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3810/Reviewer_vUht"
        ]
    },
    {
        "id": "Hok4_Y-kMPM",
        "original": null,
        "number": 3,
        "cdate": 1666726973916,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666726973916,
        "tmdate": 1669378048643,
        "tddate": null,
        "forum": "qs2YCziX2o-",
        "replyto": "qs2YCziX2o-",
        "invitation": "ICLR.cc/2023/Conference/Paper3810/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the relative error of kernel distances when applying the Random Fourier Features (RFF) to the shift-invariant kernels. While a prior work studied only the Gaussian kernel, this work generalizes the prior results to more general shift-invariant kernels. In particular, the authors show that poly(eps*log(n)) features are needed to guarantee the eps-relative errors for all the n data points. They also study a lower bound on the number of features for the Laplace kernel and show that it can be unbounded when an input has a huge magnitude-resolution ratio. In addition, they show that the RFF takes a better bound on the feature dimension on the k-means clustering compared to the Nystrom method. Finally, they provide an alternative embedding for the Laplacian kernel that requires a time in poly-log of the magnitude-resolution ratio. Experimental results evaluating the relative kernel distance under synthetic datasets are provided.\n",
            "strength_and_weaknesses": "Strength:\n\n- This work extends a prior result on the Gaussian kernel to general shift-invariant kernel distances. They also study for the Laplacian kernel the bound of the number of features can be unbounded.\n- The authors characterize that the kernel distance guarantee can be used for bounding the k-means clustering.\n- They propose an alternative embedding for the Laplacian kernel beyond the RFF. As a result, it requires the number of features poly-log in the magnitude-resolution ratio.\n\n\nWeakness:\n\n- This paper basically improves the work of (Chen and Phillps, 2017), by generalizing the results to general shift-invariant kernels. However, importantly, the proposed result is worse than that of (Chen and Phillps, 2017), which only focuses on the Gaussian kernel. This makes the contributions weak. \n- The results of the upper bound and lower bound have a different notion, i.e., the magnitude-resolution ratio only appears in the lower bound. This makes it hard to compare those bounds. Can the upper bound involve in this ratio?\n- Experiments only show the relative errors on the synthetic datasets. Since the RFF has been used in many practical applications, I think more experiments should be addressed. For example, showing the k-means clustering results and comparing the Nystrom method under real-world datasets would be nice.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written, but needs some improvements, e.g., \n- Theorem 3.1 and 1.2 (which informally states Theorem 3.1) are basically the same. There is no reason to state the same theorem twice, which can be a waste of space.\n- It would be better to use the parenthesis for citing references.\n- It seems that the inequality in (2) is wrong.\n- It would be good to have an independent section for the \"Going beyond RFF\" part.\n\nThe notion of the kernel distance is not new and many technical tools are borrowed from previous works (Chen and Phillps, 2017, Makarychev et al., 2019), hence I feel that the novelty is limited.\n\n",
            "summary_of_the_review": "This paper extends the prior work on the Gaussian kernel to general shift-invariant ones. But, their result is worse than the prior one and technical tools are also very similar. The writing quality needs to be improved and experimental results do not well support the benefit of the RFF on the kernel distance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3810/Reviewer_KzPf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3810/Reviewer_KzPf"
        ]
    }
]