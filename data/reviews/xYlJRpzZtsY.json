[
    {
        "id": "v2wXOZ8x_L",
        "original": null,
        "number": 1,
        "cdate": 1666652917293,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652917293,
        "tmdate": 1668901032125,
        "tddate": null,
        "forum": "xYlJRpzZtsY",
        "replyto": "xYlJRpzZtsY",
        "invitation": "ICLR.cc/2023/Conference/Paper3608/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduce a new metric (ROSCOE) for the generic step-by-step reasoning task. In particular, ROSCOE includes four fine-grained metrics under four perspectives: *semantic alignment*, *semantic similarity*, *logical inference* and *language coherence*, where each fine-grained metric further contains a bunch of specific metrics. The authors show the effects of the prosed the metric on both synthetic and human-labeled datasets.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is generally well-written and easy to follow.\n- The idea of evaluate the quality of step-by-step reasoning is novel and could benefit a lot of end applications.\n\nWeaknesses:\n\n- Could you justify that the new taxonomy of generic reasoning errors can cover all the possible reasoning errors? For Reasoning Errors, at least, negation should be considered. E.g. \"Anakin didn't catch 10 fish.\"\n- The authors argue that \"The existing works primarily focus on end-task performance.\" However, it is unclear if the good performance of reasoning rationales (by the proposed ROSCOE) means a good performance on end-task performance.\n- Many presentation logics should be improved. For example section 3, \"To evaluate ROSCOE,\", however, it is unclear what exactly \u201cROSCOE\u201d is. And \"ROSCOE\" is itself a metric, how could we evaluate a metric?\n- The authors propose four fine-grained metrics under four perspectives: *semantic alignment*, *semantic similarity*, *logical inference* and *language coherence*, where each  fine-grained metric further contains a bunch of specific metrics. However, it is unclear how these specific metrics are merged into one scalar value.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written and easy to follow.\n- The idea of evaluate the quality of step-by-step reasoning is novel and could benefit a lot of end applications.\n- The code is not provided but the authors promise to release in their reproducibility statement. ",
            "summary_of_the_review": "This paper present a new ROSCOE,  a suite of automatic scores for evaluate the step-by-step reasoning performance. However, the proposed new taxonomy has not be justified, and whether the good ROSCOE scores could generate a good performance on end-task performance has not been validated.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "details_of_ethics_concerns": "The evaluation of this work requires  create synthetic data and collect human evaluations on commonly used reasoning datasets, however, the  human evaluations is not clear. It might require a further ethic review.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_9E3Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_9E3Y"
        ]
    },
    {
        "id": "9MNVT9zBzB",
        "original": null,
        "number": 2,
        "cdate": 1666676492968,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676492968,
        "tmdate": 1669686579406,
        "tddate": null,
        "forum": "xYlJRpzZtsY",
        "replyto": "xYlJRpzZtsY",
        "invitation": "ICLR.cc/2023/Conference/Paper3608/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors collect data about rationale used in reasoning tasks. It includes two major datasets: \"diagnostics\" which includes human generated gold-reference rationale, and \"judgements\" which include machine generated rationale with human judged errors (see table 7 in the appendix for dataset statistics). Their suite of metrics is based off this data, and designed to measure the effectiveness of newly generated rationale. The metrics mostly are intrinsic, and don't account for downstream task performance that I can tell.\n\nRegarding the rationale and metrics, I could be missing a crucial detail... It is mentioned in sec3 that the final step in the chain is the answer, but for the most part, I have interpreted this work as if the chain does not include the answer. Since some of the metrics would not make sense if the last step is the final answer.",
            "strength_and_weaknesses": "# Strengths\n\n* Substantial datasets including human-generated rationale. Also includes machine-generated rationale annotated with errors by humans.\n\n* The dataset will enable new work to leverage the data directly to improve performance or create more rationale. But this is not all. We can use this data to measure the qualities of what is considered human-like rationale, and that is what this paper does by presenting a suite of metrics along with their analysis.\n\n# Weaknesses\n\n* Not nearly enough analysis and exploration of the new data. For example, do these rationale lead to improved (or worse) performance? Or categorizing different types of rationale and their properties.\n\n* It should be made more clear what are the assumptions about rationale, e.g. do we expect there to be only one valid reasoning chain?\n\n* There are clear downsides to the introduced metrics. The major one being I am not convinced they would recognize an equally good but different looking rationale. I don't think this is something the baseline metrics would be capable of doing, but also not convinced ROSCOE could do either. Perhaps challenging qualitative results would help. Although evaluating different types of rationale on a downstream task would be most informative.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly clear and would be valuable to the community because it provides a valuable resources (the rationale related data) and example on how to use these for research (the metrics).\n\nThat being said, the data could be more closely analyzed and I am not convinced of the usefulness of the metrics.\n\nI have many issues with the metrics, and describe a few below:\n\nFaithfulness-Step: This seems very dependent on how you define steps. A chain-of-thought (CoT) can be only a few steps using multiple tokens each, and lead to deceptively high (or low) faithfulness scores. Is this the reason for the -token version?\n\nInformativeness-Step: This seems counter intuitive to how CoT often works... CoT works especially well with natural language explanations, which probably will include text not in the source.\n\nRepetition-token: Some prompt formats (e.g. least-to-most prompting) rely on repeating information from previous steps in order to gradually construct the answer, so will do poorly here...\n\nMissing step: if there is a \"budget\" on how much can be done in the CoT, then it is potentially desirable to skip some steps and only elaborate on the most crucial ones. Also, one advantage of CoT is there are many possible reasoning chains (see self-consistency work for example).\n\nAlso in sec 4.4, the metrics based on perplexity seem somewhat strange --- isn't perplexity used to measure the model not the generation? I wonder if a model trained to accept/ reject chains based on reference chains would do better. And even though CoT can use language, I don't see why it would need to be grammatically correct.\n\nA measure that would be very informative is if the reasoning steps somehow arrive at the right answer, but using a faulty path. See Figure 1 here for a detailed example: https://arxiv.org/abs/2010.03548",
            "summary_of_the_review": "I see the contribution of the new data as a strong one. Although I don't find the suite of metrics convincing, the value of the dataset still warrants acceptance. Plus, I consider the suite of metrics as an example of what can be accomplished once this data is available. I would probably give a stronger review if the data is further explored in the main text, but I am assuming the data has intrinsic value because it is human generated and am still unsure if it can be used for good downstream performance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "details_of_ethics_concerns": "There is human-annotated data to be released. I don't expect there to be any violations due to the nature of the tasks (like logical reasoning). That being said, maybe the rationale for question answering or commonsense reasoning could have some concerning reasoning chains.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_MQPy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_MQPy"
        ]
    },
    {
        "id": "BXSwu06OzXT",
        "original": null,
        "number": 3,
        "cdate": 1666700263472,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666700263472,
        "tmdate": 1666700263472,
        "tddate": null,
        "forum": "xYlJRpzZtsY",
        "replyto": "xYlJRpzZtsY",
        "invitation": "ICLR.cc/2023/Conference/Paper3608/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a large number of metrics to evaluate step-by-step reasoning traces. They learn an embedding to evaluate similarity between steps. Authors show results on a very large and varied set of datasets.",
            "strength_and_weaknesses": "Strengths:\n1. The authors evaluate their metrics on a large set of varied datasets with all kinds of reasoning: logical, natural and synthetic language and math.\n2. This is an incredibly challenging problem and any steps towards evaluating step-by-step traces are great.\n\n\nWeaknesses:\n1. The authors propose a lot of metrics but have not sufficiently showed the sensitivity of each metric.\n2. The method depends on having a very good similarity metrics.\n3. It is not clear that the alignment metrics make sense (for example between source and hypothesis statements).",
            "clarity,_quality,_novelty_and_reproducibility": "1. Pg1: What is the \u201csource\u201d in this context? \n> \u201cand grounded with the source\u201d \n\n2. The four \u201cperspectives\u201d could be introduced more clearly, possibly with examples and counter examples or formally with some math.\n\n3. The authors do reference Dalvi et al. at other points in the paper, but their work should also be mentioned in the Automatic Metrics section since they have extensive step-by-step evaluation. Dalvi et al. do assume access to a ground truth trace, but their work should still be mentioned.\n\n4. Section 3, Taxonomy: The authors should mentioned which methods, models and datasets were used to \u201cidentify nine error types\u201d. It is important that this is representative.\n\n5. Section 4.1: Alignment was also computed by Dalvi et al. using BLEURT and a threshold of 0.28.\n\n6. Please make it clear at the start of 4.1 that you will use a fine-tuned embedding model. The embedding model must have a good understanding of logical reasoning for this to work in the first place? For example, how does the model deal with cases of the following flavour \u201cThe fox is red\u201d <\u2014> \u201cThe fox is not red\u201d and \u201cThe dog chases the cat\u201d <\u2014> \u201cThe cat chases the dog\u201d. Language models are not always good at preserving word order (tho appreciate this is a somewhat open problem).\n\n8. I assume \u201ccos\u201d is the \u201ccosine distance\u201d?\n\n9. It\u2019s not clear why you could compute alignment between the source, s and the hypothesis, h? If the hypotheses has rules and facts and the source has examples of combining these, I can image that there would not be any obvious matches? Even less so for more natural language problems.\n\n10. \u201cInformativeness-Step (Info-Step)\u201d appears to be a combination of other metrics and make it harder to tell is the model is (a) not using certain information or (b) hallucinating information. Combining these metrics would surely make it more difficult to tell which is occurring?\n\n11. Why is the repetition evaluation done at the token level? If you are using a dataset like ProofWriter (or even some math datasets) it is very likely that the same token will appear in multiple steps. It would be more helpful to know of a whole step has been repeated, which is the much more common failure mode. As an example for the \u201cvalid\u201d repeating of tokens. \u201c1. All red things are soft, the Dax is red. Therefore the Dax is soft; 2. All soft things are cubes. The Dax is soft. Therefore, the Dax is a cube. I see there is also a \u201cRepetition-Step metric\u201d.\n\n12. More general question, how do you split the outputs of the model into steps and in inputs into statements? Splitting by sentences would not accurate (example above).\n\n13. Do I understand correctly that hallucination can only be evaluated in cases where you have a reference trace?\n\n14. The redundancy measure makes sense. Alternatively, you could also have considered precision.\n\n15. Self consistency metrics appear useful.\n\n16. It is fantastic that the authors use such a wide variety of datasets!\n\n17. For clarity, in \u201cROSCOE Training\u201d what are the pairs of sentences which the authors train the model to be similar/ different to between source and hypothesis? I\u2019m unsure how sentence in the hypothesis can be considered similar to those in the source? Especially since often sentences in the hypothesis should be some combination of those in the source?\n\n18. For clarity, Table 3 is using the golden traces from the dataset and you are evaluating your metrics on these? Table 3 on its own must be interpreted with caution since it only shows that the metrics can identify \u201cgood\u201d reasoning trace, but not that they can identify errors.\n\n19. \u201cHow sensitive are ROSCOE metrics against level of errors?\u201d rather than showing aggregated scores, it would have been more helpful to introduce the specific type of error you want to detect at the same three levels and show \u201cerror level\u201d vs \u201cmetric\u201d. Figure 2 goes some way to addressing this.\n\n20. Figure 2: Why do the authors show BERT-score results on Hallucination when the other two figures do not show any Hallucination?\n\n21. In Figure 2, I would be interested to see the repetition scores on ProofWriter.\n\n22. Figure 2: It would be great to see a more systematic analysis applying perturbations (where possible) that related directly to each metric and showing \u201cnumber/level of perturbation\u201d vs. Metric.\n\n23. The datasets which are used for evaluating the metric are the same datasets used to train the embedding model. How does the metric fair on out of distribution datasets?\n\n24. Please can you give some examples of \u201calignments\u201d between source and hypothesis statements for ProofWriter, EntailmentBank and GSM8k. \n",
            "summary_of_the_review": "This is a very promising first step towards evaluating reasoning traces. My main concerns are as follows:\n(1) There is insufficient systematic analysis of the proposed metrics. It would be better to propose all metrics, apply relevant perturbations and show which metrics most accurately capture the types of error they are designed for. Following this, the authors could rank the metrics in-terms of sensitivity or  suggest the top N most sensitive metrics going forward.\n(2) It is not clear how the alignment process works, especially when aligning hypothesis statements with those in the source. This will also depend significantly on the model used to embed the sentences. The authors have also only shown evaluation on the same datasets that the metric was trained on.\n\nI would be inclined to accept this paper, since it takes a very important (early) step in the right direction. It is also great that the weights for their embedding model will be available as well as their code. However, my recommendation to accept would be contingent on the authors being clear about the limitations of their approach and adding a clearly labelled section to their paper discussing these. They must also run a more systematic analysis of the perturbations and show how these affect the metrics.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_mtZF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_mtZF"
        ]
    },
    {
        "id": "6OywP5-561S",
        "original": null,
        "number": 4,
        "cdate": 1666797499269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666797499269,
        "tmdate": 1666802394246,
        "tddate": null,
        "forum": "xYlJRpzZtsY",
        "replyto": "xYlJRpzZtsY",
        "invitation": "ICLR.cc/2023/Conference/Paper3608/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors did a very good job analyzing chain-of-though inferences for several crucial reasoning datasets and constructing their own Human Judged Datasets. ",
            "strength_and_weaknesses": "The authors introduced 18 custom metrics intended to measure the quality of reasoning. The main strength is the breadth of the analysis. I am less sure about the uses of these new metrics. A few outstanding questions: \n\n1. How are human solutions performing against correct and incorrect machine-generated answers?\n2. Can you extend information about the Human Judged Datasets, including the scale and cost of work and the intent to open-source it?\n3. Can you show more examples of errors in the appendix or provide access to a browser of samples (e.g., similar to https://minerva-demo.github.io/)? \n4. It would be interesting to see a scale of errors, that is, samples for the same problem that are correct according to the metrics, somewhat correct, somewhat incorrect, and completely incorrect.\n5. The elephant in the room is the usefulness of these new evaluation metrics for downstream tasks. Does it help with re-reranking? Can we get better scores than via majority voting?\n6. Nit: Can you explain how the frequent 1.0 score in the tables should be interpreted?",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of fine-grained reasoning metrics seems to me novel and exciting. Measurements taken in the paper seem easy to reproduce except for the human evaluation, which would require open-sourcing of Human Judged Datasets.",
            "summary_of_the_review": "The authors did a fantastic job analyzing chain-of-though inferences for several crucial reasoning datasets and constructing their own Human Judged Datasets. The impact of the metrics could be made more evident.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_8arD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_8arD"
        ]
    },
    {
        "id": "IPFguoKr5X",
        "original": null,
        "number": 5,
        "cdate": 1667356298091,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667356298091,
        "tmdate": 1667356298091,
        "tddate": null,
        "forum": "xYlJRpzZtsY",
        "replyto": "xYlJRpzZtsY",
        "invitation": "ICLR.cc/2023/Conference/Paper3608/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a new reference-free metric named ROSCOE for the step-by-step rationales that some LLMs use for reasoning tasks. The ROSCOE is a suite of metrics that cover four dimensions of step-by-step rationales (or logic chains): 1) semantic alignment, 2) semantic similarity, 3) logical inference, and 4) language coherence. The authors use five natural datasets and six diagnostic datasets for justifying the effectiveness of the proposed ROSCOE metrics. The experiments and analysis show that ROSCOE outperforms other baseline metrics such as CTC, BARTScore, and BLEURT.\n\n",
            "strength_and_weaknesses": "## Strength\n\n- a comprehensive and novel metric for an important problem --- to understand the quality of the step-by-step rationale (the so-called chain of thoughts). \n\n- very extensive experiments that compare the ROSCOE and other metrics; the reasonable choices of data and evaluation methods, and informative analysis of the behavior of ROSCOE\n\n- a nice taxonomy of reasoning errors for analysis with perturbation in building diagnostic datasets.\n\n## Weakness\n\n- lack of justification for the design of many details in ROSCOE.  \n- Sec 4 (especially 4.1) is hard to read. It's not a great idea to simply list all these sub-metrics without clear motivation and illustrative examples.  \n- the implications of the proposed metric are relatively weak -- how to improve the rationales? what are the disadvantages of existing step-by-step rationales? these questions are not touched on. \n\n## Suggestions: \n- think about how future people should use this ROSCOE metric easily and for more general cases.\n- can we use the ROSCOE analysis to guide the future design of the step-by-step rationale \n- what are the limitations of the proposed metrics so that people can keep that in mind when using the metrics\n- what will be the methods to avoid the above limitation and fix them?",
            "clarity,_quality,_novelty_and_reproducibility": "- please see the comments in the above section",
            "summary_of_the_review": "Generally, a solid paper on a novel metric, and it has a comprehensive analysis. The studied problem is pretty important and the empirical results are promising. The implication part is relatively weak as shown in the weakness and suggestion sections. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_UUD9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3608/Reviewer_UUD9"
        ]
    }
]