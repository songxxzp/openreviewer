[
    {
        "id": "vsRiHlX5JVS",
        "original": null,
        "number": 1,
        "cdate": 1666511131581,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666511131581,
        "tmdate": 1666511131581,
        "tddate": null,
        "forum": "Nayau9fwXU",
        "replyto": "Nayau9fwXU",
        "invitation": "ICLR.cc/2023/Conference/Paper5649/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a diffusion-based image translation approach. It proposes a new way to disentangle the style and content representations. The content information is represented by the key similarity of a pretrained ViT model, and the  style information is represented by the last layer of CLS token.",
            "strength_and_weaknesses": "Strength:\n1 The empirical results are pleasing. It outperforms all the existing methods.\n\n2 The authors have conducted extensive experiments on Sec4.4 and the Appendix to support their ideas. \n\nWeaknesses:\n\n1. It seems that the contribution of this paper is to propose  L_cont and L_sem to represent the content and style information. However, it is not clear why such representation is superior compared with alternatives (e.g., VGG-based ones[1]). It will be interesting if the authors could have a thorough comparison. \n\n2. there are some typos in the paper, e.g.,  in Fig. 2. The L_con should be L_cont. Please carefully proofread the paper.\n\n[1] Perceptual Losses for Real-Time Style Transfer and Super-Resolution.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The authors has conducted extensive experiments. ",
            "summary_of_the_review": "Overall, the presented idea looks new, and the performance is promising. The authors have also conducted significant experiments to support their claims. Therefore, I vote for acceptance at this stage. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5649/Reviewer_CBU4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5649/Reviewer_CBU4"
        ]
    },
    {
        "id": "NcyLTF7mGT",
        "original": null,
        "number": 2,
        "cdate": 1666619737745,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619737745,
        "tmdate": 1666619737745,
        "tddate": null,
        "forum": "Nayau9fwXU",
        "replyto": "Nayau9fwXU",
        "invitation": "ICLR.cc/2023/Conference/Paper5649/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a modified Denoising Diffusion Probabilistic Model with a proposed content preservation loss. It aims to achieve image-to-image translation with the content structure preserved and style changed. To do so, a content reservation loss and a resampling mechanism are proposed. Additionally, experiments of text and image-guided semantic image translation are established to verify the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n\n* The paper is well-written.\n* The experiments are sufficient. \n* The motivation for preserving the structure and changing the style is reasonable.\n\nWeakness:\n* The goal of keeping content structure unchanged can be verified by segmentation experiments rather than user studies.\nFor example, you can train a segmentation model on the target domain and then test it on the images generated from the source domain. Since the content structures are preserved, the predicted mask would be close to the mask of source images.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is simple, and the methodology is described clearly, which makes it easy to follow. The experimental studies are sufficient. The novelty of this paper should be above the borderline.\n",
            "summary_of_the_review": "This paper introduces a modified Denoising Diffusion Probabilistic Model with a proposed content preservation loss. It aims to achieve image-to-image translation with the content structure preserved and style changed, which is well-motivated. To do so, a content reservation loss and a resampling mechanism are proposed. The experimental studies seem to be sufficient. Thus, it might reach the publication demands.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5649/Reviewer_9VMA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5649/Reviewer_9VMA"
        ]
    },
    {
        "id": "VqCwqh1INV",
        "original": null,
        "number": 3,
        "cdate": 1666673658223,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673658223,
        "tmdate": 1666673658223,
        "tddate": null,
        "forum": "Nayau9fwXU",
        "replyto": "Nayau9fwXU",
        "invitation": "ICLR.cc/2023/Conference/Paper5649/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper works on text-guided and image-guided image translations. The authors propose a guided diffusion method that tries to 1) maintain the structure of the source image, and 2) generate semantic content that matches the guidance. Specifically, contrastive loss, CLIP loss, semantic style loss and divergence loss is used during the reverse diffusion process. Experimental results demonstrate the effectiveness of the proposed method over several guided diffusion approaches and some image-to-image translation models.",
            "strength_and_weaknesses": "*Strength*\n1. The paper is well-written. The authors clearly introduce and detail the proposed approach.\n2. The proposed method is shown to be more effective compared to existing guided diffusion methods on the text-guided image translation task.\n\n*Weakness*\n1. In common image-to-image translation, \"style\" referes to the variation which is not shared across different visual domains, which is actually defined in a data driven manner. For example, in AFHQ, style indicates the species, while in winter <> summer, style represents the color variation. Since the two losses (CLIP and semantic style) the authors use only encodes high-level semantic information, how does the proposed method handle the case that style encodes the low-level (e.g., color) variation? For example, the orange sky color is not correctly transfer to the output image in Figure 7 (b). Or more general speaking, how do the proposed method learns to differentiate what is common/unique variation that shared/not shared across different visual domains?\n2. Although better visual quality is shown in the image guided translation compared to other methods, diffusion-based model is known for (relatively) long inference time. How long does it take for the proposed method to synthesize an image from an image guidance? How does that compare with other single-shot image translation method?",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity*: This paper is well-written.\n\n*Quality and novelty*: The authors leverage guided diffusion for single shot image translation, and the visual results are more realistic compared to other methods.\n\n*Reproducibility*: I believe the authors provide sufficient detail to reproduce the results in the paper.",
            "summary_of_the_review": "Overall, the paper is well-written and the results are somewhat promising. I hope the authors can address the concerns raised in the Weakness section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5649/Reviewer_swjc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5649/Reviewer_swjc"
        ]
    },
    {
        "id": "Bkrf_dLhFP-",
        "original": null,
        "number": 4,
        "cdate": 1666808683672,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666808683672,
        "tmdate": 1666808683672,
        "tddate": null,
        "forum": "Nayau9fwXU",
        "replyto": "Nayau9fwXU",
        "invitation": "ICLR.cc/2023/Conference/Paper5649/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the problem of (text/image) guided (single) image editing using a pre-trained diffusion model. Specifically, it proposed a novel diffusion-based image translation method by disentangling style and content representations. Borrowed from the disentangling technique introduced recently (Tumanyan et al., 2022), a pre-trained Vision Transformer (ViT) has been used to aid the style and content disentanglement during the diffusion process. To accelerate the reverse diffusion process, this paper also applied two techniques, namely, semantic divergence loss and resampling strategy (Chung et al., 2022b). Experimental evaluations have been conducted on Animals and Landscapes datasets. Both quantitative and qualitative results demonstrate the strength of the proposed method over existing baselines such as Splicing ViT (Tumanyan  et al., 2022), VQGAN-CLIP (Crowson et al., 2022), and Flexit (Couairon et al., 2022).\n",
            "strength_and_weaknesses": "Strengths:\n* [S1] This paper presents a unified framework to improve guided image translation with a pre-trained diffusion model. Results on both text-guided and image-guided applications look more realistic (in terms of shape and texture) compared to the existing baselines, including very recent ones published this year.\n* [S2] The combination of different techniques including slicing Vision Transformer, Manifold Constrained Gradient (MCG), and Come-closer-diffuse-faster (CCDF) make a lot of sense. It is great to see a combination of recent techniques really make a big difference on the guided image translation quality.\n\nWeaknesses:\n* [W1] Although results are very impressive on the animals and landscapes dataset, the technical novelty of the paper is very constrained. In the Figure 2, almost all the components have been explored in the previous work. To be fair, this paper is the first one that shows great results by combining all of them in a unified framework.\n\n* [W2] As this paper leverages many pre-trained models such as a pre-trained image diffusion model, a ViT model, and a CLIP model, the reviewer feels the comparison to existing baselines in the guided image translation setting is a bit unfair. For example, what happens if we leverage pre-trained models and apply loss functions to aid the SANet (Park and Lee, 2019). It would be good to show some fair qualitative comparisons in the image translation setting (in addition to the Section 4.4). \n\n* [W3] It is questionable if the guided translation results look very diverse or not. The reviewer would like to know if this is a sign of memorizing the training set (from the pre-trained models). For example, it would be good to find the closest example in the ImageNet and show them on the side. Alternatively, it would be good to show multiple translation results from either image-guided and text-guided applications.\n\n* [W4] The reviewer noticed that FlexIT actually achieved better LPIPS score in Table 1. It is true that FlexIT is trained directly with LPIPS score, as explained in the paper. The reviewer still feels that the proposed model is structure-aware (due to disentanglement) but not very content-preserving. It would be good to elaborate a bit on this.\n\n* [W5] What are the failure cases of the proposed method? It is important to mention the failure cases in the main text and show more results in the supplementary material.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written with a good amount of details.\nWhile the results look impressive, the novelty is a bit limited as mentioned in the review.\nThe paper contains sufficient details for a domain expert to reproduce. However, it would be great if the code can be open-sourced in the future.\n",
            "summary_of_the_review": "Overall, I think this is a good paper. I am happy to raise my score if the concerns can be addressed in the rebuttal.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5649/Reviewer_VSVg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5649/Reviewer_VSVg"
        ]
    }
]