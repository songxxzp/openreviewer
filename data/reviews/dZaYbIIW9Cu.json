[
    {
        "id": "M3xmyh3Dyym",
        "original": null,
        "number": 1,
        "cdate": 1666638602336,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638602336,
        "tmdate": 1666687945589,
        "tddate": null,
        "forum": "dZaYbIIW9Cu",
        "replyto": "dZaYbIIW9Cu",
        "invitation": "ICLR.cc/2023/Conference/Paper2054/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper mainly considers the transformation from centralized training with decentralized execution (CTDE) in multi-agent reinforcement learning (MARL) to single-agent reinforcement learning (SARL). Specifically, the authors reformulate a multi-agent MDP as a special \"single-agent\" MDP with a sequential decision-making structure among agents. In this way, SARL algorithms can be used during training, and a distilled policy is used during execution. Experiments show a good performance in a few scenarios in SMAC and GRF. ",
            "strength_and_weaknesses": "Pros:\n\n- This paper takes on an interesting problem in MARL: the improvement of the CTDE scheme outside value decomposition and centralized critic.\n- The experiments are completed.\n\nHowever, I have several concerns here:\n- The paper is not well-written and not well-organized with many typos and confusing notation. It seems a hastily written paper.\n- The description of Theorem 3.1 is wrong, single step MDP is not MMDP. The proof of Theorem 3.1 is confusing. The example game is a pure coordination game, which has |A| pure strategy equilibria. However, it is the nature of the pure coordination game, which does not mean that multi-agent actor-critic algorithms create inherent local minimums.\n- The reason why the proposed method works in \u201cMulti-task Matrix Games\u201d as shown in Fig.1 is the agents are factually communicating to avoid miscoordination in such one-shot games You can check examples 3.2 and 3.3 in [1].\n- The order of actions matters.\n- If agent $i$ can know all previous agents\u2019 actions, this case is a weak version of MARL communication or agent modeling. \n- The distillation is missing in the main part of this paper.\n\nMinor:\n\n- Reference in line 3 of page 3: missing parentheses\n- see eq. (1)/appendix/section/theorem/proposition/definition -> see Eq.1/Appendix/Section/Theorem/Proposition/Definition\n- $\\Gamma M$ in the last line of page 5: missing parentheses\n- Proposition 4.1 (suitable \u2026) -> Proposition 4.1 (Suitable \u2026)\n- In Prop. 4.1, \u201cif Assumption 4.1, 4.2, 4.3 in Liu et al. (2019) holds\u201d -> \u201cif Assumption 4.1, 4.2, and 4.3 in Liu et al. (2019) hold\u201d\n- So many typos, grammar mistakes, and irregular notations!!!\n\n\n[1] When Does Evolution Lead to Efficiency in Communication Games. 1994.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: I can roughly understand the main idea of this paper, however, some points are confusing. (See Strength And Weaknesses)\n- Quality:  It seems a hastily written paper.\n- Novelty: The perspective of transformation is novel. But the underlying methodology is not surprising since it seems a weak version of MARL communication or agent modeling.\n- Reproducibility: the authors do not provide the source code.\n",
            "summary_of_the_review": "I think the authors should improve this paper with much effort. The current version is not satisfying in terms of both presentation and the core idea.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2054/Reviewer_toNr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2054/Reviewer_toNr"
        ]
    },
    {
        "id": "l0fjAvQzzy",
        "original": null,
        "number": 2,
        "cdate": 1666704085515,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666704085515,
        "tmdate": 1666704085515,
        "tddate": null,
        "forum": "dZaYbIIW9Cu",
        "replyto": "dZaYbIIW9Cu",
        "invitation": "ICLR.cc/2023/Conference/Paper2054/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a revision of the existing approach to Centralised Training Decentralised Executation (CTDE)  for the Cooperative AI problems. They propose a transformation from the MMDP to a MDP problem, thus allowing them to use Single Agent RL algorithms. This is motivated by observations (with theoretical justification) that current CTDE approaches with gradient based learners introduce a large set of local equilibria which restrict the ability to converge to optimal behaviour.\n\nThe authors then implement a simple of this transform (which amounts to transforming agents from taking simultaneous moves into sequential moves and using a single agent instead of a team). Implementation details are provided such as using Multi-Head Attention, KL Divergence and Distillation (although not all components are ablated). \n\nPerformance is measured on saturated benchmarks (SMAC) and so performance can only been seen to be a marginal improvement.\n",
            "strength_and_weaknesses": "### Strengths\n\nStrong theoretical work to show existing models are more susceptible to falling into local optima than the proposed method. \n\n### Weaknesses\n\n* Unclear why they only evaluate on 6 of the SMAC environments, there are many more. \n* They have not used any SMAC maps with > 10 agents\n* Do they provide other agents\u2019 actions at test time (as suggested by Figure 2)? If so this makes this an entirely unfair comparison to CTDE.\n* I would like to understand the importance of the ordering of agents within the \u201cvirtual\u201d episode.\n* I would like ablations to understand the importance of the MHA and KLD terms.\n* The MAPPO results are much lower than reported in the original paper - can you explain the performance difference\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n* Existing issues are well explained. \n* Proposed method could use more explaining\n* \u2018In the environment setting, we use sparse rewards with both SCORING and CHECKPOINT for our approach and all baselines. For observations, we follow (Li et al., 2021a), using simple 115 as the observation while removing the information irrelevant to the current scenario.\u201d -> what does 115 mean here?\n\n### Quality\n* I found the use of distillation poorly motivated could more information be provided to explain how this process works for each agent (are the distillations conditioned on the specific agent or are they n copies)\n\n### Novelty\n* Transformation is novel for CTDE\n",
            "summary_of_the_review": "Paper exposes a theoretical problem with CTDE which is a good contribution. \n\nThe solution proposes lacks motivation or compelling results to be taken seriously.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2054/Reviewer_SxZS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2054/Reviewer_SxZS"
        ]
    },
    {
        "id": "yFLNV7xtL5L",
        "original": null,
        "number": 3,
        "cdate": 1667531023233,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667531023233,
        "tmdate": 1667533926605,
        "tddate": null,
        "forum": "dZaYbIIW9Cu",
        "replyto": "dZaYbIIW9Cu",
        "invitation": "ICLR.cc/2023/Conference/Paper2054/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies centralized training with decentralized execution (CTDE) in cooperative multi-agent reinforcement learning (MARL). It first analyzes disadvantages of previous algorithms from an optimization perspective. Then, it proposes a framework that reduces a cooperative MARL problem to a single-agent RL problem and a corresponding algorithm called transformed PPO (T-PPO). This paper also presents experiment results on simple matrix games, Starcraft II and Google Research Football and show superior performance of the proposed T-PPO algorithm.",
            "strength_and_weaknesses": "### Strengths\nThe reduction from cooperative MARL problem to SARL problem is considered to be novel. Meanwhile, the experiment results also look promising.\n\n### Weaknesses\nOne of the major weakenesses of this paper lies in its theoretical support. In particular, although the problem of multi-agent games may contain many local minimums, it is not clear how this problem is resolved in the transformed MDP. Since the problem is in general non-convex, many local minimums may still exist in the transformed MDP. It will be better if it can provide an analysis that the proposed framework does resolve the problem of many local minimums even in some toy examples.\n\nMeanwhile, the proofs provided in Appendix seem to miss a lot of details, together with some hand-waving parts. For example, in the proof of Proposition A.1:\n- It will be better to state clear how the formulas from QPLEX are simplified.\n- In the first equation, $\\theta_1, \\dots, \\theta_n$ does not appear in the right-hand side.\n- The loss function $C$ is not clearly defined.\n- It is not clear what \"the best we can expect\" means.\n- The proposition claims to find some $\\phi$ such that $(\\theta_1, \\dots, \\theta_n, \\phi)$ is a local minimum of $C$, but the proof does not make clear how exactly $\\phi$ looks like.\n- It will also be better to add the omiited details of Theorem 3.2 back.\n\nThese hand-waving arguments and missing details make the theoretical claims in this paper less convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "As discussed before, the idea of this paper is considered to be novel. However, the writing clarity and quality need some improvement. In particular, the definitions need to be made precise, the arguments need to be made rigorous and the missing details need to be filled.\n\n### Questions\n- Does the problem of many local minimums still exist for the reduced single-agent RL problem?",
            "summary_of_the_review": "This paper proposes a novel reduction from cooperative MARL problem to SARL problem and the correspondinng experiment results look promising. However, its current theoretical support is not solid enough and its writing also needs some improvement.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2054/Reviewer_fgdt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2054/Reviewer_fgdt"
        ]
    }
]