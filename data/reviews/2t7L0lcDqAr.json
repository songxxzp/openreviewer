[
    {
        "id": "F9rR1sMOcI",
        "original": null,
        "number": 1,
        "cdate": 1666470595082,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666470595082,
        "tmdate": 1666470595082,
        "tddate": null,
        "forum": "2t7L0lcDqAr",
        "replyto": "2t7L0lcDqAr",
        "invitation": "ICLR.cc/2023/Conference/Paper1633/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submitted manuscript considers the task of fusing camera and lidar data to produce 3D object detections. The paper focuses on a specific type of fusion mechanism where 2D camera features are mapped to 3D for inference and discusses the pros and cons of late vs. early fusion. The main contribution of the paper is a loss function that penalizes inconsistencies across different paths through the network. The experiments demonstrate that the proposed loss function can lead to improved performance on both the KTTI and NuScenes benchmarks. ",
            "strength_and_weaknesses": "The proposed idea (the loss function) is interesting, and I believe it has some merit. The fact that the loss function improves performance on both KITTI and nuScenes data is, of course, a significant strength. At the same time, it is also a simple idea, and I don\u2019t think the authors have done enough to motivate that it is important. \n\nA few aspects that would be good to discuss further: \n\n* How general is this idea? The authors only demonstrate it on a very specific architecture called FocalsConv, and I personally doubt that FocalsConv will be commonly used in the future. Can the authors mention other networks that the loss function could be combined with to motivate that the generality of the idea goes beyond FocalsConv. Ideally, the authors should include such examples in the evaluation, but I am not that is necessary. \n\n* What is the relation between the proposed loss and self-supervised learning? My understanding is that the loss function can be used in a self-supervised manner. In the current version, this has not been discussed or explored in a satisfying manner. ",
            "clarity,_quality,_novelty_and_reproducibility": "As for the clarity and quality of the text itself, the authors initially state that they build on FocalsConv and introduce an additional loss function for path consistency. However, from section two and onwards, the separation between proposed novelties and existing methods is far from obvious. I personally find that this is an important weakness in the current version of the manuscript and something that I hope the authors can improve substantially; it should be easy to fix. \n\nAnother important aspect is that I don't think the description of the state-of-the-art is somewhat misleading in the sense they argue that most methods are shallow, whereas many modern methods are arguably deep according to the terminology used in the paper. ",
            "summary_of_the_review": "To conclude, \n\n*  (pros) the proposed loss function is intuitively appealing,\n*  (pros) the results demonstrate that the loss function improves performance,\n*  (con) the presented solution is not state-of-the-art,\n*  (con) some aspects are not sufficiently well explored,\n* (con) the paper could be more well written.\n\nMy current recommendation is marginally below acceptance, but I would be willing to change that if the paper is adjusted. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1633/Reviewer_gP9m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1633/Reviewer_gP9m"
        ]
    },
    {
        "id": "e08EEqeVMDb",
        "original": null,
        "number": 2,
        "cdate": 1666675022598,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675022598,
        "tmdate": 1666675022598,
        "tddate": null,
        "forum": "2t7L0lcDqAr",
        "replyto": "2t7L0lcDqAr",
        "invitation": "ICLR.cc/2023/Conference/Paper1633/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Empirical results show deep 3d fusion of lidar and camera has inferior performance compared to early stage fusion. A possible reason is the misalignment in deep fusion when deep features have large receptive fields and include overlapping regions of large areas. This paper aims to enable path-consistency between early features and deep features to enforce alignment in deep features. Path consistency is negative cosine similarity or l1 distance between features of lifted 2D features and 3D features whose input is lifted 2D features from previous stage . The experiments show 1.2%+ mAP on nuscenes test set and 0.6%+ on KITTI AP3d moderate compared to its baseline Focals Conv.",
            "strength_and_weaknesses": "Strength:\nThe approach is reasonable. Paper is generally well-written. Ablation study is extensive.\nWeaknesses:\n- The biggest concern is the results only show marginal gain compared to baseline. Especially on KITTI the results are known to be volatile.\n- This paper claims other fusion methods are not optimized in model complexity but no latency comparison is provided.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is generally well-written and I believe it's easy to implement. \n\nA few questions to better understand this paper:\n1) In figure 4(b),  should the input of F_{t+1} be concatenation or summation of y_t and P_t\\circle x_t because the 3D branch is handling fused features? If so, how to separate lifted 2D features from this 3D branch? And are feature channel size the same across different stages?\n2) Is data augmentation applied in KITTI experiments? How does the proposed method handle misalignment caused by data augmentation especially gt sampling? \n\nA minor question: in Section 6 paragraph PathFusion settings 'back-progates to parameters in stage i and the corresponding lifting operator P_i'. The lifting operator is upsampling and mapping the center of 2D regions to 3D voxels. There should be no parameters in lifting operators?",
            "summary_of_the_review": "The biggest concern is the results only show marginal gain compared to baseline. Without latency comparison to other fusion methods and other sota lidar-only methods, the proposed method has not justify the potential of wide application. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1633/Reviewer_W3vh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1633/Reviewer_W3vh"
        ]
    },
    {
        "id": "OoKecSjATsl",
        "original": null,
        "number": 3,
        "cdate": 1666866299713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666866299713,
        "tmdate": 1666866299713,
        "tddate": null,
        "forum": "2t7L0lcDqAr",
        "replyto": "2t7L0lcDqAr",
        "invitation": "ICLR.cc/2023/Conference/Paper1633/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new technique called PathFusion that aims to fuse the LiDAR and camera features, especially the deep layer features. In order to fuse the features at multiple stages, the method design a path-consistency loss to lift and align the 2D features in the camera branch to the 3D features in the LiDAR branch. With the path-consistent regularization, the PathFusion further improves the detection accuracy on both KITTI and NuScenes datasets compared to the baseline Focals Conv method. ",
            "strength_and_weaknesses": "Strength: \n+ The motivation is good. Compared to the previous methods that only fuse the features in shallow layers, this method considers the fusion of deep-layer features which is valuable to multi-modal detection.\n+ The method is easy to follow and reproducable. The definition of path consistency loss is simple. \n+ Experiments on KITTI and NuScenes datasets demonstrate the robustness of the proposed method.\n\nWeaknesses:\n- The contribution is slightly incremental. The main part of this work is the design of path consistency regularization.\n- The forced alignment of the camera and LiDAR features is doubtful. As mentioned in the introduction, the LiDAR branch mainly extracts the geometry features, while the image branch produces the semantic features. Though the 2D semantic features are transformed into the 3D space, the distribution of lifted features remains intrinsic differences. Instead of aligning the features, learning an accurate transformation is more critical. \n- The description of the lift operator is unclear. Are the 2D features in different stages upsampled to the origin resolution before the lift transformation? Is that possible to adjust calibration parameters to directly lift 2D features to 3D features?\n- In practice, the point cloud and image cannot be aligned perfectly due to the synchronization error and motion distortion. For these cases, the shallow fusion is not reliable. Is deep fusion helpful in such situations?",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the strength and weaknesses.",
            "summary_of_the_review": "This paper present PathFusion to fuse the multi-stage features between LiDAR and camera branches to improve 3D detection. A path consistency regularization is designed to align the 2D image and 3D lidar features. Experiments validate the performance of the proposed method. However, the contribution is limited, and the practice of forced alignment is doubtful. Considering the above factors, I vote for the weak reject in this round of review.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1633/Reviewer_3wCP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1633/Reviewer_3wCP"
        ]
    }
]