[
    {
        "id": "9cjLn3YXLO7",
        "original": null,
        "number": 1,
        "cdate": 1666573653987,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573653987,
        "tmdate": 1670874038011,
        "tddate": null,
        "forum": "KzkLAE49H9b",
        "replyto": "KzkLAE49H9b",
        "invitation": "ICLR.cc/2023/Conference/Paper4168/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the alignment between human brains and pretrained large language models (LMs), by learning linear mappings between LM representations and fMRI data on the same corpus. In addition, the authors propose to finetune on narrative text, and find that the alignment can be improved across models by such a process.",
            "strength_and_weaknesses": "\n## Strengths\n\n- This paper presents a reasonable finetuning strategy (i.e., finetuning on narrative text) to improve the alignment between LM representations and fMRI.\n\n- Through comprehensive experiments, this paper presents fairly convincing findings on the linear alignment between pretrained language models and human brains, suggesting that further pretraining/finetuning on narrative corpora improves the alignment. \n\n- The paper is generally clear and easy to follow.\n\n## Weaknesses \n\nThe major weakness of this paper, in my opinion, is that it doesn't involve sufficient baselines to support the claim. For example, I would expect a baseline that finetunes the language model on non-narrative corpora, e.g., Wikipedia or scientific papers: comparing the results with those by the BookSum models in this paper would make the claims much stronger.\n\nIn addition, the following points are not clear enough to me.\n\n- According to Wehbe et al. (2014), all 8 participants have read the Harry Potter books or watched the movies prior to the experiment. I'm not sure if the narrative models are in similar situations as well, i.e., have seen sufficient text relevant to Harry Potter in the pretraining process. If not, the story will be subtly slightly different from its current shape. I would suggest the authors to clarify this point.\n\n- Why did the authors choose models of a different size for BigBird? \nWhile all other investigated models have 12 layers, the selected BigBird model has 32 layers. There exists a [12-layer BigBird model](https://huggingface.co/google/bigbird-base-trivia-itc), and I wonder why the authors did not use it instead for a more uniform setting across investigated language models.\n\n  It's somewhat a commonsense in the NLP community that the base models (named following the HuggingFace convention for models that usually have 12 layers; not to be confused with the usage of \"base\" in this paper) usually have different behaviors from the large models (which usually have 24 or more layers) [1, *inter alia*]. While it's very interesting to see that the alignment get worse when the the depth of layer increases, I look forward to more evidences from experiments with other deeper models (e.g., `t5-large`).\n\nI look forward to the authors' clarification on the above points. \n\n[1] Tenney et al., ICLR 2019. https://openreview.net/pdf?id=SJzSgnRcKX\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality**: This paper is generally well written and of high quality, though the presentation could be improved. My main suggestion is to make the paper (excluding the appendices) more self-contained, by including (part of) the content of Figures 5-8 into the main content. For example, a table with numbers on a slice of sequence length would work better. \n\n**Novelty**: The main contribution of this paper is presenting the finding that finetuning on narrative corpora improves LM-brain alignment. While models and most techniques involved in this paper has been proposed, I think the paper is of enough novelty to be considered for ICLR.\n\n**Reproducibility**: I think the work is reproducible based on the information provided in the paper.",
            "summary_of_the_review": "This paper presents an intuitive finetuning strategy (i.e., finetuning on narrative text) to improve the alignment between LM representations and fMRI. Through comprehensive experiments, this paper presents fairly convincing findings on the linear alignment between pretrained language models and human brains, suggesting that further pretraining/finetuning on narrative corpora improves the alignment. There also exists some points that are not completely convincing, including lack of baselines for controlled experiment and the choices on the investigated NLP model. While I hereby give a score of 5, I am willing to raise my rating if the authors can address some or all of my concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4168/Reviewer_ajVU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4168/Reviewer_ajVU"
        ]
    },
    {
        "id": "yNf9SNnWyF-",
        "original": null,
        "number": 2,
        "cdate": 1666645982700,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645982700,
        "tmdate": 1666645982700,
        "tddate": null,
        "forum": "KzkLAE49H9b",
        "replyto": "KzkLAE49H9b",
        "invitation": "ICLR.cc/2023/Conference/Paper4168/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to study whether particular large pre-trained transformers may align with human brain activity. This is taken as a solid hint that these pre-trained transformers are learning more than simple replicating the input. These models are domain adapted on book texts where they have to learn plots. \n",
            "strength_and_weaknesses": "PRO\n\n- The paper is a solid experimental paper which presents an apparently novel idea\n\nCONS\n\n- It is difficult to interpret results if the reader is not completely aware of the field\n",
            "clarity,_quality,_novelty_and_reproducibility": "This is a very solid paper, which appears to be novel, and has the code to replicate the experiments.\nThe only concern is related to the fact that fMRI is a measure that can record only brain activity in general and not at the word level. Indeed, fMRI records the blood flux in a particular region of the brain. This is used as a proxy of brain activity. Hence, a particular activity can be associated only to a large bunch of heard words. \n",
            "summary_of_the_review": "The ideas in the paper are really interesting and fascinating. Indeed, the parallel between machines and brains has always been used to discover new insights in computer science as well as in understanding brain activity. This parallel has also been studied in:\nZanzotto, F.M., Croce, D. (2009). Reading What Machines \u201cThink\u201d. Brain Informatics. 2009.\nDell\u2019Arciprete, et al (2012). Parallels between Machine and Brain Decoding. In: Brain Informatics. 2012. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4168/Reviewer_nyjm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4168/Reviewer_nyjm"
        ]
    },
    {
        "id": "KjND94hvTSB",
        "original": null,
        "number": 3,
        "cdate": 1666674672910,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674672910,
        "tmdate": 1666674672910,
        "tddate": null,
        "forum": "KzkLAE49H9b",
        "replyto": "KzkLAE49H9b",
        "invitation": "ICLR.cc/2023/Conference/Paper4168/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work shows that after finetuning the pretrained language models on a book corpus, the models align better with brains though their language ability is not better. This better alignment seems general to all brain regions and is also shown in two different metrics. After further evaluating the alignment increase on different discourse features, the authors state that the increase in the Characters is the largest.",
            "strength_and_weaknesses": "Strength:\n\nThis work shows that further training language models on some text corpus can improve their brain alignment, which is exciting and inspiring. This is especially interesting as the text corpus does not contain the data used in the brain data, making this improvement general. Showing that this improvement exists in both brain-alignment metrics and all four models trained makes the results general. The analysis presented by these authors is also interesting, like the discourse feature analysis. In addition, I highly appreciate the share of the code used in the paper and having many results in the supplementary information. \n\nWeaknesses:\n\nTwo significant issues need to be further improved for this paper. First, more results are required to make the results even stronger and more general. Second, the writing of this paper is confusing and needs some work, which will be mentioned in the next part. There are also several minor issues that would be great to be addressed.\n\nFor the result issue, the biggest question is whether these four base models tested in the paper are strong candidates compared to the current SOTA brain model, like GPT-2. In both Schrimpf et al. 2021 and Goldstein et al. 2022, GPT-2 seems to be the best language model for the brain. What is the 20vs20 and Pearson correlation for this model on this dataset? Even if it may not be possible for GPT-2 to be finetuned on the book corpus, I think having the performance of that model is still important to tell us whether the improved performance leads to a new STOA.\n\nFor the minor issues, I will start with interpreting the 20vs20 metric. From my understanding, this metric can also be applied to the ground truth fMRI responses, which may lead to some noise ceiling measures for this metric. But I don\u2019t see any discussion or results on this. Do I misunderstand something? \n\nThen, about the results of the Character feature. Can the authors provide more measures to compare this feature group to others? For example, how many tokens are in this feature group and in other groups? Are the tokens belonging to different groups also distributed in different ways? I think this type of comparison is needed to show that the larger increase is not due to some other reasons.\nFinally, I wonder whether the authors tried the same finetuning on other datasets to show the specific usage of long context in the book corpus, like showing that finetuning on some typical corpus will not work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity needs more work. The paper is confusing in how the two brain-alignment measures are presented. The biggest problem is that the presentations for the two metrics are pretty imbalanced. For example, the authors started in Fig. 1 with a per-layer 20vs20 performance figure. What about the same figure for the Pearson correlation metric? There should be figures showing the overall/aggregated improvement for all models to support the major claim of the authors, but I cannot find those results easily. For the 20vs20 metric, I can find some \u201call\u201d measures separately in the subfigure for each model. Why don\u2019t the authors build one figure showing all these \u201call\u201d measures and put that in Fig 2 right? Fig 2 is confusing in its design: I was expecting that the right panel should show the corresponding brain measures for all models or even have two panels for each of the metrics; instead, it was just for one model, just one measure. There are also some minor inconsistencies in the writings and the figures. Like the Character increase was said to be 0.011 in the text, but the figure in the main paper only showed 0.009. After checking the Supp. Info., I found that 0.011 was for model BART.\n\nThe results seem to be reproducible. The proposed finetuning methods are also novel.  \n",
            "summary_of_the_review": "This paper shows that after finetuning the pretrained language models on a book corpus, the models align better with brains though their language ability is not better. Although the results are exciting and inspiring, I think some more work (which should be doable during the rebuttal phase) is needed to make it acceptable.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4168/Reviewer_m5W3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4168/Reviewer_m5W3"
        ]
    },
    {
        "id": "NTlagp5NEr",
        "original": null,
        "number": 4,
        "cdate": 1666734752061,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666734752061,
        "tmdate": 1666734752061,
        "tddate": null,
        "forum": "KzkLAE49H9b",
        "replyto": "KzkLAE49H9b",
        "invitation": "ICLR.cc/2023/Conference/Paper4168/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a neural network brain encoding analysis comparing the performance of Transformer architectures trained on language modeling objectives versus those trained on a narrative summarization objective. The authors argue that the narrative summarization task is a proxy for \"deeper understanding,\" and that the improved brain encoding performance of models optimized on the task (AKA BookSum models) demonstrates this point. They show that the improved brain encoding performance of BookSum models is not reducible to improved language modeling performance, and use a data-driven evaluation to further study the representational content that could be driving increased brain encoding performance.",
            "strength_and_weaknesses": "- The paper addresses a timely question about the sorts of language training objectives which produce brain-like representations. This question is of interest to several different fields within cognitive science/AI.\n\t- Note that this is not exactly the framing the authors themselves choose: they seem to believe that brain mapping performance can select between different types of models in deciding which models are \"deep\" in the same sense that human language processors are. I'm skeptical on this, but I think the methods and results have value in any case.\n- Good effort at interpreting the results with several creative follow-up experiments (sec. 5: is improved performance reducible to language modeling?; sec. 6: what sorts of representational content are driving improved performance?).\n- The notion of \"deeper understanding\" is never clearly defined *a priori* in the text, except as something that is instantiated in the BookSum task, and something which isn't reducible to language modeling.\n\t- A posteriori, it also seems to be something which yields model representations with improved brain encoding performance\u2014across all tested ROIs!\n\t- For a paper at this conference, however, I don't think this is a critical issue. I'd encourage the authors to follow up this research and test their concept of \"deeper understanding\" by considering alternative tasks other than narrative summarization.",
            "clarity,_quality,_novelty_and_reproducibility": "- Quality\n\t- There should be more controls for confounds in the training regime. As I understand it, the evaluation compares pretrained language models with the same architectures fine-tuned on another task. Two possible confounds here:\n\t\t- Effect of training environment: the mere training environment (e.g. doing gradient updates with the hyperparameters chosen by the authors) may affect brain decoding performance. The authors can control for this by fine-tuning the architectures on their original training data with a language modeling objective and with the same/similar optimization environment used for BookSum.\n\t\t- Effect of domain shift: while the training domains of the pretrained models and the Booksum models are somewhat similar, there's still a chance that the BookSum domain is what is helping the models. The authors can control for this by fine-tuning the architectures on the BookSum data (or their original training data mixed with the BookSum data) with a language modeling objective.\n- Novelty: this fine-tuning-plus-brain-mapping paradigm is not entirely novel, but this use case with narrative summarization and their follow-up empirical experiments are original to the best of my knowledge.",
            "summary_of_the_review": "Overall I think the paper does a nice job of pairing performance gains with multiple attempts at explaining their cause. I'm not especially convinced by the final interpretability analysis, but I think these results are interesting and can help to start a good discussion in the community. I vote to accept, and encourage the authors to add the controls mentioned in the \"Quality\" section above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4168/Reviewer_Z82i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4168/Reviewer_Z82i"
        ]
    }
]