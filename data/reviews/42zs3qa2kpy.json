[
    {
        "id": "NmXBGzlo_u",
        "original": null,
        "number": 1,
        "cdate": 1666545367679,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666545367679,
        "tmdate": 1670051273973,
        "tddate": null,
        "forum": "42zs3qa2kpy",
        "replyto": "42zs3qa2kpy",
        "invitation": "ICLR.cc/2023/Conference/Paper4151/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a method for offline reinforcement learning that involves fitting a more expressive density model and an episodic planning technique. The method demonstrates decent results on D4RL, a standard benchmark for offline reinforcement learning.",
            "strength_and_weaknesses": "# Strength \n* The approach is simple and produces strong results on the standard benchmark.\n\n# Weaknesses \n* Right now, the main weakness is a lack of clarity. It is not clear how the method is implemented, and the paper can be significantly improved by expanding the implementation details.\n* The approach has a limited novelty. Expressive policies have been previously studied in EMAQ [1]. Moreover, the value computation technique is similar to a soft version of EMAQ.\n* The related work section misses some important prior work in particular [1], which also introduces a similar approach.\n* At the end of section 4.2, it's mentioned that the n-step operator proposed in this work works in the stochastic setting. However, this statement is not completely clear. For example, does it mean that it's unbiased? \n\n\n[1] EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL\nSeyed Kamyar Seyed Ghasemipour, Dale Schuurmans, Shixiang Shane Gu",
            "clarity,_quality,_novelty_and_reproducibility": "The paper needs to be improve in terms of clarify. Some important prior work is missing. Given a limited discussion of implementation details, it might be difficult to reproduce the results in the paper.\n\nPlease see the strengths and weakness for more details",
            "summary_of_the_review": "In overall it's a simple a well-performing method. However, it has limited novelty and the writing needs to be improved before the paper can be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4151/Reviewer_dfM5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4151/Reviewer_dfM5"
        ]
    },
    {
        "id": "UvfNDm8X92B",
        "original": null,
        "number": 2,
        "cdate": 1666570099466,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666570099466,
        "tmdate": 1670174530083,
        "tddate": null,
        "forum": "42zs3qa2kpy",
        "replyto": "42zs3qa2kpy",
        "invitation": "ICLR.cc/2023/Conference/Paper4151/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to learn the offline RL policy by a coupling of pure behavior cloning and importance sampling. The behavior cloning is modeled by a denoising diffusion probabilistic model, and an implicit in-sample Q-learning is applied to estimate the importance sampling weight. Experiments are done in the D4RL benchmark datasets to demonstrate the effectiveness of the method. ",
            "strength_and_weaknesses": "Strengths: \n1. The performance of SfBC evaluated on D4RL is comparable to the current SOTA methods. \n2. The authors demonstrate the importance of using expressive policy class through toy examples. \n3. The paper is easy to follow and well written. \n\nWeaknesses:\n1. This paper lacks novelty. Policy improvement via weighted regression has been well studied in many papers, such as IQL and AWAC paper. The proposed Q-learning via in-sample planning is motivated from Ma et al. 2022. Applying diffusion models in offline RL is new while using diffusion models for behavior cloning seems an easy plug-in.  \n2. The paper misses some details about implementation. \n3. Computational complexity is a concern and not discussed. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well structured and easy to read. The originality is limited. Code is not provided and the paper misses some implementation details, so the reproducibility is limited. ",
            "summary_of_the_review": "This paper proposes to separate the policy learning into two parts: 1) pure behavior cloning via diffusion models; 2) importance sampling via in-sample Q-learning. The idea of using diffusion models as an expressive policy is appealing. However, I do have concerns about the novelty and computational complexity. \n\nAs I mentioned in the weaknesses, most of the components from the paper are proposed by prior works. Moreover, by looking at the Algorithm 1, performing implicit in-sample planning has high computational complexity. There are two for loops, one for K and the other for N, which is dataset size and usually huge. Diffusion models are notoriously slow in sampling, while the training algorithm needs samples from diffusion models within the K$\\times$N loop, which has too high complexity.\nI also have some detailed questions as follows: \n1. What is the number of timesteps for the diffusion model used in the paper? I guess an ablation study on the number timesteps is needed to show the effect of it.  \n2. Better provide a training curve in the appendix to show the training stability. \n3. For the reported values in Table 1, what are these values? Are them the statistics based on the last points after the training or the highest points during the training? \n5. The policy network is much larger than the current policy networks, which makes the computational cost expensive and comparison not very fair. \n6. Expect to an ablation study on the K, since it is a key parameter in Algorithm 1. \n7. Minor: typo in Equation (12), I think it is $R^{(k-1)}_{n+1}$.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4151/Reviewer_eCnE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4151/Reviewer_eCnE"
        ]
    },
    {
        "id": "9iC0kJFFIM",
        "original": null,
        "number": 3,
        "cdate": 1666701588762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701588762,
        "tmdate": 1669213291679,
        "tddate": null,
        "forum": "42zs3qa2kpy",
        "replyto": "42zs3qa2kpy",
        "invitation": "ICLR.cc/2023/Conference/Paper4151/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work uses a diffusion model as an expressive generative model for behavior cloning. They then turn this into a offline RL algorithm with model improvement by learning a Q-function and performing a form of importance sampling to rejection sample action samples with high value by sampling from the behavior clone model and resampling based on Q. They test this on some standard offline RL benchmarks.\n",
            "strength_and_weaknesses": "Strengths:\n1. The paper is well written and explains the method.\n\n2. The method is well motivated.\n\n3. The empirical results show good.\n\n4. The topic is off significant interest.\n\nWeaknesses:\n1. I have concerns about the reproducibility, I believe the norm (particularly for this topic) is to include the source code in the submission rather than a promise of future code. Generally, particularly in RL, people have struggled to exactly reproduce results just from pseudocode.\n\nFor example, I did not see in Appendix C the number of diffusion steps used (quite an important detail). It would also be helpful to discuss the runtime costs of sampling from diffusion models.\n\nIdeally the results would be over more than 3 seeds, but this is not vital.\n\nOther:\n\n[1] Is a closely related work that uses diffusion models for offline RL that should be mentioned in related work (although the method is not identical). It also makes similar claims about the importance of expressive, multi-modal policies.\n\n[1] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. \"Diffusion policies as an expressive policy class for offline reinforcement learning.\" arXiv preprint arXiv:2208.06193 (2022).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is fairly well written.\n\nQuality: The paper is of high quality.\n\nNovelty: The paper applies diffusion models to offline RL and uses a well-known solution to solve for high value actions while minimizing KL divergence from the behavior policy. The only other work applying diffusion models to model-free RL is [1], which was presumably contemporaneous with this work and the specifics of the two approaches are distinct.\n\nReproducibility: The source code is not provided (it is suggested that it will be made available later). For this style of paper, with benchmarks on a standardized set of tasks in a rapidly area of research this seems below the norm. In particular, I would imagine people would struggle to reproduce these results without access to the code.\n\n[Updated during rebuttal. The authors now provide source code. I did not test it but this addresses my concerns about reproducibility].",
            "summary_of_the_review": "A well-present piece of work applying diffusion modeling to offline RL. It would be strengthened by making the source code available to ensure it can be reproduced and built on [I note this concern is fixable during the rebuttal period].\n\n[Updated]\n\nThe authors have uploaded source code, cited additional work and made other improvements. I have increase my rating.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4151/Reviewer_3dPg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4151/Reviewer_3dPg"
        ]
    }
]