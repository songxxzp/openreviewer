[
    {
        "id": "f9E0d6GKNJ",
        "original": null,
        "number": 1,
        "cdate": 1666387362325,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666387362325,
        "tmdate": 1666387362325,
        "tddate": null,
        "forum": "3dH2aqKGzZe",
        "replyto": "3dH2aqKGzZe",
        "invitation": "ICLR.cc/2023/Conference/Paper5656/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the problem of on-device ASR self-supervised learning. In this scenario, there is a large unlabeled dataset is available for unsupervised pre-training, and a much smaller supervised set for fine-tuning. Previous models such as HuBERT, data2vec use a large transformer model for the feature pre-training. Such a transformer is resource hungry for the scenario of running on an mobile phone or an embedded device.\n\nThis work proposes 3 different components. First, there is a method to prune the input tokens. The paper uses a teacher ASR model to train a small classifier which predicts which input token is salient. Examples of salient tokens are the first of the repeating letters, or the first of several silences.\n\nSecond proposed component is the method to sparcify the model. The paper builds upon the PARP and proposes an iterative procedure of sparcification and finetuning steps.\n\nFinally, the third proposed model combines both previous models to use them for semi-supervised distillation from a teacher model.\n\nThe paper reports the results on Librispeech and Superb corpora and then evaluates the latency on a real device.",
            "strength_and_weaknesses": "Strengths:\n  - The paper tackles a challenging problem.\n  - The proposed models are sound. The design makes sense and motivated well\n  - The paper provides evaluations on several datasets: Librispeech and Suberb\n  - The paper evaluates the latency on a real device\n\nWeaknesses:\n  - There is some lack of focus in this paper: there are three models introduced. Each one is complex enough to have a small paper. This leads to a very condensed writing and each model is not described well.\n  - The results are hard to compare to previous works. For example, the Fig. 3 models are trained in a different setup than wave2vec2 or HuBERT, therefore not comparable. The PARP paper's setup is also a bit different than presented here.",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\n\nThe paper is mostly clearly written. I found that the section 3.4 could be extended with more details about the center model of the paper. For example, I did not understand how the argmax is computed (examples are sampled or really summed over the whole set?). There are a number of small issues with writing such as repeating expressions. I also found it very distracting to use such abbreviations for model names -- this is just over the top.\n\n# Quality\n\nGenerally, the paper has above average quality. The methods are sound and and there is an attempt to experimentally test all the claims. One of the highlights of the paper I liked, and I don't see frequently, is testing on a real device. As mentioned before, the paper lacks focus by introducing several disconnected methods. Then, the presentation of the results needs alignment with the prior works as the faulty baseline might diminish the support for the claims.\n\n# Novelty\n\nEach proposed method has average novelty, but the combination of them seems to be novel.\n\n# Reproducibility\n\nThe paper introduces several methods with many hyperparameters. The hyperparameters are given in the paper, but it might be challenging to reproduce all the implementation detals. The paper does not provide error bars and doesn't test the models for stability and robustness.\n\n# Other\n\nThere is a recurring mistake in the paper: difference in percentages is not a percentage https://en.wikipedia.org/wiki/Percentage_point",
            "summary_of_the_review": "Given the previous assessment, now I recommend reject.\n\nBut this paper has a potential, and here are the ways to improve it:\n  - Align the experimental results to the prior literature: HuBERT, data2vec, wav2vec, PARP. If it is not possible, provide a disclaimer (for example, that the baseline was trained on a larger corpus and therefore had better performance).\n  - Tie the different components together.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5656/Reviewer_V9mU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5656/Reviewer_V9mU"
        ]
    },
    {
        "id": "BuUDPfKyuH",
        "original": null,
        "number": 2,
        "cdate": 1666644944125,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644944125,
        "tmdate": 1666644944125,
        "tddate": null,
        "forum": "3dH2aqKGzZe",
        "replyto": "3dH2aqKGzZe",
        "invitation": "ICLR.cc/2023/Conference/Paper5656/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes two methods for efficient ASR modeling, the token/frame skipping and structured pruning. The token skipping is achieved by applying a binary classifier to detect whether a token is salient. The structured pruning is an iterative process of sparsifying, fine-tuning and recovering, until reaching a certain sparsity level. Experimental results demonstrates the effectiveness in terms of FLOPs savings against WER increases, in comparison with other popular pruning methods.\n",
            "strength_and_weaknesses": "Pros:\n1. The paper is clearly written and easy to follow. The illustrative figures and examples are helpful to understand the ideas.\n2. Examples are conducted on different backbone models including wav2vec2 and data2vec, with different sizes.\n3. The ablation study compared with random removing frames or adaptive removing, are very useful to establish a sense of how the proposed method performs compared with baselines.\n4. The pruning performance is impressive compared with other methods. I guess it is due to the token skipping\n\nCons:\n1. The idea of both methods are incremental, though combining them and show they works is the value of this paper.\n2. In figure 3 (a), why the S6-DAMON does not have values below 40% FLOPs? Is it that the WER is worse than the others?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. Novelty is not enough. No code or trained-model released for reproduction.",
            "summary_of_the_review": "Overall, though the papers' core ideas are not novel, the combination and comprehensive experimental evaluation show that it is working well. One concern is that no source code nor trained model provided for reproduction.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5656/Reviewer_JZjt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5656/Reviewer_JZjt"
        ]
    },
    {
        "id": "knR1rgDcs3",
        "original": null,
        "number": 3,
        "cdate": 1666678423386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678423386,
        "tmdate": 1666678423386,
        "tddate": null,
        "forum": "3dH2aqKGzZe",
        "replyto": "3dH2aqKGzZe",
        "invitation": "ICLR.cc/2023/Conference/Paper5656/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper applies two approaches for improving the speed of ASR models: learning to identify salient tokens and skip those that are identified not salient, and weight pruning. The results show some improvements on the SUPERB set and LibriSpeech-100h. The authors also benchmark the inference speed on Pixel 3 phone and demonstrate good speedup with the technique.\nTwo questions: \n(1) in section 4.5, when comparing different skipping methods, are the models also fine-tuned with those skipping methods? Or are they only applied during inference?\n(2) On the SUPERB set, why would the quality be better than the uncompressed model?\n\nA more recent related work on self-supervised learning for speech is missing:\nSelf-supervised learning with random-projection quantizer for speech recognition. ICML 2022",
            "strength_and_weaknesses": "Strength: the results in the paper show significant improvement compared to the previous work.\nWeakness:\n(1) The comparisons with the previous approach are limited, making it less convincing.\n(2) The comparisons with other pruning algorithms are also lacking.\n(3) SALAD is only slightly better than a simple uniform skipping.\n(4) The proposed pruning algorithm is a minor extension of PARP, making its novelty limited.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The article is clear. The contribution mostly is in the performance improvement.",
            "summary_of_the_review": "The proposed approach shows good improvement compared with PARP, but the empirical studies are a bit limited. There should also be more comparisons with other algorithms. The technical novelty is generally limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5656/Reviewer_jv1R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5656/Reviewer_jv1R"
        ]
    }
]