[
    {
        "id": "SnrTymK-sFV",
        "original": null,
        "number": 1,
        "cdate": 1666624236379,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624236379,
        "tmdate": 1666624305063,
        "tddate": null,
        "forum": "9gfir3fSy3J",
        "replyto": "9gfir3fSy3J",
        "invitation": "ICLR.cc/2023/Conference/Paper1421/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a neural representation method of neural networks (NeRN) to represent the weights of neural networks, which maps coordinates to convolutional kernels. The paper also considers spatial smoothness constraints on networks\u2019 weights  to help NeRN. Moreover, the paper also points out two applications using NeRN, including filter visualization and network structure pruning. Experiments show good reconstructions of CNNs.",
            "strength_and_weaknesses": "Strengths:\n(1)\tThe paper provides a novel method for NeRN, which surprisingly reconstruct the weights of CNNs with implicit neural representations. \n(2)\tTo boost the performance, the paper also introduces a smoothness constraint on NeRN.\n(3)\tThe paper shows two promising applications of NeRN, including weight importance visualization and meta-compression.\n\n\nWeaknesses:\n(1)\tIn the meta-compression, it is unclear how the paper performs meta-compression. The paper claims \u201cwe apply na\u00efve magnitude-based pruning on the predictor\u201d, why do not directly prune raw pre-trained weights? I do not understand how the reconstructed weight works. Given a NeRN, we can map coordinates to weights and can also filter large magnitudes or map to sparse structured weights. If so, how to select the channel number for different layers? The paper claims the NeRN may reduce the disk size. A drawback could increase computational cost since we must reconstruct weights each forward. I am not sure about the inference time compared with the original weights. Moreover, because it needs to reconstruct intermediate temporary networks, how does it reduce the disk size? It would be better if some more applications are introduced.\n\n(2)The paper mainly focuses on \u201cweight generator\u201d, while the reference [r1] focuses on \u201cweight discriminator\u201d. Is it possible to design a \u201cweight GAN\u201d to generate realistic weights (sharp and containing high frequencies) without smoothness constraints? Or some permutation constraints (cross-filter and in-filter, e.g., chain normalization rule [r1]) on weights can be exploited as extra constraints to boost the reconstruction instead of strong smoothness assumptions? Some discussions would be nice if possible.\n\n[r1]Understanding Weight Similarity of Neural Networks via Chain Normalization Rule and Hypothesis-Training-Testing\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper provides a novel method for NeRN, which surprisingly reconstruct the weights of CNNs with implicit neural representations. The paper also shows some promising applications of NeRN. Overall, in my opinion, the paper is novel and the idea of weight reconstruction is interesting. It would be a good paper that provides a possible solution to hyper-networks or meta-learning.\n",
            "summary_of_the_review": "(1) A novel method for NeRN for weight reconstruction of CNNs.\n(2) Good experimental results of reconstructed weights of neural networks.\n(3) Some weaknesses exist, but not a big question.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1421/Reviewer_HBhL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1421/Reviewer_HBhL"
        ]
    },
    {
        "id": "VipWwQkFYb",
        "original": null,
        "number": 2,
        "cdate": 1666626913272,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626913272,
        "tmdate": 1666626913272,
        "tddate": null,
        "forum": "9gfir3fSy3J",
        "replyto": "9gfir3fSy3J",
        "invitation": "ICLR.cc/2023/Conference/Paper1421/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper present NERN which is a technique for learning neural representations for convolutional NNs. The parametrize the weight space by (layer, filter, channel) and train a MLP to predict the  k x k convolutional kernels. They combine different loss functions, experiments on CIFAR and ImageNet as well as provide multiple ablation studies.",
            "strength_and_weaknesses": "The technique seem novel and interesting. The paper in itself is sound and thorough. I especially like the smoothness regularization and experiments around it, e.g. the positional encodings. The weakness is the comparison in performance to other related work, like knowledge distillation etc. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, and novelty and reproducibility are strong. I quickly inspected some of the code and it looks good. The only thing missing is comparison with other methods.",
            "summary_of_the_review": "Great paper. I only miss comparison with other methods. Would love to hear authors comments on this. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1421/Reviewer_uYwd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1421/Reviewer_uYwd"
        ]
    },
    {
        "id": "Zye_EKFbSFr",
        "original": null,
        "number": 3,
        "cdate": 1666851082748,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666851082748,
        "tmdate": 1666851082748,
        "tddate": null,
        "forum": "9gfir3fSy3J",
        "replyto": "9gfir3fSy3J",
        "invitation": "ICLR.cc/2023/Conference/Paper1421/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors have proposed several ways to enhance the smoothness of the reconstructed weights. They show that incorporating a smoothness constraint over the original network's weights aids the neural networks towards a better reconstruction. They illustrate the utility of smoothness in reconstructed weights by experimenting on several real datasets.",
            "strength_and_weaknesses": "Strengths:\n1. The problem is interesting and well-motivated. The authors point out that by changing the order in which the neural network predicts the kernels, we can improve the smoothness of the weights and yield better reconstruction accuracy.\n2. Extensive experiments. The authors show the effectiveness of several smoothness-enhancing techniques and validate them on various datasets.\nWeakness:\n1. It introduces both size and time overhead to apply permutation smoothness. I am wondering about the time complexity to find proper permutations. Is it scalable with the network size?\n2. It would be better if the authors can provide their intuitions of why cross-filter permutation is in general the best strategy. Is this conclusion universal or dependent on the data the original network is trained on or dependent on the architecture of the original network?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and has some novelty.",
            "summary_of_the_review": "Overall, the paper is well-written and well-motivated. The experiments are extensive.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1421/Reviewer_MGd2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1421/Reviewer_MGd2"
        ]
    },
    {
        "id": "5OHPj6x95q_",
        "original": null,
        "number": 4,
        "cdate": 1666852927169,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666852927169,
        "tmdate": 1666852927169,
        "tddate": null,
        "forum": "9gfir3fSy3J",
        "replyto": "9gfir3fSy3J",
        "invitation": "ICLR.cc/2023/Conference/Paper1421/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors present a novel method inspired by neural representations like NeRFs, SIRENs, and NeRVs to represent convolutional neural networks. They use position encodings as the input coordinates to for the 5-layer MLP NeRN to represent the location of the convolutional kernel within the architecture. the output of the NeRN is the parameters of the corresponding kernel. They also propose a hybrid objective function and some smoothing techniques to improve the model performance. Their approach is evaluated on 3 different ResNet architectures on CIFAR-10, CIFAR-100, and ImageNet and achieves comparable performance to the performance of the original architectures while using less space.",
            "strength_and_weaknesses": "# Strengths\n- The paper is very well written and easy to read.\n- The results are compelling.\n- The method is novel to my knowledge.\n- The approach considered suggests future explorations that could be useful.\n\n# Weaknesses\n- Some sections are not as clear as they could be. For instance, the section detailing the permutation-based smoothness was hard to understand and could be better explained, perhaps with the addition of an extra diagram.\n- Some of the decisions are not extremely well motivated or ablated against. For instance:\n  - What is the difference between using positional encodings and using directly using coordinates like those used in the SIREN?\n  - Why is smoothness designed in this way? There doesn't seem to be any strong motivation for why neighboring kernels should look similar. What assumption is this smoothness based on?\n- I would like some experiments with non-ResNet CNN architectures to be explored. Right now it's hard to say if the NeRN generalizes to any other architectures even though that is what is implicitly assumed by the paper.\n- The performance of the NeRNs is still consistently lower than that of the original CNNs. What might be some reasons for this? It seems like increasing the capacity of the NeRN helps but the model seems to be limited in some way based on the results.\n- How does this method compare to HyperNets which also attempt to predict the weights of a network albeit in a different way?",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is very clearly written and easy to understand except for some sections I have mentioned in the Weaknesses section.\n- The quality of this paper is high. The experiments are thorough, and the discussion is compelling.\n- I have not attempted to reproduce the results of this paper.\n\n# Minor corrections\n- line 4 on the second paragraph of page 5: wights should be weights.",
            "summary_of_the_review": "I think this is a really interesting paper that has the opportunity to lead to some further interesting research directions in the community.\nHowever, there are some weaknesses I pointed out in the weaknesses section that I would appreciate if the authors could address.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1421/Reviewer_jngE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1421/Reviewer_jngE"
        ]
    }
]