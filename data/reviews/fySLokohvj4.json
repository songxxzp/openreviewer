[
    {
        "id": "in25siZ3nqe",
        "original": null,
        "number": 1,
        "cdate": 1666458919568,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666458919568,
        "tmdate": 1670876373275,
        "tddate": null,
        "forum": "fySLokohvj4",
        "replyto": "fySLokohvj4",
        "invitation": "ICLR.cc/2023/Conference/Paper3544/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the problem of general functions from a general function class under a bandit feedback model. Specifically, this work considers the setup of heteroscedastic noise where in the noise varies with different actions, possibly depending on the action taken by the learner. Under this scenario, the authors proposes a new algorithm that can leverage the heteroscedasticity of the noise and incurs a regret bound dependent on it, allowing it to take advantage of low noise regimes. The authors also propose a variant for the generalized linear bandit model and establish its regret performance.",
            "strength_and_weaknesses": "Strengths:\n- The paper considers an important problem with real applications and has proposed several algorithm to tackle the challenges of heteroscedastic noise that offer improved performance over existing ones.\n\nWeaknesses:\nIn additional to the comments about novelty in the next section, I have following questions/comments.\n- The algorithm assumes availability of $\\sigma_t$ from the environment which is different from estimating it. Can the authors support the feasibility of such an assumption by perhaps an actual application?\n- In Sec. 5. it says, \"In this section, we consider the original setting introduced in Section 3 without Assumption 4.1.\", where Assumption 4.1 is the assumption sub-Gaussianity on the noise. However, in proof of theorem 5.1, sub-Gaussianity of noise plays an important role and the result of this theorem is used throughout the section. Can the authors please explain this inconsistency?\n- While I understand that the main contributions of this work are theoretical in nature, I feel it is important to provide empirical evidence, especially while submitting it to a conference like ICLR. I am also interested in seeing the practical performance of these algorithms because of their similarity to Sup-family of algorithms (see next section) which are known to perform poorly in practice. Having some simulations might provide better insight into the algorithm performance and its ability to leverage heteroscedastic noise.\n- As a minor comment, I think the exposition would benefit by adding some more intuition about the algorithms while describing them.\n- Typos: provaly -> provably (pg 2, second paragraph), variaces -> variances (pg 5, last paragraph before Sec. 4).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "My main concern with this paper is in terms of novelty, as I am unable to pinpoint the main novel contribution of this work. \n- In terms of algorithmic framework, the proposed $ML^2$ algorithm is essentially a minor variant of the sup-family of algorithms, i.e., Sup-Lin-UCB (Auer, 2002), Sup-Kernel-UCB (Valko et al, 2013). The only difference is that the authors use the $\\sigma_t^2$ provided by the environment instead of say, given by the GP model in Sup-Kernel-UCB. Hence, from the algorithmic viewpoint, $ML^2$ offers limited novelty.\n- In terms of analysis, most of the analysis is based upon combining existing results in related work. The only difference from other works is the use of $\\sigma_t$, instead of a global bound $R$, which from my understanding of the appendix does not seem to offer any major technical hurdle. Moreover, since the authors also do not point a novelty in terms of analysis in the main text, it reinforces my earlier point.\n- In terms of regret bounds: From the \"main contributions\" section in the introduction, it seems that most of the contribution is in terms of improved regret bounds that have a variance-dependent term. However, it is not fair to compare the bounds obtained in this work directly with classical results in homoscedastic noise. This is because under this setup the author assume access to additional information about $\\sigma_t$, which is not assumed in the homoscedastic version. From what I understand, the variance-dependent improvement in the regret bounds comes due to availability of this additional information. Furthermore, the access to this additional information and its use in algorithm takes away most of the challenge in algorithm design in the heteroscedastic setup.\n\nSo overall, unless I am missing out on something major, I don't see the contribution of this paper at this stage to be novel enough to warrant a publication.\n\nEDIT:\nAfter discussion with authors, I am increasing the score a bit. However, as mentioned in the comment to them, I still don't think the novelty in analysis is strong enough to warrant a publication.",
            "summary_of_the_review": "As mentioned above, I am not convinced about the contribution being novel enough, which is why I am giving the paper a score of 3.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3544/Reviewer_pkxU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3544/Reviewer_pkxU"
        ]
    },
    {
        "id": "vM99MJdoks",
        "original": null,
        "number": 2,
        "cdate": 1666597396992,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597396992,
        "tmdate": 1666604410698,
        "tddate": null,
        "forum": "fySLokohvj4",
        "replyto": "fySLokohvj4",
        "invitation": "ICLR.cc/2023/Conference/Paper3544/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work considers the bandits' problem with general function classes and heteroscedastic noise. Under the assumption that the variance of the noise is simultaneously given with a reward after each round, they give the first variance-aware regret bound in terms of Eluder dimension. Their main technical innovation is to propose a multi-level learning framework that partition the historical data into several layers according to the given variance and applies base bandits algorithms on that. Besides the result on general function classes, they further give results on generalized linear functions under this \"multi-level\" framework. By using the generalized linear online-to-confidence-set conversion (GLOC) algorithm proposed by Jun et al. (2017), equipped with FTRL, they show that the results under such function classes can be further improved by $\\widetilde{O}\\left(\\sqrt{\\operatorname{dim}_E \\log N_\\alpha R T}\\right)$",
            "strength_and_weaknesses": "Strength: Their results are solid. Their proposed \"multi-level\" framework has some novelty.\n\nWeakness:  The known variance assumptions largely reduce the difficulty of the whole problem. With this info and given the previous VOFUL algorithm which also considers the multi-level variance, this idea seems not very surprising to me. Furthermore, its generalized linear function results also seem a bit incremental. (Their first improvement that change R to $\\sigma_\\text{max}$ seems very direct given the variance is known. Their further improvement on d is not clear to me how significant it is...)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written.",
            "summary_of_the_review": "The results are solid and clear. My only concern is their techniques are a bit incremental and their known variance assumptions make those techniques even easier. But I am willing to raise my score if there are some technical novelties that I missed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3544/Reviewer_npEC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3544/Reviewer_npEC"
        ]
    },
    {
        "id": "56068ObR1tk",
        "original": null,
        "number": 3,
        "cdate": 1666930173604,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666930173604,
        "tmdate": 1666930173604,
        "tddate": null,
        "forum": "fySLokohvj4",
        "replyto": "fySLokohvj4",
        "invitation": "ICLR.cc/2023/Conference/Paper3544/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied bandits with general function class and heteroscedastic noise. A variance-dependent regret bound is derived.",
            "strength_and_weaknesses": "Extending linear model to nonlinear model with provable guarantee is an important topic in bandits problems. However, the approach of using eluder dimension is a bit incremental. There is no much technical novelty as far as I know. \n \n1. It is well known that only linear and generalized linear models have bounded eluder dimension, and the eluder dimension of one-layer neural network has exponential dependency on d. Using the Eluder dimension does not provide much more insights than linear models. So it is unclear to me how \"general\" the function class is.\n \n2. The efficiency column of Table 1 is very misleading. It is also well known that UCB based on eluder dimension is computationally-inefficient for non-linear function class. I do not think an empirical risk minimization oracle is enough. If we look at line 5 in Algorithm 1, you need an optimization oracle to find f. This is a much stronger oracle. \n \n3. Most techniques and algorithmic designs seem to be used before (variance-aware algorithm, multi-level partition scheme of the observed data). This restricted the technical novelty of the paper.\n \nLinear Contextual Bandits with Adversarial Corruptions, NeurIPS 2021. \nNearly minimax optimal reinforcement learning for linear mixture markov decision processes. COLT 2021.\n \nExtending the UCB-style algorithm from linear case to non-linear case using Eluder dimension also has been used in multiple papers before. \n \nReinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension. NeurIPS 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Good.",
            "summary_of_the_review": "This paper is a bit incremental since it combined eluder dimension approach with variance aware confidence set. Several limitations:\n \n1. Eluder dimension provided little new information beyond linear. \n2. The algorithm is not computationally efficient with non-linear function approximation.\n3. The author should carefully discuss technical novelty beyond existing works since this is a pure theory paper. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3544/Reviewer_Hrpw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3544/Reviewer_Hrpw"
        ]
    },
    {
        "id": "cpQ9l0yUSxj",
        "original": null,
        "number": 4,
        "cdate": 1667172403756,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667172403756,
        "tmdate": 1667172403756,
        "tddate": null,
        "forum": "fySLokohvj4",
        "replyto": "fySLokohvj4",
        "invitation": "ICLR.cc/2023/Conference/Paper3544/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers a stochastic bandit model where the reward function belongs to a class of uniformly bounded functions and the additive noise can be heteroscedastic. A multi-level learning framework is proposed to tackle this general bandit model where this paper designs an algorithm to construct the variance-aware confidence set. For generalized linear bandits, FTRL is introduced to achieve a tighter regret.  \n",
            "strength_and_weaknesses": "Strength: This paper is well written. The proposed model extends the previous generalized linear ones with sound theoretical results.\n\nWeakness: No experimental results are provided to support the theoretical results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper talks about extended models related to generalized reward function in stochastic bandit problems with good quality and clarity.\n",
            "summary_of_the_review": "Overall, I think that the studied model in this paper is well-motivated. The introduced variance-dependent regret and FTRL in the extended model are interesting and with sound analysis. Therefore, I recommend \u201cweak accept\u201d.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3544/Reviewer_gqkL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3544/Reviewer_gqkL"
        ]
    }
]