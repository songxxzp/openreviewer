[
    {
        "id": "a5XptLgsFS",
        "original": null,
        "number": 1,
        "cdate": 1665994433962,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665994433962,
        "tmdate": 1670475991109,
        "tddate": null,
        "forum": "9MbhFHqrti9",
        "replyto": "9MbhFHqrti9",
        "invitation": "ICLR.cc/2023/Conference/Paper1723/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method to learn object detectors from text-to-image synthesis data. The idea looks interesting to the community.\nIt first generates sentence description with prefix constraints and GPT-2, then generates imaginary images with DALLE-mini. \nA WSOD is applied on sythensis image. The whole method is named ISOD. Experimental on PASCAL VOC 2007 shows that ISOD achieves close performance to WSOD on real images and annotations, while the performance could be even boosted when gradually increases real images and annotations. ",
            "strength_and_weaknesses": "Strong points\n+ The idea looks very interesting to me. \n\nWeak points\n- Some descriptions are not very clear. For instance, \n\n       - what are the prefix in the language models? How different prefix make impacts for the final results? \n\n       - In Table-1, there are some classes (boat, chair, etc) that Imaginary perform worse than CLIP, it lacks explanations. Are these due to the image synthesis errors?\n\n- The idea looks a bit straightforward, but the formulation is a bitter weak. It does not take the language and text-to-image generation errors into considerations. It would be perfect to have a GAN like loop between detectors and generators. \n\n- For illustration part, I would suggest the author to show some cases why ImaginaryNet does not works but WSOD works, so that possible new ideas could be risen from analysis. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The overall idea looks somewhat novel to me, although there are im2real based on Gaming engine before, but using image-to-text, to the best my knowledge, this is the first. \n\n- Although the paper just consists of several public available modules such as GPT-2, DALLE-MINI, WSOD, it still lacks of details or detailed studies on the language generation prefix. ",
            "summary_of_the_review": "This paper could be viewed as WSOD on image-to-text synthesis datasets. It needs to handle the domain differences between generated images and real images. The idea is overall novel, but the formulation is a bitter weak and the studies also not that comprehensive from my perspective. \n\nI think the idea deserves a top conference, but the paper needs thorough studies to before it could be published. \nI may change my rating based on the rebuttal. \n\n\n==============================\nThe authors give detailed response to all my concerns. I raise the rating from 5=>6. \nHowever, I would seriously suggest the authors make a revision to reformat the story of the work to move some important contents in appendix into the main body. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1723/Reviewer_va3v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1723/Reviewer_va3v"
        ]
    },
    {
        "id": "wRQ8UzmoKI8",
        "original": null,
        "number": 2,
        "cdate": 1666527180003,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666527180003,
        "tmdate": 1666527180003,
        "tddate": null,
        "forum": "9MbhFHqrti9",
        "replyto": "9MbhFHqrti9",
        "invitation": "ICLR.cc/2023/Conference/Paper1723/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors study the problem of training an object detector without any real images or annotations. For this end, they propose generating synthetic (realistic) images using a pre-trained text-to-image model conditioned on words for the object categories as interest, and following a weakly-supervised object detection pipeline to train a detector given the generated image and the class labeled that conditioned it. The authors evaluate their approach on PASCAL VOC (and partially on MS COCO) with a comparison to other pre-training based object detectors, weakly-supervised detectors and a supervised method.\n",
            "strength_and_weaknesses": "Strengths:\n+ Annotation is a labor-intensive task and mechanisms for reducing the cost of this process are very valuable.\n\n+ A novel paradigm is introduced for using pre-trained text-to-image models for training object detectors.\n\n+ Strong results over the baseline CLIP-based models.\n\n\nWeaknesses:\n\n1. My only concern with the paper is that it is lacking a comparison with other CLIP-based detection approaches introduced in Introduction.\n\n\nMinor comments:\n\n- \"Humans can easily detect a known concept from language description without the demand of training in reality.\" => I would be very careful with such claims. I kind of disagree with this as there must have been some kind of (pre)training for humans to recognize the concept as known, either the language model, the other objects or the parts of the object/concept etc.\n\n- First paragraph of the Intro: \"RPNs or RoI heads\" => Acronyms are used without introduction.\n- \"which is \u201dA photo of a\u201d\" => The opening quotation is incorrect. You can use `` in Latex to do this.\n- \"image encoder consisted of multi-layers\" => \"image encoder consisting of multi-layers\".\n- \"a RPN network\" => \"an RPN network\".\n- \"IMAGINARYNET apply RoI Pooling\" => \"IMAGINARYNET applies RoI Pooling\". Please cite the relevant paper here for RoI pooling.\n- \"IMAGINARYNET obtain\" => \"IMAGINARYNET obtains\".\n- \"IMAGINARYNET sample\" => \"IMAGINARYNET samples\". The same problem recurs through out the text. Please check the paper carefully for more.\n- \"OICR(Tang et al., 2017),\" => \"OICR (Tang et al., 2017),\".\n- \"and the structure of them are shown in Fig (2).\" => \"with the structure shown in Fig (2).\"\n- Eq 2, 3: dot should be placed after the eqs.\n- \"the (Tang et al., 2017),\" => \"Tang et al. (2017),\". You can use a different cite command to get Tang et al. (2017).\n- \"for reaching no real images and manual annotations\" => \"for obtaining no real images and manual annotations\".\n- \"the Edge Boxes algorithm\" => Provide citation.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is around %10 of the papers in terms of clarity, quality, novelty and reproducibility.",
            "summary_of_the_review": "Novel method, strong results. Missing important comparisons to other CLIP-based approaches.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1723/Reviewer_DNcv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1723/Reviewer_DNcv"
        ]
    },
    {
        "id": "Xc62lkSmir",
        "original": null,
        "number": 3,
        "cdate": 1666624233136,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624233136,
        "tmdate": 1666624233136,
        "tddate": null,
        "forum": "9MbhFHqrti9",
        "replyto": "9MbhFHqrti9",
        "invitation": "ICLR.cc/2023/Conference/Paper1723/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a scheme for training object detection networks using self-generated synthetic data.  The system presented herein does not require manually-annotated real images, which are hard to come by.  Rather, the system leverages the spectacular advances in text-to-image generation system to construct visually realistic data that serves the role of traditional object detection datasets.  The proposed scheme is able to benefit from the \"real\" data when available.  The paper also includes a set of experiments that confirm the useful of this approach.",
            "strength_and_weaknesses": "The approach presented herein is well-motivated; however, given prior works that also advocate the use of synthetic data for training deep learning model, the novelty-aspect of this work is somewhat low.  Section 2.2, for example, lists some of these works.  Reading this section it seems that existing schemes are not as robust (or useful) for model training simply because the quality, i.e., visual realism, of the generated images do not match \"real\" images.  This subsequently leads to a domain gap, which must be addressed by using manually-annotated real images.  If this is true then one may reasonable assume that with our increased ability to generated visual realistic images, some of the existing methods may work as well as the approach developed in this work.  I feel that the paper needs to do a better job of distinguishing itself from prior art.\n\nAlong the same lines, while the work shows that ImaginaryNet outperfroms CLIP, it is not immediately obvious to me why is it so.  Posing this question another way, do you think CLIP can achieve comparable performance if allowed to train on larger training data (generated data, of course). \n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the writing is good.  The paper does suffer from low novelty since others have explored similar ideas to train deep learning models.  The engineering apsects of the work are clear; however, I couldn't get an intuitive feel for why the paper outperforms a method such as CLIP.  I feel that the paper will benefit from including a summary of how this work is different from prior work and what aspects of the model leads to better performance.  Is it training?  Is it because the model has a lot more capacity than some other models? Is it because we now have models to generate highly realistic visual data?",
            "summary_of_the_review": "The work is interesting and it is moves the field of deep learning in the right direction.  The approach presented here reduces our reliance on manually annotated real data for the task of object detection, which is highly desireable.  At the same time it is somewhat difficult to ascertain what makes this model better than other approaches that also use generated data.  The novelty, I feel, is on the lower-end of the spectrum. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1723/Reviewer_n5tS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1723/Reviewer_n5tS"
        ]
    },
    {
        "id": "dN_8U2jiuED",
        "original": null,
        "number": 4,
        "cdate": 1666666169606,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666169606,
        "tmdate": 1670953630759,
        "tddate": null,
        "forum": "9MbhFHqrti9",
        "replyto": "9MbhFHqrti9",
        "invitation": "ICLR.cc/2023/Conference/Paper1723/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a framework, named ImaginaryNet, to train object detectors using images generated by a text-to-image model and class labels from a pre-trained language model.\nThis is also a learning scheme, named Imaginary-Supervised Object Detection, for training object detectors without requiring real images and manual annotations. This scheme is based on a weakly-supervised object detection (WSOD) algorithm, i.e. OICR, with the object proposal and class labels obtained from ImaginaryNet.\nThe paper's main contribution is in using the language model and synthesis model to build a learning scheme on top of the existing WSOD scheme without requiring real images or manual annotations.\n",
            "strength_and_weaknesses": "Strength:\n1. Using language and synthesis models is a clever idea to improve the existing WSOD algorithm. \n2. Extensive experimental results to demonstrate the strength of the proposed method\n\nWeakness:\n1. The effect of the limitation caused by synthesis models was not well studied and discussed although there is an ablation study on the effectiveness of the language model.\n2. The experiments and ablation studies sections are not well organized and hard to follow. There is some confusion in the settings, e.g. ImaginaryNet ISOD, WSOD, SSOD.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of the paper is acceptable. More clarity is needed in the experiments and ablation studies. The overall ideas of the work are original, however, the framework is developed by putting together existing models in a pipeline, e.g. language model via GPT-2, text-to-image synthesis model via DALLE-mini, and proposal generator via selective search.\nBased on what is described in the paper, the proposed framework should be reproducible.",
            "summary_of_the_review": "My initial rating is a weak reject. The main reason for rejection is that the paper did not put enough focus on the synthesis model. This makes the paper's claim of learning object detectors without real images weaker since there could be an object class that the synthesis model cannot generate without learning it first. In that case, we still need to have real images. For example, learning a \"human hand\" detector would require the synthesis model to generate an image with a \"human hand\".",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1723/Reviewer_w1NM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1723/Reviewer_w1NM"
        ]
    }
]