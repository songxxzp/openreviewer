[
    {
        "id": "uTHmRJai4d",
        "original": null,
        "number": 1,
        "cdate": 1666550333483,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666550333483,
        "tmdate": 1669122058955,
        "tddate": null,
        "forum": "wPLEzBcSC7p",
        "replyto": "wPLEzBcSC7p",
        "invitation": "ICLR.cc/2023/Conference/Paper4669/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method for continual learning (Cognitive Continual Learner, CCL) with three components: a \u2018working model\u2019 that learns to classify from raw input, an \u2018inductive bias learner\u2019 that learns to classify from a processed version of the input in the form of shape information, and a \u2019semantic learner\u2019, which is a stochastic moving average of the parameters of the working model. Apart from the classification losses on the first two modules, there are also few auxiliary \u2018knowledge sharing\u2019 objectives: a bidirectional MSE loss between the outputs of the working model and the inductive bias learner, enforcing them to make consistent predictions, and MSE losses between the output of the semantic learner and each of the working model and inductive bias learner, constraining each of them to not veer too far from historical predictions. The model also uses a replay buffer maintained with reservoir sampling. Experiments in both the task incremental and class incremental settings are run on Sequential CIFAR-10, Sequential CIFAR-100, GCIL-CIFAR100 (which blurs boundaries between tasks) and a custom dataset introduced in the paper called Domain2IL, which is a subset of the existing DomainNet dataset, curated to make the class distributions more balanced and uniformly distributed. CCL is compared to a number of replay-based baselines and is shown to be the best performing across all tasks. Some additional analyses are run to demonstrate the robustness of the method, how it fairs with the plasticity-stability tradeoff, and some ablations of the components of the model.",
            "strength_and_weaknesses": "- The method in the paper has a number of interesting features:\n    - (i) the knowledge sharing between multiple modules. Knowledge sharing via distillation between two modules (that usually evolve at different timescales) has been used in continual learning in a number of existing methods (e.g. Progress and Compress, Schwarz et al 2018), but CCL enforces consistency between 3 modules.\n    - (ii) enforcing consistency between two different views of the data (the raw input and the shape information)\n- The empirical results are strong, and are run on relevant benchmarks comparing against suitable baselines.\n\nOpportunities for improvement / questions\n- There are very important details missing about the implementation that make it difficult to fully understand the method, and certainly impossible to reproduce (given that no code is provided).\n    - (i) There seems to be no explanation of how the shape information is extracted from the data and represented to the inductive bias learner. This appears to be a key feature to the model. Indeed in 5.3 it is even stated that the primary benefit of CCL comes from the shape information.\n    - (ii) The stochastic momentum update does not seem to be described - what is the nature of the stochasticity in this update and how important is it to the performance of the model?\n- In the ablations it would be interesting to see what would break the model - all the ablations seem to fair quite well, for instance much better than the experience replay baseline, so it is difficult to understand which aspects of the model are the most important. Maybe some further ablations are needed to ascertain this, e.g. how would the working model perform by itself without either the SM or the IBL? Would it be correct to say that this should be similar to the performance of ER?\n- It is mentioned that some of the results in Table 1 are taken from the original papers but some are run again by the authors - but it is not clear which are rerun and which are not.\n- The plasticity and stability metrics used are not obviously reflective of the conventional meaning of the terms. Stability is computed as the average accuracy all tasks 1:T-1 after learning of the final task T, but this metric is highly dependent on the initial performance on tasks 1:T-1 - would it not be more informative to report a more standard metric like forgetting or backward transfer (Lopez-Paz et al. 2017)? Also, why do the plasticity and stability values in Figure 2 go above 100%?\n- What is the rationale for using an MSE loss for the Knowledge-sharing loss rather than say a Kullback-Leibler loss, when the outputs of the WM and IBL networks are probability vectors?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is decently written but lacking clarity with respect to the details of the implementation, which makes it impossible to reproduce, given that no code is provided.",
            "summary_of_the_review": "Update: I have increased my score to 6 after authors responded to my concerns.\n------\nThis paper presents an interesting method for continual learning, that features enforcing consistency between multiple modules, that is justified by strong experimental results on a number of CL visual classification benchmarks comparing to a number of relevant replay-based baselines. In its current form, the paper is lacking a number of important implementational details that render it entirely unreproducible (described in more detail above), which is the main reason I think it cannot be accepted as is. It is also difficult to tell from the ablations which components of model architecture and loss functions are most important to performance, as all the ablations do well - I think further ablations are needed in order to determine this (as descibed above), which would provide a lot more insight into why the model works well. I would be willing to increase my score if these two points were addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4669/Reviewer_6gTb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4669/Reviewer_6gTb"
        ]
    },
    {
        "id": "UuPdCnxk8q",
        "original": null,
        "number": 2,
        "cdate": 1666628569876,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628569876,
        "tmdate": 1669365984036,
        "tddate": null,
        "forum": "wPLEzBcSC7p",
        "replyto": "wPLEzBcSC7p",
        "invitation": "ICLR.cc/2023/Conference/Paper4669/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a new continual learning method inspired by human abilities to continually learn with no catastrophic forgetting. Based on aspects in cognitive architecture, 3 modules are combines in the proposed solution.  The authors suggest that the shape of objects is not well leveraged in current CNNs and use a specific module for learning from shapes. The third module acts as a memory module and is updated via momentum updates. Experiments on cifar10 and cifar100 show improvements over compared methods. Also a domain incremental sequence is proposed based on DomainNet benchmark.\n ",
            "strength_and_weaknesses": "+ New brain inspired solution to continual learning.\n+ Good results.\n- A part from the cognitive emulation, there doesn't seem good reasons behind the chosen design strategies.\n- 3 networks are used increasing the computational footprint of the proposed  solution. Computational aspect is not discussed. A  fair comparison with other methods would be by increasing the buffer size to be equal to the amount of storage due to the extra networks.\n- The method requires large number of hyperparameters.\n- Not sure why the new domain incremental sequence is introduced. Core50 domain incremental learning sequence could be used. \n- Only cifar based sequences are considered, what about ImageNet, TinyImageNet or Core50?\n- Not sure why the authors picked only 5 tasks sequence, usually evaluation is done with different number of classes per task in order to show the robustness of the method. \n- Not sure that the method is state of the art, there seem to be new published works that show better performance, e.g., PoDNet[1]\n- The use of the shape limits the applicability of the method on recognizing objects with discriminating characteristics other than shape, e.g., a horse and a zerbra.\n[1] Ashok, Arjun, K. J. Joseph, and Vineeth Balasubramanian. \"Class-Incremental Learning with Cross-Space Clustering and Controlled Transfer.\" ECCV 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear but no clear motivation behind the different used losses.  I am mostly worries about  the hyper-parameters. It seems they were picked based on overall performance. Usually, hyper-parameters are set based on held-out set, e.g., a subset of the sequence  or set for one sequence and fixed for others. Here there is a large number of hyper-parameters with wide range and no discussion on how they were set.",
            "summary_of_the_review": "A  new cognitive inspired solution to continual learning is proposed with multiple networks. Some aspects are missing and comparisons are limited. To this reviewer, the paper int its current shape is not robust enough.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "A solution to continual learning is proposed that doesn't seem to flag any ethical concern.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4669/Reviewer_BtBz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4669/Reviewer_BtBz"
        ]
    },
    {
        "id": "Yfgokpkf5WQ",
        "original": null,
        "number": 3,
        "cdate": 1666691337533,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691337533,
        "tmdate": 1668544334316,
        "tddate": null,
        "forum": "wPLEzBcSC7p",
        "replyto": "wPLEzBcSC7p",
        "invitation": "ICLR.cc/2023/Conference/Paper4669/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper draws inspiration from the cognitive science literature to design a multi-module architecture for continual learning from images. More specifically, the proposed \u201cCognitive Continual Learning\u201d consists of a working model (which processes images in the typical way), an inductive bias learner (which receives pre-processed shape information) and a semantic memory (which is a delayed version of the working model), whereby the predictions of the different models are encouraged to be similar to each other by various additional loss terms. In extensive empirical comparisons the authors show that the resulting model performs better than several established continual learning methods (e.g., DER++, Co2L and CLS-ER) on variants of task-, domain- and class-incremental learning.\n",
            "strength_and_weaknesses": "The writing and organization of the paper are generally clear, and I find the proposed multi-module architecture an interesting contribution. The Domain2-IL dataset protocol that is constructed could also be a useful contribution (although I\u2019d recommend the authors to reconsider the name).\n\nThe paper however has some important issues that require addressing.\n\nFirst, the fairness of the empirical comparisons. The main empirical component of the paper is comparisons against methods such as ER, DER++ or CLS-ER. However, an important confound in this comparison is that the proposed CCL method uses shape information while the other methods do not. That is, the proposed CCL method uses additional information (or you could see it as additional pre-processing or augmentations) that is not available to the other methods, making these comparisons unfair.\nRather than including these comparisons and trying to make a claim of \u201cstate-of-the-art\u201d, I would recommend the authors to instead focus on carefully and systematically characterizing the performance of their proposed methods and its different components (e.g., along the lines of the ablation experiments in Table 2, but more extensively).\n\nAnother important issue is the way this paper describes the \u201ccontinual learning settings\u201d, and its claim that the proposed CCL shows improvement \u201cacross all continual learning settings\u201d. By \u201call continual learning settings\u201d this paper seems to mean Task-IL, Domain-IL, Class-IL and General Class-IL. The distinction between Task-IL, Domain-IL and Class-IL seems based on this paper (https://arxiv.org/abs/1904.07734, although the paper is not discussed or cited, which it probably should be). With regards to these three scenarios I could agree with referring to them as \u201call settings\u201d (because these are the three ways in which a given sequence of tasks can be performed), but the addition of General Class-IL complicates things. General Class-IL is not just about the way a sequence of tasks is performed, but additionally has restrictions about how that sequence is constructed (e.g., gradual shifts between tasks). When also considering the way a task sequence is constructed, there can be many more settings than just the ones considered in this paper.\n\nIt is of course fine to consider a continual learning set up as described by General Class-IL, but it is important not to contrast it (or put it at the same level of abstraction) with Task-IL, Domain-IL and Class-IL. Rather, General Class-IL is a subset of Class-IL, and I think it is important to be clear about that.\n\nAs a final note, it might be good to reconsider the name of the proposed Domain2-IL protocol. Currently this name has the risk of suggesting that it is a modified version of the Domain-IL scenario, while it is an instantiation of the Domain-IL scenario (i.e., similar to for example permuted MNIST, but \u2013 as the authors rightfully point out \u2013 a more realistic one).\n",
            "clarity,_quality,_novelty_and_reproducibility": "I have some concerns for this paper regarding reproducibility, in particular with regards to the hyperparameter selection. It is stated in the paper that hyperparameter selection is performed, but it is not reported what values were explored and how they were explored. Moreover, for the proposed CCL method the selected hyperparameters are reported in the Appendix, but for the methods that are compared against no such hyperparameters are reported. In addition, it is not explained how the authors ensured that the hyperparameter selection procedure for the methods that are compared against was done in a way similar to their proposed method.\n\nCode is also not provided to the reviewers, and a proper check for reproducibility issues is therefore not possible.\n\nThe clarity of the paper is generally good. I think the paper also has a clear novel component in the proposed use of shape information in continual learning (although the empirical comparisons are not fairly performed as a result of that, see above) and the proposed architecture.\n",
            "summary_of_the_review": "Although I find the proposed multi-module architecture interesting, I believe the provided empirical comparisons are not fair. There is also an important issue regarding the paper\u2019s description of continual learning settings.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4669/Reviewer_e4pE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4669/Reviewer_e4pE"
        ]
    },
    {
        "id": "kBjPNSNAcs",
        "original": null,
        "number": 4,
        "cdate": 1667217183140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667217183140,
        "tmdate": 1667217183140,
        "tddate": null,
        "forum": "wPLEzBcSC7p",
        "replyto": "wPLEzBcSC7p",
        "invitation": "ICLR.cc/2023/Conference/Paper4669/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Cognitive Continual Learner (CCL) which is composed of three modules. An explicit module that learns from the input and two implicit modules (inductive biases and semantic memories) that share indirect contextual knowledge. CCL is evaluated under a number of continual learning settings, including on a novel benchmark, and compared with recent experience replay continual learning methods, demonstrating favourable performance.",
            "strength_and_weaknesses": "Strengths:\n+ The proposed system is simple combing several elements from recent work but outperforms all baselines.\n+ Extensive evaluations are conducted under numerous continual learning settings.\n\nWeakness:\n- CCL was compared with a number of recent baselines but the comparison only focused on classification performance disregarding complexity. How does CCL compare with these methods in terms of memory, complexity, etc?\n- Common continual learning metrics such as forward and backward knowledge transfer are not reported.\n- No comparison to non experience replay methods.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality. The paper is well-written, clear, and easy to follow.\nNovelty. While all constituent components are not novel, the proposed system in totality is novel.\nReproducibility. Most details are provided to reproduce the results.",
            "summary_of_the_review": "The proposed system is interesting combining a few successful elements from recent literature but empirical evaluations (metrics, baselines) require improvement.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4669/Reviewer_J9Rf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4669/Reviewer_J9Rf"
        ]
    }
]