[
    {
        "id": "nyz_rzG1IK",
        "original": null,
        "number": 1,
        "cdate": 1666634017726,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634017726,
        "tmdate": 1666634017726,
        "tddate": null,
        "forum": "vaxnu-Utr4l",
        "replyto": "vaxnu-Utr4l",
        "invitation": "ICLR.cc/2023/Conference/Paper5905/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents WikiWhy, a Question Answering dataset of \"Why\" questions constructed from Wikipedia text. WikiWhy contains questions requiring two steps of reasoning or based on a rationale set.",
            "strength_and_weaknesses": "## Strong\n\n* The Cause and Effect QA with text generation problem is interesting (WikiWhy tasks 2 and 3)\n* The automatic evaluation metric proposed is interesting.\n\n## Weak\n\n* (Minor) Table 1 has messy formatting. The Size and Topic columns should be right-aligned to support easy comparisons.\n* Some hyper-parameters are missing (see the Clarity, Quality, Novelty, and Reproducibility section below).\n* (Minor) The Conclusion states, \"With this paper, we release WIKIWHY, a Question Answering dataset interested in understanding and facilitating the improvement of LLMs\u2019 reasoning capability.\" The word \"interested\" is a strange choice for this sentence. I recommend rephrasing.",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is well-written and easy to follow. I particularly appreciate Figures 1 and 2, which give a good overview of how WikiWhy was constructed.\n* There are many figures and tables in the appendices to help the reader understand the details of the paper and support reproduction. That being said, I cannot find all the hyper-parameters used for the GPT-2 Task 2 experiments.",
            "summary_of_the_review": "An interesting QA dataset. The paper has some minor issues.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5905/Reviewer_V2He"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5905/Reviewer_V2He"
        ]
    },
    {
        "id": "ms0WC77Whx",
        "original": null,
        "number": 2,
        "cdate": 1666734346388,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666734346388,
        "tmdate": 1666734346388,
        "tddate": null,
        "forum": "vaxnu-Utr4l",
        "replyto": "vaxnu-Utr4l",
        "invitation": "ICLR.cc/2023/Conference/Paper5905/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new QA task on answering and explaining cause-and-effect questions. Different from previous approaches, the authors create the WIKIWHY dataset which contains the explanation within cause-effect relations. Taking the effect as the question source, the goal is to not only get the answer (cause) but also predict the explanations in sequential or set settings. Experiments show that the state of the art generative models are good at QA task but still a large space for improvement on the explanation task.\n\n",
            "strength_and_weaknesses": "## Strength:\n- It is the first work that investigates Q&A over cause-effect reasoning with explanations.\n- The authors create a consolidated pipeline to define the problem and build the dataset, resulting a WIKIWHY dataset covering 11 topics.\n-The authors propose new evaluation metrics for this cause-and-effect question answering task. Experiments verifies the reasoning ability of LLMs. \n\n## Weakness:\n- The problem formulation in which a sequence or a set of explanations are required make this task intrinsically difficult to evaluate.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This work is generally well-written and easy to follow.\n\nQuality: This work is technically sound.\n\nNovelty: The novelty is not on the approach side but it is novel in terms of the task proposal, the dataset and the evaluation protocol.\n\nReproducibility: This work can be reproduced using the shared source code.\n\n",
            "summary_of_the_review": "In general, this is an interesting work. Though there are some improvement space from the problem formulation and evaluation, this version of work is still good for the task proposal, dataset preparation and the experiment design.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5905/Reviewer_reEb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5905/Reviewer_reEb"
        ]
    },
    {
        "id": "0ydF5sg-LWb",
        "original": null,
        "number": 3,
        "cdate": 1666748259657,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666748259657,
        "tmdate": 1666748259657,
        "tddate": null,
        "forum": "vaxnu-Utr4l",
        "replyto": "vaxnu-Utr4l",
        "invitation": "ICLR.cc/2023/Conference/Paper5905/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Goal: The paper introduces a resource for explaining the cause-and-effect relationships on a wide variety of topics. \n\nMethod: The resource is crowdsourced in two steps. First, crowdworkers are asked to turn potential causal statements (identified via discourse cues) into Cause-effect questions. Then, the crowdworkers are asked to explain the cause effect relation to obtain \u201cwhy\u201d answers. \n\nUsing this process, a resource of about 9K why questions was curated from Wikipedia articles covering a wide variety of topics. \n\nBenchmarking: The paper presents benchmarking results that show the \n\nKey Contributions: The main contribution is in the resource itself. The resource presents a new test bed to assess abilities of NLP models to reason about cause and effect relations. The task requires a wide variety of knowledge and the benchmarking shows that the task is difficult for the current models and definitely warrants further research. \n",
            "strength_and_weaknesses": "Strengths:\n\nThe paper introduces a high quality resource that fills a gap regarding evaluation of LLM\u2019s ability to not only provide answers about causes and effects but also explain why using a wide variety of knowledge.\n\nThe paper is well-written and provides clear descriptions of how the data was collected, providing enough information to verify the quality of the resource.\n\nThe benchmarking experiments convincingly demonstrate the difficulty of the task. This resource could be a key driver for research into this problem space. \n",
            "clarity,_quality,_novelty_and_reproducibility": "As stated above, the paper presents the main ideas, the data collection procedure and the experimentation details with clarity. \n\nThe paper fills an important gap. Novelty or originality is not pertinent here with respect to the methodology.\n\nThere are adequate amount of details here and the main tasks are structured simply, so the data could be reproduced to some approximation with reasonable effort.",
            "summary_of_the_review": "The paper makes an interesting contribution to explainable reasoning for cause-effect relations, which can drive interesting research into this space.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5905/Reviewer_RHVP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5905/Reviewer_RHVP"
        ]
    },
    {
        "id": "pxVcdxJpSuC",
        "original": null,
        "number": 4,
        "cdate": 1667368313676,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667368313676,
        "tmdate": 1667368313676,
        "tddate": null,
        "forum": "vaxnu-Utr4l",
        "replyto": "vaxnu-Utr4l",
        "invitation": "ICLR.cc/2023/Conference/Paper5905/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "## Summary\n\nThis paper introduces a new dataset, WikiWhy, along with an explanation task, that aims to measure the reasoning and common sense abilities of generative language models. The work describes the motivation for this new dataset and task, mainly that most existing question-answering tasks require limited reasoning and common-sense knowledge. The paper describes how the new dataset was collected, based on Wikipedia with a couple of MTurk tasks: (1) generating question-answer pairs given a specially selected passage, and (2) generating a series of sentence-oriented rationales. The paper describes experiments using baseline models (based on GPT-2 with fine-tuning, and GPT-3 with prompting), demonstrating low performance on the proposed task. Finally, the paper proposes an automatic metric for evaluating system outputs on the new task, demonstrating correlation of the metric with human judgements.",
            "strength_and_weaknesses": "## Strengths\n\n1. The paper addresses an important weakness in many existing question-answering datasets that focus on answering questions with limited feedback on system reasoning or explanations. The proposed formulation of explicitly requiring systems to produce causal explanations to \u201cwhy\u201d questions is an interesting step,\n\n2. The proposed task formulation is intuitive and elegant. While the two types of explanations proposed (step sequence and sets) are likely not comprehensive, they probably cover a large proportion of explanations to simple \u2018why\u2019 questions,\n\n3. Collecting datasets is challenging, but can be helpful directing improvements to future systems. The release of this dataset to the community will likely help drive new work in QA explanations and reasoning.\n\n4. The baselines described in the paper are quite reasonable starting points for modeling,\n\n5. The human judgements of the baselines are surprisingly low which means that this task has large room for improvement in future work. (Note to authors: it would be nice if the baseline predictions were released along with your code+dataset so that they can be inspected by the community).\n\n## Weaknesses\n\n1. The main weakness in this work relates to the protocol used to elicit \u2018why\u2019 questions conditioned on a passage that contains a causal connective. Unfortunately, I believe this contains some of the same issues the community has observed with SQuAD task, namely: (1) generated questions are artificial since conditioned a specific passage, and rarely reflect naturally occurring \u2018why\u2019 questions, and (2) unusually high overlap in phrasing between the question and the answer passage, and (3) reliance of passage context for question to be fully interpretable. Fortunately, the paper addresses issue (3), but still has issues (1) and (2).\n\n   Looking over most questions in the dataset, they mostly contain the issues listed. Examples:\n\n   a) Question: \"Why did the member of The Cure, Lol Tolhurst, make minimal contributions to the album Disintegration?\"\n\n     Passage excerpt: \u201c... revealed that while Tolhurst had contributed to the song \"Homesick\", his contributions to the rest of the album were minimal due to his alcoholism\u201d\n\n\n    b) Question: \u201c\"Why is it unlikely that any sequels would be made for the Power Rangers film?\"\n\n      Passage excerpt: \u201cHowever, in May 2017, Forbes noted that due to the underwhelming performance of the film in most markets, it was unlikely any sequels would be made.\u201d\n\n    In these examples (and others I browsed) it seems that (1) questions are formulated slightly unnaturally, (2) very high word overlap between answer phrase and question, (3) questions are all answerable within a single sentence.\n\n    It is probably too late and costly to ask authors to revise their protocol for question generation. For future reference, one protocol that avoids the pitfalls of SQuAD is the TyDiQA dataset (https://arxiv.org/pdf/2003.05002.pdf).\n\n 2. Probably due to the issue above, the retrieval performance using BM25 is unusually high  (Section 5.1). For comparison, the \u201cDeep Passage Retrieval\u201d paper (https://arxiv.org/pdf/2004.04906.pdf) shows BM25 performance in the 40s for Natural Questions. Similarly, answer identification should be quite easy as well for a model fine-tuned on this data given the high word overlap between answer phrase and question.\n\n   So while this dataset will be useful to understand whether models are able to produce good reasoning/explanations for certain types of \u2018why\u2019 questions, it is, unfortunately, not a good dataset for QA retrieval and answer identification.\n\n3. There is a part of the human evaluation (Section 5.3), specifically the Win/Tie/Lose which seems problematic. In our previous experience, disclosing to raters which outputs are \u201creference\u201d and which are \u201csystem\u201d biases ratings measurably. It is typically needed to make this type of analysis blind by (1) randomly shuffling which output is reference vs. system when presenting them to users, and (2) never disclosing the source of the explanation (reference or system).\n\n## Other notes / suggested citations or references:\n\n1. A dataset that seems quite related is ELI5 (https://arxiv.org/abs/1907.09190 , https://facebookresearch.github.io/ELI5/) which also aims to measure model\u2019s ability to generate QA explanations.\n\n2. Another dataset is the TellMeWhy (https://arxiv.org/pdf/2106.06132.pdf) which asks models to produce simple, reasoning-based explanations for answers in the context of stories.\n\n3. One other dataset that I thought worth mentioning in the related work is e-SNLI (https://arxiv.org/pdf/1812.01193.pdf , https://github.com/OanaMariaCamburu/e-SNLI ). While not strictly a QA task, the explanations collected in e-SNLI are quite related to the ones presented in this work.\n\n4. Another system that should be referenced is WT5 (https://arxiv.org/pdf/2004.14546.pdf) which is a T5-based system for generating explanations/rationales. Some of the checkpoints are available at https://github.com/google-research/google-research/tree/master/wt5#released-model-checkpoints and could serve as a stronger baseline than GPT-2, if fine-tuned for your dataset/task.\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity:\nThe writing and presentation of the work are quite good. I had no issues understanding the basic ideas presented and experimental results. One issue is that there is some important content that has been pushed to the Appendix .\n\n### Quality:\nOverall, except for the protocol issue I outlined in question elicitation, I think the work is generally high quality,\n\n### Novelty:\nThis work is moderately novel: explanations and reasoning is not a new topic in QA (see the related work along with my suggested references). Nonetheless, the dataset resource, along with proposed task formulation should be a worthwhile addition to this area.\n\n### Reproducibility:\nAs far as I can tell, most of the resources for annotation and baselines experiments are made available. The one lacking resource is in Section 5.3 (Human Evaluation) which used student raters. The specific interface was not made available (like the MTurk one from Appendix A4), but perhaps it was just a spreadsheet?\n",
            "summary_of_the_review": "Overall I\u2019m leaning to recommend acceptance of this paper. It has become clear to the QA community that we should move beyond factoid seeking queries and \u201cexact match\u201d as an evaluation metric. This paper proposes an interesting task variant. Despite some of the flaws in the data collection protocol (see above), I think this resource will be of interest to the community. The task formulation may also be extended in the future for other types of rationales. Finally, the initial baseline experiments indicate a large gap for improvement.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Responsible research practice (e.g., human subjects, data release)"
            ],
            "details_of_ethics_concerns": "The work described in this paper involves the construction of datasets that involved both MTurk workers and student raters. While the paper contains an 'Ethics Statement' describing some of the aspects of the work, I am not knowledgeable to determine if the information is sufficient.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5905/Reviewer_tSWL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5905/Reviewer_tSWL"
        ]
    }
]