[
    {
        "id": "NYoVNrbo547",
        "original": null,
        "number": 1,
        "cdate": 1666636827750,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636827750,
        "tmdate": 1666636827750,
        "tddate": null,
        "forum": "iEE0MadUaZh",
        "replyto": "iEE0MadUaZh",
        "invitation": "ICLR.cc/2023/Conference/Paper6279/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an exploration method inspired by the importance of social learning in humans. The method \u2018Help Me Explore\u2019 combines individual epodes for practicing already known goals with active learning via \u2018social queries\u2019 for goals just beyond the agent\u2019s current abilities, as suggested by a social partner. Specifically, social queries are made when the learning progress on known goals is low. The primary task the paper focuses on is a 5 block rearrangement task.",
            "strength_and_weaknesses": "Strengths\n- The underlying intuition of querying a \u2018social partner\u2019 for goal suggestions just beyond the agent\u2019s capabilities makes sense, and learning when to query for that information with a mostly autotelic agent is an interesting problem. \n- The chosen task seems difficult and interesting, and the authors show ablations across various settings of their method to justify the mixture of social and autotelic learning.\n\nWeaknesses\n- It\u2019s not clear how well this method extends beyond the block stacking environment, as (from my understanding) the social partner has to be hard-coded with underlying knowledge of the task to create a zone of proximal development which is used to select the \u2018beyond\u2019 goals. This creates a bit of an unfair comparison with the baseline ACL methods, which propose goals without any underlying expert knowledge of the task / difficulty progression.\n- Not clear what authors mean by \u2018current ACL strategies are all limited to generate goals from distribution of effects agent has already experienced\u2019 \u2014 eg. methods like GoalGAN (Florensa et al., 2018) try to generate goals of intermediate difficulty, which may not be goals the agent has seen/achieved before.\n- Furthermore, in some tasks it may not be straightforward to create a graph showing progression of goal difficulties, as with block stacking \u2014 e.g. locomotion across changing/mixed terrain. Rather than directly giving expert knowledge, perhaps another way of showing the usefulness of social learning without necessarily using expert knowledge is to query a human teacher instead, or to simulate imperfect social partners by injecting some noise into the proposed \u2018beyond\u2019 goals?\n\nAdditional Feedback\n- Figure 1 could be made larger, currently the text is hard to read. Same with Figure 2.\n- Fourth line under 3.2 typo: goal-conditioned \u2018lagent\u2019",
            "clarity,_quality,_novelty_and_reproducibility": "The paper and overarching motivation is clear. Components of this work exist in prior work, but the underlying algorithm and block manipulation environment seem novel. Code is provided for reproducibility.",
            "summary_of_the_review": "I recommend a 5. While the idea is interesting, the paper would be strengthened with a wider set of experimental environments (where it would be harder to hard-code a social partner) and/or querying humans as social partners. Currently the comparison against ACL methods seem a little unfair, as those methods do not make use of expert knowledge of what the zone of proximal development would be, but HME does, with the hard-coded social partner.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6279/Reviewer_sim2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6279/Reviewer_sim2"
        ]
    },
    {
        "id": "gSIQggeEcy",
        "original": null,
        "number": 2,
        "cdate": 1667415443598,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667415443598,
        "tmdate": 1669298855254,
        "tddate": null,
        "forum": "iEE0MadUaZh",
        "replyto": "iEE0MadUaZh",
        "invitation": "ICLR.cc/2023/Conference/Paper6279/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces a goal-shaping method to overcome sparse exploration challenges. The key idea is to provide a goal-conditioned learning agent with both \u201cfrontier\u201d and \u201cbeyond\u201d goals; but only on demand, when unaided learning stagnates. Empirical demonstrations show this can achieve stacks of 5 blocks in a simulated robotics task. ",
            "strength_and_weaknesses": "Strengths\n- The problem is well-framed and motivated\n- The empirical validation is thorough and the results are backing up the proposed methods\n- The head-line result of stacking 5 blocks is compelling\n- The links to social learning are interesting and likely to inspire follow-up work\n\nWeaknesses\n- The specific \u201csocial partner\u201d is broadly named, but quite tailored to the specific stacking task: how general is the approach if there is a less obvious decomposition into sequential stepping stones (e.g. how could this system teach an agent how to play StarCraft?)\n- I think the paper should discuss its limitations more explicitly: e.g. how important is the full observability assumption made in Sec 3.1? \n- There is a lack of clarity on some important aspects, e.g. around episode boundaries, termination, whether reward is accumulated after a goal is achieved, whether each episode is reserved for a single goal, etc.\n- The reliable success on 5-block stacking is great: why not try 6 blocks to see where/whether the method breaks?\n- The hand-crafted evaluation set is a bit disappointing, would there not be a way to define criteria for \u201cinteresting, non-overlapping, complex goals\u201d and have the set emerge from data? That would boost generality of the methodology?\n- The on-demand SP querying section seems very ad-hoc: why does Eq (1) contain an exponential if its argument never strays far from 0 (i.e it is essentially linear)? How does the LP difference take into account the fact that the early and later goal sets may differ? How important/justified is the indirection to compress success rates into a neural network instead of just using counts to determine LP? (and how much does its generalization rely on the goal representation?) Also, how do you prevent biased predictions in that network, caused by under-training on rare goals?\n\nSmaller concerns\n- It might be nice to tie back the conclusion to the first sentence of the intro: what type of open-ended learning is feasible now that wasn\u2019t before?\n- Don\u2019t introduce the identical mathcal{C} and mathcal{G} under two notations, pick one and clarify that paragraph\n- The last paragraph of Sec 3.1 is both vague (exploration, discovery, mastery are all undefined) and incorrect: you don\u2019t seek the \u201cmaximum number of goals\u201d, but rather mastery of a specific set of hard-to-reach ones.\n- Fig 1: how do discovered goals inform the social partner (top arrow)?\n- End of Sec 1: mention this is a simulated robotics environment, not an actual robotic arm\n- The references are quite heavily relying on a particular Bordeaux research group, you might want to re-balance those a little.\n- Sec 3.1: I\u2019d recommend using indicator function notation for the reward, instead of the pythonic \u201c==\u201d.\n- Is the query probability re-assessed \u201ceach new episode\u201d as stated in Sec 3.3, or held constant actress chunks of 6000 episodes?\n- Sec 4.1: how do you sample 20 goals from the C1 set that just contains 10 (Table 1)?\n- VDS: how do the 3 nets preserve diversity? What\u2019s the softmax temperature? Why once do max-over-50 and once softmax-over-1000?\n- Sec 4.2 \u201cindeed [not] good stepping stones\u201d: can you show this?\n- Can you add a plot of P_query over the course of training?\n- Fig 3 could benefit from log-scale, and definitely is confusing to have one out of 8 subplot use a different y-scale to the rest\u2026\n- Table 2 would probably be clearer as an xy-plot?\n- Table 2: what is \u201cw/o intern\u201d? The results you would have gotten without the help of an intern :-)\n- Please use 7% instead of 6.98% \u2013 given the large error-bars, they are equally correct, but one of them is readable too.\n- Sec 5: 0.5% is 1250 episodes, not 12500?\n- There are multiple duplicate citations\n- Appendix A1: either refer to what it adds to the paper from the main text, or omit the section\n- Table 4: what are \u201cshortest and safest paths\u201d? Is is a table that belongs into another paper?\n\nTypos\n- \u201clagents\u201d (twice)\n- numerous missing commas, namely in front of \u201cwhich\u201d clauses\n- \u201cleftest\u201d -> \u201cleft-most\u201d\n- discoverS a wider\n- Vygotsky\u2019S concept\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, the message is clear. The figures are decent, and convey a lot of information. The novelty is decent, the specific form of the shaped goal curriculum is new. The reproducibility is poor: the appendix is clearly unfinished, many hyper-parameters are missing, a large part of the agent design is not discussed, the authors don\u2019t discuss compute hardware, or how they tuned hyper-parameters (and which ones).",
            "summary_of_the_review": "This is a well-written paper with an interesting new idea, backed up by solid empirical evidence. It has a number of major flaws, and lots of minor ones, however, and I really hope the post-rebuttal version will address them, so that I can advocate for its acceptance with good conscience.\n\n--- update post rebuttal ---\nUnfortunately, the authors did not address my questions in any satisfying way, nor update the submission.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6279/Reviewer_qrSi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6279/Reviewer_qrSi"
        ]
    },
    {
        "id": "pzOx3ZNgbUd",
        "original": null,
        "number": 3,
        "cdate": 1667456666976,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667456666976,
        "tmdate": 1667456666976,
        "tddate": null,
        "forum": "iEE0MadUaZh",
        "replyto": "iEE0MadUaZh",
        "invitation": "ICLR.cc/2023/Conference/Paper6279/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to learn a goal conditioned policy based on goals generated from a pre-defined goals and the agent's own goals. \n",
            "strength_and_weaknesses": "Weaknesses:\n1. The paper starts with grandiose claims of tackling \"open-ended learning\". However, open-ended learning involves learning to perform across diverse environments. But the definition of open-ended learning in this work seems restricted only to learning different skills in a given environment. In experiments, it is mostly restricted to goal conditioned environments and learning a goal conditioned policy.\n2. \"But where do goals come from? Almost always, they are sampled from a fixed distribution over a predefined goal space; i.e. they come from an engineer.\" There are numerous works where the goals are NOT generated from a fixed distribution (listed in the references below)\n3. The previous statement makes us believe that in this work, the goals are not generated from a fixed distribution. However, a few paragraphs later, the authors note that \"In this second challenge \u2014 the one we focus on \u2014 agents must learn to organize their own learning trajectories by prioritizing goals with the objective of maximizing long-term skill mastery.\" i.e, this work focuses on learning a goal conditioned policy from pre-defined goals. \n4. \"In social episodes, a social partner suggests a novel goal to the agent and decomposes it into two consecutive sub-goals: 1) a frontier goal that the agent already discovered and, if it is reached, 2) a beyond goal never achieved by the agent but just beyond the its current abilities.\" The social agent keeps a list of all the goals discovered so far and a list of all the goals to be reached. This is not tractable in most environments.\n5. One of the contributions listed is: \"an active learning mechanism allowing the agent to self-monitor its learning progress and, when it stagnates, query the social partner for a goal suggestion\". This seems like a standard active learning setting and not a novel contribution. \n\nReferences:\n[1] Learning with AMIGo: Adversarially Motivated Intrinsic Goals. Campero et al, 2020\n[2] Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. Sukhbaatar et al, 2017\n[3] Asymmetric self-play for automatic goal discovery in robotic manipulation. OpenAI et al, 2021\n[4] An automatic curriculum for learning goal-reaching tasks. Zhang et al, 2021\n[5] Automatic curriculum learning through value disagreement. Zhang et al, 2020\n[6] Exploration via hindsight goal generation. Ren et al, 2019\n[7]  Automatic goal generation for reinforcement learning agents. Florensa et al, 2018\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and novelty are questionable. ",
            "summary_of_the_review": "Based on the weaknesses listed, I recommend to reject this paper",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6279/Reviewer_JZLF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6279/Reviewer_JZLF"
        ]
    },
    {
        "id": "-b1RvT8J9u",
        "original": null,
        "number": 4,
        "cdate": 1667493854746,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667493854746,
        "tmdate": 1667493854746,
        "tddate": null,
        "forum": "iEE0MadUaZh",
        "replyto": "iEE0MadUaZh",
        "invitation": "ICLR.cc/2023/Conference/Paper6279/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper argues that artificial agents may benefit from socially-guided skill discovery. This paper presents a protocol for active learning that enables an agent to query a partner for a goal. The partner is an auxiliary agent that maintains a model of the probability that an agent could reach goals in the goal-space. The paper claims that this agent outperforms existing baselines, including curriculum-based agents.",
            "strength_and_weaknesses": "Strengths:\n\n1. This paper takes care to embed its understanding social agents within the broader interdisciplinary literature.\n\nConcerns:\n\n1. **In figure 2a, the success rate of multiple agents is depicted across episodes. There are a few aspects of this plot that I find concerning**\n\ni. I am concerned that the experiment was run over hours rather than a standardised number of time-steps or episodes. By limiting the learning by wall-time, both the algorithm and the implementation of an agent are being evaluated. This means that code-level implementation choices that are independent of the learning method being evaluated will influence the reported performance. This is not a fair comparison.\n\nii. I am concerned that the results are reported over 5 seeds. Reporting the standard deviation over results for so few seeds is misleading. Moreover, given the results are overlapping in many locations, it's difficult for me to conclude that HME outperforms the comparators.\n\niii. I am struggling to understand the star notation in figure 2a. In this case, I'm not sure what the \"left-most\" algorithm is. I am assuming it is HME-50. It's unusual to report statistical differences across time-series data as presented. It would be better to report over summary statistics (error after learning). I have doubts that with 5 seeds that the data meets the requirements for a T-test (e.g., normality). The paper should provide information that demonstrates that this data meets the assumptions for the tests that have been done. \n\nyou're more likely to find a statistically significant difference when you're making many comparisons. To account for this, you should use a Bonferroni correction (or similar). In that case, it would require dividing by the number of comparisons made. As a result alpha will be less than 0.05, I believe. This would mean that the evaluation episodes for which there is a statistically significant difference would decrease. \n\nReporting the actual P values would be helpful in this case.\n\niv. At this point, I might wonder why the confidence intervals were not plotted, given the comparisons made. If you have the confidence intervals, why would standard deviation be reported for the error bars?\n\nv. It is claimed that \"non-social baselines reach a plateau of around 0.75\". Examining the figure, it seems that all but go-exp ss achieve scores higher than 0.75, and that the average final performance of most are somewhere around 0.8-0.9. This looks like a misrepresentation of the data. \n\nvi. It looks like HME-50 and HME-50b perform equitably in terms of success rate. This suggests that there is little difference in final performance between suggesting goals that are exclusively beyond the agent's perceived skill level, and suggesting goals that are at the frontier of the agent's skill.\n\n2. **some aspects of the evaluation strategy are unclear**\n\nThe agent is evaluated on \"interesting evaluation goals that humans could easily describe\". What makes a goal interesting or easy to describe isn't elaborated on, making it challenging to assess whether this is a reasonable evaluation strategy.\n\n3. **Figure 2b could be interpreted in several ways**\n\nI am struggling to understand what is being reporting in figure 2b. To my understanding, It counts the number of positions that are reached by the agent that are---according to the SP agent---adjacent to unknown configurations (configurations that have yet to be achieved?). In this case, HME-50 B reaches more stepping stone configurations across most episodes with lower variance(?) than HME-50. If I understand this correctly, it means that the agent is reaching more positions that are perceived by the SP to be at the frontier of the underlying agent's ability. \n\nit is concluded that *\"this suggests that the role of the frontier goal is to enable the agent to reach the\nbeyond goal from the first time.\"* If I understand this claim correctly, we cannot conclude this from the data presented. All we can say is that the agent that is proposing goals beyond its skill level is able to reach the frontier more regularly. \n\nA plot that depicts the number of new goals an agent reaches, or the rate at which the frontier expands, would be better able to support the claims being made in the paper, to my understanding.\n\ni. Figure 3 is mentioned twice in the text, from what I can tell. There is no Y axis labeled, and there is no description in the caption: I cannot tell what is being plotted here without hunting through the text. In the text an average success metric, and a social request metric are defined. I don't know what it means for the agents to have a score of 600 in one of the categories. I do not know what the error bars are. \n\nLabelling is absent from figure 5 as well.\n\nii. The axes are different for each sub-plot, making them difficult to visually compare.\n\niii.  \"As shown in the top row of Figure 3, ACL agents do not even discover the hardest configurations.\" In the top row of figure 3, I see HME-50 included. It would be helpful if you direct the reader's attention to specific sub-figures.\n\n4. **It is unclear why 50 was the beta parameter chosen for the baseline comparisons.**\nExamining figure 5  seems to suggest that HME-20 has the best performance (although there is higher variance).\n\n5. (stylistic suggestion) Figure 4 is challenging to interpret. In this figure, the success rate for all beta values chosen is presented. In this case, I cannot readily see the difference between the beta values---especially for the less complex stacking classes---because all of the lines overlap. Grouping the lines by category and presenting all the beta values on a single plot would help with the interpretability. Right now, it is challenging for me to see the difference between the success rates. For instance, one plot with HME 20, 50, 100, 200 plotted for the success rate of S4.\n\n6. **Frequently the claims of the paper extend beyond the truth.**\n\nIn a couple of locations *very* strong claims are made.\n\n* *agents must learn to organize their own learning trajectories by prioritizing goals\nwith the objective of maximizing long-term skill mastery*\n\n*Must* is a very strong word. Agents may benefit from prioritising goals, but it's not a necessity. Certainly many sparse reward problems have been solved by other means. \n\n* *[e-greedy action selection] is not enough to experience informative rewards in hard exploration problems*\n\nIf an agent is following a random behaviour policy, it can experience reward in sparse environments (as defined in the paper). It might be highly improbable, but it is certainly not impossible. \n\nMinor points:\n\n1. Terms are used without being defined (e.g., SP is used several times before being defined in 3.2)\n2. Some of the citations feel a bit strange. For instance, Mnih et al. 2015 is used as a citation for epsilon-greedy action selection.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Issues in clarity discussed above:\n\n* minor typographical issues\n* claims extend beyond evidence\n* ordering of ideas makes it challenging to comprehend\n* axes on plots are left unlabeled, making it difficult to interpret empirical results\n* terms are used before being introduced",
            "summary_of_the_review": "* In several places, the claims of the paper extend beyond the evidence\n* plots are not clearly explained, making it a challenge to interpret the empirical claims\n\nFor these reasons, I suggest a strong reject.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6279/Reviewer_LbgX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6279/Reviewer_LbgX"
        ]
    }
]