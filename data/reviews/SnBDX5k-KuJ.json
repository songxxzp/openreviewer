[
    {
        "id": "fL_MPwJ7KE2",
        "original": null,
        "number": 1,
        "cdate": 1666513351559,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513351559,
        "tmdate": 1666513617190,
        "tddate": null,
        "forum": "SnBDX5k-KuJ",
        "replyto": "SnBDX5k-KuJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5770/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper decomposes the CIL problem into two prediction steps, which are within-task prediction and task-id prediction. Several strategies are designed to tackle these sub-problems, e.g. HAT, PCA, single head classifier fine-tuning. Experimental results show that the proposed EWT works better than existing baselines on pre-trained transformer network.",
            "strength_and_weaknesses": "Strength:\n1. This paper proposed a new perspective to decompose the CIL probability into WTH and TIP probability.\n2. For the above two sub-problems, the paper proposes a simple yet effective solution based on HAT, PCA, and features replay.\n3. The proposed EWT outperforms the existing methods without exemplar replay.\n\nWeaknesses:\n1. The pre-trained model used in the experiments still too strong for CIFAR and Tiny-/Sub-ImageNet.\n2. The proposed methods could be highly dependent on the good feature representation obtained by pre-trained model. If not,  some experiments under normal settings are needed to prove it.\n3. There is no quantitatively comparison of additional storage between EWT and other replay-based methods.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the decomposition of CIL proposed in this paper is good, and the authors designed some significant strategies to solve it. But these  strategies are based on existing methods and may rely heavily on pre-trained models. The novelty is not enough.",
            "summary_of_the_review": "Based on the above comments, I tend to reject this paper. The main reason is that the experimental settings are insufficient to prove the effectiveness of the method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5770/Reviewer_D1ii"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5770/Reviewer_D1ii"
        ]
    },
    {
        "id": "O2d4Dz54V_",
        "original": null,
        "number": 2,
        "cdate": 1666674168416,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674168416,
        "tmdate": 1666674168416,
        "tddate": null,
        "forum": "SnBDX5k-KuJ",
        "replyto": "SnBDX5k-KuJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5770/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers class incremental continual learning, in which disjoint subsets of classes are introduced one at a time. A multi-head architecture is used which shares a pre-trained feature extractor (backbone). Forgetting is addressed by using HAT (Serra et al., 2018) which applies a separate binary mask to the backbone for each task (subset of classes).\nThe authors decompose the prediction into task-id prediction (TIP) probability and within-task prediction (WTP) probability. Both are estimated by making use of class-specific approximations of the features distribution.\nThe resulting method is shown to outperform competitive baselines and the design decisions are motivated with ablation experiments.\n",
            "strength_and_weaknesses": "I think that the method is comprised of insights which could be of interest to the community. Moreover, the experiment compare the method many baselines and conduct ablation studies.\nOne potential weakness is that the approach relies on a pre-trained backbone. Another issue is that the \u201cRelated Work\u201d section does not describe and compare to the most closely related work, but rather just cites it.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The problem setting is clearly defined and the method is well explained.\nIt is hard to judge the novelty of the method because, while related work is referenced, it is not described, thus, the difference from similar past work is not clear.",
            "summary_of_the_review": "The paper present  interesting ideas and satisfactory experiments. Conversely, I am uncertain about the novelty of the method.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5770/Reviewer_cHhV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5770/Reviewer_cHhV"
        ]
    },
    {
        "id": "IC6oeTCMGF",
        "original": null,
        "number": 3,
        "cdate": 1666689714313,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689714313,
        "tmdate": 1666689714313,
        "tddate": null,
        "forum": "SnBDX5k-KuJ",
        "replyto": "SnBDX5k-KuJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5770/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses continual learning where task IDs are not available.\nThe authors propose a learning method preventing catastrophic forgetting and providing task ID estimation capability.\nThe proposed method utilizes batches of features for each task generated from the generators obtained with incremental PCA. At this time, the OOD generator learned from past tasks is also used. The method estimates the ID for the current sample as that of the closest task in terms of the Mahalanobis distance from the mean and variance of the distribution of features of each task.\nExperimental results on multiple public datasets demonstrate that the proposed method performs better than existing methods.",
            "strength_and_weaknesses": "*Strength\n- Experimental results of the proposed method showed promising performance.\n\n*Weaknesses\n- The important assumption of why the class labels of tasks are disjoint is not well justified.\n- The problem setting is unclear. It is described in about four lines in the Definition paragraph but without a specific flow. It is difficult to understand what they actually do. For example, I cannot see when task IDs are estimated, when tasks are switched, tasks come in altogether; they come once, they don't come again, or they come repeatedly. Will all data be discarded each time, or will it be discarded for each task?\n- The motivation of Eq.2 is unclear. Why do the authors introduce pseudo feature vectors Z? The description of the method is also unclear. How can we generate y for Z? Where does B come from?",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity\n- The subscripts for w_k,j,l are not defined.\n- The motivation for the regularization in Eq.7 is unclear.\n- Why do they use the OOD samples in the computation of WTP?\n- It is better to show the results after each task as well as the average performance over them. The authors only provide average classification accuracy after the final task.\n\n*Quality\n- Please see the above comments.\n\n*Novelty\n- The proposed method seems to be novel.\n\n*Reproducibility\n- Code is available.",
            "summary_of_the_review": "The clarity is low. The motivation and details of the proposed method are unclear.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5770/Reviewer_vcxy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5770/Reviewer_vcxy"
        ]
    },
    {
        "id": "IXayuZ8SDj",
        "original": null,
        "number": 4,
        "cdate": 1666716772922,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666716772922,
        "tmdate": 1666716772922,
        "tddate": null,
        "forum": "SnBDX5k-KuJ",
        "replyto": "SnBDX5k-KuJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5770/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed an approach for class incremental learning by leveraging the conditional of the labels given the data (covariates), which they decompose in terms of the labels given the task (consistent with task incremental learning) and the task given the data (task predictor), this without the need of a replay buffer or pseudo-rehearsal. The task prediction is realized incrementally via independent principal component analysis and quantified via Mahalanobis distance.",
            "strength_and_weaknesses": "The authors propose a relatively simple approach to address CIL tasks via estimating features for each task using iPCA and hard attention masking (i.e., the HAT core) to prevent catastrophic forgetting. The approach simple and intuitive, computationally efficient and leverages the hard attention mechanism in HAT as a means to mitigate catastrophic forgetting.\n\nAs a drawback, the authors seem to omit the existing literature in expansion-based approaches for continual learning. Further, expansion-based approaches have demonstrated excellent performance (at the time, so of which, do not consider transformer architectures) in terms of catastrophic forgetting at the expense of moderate parameter growth. For instance, the authors do not mention Mehta et al. 2021, a nonparametric network-expansion approach that also uses a Gaussian distribution and a likelihood similarity (with a Mahalanobis distance calculation at its core) to predict task labels. The latter, though probably will not be able to compete in terms of performance, underscores that the idea of using a summary in representation learning space and a similarity function as a means to identify tasks is not new, thus reducing the novelty of the proposed approach.\n\nThe experimental results are extensive and convincing, specially the fact that all baselines use the same architecture and that the transformer was trained to exclude classes similar to CIFAR and Tiny-ImageNet. The details of the experiments are clear and the ablation studies in Table 2 and 3 help understand the contribution of each component of the proposed approach.\n\nAs a minor note, Figure 2 is hard to see, so changing the visualization or simply replacing it by a Table will improve the presentation.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed approach, which is conceptually simple, is well justified, described and motivated. The paper is well written and experiments are comprehensive. Novelty is limited by similar approaches using distances over summaries in representation space. The authors facilitate reproducibility by providing a detailed account of the model, optimization, experimental settings and source code.",
            "summary_of_the_review": "The authors propose a conceptually simple approach for CIL based on incrementally estimated summaries over previous tasks via iPCA and the hard attention masking of HAT. The resulting approach is computationally efficient, leverages a pre-trained transformer encoder and delivers excellent performance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5770/Reviewer_JguF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5770/Reviewer_JguF"
        ]
    }
]