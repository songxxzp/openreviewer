[
    {
        "id": "5VrQ5OGYITl",
        "original": null,
        "number": 1,
        "cdate": 1665956145713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665956145713,
        "tmdate": 1669419173182,
        "tddate": null,
        "forum": "5ktFNz_pJLK",
        "replyto": "5ktFNz_pJLK",
        "invitation": "ICLR.cc/2023/Conference/Paper1982/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Summary.\n\nThis paper is dedicated to developing efficient Shapley value approximation for vision transformers. The authors first leverage an attention masking approach to evaluate ViTs with partial information, and then they propose a procedure for generating Shapley value explanations via a separate, learned explainer model. Extensive experiments with diverse comparisons are conducted.",
            "strength_and_weaknesses": "Pros.\n\n1. Confidence intervals of experimental results are provided.\n\n2. Multiple metrics and baseline approaches are considered.\n\n3. The paper is well-written and easy to follow.\n\nCons.\n\n1. It is very hard to say whether model-agnostic can perform better. More discussions between model-dependent and model-agnostic approaches are needed. In my opinion, involving more ViT-specific information can produce a reliable result.\n\n2. The authors first consider the ViT evaluation with partial information and then leverage a separate, learned explainer model to generate the Shapley value. It seems these two parts are loosely connected.\n\n3. Only one natural image dataset and one medical image dataset are used for experiments. More datasets are needed to support the effectiveness of the paper's proposals.\n\n4. Related works are outdated. More references from 2021 and 2022 should be included.\n\n5. More ViT backbones should be considered.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good.\n\nQuality and Novelty are fair.\n\nReproducibility is unknown.",
            "summary_of_the_review": "Good idea, but not more discussions and experimental analyses",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1982/Reviewer_trn3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1982/Reviewer_trn3"
        ]
    },
    {
        "id": "EsfXLtPc1I0",
        "original": null,
        "number": 2,
        "cdate": 1666516780092,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666516780092,
        "tmdate": 1666516780092,
        "tddate": null,
        "forum": "5ktFNz_pJLK",
        "replyto": "5ktFNz_pJLK",
        "invitation": "ICLR.cc/2023/Conference/Paper1982/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a novel approach to compute Shapley value explanations for vision transformers. To evaluate shapley values for any model we need model's predictions on masked feature inputs. Authors propose a novel way to approximate these by training a ViT model using a loss function designed specifically for Shapley values. The method is evaluated using explainability metrics that do not require ground truth information. ",
            "strength_and_weaknesses": "Overall the approach of leveraging ideas from transformer training to compute shapley is interesting and building on the work of Jethani et al. is novel. Authors also present analytical error bounds on shapley approximation which can be useful. The experimental results are convincing. \n\nOne possible explanation evaluation approach to consider is the following, where explanations are compared with ground truth: \nhttps://arxiv.org/pdf/2104.14403.pdf\n\nIt would be good to discuss how ideas presented in this work could be utilised/extended for text data. ",
            "clarity,_quality,_novelty_and_reproducibility": "Well written and clear. \nSee above. ",
            "summary_of_the_review": "See above. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1982/Reviewer_qY28"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1982/Reviewer_qY28"
        ]
    },
    {
        "id": "Bkw1Njop8um",
        "original": null,
        "number": 3,
        "cdate": 1666604977619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604977619,
        "tmdate": 1666604977619,
        "tddate": null,
        "forum": "5ktFNz_pJLK",
        "replyto": "5ktFNz_pJLK",
        "invitation": "ICLR.cc/2023/Conference/Paper1982/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new way to estimate Shapely values for Vision Transformers. It is required to evaluate the model on partial inputs for estimating Shapely values. For CNNs, this is difficult (e.g. masking areas with gray produces out-of-distribution samples). However, the inputs of vision transformer can be more easily removed, as the attention itself can be masked. \n\n\n\nAs the estimation of Shapely values is notoriously difficult, the paper proposes to learn them. For this, an existing loss function is used from Jethani et al. (2021). Furthermore, the authors show this loss function to bound the Shapely value estimation error.\n\n\n\nThe evaluation is done against several baselines on the ImageNette and MURA datasets.\n\n## ",
            "strength_and_weaknesses": "**Strengths:**\n\n- This work provides new theoretical insight into how the optimized objective bounds Shapley values.\n- The evaluation is done against a large selection of baselines\n- The method outperforms the baselines consistently\n\n**Weakness**:\n\n- The proposed approach could have also been tested on NLP transformer models.\n- Training an additional network adds complexity.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The work is written clearly. \n\n**Quality:** The quality is good.\n\n**Reproducibility:** Their approach is well described, including the relevant hyperparameters. However, the authors did not comment whether they plan to release their source code. \n\n**Novelty:** The work is novel. On the more applied side, using the ViT to mask the input partially is novel, and on the theoretical side, Theorem 1 is a novel contribution.\n\n**Minor issues:** All citations are green. When printed in back and white, they are barely readable. Please use the default style.",
            "summary_of_the_review": "This is a good paper. It has both interesting applied contributions (using ViT for masking) and theoretical contributions. Also, the presentation is well done. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1982/Reviewer_eEy2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1982/Reviewer_eEy2"
        ]
    },
    {
        "id": "xD1qk3g_ToY",
        "original": null,
        "number": 4,
        "cdate": 1666606242332,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606242332,
        "tmdate": 1669898421359,
        "tddate": null,
        "forum": "5ktFNz_pJLK",
        "replyto": "5ktFNz_pJLK",
        "invitation": "ICLR.cc/2023/Conference/Paper1982/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper builds on the FastSHAP approach (Jethani et al., 2021) and applies it to the ViT attention-based architecture used in image analysis. The goal of FastSHAP is to learn a dedicated predictor to estimate the Shapley values expressing input feature attribution for explainability purposes. Another contribution is a mathematical justification of the FastSHAP loss used to estimate the Shapley value predictor. The approach is evaluated on two small-size datasets using insertion, deletion and faithfulness metrics.\n",
            "strength_and_weaknesses": "== Strengths ==\n\n\n- Well funded mathematical result, at least seems to be (I didn\u2019t check the proofs in the appendix).\n\n- Reasonable number of experiments to justify the approach, although on small size image databases (ImageNette & MURA).\n\n\n== Weaknesses ==\n\n- Limited novelty: rather straightforward application of FastSHAP to a single attention-based architecture, ViT (see the \u201cnovelty\u201d section).\n\n- No explicit exploitation of attention information as done in  (Chefer et al., 2021), for instance. Attention is only exploited for the optimization of the Shapley value estimator.\n\n- Small size of the datasets used for evaluation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity & Quality\n\nWell written and easy to follow, with a detailed description of the approach.\n\nThe writing should make more obvious what is different from the FastSHAP approach, in what respect the fact that an attention-based architecture is studied introduces modification from the original work. Is it just that we can express \u201cattribute\u201d and mask function at the token level instead of the pixel level for images?\n\n\n- Novelty\n\nIt seems to me that the proposed approximation scheme is not limited to a ViT architecture, or even an attention-based architecture. It seems feasible because the input data is represented by a small number of non-overlapping patches, reducing the size of the underlying combinatorics.\n\n- Reproducibility\n \nA link to the code for ViT Shapley has been removed from the submission.\n\n- Miscellaneous questions and comments\n\nYour statement that Attention-based explanations \u201chave not been shown to perform well in vision tasks\u201d should be better motivated.  A better scientific question would have been to state whether attention weights, which introduce soft selection in the decision process,  contain enough information to generate explanation expressed as feature attribution, or not.\n\nIt seems to me, as I understand it, that when using a ViT architecture  the number of removable features is the number of patches/tokens ($= d = 196$), which is much smaller than the number of image pixels. Won\u2019t it be possible to compute the exact Shapley values on a few samples for verification of the predictor $\\Phi_{ViT}$?\n\nSInce SHAP value prediction results from learning in your approach, what is the impact of sampling on the bound of theorem 1? In other words, the paper provides a bound justifying optimization: can it be extended or modified to account for generalization?\n\nI didn\u2019t understand if the baselines contain the full approach of (Chefer et al., 2021): is it what is called LRP? In fact, I found it quite difficult to compare the performances between the two approaches given that the benchmarks used are different.\n\nThe practical use of the approach for real applications seems limited: learning the Shapley value predictor requires a lot of computation. By comparison, (Chefer et al., 2021) seems to me much simpler to implement and requires less computation (no learning, and no extra network at inference time, only gradients and coordinate wise multiplication and sum). Can you elaborate on that?\n",
            "summary_of_the_review": "The main contribution of the paper is to apply the FastSHAP approach (Jethani et al., 2021) to ViT architecture, an attention-based model, and to propose a mathematical justification that the loss used to estimate the Shapley value predictor.\n\nThe application to ViT architecture is rather straightforward: deactivate the feature/tokens using the attentional weights. The mathematical justification of the loss seems well funded but is independent from the attention-based architecture. The approach is evaluated on small size databases using standard metrics but is not compared, as it seems to me, to another feature attribution study that explicitly exploits attention to build the explanation (Chefer et al., 2021).\n\nThe paper is well written and clear, but the content, for me, is rather at the level of a specialized workshop given the nature of its novelty (application of a known approach to another neural architecture and small theoretical contribution).\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1982/Reviewer_QpHf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1982/Reviewer_QpHf"
        ]
    }
]