[
    {
        "id": "g6L8JNkCBl",
        "original": null,
        "number": 1,
        "cdate": 1666555742396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555742396,
        "tmdate": 1666555742396,
        "tddate": null,
        "forum": "nAvBCvT5oA",
        "replyto": "nAvBCvT5oA",
        "invitation": "ICLR.cc/2023/Conference/Paper4803/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an approach for learning portable options by training and ensemble of options and enforcing that the ensemble members learn feature representations that attend to different parts of the observation. The approach then learns to determine the feature set that generalizes best across environments to choose the most portable option. The paper demonstrates learning of portable policies in ProcGen and portable options in Montezuma\u2019s revenge.",
            "strength_and_weaknesses": "# Strengths\n\n- the problem of learning portable options is very important: the whole point of learning options is to \u201cport\u201d them to different tasks, but often options trained on observations from one task overfit to spurious features and then don\u2019t generalize well to another \u2014 approaches like the one proposed here, that aim to address this problem, are of high relevance to the community\n\n- the intuition of the proposed (learning ensembles with disjoint feature sets) is novel to my knowledge and intuitive\n\n- I like the structure of the paper, in which the authors first explain their idea in the context of policy learning and only afterwards extend it to the more complex case of option learning (which additionally requires to learn initiation and termination sets) \u2014> disentangling the complexity in this way makes it much easier to follow the description of the method\n\n\n# Weaknesses\n\nThe main weakness of the submission is the experimental evaluation of the proposed approach. The paper proposes an approach for option learning, but never evaluates whether the learned options are actually useful for downstream learning. There are no comparisons to prior works and no ablation studies of the components of the proposed approach. There is no qualitative analysis of the learned feature representations, even though they are the main novelty proposed in the approach.\n\nThe experimental analysis will need substantial improvements to verify the usefulness of the approach.\n\n(1) Evaluations on using the options for downstream task learning should be added. \n\n(2) A comparison to (a) prior option learning works and (b) the exact same setup as the proposed approach but without the ensemble learning part should be included.\n\n(3) Detailed analysis experiments of the many moving parts of the proposed approach should be added: how do different number of ensemble elements or different regularizing coefficients influence the learned options?\n\n(4) Qualitative results and visualizations should be added, to show what features the ensemble identifies as transferrable, in what situations transfer works well (compared to baselines), in what situations the proposed approach fails etc\n\nThe current experimental evaluation does not allow to properly judge the proposed approach and, due to a lack of qualitative results, it is hard to understand why some of the trained options fail or why their performance *degrades* over training.\n\n-----\nApart from the experimental evaluation, there are a few additional weaknesses:\n\n(A) The description of the extension of the ensemble approach to learning options (instead of policies) was hard for me to follow, particularly I had trouble understanding the second-to-last paragraph in section 4.2 \u2014 some intuitive examples for options and feature sets, e.g. from Montezuma\u2019s revenge, and a figure visualizing the approach could help here.\n\n(B) The proposed objective encourages the learned representations between ensemble elements to be disjoint \u2014 it seems to me that this can make optimization tricky since there are many local optima in which important features are \u201ctrapped\u201d within disjoint feature sets of different ensemble members, making me wonder how hard it is to tune this approach on tasks that require the interplay of many features for policy learning.\n\n(C) More generally, like many option papers, the proposed approach has many moving parts (learning ensembles of policies, initiation and termination classifiers, additional regularization terms with coefficients that require tuning, bandit-optimization of which policy from the ensemble to pick for execution). This suggests that it will be hard / require a lot of tuning to apply this approach to complex problems. Again, this is generally true for many option learning approaches, but the paper introduces additional complexities that go beyond conventional option learning and exacerbate the problem.\n\n(D) The related work section misses some references to relevant work on learning state abstractions for generalization in RL. See e.g. Amy Zhang\u2019s McGill PhD thesis from 2021 for an overview of relevant works.\n\n\n# Questions\n\n\n- Can we learn a single embedding space shared between initiation and termination classifiers and policy? This could substantially reduce the number of learned components and required tuning.\n\n- Fig 3 shows that the number of ensemble elements needs to be tuned separately for each task. Is there a good way to choose the number of ensemble elements without just trying all of them?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear in introduction and the first part of the approach section. The writing in the later part of the approach section (that explains extension of the idea to the option setting) as well as in the experimental evaluation however is hard to follow and lacks visualizations. \n\nThe quality of the experimental evaluation is severely lacking. \n\nThe approach is novel to the best of my knowledge and an interesting idea.",
            "summary_of_the_review": "I am a bit torn about this submission. I really like the tackled problem and the approach seems interesting to me. There are some doubts about the empirical performance of the approach which I have outlined in \u201cweaknesses\u201d above. These would require a solid experimental evaluation to address. However, the experimental evaluation of the current submission is severely lacking. Thus I do not recommend acceptance of the paper in it\u2019s current form and suggest the authors extend the experimental evaluation and resubmit their paper at ICLR or another fitting venue.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4803/Reviewer_Q9hT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4803/Reviewer_Q9hT"
        ]
    },
    {
        "id": "PnnfxyJDew",
        "original": null,
        "number": 2,
        "cdate": 1666582094368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582094368,
        "tmdate": 1666582094368,
        "tddate": null,
        "forum": "nAvBCvT5oA",
        "replyto": "nAvBCvT5oA",
        "invitation": "ICLR.cc/2023/Conference/Paper4803/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper tackles the problem of learning reusable option policies to achieve cross-task generalization using reinforcement learning. It describes a method for learning feature spaces that can be reused by an option-like policy for (1) determine the next action to take (2) whether it is executable and (3) whether it should terminate. The core technical component is an ensemble feature learner (Kim et al., 2018) that aims to discover a set of feature extractors that are both diverse and optimizes some objective. In this paper, the ensemble feature learner is used to discover features for an ensemble of RL-based option policies for optimizing task rewards. The same ensemble feature learner is also used to learn an ensemble of initiation set and termination set classifier. The core component of learning reusable policies using feature ensemble learner is evaluated on a set of Procgen environments. The full method (including the initiation and termination set classifier) is evaluated on the montezuma\u2019s revenge domain, where different rooms are considered different environments. \n",
            "strength_and_weaknesses": "Overall I like the premise of the paper. Options should be able to port to other environments or different parts of the state space, as opposed to only function in the state distribution that it\u2019s trained on. I also applaud that the paper chooses to evaluate its method on a pixel-input domain as opposed to more \u201cconvenient\u201d environments where each input dimension is already meaningful.\n\nHowever, I have to admit that I have a hard time appreciating the method itself and the conclusion drawn by the evaluation. I also had a hard time understanding the method itself, despite multiple attempts at reading the section. Below I will describe my comments in detail.\n\nWriting quality\nThe structure of the paper is clear, and the introduction is enjoyable to read. However, the method section is difficult to understand, especially section 4.2 and 4.3 and their connections wit the previous subsections. Here is a list of points that I do not fully understand\nI understand why it\u2019s difficult to evaluate the initiation condition, but if the method assumes a reward function is given, why can\u2019t it use the reward function to determine the termination set?\nThe third paragraph of 4.2 says \u201cIf the initiation and termination set are both incorrect during the same option execution, the option will appear to succeed but will have an incorrect instantiation\u201d. I understand false positives are likely, but the sentence appears to state that the option is guaranteed to succeed if both classifiers are wrong. Why is that?\nI\u2019m generally confused about the role of the markov classifier. The paragraph says the markov classifier \u201clearns the initiation or termination set of a new option instantions\u201d, What does that mean?\nHow does markov classifier allow the option to \u201creturn to a specific instantiation\u201d? Does it reset the simulator status?\nIn the 5th paragraph, the \u201cclassification loss\u201d is used to determine policy success. The classification loss of which classifier? And why is loss used, not the prediction? And why is a true positive state \u201calmost indistinguishable from the previously seen state the classifier deems positive\u201d since the invariant feature should only be a subset of the entire space?\nHow is the portability estimate in 4.3 used by the classifier ensembles concretely?\nIn section 4.4, what exactly is the \u201ctrue determination state\u201d in eq 5? What is the \u201ctrue\u201d meant to contrast? \n\nMy second major comment is regarding the conclusion drawn in the experiments in general, especially in Section 3.2. The paper seems to imply that the improved performance is solely due to the reusable feature set extracted via the ensemble feature learning scheme. However, another factor that may play a critical role is the ensemble policies themselves. It is widely known that ensemble could aid exploration. Thus to correctly attribute the performance gain, It would be critical to ablate this factor and evaluate the ensemble RL policy without the feature learner. Similar counter arguments can be made for all later experiments, as they all rely on the same ensemble RL strategy. \n\nFinally, the paper has many moving parts and little ablation studies. The ambiguous evaluation metrics that entirely relies on the learned classifier makes the experiment results less clear. I believe the paper will benefit tremendously from a set of experiments using domains that support option-level skill evaluation with ground truth initiation and termination set. For now, it is really difficult to understand the validity of the conclusion drawn from the experiments.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the motivation is clear and enjoyable to read. The method section and the experiment section are really difficult to understand.\nQuality: the motivation and the premise are clear. But I had a hard time evaluating the validity of the method and the evaluation. \nOriginality: learning reusable feature set for options framework is important and novel. ",
            "summary_of_the_review": "Overall I really like the premise and the motivation of the paper. However, the writing / clarity is lacking and the experiment evaluation is insufficient. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4803/Reviewer_8Rro"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4803/Reviewer_8Rro"
        ]
    },
    {
        "id": "mjj_rlC39jZ",
        "original": null,
        "number": 3,
        "cdate": 1666907702587,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666907702587,
        "tmdate": 1666907702587,
        "tddate": null,
        "forum": "nAvBCvT5oA",
        "replyto": "nAvBCvT5oA",
        "invitation": "ICLR.cc/2023/Conference/Paper4803/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an approach for learning policies that can enable better generalization across tasks. This is done using an ensemble of learners, each of which uses attention to focus on a different part of the feature space, and then using bandit-exploration to select a particular policy. Further, the authors include an ensemble of classifiers to account for initiation and terminations sets, and argue that the combination of the classifiers and the policies give rise to transferable options. Generalization results are included on ProcGen, and Montezuma's revenge (across multiple levels). ",
            "strength_and_weaknesses": "Strengths \n1. Generalization through learning diverse representations\nUsing an ensemble of policies each of which focus on different parts of the input, and then selecting among them using a bandit-style objective to find the most robust policy is an intriguing idea. There has been work that focuses on learning the right representations of the environment to enable better generalization [1], and this paper approaches the same problem by instead encouraging diverse representations for multiple learners (enforced using distance in the embedded space), and then selecting for the most robust one. \n\nWeaknesses - \n\n1. Complexity of approach, specificity of Montezuma experiments, relation to exploration algorithms\n\nThe overall approach has a large number of moving parts - the policy ensemble, the classifier ensemble, the markov classifier. For the experiments included in particular in the paper (Montezuma's revenge), the initiation and termination conditions need to be specified by hand for the classifiers. Even the options are hand-specified, and not learned for the task (the authors seem to indicate that there is further training for these options, but this isn't clear). Also evaluation is done on specific tasks within the game (rolling skull, spider etc). Contrast this to results in self-supervised exploration. Go-Explore [2] was able to enable an agent to learn how to play Montezuma's revenge without any of these constraints (hand specified option/initiation/termination conditions) and without any rewards, and keep generalizing across different levels also without any restriction to the particular task considered. \n\n\n2. Sensitivity of ensemble size for policy ensemble - \n Even for the policy ensemble results on the ProcGen environment, the policy ensemble size needs to be tuned for each environment, and performance does not generally improve with more policies in the ensemble. The authors argue this is because 'too much time is spent exploring policies that do not generalize'. However, there is no analysis that shows this to be the case. It might be difficult for practitioners to adopt this approach if there is heavy reliance on tuning this hyper-parameter. The results on the ProcGen environments show that no single choice for ensemble size performs consistently (they each are close to the 1-member ensemble case in atleast one environment). \n\n\n[1]: Eysenbach, Ben, Russ R. Salakhutdinov, and Sergey Levine. \"Robust predictable control.\" \n[2] Ecoffet, Adrien, et al. \"Go-explore: a new approach for hard-exploration problems.\"",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite easy to follow up until section3. However, I found section4 which brings together the entire approach with the ensemble of classifiers and markov classifier, as well as the various specific conditions used in the montezuma experiments to be more convoluted. \nDue to the complex nature (multiple moving parts) of the overall system, it might be difficult to reproduce.\n",
            "summary_of_the_review": "Overall, I am not in favor of acceptance since the full presented system is evaluated for particular choices of options/initial/final conditions and specific tasks, while we have have algorithms from the self-supervised learning literature like Go-explore which can keep exploring and generalizing (for the same environment considered in this paper). The idea of learning diverse features to find one that's robust to new tasks seems interesting but performance is very dependent on ensemble size as described previously, and this will hinder widespread adoption. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4803/Reviewer_rc41"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4803/Reviewer_rc41"
        ]
    }
]