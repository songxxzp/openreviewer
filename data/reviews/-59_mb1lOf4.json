[
    {
        "id": "jxir013F8l",
        "original": null,
        "number": 1,
        "cdate": 1666212077193,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666212077193,
        "tmdate": 1666212077193,
        "tddate": null,
        "forum": "-59_mb1lOf4",
        "replyto": "-59_mb1lOf4",
        "invitation": "ICLR.cc/2023/Conference/Paper2210/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes adding an elastic net penalty while calculating client updates to the server model. The authors show that this augmentation can be implemented with a variety of standard FL optimization methods, provide theoretical convergence analysis for well behaved (convex, smooth) objective functions, and show empirically that adding this loss produces better results, both in terms of model update communication cost, and accuracy. ",
            "strength_and_weaknesses": "Strength\n- For one variant authors provide theoretical evidence that the augmentation does not harm convergence\nWeakness\n- Theory makes very strong assumptions \n- Theory does not prove improvement over baseline method, nor does it help much with practical questions (selecting tuning parameters etc). \n- Experiments show only marginal improvements over the baseline in terms of both compression and quality. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written, and derivations are clear and easy to follow. Experimental setup is quite easy to follow. ",
            "summary_of_the_review": "The work is accurate, but does not add sufficiently novel ideas. Utilizing elastic loss while training server side is already well established, and the loss function is well studied. Adding this loss to regularize the difference between the server model and client updated model is a fairly simple extension, and neither the theory nor experimental results are super exciting. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2210/Reviewer_jTwe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2210/Reviewer_jTwe"
        ]
    },
    {
        "id": "ZYYiw0bZjF8",
        "original": null,
        "number": 2,
        "cdate": 1666665930219,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665930219,
        "tmdate": 1666665930219,
        "tddate": null,
        "forum": "-59_mb1lOf4",
        "replyto": "-59_mb1lOf4",
        "invitation": "ICLR.cc/2023/Conference/Paper2210/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The papers shows how to apply elastic net [Zou & Hastie 2005] to existing federated learning (FL) schemes. Specifically, three FL schemes are selected: FedProx (already incorporates $\\ell_2$ regularization) [Li et al. 2020], SCAFFOLD [Karimireddy et al. 2020], and FedDyn [DAE Acar et al. 2021. At a high-level, elastic net is a regularization technique that combines $\\ell_1$ and $\\ell_2$ regularization.\n\nThe paper supports the claim that applying elastic net to the aforementioned FL schemes resolves the client drift and communication cost problem, in one fell swoop, through the following empirical evaluations:\n\n1. Sparsity (communication cost): Table 2: \"number of non-zero elements cumulated (accumulated?) over all round simulated with 10% client participation for IID and non-IID settings in FL scenarios.\"\n\n2. Entropy (client drift): Table 3 and Table 4: \"cumulative entropy values of transmitted bits with 10%\" and \"100% client participation for IID and non-IID settings in FL scenarios.\"\n\nAdditionally, test accuracy, as a function of communication rounds, results are also provided.\n\nFurthermore, the paper utilizes a distribution plot (Figure 2 -- FedDyn, elastic net applied to SCAFFOLD & FedDyn), and reference to the text by Cover & Thomas 2006 to substantiate the claim that \"FedElasticNet can reduce the entropy by transforming the Gaussian distribution into the non-Gaussian one.\"\n\n\n",
            "strength_and_weaknesses": "Main concerns:\n1. FedProx and FedDyn are also FL schemes that utilize regularization ($\\ell_2$-like), thus this paper adds the $\\ell_1$ term with a sign adjustment.\n2. The last sentence in page 6: \"We optimize the hyperparameters depending on the evaluated dataset: learning rates, $\\lambda_2$, $\\lambda_1$\" is dubious as it relates to novelty since it can come down to the task of tuning these parameters to achieve the desired result, which is acceptable for application.\n3. Table 2 shows that the number of non-zero elements are reduced across the board, but it does not show \"when\" the non-zero elements become zero. This can be shown by a plot of non-zero elements vs communication round.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is readable, but the empirical evaluations are subjective.",
            "summary_of_the_review": "The novelty of this paper is scarce since the main idea is to show how to use elastic net under a limited set of FL schemes. The claims in the paper are also informal.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2210/Reviewer_Rp9d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2210/Reviewer_Rp9d"
        ]
    },
    {
        "id": "u2Gkzbx5w2o",
        "original": null,
        "number": 3,
        "cdate": 1667540188274,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667540188274,
        "tmdate": 1667676974166,
        "tddate": null,
        "forum": "-59_mb1lOf4",
        "replyto": "-59_mb1lOf4",
        "invitation": "ICLR.cc/2023/Conference/Paper2210/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Federated Learning trains a global model collaboratively over a set of clients , where the data is kept locally at the clients and only local gradients or parameters are communicated periodically with the server. Communicating large number of bits mights often slow down convergence and thus various compression schemes are used to deal with communication latency. Another challenge in the federated framework is heterogenous data distribution across clients. \n\nThis paper tries to tackle both these issues. In particular, they propose FedElasticNet a new framework for communication- efficient and drift-robust FL. They exploit $\\ell_1, \\ell_2$ regularization to deal with communication efficiency ( since $\\ell_1$ regularizer ensures sparse solution ) and client drift respectively. ",
            "strength_and_weaknesses": "#### Strengths\n1. The idea is simple and intuitive.\n2. The paper is overall well written and motivated.  \n3. The authors show $O(\\frac{1}{T})$ convergence in the smooth, convex setting \n\n#### Concerns \n1. Table 1 - what do these symbols mean ? Please update the caption for readability.\n2. Theorem 3.1 The second term depends on d -  will it be negligible when we are dealing with practical large models ? A supporting plot simulating the convergence rates would be helpful. \n\n*Client Drift*\n\n3. Results on non-IID: One claim of the paper is that it can effectively deal with both client drift and communication compression. However, non-IID exp is extremely limited, only Shakesepeare and one non-iid setting; To support the drift robust claims - there needs to be more experiments on multiple dataset , models, and across different strength of heterogeneity comparing with multiple method that only deal with client drift / data heterogeneity.  \n\n4. In the non-iid setting, SCAFFOLD which is a standard approach to deal with client drift seems to perform equally well or even better than the proposed solution; Can you add more discussion on this and what are the scenarios when SCAFFOLD is better and when the proposed algorithm is better. \n\n5. The theory is based off FedDyn. Now, FedDyn and SCAFFOLD already deal with the client drift / heterogeneity and thus it is hard to flesh out the roles of the regularization penalty introduces in this paper.\n\n6. Analysis on FedAvg + $\\ell_1$ + $\\ell_2$ i.e. Algorithm 1 needs to be done - to clearly show if there is any advantage from these penalty terms. \n\n*Communication Compression*\n\n7. One effective way to deal with communication compression is to use contractive compression operators ex. quantization, top-k etc. To deal with the slow convergence due to additional terms appearing in the convergence result due to compression, it is standard practice to use an error feedback mechanism that ensures linear convergence at the same rate. One can simply use any of the drift robust algorithm (SCAFFOLD e.g.) apply communication compression with EF and be communication efficient.\nFor fair comparison, you need to compare with different drift robust algo + compression + EF with the proposed solution.\nEmpirically, for different compression rate ( using Top-k, Sign, Quantization etc compressor + Error Feedback compression operation C ) a clean experiment is to compare : FedAvg + C + EF vs FedAvg + $\\ell_1$ + $\\ell_2$ \n\n8. Also in theory, it is beneficial to discuss the convergence rate obtained (additional terms) with that in case of EF + Compression (Quantize, top k , q sparse etc several available approaches )\n \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, the idea is simple and novel.  ",
            "summary_of_the_review": "Overall, while I feel the idea is nice - the paper in its current state lacks to establish the proposed claims -\nEmpirically we need ablations on: \n( a ) How does it compare with existing drift robust algorithms \n( b ) How does it compare with  existing algorithms that can deal with data heterogeneity. \n( c ) No mention of compression + EF in the paper - which is the standard practice in any FL setting. Why would I use this method and not C + EF ? ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2210/Reviewer_odbQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2210/Reviewer_odbQ"
        ]
    },
    {
        "id": "vavP5wskNd",
        "original": null,
        "number": 4,
        "cdate": 1667548502133,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667548502133,
        "tmdate": 1667548502133,
        "tddate": null,
        "forum": "-59_mb1lOf4",
        "replyto": "-59_mb1lOf4",
        "invitation": "ICLR.cc/2023/Conference/Paper2210/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes FedElasticNet which applies elastic net to federated learning (FL) of deep neural nets. The paper proposes two variants based on FedAvg and FedDyn, and provides the convergence analysis of the latter. Experiments are conducted to show the benefits of the proposed methods.",
            "strength_and_weaknesses": "Strength:\n1. The topic of improving the efficiency and reducing client shift in FL is important.\n\nWeakness:\n1. The novelty is not significant. Both $l_1$ and $l_2$ regularizations are mature techniques in statistics, and deep neural network training. I do not find this method very interesting. Using Lasso to get sparsity is doable, but is very tricky to tune and leads to biased gradient estimation which would hurt the model performance. Indeed, the theory (Theorem 3.1) containg a nonvanishing constant is worse than the baseline FL rate, and emprical performance might also be worse in some cases as we see from the figures. Thus, the proposed method seems not very valuable both theoretically and empirically. Also, elastic net introduces two more hyper parameters $\\lambda_1$ and $\\lambda_2$ which make the method harder to tune. Thus, much stronger motivation is needed.\n\n2. The related work section should be improved. The paper misses many recent papers on communication-efficient FL.\n\n3. The presentation is not satisfactory. \n\n(i) In Algorithm 1, why is the local update presented in this way? How do you solve that optimization problem locally? In practice we may use SGD as in Algorithm 2? Why are they inconsistent? Are we using stochastic optimziation?\n(ii) In the theory part, there is no formal statement of the assumptions. I do not know the setting of this analysis. Particularly, which part is related to the client drift? The assumptions should be stated very clearly.\n",
            "clarity,_quality,_novelty_and_reproducibility": "please see above",
            "summary_of_the_review": "In general, the paper is not very well written, and the motivation and performance of the proposed algorithm is poor. Thus, I think the paper does not meet the bar of ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2210/Reviewer_9ArF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2210/Reviewer_9ArF"
        ]
    }
]