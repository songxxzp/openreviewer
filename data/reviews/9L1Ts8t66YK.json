[
    {
        "id": "kxm_kftFNvp",
        "original": null,
        "number": 1,
        "cdate": 1666541280465,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541280465,
        "tmdate": 1671129477285,
        "tddate": null,
        "forum": "9L1Ts8t66YK",
        "replyto": "9L1Ts8t66YK",
        "invitation": "ICLR.cc/2023/Conference/Paper725/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a cross-graph augmentation method to simulate global semantic shifts. In particular, the paper first analyzes the limitations of invariant graph contrastive learning (I-GCL). Then the authors explore equivariance for cross-graph augmentation to mitigate the limitations of I-GCL. The authors conduct both unsupervised learning and transfer learning experiments the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Pros:\n1. The question studied is important and interesting.\n2. Using equivariance can help resolve the sensitive augmentation.\n\nCons:\n1. In Cross-graph augmentation, how do the authors generate labels for instances as\nlabels are unknown in contrastive learning. The authors claim that they mitigate structural differences between two graphs. However, randomly padding virtual nodes and edges may change the latent distribution of two graphs.\n2. In experimental settings, the details of evaluation methods and reported baselines performance are missing. E.g. how do the authors report the baselines performance? Are they reimplementing all baselines or they reporting performances from original papers? Which accuracy is refer to the reported accuracy? The last epoch accuracy or the test accuracy in terms of the best validation accuracy? It is really hard to reproduce the reported performance without these details.\n3. As far as I know, the reported results are from different evaluation methods. InfoMax and ContextPred are reporting the test accuracy of the best validation epoch, while GraphLOG is reporting the performance of the last epoch. The authors need to report results under a same experimental settings.\n4. In results tables, do the authors use a same backbone for all baselines? As authors mentioned, they are using a BarlowTwins as backbone for unsupervised task and employing a GraphTrans for transfer learning task. Do they also using these backbones for other baselines?",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method needs more detailed experimental instructions.",
            "summary_of_the_review": "Overall, this paper takes one the most important research problems of graph learning: graph contrastive. The idea of exploring equivariance cross graph augmentation is interesting. However, I feel this paper is not ready as there are several major concerns regarding the framework design, experiment. Hopefully, the authors can address these concerns and make this paper more solid in the next submission.\n\n##\nScore updated after reading the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper725/Reviewer_LCfW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper725/Reviewer_LCfW"
        ]
    },
    {
        "id": "-0uxS3vlkB",
        "original": null,
        "number": 2,
        "cdate": 1666617502842,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617502842,
        "tmdate": 1666617502842,
        "tddate": null,
        "forum": "9L1Ts8t66YK",
        "replyto": "9L1Ts8t66YK",
        "invitation": "ICLR.cc/2023/Conference/Paper725/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, an equivariant graph contrastive learning method is proposed to perform cross-graph augmentation, which is reasonable and effective for GCL. Experiments are performed both unsupervised learning and transfer learning tasks to show the model effectiveness.",
            "strength_and_weaknesses": "Pros:\n1.\tThe studied problem with equivariant graph contrastive learning is important and new for GCL domain.\n2.\tThe theoretical discussion of graph interpolation is given.\n3.\tComponent-wise model evaluation is provided to show the model effectiveness.\n4.\tHyperparameter sensitivity is conducted in the evaluation section.\n\nCons:\n\nIn the evaluation section, various baselines are compared, such as JOAO, GraphCL, and ADGCL. For the compared various GCL methods, the parameter setting details of those baselines can be described. \n\nAdditionally, the computational complexity of the new E-GCL method can be analyzed compared with representative GCL baselines. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This work solves an important problem with a technically sound solution. Conducted experiments are comprehensive to demonstrate the effectiveness of the new approach. The source code has already been released for reproducibility. ",
            "summary_of_the_review": "Inspired by the equivariant SSL, this work proposes to enhance graph contrastive learning with cross-graph augmentation under an equivariant SSL framework. Experiments with different tasks show the advantage of the new approach.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper725/Reviewer_f85p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper725/Reviewer_f85p"
        ]
    },
    {
        "id": "akRl4M9-kR",
        "original": null,
        "number": 3,
        "cdate": 1666643896695,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643896695,
        "tmdate": 1666643896695,
        "tddate": null,
        "forum": "9L1Ts8t66YK",
        "replyto": "9L1Ts8t66YK",
        "invitation": "ICLR.cc/2023/Conference/Paper725/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper first points out that aggressive augmentations may worsen representation and lead to sub-optimal performance. To mitigate this issue, they propose a novel contrastive learning framework (E-GCL) that employs the equivariance principle to implement cross-graph discrimination. To this end, the authors propose insensitive transformations P and sensitive transformations H for the intra-graph augmentation and cross-graph augmentation, respectively. The author conducted detailed experiments to prove their framework\u2019s effectiveness.",
            "strength_and_weaknesses": "Strength 1: The contribution of this paper is clear; this paper is well written.\nStrength 2: This paper deals with a non-trivial and challenging issue which are neglected by current research. \nStrength 3: The experiment was performed on several datasets. The results show superior performance over existing competitive approaches.\n\nWeakness 1: In the introduction part, the authors point out that \u2018Nonetheless, it is hard, without domain knowledge (Dangovski et al., 2022; Chuang et al., 2022) or extensive testing (Dangovski et al., 2022), to tell apart sensitive and insensitive augmentations.\u2019  Indeed, it is an unresolved challenge to tell if a data augmentation will cause global semantic shifts. Learning the semantic shifted view can lead to sub-optimal performance. However, how the equivariant self-supervised learning can mitigate this issue? In the proposed framework, the intra-graph augmentation is kept. Hence, the semantic shifted view can still damage the model.\n\nWeakness 2: In Section 3.2, the author applied the \u2018Group Averaging\u2019 to \u2018make the interpolations insensitive to relative permutations\u2019. However, in the Section 3.1, the author claims that \u2018Equivariance requires that\u2026, any change applied to the graph should be faithfully reflected by the change of representation.\u2019 This is not consistent. Can the author give more discussions on why the model should be insensitive to relative permutations during cross-graph augmentation? And why apply the \u2018Group Averaging\u2019 can help improve performance? There is no further experiment or discussion in Section 4 to back up this design.\n\nWeakness 3: In Section 4.3, the authors demonstrate the alignment and uniformity losses of I-GCL and E-GCL. It can be observed that the \"E-GCL with cross-aug\" setting has better alignment and uniformity losses. Can the author give some discussion on this finding? Does this imply if getting rid of the intra-aug, the model may have a better downstream task performance? If possible, the author should conduct ablation study on the \"E-GCL with cross-aug\".",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written with good novelty. And the code is available in the appendix.",
            "summary_of_the_review": "This paper is well-written and deals with a non-trivial challenge. The experiments prove the model\u2019s effectiveness. However, theoretical justification for the proposed design and discussion on some experiments are missing. The authors should address these issues. Overall, this paper is above the acceptance borderline if the authors can address all the concerns mentioned above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper725/Reviewer_itQe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper725/Reviewer_itQe"
        ]
    },
    {
        "id": "mWyLnVVv9D3",
        "original": null,
        "number": 4,
        "cdate": 1666854989111,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666854989111,
        "tmdate": 1666854989111,
        "tddate": null,
        "forum": "9L1Ts8t66YK",
        "replyto": "9L1Ts8t66YK",
        "invitation": "ICLR.cc/2023/Conference/Paper725/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an equivariant graph contrastive learning framework that adopts two principles: invariance to intra-graph\naugmentations and equivariance to cross-graph augmentations. They use Mixup as the method for cross-graph augmentation and argue that the cross-graph augmentation captures global semantic shifts and yield better performance.",
            "strength_and_weaknesses": "Strength:\n1. Extensive results on multiple datasets.\nWeakness:\n1. Novelty: Though the authors make a great effort in explaining the concept of equivalence, the framework itself boils down to a combination of Mixup and contrastive learning. This combination is not new. For instance, at a high level, [1] and [2] both adopt the combination.\n2. The experimental results are not very convincing. In table 1(a), on the datasets that the proposed method performs the best, the increase is usually below 1% , not to mention that on the remaining 3 datasets, the proposed method underperforms. Similarly, in table 1(b), the proposed method only wins 4/8 datasets and by a small margin.\n\n\nLiterature:\n[1]i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning\n[2]Improving Contrastive Learning by Visualizing Feature Transformation",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear, but the novelty needs to be improved.",
            "summary_of_the_review": "In general, I am not a fan of this paper mainly for the following two reasons: (1) Combining Mixup and contrastive learning is not new as several papers have adopted a similar idea though in the CV domain. (2) The experimental results are not exciting.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper725/Reviewer_sATH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper725/Reviewer_sATH"
        ]
    }
]