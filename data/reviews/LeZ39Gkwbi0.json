[
    {
        "id": "y-qaHZpT2z",
        "original": null,
        "number": 1,
        "cdate": 1666543291053,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666543291053,
        "tmdate": 1666543291053,
        "tddate": null,
        "forum": "LeZ39Gkwbi0",
        "replyto": "LeZ39Gkwbi0",
        "invitation": "ICLR.cc/2023/Conference/Paper4038/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a ProtoGNN for non-homophilous (or heterophilous) graphs, which could augment existing GNNs by combining node features with structural information. ProtoGNN could learn multiple prototypes for each class with slot attention. To further exploit the power of multiple prototypes, this paper uses two regularization losses: compatibility loss and orthogonality loss. The experimental results show that the proposed method is useful for several datasets.  ",
            "strength_and_weaknesses": "Strengths:\n1. Using multiple prototypes for each class is interesting.\n2. The experiments are conducted on both homophilous and heterophilous graphs.\n3. The writing of the paper is clear in general.\n\n\nWeakness:\n1. It is not quite clear how does self-attention perform. Self-attention is cheaper and more classic than slot-attention, and thus it is necessary to compare slot-attention with self-attention.\n2. ProtoGNN only significantly outperforms the SOTA method GloGNN++ on 3 over 7 datasets: Penn94, Genius and US-election. It is necessary to conduct t-tests for Twitch-gamers, Cornell5 and Amherst41.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is clear. \n\nThe overall quality is good. \n\nNovelty is somewhat limited as \n(1) slot attention is proposed by prior methods,\n(2) the rationality of choosing slot attention rather than classic self-attention is not discussed. \n\nReproducibility looks good as the implementation details are presented.",
            "summary_of_the_review": "Overall, the proposed approach is interesting and the writing is clear. However, it is not quite clear why the paper uses slot attention rather than self-attention. Besides, the experimental results in Table 4 show that ProtoGNN only significantly outperforms GloGNN++ on 3 out of 7 datasets. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4038/Reviewer_A8KJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4038/Reviewer_A8KJ"
        ]
    },
    {
        "id": "b-m23Pm2lR",
        "original": null,
        "number": 2,
        "cdate": 1666578321868,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578321868,
        "tmdate": 1666688990033,
        "tddate": null,
        "forum": "LeZ39Gkwbi0",
        "replyto": "LeZ39Gkwbi0",
        "invitation": "ICLR.cc/2023/Conference/Paper4038/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extends the transform-based GNN by enhance the memory with multiple cluster prototypes. It justifies the correctness by effective considering the nodes from the same class but with long range. Experimental evaluations demonstrate its effectiveness.",
            "strength_and_weaknesses": "Strength\n\n- The writing and organization are good. \n- The experimental evaluations are sufficient.\n\nWeakness \n\n- The novelty is very limited. It is not novel to introduce cluster prototypes into the transformer-based GNNs such as [1]\n\n[1] Junjie Xu, Enyan Dai, Xiang Zhang, Suhang Wang. \u201cHP-GMN:Graph Memory Networks for Heterophilous Graphs\u201d Accepted by The IEEE International Conference on Data Mining\u00a0(ICDM 2022)",
            "clarity,_quality,_novelty_and_reproducibility": "- The clarity and reproducibility are good. \n- The novelty and originality are weak.",
            "summary_of_the_review": "My main concern is the limited novelty. It is not novel to introduce cluster prototypes into the transformer-based GNNs, although this may be the first which considers multiple prototypes.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4038/Reviewer_mEwF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4038/Reviewer_mEwF"
        ]
    },
    {
        "id": "CSwTVZPZERm",
        "original": null,
        "number": 3,
        "cdate": 1666678855089,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678855089,
        "tmdate": 1666678855089,
        "tddate": null,
        "forum": "LeZ39Gkwbi0",
        "replyto": "LeZ39Gkwbi0",
        "invitation": "ICLR.cc/2023/Conference/Paper4038/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper targets the incapability of GNNs on heterophilous graphs and proposes to process the node feature and graph structures separately. Specifically, Multiple node feature prototypes are first generated with only the node feature information. Then, arbitrary GNN is used to obtain a structural view (embedding) of each node. Finally, the prototypes are aggregated into the structural embeddings via an attention based mechanism. Experiments are conducted on multiple datasets and compared with multiple baselines.\n",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper is well written and easy to follow. The explanation on the model is very clear with a good logic.\n\n2. The studied problem of heterophilous graph learning is meaningful.\n\n3. The experiments are comprehensive, 10 datasets are used and some of them are very large. Also, many baselines are included in the commparison. \n\n\nweakness:\n\n1. when generating the structural view, if the GNN are the ones for homophilous graphs, then the obtained embedding should not be of high quality, and accordingly there is no reason to believe they can be used to properly aggregate the prototypes. But if the adopted GNNs are the ones for heterophilous graphs, then the mechanism proposed in this paper is only an incremental part of the whole model instead of a key module.\n\nTherefore, whether the proposed module works well highly depends on whether the adopted GNN backbone is good enough, making the contribution of this work not very significant. Also, according to the experiments, the improvement gained by including ProtoGNN is indeed not significant.\n\n2. what is the backbone GNN used in Table 4\n\n3. It would be better to also show the homophily matrix of the homophilous graphs. In the current version, althoug the homophily matrix of the heterophilous graphs are given, but it seems that most of the classes have half homophilous edges and half heterophilous edges, and not sure how to understand these values. E.g. is 0.5,0.5 enough to say this class is heterophilous enough? \n\n4. Are all the heterophilous datasets binary classification datasets with only two classes? Why is this? Isn't there any datasets with more classes?",
            "clarity,_quality,_novelty_and_reproducibility": "Very good clarity\n\nQuality is OK in terms of model design, and is good in terms of experiments\n\nNovelty is OK\n\nNot sure about the reproducibility, but since the module is reasonable, should be reproducible",
            "summary_of_the_review": "Overall, this paper is well written and targets a meaningful problem, but the contribution is not significant, because the performance is mainly determined by the backbone, which is also shown in experiments.\n\nTherefore, I think this paper is only slightly above acceptance threshold",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4038/Reviewer_Fqiu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4038/Reviewer_Fqiu"
        ]
    },
    {
        "id": "2Wq1yRAIdK",
        "original": null,
        "number": 4,
        "cdate": 1666710172592,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666710172592,
        "tmdate": 1666710988451,
        "tddate": null,
        "forum": "LeZ39Gkwbi0",
        "replyto": "LeZ39Gkwbi0",
        "invitation": "ICLR.cc/2023/Conference/Paper4038/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work attempts to deal with node heterogeneity for graph learning task. The proposed method dissects the node representations into feature and structure views. Finally a prototype-based node representation method is designed by separately modeling feature view with prototype-based MLP network and encoding graph structure with GNN.    ",
            "strength_and_weaknesses": "Node label heterogeneity raise unique challenge for effectively learning node representation. This work takes an important step to deal with it by not fully relying on a pure GNN model. However, there are still important questions that are not answered.\n\nStrength:\n1. A prototype-based method is proposed to deal with graph heterogeneity issue.\n2. Special regularizations are designed along with the proposed method for effectively controlling the prototype distribution.\n3. Extensive experiments are conducted on multiple datasets to show the classification performance of the proposed method.\n\nWeakness:\nMy main concern is about the novelty and motivation behind this work. More specifically, it's difficult for me to clearly catch up with the motivation and exact contribution to advancing GNN method for dealing with label heterogeneity issue. More specially, I have following concerns.\n\n1. The justification in the Introduction section is too weak to explaining why prototype can deal with the limitations mentioned in the second and third paragraphs.\n\n 2. It deserves more words to explain why we need multiple prototypes for each classes, and what is the connection between aggregating features from distant nodes and solving non-homophilous problem. The difference from previous works which also uses prototype or clustering centers is not discussed.\n\n3. Besides, it's difficult to understand what is the contribution of this work to advancing GNNs for better adapting to non-homophilous dataset. From the architecture shown in this work, we can see that the key component is a MLP-based encoder, while GNN layer can be initialized any GNN model. What if the selected GNN module like vanilla GCN, GAT etc. can not deal with the label heterogeneity problem? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not difficult to follow, but the novelty and quality are limited.",
            "summary_of_the_review": "Though experimental results show promising aspects of the proposed method, it still needs a significant improvement over the presentation quality, especially the clarity of motivation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4038/Reviewer_v2yj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4038/Reviewer_v2yj"
        ]
    }
]