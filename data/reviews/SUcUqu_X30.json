[
    {
        "id": "5T_z6H3aM9",
        "original": null,
        "number": 1,
        "cdate": 1666542212064,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542212064,
        "tmdate": 1666542279889,
        "tddate": null,
        "forum": "SUcUqu_X30",
        "replyto": "SUcUqu_X30",
        "invitation": "ICLR.cc/2023/Conference/Paper161/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed manipulating the attention learned during self-supervised learning toward more meaningful regions (cells against the background). Precomputed masks of cells are injected into the self-attention modules, i.e., combined with the self-attention before the softmax. Attentions are further divided into groups of cells, backgrounds, and cross-cell-backgrounds. Several public datasets are employed here for the experiments and evaluation. Superior results of the proposed method are reported in comparison to self-supervised learning baselines (pre-trained with DINO). ",
            "strength_and_weaknesses": "Strength:\n+ The manuscript is overall well-written and easy to follow\n+ The idea of injecting prior-based attention into the self-attention module is novel and seems effective, at least for pathological image-related applications.\n+ Many public datasets are employed here for the experiments\n\nWeakness:\n- I am concerned about the comparison study, considering only baselines are compared. If the proposed attention add-on should be regarded as a general approach, other self-supervised learning frameworks/networks should be considered in the comparison study as well. Otherwise, SOTA results on these utilized datasets could also be reported to get a sense of how the proposed framework generally works.\n- The definitions of M_self and M_cross are not well-introduced and discussed. I am unsure about the philosophy of setting the values to either 0 or minus infinite. Moreover, the M_self and M_cross are added to qk^T directly without considering the scale of these two vectors. Is it better to be a weighted sum?\n- How the accuracy of M_self and M_cross will affect the overall training performance should also be investigated and discussed.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of the manuscript is good. The idea of injecting prior-based attention in the self-attention module is novel, though the design could be improved and further investigated. Nevertheless, I believe it should be easy to reproduce the proposed method.",
            "summary_of_the_review": "The overall quality of the presented work is good, though there are some flaws in the introduction of the method and experimental settings. Adjusting the attention for medical images is essential in addition to the standard self-attention-based pre-training. The authors proposed a novel way toward that direction.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper161/Reviewer_6NiW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper161/Reviewer_6NiW"
        ]
    },
    {
        "id": "K8nU5EzKIph",
        "original": null,
        "number": 2,
        "cdate": 1666588084377,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588084377,
        "tmdate": 1666588084377,
        "tddate": null,
        "forum": "SUcUqu_X30",
        "replyto": "SUcUqu_X30",
        "invitation": "ICLR.cc/2023/Conference/Paper161/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper mentioned an observation: the vanilla SSL (for example, DINO) usually learned sparse features, which is suitable for object-centric tasks but not good for non object-centric tasks. As a result, the paper proposed using domain-priors together with several pretext tasks to learn de-sparsification features. The experiments on several datasets (slide-level/patch-level classification tasks) show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n1. The observation is quite interesting and it makes sense.\n2. The proposed strategies (domain-priors, pretext tasks..) is simple but make sense to de-sparsification the attention map.\n3. The results on several datasets are promising. The authors indeed conduct many experiments and well describe the datasets and corresponding experimental results.\n\nWeakness:\n1. The paper only considers on SSL framework, what's the situation in other SSL frameworks?\n2. It is hard to apply the proposed method on other tasks, because we first need a well trained segmentation model which are not easily achieved in many cases. \n3. Suppose we have segmentation ground truth and train the segmentation model, mabye we can just design a multi-task (ssl+supervised segmentation) learning framework and then achieve the de-sparsification effects (segmentation force the network to learn details).\n4. The claim \"We believe our work opens exciting avenues toward utilizing biology-relevant concepts and enhancements in neural networks\" is weird, where do you use biology-relevant concepts and enhancement?\n\nConcerns:\n1. I can imagine that the SSL models (e.g., DINO) on imagenet classification tasks produces sparse attention features (object-centric). I think that's because the task (classify those images mainly need to focus on the object) and data (object-centric). However, if the data is non-object-centric (for example, pathology slides), then the task (if well trained) should be able to force the model to learn more dense features, since it is these dense features which can contribute to correct classification. If it is in such a case, do we still need the de-sparsification step?\n2. Maybe some augmentation which are well designed for non-object-centric data can better solve this problem. I believe the sparsification is due to the data and the task (classification), so if we use some kind of augmentation to generate the two views (considering the dense essence of the data), the SSL should be able to learn the de-sparsification features.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is easy to follow and well organized.\nQuality: Good.\nNovelty: The observation is novel to me. The proposed solutions to de-sparsication is somewhat novel.\nReproducibility: The paper doesnot provide the codes with the submission.",
            "summary_of_the_review": "This is an interesting paper to solve sparse feature problem in SSL for pathology images. Generally, I'm interested in the observation part and fine with the proposed method (I agree the proposed method can largely alleviate the problem). However, I donot think this paper touches the essence of the phenomenon, which I think is due to data and the task (and we can use certain data augmentation to generate better views and thus enforce the model to learn better \"dense\" features). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concern.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper161/Reviewer_nwTd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper161/Reviewer_nwTd"
        ]
    },
    {
        "id": "aSKWmKewLTf",
        "original": null,
        "number": 3,
        "cdate": 1666682308421,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682308421,
        "tmdate": 1668577009018,
        "tddate": null,
        "forum": "SUcUqu_X30",
        "replyto": "SUcUqu_X30",
        "invitation": "ICLR.cc/2023/Conference/Paper161/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a digital pathology representation learning method that uses a vision transformer. The method splits a given whole slide image (WSI) into multiple crops and the cropped images are represented by using multi-head self-attentions (MSAs): Each cropped image is split into patch images, the patches are encoded by MSAs, and an average pooling is applied to the feature vectors obtained by the MSAs as a feature vector of the crop. Different from conventional vision transformer, the proposed method separates the patches into two regions: A cell-region and a back-region. The former is a set of the patches with cell nuclei inside and the latter is a set of the patches without cell nuclei inside. The method computes six different feature vectors for each crop. Mixing the patches from both regions and using vision transformers, the proposed method obtains a feature vector for each of the two regions by applying average pooling to the resultant feature vectors of the patch images. Separating the patches for each region and applying vision transformer, the method obtains another feature vector for each region. This feature vector is obtained based on the context within each region. Other two feature vectors are obtained based on the context between the two regions, which is described by the attention matrix computed basically from pairs of two patches from different regions. It is shown that separating (disentangling) the patches into the two regions, one can diversify the strength of the attentions and the performance of the classifiers can be improved.\n",
            "strength_and_weaknesses": "Strength:\nAs the authors mention in the paper, WSIs have characteristics different from natural images. In pathology images, cell nuclei are spatially dispersed, and the features of each cell nucleus, the spatial patterns of the multiple cell nuclei and the cytoplasm textures can all contribute to pathological image diagnosis. The authors point out the importance of the divergence of the attentions and find that the proposed method can diversify the attentions. The diversified attention would describe not only the relationships between spatially close patches but also the relationships between more distant patches distributed in wider areas. The experimental results demonstrate that the proposed method improves the performance of classification significantly.\n\nWeakness:\nThe method employs a cell segmentation method that requires labeled data and hence the reviewer questions whether the proposed method can be classified as a self-supervised learning (SSL) method and whether the performance comparison with SSL methods is fair. The labeled data used in the training of the cell segmentation could implicitly teach that the nuclear regions would play important roles in the classification tasks.\n\nThe analysis of the obtained attentions is not satisfactory. The experimental results show that the attentions are diversified but the details of the obtained attentions between tokens are not indicated. As shown in the left image in Fig.5, each crop image would include pathologically different regions such as the necrosis regions and the tumor ones. The attention matrix can represent the relationships between the patch images. Did the attention matrices obtained by the proposed method and by other conventional methods represent the pathological difference of the regions? By disentangling the representation, did the representations obtained by the proposed method become more valid from a pathological point of view?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The reviewer believes the idea of the disentanglement for the divergence of attention is novel but the role of the labeled images used in the training of cell nuclear segmentation is not small. In addition, the analysis of the resultant attention is not satisfactory. It would be better to compare the proposed method with multi-scale transformers in the introduction to clarify the strength of the proposed method. Existing methods that represent WSIs with multi-scale transformers aim to represent not only local features at the level of individual cells but also global features between cells and the cytoplasm. Attention matrices of more distant tokens are computed for the representation of the latter global features and would have some relationships with the diversified attentions obtained by the proposed method.",
            "summary_of_the_review": "Because of the weakness described above, the reviewer rates this paper just below the borderline. I am willing to rase the rating if it is appropriately explained the reason why the proposed method can be classified as a SSL method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper161/Reviewer_vgEj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper161/Reviewer_vgEj"
        ]
    }
]