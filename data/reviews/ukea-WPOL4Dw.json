[
    {
        "id": "B1gb6jezIRw",
        "original": null,
        "number": 1,
        "cdate": 1666425767174,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666425767174,
        "tmdate": 1669445410278,
        "tddate": null,
        "forum": "ukea-WPOL4Dw",
        "replyto": "ukea-WPOL4Dw",
        "invitation": "ICLR.cc/2023/Conference/Paper4250/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a method to identify relevant numbers in text, replace them with smaller numbers that are more accurate to the LM, and use the input-output pair to search for a linear system.",
            "strength_and_weaknesses": "Strength:\n1. Novelty is high in my opinion. Numerical instability is a long-standing problem in neural-based reasoning. While most research attempts to use neural-symbolic methods, this paper figured out a way to identify relevant numbers and outsource the finding solution steps to classicial methods.\n2. Experiments are through with good analysis. Comprehensive ablation studies are performed to identify the contributions of variations in LMs and different linear system search methods.\n\nWeakness:\n1. Currently the method is limited to linear system, which probably is OK since most arithmetic in natural language text are not complex. But there are areas where non-linear solutions are required (imagine solving high-school math), how can the method be extended to that case?\n2. It would be great if the authors can show some failure cases. I wonder which step leads to the most failures? Is it finding the right number in the text, or the search for system solution?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with concise explanations and good amount of illustrations. Quality is high with good novelty. Not reproducible though as code is not provided, and it is not trivial to reproduce the paper from scratch.",
            "summary_of_the_review": "The proposed method is novel and thoroughly evaluated, and proven to be effective. It is however not clear if the method can be extended to more complex numerical reasoning tasks.\n\n-----------After rebuttal--------------\n\nI am happy with the rebuttal and I still recommend acceptance for this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_Dv6a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_Dv6a"
        ]
    },
    {
        "id": "WfxKEJaqEIy",
        "original": null,
        "number": 2,
        "cdate": 1666522674268,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666522674268,
        "tmdate": 1670964519602,
        "tddate": null,
        "forum": "ukea-WPOL4Dw",
        "replyto": "ukea-WPOL4Dw",
        "invitation": "ICLR.cc/2023/Conference/Paper4250/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes SOLIS, where during model inference, complex numbers are substituted with simpler numbers (anchors) from 1 to 20, the language model outputs the arithmetic between the anchors, and through these, the original answer of the arithmetic between the complex numbers are derived through solving linear systems. The operand (to be substituted by anchors) is chosen from the input text through the significance of their perturbations on the model performance. Finally, after having the operands and their anchors, the required output is generated through either one of the proposed 3 approaches to solving linear systems - analytical, search-based, and heuristics. \n\nAlthough missing some crucial baselines (discussed in the section below), the authors show that LMs can improve their performance for numeral prediction with the incorporation of their SOLIS framework.\n\n## Edit After Reviewing Author Response\n\nAfter reviewing the author response, I have updated my recommendation to a weak rejection of the paper.  This is primarily due to the clarification of the goals by the authors and incorporating (and demonstrating improvement on) the GenBERT baseline. The reason for non-acceptance despite the paper having some novel contributions and being well-written has to do with my initial comment on the lack of the proposed model to work with both textual and numeric data which is something (also acknowledged by authors in the response) that some previous works are able to do (see references in `summary of review` below).\n",
            "strength_and_weaknesses": "## Strengths:\n1. The ideation is interesting - authors leverage the fact that LMs are good at arithmetic for numerals 1-20 that are highly frequent in the training corpus and use this for eliciting arithmetic operations between more complex numbers.\n\n\n2. The ideas are clearly expressed in the writing/figures/tables and thus are easy to grasp.\n\n## Weakness:\n\nAmong the two weaknesses of the paper, the first is a deal-breaker. The authors establish the premise for the need of numeric comprehension in language models as 1) numeric reasoning forms the crux of understanding text, tables, and knowledge bases 2) extrapolation for numerals outside of the pre-training corpora. Here, the approach that the authors have taken simply appends a system on the top of untouched LMs that does the computations for the LM. This appendage does improve performance when the task is strictly to output a number. However, tracing this back to the original premise of the paper, unless the representations of numbers of these LMs change or adapt, this appendage makes no difference in the understanding of numerals by the LM. Case in point, a DROP dataset instance where the LM has to understand numerals in the paragraph but the output is textual - I believe this appendage provides no significance in the understanding of numerals. This is further corroborated by the paper itself - section 5.3 - where the authors state that for their experimentation, they have specifically chosen instances of the DROP dataset that have numeric outcomes.\n\nTwo essential works that also aim to improve numerical comprehension and also evaluate themselves on the DROP dataset (including instances where the output is textual), one of which is cited in the paper, is not used as baselines:\n\n1. Geva, M., Gupta, A., & Berant, J. (2020). Injecting numerical reasoning skills into language models.\n\n2. Sundararaman, Dhanasekar, et al. \"Methods for Numeracy-Preserving Word Embeddings.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and clarity of the paper is good - it is written and presented well. The proposed solutions are original, however, are not pertinent to the problem statement.",
            "summary_of_the_review": "The lack of important baselines I can overlook as the idea is interesting, however, I base my recommendation on the first part of the paper weakness - the offered solution is antithetical to the premise of the paper. If the problem statement is that language models struggle to understand numbers that appear in tables, text, and knowledge bases, the solution cannot be to approximate a calculator as an addition to the inference chain. To elaborate, take a supposed instance of any QA dataset (SQuAD or DROP) where the context is \"X scored 4 goals while Y managed to only score 3\", the question is \"Who scored more goals?\" and the answer is textual - \"X\". The framework suggested can compute arithmetic between 4 and 3, but does not help the LM better understand what 4 and 3 are such that it can make an informed decision. There are several lines of work that work on the premise that this paper takes, but provide interventions that can be applied to both textual and numeric data:\n\n1. Spithourakis, Georgios, and Sebastian Riedel. \"Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers.\" Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018.\n\n2. Jiang, Chengyue, et al. \"Learning Numeral Embedding.\" Findings of the Association for Computational Linguistics: EMNLP 2020. 2020.\n\n3. Zhang, Xikun, et al. \"Do Language Embeddings capture Scales?.\" Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. 2020.\n\n4. Geva, M., Gupta, A., & Berant, J. (2020). Injecting numerical reasoning skills into language models.\n\n5. Sundararaman, Dhanasekar, et al. \"Methods for Numeracy-Preserving Word Embeddings.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_qbBT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_qbBT"
        ]
    },
    {
        "id": "jItdnZFAzVl",
        "original": null,
        "number": 3,
        "cdate": 1666631899962,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631899962,
        "tmdate": 1670343014897,
        "tddate": null,
        "forum": "ukea-WPOL4Dw",
        "replyto": "ukea-WPOL4Dw",
        "invitation": "ICLR.cc/2023/Conference/Paper4250/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The author proposed a novel method combining a large language model with solving linear systems to improve the calculation over large number performance.\nContributions:\n1. A novel approach that takes advantage of neural networks' ability to solve a simple math problem, and generalizes the ability through number substitution and linear system solving.\n2. Natural language model is agnostic so the approach is generalizable to different models.\n3. Have good performance gain even with zero shot\n\n---------\n\nUpdate about my final recommendation: \nI am convinced with the rebuttal material and happy to accept this paper. \n",
            "strength_and_weaknesses": "Strengths: \n1. The paper writing is very clear and the story is convincing.\n2. The study using different solving strategies demonstrates the approach can be configured toward different needs\n3. The experimental studies show it is pretty generic and can improve different neural baselines\n\nWeakness:\n1. The experiment setup is a little bit vague. There are multiple setups, including zero-shot, few shots, and fine-tuning. However, it is hard to tell which setup is used to perform one experiment just from the figure and the caption in the evaluation section. \n2. Expressiveness: The system can only handle up to the linear system, and cannot generalize to aggregation, or recursion yet. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: 3.5/5 \nPro: The mathematics and paper writeup is clear, and motivates people well in the introduction section.\nCon: The experimental presentation can still be improved. For example, label the experiments with ZS, FS, and fine-tune setups.\n\nQuality: 4/5\nThe author's statements are well supported by both theory and experiments. To make the evaluation more complete, maybe an ablation study can be performed to see how much performance can be obtained under the ZS, FS, and fine-tuning setup, and compare the time consumption and accuracy using different solving strategies. \n\nNovelty: 4/5\nThe idea of converting hard numerical problems into easy ones, and using a linear system to abstract the internal reasoning structure is very interesting. ",
            "summary_of_the_review": "The author proposed a novel method combining a large language model with solving linear systems to improve the calculation over large number performance. The experiment shows this model-agnostic approach can improve the performance of different natural language models by a non-trivial amount. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_Lb3T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_Lb3T"
        ]
    },
    {
        "id": "bMl-qj3fYFc",
        "original": null,
        "number": 4,
        "cdate": 1666794278224,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794278224,
        "tmdate": 1666794278224,
        "tddate": null,
        "forum": "ukea-WPOL4Dw",
        "replyto": "ukea-WPOL4Dw",
        "invitation": "ICLR.cc/2023/Conference/Paper4250/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduced a clever and simple method that improves the arithmetic capabilities of LMs.",
            "strength_and_weaknesses": "The method is easy to understand and implement and offers improvements on various reasoning datasets. It needs to be clarified, though, whether there exists a path toward applications beyond systems of linear equations. I also would like to know whether the method helps when combined with majority voting.\n\nThe Drop dataset used for evaluations in this paper is excellent, but its arithmetic is quite restricted. Are there any non-synthetic NLP datasets that have systems of linear equations? How much gain would we see on GSM8K and MATH? Can you perform a qualitative analysis for Drop, GSM8K, and MATH, highlighting the types of errors that were fixed?",
            "clarity,_quality,_novelty_and_reproducibility": "The substitution method proposed by the authors is interesting, novel, and easy to reproduce. I am less convinced about its usefulness, though.",
            "summary_of_the_review": "The authors introduced a clever and simple method that improves the arithmetic capabilities of LMs. Assessing the full usefulness of this method may require more work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_8HSz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_8HSz"
        ]
    },
    {
        "id": "-Ukb3Aek1q",
        "original": null,
        "number": 5,
        "cdate": 1670824235327,
        "mdate": 1670824235327,
        "ddate": null,
        "tcdate": 1670824235327,
        "tmdate": 1670824235327,
        "tddate": null,
        "forum": "ukea-WPOL4Dw",
        "replyto": "ukea-WPOL4Dw",
        "invitation": "ICLR.cc/2023/Conference/Paper4250/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method to infer the arithmetic expressions performed by a language model while solving a mathematical reasoning question with a chain-of-thought style output. The inferred expressions are then calculated outside of the LM. This aims to improve the performance of LLMs on mathematical reasoning tasks by increasing the accuracy on arithmetic calculations. ",
            "strength_and_weaknesses": "Strength:\n\n- The method itself appears to be novel. It involves extracting the relevant numbers and substituting large numbers in the chain-of-thought answer with random small numbers where the LM can reliably perform calculations on. By sampling a set of these input and output correspondences, the authors specify and solve a linear system of equations to elicit the arithmetic relationships. \n\n- The method leads to performance gains in the settings evaluated in this paper.\n\nWeakness:\n\n- The main issue with this work is that this method is a very complicated way to do something that is much simpler in comparison (simple arithmetic calculations). Despite its complexity, the method does not guarantee that the correct calculations are extracted and does not scale well to more complex expressions. It seems that similar performance gains can be achieved by prompting the LLMs to output its reasoning steps in the format of mathematical expressions, and sending the evaluation to an external calculator. This approach was already explored in [1] with some success. One can also imagine finetuning the model to output well-formatted arithmetic expressions in a task-agnostic way. A version of this was explored in [2] on the GSM8k dataset. These alternatives are much simpler and more general than the proposed approach, and the paper has not benchmarked against these simpler approaches to show that it offers a meaningful improvement. \n\n- The idea of formalizing the reasoning steps of a LM through perturbing its output and inferring the relationships may be an interesting framework, but there is no discussion of extending this particular approach to something that cannot be readily achieved by simple methods. \n\n[1] https://arxiv.org/pdf/2201.11903.pdf\n\n[2] https://arxiv.org/pdf/2110.14168.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and it's nice that the paper includes a discussion on its limitations. However, there are important benchmarks missing and thus the evaluation is incomplete. ",
            "summary_of_the_review": "The paper presents an interesting idea for eliciting the mathematical expressions that the LM is implicitly computing, and offloading such computations to an external calculator. However, the proposed approach is extremely complicated compared to the problem that it's trying to solve, and does not generalize well to other situations nor scale to more complex arithmetic expressions. There are simpler and more general approaches that are likely to perform well and these alternatives are not benchmarked. Therefore I think the current paper does not convincingly demonstrate the value of the proposed approach.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_AWSU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4250/Reviewer_AWSU"
        ]
    }
]