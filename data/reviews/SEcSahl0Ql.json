[
    {
        "id": "xiRw8nfxeN",
        "original": null,
        "number": 1,
        "cdate": 1666641982638,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641982638,
        "tmdate": 1666641982638,
        "tddate": null,
        "forum": "SEcSahl0Ql",
        "replyto": "SEcSahl0Ql",
        "invitation": "ICLR.cc/2023/Conference/Paper6328/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes using a neural network to iteratively repair sequential circuits represented as And Inverter Graphs (AIGs) against temporal logic specifications. The architecture is a variation of a transformed that separately takes as input the circuit and the specification. The specification is also separated into its assumptions and its guarantees. The network is then trained by (i) sampling circuits and specifications from well established benchmarks (ii) randomly introducing errors by changing the textual representation of the AIG.\n\nEmpirical results suggest that this technique is indeed able to correct incorrectly synthesized specifications and that repeated applications further improve performance. Importantly this can be used in combination with neural network based synthesis of the initial circuit.",
            "strength_and_weaknesses": "# Strengths\n1. The problem considered is a very important and difficult problem with a wide range of applications. Further, neural approaches make sense since in practice scalable synthesis in electronic design automation involves specializing the synthesis software by hand.\n\n2. The technique empirically improves synthesis on established benchmarks. This is measured as Levenshtein distance in the AIGER format which intuitively captures a degree of close-ness.\n\n# Weakness\n\n1. The technique seems to be limited to incredibly small circuits.\n    - To give context, the upper limit on 61 symbols is easily exceed by small adders (a naive adder using the popular py-aiger tool uses 65 symbols.)\n\n2. Unconvinced by human generated error argument.\n   - Having worked with AIGER a fair amount, most large circuits I create are done using tools (e.g., py-aiger) and not by hand. Thus, the argument that the introduced errors are natural seems odd.\n   - A more common setting is that the specification (or unit tests) used when designing the circuit were slightly wrong, either because:\n     1) It was missing assumptions.\n     2) It was missing guarantees.\n     3) Used bounded time horizon reasoning.\n   - I would be much more interested in a neural repair that could fix this more common class of errors.\n\n3. Perhaps I missed something, but there seems to be a missing comparison with non-neural techniques. As the authors point out, this is a very mature field, so it would be have been nice to see a detailed comparison.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the technique is certainly novel. I believe that the basic architecture and dataset could be reproduced from the description given in the paper.\n\nThe clarity of the baseline comparison could be improved (see weakness above).",
            "summary_of_the_review": "Overall, I believe the future potential of the propose method out weight the clear limitations on scale and questionable motivation for the error insertion. Circuit repair (and circuit synthesis) are computationally difficult problems in general and the field largely relies on hand tuned heuristics. Supporting data-driven techniques is a natural means to accelerate synthesis in specific target domains.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6328/Reviewer_NdwS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6328/Reviewer_NdwS"
        ]
    },
    {
        "id": "n7IFgPumQD",
        "original": null,
        "number": 2,
        "cdate": 1666699968369,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699968369,
        "tmdate": 1666700166397,
        "tddate": null,
        "forum": "SEcSahl0Ql",
        "replyto": "SEcSahl0Ql",
        "invitation": "ICLR.cc/2023/Conference/Paper6328/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the application of transformer models to repairing circuits given formal specifications. The transformer takes a faulty circuit represented in AIGER format and a specification in LTL, and outputs a circuit that hopefully satisfies the specification. The paper also comes with a detailed description of the generation process of datasets.",
            "strength_and_weaknesses": "Strength:\n\n+ The approach is technically sound. While there have been a variety of work on generating logical formulas, formal specifications, or formal proofs with language models these days, the problem this paper is targeting (with language models) looks novel to me.\n\n+ The empirical evaluation is extensive, and validates the proposed framework. The difficulty measures and Levenshtein distance help better understand the capability of language models on the task.\n\nWeakness:\n\n- The specifications are restricted to 5 inputs and 5 outputs, no more than 12 properties. Did you observe a degradation in performance as the size of specifications increases?\n\n- What are the advantages of hierarchical transformers over vanilla ones? Although the experiments are extensive, I can't seem to find this particular dimension of evaluation. Also, would fine-tuning a pre-trained language model be as good as training a hierarchical transformer from scratch?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. The approach looks novel to me. The authors promised the release of datasets and code for reproducibility. ",
            "summary_of_the_review": "The framework is well-designed and the performance looks good. I lean toward accepting the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6328/Reviewer_LxSj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6328/Reviewer_LxSj"
        ]
    },
    {
        "id": "4B8y5lO9Vyk",
        "original": null,
        "number": 3,
        "cdate": 1666736069182,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666736069182,
        "tmdate": 1666736069182,
        "tddate": null,
        "forum": "SEcSahl0Ql",
        "replyto": "SEcSahl0Ql",
        "invitation": "ICLR.cc/2023/Conference/Paper6328/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper extends hierarchical transformers (neural network models for\nsynthesising circuits from linear time specifications)  to build models  for\nthe repair of circuits so that they satisfy a certain linear time property. The\nmodels, which are also used in an iterative procedure to do reactive synthesis,\nare trained on a novel algorithm for introducing faults to a dataset.",
            "strength_and_weaknesses": "+ Good empirical analysis that shows the efficacy of the proposed method in\ncircuit repair and its superior performance in reactive synthesis when compared\nwith the state-of-the-art.\n\n- Highly incremental to previous work (the model proposed is a variation of the\n  work from Li et al. (2021) and the data generation algorithm is a standard\n  fault injection algorithm).\n\n- Some comparisons with related work are shallow. For instance, the\n  advantages/disadvantages of the present method to symbolic synthesis are not\n  discussed.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper  is not entirely self contained with many of the details given in the\nAppendix. Though it would be impossible to fit everything in the page limit, I\nthink that some additional details on major concepts could be given; for\ninstance, the reactive synthesis problem is never defined and the term\n'attention' is used a number of times but never explained. The novelty of the\npaper is not particularly strong as it is highly incremental to Le at al.\n(2021).",
            "summary_of_the_review": "The paper introduces neural models for circuit synthesis which exhibit gains\nover the state-of-the-art, albeit being highly incremental to previous work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6328/Reviewer_ocom"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6328/Reviewer_ocom"
        ]
    },
    {
        "id": "txJbBcecY30",
        "original": null,
        "number": 4,
        "cdate": 1667238118210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667238118210,
        "tmdate": 1667238118210,
        "tddate": null,
        "forum": "SEcSahl0Ql",
        "replyto": "SEcSahl0Ql",
        "invitation": "ICLR.cc/2023/Conference/Paper6328/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Given a formal specification in LTL, this paper introduces a transformer architecture that aims to transform a defective circuit into a repaired one, in accordance to the spec.   The primary contribution is in the transformer neural architecture, which they call the separated hierarchical transformer since it has separate local layers that do not share model parameters. \n\nIn addition, they introduce a data generation procedure that models human errors to encourage generalization to more complex specifications.",
            "strength_and_weaknesses": "### Main strengths:\n\n- Well motivated problem\n- Mostly well written and easy to follow\n- Comprehensive experimental analysis (with some caveats)\n\n### Main weaknesses:\n\n- Most of the value of this contribution rests upon whether the following causal claims are true and well-justified: the new architecture and/or the data-augmentation procedure caused the improvements in performance of state of the art.  Despite a number of different experimental analyses in the paper, determining this is not straightforward.  In particular, I cannot see where if anywhere the number of parameters are controlled for.  The paper does say that using separated heads leads to an increase in the number of parameters, but I do not see any evidence in this paper to suggest that performance increases over previous methods is not attributed simply to this network being larger.  Also, the experimental results do not allow us to distinguish whether improvements are from the model changes or from the data changes.\n- I have some reservations about the use of Levenshtein distance as a metric for the quality of a synthesized circuit.  Obviously there is no \u201cright\u201d metric, and Levenshtein may be used in prior work, but a more semantic property could be used in addition to convey improvement.  For instance, one could try to look at (some approximation of) the increase or decrease in the number of satisfying traces",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well written and straightforward to follow.  The exception are the figures, which would benefit from more information in the captions.\n\nThe novelty of this paper is limited.  The main contributions are a modest variant on an existing hierarchical transformer architecture, and a straightforward data augmentation procedure.  Individually or together these do not seem substantially novel, but that by itself is not necessarily a problem.\n\nThe description is precise enough to attempt to reproduce this work.  The authors state in several places that they will produce the source code for this work, which will improve reproducitibility further.\n\n### Questions\n\n- Clarify \u201cfiltered out samples that exceed our model restrictions\u201d\n- It\u2019s not clear whether \u201cafter last iteration\u201d in Table 1 means after exactly 2 iterations of after some arbitrary number.\n- In Figure 4 has an \u201cerror\u201d label purple, but I do not see any corresponding purple in the graph itself.  Is this just too small to see on the graph?\n- Clarify \u201caccuracy\u201d in Figures as semantic accuracy and provide explicit meaning to [0, 1] range.",
            "summary_of_the_review": "- Well written paper with comprehensive experimental analysis\n- Limited novelty\n- Would like to see much more evidence that claimed contributions are responsible for improved performance",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6328/Reviewer_Y3T4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6328/Reviewer_Y3T4"
        ]
    }
]