[
    {
        "id": "1bXF0fw1cBx",
        "original": null,
        "number": 1,
        "cdate": 1666416650393,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666416650393,
        "tmdate": 1666416650393,
        "tddate": null,
        "forum": "IM4xp7kGI5V",
        "replyto": "IM4xp7kGI5V",
        "invitation": "ICLR.cc/2023/Conference/Paper5168/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the maximum-margin bias of gradient flow on quasi-homogeneous networks under data separation. In contrast to homogeneous networks, different parameters in a quasi-homogeneous network are allowed to have different homogeneity factors; this model can represent bias terms, skip connections, normalization layers, etc. This paper shows that the final maximum-margin bias is determined by the parameters with the maximum homogeneity factor, which naturally generalizes prior results on homogeneous networks. Specifically, it is proved that the final solution is a KKT point of a norm-minimization problem for the parameters with the maximum homogeneity factor, under a data separation condition. It is also discussed that quasi-homogeneous maximum margin can degrade robustness and induce neural collapse.",
            "strength_and_weaknesses": "I think this is a nice paper. The quasi-homogeneous model is very expressive and allows common layers used in deep learning, which can be an interesting model to analyze. The proof naturally generalizes prior analysis on homogeneous networks, and reveals that gradient flow will asymptotically focus on the parameters with largest homogeneity factors. I have the following comments:\n1. Part of Assumption A2 can be included in the definition of quasi-homogeneity, otherwise the definition may look too flexible. For example, given a deep linear network with no bias, we can let lambda be 1 for the last layer and 0 for other layers; this does not violate the definition, but the boundedness condition in A2 does not hold.\n2. Since some $\\lambda_i$ can be 0, $\\hat{\\theta}$ may not be uniquely defined. Can you make its definition more rigorous?\n3. Can you explain how conditional separation (Assumption A7) is used in the proof?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is novel and well-written.",
            "summary_of_the_review": "This paper introduces an expressive model of quasi-homogeneous networks, and analyzes the maximum-margin bias of gradient flow on such models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5168/Reviewer_UHdE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5168/Reviewer_UHdE"
        ]
    },
    {
        "id": "_c3rTGjQn5l",
        "original": null,
        "number": 2,
        "cdate": 1666466822281,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666466822281,
        "tmdate": 1670098906963,
        "tddate": null,
        "forum": "IM4xp7kGI5V",
        "replyto": "IM4xp7kGI5V",
        "invitation": "ICLR.cc/2023/Conference/Paper5168/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the implicit bias of gradient flow with an exponential loss on a class of neural networks called \u201cquasi-homogeneous networks\u201d. This class, which is introduced in the paper, is very rich and includes, e.g., ReLU networks with bias terms and residual connections. The implicit bias of such networks is not implied by previous results, which consider mostly homogeneous networks. The results hold for both binary classification, and multiclass classification with the cross-entropy loss. The implicit bias of quasi-homogeneous networks is characterized by asymmetric margin maximization (which involves only a subset of the parameters). The authors also conjecture that an even more general property holds (a certain cascading weight minimization property). Also, they obtain some implications in the context of robustness and neural collapse. ",
            "strength_and_weaknesses": "The implicit bias of gradient flow in neural networks has been extensively studied in recent years. It is believed that understanding the implicit bias may be a key to understanding generalization in deep learning. In the context of classification with exponential-type losses, the most prominent prior result is by Lyu and Li (2019), and it implies a bias towards margin maximization in parameter space (or at least towards stationary points of the margin maximization problem) in homogeneous networks. The current work extends this result to a large family of non-homogenous networks, which includes ResNets and networks with bias terms. I believe that this contribution is very significant, as it makes a step towards understanding the implicit bias in practical settings. Therefore, I think that it is a very strong paper. \n\nMy main comment is that I think that the paper can benefit from a more detailed discussion on the assumptions made in Theorem 4.1. The authors give several examples of quasi-homogeneous networks, both in the main text and in the appendix, and convince the reader that the class of quasi-homogeneous networks is very rich and includes \u201cnearly all neural networks with homogeneous activations\u201d. While I agree with this statement, I think that the more relevant question is when the other assumptions hold. For example, the authors discuss \u201cnetworks with residual connections\u201d as an example of quasi-homogeneous networks. However, it seems that these networks do not satisfy Assumption A2, since f is not bounded on the unit \u201csphere\u201d (in this example, the lambda of the first layer is 0 and of the second layer is positive, hence the unit sphere includes all networks where the second layer\u2019s weight is 1, and by increasing the first layer we can get arbitrarily large outputs). I suspect that many readers might get the impression that since the family of quasi-homogeneous networks is very rich, then the theorem essentially holds in all interesting settings, but it does not seem to be the case. Therefore, such a discussion, in my opinion, is important. A more concrete question for discussion is: when does the theorem apply in ResNets? (I suppose that if the residual block includes only trainable parameters, both skipping and non-skipping, and does not include the identity matrix, then the assumptions should hold if the skip-connections are necessary for separating the data and assuming directional convergence, right?)\n\nSome additional comments:\n- In the proof of Theorem 4.1, you show that the KKT point satisfies the MFCQ condition. It implies that the KKT conditions are necessary for optimality, right? I think that it is a useful fact and it might be worth mentioning in the main text.\n- Page 6: \u201cSee App. C for a discussion on how to determine the highest-order\u2026\u201d \u2014 Do you mean App. A?\n- Page 7: \u201c$(w_3)$\u201d should be \u201c$(w_1)$\u201d?\n- In Sec. 5, it might be worth discussing that also homogeneous linear networks may converge to non-robust solutions. For example, diagonal linear networks prefer sparse solutions and do not maximize the $\\ell_2$ margin.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, the results are novel and the experiments are well-documented.",
            "summary_of_the_review": "As I discussed above, I think that it is a strong paper and recommend \u201cstrong accept\u201d.\n\n-----------------------------------------------\n\nAfter reading the authors' response, I stick with my high score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5168/Reviewer_i1KS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5168/Reviewer_i1KS"
        ]
    },
    {
        "id": "IBbmqhCqEC8",
        "original": null,
        "number": 3,
        "cdate": 1666535122687,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535122687,
        "tmdate": 1668775638839,
        "tddate": null,
        "forum": "IM4xp7kGI5V",
        "replyto": "IM4xp7kGI5V",
        "invitation": "ICLR.cc/2023/Conference/Paper5168/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies the implicit bias of quasi-homogeneous networks. Under a conditional separation assumption, it is proved that gradient flow converges to a KKT solution of a certain minimum norm optimization problem. The latter norm depends on a subset of the parameters such as the last layer bias of a fully connected network. Based on the main result, it is shown in a certain setting that the inductive bias of quasi-homogeneous networks is towards non-robust solutions. Furthermore, it is proved that the neural collapse phenomenon occurs for quasi-homogeneous models under certain assumptions.",
            "strength_and_weaknesses": "Strengths:\n1. Strong and novel theoretical results.\n2. Novel definition of quasi-homogeneous networks that may help to understand neural networks in new settings.\n3. Interesting experiments on robustness.\n\nWeaknesses:\n\nThe major weakness is that Theorem 4.1 in its current form is inconsistent. Since the quasi-homogeneous parameter is not unique, for a given network the theorem implies that gradient flow converges to different minimum norm problems. This is impossible, unless:\n1. All such problems are equivalent, or:\n2. Given the assumptions, there is a unique parameterization.\n\nThe authors provide several examples where (1) or (2) above hold, but there is no general proof that they hold for all quasi-homogeneous models. If the theorem implies that either (1) or (2) hold for all quasi-homogeneous models, then this should be stated in the theorem. \n\nAnother issue with the inconsistency is that Definition 3.1 holds for all x. However, the main result only concerns with the training set, therefore it seems that the definition can be only with respect to x in the training set. If we change definition 3.1 to hold for x in the training set, this changes the possible parameterizations of quasi-homogeneous models and the main result. It is not clear which definition should be used.\n\n\nAnother weakness is that conditional separation assumption seems strong. There should be more discussion on this assumption and maybe proofs that it holds in certain realistic settings.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality. Most of the theoretical results are strong and solid, except the inconsistency of Theorem 4.1.\n\nNovelty. The definition of quasi-homogeneous networks and the results on their inductive bias are new.\n\nClarity. Mostly the paper is well-written.\n",
            "summary_of_the_review": "This paper presents new and interesting results on the inductive bias of non-homogeneous networks. However, I cannot accept this paper in its current form, because one of the theoretical results is currently inconsistent. If the authors can clarify the statements and write a consistent theorem then I will increase the score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5168/Reviewer_CPzm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5168/Reviewer_CPzm"
        ]
    },
    {
        "id": "JX2P7Mo_hs",
        "original": null,
        "number": 4,
        "cdate": 1666873611760,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666873611760,
        "tmdate": 1669969365972,
        "tddate": null,
        "forum": "IM4xp7kGI5V",
        "replyto": "IM4xp7kGI5V",
        "invitation": "ICLR.cc/2023/Conference/Paper5168/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the max margin bias that arise in the gradient flow training procedure of *quasi-homogeneous* neural networks. Quasi-homogeneous networks can be thought of as a generalization of homogeneous networks that are previously studied in the literature (e.g., Lyu & Li (2019) and Ji & Telgarsky (2020)), and they contain a significantly wider range of networks such as ones with bias terms, skip connections, normalization layers, etc. \n\nFor this general class of networks, the paper extends the results by Lyu & Li (2019) to show that the direction at which the network parameter vector converges to corresponds to a first-order KKT point of a (nonconvex) optimization problem (denoted as (P) in the paper). The key difference from Lyu & Li (2019) is that the objective function in (P) is no longer the standard $\\ell_2$ norm; instead, it is the $\\ell_2$ norm of a *subset* of parameters defined by the quasi-homogeneity.\n\nThis result highlights that the gradient flow dynamics is biased toward achieving the maximum margin with respect to only a subset of network parameters. This \"asymmetry\" leads to some interesting consequences, namely that 1) it can hurt robustness of the resulting classifier and 2) it can provide an explanation to the recently observed *neural collapse* phenomenon.",
            "strength_and_weaknesses": "Strengths\n\n1. This paper generalizes the existing theoretical research on homogeneous networks to a greater extent, and the newly introduced class of quasi-homogeneous networks is very general. Hence, the paper develops a theory that applies to many practical networks. \n\n2. Through this extension, the paper uncovers that the max margin bias of gradient flow only applies with respect to a subset of parameters, which is quite an interesting discovery. Moreover, this observation can help explain the empirically observed neural collapse phenomenon.\n\n3. Although I believe assumption A7 is unrealistic in some practical scenarios (see the weaknesses part), the authors present a motivating example (Section 5) that allows us to imagine what might happen without the assumption. They propose a neat conjecture (Conjecture 5.1) which naturally extends Theorem 4.1 to settings without A7. This uncovers an interesting future research direction.\n\nWeaknesses\n\n4. Among the assumptions required for Theorem 4.1, the one that seemed the most unrealistic to me was A7, namely that the highest-degree parameters must contribute to the classifier for it to separate the dataset. The assumption does not hold in some practical scenarios, for example when we have a 1-hidden-layer fully-connected network with bias terms (hence the bias $b^2$ of the output layer is the highest-degree parameter) and the dataset can be separated without the use of $b^2$.\n\n5. As noted by the authors, the diagonal PSD matrix $\\Lambda$ that defines quasi-homogeneity may not be unique. For example, for networks with normalization layers, one can choose *any* values of $\\lambda$ for scale-invariant parameters. The authors discuss in Appendix A on how we can disambiguate between different possible choices of $\\Lambda$ by leveraging assumption A7 and taking a closer look at the optimization problem (P). However, this is only a \"proof by example\" and the paper does not concretely prove that there is only one possible choice of $\\Lambda$ for which Theorem 4.1 applies. Also, it is unclear how we can disambiguate the equivalently working values of $\\Lambda$ without the help of A7. I believe this is the main downside of this paper. \n\nMinor comments\n- To me, the word \"highest-order\" for parameters with $\\lambda_{\\max}$ seems a bit counterintuitive, because these parameters are often those with the lowest \"polynomial degree.\" For example, for $f(x;\\theta) = w^2 \\sigma(w^1 x + b^1) + b^2$, the $b^2$ term with the smallest \"degree.\"\n- In \"Geometric properties\" paragraph on page 4, $\\nabla_\\alpha f(x; \\psi_\\alpha(\\theta)) = \\nabla_\\alpha e^\\alpha f(x;\\theta)$?\n- From Lyu and Li (2019) I believe A2 is stated for a fixed given value of $x$, but the paper doesn't clearly state that. As a result, I once got confused why $|f(x;\\theta)| \\leq k$ must hold for all $x$.\n- It'd be good to put a label (P) next to the optimization problem in Theorem 4.1?\n- At the end of Section 4, \"See App C\" -> \"See App A\".\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is mostly clear. The paper is well-written and I found it easy to digest.\n\nAnalyzing the class of quasi-homogeneous networks looks novel, as far as I can tell. In terms of proof techniques, the paper indeed builds upon the techniques developed in Lyu and Li (2019) but it looks like the authors had to overcome some challenges that arise from quasi-homogeneity. However, I did not have the time to fully examine the proofs.\n",
            "summary_of_the_review": "The paper investigates the max margin bias in a very general class of neural networks referred to as quasi-homogeneous networks. The investigation uncovers some interesting and unexpected bias, namely the implicit bias applies with respect to only a subset of parameters. Also, this theoretical result has some important implications to neural collapsing which is of interest.\n\nWhile the paper presents interesting theoretical results, one crucial downside is that the quasi-homogeneity is not completely well-defined. For a given network there can be multiple \"subsets\" of network parameters, and fully disambiguating these cases could use some additional effort.\n\nOverall, my current rating is positive but not too positive; I would be happy to raise my score based on the authors' response.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5168/Reviewer_bFVN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5168/Reviewer_bFVN"
        ]
    }
]