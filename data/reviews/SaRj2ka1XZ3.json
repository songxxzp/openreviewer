[
    {
        "id": "6zAQ8BkcCf",
        "original": null,
        "number": 1,
        "cdate": 1666579742026,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579742026,
        "tmdate": 1666633027012,
        "tddate": null,
        "forum": "SaRj2ka1XZ3",
        "replyto": "SaRj2ka1XZ3",
        "invitation": "ICLR.cc/2023/Conference/Paper3522/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors propose using LMs to generate programming problems and solutions. These synthetic problems and solutions then are used to fine tune LMs for program generation task. This approach leads to improved test accuracy in program generation task. ",
            "strength_and_weaknesses": "Strengths:\n\n- A novel approach to improve LM-based program generation by first generating synthetic problems and solutions and then finetuning LMs on the generated problems.\n- Highly improved test results\n- Ablation studies of model and interpreter distillation\n\nWeaknesses:\n\n- Authors cannot finetune Codex and therefore there are no results for using the authors' approach to improve state of the art results for the problem. See Figure 12 for the gap of the Neo models even with fine tuning vs Codex models. It is possible (likely?) that Codex models will benefit from finetuning using the additional synthetic problems/solutions, but we don't know for sure. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written.\nI believe the approach is novel.\nSince authors plan to release the dataset of synthetic puzzles and solutions and based on the descriptions of their work in the paper, the results should be reproducible.\n\n---------------------------------------------------------------\nICLR review form does not provide a section for comments/questions about the paper. I am going to post them here:\n- It would be good to have some discussion of how this work could transfer to program generation from English prompts.\n- There is related work on generating buggy programs and fixes for them in the automated program debugging area. I wonder if this work could also generate dataset for buggy/incorrect puzzle solutions and correct puzzle solutions that could be used in automated program debugging. Also I wonder if there is a way to introduce the automated program debugger into the loop to improve either finetuning or prompts for the correct puzzle solution generation. Possible areas to explore in the future work.\n",
            "summary_of_the_review": "I believe the work presented here is useful for improving language model ability to generate programs. It seems to be novel, and it achieves marked improvements when used to fine tune LMs. There is a reasonable expectation that this research could improve state of the art results and also be applied in further advancing language model-based program generation. I recommend accepting this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3522/Reviewer_cLPb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3522/Reviewer_cLPb"
        ]
    },
    {
        "id": "73C2ZBuj6q",
        "original": null,
        "number": 2,
        "cdate": 1666664051058,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664051058,
        "tmdate": 1666664051058,
        "tddate": null,
        "forum": "SaRj2ka1XZ3",
        "replyto": "SaRj2ka1XZ3",
        "invitation": "ICLR.cc/2023/Conference/Paper3522/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes to perform data bootstrapping by using a performant language model generate prompts that specify puzzles and finetune models such data (after filtering for correctness). ",
            "strength_and_weaknesses": "Pros:\nThis method has some potential to augment data in order to improve model's general code understanding, generation and reasoning abilities. It is however similar to what Codex paper describes in their data generation process based on filtering with integration tests. However, this work focuses on puzzles.\n\nQuestions:\n- The format of such puzzles are quite specific. That is, it expects f(.) and g(.) such that f(g(x)) returns True. This format can also be simplified since g() seems to take no argument and returns a value. The simplified format can be 'assert f(' as prompt, then ask the model to generate the actual values. Or is the f(g(x)) format used for something more general? The paper shows a general function f that takes multiple arguments in Figure 3. Are the extra arguments simply ways to take in values? \n\nCons:\n- The experiments can be a bit more thorough. It makes sense that if we finetune on such puzzles, the performance on a held-out set of puzzles will be improved. While it is valid to focus on such puzzle generation as one of the tasks for evaluation, a more important question is whether it translates to general coding abilities. \n- A better experiment setup is to perform data bootstrapping; that is, we can generate samples and verify, then finetune on the puzzles, then measure the performance on the general code generation benchmarks such as HumanEval or MBPP. \n- What of type guardrails are used to detect that the puzzles are not trivial? For example, if the function looks complicated but due to some reason it always goes to the branch that returns true, then any function g() would solve such puzzle.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarify: The paper is quite well written\nQuality: This is an intriguing idea that has a good potential. But more experiments are needed\nNovelty: The data generation + filtering has been explored in the Codex paper. The puzzle format is explored in another work. This paper somewhat combines the idea of the two.\nReproducibility: Due to simplicity, I do not doubt that it should be quite reproducible.",
            "summary_of_the_review": "Overall, this is a nice method where the performance on the data bootstrapping method is quite expected. For completeness, the paper can push a bit further on studying the general code generation abilities after finetuning of puzzles, as described in the section above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3522/Reviewer_bB2P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3522/Reviewer_bB2P"
        ]
    },
    {
        "id": "djWiwtxArL3",
        "original": null,
        "number": 3,
        "cdate": 1666708851222,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666708851222,
        "tmdate": 1666708851222,
        "tddate": null,
        "forum": "SaRj2ka1XZ3",
        "replyto": "SaRj2ka1XZ3",
        "invitation": "ICLR.cc/2023/Conference/Paper3522/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method to improve a language model's (LMs) ability to generate programming puzzles. The primary contribution of the paper is algorithmic. Other contributions include the creation of an open-sourced large set of programming puzzles as well as experiments demonstrating the benefits of self-play (where a LM generates puzzles and then fine-tunes on them for improved test-set performance).",
            "strength_and_weaknesses": "Strengths\n  * The paper investigates an interesting and important problem involving LMs. Techniques for improving LM code-gen performance without significant labeling effort will have broad impact.\n  * The paper is well written. The main ideas (e.g., prompt setup) of the paper are described carefully with illustrative examples. I enjoyed reading the paper.\n  * The main algorithmic ideas are intuitively clear. The paper is technically sound.\n  * The experiments demonstrate the utility of the main algorithmic ideas (generation, verificationnn, fine-tuning) on the test set puzzles.\n\nWeaknesses\n  - A more detailed characterization of the program space representable by $f$ would be appreciated. It seems like f is a large subset (if not all) of Python but I'm not sure.\n  - The paper could better identify the novel algorithmic contributions. For example, is the prompt generation idea novel?\n  - While the paper does some analysis of the generated data via embeddings and UMAP, I would have liked to see a more detailed analysis and discussion of the generated puzzles. For example, how similar or different are they to the P3 train and test sets? Any overlaps?\n  - Although Figure 7 makes it clear that fine-tuning has plateaued, it is not clear why. A deeper analysis of this in conjunction with the analysis of the generated puzzles would have been nice (but outside the scope of the paper). Overall, this seems like an important line of work as the paper states and leaves for future work.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n  * I think the paper is very well written. The use of illustrative examples makes it easy to follow.\n\nQuality\n  * I think the paper is technically sound. However, some important aspects (e.g, program space of $f$) need clarification.\n\nNovelty\n  * The overall algorithmic novelty is a bit unclear to me. I think the paper could make this aspect a bit clearer.\n\nReproducibility\n  * There seems to be sufficient algorithmic and experimental detail. I have no major concerns here. Code would have been nice to have.\n",
            "summary_of_the_review": "The paper tackles an interesting and important problem. Ideas are mostly clear and experiments show improvements. Some aspects (algorithmic novelty, program space of $f$) need clarification which are reflected in my scores (and open to revision depending on author feedback).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3522/Reviewer_qoTN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3522/Reviewer_qoTN"
        ]
    },
    {
        "id": "6QxBAHe0V-u",
        "original": null,
        "number": 4,
        "cdate": 1666819932974,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666819932974,
        "tmdate": 1666819932974,
        "tddate": null,
        "forum": "SaRj2ka1XZ3",
        "replyto": "SaRj2ka1XZ3",
        "invitation": "ICLR.cc/2023/Conference/Paper3522/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper argues that programming puzzles and their machine verified solutions generated by a Language Model (LM)  can be used to improve the performance of the same LM. It presents a \u201cdata generation pipeline\u201d where puzzles for a training set are used as a prompt for the LM to generate new puzzles that complete the prompt. A filtered set of the generated puzzles are used as parts of another prompt for the LM to generate solutions for those puzzles. The generated puzzle-solution pairs are then verified to create a larger training set. Evaluation results show that GPT Neo models fine-tuned with this larger dataset provide improved results measured using the Pass@k metric.\n",
            "strength_and_weaknesses": "Strength\n\n- This work explores an interesting concept of using \u2018self-play\u2019-like strategy to improve language models for code generation and leverages existing work on programming puzzles to propose a method to demonstrate the effectiveness of the idea.\n- Dataset of 1M synthetic puzzles shared by this work could potentially help future research along this direction.\n\nWeakness\n\n- The data generation pipeline does not specify when to stop generating puzzles or solutions in a single iteration. During evaluation 2 iterations are used for Neo-models and 1 for Codex (due to fine-tuning limitations). The overall approach seems adhoc. How to determine the quality of generated puzzles?\n- It would be interesting to see if the performance improvements are tied to specific domains of problem.  This would also reflect on the generalizability of the proposed approach over problem instances.\n- \u201cIn each of the n iterations, the same LM is used for both generation and fine-tuning\u201d -  is the fine-tuned version replaces the base version in each iterations (i.e., is fine-tuned version always expected to be better in each iteration?)\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The contributions of the work are clearly stated. The descriptions are easy to follow. \n\nNovelty: This paper explores the concept of 'self-play' with respect to code generation tasks and provide a new direction of leveraging programming puzzles as a way to synthesize new 'instructive' samples. \n\nReproducibility: Evaluation uses publicly available dataset. The fine-tuned models and 1M datasets used for fine-tuning are not yet available. Details about the prompt used for data generation are presented.",
            "summary_of_the_review": "This work explores a potentially effective idea of improving language models for code. The proposed solution seems effective at improving models performance at python based programming puzzle task. However, the proposed data generation pipeline lacks  some details (e.g., determining number and quality of the generated samples, selection of fine-tuned LM in each iteration ). Also, the evaluation could be more informative if accuracy across the domain of problems are discussed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3522/Reviewer_t8tu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3522/Reviewer_t8tu"
        ]
    }
]