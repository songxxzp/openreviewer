[
    {
        "id": "wHAQRAoWYFk",
        "original": null,
        "number": 1,
        "cdate": 1666579863277,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579863277,
        "tmdate": 1666579863277,
        "tddate": null,
        "forum": "P-73JPgRs0R",
        "replyto": "P-73JPgRs0R",
        "invitation": "ICLR.cc/2023/Conference/Paper3275/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work provides a solid theoretical understanding of the role of graph convolutions (GC) in multi-layer neural nets. The theoretical analysis is based on the node classification problem of a non-linearly separable Gaussian mixture model coupled with a stochastic block model (SBM). Under some mild assumptions, the authors proved theoretically that, from various perspectives, GC is beneficial for improving the capability of multi-layer nets for the node classification task. Extensive experimental studies on both synthetic and real-world data are provided to validate the theoretical results.",
            "strength_and_weaknesses": "Strengths:\n+ The motivation to study theoretically the function of graph convolution operators in multi-layer networks is interesting;\n+ The theoretical findings as well as the corresponding analysis are beneficial for GNN researchers to get a better understanding on the effects of graph convolution layers; \n+ The authors show that for multi-layer networks equipped with graph convolutions, the classification capacity is determined by the number of graph convolutions rather than the number of layers in the network. This is also interesting and can motivate more future work toward a more profound understanding of the capability of deep GNNs.\n\nWeaknesses:\n+ The assumption behind the theoretical analysis should be clarified. To what extent can the theoretical results be generalized to broader cases, instead of a non-linearly separable Gaussian mixture model coupled with a stochastic block model?\n+ Potential limitation of the current theoretical analysis is not clear\n+ As mentioned in the first point of contributions, a single graph convolution, and two graph convolutions both improve the threshold by a certain factor. What are the key differences between the theoretical analysis for these two cases? Is there any space to further expand the theoretical results to multiple graph convolutions? This may help interested readers have a clear mind on this line of research.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality, clarity, and originality of this work are solid. ",
            "summary_of_the_review": "Overall, I vote for accepting. The theoretical results and associated proofs are solid. The experimental results verify well their theories. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3275/Reviewer_k2Qb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3275/Reviewer_k2Qb"
        ]
    },
    {
        "id": "Jccv69dsov",
        "original": null,
        "number": 2,
        "cdate": 1666682868494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682868494,
        "tmdate": 1666682868494,
        "tddate": null,
        "forum": "P-73JPgRs0R",
        "replyto": "P-73JPgRs0R",
        "invitation": "ICLR.cc/2023/Conference/Paper3275/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides some theoretical conclusions, including:\n- GNNs are better than MLPs on graph data provided that node features are sampled from XOR-GNN (though it may be a little strong). \n- Any combinations of graph convolutions have similar performance as long as the number/order of graph convolutions is the same. \nSome experiments are also conducted to verify the theoretical conclusions. ",
            "strength_and_weaknesses": "### Strength\n\n- The paper is well-written and easy to follow. The mathematical notations are clarified. \n- The paper theoretically shows that GNNs are better than MLPs for graph data under some preconditions (even though it may be a little strong).\n- The provided insight about \"what really matters is the number of graph operations rather than the way to combine it\" is interesting and impressive. \n\n### Weakness\n\n- The fundamental assumption of $X$ is that each feature vector is sampled from the XOR mixture of Gaussian models, XOR-GMM. Maybe it is strong for most cases. In particular, the graph is usually used for non-Euclidean data. What's more, why do we need the random variable $\\eta$ in XOR-GMM? \n- As the discussion is limited to XOR-GMM (Part-1 of Theorem 1), the authors only discuss the two-layer and three-layer cases, which is a little tricky.\n- It might be inadequate to use the same bound of both $p$ and $q$, since there should be a difference between the inter-class and intra-class probability. \n- The theoretical conclusion concentrates on the binary classification. Could the theory be generalized into the multi-class case? \n- In the footnotes of Page-3, the authors claimed that \" they readily generalize to other normalization methods\". Could the authors provide more discussions about how to generalize the shown proof to other normalization methods? I'm curious about this question but I could not check the proof very carefully due to the tight review deadline. \n- The readability could be further improved. For example, \n    - The definition of $\\gamma$ could be introduced in Theorem 1, so that the readers could find the variation of $\\gamma$ in both Theorem 1 and Theorem 2. \n    - At the end of the 3rd paragraph in Section 2.1, the subscript of $\\{X_i\\}$ should be $i\\in[n]$. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written and easy to follow. All notations are clearly defined. \n\n- The idea to theoretically investigate the superiority of GNNs compared with MLPs and the combination of graph convolutions seems novel and promising to me. \n\n- The experiments seem easy to reproduce. ",
            "summary_of_the_review": "I would like to update my score after reading other reviews and the response from the authors. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3275/Reviewer_4qct"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3275/Reviewer_4qct"
        ]
    },
    {
        "id": "Jq1nT_QBg-",
        "original": null,
        "number": 3,
        "cdate": 1666737902083,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666737902083,
        "tmdate": 1666737902083,
        "tddate": null,
        "forum": "P-73JPgRs0R",
        "replyto": "P-73JPgRs0R",
        "invitation": "ICLR.cc/2023/Conference/Paper3275/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the impact of graph convolutions on a binary node classification problem. In this setting, the relationships among nodes are sampled from a given stochastic block model and node features are associated with a Gaussian mixture model. Theoretical results on the performance of a two or three layer graph convolutional network on the node classification task are provided. Additionally, the performance is characterized in terms of number of graph convolutions and their placement. Interestingly, it is shown that placement of graph convolution in the first layer jeopardizes the node classification performance. Theoretical results are validated via experiments on synthetic and real world datasets. ",
            "strength_and_weaknesses": "Strengths:\n\nThis paper enhances the understanding of graph convolutions in learnability. Although the considered node classification problem is parametric and simpler than what is usually encountered in practice, the results in this paper are backed by rigorous theoretical analyses. Experimental validations on synthetic and real-world datasets are also provided.\n\nWeaknesses:\nI did not identify any major weaknesses. However, I could not understand how the hyper-parameters in the experiments were selected. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The theoretical arguments and observations are backed by good quality discussions. I did not check the proofs rigorously, however, the theoretical findings on SBM design for the node-classification task are along expected lines. Insights into the placement of graph convolutions in various layers of the network are illuminating. ",
            "summary_of_the_review": "This paper provides a solid theoretical contribution on the learnability of graph convolutional networks for a node classification task. My initial recommendation is accept. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3275/Reviewer_vTDa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3275/Reviewer_vTDa"
        ]
    },
    {
        "id": "mR0z6Knmx8_",
        "original": null,
        "number": 4,
        "cdate": 1666824259831,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666824259831,
        "tmdate": 1666824928168,
        "tddate": null,
        "forum": "P-73JPgRs0R",
        "replyto": "P-73JPgRs0R",
        "invitation": "ICLR.cc/2023/Conference/Paper3275/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors try to prove that the performance of graph convolutions placed in different combinations among the layers of a neural network is mutually similar for all combinations of the placement.",
            "strength_and_weaknesses": "## Strength\nThe analysis is comprehensive.\n\n## Weaknesses\n1. The assumption of the analysis is over-simplified.\n2. Some claims and experimental results are only valid for homophilic graphs, but not for heterophilic graphs. But I think it has the potential to be extended to homophily/heterophily problem.\n\n## Questions and Comments\n\n1. The 4th line in the first equation in section 2.2, is that f^L or H^L?\n\n2. How is your assumption over p,q related to homophily/heterophily problem identified in [1,2], e.g. heterophily is not always harmful and homophily is not always necessary for GNNs.\n\n3. How is your analysis generalized to more complex settings, e.g. multi-class classification or more general conditions on data generation distributions.\n\n4. \u201cthe placement of graph convolutions does not matter as long as it is not in the first layer\u201d. Why the graph convolutions cannot be put into the first layer? How does this align with your previous analysis?\n\n5. \u201cas claimed in Theorem 2, networks that utilize the graph perform remarkably better than a traditional MLP that does not use relational information.\u201d I don\u2019t think this conclusion is still valid on heterophilic graphs, see [2].\n\n6. Why not do more convolutions, e.g. 5 or 10? Do your claim and analysis still hold?\n\n[1] Ma Y, Liu X, Shah N, et al. Is homophily a necessity for graph neural networks?[J]. arXiv preprint arXiv:2106.06134, 2021.\n\n[2] Luan S, Hua C, Lu Q, et al. Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?[J]. arXiv preprint arXiv:2109.05641, 2021.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: good\nNovelty: medium\nReproducibility: good",
            "summary_of_the_review": "The theoretical analysis looks fancy, but some claims and conclusions seem trivial. The significance of this paper is not as strong as its analysis. It is a borderline paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3275/Reviewer_Wv5W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3275/Reviewer_Wv5W"
        ]
    }
]