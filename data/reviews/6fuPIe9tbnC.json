[
    {
        "id": "2ikkLImAikA",
        "original": null,
        "number": 1,
        "cdate": 1666506154054,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666506154054,
        "tmdate": 1666506154054,
        "tddate": null,
        "forum": "6fuPIe9tbnC",
        "replyto": "6fuPIe9tbnC",
        "invitation": "ICLR.cc/2023/Conference/Paper1367/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method for disentangling data into latent factors of variation by enforcing structure within Koopman matrices used to model the data's time evolution. This procedure allows for disentangling the static and dynamic components of the data, and furthermore disentangling features within those components (e.g. hair color, skin color, etc.). Experiments show the ability to use trained models to swap features within images and improve on existing approaches in benchmark tasks.",
            "strength_and_weaknesses": "Strengths:\n- To my knowledge the work is a novel contribution to two field (Koopman-based modeling and disentangling of complex data)\n- Experiments were interesting, relatively extensive, and served as a good illustration of the benefits of the proposed method\n\nWeaknesses:\n- The process of multifactor separation seems potentially cumbersome and I'm not sure how easily it could be extended to different problems.\n- A few questions about the algorithm itself that should be clarified.",
            "clarity,_quality,_novelty_and_reproducibility": "To my knowledge the work is original. The writing and experimental results are clear, and the information provided in the paper and appendix is pretty comprehensive.",
            "summary_of_the_review": "I found this paper to be interesting and of good quality. I thought this was a good extension to prior Koopman-based modeling approaches and seems to provide value within the field of data disentangling.\n\nA few comments/questions:\n- In the loss penalty applied to the dynamic eigenvalues we seem to be explicitly imposing that the eigenvalues are less than one. In a dynamical system, this corresponds to having no unstable modes. For image data, how would having an \"unstable\" mode with eigenvalue magnitude >1 manifest itself? Do you ever see this in cases where the loss penalty isn't applied?\n- This paper is working with datasets where there are clear distinctions between attributes within the dataset (especially in the sprite dataset). Can you set the number of static modes to be the number of factors that vary within the dataset (i.e. if only hair color, skin color, and shirt color vary, can you get away with only using three static modes)? If not, why do you think that is?\n- In dimensionality reduction techniques that rely upon SVD, it is usually assumed that the modes with the largest eigenvalues are most important, i.e. they explain most of the variance in the data. Did you see anything similar in your experiments? If you were working with a dataset for which you had less a priori knowledge about important attributes, do you think you would be able to discover them through looking at the dominant modes?\n- What role does the LSTM play in the encoder architecture? It would be possible to perform dimensionality reduction with the convolutional layers alone. Given that the temporal relationship between encodings is being modeled by the Koopman operator and not the LSTM, why do you need to run the encodings through an LSTM first?\n- In the appendix it states \"Thus, we keep the real part of z, and we eliminate its imaginary component\". This seems like it would be throwing out some information? Why do you think this is justified, and did you try alternatives like calculating the magnitude, feeding both the real and imaginary components to the decoder, etc.?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1367/Reviewer_WaMN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1367/Reviewer_WaMN"
        ]
    },
    {
        "id": "aducg51g2_",
        "original": null,
        "number": 2,
        "cdate": 1666511795037,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666511795037,
        "tmdate": 1666561371736,
        "tddate": null,
        "forum": "6fuPIe9tbnC",
        "replyto": "6fuPIe9tbnC",
        "invitation": "ICLR.cc/2023/Conference/Paper1367/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new autoencoder method for sequence disentanglement. This work extends the prior works to handle multi-factor sequential disentanglement. While keeping the general encoder and decoder structure, the model adds a Koopman module for the latent space. This work holds the assumption that the underlying dynamics can be represented linearly in the latent space. The Koopman layer computes the Koopman matrix and uses eigenvectors as static (eigenvalue 1) or dynamic factors (others).",
            "strength_and_weaknesses": "Pros:\n1. Handling multi-factors is a very tough challenge. This paper proposes a very interesting idea to address this problem.\n2. The experiments are thorough and the performance looks promising.\n3. The structure of the paper is clear and informative.\n\nCons:\n1. How do you decide length T?\n2. While I think the claim \"eigenvectors of the matrix C whose eigenvalue is 1 represent time-invariant factors\" does make sense, how do you set thresholds and decide whether the eigenvalue is close to 1 or not?\n3. How costly is the whole eigendecomposition?\n4. Deriving a new C for each Z might cause overfitting. Have you considered stationary assumptions?\n5. For more real-world problems where there might not be explicit semantic meanings, how do you map the eigenvectors to semantics?\n6. Exploring the predictiveness of past for future has been seen in some prior works like predictive information [1,2]. You can also discuss the similarities and differences.\n\n[1] Clark, D., Livezey, J. and Bouchard, K., 2019. Unsupervised discovery of temporal structure in noisy data with dynamical components analysis. Advances in Neural Information Processing Systems, 32.\n\n[2] Bai, J., Wang, W., Zhou, Y. and Xiong, C., 2020, September. Representation Learning for Sequence Data with Deep Autoencoding Predictive Components. In International Conference on Learning Representations.",
            "clarity,_quality,_novelty_and_reproducibility": "This is the first work that introduces Koopman theory into sequential disentanglement problem. In general, the paper is written clearly and the experimental results seem to be promising.",
            "summary_of_the_review": "This is a novel work with noticeable contributions. The goal is clear and the work improves over existing works. But the computation cost and some details (hyper-params, thresholds) can be elaborated on.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1367/Reviewer_oLc4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1367/Reviewer_oLc4"
        ]
    },
    {
        "id": "cPEF333OOM",
        "original": null,
        "number": 3,
        "cdate": 1666623633712,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623633712,
        "tmdate": 1666623633712,
        "tddate": null,
        "forum": "6fuPIe9tbnC",
        "replyto": "6fuPIe9tbnC",
        "invitation": "ICLR.cc/2023/Conference/Paper1367/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A method for disentangled representation learning from sequential data is proposed. It is based on the eigendecomposition of an estimation of the Koopman operator of dynamics. The method is demonstrated with standard benchmark datasets.",
            "strength_and_weaknesses": "### Strengths\n\n- The idea of using Koopman operator's spectrum for static/dynamic disentanglement is very interesting and reasonable.\n- The proposed method is examined against different datasets with different evaluation metrics.\n\n### Weaknesses\n\n1. Despite the nice reasoning for static/dynamic disentanglement, multifactor disentanglement is not well supported by theoretical arguments. I do not think it is a fatal flaw, but it is certainly a weakness as one of the main claims of the paper is the capability of multifactor disentanglement.\n\n2. The definition of $\\mathcal{L}_\\text{dyn}$ penalize $| \\lambda |$ regardless of $\\angle \\lambda$. This sounds strange because even if $\\vert \\lambda \\vert=1$, dynamic modes with $\\angle \\lambda \\neq 0$ represent (energy-preserving) oscillations, which would be regarded as dynamic factors. I guess, in the presented experiments, the current definition of $\\mathcal{L}_\\text{dyn}$ worked as expected only because such energy-preserving oscillations were not dominant in the datasets.\n\n### Other comments, not necessarily weaknesses\n\n3. Multifactor-ness only holds for static factors. While it might be a limit of the proposed method, it is understandable because, in many datasets, dynamic factors correspond to dissipative dynamic modes (with, say, $\\vert \\lambda \\vert < 0.9$) that disappear rapidly, and thus considering each component of possibly-multiple dynamic factors is very difficult.\n\n4. The subspace identification process in Section B.5 should be, even if very briefly, mentioned somewhere around Eqs. (6)--(8) because it would be a big question when reading there.\n\n5. Just before Eq. (3), $\\phi$ should be called as a *left* eigenvector, instead of just saying eigenvector.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is very good. The experiments nicely support the claim of the paper. The method is at least technically novel to some extent, while I cannot really assess the significance of the empirical results in terms of disentanglement research. The results seem to be reproducible with the attached codes, while I did not try them by myself.",
            "summary_of_the_review": "This is a nice paper introducing an interesting idea for static/dynamic disentanglement of sequential data. While the discussion could be deepened in a few aspects, the current paper looks sufficiently good for publication in ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1367/Reviewer_tz1i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1367/Reviewer_tz1i"
        ]
    }
]