[
    {
        "id": "mTkM_68FP14",
        "original": null,
        "number": 1,
        "cdate": 1666630416989,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630416989,
        "tmdate": 1666630416989,
        "tddate": null,
        "forum": "hF1WEiIYPNb",
        "replyto": "hF1WEiIYPNb",
        "invitation": "ICLR.cc/2023/Conference/Paper4188/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new reinforcement learning algorithm: Query the Agent.\nThis algorithm uses uncertainty estimates derived from an ensemble actor-critic setup to bias replay and learning around more uncertain states and actions.\nThe authors support their claims mainly through evaluation on an \"M-maze\" experiment, where QTA outperforms baselines.",
            "strength_and_weaknesses": "There are several things to like about this paper:\n- The overall quality of writing and polish is pretty high.\n- The core QTA algorithm seems reasonable, and some of the pieces of network design/modifcations to ensemble uncertainty to form \"PUN\" seem useful.\n- The experiments and analysis are quite well presented.\n- The accompanying code and reproducibility appears to be extremely high.\n\nHowever there are a few places where the paper could be improved:\n- The improvements relative to baselines (MEGA, BootDQN, ...) all seem to be more incremental than transformative. Typically, when people look at these difficult maze-style domains you really care about *scalability* rather than pure performance (since these are just toy problems after all)... in terms of scalability, it does not seem very significant differences as the problems grow.\n- Related to the point above, these experiments seem a little \"adhoc\"... I would have liked to see something like a regret analysis/proof for tabular settings and/or an evaluation on some kind of scalable domain like DeepSea (behaviour suite for reinforcement learning).\n- The discussion of related work, while pretty comprehensive, seems quite piecemeal.. I think it would be possible to consolidate and digest some of the related approaches (note that many of these are essentially ensemble-based uncertainty). It would be nice to get some kind of bigger-picture evaluation of what types of uncertainty and techniques really are what make QTA valuable versus these other approaches. Looking at something like \"Deep Exploration via Randomized Value Functions\" (Osband et al) might be a good overview of the type of approximate posterior sampling that many of these algorithms instantiate..\n- Some pieces of the algorithm/design could benefit from being spelled out more explicity. For example, action selection is only described via reference to the DDPG paper... I think this should be made more explicit... what type of exploration policy is used in action selection? I think this is unusual if it relies on the same kind of boltzmann/dithering as DDPG... could not scale to large domains that require deep exploration.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall I think the paper has strong clarity and reproducibility.\nSome of the concerns I have are more centered around how incremental the work could be.",
            "summary_of_the_review": "This paper presents a new RL algorithm designed to leverage epistemic uncertainty estimates for improved learning.\nOverall, I think there are several things to like about the paper, but I'm left with more of a feeling that these improvements are more incremental than transformative.\nThis could be a reasonable paper in the conference, but I think it could benefit from more clear and decisive results and/or comparisons to existing work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4188/Reviewer_tn6E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4188/Reviewer_tn6E"
        ]
    },
    {
        "id": "sFJJ-zekIvw",
        "original": null,
        "number": 2,
        "cdate": 1666678352824,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678352824,
        "tmdate": 1666678352824,
        "tddate": null,
        "forum": "hF1WEiIYPNb",
        "replyto": "hF1WEiIYPNb",
        "invitation": "ICLR.cc/2023/Conference/Paper4188/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In recent years, goal conditioned reinforcement learning approaches have received a lot of attention in the literature. However, curricula for selecting goals during training have predominantly been based on heuristics, which still lead to poor sample-efficiency in realistic scenarios. The paper proposes using curricula based on the agent's epistemic uncertainty. An agent's epistemic uncertainty is it's lack of knowledge. Using this epistemic uncertainty to select goals provides a natural curriculum where the agent gradually acquires knowledge. The authors propose predictive uncertainty networks, a novel uncertainty estimation method for quantifying the agent's epistemic uncertainty. The uncertainty estimates are used as scores in a prioritized replay-based mechanism for making gradient updates. Additionally the replay buffer also involves hindsight experience-like goal selection. Finally the authors implement the proposed algorithm in a procedurally-generated 2D mazes.",
            "strength_and_weaknesses": "**Strengths**\n\n- Leveraging the epistemic uncertainty of the learner to establish a curriculum of goal for training is a neat idea and is a natural way to characterize the learning process - the agent gradually learns to achieve goals that it has high uncertainty over - leading to improved sample efficiency. \n- The empirical analysis in the paper is quite thorough and well done. \n\n**Weaknesses**\n\n- In my view the main weakness of the paper is the limited experimental setups considered. The experiments are done only on 2D mazes which do not seem sufficient as evidence for the claims made in the paper. \n- Using the uncertainty estimates directly has the potential to suffer from the noisy-TV problem. That is, states of higher-entropy might be selected as goals even though they do not serve as informative goals for the agent. \n- The idea of using epistemic uncertainty to guide exploration in reinforcement learning is not particularly novel and has been explored in the past [1, 2]. [2] also addresses the previous point as instead of simply using the uncertainty to weigh the points, they define an acquisition function to quantify the information gain. \n\n[1] - DEUP: Direct Epistemic Uncertainty Prediction\n\n[2] - An Experimental Design Perspective on Model-Based Reinforcement Learning",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe paper is well written, and easy to follow with sufficient details described for the method.\n\n\n**Quality and Novelty**\n\nWhile some parts of the proposed approach such as the uncertainty estimation method are novel, the overall approach is not novel. The empirical analysis is through and well done but the limited variety of environments considered limits impact.\n\n**Reproducibility**\n\nThe authors provide code along with the submission, and most of the method details are described in the paper. ",
            "summary_of_the_review": "in summary,  while the presented approach is interesting and is accompanied by a through and insightful empirical analysis, the significance of the approach is limited by simple environments and limited novelty. Results on more diverse environments, for instance from bsuite [1], would improve the claims of the paper significantly. I encourage the authors to incorporate the feedback during the discussion and rebuttal. \n\n[1] Behaviour Suite for Reinforcement Learning",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4188/Reviewer_beUS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4188/Reviewer_beUS"
        ]
    },
    {
        "id": "AiCoZiPwGdU",
        "original": null,
        "number": 3,
        "cdate": 1666867424046,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666867424046,
        "tmdate": 1666867424046,
        "tddate": null,
        "forum": "hF1WEiIYPNb",
        "replyto": "hF1WEiIYPNb",
        "invitation": "ICLR.cc/2023/Conference/Paper4188/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new goal selection algorithm for training curricula of goal-oriented reinforcement learning agents. The algorithm, called query the agent, estimates epistemic uncertainty of the agent in the state space and sets goals in areas with higher epistemic uncertainty. The algorithm is evaluated on a set of generated 2-dimensional mazes.",
            "strength_and_weaknesses": "Strengths: the paper considers application of techniques of adaptive active learning to goal-conditioned reinforcement learning problems. The idea is explained in great detail, compared with other ideas to account for uncertainty in the literature. The supplementary material is provided, with the code to reproduce all experiments in the paper.\n\nWeaknesses: the paper claims that\n\n1) goals should be set with regions with high epistemic uncertainty;\n2) states are described by real vectors;\n3) the mean of standard deviations of each component of the state vectors is a good measure of epistemic uncertainty.\n\nThe first claim seems to be a feasible option, but unsupported. An alternative option might be, for example, setting goals such that paths pass through regions of high uncertainty (rather than end in them). I do not argue that  a different option can be better, I just point that the choice to set goals where uncertainty is high seems to be rather arbirtrary, unless supported at least by some discussions.\n\nThe second claim seems to cover many applications of deep reinforcement learning, however it must be stated explicitly. There are models with other state representations.\n\nThe third claim is problematic. There is extensive research in quantify sample-based uncertainty (just one citation for example, https://link.springer.com/article/10.1007/s10994-021-06003-9).  Even if  second-order approximation of the distribution is used, the covariance matrix is only poorly characterized by the sum (or the mean) of the diagonal elements.\n\nThe ideas behind the algorithm seem to possibly usable, but unsupported theoretically. An alternative would be to support than experimentally. However, the experiments are performed only on a set of relatively small 2-dimensional mazes. One does not really need deep neural networks with complicated architecture to solve those mazes, so while they can be used to confirm that the idea basically works, they are not a sufficient empirical evaluation for the above heuristic claims. One obvious concern, but there are others, is that mean standard deviation of components will probably work in 2 dimensions, but it will have no chance to work in 20 or more. In high dimensions L2 norm between points in the space vanishes to be informative. It is called 'the curse of dimensionality'.\n\nFinally, judging by both the text of the paper and by the code in the supplementary material, the algorithm has dozens of hyperparameters, (probably set GIVEN the data set of mazes, btw). It is not clearly why those values were chosen, and how to chose the best values in general. Without that, the algorithm is not going to be generalizable.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well structured but has many typos. The text has to be proofread to be conveniently readable.\n\nI think that to better realize the novelty of this paper, uncertainty based active learning techniques must be also considered, not just the context of reinforcement learning. This is not addressed in the body of the paper.\n\nThe experiments can be run, and the results can possibly be reproduced on the same data set. However, because the implementation has so many parameters, and because most of the code is uncommented and hard to navigate, I doubt 'extended reproducibility' that is applying the algorithm to other domains is going to succeed.",
            "summary_of_the_review": "Interesting research direction, however the results as presented are too early for  publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4188/Reviewer_dsJh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4188/Reviewer_dsJh"
        ]
    },
    {
        "id": "hT6wLUbHyg",
        "original": null,
        "number": 4,
        "cdate": 1667247436103,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667247436103,
        "tmdate": 1667247436103,
        "tddate": null,
        "forum": "hF1WEiIYPNb",
        "replyto": "hF1WEiIYPNb",
        "invitation": "ICLR.cc/2023/Conference/Paper4188/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\"Query The Agent\" is an improvement on curriculum learning for deep reinforcement learning. If I understand, to remedy the data inefficiency of RL, this approach applies \"curricula\" in the form of intermediate learning goals that are informed by the epistemic uncertainty of the agent, e.g. the agent's lack of understanding of parts of the state space.  The agent benefits by exploring (trying to achieve) appropriate goals -- those not to easy or too hard. Uncertainty is represented by running multiple critics.  \"Epistemic uncertainty\" is estimated by running multiple critics, based on a neural network predictor called a \"predictive uncertainty network\" and looking at their standard deviation.   \n\nThe modifications tested in the paper show measurable improvement compared to recent \"MEGA\" and \"VDS\" methods. ",
            "strength_and_weaknesses": "Granted no familiarity with the few recent papers on which this work depends, my sense is that this is a study of an incremental exploration of a set of proposed modifications to RL critics demonstrated in a toy domain.   It's not clear what the larger relevance and value of this work is.  The paper may be \"correct\" but the take-aways appear marginal if any. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well written., but addresses a limited audience, perhaps no larger than the authors of some of the recent papers it cites.  As for understandability the authors do offer full versions of the code used for the simulations. However in terms of following the paper's argument its explanations are lacking. Save for an example of the value of curriculum learning, the introduction does not give insight into the method, relying on newly coined terms such as PER, without description of what they entail. The reader gets a clue about how the paper uses the term \"epistemic uncertainty\" when mentioned in the introduction as \"encouraging agents to explore regions of the state space the agent least understands\" -- but this would apply to any exploration method.  it would be better if critical and new terms were introduced early and defined in a way to make the paper accessible to a wider audience.\n\nAlso I suggest the authors number equations in the final version.  ",
            "summary_of_the_review": "The high level impression from the paper is of an minor improvement made among several related recent approaches in a simulated environment.  I appreciate the value of simple experiments to illustrate a point, but wonder where the applicability might be to less specific and more realistic problems.  To truly evaluate the paper one needs to understand the related \"baseline\" work from a few specific papers from the past few years to which the current work is compared. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4188/Reviewer_6NMf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4188/Reviewer_6NMf"
        ]
    }
]