[
    {
        "id": "__v-WfygIoi",
        "original": null,
        "number": 1,
        "cdate": 1666506691982,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666506691982,
        "tmdate": 1668762282020,
        "tddate": null,
        "forum": "vZTp1oPV3PC",
        "replyto": "vZTp1oPV3PC",
        "invitation": "ICLR.cc/2023/Conference/Paper452/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Transformer-M, a Transformer-based model that can take both 2D and 3D molecular formats as input. It adopts various positional encoding techniques to unify 2D and 3D information into transformer as attention bias terms. This model is claimed to be a general-purpose model for molecular tasks. Experiments on several 2D and 3D molecular tasks have been conducted to evaluate the developed method.",
            "strength_and_weaknesses": "\n#####Strength\n\n1. The motivation of developing a general-purpose model for molecular tasks is great and might be inspiring to the community.\n\n2. The empirical result of the proposed method is competitive, especially on PCQM4Mv2, where the task is the same as the pretraining objective.\n\n3. This manuscript is well organized and easy to follow.\n\n#####Weaknesses\n\n1. The main concern for this paper is that the novelty is not enough to me. To be specifically, both the 2D and 3D branches are proposed by previous work. The positional encodings are verified to be effective by existing work. This work combines these two branches as a single model, which is not novel in terms of technical contribution.\n\n2. Also, the empirical result on PCQM4Mv2 is good. However, the effectiveness of pretraining Transformer-M has not been demonstrated well, empirically. Specifically, the methods used for comparison in Table 2 (PDBBind) and Table 3 (QM9) do not use the same extra data for pretraining. In this sense, the comparison is not so rigorous and fair to me. Even though, the performance on QM9 is not good, except for the energy-related tasks that are consistent with the pretraining tasks. This cannot support the claim that the proposed Transformer-M is a general-purpose model.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. I believe the results can be reproduced well. My concern is the novelty, as detailed before.",
            "summary_of_the_review": "Overall, I think this work is a good start for developing a general-purpose model for molecular tasks. The empirical results can be valuable to the community. However, the novelty of this work is below the standard of ICLR.\n\n#####Questions#####\n\nHow does Transformer-M deal with the molecules where there might have multiple shortest path between two atoms?\n\n\n#####After rebuttal#####\n\nSince most of my original concerns are well addressed, I increased the score from 5 to 6. I highly recommend the authors to include the additional experiments in the revision and to consider the technical details of the method, as in my response below.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper452/Reviewer_JC8f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper452/Reviewer_JC8f"
        ]
    },
    {
        "id": "pQ3xPfpnw3",
        "original": null,
        "number": 2,
        "cdate": 1666573294536,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573294536,
        "tmdate": 1666573294536,
        "tddate": null,
        "forum": "vZTp1oPV3PC",
        "replyto": "vZTp1oPV3PC",
        "invitation": "ICLR.cc/2023/Conference/Paper452/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper documents a simple method for combining the input representations of a small molecule in 2D and 3D in a single neural network model. The work demonstrates how one can train models using subsets of 2D, 3D, or mixed representations, by modifying the attention head of a transformer architecture to explicitly add a sum over channels of pairwise features generated from (2D) small molecule graphs and channels of pairwise features generated from the small molecule coordinates in 3D. The authors combine a small number of concepts from previous state-of-the art models for small molecules in selecting these features and the unsupervised pre-training tasks, and they manage to show that their new architecture achieves peak performance in the opengraph large-scale challenge for small molecules, and decent performance throughout other tasks.\n",
            "strength_and_weaknesses": "A core strength of this paper is that the high-level idea is simple and clean, yet the results show that it works well: Transformer-M ranks top on the small-molecule OpenGraph large-scale challenge, a world-wide public competition challenge.\n\nThis work is rather strong.  The paper starts to explain the rationale for why the method works as well as it does by performing an ablation study.  I'd have liked to see a couple additional ablations, or other ways to explain why the method works so well: can one keep 3D position denoising for the 3D entries, but drop the joint pre-trainig? How strongly do the various specific features in the pair-wise channels influence the final result?  Do we expect the 2D and 3D features to be \"aligned\" for a given molecule after pre-training the model? Finally, and only as a minor curiosity, what is the impact, if any, of the mode distribution on a downstream task like the QM9 (and if you had that info, could you possibly combine tables 4 and 5 to a single one?)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is rather original in combining 2D or 3D information of small molecules at will in a transformer.  The quality of the work is high: it builds on top of other existing state-of-the-art ideas, and yet it leaves room for future improvement.  The paper is clear, and the main point of this work is rather simple and powerful. The paper comes with code (though I didn't try to test it myself).\n",
            "summary_of_the_review": "This paper ranks top in a rather important small-molecule benchmark; the idea is simple and powerful and the implementation is straightforward.  I think it deserves publication at ICLR2023.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper452/Reviewer_yHnH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper452/Reviewer_yHnH"
        ]
    },
    {
        "id": "Nnt65getf5",
        "original": null,
        "number": 3,
        "cdate": 1666677486968,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677486968,
        "tmdate": 1668830359987,
        "tddate": null,
        "forum": "vZTp1oPV3PC",
        "replyto": "vZTp1oPV3PC",
        "invitation": "ICLR.cc/2023/Conference/Paper452/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Molecules can be represented in a variety of chemical data modes, such as a 2D graph or a collection of atoms in 3D space. Most previous work in molecular representation learning has designed networks for a specific data mode, with the risk of failing to learn from other modes. The authors argue that a neural network that is chemically generalized should be able to handle molecule-related tasks across data modes. To accomplish this, the authors created Transformer-M, which is based on the Transformer and can be fed with 2D or 3D molecular data. The results of experiments indicate that Transformer-M performs well in 2D, 3D, and 2D&3D tasks.",
            "strength_and_weaknesses": "Strengths\n\n\u2022 The paper is generally well-written and easy to follow. \n\n\u2022 Transformer-M can encode 2D or 3D structural information as bias terms and add to the attention matrix. It also encodes atomic centrality and adds to the atom features. Based on this, the model can obtain molecular representations from different data modes.\n\n\u2022 During pretraining, the authors label the data with different data modes for joint training. This training strategy may improve the performance of Transformer-M on downstream tasks from the results.\n\nWeaknesses\n\n\u2022 2D and 3D information encoding are from previous work. This work just simply combines them, and the model architecture is the same as Graphormer[1]. The novelty is not enough.\n\n\u2022 Supervised pretraining based on the prediction of homo-lumo gap may lead to negative transfer. For example, on QM9 in downstream experiments, Transformer-M performs poorly on most tasks other than homo, lumo, and gap. This may be contradictory to the description \"general-purpose neural network model\" claimed in this paper.\n\n\u2022 Lack of description of PDBBind data processing and splitting in downstream tasks.\n\n\u2022 Absence of some ablation experiments: \u2460(p2D, p3D, p2D&3D) = (1:0:0) / (0:1:0) / (0:0:1); \u2461Only using the 3D position denoising task while pretraining.\n\nOther questions\uff1a\n\n\u2022 Do the authors consider encoding 1D molecular data mode, e.g., SMILES,  simultaneously?\n\n\u2022 What do the authors think about the possibility of negative transfer on downstream tasks due to the supervised signal introduced during pretraining?\n\n\u2022 Whether there is data leakage during finetuning on PDBBind, because we know that the general, refined, and core sets have overlapping parts.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is reader friendly.\nQuality and Novelty: Not very novel. It looks like a simple combination of previous works.\nReproducibility: The code will be released. But I don't have much confidence in the reproduction of the results on PDBBind.\n",
            "summary_of_the_review": "In summary, this paper is reader-friendly, but novelty is not enough, and some descriptions of the experiments are not clear enough.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper452/Reviewer_8vsr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper452/Reviewer_8vsr"
        ]
    },
    {
        "id": "6oCV4fS5zs",
        "original": null,
        "number": 4,
        "cdate": 1667078338347,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667078338347,
        "tmdate": 1667373206283,
        "tddate": null,
        "forum": "vZTp1oPV3PC",
        "replyto": "vZTp1oPV3PC",
        "invitation": "ICLR.cc/2023/Conference/Paper452/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a simple modifications over the Graphormer 3D [1] , which previously encoded node, graph level information through additional biases added to the self-attention modules, namely node-encoding via in/out degree embeddings, path encoding via shortest path and edge embedding aggregation along this path. To encode the spatial and centrality information for 3D molecules they embed these using a GBF assuming a fully connected graph as opposed to using cutoffs. \n\nNow, the authors simply introduce a switching mechanism in this model, where the corresponding 2D/3D encodings are switched on and off.  This is done as it may be the case that 2D information is available and 3D information is not on a particular input sample, so to allow the model to distill \"3D\" level information to enrich the 2D representation, for example it is the case for the PCQM4Mv2 dataset which includes a highly optimized conformer for the train-set but not the rest.\n\n\n\n[1] Shi, Yu, et al. \"Benchmarking graphormer on large-scale molecular modeling datasets.\" arXiv preprint arXiv:2203.04810 (2022).",
            "strength_and_weaknesses": "The modifications are only a few lines of code over the original Graphormer 3D proposed in [1]. Still, this simple modification results in a new STOTA on PCQM4Mv2, namely introducing a mask in the input based on whether the network should operate in 2D/3D/Both.\n\nThe experiments on the task probabilities, which is the primary modification, are limited; changing this ratio can drastically change the model's performance. Most of the gains are attributed to the pretraining task and 3D encoding in the bias term, which was previously leveraged in [1].\n\nI would have liked to see more experiments on trying to further leverage 3D information on 2D inputs that don't have the position information, for example, adding conformers as well. Would the network operated in 3D/2D mode perform the best over simply 2D? \nWhat happens if you generate multiple conformers and switch these modes on and off? Would there be a gain in performance? The authors could have done more experiments on this front.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow.",
            "summary_of_the_review": "To summarize it's a simple idea that works, and the empirical results are good.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper452/Reviewer_do3S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper452/Reviewer_do3S"
        ]
    }
]