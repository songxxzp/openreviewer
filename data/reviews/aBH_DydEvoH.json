[
    {
        "id": "G32yBajxmlp",
        "original": null,
        "number": 1,
        "cdate": 1666674273955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674273955,
        "tmdate": 1666674273955,
        "tddate": null,
        "forum": "aBH_DydEvoH",
        "replyto": "aBH_DydEvoH",
        "invitation": "ICLR.cc/2023/Conference/Paper3565/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes Implicit Language Q-Learning (ILQL), a RL method, adapted from Implicit Q-learning, which can learn controlled language generation from a reward signal and a large-scale dataset of mixed quality in an offline fashion. The authors empirically validate their method on a dialogue task and reddit comment generation.",
            "strength_and_weaknesses": "Strengths:\n* The paper is well written and easy to follow.\n* As reinforcement learning gains in popularity for NLP applications, this paper provides an important contribution by introducing scalable offline RL for language.\n* The authors demonstrate that the method is easy to use and effective, and can handle existing data. In particular, it can learn to combine aspects of behaviors to achieve a higher reward than obtainable from the average of the data.\n* The authors validate their method on a goal-directed question asking task (Visual Dialogue) and a Reddit comment generation task.\n\nWeaknesses:\n* ILQL requires a larger computational budget than supervised learning due to the additional cost of the behavioral model.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and introduces a new offline RL method tailored to language.",
            "summary_of_the_review": "This paper introduces and validate a useful and effective new method for using reinforcement-learning to fine-tune large language models in a scalable offline fashion.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3565/Reviewer_LVpS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3565/Reviewer_LVpS"
        ]
    },
    {
        "id": "MuzTsxsyml",
        "original": null,
        "number": 2,
        "cdate": 1666770482996,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666770482996,
        "tmdate": 1666770482996,
        "tddate": null,
        "forum": "aBH_DydEvoH",
        "replyto": "aBH_DydEvoH",
        "invitation": "ICLR.cc/2023/Conference/Paper3565/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an offline RL method for natural language generation, ILQL, which extends a recently proposed offline RL method, Implicit Q Learning. The base method, Implicit Q Learning, uses expectile regression to learn a q function by limiting itself to only observed dataset actions. This paper translates the increased stability from this algorithm  to the NLP setting, while also presenting a few tricks required to making training work for NLG (e.g. pushing down weight of unseen tokens via a regularization term).\n\nAuthors first show a toy wordle example where the multi-step updates enabled by ILQL greatly improve performance in a contrived setting. They then evaluate on more realistic, open-ended visual dialog and reddit comments tasks, showing significant gains over standard finetuning, naive SARSA, and some other offline RL baselines. One of the particularly interesting experiments is that ILQL is robust to changes in the reward function (e.g. avoiding yes/no questions in visual dialog).",
            "strength_and_weaknesses": "# Strengths\n\n- Novel and sensible adaptation of a recent offline RL method, implicit Q learning, to the natural langauge generation setting, as well as various tricks that are needed to make learning work in this setting\n- Thorough experimentation including toy examples as well as more realistic settings in visual dialog and reddit comment generation, with comparison to naive SARSA updates as well as full finetuning/finetuning only on high-reward trajectories.\n- Thorough comparison to other baselines (when such baselines are included)---I appreciate authors' effort to sweep over hyperparameters and explain baselines in detail in Appendix---I think this is much more thorough than your typical ML paper.\n- I like that the method clearly states limitations (e.g. longer training times compared to standard supervised RL, limitations to suboptimal data) in conclusion.\n\n# Weaknesses\n\n- I'm surprised that comparison to other offline RL baselines is deemed an \"ablation\" and only depicted in section 6.3/Table 3, and not throughout. In the main VisDial (table 1) and reddit results (table 2), ILQL is only compared to naive SARSA as well as FT/Filtered FT. Comparison to other offline RL methods used in the literature doesn't feel like an ablation to me - it feels like a fundamental comparison: is the added complexity required by ILQL worth the effort over other (perhaps conceptually simpler) offline RL algorithms? Moreover, how do the other offline RL algorithms handle reward shifts?\n\n## Minor\n\n- I find \"Easy to Use\" to be a fairly subjective/meaningless criterion in Figure 2.\n- Page 6: Appendix ??\n- It'd be great to explicitly point to Appendix A.4 in the paragraph \"Ablations on choice of Offline RL algorithm\".",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clear, and authors have included a reproducibility statement which is more than sufficient. I appreciate inclusion of all the relevant hyperparameters and attached source code.",
            "summary_of_the_review": "This paper clearly presents a novel offline RL method, ILQL, which shows strong results across a variety of diverse NLg tasks, especially in more adversarial settings (e.g. reward shift). Although comparison to offline RL baselines are not as thorough as the rest of the paper, I'm inclined to accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3565/Reviewer_jx4F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3565/Reviewer_jx4F"
        ]
    },
    {
        "id": "FMqxzlSSDr",
        "original": null,
        "number": 3,
        "cdate": 1666929194865,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666929194865,
        "tmdate": 1667317638917,
        "tddate": null,
        "forum": "aBH_DydEvoH",
        "replyto": "aBH_DydEvoH",
        "invitation": "ICLR.cc/2023/Conference/Paper3565/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates IQL, an offline RL training algorithm, applied to training a language model to generate a sequence that will maximize a utility function (such as a user reward model). ",
            "strength_and_weaknesses": "Strength:\n- The paper proposes a novel learning paradigm ILQL (Figure 3). The idea of directly \"adding\" advantage Q-V to p(vocab) is very appealing. \n- The paper also re-configured the CQL pessimism penalty and re-adapted it to work with discrete action space, which is novel.\n- The experimental result seems strong to convince me this algorithm works on a wide range of task domains.\n\nWeakness:\n- Section 5 is wrong, which affected all the follow-up experimental comparisons between ILQL and \"SARSA\". There are a few reasons why this is wrong (or \"trivial\"): \n   1. I don't think the author understands the concept of SARSA vs. Q-learning (ILQL). SARSA, first of all, is an on-policy learning algorithm, while Q-learning (ILQL) is an off-policy learning algorithm. In the offline RL setting, an on-policy learning algorithm would never make any sense because on-policy exploration is forbidden. \n   2. SARSA provides a convergence guarantee under two specific conditions: 1). Infinite visits to every state-action pair; 2). The learning policy becomes greedy in the limit. Offline RL setting (and Figure 4 worked example) satisfies NONE of these conditions. If we read the text in the paragraph, the learning policy $\\pi_{\\text{upper bound}}$ was never updated (it stays fixed). The suboptimal result is trivial and obvious. I encourage the author to read [1]. Comparing Q-learning with SARSA in an offline RL setting makes no sense. \n   3. I think this paper tries to follow the original IQL too closely and re-iterate the point made in IQL but without enough finesse and care. The original IQL paper never suggested that SARSA would ever work in an offline RL setting. Also, IQL > SARSA is already shown in the original IQL paper; what is the point of showing this again?\n   4. Overall, I think Section 5 is largely misleading to people who are not familiar with RL and particularly Offline RL.\n\nSuggestion:\n- The equation on page 5, $L^c_{Q, V}(\\theta)$, the author should be able to offer a short proof (of a few lines) that show it's equivalent to CQL pessimism penalty. This would make the statement in the paragraph more concrete: \"this CQL loss term is no more expensive than, and in fact equivalent to, a standard cross-entropy loss at the token level\".\n- I think the authors should consider re-orient this paper to be more NLP-focused and drop the comparison to SARSA in this offline setting. The results aren't surprising to any RL researchers and only serve to confuse them. The proposed algorithm ILQL is novel enough! It doesn't need the motivation to do better than SARSA.\n- I think the authors should rethink the pitch and conduct experiments that make sense to the pitch.\n\nReferences:\n\n[1] Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms (https://link.springer.com/content/pdf/10.1023/A:1007678930559.pdf) ",
            "clarity,_quality,_novelty_and_reproducibility": "There is one broken hyperref link on page 6 that refers to an Appendix section.\n\nOther than that, the algorithm seems generally novel. The quality and clarity seem ok.",
            "summary_of_the_review": "The paper is generally interesting, and IQIL is sufficiently different from other Offline RL + LM fine-tuning/training papers. However, I don't think the main claim (IQIL/Q-learning > SARSA) in an offline RL setting makes sense. Though this paper has so many interesting ideas, great experimental results, and on a diverse collection of datasets, I cannot recommend an accept at this point.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3565/Reviewer_8Evy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3565/Reviewer_8Evy"
        ]
    }
]