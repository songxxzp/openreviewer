[
    {
        "id": "J6XS2QmyFU4",
        "original": null,
        "number": 1,
        "cdate": 1666395235188,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666395235188,
        "tmdate": 1666395235188,
        "tddate": null,
        "forum": "KbYevcLjnc",
        "replyto": "KbYevcLjnc",
        "invitation": "ICLR.cc/2023/Conference/Paper429/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on collaborative writing based on pretrained LMs. In particular, it proposes a collaborative LM that writes drafts, adds suggestions, proposes edits, and provides explanations. To training is performed on edits and cited documents (at the next step) that is assumed to be available a priori as well as an infilling technique that allows generating data for new domains to increase applicability and training diversity. The evaluation focuses on planning in settings with edits and histories available across different domains, downstream editing tasks with human instructions, and quoting & citing, shows improved performance compared to unsupervised baselines or baselines pretrained on human instructions.",
            "strength_and_weaknesses": "**Strengths**\n\nIdea of training LMs to perform collaborative writing is interesting and the application exciting. Current tools for editing do not support such advanced functionalities and could have large  commercial potential. \n\nShowcases feasibility of collaborative writing with pretrained LMs (proof of concept) and improvements over unsupervised pretrained LMs and LMs that have been trained on a broad set of human instructions. \n\n**Weaknesses**\n\nThe approach requires training different LMs for four functionalities which is not ideal from maintainability and efficiency standpoint. Discussion about this design choice versus pretraining a single model to perform different actions would be useful.\n\nAssumption is that a large dataset of annotation is available for training and access to citations/documents which are not realistic. Even though an approach for extending to other domains is proposed, it still relies on annotated data and additional costly steps for data augmentation and training. This limitation has not been examined or discussed thoroughly in the paper. \n\nLack of baselines with vanilla in-context learning (ICL) with editing/performing undo/explaining examples. There have been a number of papers demonstrating that ICL can work surprisingly well. It would be interesting to compare with simple in-context learning with a few examples of edits for instance. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and centered around the application of collaborative editing. Descriptions were clear in most places; some less clear aspects were on the technical novelty, benefits compared to simple ICL baselines, and generalizability/science impact beyond making the collaborative application possible. \n\nThe novel aspects of this work include decomposing problem and training pretrained LMs on objectives for different writing purposes (edit, undo, explain, generate document). Also, leveraging them to augment data for domains that lack editing and document histories. Overal, interesting approach for a proof-of-concept but potential for adoption might be limited due to technical limitations (e.g. training data assumptions, lack of comparison to simple alternatives).\n\nThe data and code will be released so it should be possible to reproduce the results in this work; I don't have any major concerns on this front since training details are also included in the supplementary. ",
            "summary_of_the_review": "An interesting proof-of-concept paper centering around an exciting application: collaborative writing with pretrained LMs. The approach has novel aspects such decomposing problem and synthesizing data for planning but it makes some unrealistic assumptions about the data availability and lacks comparison to simple few-shot baselines. Synthesizing data for planning should be useful for the research community but the technical part overall might have limited impact for the reasons explained above.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper429/Reviewer_WDUk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper429/Reviewer_WDUk"
        ]
    },
    {
        "id": "OHmiZQWC27",
        "original": null,
        "number": 2,
        "cdate": 1666465596251,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666465596251,
        "tmdate": 1666465596251,
        "tddate": null,
        "forum": "KbYevcLjnc",
        "replyto": "KbYevcLjnc",
        "invitation": "ICLR.cc/2023/Conference/Paper429/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a collaborative language model setup trained on Wikipedia edition history. PEER stands for plan, edit, explain, and repeat. The work is closely related to text editing and instruction LM literature. The paper makes an assumption that the grounded knowledge for editing is given. The authors trained several variances models with T5 including PEER-Edit, PEER-undo, PEER-explain, and PEER-Document, leveraging some of them to augment training data. They evaluated their models on several setting such as Natural Edits with comments, 0-shot testing on downstream tasks, citation and quote, and iterative editing. They showed their models are better than baseline with similar size and even better than some LLMs. ",
            "strength_and_weaknesses": "S:\n- The idea of controlling the generation of language models step-by-step in a recurred manner is interesting.\n- The authors have evaluated their models on multiple settings, proving that their models trained on Wikipedia can potentially generalize to multiple downstream tasks.\n\nW:\n- Human evaluation is missing and could add more insights to the interactive process.\n- Even though there is a \u201cexplain\u201d component in PEER, it is not evaluated and studied regarding its correctness. \n- No error analysis about the generated plans and the edited text. The authors only put a sentence at the end of the Appendix saying that \u201cwe still found PEER to generate false statements or claims not backed up by the provided documents in many cases\u201c, but in the main paper there is no discussion or statistics on this weakness.",
            "clarity,_quality,_novelty_and_reproducibility": "\n- There is a recently released and co-occurrent paper that is very relevant to this work: EDITEVAL: An Instruction-Based Benchmark for Text Improvements. People can take it as a reference.\n- Even though some baselines (e.g., Text editing by command) only do sentence-level editions, their model can still give better numbers than \u201ccopy\u201d. You can either repeatedly put commands to each sentence or just treat a few sentences together as one single long string. \n- Better to provide evaluation metrics to the main paper, e.g., SARI is not that familiar to me, at least add a citation there.\n- The novelty of the modeling side is minor, and the authors do not explain clearly what their strategy is to determine data augmentation for weakly-supervised setting in the main paper. (Actually, I just found there is one section in Appendix C, which can answer some of my questions. )",
            "summary_of_the_review": "Overall, I think this is an exciting paper that tried to improve the usability of controllable language modeling. Making LMs controllable and interactive is definitely an essential next step for our field to move forward. My biggest concern is evaluation. It is hard to persuade people this model is ready without solid human evaluation since this task could be open-ended: the edition is the actual usage can be ambiguous, and the same plan can lead to multiple different outputs, similar to open-domain dialogues.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper429/Reviewer_fvQS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper429/Reviewer_fvQS"
        ]
    },
    {
        "id": "Yj69ifaHjVy",
        "original": null,
        "number": 3,
        "cdate": 1666583719279,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583719279,
        "tmdate": 1666583719279,
        "tddate": null,
        "forum": "KbYevcLjnc",
        "replyto": "KbYevcLjnc",
        "invitation": "ICLR.cc/2023/Conference/Paper429/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors proposed PEER, a language model that includes four skills: plan, edit, explain, and repeat. To better use the Wikipedia edit history with missing parts, the author proposed four infilling operations to overcome this issue: PEER-edit, PEER-undo, PEER-explain, PEER-document. The authors utilize the pretrained T5 to initialize PEER and train on Wikipedia edits. The experiment result shows the effectiveness of PEER on the Natural Edits and many downstream datasets.",
            "strength_and_weaknesses": "Strength:\n- This paper was the first to combine multiple collaborative writing skills together into a language model.\n- Solid experiment results show its effectiveness on various editing tasks in a zero-shot fashion and also citing and quoting tasks.\n- Enable collaborative editing for generating text as shown in Table 4.\n- New datasets: Natural Edits, NE-Cite and NE-Quote are introduced.\n\nWeaknesses:\n- Many of your downstream tasks include Wikipedia, which may be included in the training set of T5. Is there any way to prevent this issue?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity&Quality: The paper is well-written and easy to follow.\nNovelty: Good novelty -- new setting & model (collaborative editing) are developed. A new dataset is introduced.\nReproducibility: The authors didn't provide the code so we cannot reproduce it.\n",
            "summary_of_the_review": "In general it's a good paper with solid results. I would like to recommend this paper be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper429/Reviewer_tmW2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper429/Reviewer_tmW2"
        ]
    },
    {
        "id": "GJgRL221sS",
        "original": null,
        "number": 4,
        "cdate": 1666784917442,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666784917442,
        "tmdate": 1666949372207,
        "tddate": null,
        "forum": "KbYevcLjnc",
        "replyto": "KbYevcLjnc",
        "invitation": "ICLR.cc/2023/Conference/Paper429/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a collaborative language model called PEER which is trained to imitate the process of collaborative writing. PEER with four T5 models can support the actions of editing, undo, adding explanation, and document generation. To overcome the problem of data scarcity and improve the generalization ability of PEER, the authors adopt self-training to infill the intermediate parts of training data, which increases the quality, amount and diversity of training data. Experimental results show the effectiveness of PEER across various domains and editing tasks.",
            "strength_and_weaknesses": "Strengths:\n\n1) Collaborative language modeling is an interesting and promising direction in NLG, which increases the interpretability and controllability of traditional NLG models. This direction also connects with the industrial products about writing assistants, making the technique of NLG more applicable.\n2) The proposed method based on self-training and instruction tuning is simple and effective, which directly supports the motivation to imitate the collaborative writing process.\n3) The experiment part is well organized. The four research questions are essential for collaborative language modeling. And the authors provide empirical results on diverse tasks and datasets to successfully answer each question.\n\nWeaknesses:\n\n1) Since PEER contains four different models which infill the missing part of training data, the authors should test the performance of each model (including PEER-Edit, PEER-Undo, PEER-Explain, and PEER-Document) to demonstrate the quality of augmented data. Especially the performance of PEER-Document should be shown and analyzed, because in my view it\u2019s hard for a pre-trained model with 3B model parameters to generate high-quality knowledge documents only given the texts before / after editing and plans.\n\n2) More baselines should be included in the main experiments of text editing. For example, Table 1 only contains a copy-based baseline, which may exaggerate the improvement of PEER. At least the Wikipedia subset which contains the edit histories should have more baselines.\n\n3) In this paper, the plan is just defined as a short text sequence like instructions. This setting seems a little bit toy because it doesn\u2019t clearly describe which part should be edited. This may put more burden on the design of instructions. From Figure 3, I find that the instructions mostly contain the position information, such as \u201cadd citation for the model being developed by Meta AI\u201d. But it\u2019s obviously hard when the text is long and has many objectives to describe. Thus, the design of plans should be further discussed to make this scenario more realistic.\n\n4) Typo: the second x_t \u2013> x_{t+1} on the right side of Figure 2 (PEER-Document)",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is overall a high-quality paper with clear clarification and novelty mainly on the applicational side. The reproducibility is degraded due to the complex pre-processing steps of datasets and the lack of codes.",
            "summary_of_the_review": "This paper proposes a novel collaborative language model and gives strong empirical results. I think it\u2019s an insightful paper for NLP community and will recommend acceptance if the authors can successfully solve my concerns in the rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper429/Reviewer_eo6j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper429/Reviewer_eo6j"
        ]
    }
]