[
    {
        "id": "tGZRv5fe5f6",
        "original": null,
        "number": 1,
        "cdate": 1666575832493,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575832493,
        "tmdate": 1666575832493,
        "tddate": null,
        "forum": "IHGnybgLo1Z",
        "replyto": "IHGnybgLo1Z",
        "invitation": "ICLR.cc/2023/Conference/Paper1434/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper provides analysis of out-of-domain detection for different tasks in document understanding domain, including document classification and information extraction. Authors create a benchmark dataset for out-of-domain detection and study the performance a 4 main categories of models for document based on text, image, text+layout, and text+layout+image.",
            "strength_and_weaknesses": "Strengths: \n* This paper provides a first benchmark dataset for out-of-domain detection for document understanding.\n* It provides comprehensive experiment studies on the different categories of multimodal transformer based models for OOD detection.\n\nWeaknesses:\n* The distinction between the in-domain and out-domain types of OOD data is loose. There should be a more rigorous way of defining this two categories based on some concrete similarity metrics.\n* The experiments design is not convincing. For example, the impact of fine-tuning on improving the OOD detection is not surprising, especially as you have defined the in-domain OOD data based on the classification accuracy in the first place.\n* The conclusions drawn from the experiments are not rigorous enough (e.g. Figure 5). For example, in zero-shot OOD detection, the architecture of the model seems to play a more important role than the OOD data being in-domain vs out-domain.",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity:  the manuscript is well written and clear.\n* Quality: the flow of experiments is clear and logical.\n* Novelty: the novelty is marginal.\n* Reproducibility: enough details are included to enable reproducing the results contingent on the data being publicly released.",
            "summary_of_the_review": "Overall, this paper needs improvements to meet the bar of ICLR. Specifically, the in-domain vs out-domain data needs to be more rigorously defined and experiments designs be modified to make it easier to understand the impact of model architecture vs the data domain, thus making the conclusions more reliable.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1434/Reviewer_oNdB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1434/Reviewer_oNdB"
        ]
    },
    {
        "id": "963WYH4kOC",
        "original": null,
        "number": 2,
        "cdate": 1666683115032,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683115032,
        "tmdate": 1671312122275,
        "tddate": null,
        "forum": "IHGnybgLo1Z",
        "replyto": "IHGnybgLo1Z",
        "invitation": "ICLR.cc/2023/Conference/Paper1434/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors analyze the characteristics of spatial information in document understanding and propose a framework to leverage spatial information for document OOD detection. They start from an analysis on the effects across different OOD types and emphasize the importance of spatial information for document understanding. Following the observation, the authors propose to employ an add-on module and adapt transformer-based neural language models to document domains. Analysis and experiments on RVL-CDIP and IIT-CDIP demonstrate their observation and the improvement of Spatial-aware models over the conventional ones.",
            "strength_and_weaknesses": "Strengths\n* In-depth analysis shows the motivation and the potential of leveraging spatial information for document understanding.\n* Extensive experiments not only demonstrate comprehensive comparisons, but also indicate that spatial-aware models outperform conventional models.\n\nWeaknesses\n* Writing needs to be significantly polished. The paper is not very easy to follow. Plus, there are many typos like \u201cspecial-aware\u201d.\n* The authors do not mention the plan of open-sourcing their implementation of spatial-aware models and analysis scripts for the reproducibility.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* For clarity, the authors need to enhance their presentation by significantly polishing organization and writing.\n* The results are interesting with good technical quality, and the proposed spatial-aware adapter also provides the novelty.\n* As mentioned in weakness, the paper does not provide the information about reproducibility.\n",
            "summary_of_the_review": "In sum, I would recommend \u201c8: accept, good paper\u201d based on the extensive analysis and experimental results. Although there are some flaws in presentation and clarity, I believe the authors should be able to address the issues in the final version.\n\nEdit: After the reviewer discussion, we decide to change to the score to \"6: marginally above the acceptance threshold\".\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1434/Reviewer_4z2u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1434/Reviewer_4z2u"
        ]
    },
    {
        "id": "lEOTwcObZRE",
        "original": null,
        "number": 3,
        "cdate": 1667606420297,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667606420297,
        "tmdate": 1670630284242,
        "tddate": null,
        "forum": "IHGnybgLo1Z",
        "replyto": "IHGnybgLo1Z",
        "invitation": "ICLR.cc/2023/Conference/Paper1434/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Summary: The authors aim to provide a systematic and in-depth analysis on OOD detection for document understanding models. They deduce that spatial information is critical for document OOD detection. The authors also propose a simple yet effective special-aware adapter, which serves as an add-on module to adapt transformer-based language models to document domain. They perform experiments to show that their method consistently improves ID accuracy and OOD detection performance compared to baselines. ",
            "strength_and_weaknesses": "Strengths\n1. The authors position the paper as a analysis of OOD for document understanding domain. Given the practical implications and prevalence of documents, this is important work.\n2. The authors evaluate recent transformer based document understanding models for ID and OOD behavior and try to made deductions based on experiments. \n \nWeaknesses\n1. The authors inherently start with an assumption that OOD detection is multi-modal and that single modality cannot solve OOD. They do not show experimental justification on this assumption. Why is a single modality not sufficient? \n2. Miss citing relevant document understanding papers see below. In fact DocFormer has the sota performance on RVL-CDIP dataset (main considered dataset in this paper) and is not cited, nor discussed.\n3. Page 3 - the notation does not seems correct. If threshold gamma decides ID vs OOD, it is possible a model f, wrongly predicts a label for ID test-data. the authors do not dis-ambiguate this scenario in notation.\n4. Page 4 - the authors miss gradient based OOD detection. Logit and distance based are not the only broad research directions for OOD.\n5. Fig. 2 (a) is not entirely correct. e.g. in Spatial+Vision+Language have spatial embedding layers. The authors do not make it clear how this is different from their spatial-aware adapter and in which way it benefits over the embedding approach.\n6. The deductions made in Sec 3.1 seem faulty and has some un-answered assumptions and . e.g. for receipts the authors claim \u201cimprovement of fine-tuning is less significant for out-domain OOD data.\u201d It\u2019s possible that the pre-training data had several receipts which led to ViT exhibiting this behavior. Also why was this experiment only done using vision-only model? its possible text/multi-modal exhibit different behaviors\n7. Page 7, Zero-shot performance. The experiment seems faulty. The IIT-CDIP data the authors use, have the data which the authors deem OOD although in un-labelled form (IIT-CDIP has letters, forms, etc.) For strict OOD zero-shot performance, shouldn\u2019t the authors remove such data? Please comment.\n\n[1] DocFormer - end to end document understanding using transformers ICCV 2021\n[2] more recent document understanding papers also not cited. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - The paper presentation and writing style is clear and a reader in this domain can understand the material.\n\nQuality - Please see the Weakness section for several issues in experimental \n\nNovelty - not sufficiently novel for ICLR. The paper is more of an analysis piece and it has issues which need to be addressed.\n\nReproducibility - no code shared",
            "summary_of_the_review": "Based on the weaknesses in experimental setup, I vote reject for ICLR 2023. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1434/Reviewer_Dbcz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1434/Reviewer_Dbcz"
        ]
    }
]