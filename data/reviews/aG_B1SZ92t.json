[
    {
        "id": "Cj-YE31gOSF",
        "original": null,
        "number": 1,
        "cdate": 1666525472172,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666525472172,
        "tmdate": 1666525472172,
        "tddate": null,
        "forum": "aG_B1SZ92t",
        "replyto": "aG_B1SZ92t",
        "invitation": "ICLR.cc/2023/Conference/Paper4521/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work analyzes high-sensitivity directions in the deep neural policy landscape and proposes a novel tracing technique to detect correlated non-robust features learned by deep reinforcement learning policies. It firstly defines $v$ as a high-sensitivity direction from state $s$ and $\\hat s$ and then approximates $v$ by using the softmax cross entropy loss between the softmax policy and the argmax policy. It also proposes the RADEN algorithm to measure the effects of environmental changes on the correlated non-robust features visually and quantitatively by the Eigenvector G corresponding to the largest eigenvalue of L for the given state set S. Experiments on the Arcade Learning Environment are done to evaluate the idea.\n",
            "strength_and_weaknesses": "Strengths And Weaknesses: \nPros: This paper studies a significant problem to understand the representations learned by deep neural network policies. The method proposed, as far as I know, is the first one to introduce a tracing technique to detect correlated non-robust features. The overall idea is interesting and innovative. \nCons: \n\t(1) I think this paper may need lots of improvement in its writing. It's informal to just use Eqn to refer to the equation, such as Eqn(5) rather than Equation (5) or Eq. (5). There are a lot of sentences in the text that confuse me.\n\t(2) The title of the paper mentions the use of spectral analysis, but the methods section does not clearly explain how to use spectral analysis.\n\t(3) The experimental section does not have an introduction to the experimental setup.\nQuestions:\n\t(4) The title of the paper mentions policy manifold, but there is no definition of policy manifold in the context. Please give a clear explanation of the policy manifold.\n\t(5) Please give a clear Explanation of the relationship between the learned representations of neural network policies and policy manifold.\n\t(6) What is the distributional shift in section 3 question 2?\n\t(7 )Give an explanation of \u03b5 and \u03c9 in Definition 3.1.\n\t(8) Give a clear explanation of the experimental graph, including the meaning of the coordinate axes, etc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is poor.",
            "summary_of_the_review": "The main body of the paper is far away from the title and claimed idea.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_dkrX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_dkrX"
        ]
    },
    {
        "id": "t4AkEWN2We",
        "original": null,
        "number": 2,
        "cdate": 1667267368026,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667267368026,
        "tmdate": 1667267368026,
        "tddate": null,
        "forum": "aG_B1SZ92t",
        "replyto": "aG_B1SZ92t",
        "invitation": "ICLR.cc/2023/Conference/Paper4521/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a method to analyze the vulnerabilities of deep reinforcement learning policies. Experiments are conducted to illustrate the non-robust features, and show how adversarial attack techniques and robust training affect these features. ",
            "strength_and_weaknesses": "Some questions or suggestions about the paper: \n\n1. The authors should address more why Definition 3.1 is an important notion that we should care about. And the formulation of $J(s, s_g)$ also seems to be arbitrary. It doesn\u2019t make sense to let one policy to be the softmax policy and the other be the argmax policy. I guess the author is trying to show that in this setting, the gradient of $J$ is equivalent to Definition 3.1, but the equation (after the sentence \u201cTherefore by definition of the softmax policy we have\u201d) is clearly problematic, missing the factor $T$ somewhere. Even after correcting the equation, the formulation of $J$ still doesn\u2019t make sense, one can just let both policies be softmax and still get a similar relation. Furthermore, wouldn\u2019t it be much simpler to directly define $v$ to be the gradient of some metric function $J$?\n\n2. The authors further define the notion $G_S$ given a set of states. However, the meaning of $G_S$ is also unclear. I would expect each state has its own sensitive direction, which may be of some interest for studying the instability of the policy under each state. But I don\u2019t understand why the seemingly \u201caveraged\u201d vector $G_S$ still conveys the information that worth studying. \n\n3. Propositions 3.1 and 3.2 are straightforward facts, and could be stated more concisely. And the boundedness condition of $\\Lambda$ seems to be somehow unimportant. \n\n4. The authors should give a more formal definition on what is a non-robust feature. According to my understanding, it refers to the high-sensitivity direction $G_S$, though I don\u2019t know why it should be called a feature. $G_S$ is clearly a vector in the raw state space, whose dimension could be prohibitively large for the downstream computation. \n\n5. The figures are hard to read and understand. While Figure 1 and Figure 2 are both RADEN results, in the text only Figure 2 is referred to. In Figure 1, the legend of the color is inconsistent across each subfigure. And the author should explain the meaning of these figures, does the highlighted areas imply the coordinates that are more vulnerable?\n\n6. The authors claim the non-robust features still exist in adversarial training. They should given more explanations how they get this conclusion. The direction $G_S$ always exist, because any symmetric matrix always has the eigenvector with largest eigenvalue. Maybe it\u2019s better to explain which shapes of $G_S$ indicate there exist non-robust features and which do not?",
            "clarity,_quality,_novelty_and_reproducibility": "The overall writing is good. But more explanations should be added to certain parts, or it could cause some difficulties in understanding the paper. ",
            "summary_of_the_review": "The authors propose a new method to study the non-robustness of the DRL policy. I suggest the authors add more explanations to their method and results. Instead of directly giving the definition of the notions $G_S$ and $\\Lambda$, maybe it's better to address why these notions are important for understanding the DRL policy, and how they can be used to guide robust training. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_B6vV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_B6vV"
        ]
    },
    {
        "id": "qw2xigGBhZI",
        "original": null,
        "number": 3,
        "cdate": 1667318133251,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667318133251,
        "tmdate": 1667318133251,
        "tddate": null,
        "forum": "aG_B1SZ92t",
        "replyto": "aG_B1SZ92t",
        "invitation": "ICLR.cc/2023/Conference/Paper4521/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a method to determine the correlated high-sensitivity directions  in the deep neural policy manifold across space and time in the context of reinforcement learning. In this direction, they first define the concept of a high-sensitivity direction for the Q-function at a given state, and show that it is equal to the gradient of some cross-entropy loss at that state. The high-sensitivity direction will contain non-robust features, along which the Q-function changes the most. To capture the correlated non-robust features, the authors then propose to aggregate the information on high-sensitivity directions from a collection of states visited under some given policy. In particular, they suggest to use the direction with the highest average correlation with the gradients of all visited states, and show how this direction can be computed with a spectral algorithm. In addition, they introduce a feature correlation quotient, which captures how correlated the non-robust features from one set of states S' are to a different set of states S.\n\nBased on these contributions, the authors then empirically investigate various questions. First, the effect of adversarial attacks on the learned correlated non-robust features, where they show that these can change quite significantly under adversarial perturbations. Second, the effects of adversarial training on the correlated non-robust features, where they show that they are more tightly concentrated in a small number of coordinates but in different locations that under vanilla training. Third, the effects of distributional shift, where they show that different transformations will change the location of the regions of correlated non-robust features.",
            "strength_and_weaknesses": "Strengths\n- The problem of determining the non-robust features in the neural policy manifold is very important with several potential applications.\n- The definition of high-sensitivity directions is well-motivated and intuitive. The connection to the softmax cross entropy is also interesting.\n- The authors complement the average high-sensitivity direction with the feature correlation quotient, and investigate how the two notions are related in their empirical analysis.\n- The authors provide an empirical study of how correlated non-robust features also arise under adversarial attacks, adversarial training or distribution shift, and how they differ from those in vanilla training.\n\nWeaknesses\n- I am not clear how the paper shows that the direction with the highest average correlation can be so useful in practice. I am convinced about the usefulness of high-sensitivity directions at a given state. But I feel that the authors have not explained with convincing arguments the necessity and usefulness of the average high-sensitivity direction and the corresponding feature correlation quotient. For instance, assume we do RL. At a given time step, we would naturally be interested in the non-robust features and try to mitigate their impact; note that these may vary from one timestep to another. The average quantity certainly deserves merit, but I believe the authors should have motivated it further.\n- I feel that a large part of the experimental analysis is about investigating how the proposed qualitative and quantitative metrics perform under vanilla training, as well as under adversarial attacks, adversarial training, or distributional shift. The findings are for sure interesting. But the question that was left unanswered: how can we leverage all these findings to improve RL? The authors devote one paragraph at the end of Section 4 to discuss how their framework can in principle provide fine-grained vulnerability analysis, but it would have made more sense in my opinion to show in practice rather than in theory how the introduced concepts can improve RL through improved vulnerability analysis (e.g., in a given MDP). I am concerned that much of this discussion has a very theoretical character.\n- Some claims are not fully explained. As an example, the authors claim that there is a change in orientation in the Fourier transform: if the larger entries of the Fourier transform for the vanilla trained policy are more spread out along one axis, the adversarially trained Fourier transform is more spread along the other. This is true in the figure, but I am not sure that an adequate explanation was given.\n- I did not find the contributions to be particularly novel - I liked the definition of high-sensitivity directions but I have some concerns with the treatment for average high-sensitivity directions. ",
            "clarity,_quality,_novelty_and_reproducibility": "- I found the paper quite hard to read, which may have to do with the writing style of the authors. I also feel that even though most parts were clear, there were some parts that were not adequately justified (see the weakness section above).\n- Given the paper is not very theoretical, I believe it would have benefitted a lot from a more extensive experimental evaluation, where the authors would show with specific examples how the improved vulnerability analysis stemming from their framework can lead to improved RL.\n- Overall, I do not dispute the quality of the experiments and the authors have put effort in generating the various heatmaps. However, as mentioned previously, additional experiments could have been much more convincing about the utility of the proposed framework.\n- I think the ideas in the paper are fairly novel (e.g., the high-sensitivity directions), but not significantly novel or disruptive. There are interesting ideas though.\n- I believe the work can be reproduced - the authors provide details about their experiments and several detailed plots and tables. It would of course help if the authors release their code, after their work has been accepted.",
            "summary_of_the_review": "Overall, I believe that this is a work with interesting ideas and experiments. However, I am not convinced that is ready for publication in its current form; I think that was is mainly missing is some empirical \"proof\" that the proposed framework can really improve RL via a better fine-grained vulnerability analysis, e.g., through concrete examples. The authors claim repeatedly that that this is the case, but, given the paper is not so theoretical, I would have expected a more through empirical evaluation where the authors clearly demonstrate the benefit of the proposed methodology.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_iSgv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_iSgv"
        ]
    },
    {
        "id": "5JuJlhBqlmP",
        "original": null,
        "number": 4,
        "cdate": 1667318909721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667318909721,
        "tmdate": 1667318909721,
        "tddate": null,
        "forum": "aG_B1SZ92t",
        "replyto": "aG_B1SZ92t",
        "invitation": "ICLR.cc/2023/Conference/Paper4521/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies how to visualize high sensitivity directions in reinforcement learning. Given a set of states, authors proposed a RADEN algorithm to find the leading eigenvector of certain covariance matrix relevant to the perturbation analysis. The obtained leading eigenvector indicates the directions of vulnerability. Plenty of experimental results are presented to support the effectiveness of the proposed algorithm as well as extend to broader context of interest in reinforcement learning, e.g., distributional shift.",
            "strength_and_weaknesses": "================= Strength =================\n\nExperimental study is comprehensive and provides many graphical illustrations.\n\nThe focus on the robustness of policies in reinforcement learning is of importance to many applications.\n\n================= Weakness =================\n\nAlthough RADEN helps to visualize high sensitivity directions, it is unclear how to incorporate such information for better design policy learning algorithms. In other word, practical implication of RADEN seems to be limited.\n\nIf I were not missing anything, spatial and temporal patterns of vulnerabilities in learned policies are very vaguely discussed, especially the temporal part. I don't think collecting states along a trajectory is an equivalence to temporal dependencies, as the vulnerabilities found by RADEN do not have temporal or even spatial information. The contributions are a bit overclaimed.\n\nThere lacks a coherent motivation of why studying the proposed four questions. The transition from the second paragraph to the third paragraph in Introduction is rather abrupt.",
            "clarity,_quality,_novelty_and_reproducibility": "There is a technical issue in the third display on page 4 in deriving $J(s, s_g)$. By sending $T \\to 0$, $J(s, s_g)$ does not converge --- missing a pre-factor $T$.\n\nBoth Proposition 3.1 and Proposition 3.2 are elementary and direct consequences of their corresponding definitions. The proofs do not provide any insights either.\n\nFigure 1 is not explained in the experiment section. It is also confusing because of inconsistent gauge of colors.\n\nFigure 4 demonstrates that adversarial training and vanilla policy learning yield different Fourier spectrum in vulnerabilities. How do we understand the differences? Does it suggest that adversarial training is better, since it is relatively difficult to find attacks corresponding to the spikes in the Fourier spectrum?\n\n",
            "summary_of_the_review": "I think the manuscript in its current form needs substantial revision to meet the acceptance standard. My major concern is the unclear implications of the proposed method.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_eHD9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_eHD9"
        ]
    },
    {
        "id": "9bhJxDVfGmF",
        "original": null,
        "number": 5,
        "cdate": 1667321868492,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667321868492,
        "tmdate": 1667321868492,
        "tddate": null,
        "forum": "aG_B1SZ92t",
        "replyto": "aG_B1SZ92t",
        "invitation": "ICLR.cc/2023/Conference/Paper4521/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors have considered the problem of understanding the sensitivity of deep reinforcement learning representation landscape. The authors have proposed the tracing technique to detect correlated non-robust features. The authors have defined \"high sensitive\"  direction of Q function in Definition 3.1. This helps to define the high sensitivity direction in Definition 3.2, and eventually connected it to the eigenvector of the highest average correlation with the gradients of the states. The authors have provided experimental results as well in the paper. ",
            "strength_and_weaknesses": "*Strengths*\n- The authors are considering an interesting problem of understanding of the representations learned by DNN policies in RL. This is of sufficient interest to the community. \n\n*Weakness*\n\n- What is non-robust direction? It is used on page 2 without clear definition/explanation. \n- The motivation is not clear to me from the introduction. The authors started talking about the adversarial training issues in the standard supervised learning and then directly mentioned the challenges in deep reinforcement learning. A little more context and motivation is needed.\n- There is no sufficient-related work to contrast the contributions with respect to the literature. \n- Sec 2.2. and 2.3 seems redundant. \n- In definition 3.1, is v and w are the same? \n- The constraint in (7) is the equality constraint, can you comment if the contraint is convex, or if unique solution to problem in (7) exists? \n- What is T in the definition of the policy after (7) ?\n- \"Eqn 7 can be approximated by using the softmax cross entropy loss.\"--why? \n- How to calculate the  gradient of J with respect to state s? Would that require access to the true transition model P ?\n- What is the meaning of non-robust and robust features with respec to the definition on 3.3? \n- I am not sure what Figure 3 is describing? \n",
            "clarity,_quality,_novelty_and_reproducibility": "-The paper is written poorly and hard to follow. \n- The novelty is not clear from the description in the paper. ",
            "summary_of_the_review": "The paper is difficult to follow. The ideas is interesting but the paper requires a lot of work before publication.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_gVJV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4521/Reviewer_gVJV"
        ]
    }
]