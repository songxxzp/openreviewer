[
    {
        "id": "qbCcD8spF1",
        "original": null,
        "number": 1,
        "cdate": 1666645836643,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645836643,
        "tmdate": 1666645836643,
        "tddate": null,
        "forum": "GvMuB-YsiK6",
        "replyto": "GvMuB-YsiK6",
        "invitation": "ICLR.cc/2023/Conference/Paper3272/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method called iPrompt to achieve interpretable autoprompting, that iteratively generates possible explanations of the input data, rerank them, and finally generate new explorations via truncating generated candidates and using them as prefixes to generate new candidates.\n\nThe authors show experiments over Synthetic Math and ANLI where the proposed approach outperforms AutoPrompt and Average Suffix decoding. On sentiment classification datasets the authors show quite competitive performance of iPrompt compared to human-written prompts.",
            "strength_and_weaknesses": "Strengths:\n- The problem of auto generating prompts/explanations given a dataset is important to the research community on prompting/language models.\n- The idea of generating prompt candidates and then reranking/refining them as optimization is somewhat novel.\n\nWeakness:\n- The experiment results are relatively weak.\n1) the main results are only shown for two synthetic tasks, math and ANLI, and are only compared to two relatively weak baselines, AutoPrompt and suffix decoding. Why there is no human-written baselines? In addition, there are more recent methods that try to find better discrete prompts like:\n\na) Gao et al. Making pre-trained language models better few-shot learners. ACL 2021.\n\nb) Deng et al. RLprompt: Optimizing discrete text prompts with reinforcement learning. EMNLP 2022. \n\nc) Schick et al. Automatically identifying words that can serve as labels for few-shot text classification. COLING 2020.\n\nwhich should be cited and compared with if possible.\n\n2) Table 5, on the more realistic task sentiment classification, iPrompt was compared with human-written prompts which shows a relatively better performance. How are the human-written prompts determined? Existing work has shown that instructions/prompts have a large effect on the final task performance, hence for a fair comparison the authors should show best/worst/avg results w.r.t. multiple possible human-written prompts, e.g., from PromptSource (https://github.com/bigscience-workshop/promptsource) where multiple prompts are crowd-sourced for various tasks including sentiment classification.\n\n3) In Table 3, for ANLI, iPrompt is on par with average suffix decoding. Is there any analysis to explain why it can't achieve a better performance on this task?\n\n- All the experiment results are shown on relatively easy tasks (where the prompt can be easily defined/written). Can the authors show results on more complex NLP tasks? E.g., question-answering, natural language inference (the actual task instead of the synthetic one used in the paper). This could better demonstrate how the proposed method is able to generate more complex prompts and understand \"what each task is actually doing\" rather than just providing label choices.\n\n- Better interpretability of the generated prompts is a major claimed contribution of this paper, but it's not well supported and there is no associated study on this. Can the authors provide a human eval to show the generated prompts indeed have better interpretability (and how close it is to human-written prompts)? Based on Table 4, it's really hard to tell how interpretable the iPrompt-generated prompts are, since many of them do not make much sense to humans, e.g., \"Select currency code for a new\" (truncated randomly), \"What is the opposite of 1\" (what is \"1\"?). Especially for the more realistic task sentiment classification, the prompts generated by iPrompt do not seem meaningful at all.\n\n- Some experimental details are not clear. What is $n$ used in experiments for each task? Is there an ablation study on $n$ to show how many samples are needed for iPrompt to perform well?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality: Overall the paper is well-written. Some details are not clear, e.g., what is $n$ used in experiments.\n\nNovelty: The idea of generating interpretable prompts is somewhat novel.\n\nReproducibility: code is provided in supplementary materials.\n",
            "summary_of_the_review": "Overall the problem studies is interesting and the method is somewhat novel, but the experimental results are fairly weak and the interpretability of the generated prompts are not well supported. Thus I recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3272/Reviewer_a8A9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3272/Reviewer_a8A9"
        ]
    },
    {
        "id": "t1W3hAz6cp",
        "original": null,
        "number": 2,
        "cdate": 1666697258294,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697258294,
        "tmdate": 1666702280032,
        "tddate": null,
        "forum": "GvMuB-YsiK6",
        "replyto": "GvMuB-YsiK6",
        "invitation": "ICLR.cc/2023/Conference/Paper3272/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method named **iPrompt**  to automatically generate high-quality prompts using LLM. iPrompt iteratively mutates top prompts by (1) generating explanations with an LLM (2) reranking them based on their performance and (3) exploring new candidates. The authors verify the effectiveness of the proposed method on Math, Sentiment, NLU, and fMRI datasets.",
            "strength_and_weaknesses": "Strength:\n1. The paper is good writing and easy to follow. \n2. The proposed method can search readable prompts without access to LLM parameters and gradients.\n\nWeakness:\n1. The proxy task(evaluation task) is important for the prompt search, however, the authors did not investigate more on the selection of the proxy task. The current proxy task is to measure the closeness and similarity to the \"ground-truth prompt\". However, what is the definition of a \"ground-truth prompt\"? The author did not elaborate more. If it is some existing description of the task label, it may not a good prompt to serve as a \"ground-truth\" prompt. Did the author explore other evaluation methods to select good prompts?\n2. Some results(e.g. Table5) are not convincing. As the PLM zero-shot performance is sensitive to the prompt, the authors should report the average accuracy and std across several prompts when compared with other baselines. The Human-written performance of Rotten Tomatoes/SST-2/IMDB seems too low, as I have simply tried several manual prompts on GPT2-xl(1.3B)  and achieve much higher accuracy. (i.e., the IMDB accuracy of prompt *' The IMDb movie review in negative/positive sentiment is: \"<S1>\"'*  is 83.4)\n3. The compared baseline is weak. More recent baselines should be considered, e.g., Making pre-trained language models better few-shot learners(https://arxiv.org/pdf/2012.15723.pdf)\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality: The paper is well-written and has a clear flow. \n\nNovelty: The idea that using the PLM model to generate a prompt by itself is not new. However, the proposed design of the prompt evolution algorithm is interesting. \n\nReproducibility: Code is provided in the supplementary materails.",
            "summary_of_the_review": "Overall, this paper investigates a practical and interesting direction that generates readable and high-quality prompts by PLM itself. The designed algorithm is well-motivated. However, the experiment part is relatively weak and cannot prove the empirical contribution of this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3272/Reviewer_R7d2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3272/Reviewer_R7d2"
        ]
    },
    {
        "id": "QCgcuOjRvUy",
        "original": null,
        "number": 3,
        "cdate": 1666712038342,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666712038342,
        "tmdate": 1666712038342,
        "tddate": null,
        "forum": "GvMuB-YsiK6",
        "replyto": "GvMuB-YsiK6",
        "invitation": "ICLR.cc/2023/Conference/Paper3272/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper attempts to find interpretable prompts for the few-shot tasks. An iterative local search algorithm is proposed. The algorithm exploits the ability of LLMs to provide interpretable prompt. The algorithm is evaluated on some simple datasets.\n",
            "strength_and_weaknesses": "Strength:\n1. According to Table 4, the prompts found by iPrompt are interpretable.\n2. According to Table 3, the prompts found by iPrompt have higher generalization ability than baselines.\n\nWeak.\n1. the problem is not well-defined. In particular, how to assess the interpretability of prompts? The metrics proposed in this paper only consider \"whether the generated explanation contains one of a set of problem-specific keywords\". I would expect a more reasonable metric.\n2. Whether the algorithm is reasonable is not explained. (1) The iteration does not always converge. (2) Why does the algorithm need multiple iterations? I do not understand the necessity of this iteration. (2) How to ensure that the model generates explanations for the relationship between input and output in the proposal step, e.g. in Fig 3, I don't understand why the LLMs generate \"combine the numbers\".\n3. I have doubts about some experiments. I think some baselines are designed incorrectly. For example, (1) why is the accuracy of GPT-3 only 28.4 for the simple Inverse synthetic math task and 0.6% for the SST-2 task? I think the authors deliberately chose the setting that favors their method. (2) AutoPrompt apparently does not have the ability of generalization across models, so it is not suitable as a baseline for Table 3.\n(4) The task involved in the experiment is very simple. I think this means that the method proposed in this paper is not suitable for solving complex tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to Strength And Weaknesses",
            "summary_of_the_review": "Overall I feel that the problem is not clearly defined. This further led to confusion in the experimental design and inappropriate selection of baselines.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3272/Reviewer_5C3P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3272/Reviewer_5C3P"
        ]
    }
]