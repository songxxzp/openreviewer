[
    {
        "id": "VSpFZ05ECY7",
        "original": null,
        "number": 1,
        "cdate": 1666329423010,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666329423010,
        "tmdate": 1666329423010,
        "tddate": null,
        "forum": "r3-aLHxn2nB",
        "replyto": "r3-aLHxn2nB",
        "invitation": "ICLR.cc/2023/Conference/Paper6370/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new graph contrastive paradigm named CLEP, which learns the latent community structure via variational inference for contrastive learning. It achieves better graph classification performance under both self-supervised and semi-supervised settings. The thorough ablation study also demonstrates the benefits and potential of learning the independent hidden communities.",
            "strength_and_weaknesses": "Strengths:\n1. The method proposed in this paper is novel and well-supported, basically, community detection is also an important problem in graph mining, learning the hidden community structure for contrastive learning is an interesting problem.\n2. The empirical results are solid, consistent improvements over baselines are demonstrated and the authors perform lots of ablation studies.\n\nWeaknesses:\n1. Some parts of the paper are not clearly explained. The authors do not provide an explanation of why Gamma distribution and Weibull distribution are selected in section 3.4. In Table 3, what are task 1 and task 2? Some datasets in Table 3 are themselves binary classification ones, why they are multi-class classification?\n2. The graph classification task itself is not that significant, even considering that the number of classes in these datasets is limited. It would be better if the authors could extend the method to other graph mining tasks. \n3. The sensitivity analysis on the number of clusters K is missing.",
            "clarity,_quality,_novelty_and_reproducibility": "Most of the paper is clearly written, but some parts need more illustration. The paper has good quality in general. The method and the problem studied in this paper are very novel. This paper has a detailed description of its model architecture and training steps, which augment its reproducibility.",
            "summary_of_the_review": "This paper is very innovative in its method and the problem it studies is also significant. The empirical results are also solid followed by a thorough ablation study. Although some parts need more explanation, this paper in general has a high quality. The main limitation of the model is its applicability, which right now only applies to the graph classification task. I vote to accept this paper, but additional experiments to demonstrate the advantage of the model on other graph mining tasks will be appreciated.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_zCcf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_zCcf"
        ]
    },
    {
        "id": "d0GBYROPDpi",
        "original": null,
        "number": 2,
        "cdate": 1666634330864,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634330864,
        "tmdate": 1666634330864,
        "tddate": null,
        "forum": "r3-aLHxn2nB",
        "replyto": "r3-aLHxn2nB",
        "invitation": "ICLR.cc/2023/Conference/Paper6370/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a probabilistic method to combine graph generation and contrastive learning. A key innovation is to factorize the graph into several subgraphs. Each subgraph represents a facet of interactions in a sub-community. The learned latent representation for each node is also factorized into sub-representations w.r.t. each community. Using this fine-grained representation, the paper unifies graph generative models and contrastive learning on the community level, demonstrating better classification performance than previous methods.",
            "strength_and_weaknesses": "Strengths: \n1. This paper unifies two unsupervised learning approaches: graph generative models and graph contrastive learning in a probabilistic framework, which is novel to me.\n2. The idea to factorize the graph facilitates the representation learning framework, demonstrated by experimental results on downstream classification tasks. \n\nWeaknesses: \n1. The motivation to factorize the graph into subgraphs is unclear to me. Since assuming multiple communities will significantly enlarge the number of parameters needed to represent nodes, it is better to justify this complication is worthy.\n2. The experimental part is inadequate for me. E.g., (1) More experiments on studying community structures can better justify the proposed method. (2) Since the paper uses a Gamma prior to the latent representation, it should observe some sparsity level within the node representation. Sometimes, sparsified representations can demonstrate a clearer community representation. But a sparsified representation is probably not optimal for classification tasks. It would be better to include discussions on the choice of Gamma hyper-parameters.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written, but please take care of some typos. E.g., what \"CEGCL\" means in the abstract?",
            "summary_of_the_review": "This paper proposes a novel idea to combine existing approaches to achieve better classification performance via an augmented representation. The performance gain needs some experimental justification to make it more convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_mFPj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_mFPj"
        ]
    },
    {
        "id": "B9-3LQM2L7",
        "original": null,
        "number": 3,
        "cdate": 1666774796275,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666774796275,
        "tmdate": 1666774796275,
        "tddate": null,
        "forum": "r3-aLHxn2nB",
        "replyto": "r3-aLHxn2nB",
        "invitation": "ICLR.cc/2023/Conference/Paper6370/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes to integrate community-aware graph generation (or edge partition) and graph contrastive learning, which is claimed to encode both intra-graph and inter-graph information. Specifically, the basic contrastive loss is extended based on community-aware edge partition, incorporating contrastive learning on both original-augmented and community-community graph pairs. Extensive experiments are conducted on multiple datasets with unsupervised and semi-supervised graph classification, but the results can not demonstrate the effectiveness of the proposed CLEP.\n",
            "strength_and_weaknesses": "Strength\n1. The paper is written well, and the method is described clearly.\n2. Combining the idea of graph contaning multiple conceptual community-aware interactions and graph contrastive learning is novel, although these two methods are borrowed from existing works.\n3. The probabilistic perspective for the proposed CLEP is clear and robust, including the interpretation of the extended contrastive loss function as well as model optimization via varational inference.\n\nWeakness\n\nOverall, the weakness is primarily about experimental results:\n1. Compared with baselines, the improvement in unsupervised classification seems trivial, although the authors claim it's larger than one or two s.t.d. Therefore, the proposed CLEP may not be effective in terms of graph classification.\n2. For semi-supervised classification, only comparing with GraphCL is not convincing, and other baselines should be included, like LaGraph, MVGRL, etc.\n3. For the ablation study, the influence of K (#communities) should be evaluated, especially for K=1. This is necessary to demonstrate the effectiveness of the proposed community-aware contrastive learning.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is well, including writting and model description.\nThe quality is relatively high.\nThe novelty is relatively limited, since two key methods are from existing works.",
            "summary_of_the_review": "Although the idea is interesting and the proposed method seems promising, the experiments are not convinced for the effectiveness of model design.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_AdF9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_AdF9"
        ]
    },
    {
        "id": "RFgxr6mZNfL",
        "original": null,
        "number": 4,
        "cdate": 1666966267107,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666966267107,
        "tmdate": 1666966267107,
        "tddate": null,
        "forum": "r3-aLHxn2nB",
        "replyto": "r3-aLHxn2nB",
        "invitation": "ICLR.cc/2023/Conference/Paper6370/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a probabilistic graph learning framework named CLEP, which unifies graph generative models with contrastive learning paradigm to extract intra- and inter-graph information. CLEP consists of two major components: 1) the graph generative model, which defines a set of encoders to capture the hidden graph structures from different communities; 2) the contrastive paradigm, which is used to convert the learned hidden structures to high-quality graph embeddings. Intensive experiments have been conducted to verify the performance of the proposed CLEP.",
            "strength_and_weaknesses": "Strengths: \n1. In general, the main idea and the proposed model are clearly explained. The proposed model is described in detail, and a competent researcher would be able to reproduce the model.  \n\n2. Intensive experiments have been conducted to verify the performance of the proposed CLEP. Apart from the node classification result on eight real-world benchmarks, this paper also conducted exploratory experiments to highlight the versatility of CLEP on different downstream tasks.\n\nWeaknesses:\n1. The motivation of the paper is quite unclear. At the beginning of this paper, the author argues that \u201cAn integration of graph generative models and graph contrastive learning methods potentially aggregates both intra- and inter-graph information and combines the complementary strengths of these two classes of models, which would further benefit representation learning\u201d. But what is the definition of intra/inter graph information? And how does such information benefit graph learning? More importantly, how does the proposed framework handle such problems, i.e., is there any tailored component or mechanism proposed to extract intra and inter graph information?\n\n2. The concepts used in this paper are somehow casual and confusing. For example, the authors highlight the \u201cintra- and inter-graph information\u201d in the introduction section but quickly turn around to how to capture \u201cheterogeneous community-specific information\u201d in the whole methodology section. How to define \u201cintra- and inter- graph information\u201d and \u201cheterogeneous community-specific information\u201d in the learning framework? And what is the correlation between these concepts? The continuously introduced confusing concepts make this paper very hard to follow.\n\n3. Some important points of the paper are difficult to understand. For example, the authors summarized the proposed method with the sentence, \u201cTo better capture the potentially heterogeneous community-specific information, we define a set of encoders to process the information that comes from different communities\u201d on page 2. Then, first, what\u2019s the community-specific information? Second, why is such information heterogeneous? How to deal with the heterogeneity? Third, how does the proposed CLEP determine the number of communities, i.e., K, for different datasets? Is K a tunable hyperparameter? If so, is the number of communities crucial for the model\u2019s learning capability? If not, is there any tailored mechanism used in this paper to automatically choose the suitable K for different datasets? From the whole paper, I cannot find the answers to the questions listed above.\n\n4. Some important baseline methods are missing. This paper supposes that edges in the graph are built based on K conceptual latent factors. In fact, the same setting has been intensively studied in many disentangled graph learning frameworks, such as DisenGCN(Disentangled Graph Convolutional Networks) and DGCL (Disentangled Contrastive Learning on Graphs).\n\n5. Parameter analysis is totally missing in this paper. As the key hyper-parameter in the proposed model, the positive temperature (\\tau_SEA) used in Equation (3) is not well studied.\n\n6. There are many typos. For example, the author mentioned that \u201cInspired by the \u201cassembly\u201d behavior of communities in graph generation, CEGCL learns community-specific graph embeddings and assemble them together to represent the entire graph\u201d in the abstract. However, it seems that \u201cCEGCL\u201d is a totally new term that has never appeared in this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the main idea and the proposed model are clearly explained and technically sound, and a competent researcher would be able to reproduce the model. But the writing of this paper can be improved, since there are many typos, and some important points of the paper are difficult to understand.",
            "summary_of_the_review": "Generally speaking, the core idea of this paper is interesting. However, there are several aspects that raise non-negligible questions about the presented framework.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_9b1o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_9b1o"
        ]
    },
    {
        "id": "FL1FMGXDnu",
        "original": null,
        "number": 5,
        "cdate": 1667462825694,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667462825694,
        "tmdate": 1667462825694,
        "tddate": null,
        "forum": "r3-aLHxn2nB",
        "replyto": "r3-aLHxn2nB",
        "invitation": "ICLR.cc/2023/Conference/Paper6370/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, author introduce a probabilistic framework called contrastive learning with edge partitioning (CLEP) that integrates generative modeling and graph contrastive learning. CLEP models edge generation by cumulative latent node interactions over multiple mutually independent hidden communities.",
            "strength_and_weaknesses": "**Weaknesses**\n* I don't understand why the author need to use generative model in contrastive learning (CL) and the ELBO that the author propsed seems unrelated to the CL. Can the author explain that.\n\n* The pipline of the over framework is hard to follow. How to generate the embeddings for downstream tasks?\n\n* The authors claim the community information is important in their case. why is that? How do the author verify that the improvement is comes from adding community information. \n\n* To introduce the community information, this work is similar to clustering base GCL.  Related work [1,2] should be compared.\n\n[1] Prototypical Graph Contrastive Learning\n\n[2] Graph InfoClust: Leveraging cluster-level node information for unsupervised graph representation learning\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Fair\n\nQuality: Fair\n\nNovelty: Limited\n\nReproducibility: Not Sure\n",
            "summary_of_the_review": "It quite hard to follow the author's idea, It is seems not significant to introducing the generative model in the context of CL. Overall pipline is not clear.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_JYUf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6370/Reviewer_JYUf"
        ]
    }
]