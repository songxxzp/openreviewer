[
    {
        "id": "HWhDcwY8AhZ",
        "original": null,
        "number": 1,
        "cdate": 1666490398135,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666490398135,
        "tmdate": 1666490398135,
        "tddate": null,
        "forum": "jyHAGzMu-1Q",
        "replyto": "jyHAGzMu-1Q",
        "invitation": "ICLR.cc/2023/Conference/Paper1414/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "this paper proposes to develop linguistic communication between decentralized agents via multiple view contractive learning. which is an interesting problem. it is suggested to combine the contractive loss with the RL loss with a predefined hyperparameter. ",
            "strength_and_weaknesses": "Strength:\nThe problem of linguistic communication between decentralized agents is important \n\nWeaknesses:\n1.  \"Linguistics studies language and its structure, including morphology, syntax, phonetics, and semantics\". I do not see any of these in the paper, even though \"linguistic communication\", linguistics and similar terms are used to describe the advantages of this paper and novelty. There is not language and not linguistics, but the standard multiply view constrictive learning of observations.\n\n2. contrastive (and multi-view) losses can be useful in many scenarios. this paper shows one more such scenario. I am not sure this is enough for a full paper in ICLR. In my opinion the contribution to the knowledge and understanding of linguistic communication for decentralized RL is minor. \n\n3. The current comparison between different approaches is indirect evidence only, because a) there can be different number of parameters in DNNs, (which is not mentioned), b) with more powerful harsher one can achieve better results (more training..). \n\n4. there are no conclusions but rather a summary of the paper. \n\n\n\na few minor comments regarding the phrasing:\n\n1. \"..but many works focus on simple, non-MDP environments.\" what simple non-MDP environments do you mean? \n2. \"...requires fewer assumptions about other agents...\" which assumptions about other are assumed in centralized and decentralized setting? that will help to understand \"..fewer..\". It is desired to provide evidence.",
            "clarity,_quality,_novelty_and_reproducibility": "the use of the terms linguistics/language/communication is not clear.\nthere are numerous works on the combination between contrastive loss and RL loss.\nthe comparison to previous works is implicit evidence only - because the works from 5-6 ago \nutilized simpler / smaller architectures and  less computational resources. \n\n",
            "summary_of_the_review": "there is not linguistics/linguistic-based communication. the objective is standard. there is no general  principles/conclusions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1414/Reviewer_Tpss"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1414/Reviewer_Tpss"
        ]
    },
    {
        "id": "BTOlGHCdxh",
        "original": null,
        "number": 2,
        "cdate": 1666515073717,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666515073717,
        "tmdate": 1666515408895,
        "tddate": null,
        "forum": "jyHAGzMu-1Q",
        "replyto": "jyHAGzMu-1Q",
        "invitation": "ICLR.cc/2023/Conference/Paper1414/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In the paper, the authors propose to communicate between different agents using contrastive learning in a decentralized setting. \n\nAs their main contribution of making agents communicate based on contractive learning for multi-agent reinforcement learning in the decentralized setting, they evaluate the proposed model in different environments and obtain promising results. ",
            "strength_and_weaknesses": " The paper proposes a communication method based on supervised contractive learning with sufficient experiments to evaluate the proposed method.\n\nThe weakness of this paper:\n1. The problem investigation in Section III is confusing, especially in the usage of symbols with lower and upper case, such as s and S for real states. \n2. Meanwhile, is it right for the transition function P->SxAxS?\n3. Some citations are in the wrong formation, such as the caption of Fig. 3 for DBSCAN and t-SNE.\n4. Compared with the workshop paper \u201cLEARNING TO GROUND DECENTRALIZED MULTIAGENT COMMUNICATION WITH CONTRASTIVE LEARNING\u201d,  the current version is updated less in the method.",
            "clarity,_quality,_novelty_and_reproducibility": "There is still a lot of room for improvement in the clarity of the paper.\n\nIt is an exciting job to make agents communicate using contrastive learning in a decentralized setting. While compared with the workshop paper \u201cLEARNING TO GROUND DECENTRALIZED MULTIAGENT COMMUNICATION WITH CONTRASTIVE LEARNING\u201d,  the current version is updated less in the method.\n\nAs for Reproducibility,  I believe that it is easy to reproduce the results of the paper reported for the reasons that all technologies adopted in this paper are all used in other related works. ",
            "summary_of_the_review": "It is an exciting job to make agents communicate using contrastive learning in a decentralized setting. However, the current version is not suitable for ICLR.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1414/Reviewer_dj4f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1414/Reviewer_dj4f"
        ]
    },
    {
        "id": "BMQQ0UuzDh",
        "original": null,
        "number": 3,
        "cdate": 1666691455890,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691455890,
        "tmdate": 1666691837993,
        "tddate": null,
        "forum": "jyHAGzMu-1Q",
        "replyto": "jyHAGzMu-1Q",
        "invitation": "ICLR.cc/2023/Conference/Paper1414/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a self-supervised learning objective using a contrastive loss to learn a medium/language for communication between agents to complete a task (in a decentralized training setting). Similar to Lin et al. (2021), the messages here are proposed to be encodings of observations. To learn these encodings/ messages, the contrastive objective uses positive anchors sampled from the current trajectory around a window and negative samples from other messages in the batch. The authors test their method (CACL) on three MARL tasks: Predator-Prey, Find-Goal, and Traffic Junction. Through different analyses for probing representations and testing for zero-shot coordination, the authors present their method's effectiveness.",
            "strength_and_weaknesses": "### Strengths:\n\n- The authors provide a thorough analysis of their proposed method through various embedding probing techniques:\n    - qualitative plots for different scenarios,\n    - quantitative measures of similarities for messages,\n    - transfer of message encodings to auxiliary tasks\n- The authors also do a great job of comparing their method to well-known techniques in the literature and show how their approach is better than the others for a wide range of measures\n- The section on zero-shot crossplay and the associated analysis was quite interesting to read!\n- **************************Clarity:************************** The paper, the methods, the metrics were quite easy to read and understand.\n\n### Weaknesses:\n\n- I think the paper's main weakness lies in justifying the approach's motivation for the relevant problems. I found the introduction and discussion relatively weak in their description of the broader motivations of the work, particularly those relevant to emergent communication: applications/ transfers to the real world and understanding human communication.\n- More importantly, similar to Lin et al. (2021), it wasn't clear if this was actually communication; sending the complete representation of the observation without accounting for the other agent as there is no bottleneck. A potential baseline that could test this could use a contrastive loss to learn observation embeddings (w/o multiagent dynamics), freeze that and train agents to communicate by using these. It would help us understand how the setting of sending messages and communicating changes representations.\n- **************************Novelty:************************** The approach essentially combines Lin et al. (2021) and with a contrastive info-NCE objective to learn representations.\n\n### Questions:\n\n- In the crossplay setting (Table 4), the interplay agent outperforms the intraplay agent for CACL-AEComm and CACL-PL. Why do the authors think that the interplay agent outperforms agents specifically trained to communicate with each other?\n- Is there a systematic split between the training tasks for the agents and the evaluation ones?",
            "clarity,_quality,_novelty_and_reproducibility": "Detailed in the main review.",
            "summary_of_the_review": "The paper is thorough in comparing with baselines and probing to understand the representations learned by the proposed method. The paper lacks a proper discussion/justification for the setting, the impact of the method used, its implications for different tasks or the emergence of language. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1414/Reviewer_xPcm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1414/Reviewer_xPcm"
        ]
    }
]