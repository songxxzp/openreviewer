[
    {
        "id": "EtSN82VwIV",
        "original": null,
        "number": 1,
        "cdate": 1666620234705,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620234705,
        "tmdate": 1669456531526,
        "tddate": null,
        "forum": "xsNTv784iah",
        "replyto": "xsNTv784iah",
        "invitation": "ICLR.cc/2023/Conference/Paper5158/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Traditional MAML is well used in few-shot learning domains. However, recent works show that MAML does not perform well in a fast adaptation. This paper gives a detail about feature reuse phenomenon and found the relationship between lower layer and the distribution differences in training and testing.",
            "strength_and_weaknesses": "Pros\n\nIn this work, the problem of MAML is divided into two main challenges including algorithmic challenge and architectural challenge . algorithmic challenge represents the performance of cross domain under the domain shift and architectural challenge is related to the model structure such as pixel input and the dimension of the output. These two challenges are reasonable and the results seem good in the experiment section.\n\nCons\n\nNP-MAML updates the body layer with a distance using prototypes. In this respect, for the fair comparison, the architectural challenge needs to be compared with embedding methods and it is similar to embedding few-shot learning methods such as a prototypical network. ",
            "clarity,_quality,_novelty_and_reproducibility": "In my opinion, this work is the combination of a prototypical network and MAML framework. There is no code for reproducibility.\n",
            "summary_of_the_review": "I think this method needs to be compared to SOTA-embedding few-shot learning methods for verifying the effectiveness of MAML + prototypical network not the BOIL and ANIL which are using a fully-connected layer.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5158/Reviewer_gDT4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5158/Reviewer_gDT4"
        ]
    },
    {
        "id": "e2PHsohHoVG",
        "original": null,
        "number": 2,
        "cdate": 1666754536305,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666754536305,
        "tmdate": 1666754536305,
        "tddate": null,
        "forum": "xsNTv784iah",
        "replyto": "xsNTv784iah",
        "invitation": "ICLR.cc/2023/Conference/Paper5158/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors investigate MAML's inner loop adaptation dynamics in a variety of different task distributions and find that in some cases, earlier layer inner loop adaptation is key for improvements, i.e. heterogeneous tasks. They also propose a method that builds on MAML that is able to improve performance in heterogeneous tasks, while remaining competitive in the remainder of the tasks.",
            "strength_and_weaknesses": "Strengths:\n\n- Excellent and thorough empirical evaluations in terms of task formulations\n- Interesting empirical insights about MAML\n- Novel proposed method that is both well-motivated and effective\n\nWeaknesses:\n\n- The paper seems to be missing key related work, such as [1, 2, 3] and many others, but those three in particular studied MAML and came up with lots of useful insights that could have been integrated into this work.\n- Possibly a side-effect of the above, the empirical experiments don't seem to incorporate some of the more powerful MAML-related models or ablation with such SOTA models along with the proposed method on top. As a result, it's hard to judge whether the proposed model is unique in its improvement in heterogeneous cases or whether its benefits disappear when used in conjunction with other powerful and existing SOTA models.\n\n*Request: You state that there are 'no straightforward' ways of making MAML heterogeneous, however, what I've seen done, is learning a single 'class' weight matrix, that is cloned N times for an N-class problem, and then fine-tuned in the inner layer -- therefore learning a generic weight matrix that is quickly adaptable for any new N-way task coming in, no matter what N is. I'd love to see that baseline.\n\n\n1. Rusu AA, Rao D, Sygnowski J, Vinyals O, Pascanu R, Osindero S, Hadsell R. Meta-learning with latent embedding optimization. arXiv preprint arXiv:1807.05960. 2018 Jul 16.\n2. Antoniou A, Edwards H, Storkey A. How to train your MAML. arXiv preprint arXiv:1810.09502. 2018 Oct 22.\n3. Antoniou A, Storkey AJ. Learning to learn by self-critique. Advances in Neural Information Processing Systems. 2019;32.\n\nFor additional few-shot papers that could be of interest, to ensure completeness, might be worth looking at\n\n4. Hospedales T, Antoniou A, Micaelli P, Storkey A. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence. 2021 May 11;44(9):5149-69.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clearly communicated and is of high quality. The novelty of the work is about 6/10, because it tackles a diverse set of under-explored task distributions, and discovers cool insights about MAML.",
            "summary_of_the_review": "I gave this paper a weak accept due to its interesting empirical insights and novel direction of exploring heterogeneous task distributions, and would be willing to increase my rating if:\n\n- More SOTA models are included in the ablation studies.\n- The proposed method is incorporated on top of existing and suitable SOTA methods to check if there are any benefits once a model is sufficiently strong.\n- Some experiments are ran with the baseline stated in the strengths and weaknesses section, as it would dispel any doubts about how a more 'default' MAML model would perform in heterogeneous settings when it is enabled to do so by learning a quickly adaptable 'single class' weight matrix, that is cloned N times for a given N-way task and fine-tuned on the inner loop.\n- More care is taken to include relevant related work, as it feels like the currently included work includes some historical work, and some more recent papers but misses much of the in-between work that is directly related with what the authors are doing in this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5158/Reviewer_nVJV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5158/Reviewer_nVJV"
        ]
    },
    {
        "id": "9qLB5HFrrxS",
        "original": null,
        "number": 3,
        "cdate": 1666818169238,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666818169238,
        "tmdate": 1666818169238,
        "tddate": null,
        "forum": "xsNTv784iah",
        "replyto": "xsNTv784iah",
        "invitation": "ICLR.cc/2023/Conference/Paper5158/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new variant of the MAML algorithm, called \"NP-MAML\" for non-parametric MAML. The method uses a non-parametric head (final layer) using distances to a prototype to assign labels to new data, while still updating the body of the network in the meta-learning inner loop. The paper compares this method to related methods (BOIL/ANIL/MAML) on few-shot image classification tasks.",
            "strength_and_weaknesses": "Strengths:\n- Simple idea, well motivated\n- Clear experiments that support the main conclusions of the paper\n\nWeaknesses:\n- Would be nice to see experiments on other domains (besides images)\n- The representation similarity experiments are intriguing, but could be further developed\n\nMinor comments:\n- In the introduction, the BOIL acronym is introduced in the fourth paragraph without explaining what it is.\n- Missing header in Table 1",
            "clarity,_quality,_novelty_and_reproducibility": "The work is to the best of my knowledge novel. The background, methods, experiments and discussion are all clearly written.",
            "summary_of_the_review": "Overall, I thought this was a nice and interesting modification to MAML. I think the paper could be improved by demonstrating the performance of NP-MAML on additional tasks, as well as providing more intuition for what types of changes are happening in the representation (extending the CKA analysis\u2014although perhaps this latter point is better left for future work).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5158/Reviewer_wHQT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5158/Reviewer_wHQT"
        ]
    },
    {
        "id": "cgVcsXrliyX",
        "original": null,
        "number": 4,
        "cdate": 1666989708500,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666989708500,
        "tmdate": 1669399210995,
        "tddate": null,
        "forum": "xsNTv784iah",
        "replyto": "xsNTv784iah",
        "invitation": "ICLR.cc/2023/Conference/Paper5158/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "* This paper proposes an approach to few-shot classification developing on related works such as MAML and Body Only Inner Loop (BOIL).\n* In few-shot learning settings which involve domain shift between the training and evaluation settings, \nmodels necessarily require some adaptation in their parameters in order to be able to solve the testing time tasks. This is because the representations learned by the earlier layers need to change in order to be effective on the new task.\n* The BOIL method approaches this problem by forcing the inner layers of the network to adapt in the inner loop of meta-learning.\n* However, the BOIL method is constrained in that one must pre-determine the number of output classes at training (and therefore at testing time, which must be equal to the number at train time) because this architectural property of the model cannot change once it is trained. This is a limiting factor when seeking to apply few shot models in practice, when testing scenarios may necessarily be different from training scenarios.\n* This work proposes a method to combine a non-parametric classification head with an adaptation procedure for a network body. This allows both the lower level representations of the model adapt and also the number of classes at test time to vary.\n* In evaluation, the paper standardises a setting where a 4-conv architecture (used in the original MAML paper) is trained on MiniImageNet, and then considers various evaluation paradigms. These include: testing on MiniImageNet (in-domain), testing on FC100 (out of domain), varying the number of ways/shots at evaluation time vs training time.\n* The paper also conducts representational similarity analysis on the methods.\n* The key empirical finding is that the proposed strategy can improve performance in settings over BOIL, MAML, and ANIL (uses no inner loop during training for the network body) where domain adaptation and changes in ways/shots between train and test time occur. \n* In settings where one or both of these do not change, the method mostly performs competitively with the other strategies.\n\n",
            "strength_and_weaknesses": "## Major comments\n\nThe paper's key idea to try to make an algorithm like BOIL more flexible by allowing a non-parametric head makes sense and is generally well-motivated. The experimental results are also overall encouraging, though I am not sure why the proposed method performs poorly compared to baselines in Table 3c, in-domain. \n\nHowever, the paper was hard to read and I feel that the main methodological idea was not that well explained or motivated. Here are a concrete set of improvement areas:\n\n* There are a number of typos/lack of careful definitions in the presentation of the mathematical part of the paper. It is possible to decipher some of these, but it makes understanding the method a lot harder. For example: in eqn 9, what is $d$? What is $l$? I think $l$ is a typo for $k$, but this makes following it harder. In Equation 10 and 11, why are we subtracting $c_i$ and $c_j$? Are these meant to be commas (like in equation 9)? What are $d_{\\text{support}}$ and $d_{\\text{query}}$? What is $g_{\\text{body}}(S;\\theta)$?I don't think these are clearly defined enough, and it makes the contribution very hard to follow.\n* The intuition behind the objective in Eqns 10 and 11 was not explained clearly enough. The figure in the appendix helps, but given this is a very important part of the paper, I think it needs more explanation. Why is this particular form of the objective better than something else?\n* Relatedly, the paper says that the BOIL formulation cannot work well with the NIL head. I am not fully sure why this is the case, and the explanation in the paper was not clear enough -- this seems like the simplest baseline to try. Also, even if it doesn't work, it should be evaluated in the experiments as the most obvious baseline method.\n* Given a major part of the study is domain shift, studying only one setting of domain shift from MiniImageNet to FC100 is in my opinion, not sufficient. I think the paper should at least present analysis on Cars, CUB, or something like that from the BOIL paper. In addition, it would be good to evaluate methods at least overall on Meta-Dataset, given it's a more realistic testbed for FSL methods.\n* Unless I missed it, Table 3 doesn't get much attention in the text, and I would be curious to know more about why the method does worse in the in-domain setting here. The performance deltas in Table 3c, MiniImageNet are quite substantial.\n\n\n## Minor\nSome other typos/suggestions:\n* BOIL could use a citation in the intro.\n* In eq 8, the transpose operator and leading dimension are both using $T$. It's better to use a different letter for the dimension, or change the way the transpose is typeset.\n* Details on proto-MAML are not very clear -- what is the subscript $k$? \n* End of section 2.2.2: typo when defining the last convolutional block?\n* Table 1 is missing a heading -- I think the FC100 heading is in the wrong place, and MiniImageNet heading is missing. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The key idea in the work is to the best of my understanding original, but the explanations in the paper and attention to detail in presentation are not sufficient, making it very hard to follow and appreciate. ",
            "summary_of_the_review": "Overall, I think this idea has promise. However, there are too many errors in the paper's presentation and a general lack of motivation about the method used for it to be accepted at this stage. I think the paper needs substantial revisions and cleaning up in the methodological presentation, as well as some more comprehensive experiments in the domain-shift setting (with other datasets), before I can recommend it for acceptance. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5158/Reviewer_SRwM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5158/Reviewer_SRwM"
        ]
    }
]