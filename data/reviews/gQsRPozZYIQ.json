[
    {
        "id": "WW4ypMIPNE",
        "original": null,
        "number": 1,
        "cdate": 1666620131251,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620131251,
        "tmdate": 1666620131251,
        "tddate": null,
        "forum": "gQsRPozZYIQ",
        "replyto": "gQsRPozZYIQ",
        "invitation": "ICLR.cc/2023/Conference/Paper1105/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a simple variation of batch normalization with a bounded scaling parameter to estimate the importance of each channel. To only suppress unimportant units while preserving important units, the authors design a regularization loss on these bounded scaling factors, with additional hyperparameters. The proposed approach is validated on CIFAR and ImageNet with VGG, ResNet, and MobileNet models.",
            "strength_and_weaknesses": "Strength\n- The idea that shrinks only the outputs of unimportant neurons while maintaining the others is well-motivated.\n- The sigmoid batchnorm is sufficiently simple for people to build on, although the proposed regularization loss involves additional hyperparameters.\n- I believe the experimental validation is quite strong and extensive; various architectures with different capacities were used, and the baselines are recently proposed methods and competitive.\n- The authors provide detailed analyses on the hyperparameters and where to be pruned, which help understanding the network behavior. \n- The paper is generally quite thorough and generally feels complete: I believe it is generally ready for publication if it is decided by the reviewers that significance is sufficient to warrant publication.\n\nWeakness\n- As the authors described, this paper and Polarization Regularizer (Zhuang et al., 2020) share the same motivation and take a similar approach in terms of optimizing batchnorm scaling factors in a desired way. Although the authors briefly mentioned the difference in the related work section (i.e., While the method effectively increases the margin between important and unimportant neurons, it does so by both shrinking and expanding weights.), I am not sure why the proposed method should be better than Polarization Regularizer. Could the authors provide the merits of your work over Polarization Regularizer in more detail? \n- I am not sure whether the proposed method is effective for ResNets and is generally applicable to resblock-equipped networks. For instance, in the right part of Table 2, RNI slightly outperforms the baselines but with more FLOPs at 90% pruning ratio. In the right part of Table 3, at 90% pruning ratio, the accuracies are very similar. Furthermore, in Table 4, the superiority of RNI is more questionable: Slimming and Polarization yield better results with less FLOPs at 50% ratio.\n- I think the comparison in Table 5 should be improved; matching the number of parameters may be too naive. I would recommend the authors to add FLOPs to Table 5 and perform additional experiments using the network with similar FLOPs or latency. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The idea to obtain scaling factors that enable better discrimination between unimportant and important neurons is well-motivated and intuitive.\n- The paper is clearly written and well polished. I think most members of the community would be able to easily dissect it without requiring substantial prior knowledge and without being misled.\n",
            "summary_of_the_review": "In general, I believe this work did a good job in terms of designing interesting sparsity-inducing regularization for structured pruning. Although the in-depth comparison with Polarization Regularizer is necessary and some results are questionable, the experiments and analyses are quite good and thorough.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1105/Reviewer_uAwb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1105/Reviewer_uAwb"
        ]
    },
    {
        "id": "pYs6kfTEa2s",
        "original": null,
        "number": 2,
        "cdate": 1666666868318,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666868318,
        "tmdate": 1666722714178,
        "tddate": null,
        "forum": "gQsRPozZYIQ",
        "replyto": "gQsRPozZYIQ",
        "invitation": "ICLR.cc/2023/Conference/Paper1105/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed a channel-pruning method by regularizing the BatchNorm scaling parameters, and proposed a BatchNorm variation suitable for pruning. The authors point out that the previous BatchNorm-based pruning methods shrink all parameters with an equal gradient irrespective of their importance. Thus, the authors propose to remove the offset $\\beta$ in BatchNorm and bound the scale parameter $\\gamma$ by applying the sigmoid function before multiplication, and the Receding Neuron Importance regularization for pruning. However, the design is lacking in theoretical support and the authors didn\u2019t prove the proposed method achieved better accuracy than previous methods.",
            "strength_and_weaknesses": "Channel pruning is an important task in neural network compression and the effort towards better channel pruning methods is appreciable.\n\nExperimental results are weak. The results on the ImageNet dataset are much worse than the previous state-of-the-art results.\n\n[ref1] Soft filter pruning for accelerating deep convolutional neural networks\n\n[ref2] Pruning filter via geometric median for deep convolutional neural networks acceleration.\n\n[ref3] Metapruning: Meta-learning for automatic neural network channel pruning\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow and the quality is good.\n\nNovelty is limited. The original idea of regularizing BatchNorm parameters for channel pruning is proposed in Slimmable Network (Liu et al., 2017). Based on that, this paper proposed the adjustment for BatchNorm, which seems marginal and not well-supported by either mathmatical proof or strong experimental results.",
            "summary_of_the_review": "The paper is well-written, however, the proposed method seems incremental, the design is lacking in theoretical support and the experimental results are weak.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1105/Reviewer_SWo9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1105/Reviewer_SWo9"
        ]
    },
    {
        "id": "gr5bBetYU_",
        "original": null,
        "number": 3,
        "cdate": 1667066526320,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667066526320,
        "tmdate": 1667066526320,
        "tddate": null,
        "forum": "gQsRPozZYIQ",
        "replyto": "gQsRPozZYIQ",
        "invitation": "ICLR.cc/2023/Conference/Paper1105/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new way to measure the 'importance' of a neuron, by re-parameterizing the scaling parameters of the batchnorm layers. The proposed method is to use a sigmoid on the scaling parameter, to ensure that it lies in [0,1], and the value of the sigmoid reflects the importance. Using this value (value closer to 1 means important), the 'unimportant' neurons are pruned in a one-shot manner. The paper also proposes a new regularizer for this scaling parameter, to control the amount of neurons to be pruned. The results on simpler image classification tasks like CIFAR-10, -100 suggest that this method indeed outperforms other methods, particularly at higher pruning levels. However, on more complex task like Imagenet, it seems to perform worse than the UCS method. Further, the algorithm is only evaluated on image tasks (CIFAR and ImageNet), and two architectures (ResNet and VGG).",
            "strength_and_weaknesses": "Strengths:\n1. The proposed measure and regularizer seem to be a more principled magnitude based pruning method than existing methods that directly work on the magnitude and apply an L1 regularizer. \n\n2. The results on CIFAR-10, 100 suggest that indeed this method has potential, since it outperforms existing methods on higher pruning ratios.\n\nConcerns:\n1. The proposed method is only tested on 2 architectures: ResNet and VGG, and on image tasks CIFAR and ImageNet. \n           \n- Can the method be applied to Transformer architecture on language tasks? \n- The paper applies the method only for one-shot pruning. Can this be applied in multiple iterations, pruning a certain fraction of neurons in each round?\n- Can this method be used to prune pre-trained networks? For example, take a pretrained network and fine-tune if for a few epochs on a downstream task, and then apply this pruning method.\n\n2. Although the method performs well on simpler tasks like CIFAR, it performs worse than UCS on difficult tasks like ImageNet. The paper says that this is because \"This again indicates that for more difficult datasets, global methods over-prune the wrong layers\". I think this is a major limitation. However, I think this might be remedied, for example, by a per-layer re-normalization of the importance scores, so that a similar ratio of neurons are pruned in every layer.\n\n3. (Minor) The paper promotes the proposed method by claiming that it measures the 'importance' of neurons by bounding the scaling parameter of batchnorm in the range [0, 1]. However, I am not fully convinced by this. Consider the case when a neuron has a low scaling parameter, but in its outgoing weights to the next layer, a few (say 10%) of the weights are large. That means that even though the output magnitude of that neuron is low, some neurons in the next layer give high weightage to its output. In that case, this neuron can still be important. In short, what I mean to say is that just the magnitude of a neuron's output might not necessarily capture its 'importance'. While this method still seems more principled than previous methods at using the scaling parameter's magnitude for pruning, I do not agree that it captures the importance of the neuron. I would suggest clarifying this in the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. I have not tried to run the provided code to reproduce the results.",
            "summary_of_the_review": "I think the proposed method to measure the 'importance' of a neuron is more principled than previous methods that use magnitude, but I think the method can be more refined to deal with the problem of over-pruning the wrong layers for complex tasks, and by adding more experiments to show how the method performs for different architectures and tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1105/Reviewer_LW7m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1105/Reviewer_LW7m"
        ]
    }
]