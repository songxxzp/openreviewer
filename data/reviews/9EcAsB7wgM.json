[
    {
        "id": "qnZpUic0jS",
        "original": null,
        "number": 1,
        "cdate": 1665687999870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665687999870,
        "tmdate": 1665699549169,
        "tddate": null,
        "forum": "9EcAsB7wgM",
        "replyto": "9EcAsB7wgM",
        "invitation": "ICLR.cc/2023/Conference/Paper1851/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a training strategy by applying sparsification to obtain a more interpretable network. The training loss combines the feature diversity loss and sparsification loss used in Glm-saga. The measure of feature diversity is proposed and the paper shows that classes with the diverse feature can improve the accuracy. ",
            "strength_and_weaknesses": "\nStrength:\n\nRegularizing the sparse feature for a more interpretable network is a doable method. \n\nWeakness:\n\n1. The paper is not easy to follow with missing details and an unorganized structure. For example, $\\lambda_1, \\lambda_2$ is not explained in Section 3.2.2. In Section 3.2.1, $L_{final}$  is defined as diversity loss plus cross-entropy, while in Section 3.3 $\\mathcal{L}_{target}$ is cross-entropy alone. \n\n2. The main contribution of the paper is not clear. For example, the claimed contribution includes:\n\na). Feature diversity loss: Such loss is widely explored in the existing literature like MCL, FRL, CCMP. It is unclear why the proposed $\\mathcal{L}_{div}$ is better. \n\nb). Feature sparsification loss: It is a direct follow-up of glm-saga. And the sparsification in the final layer leading to better interpretability was also investigated. \n\nc). Measure of interpretability: The measurement in this paper is coarse by counting the dimension of the feature. A low dimensionality does not necessarily lead to better interpretability. For example, one can inject an identity matrix after the final FC layer. The identity matrix is fully sparsed, but meaningless. Therefore, a better measurement would be the one-to-one correspondence between a concept and a single feature vector. Such measurement is already well-explored in Net-dissection (Bau et al., 2017).",
            "clarity,_quality,_novelty_and_reproducibility": "See Strength And Weaknesses. ",
            "summary_of_the_review": "This paper targets an interesting problem. However, the contribution of the paper is not clear. Before having a better understanding of the challenges that this paper resolves, I tend to vote for a reject recommendation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1851/Reviewer_iicb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1851/Reviewer_iicb"
        ]
    },
    {
        "id": "GK6xjJ6_7Z",
        "original": null,
        "number": 2,
        "cdate": 1666486945773,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666486945773,
        "tmdate": 1666532584869,
        "tddate": null,
        "forum": "9EcAsB7wgM",
        "replyto": "9EcAsB7wgM",
        "invitation": "ICLR.cc/2023/Conference/Paper1851/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed model that makes predictions from a sparse set of features. To do so, it employs a novel diversity-based loss and a feature selection process. The model is shown to achieve competitive performance from only very few features (e.g. 5). Empirical analysis on the learned features is able to map them to human-understandable semantic concepts. ",
            "strength_and_weaknesses": "Strengths: \n\n1. The paper differs from most vision interpretability papers in proposing a self-explaining model rather than focusing on post hoc explanations. \n2. This paper proposed a very sparse model that achieves very good performance, using a novel loss formulation. \n\nWeaknesses: \n\n1. My key concern is the analysis of the learned features. First, the global analysis seems to require dense attribute-level annotations, such as what's found in the CUB dataset. However, these attributes are expensive to collect and not often provided.\n2. A hidden assumption behind the above analysis is that the model does use these semantic attributes for decision making. This seems to be a strong assumption, as various works [e.g. 1, 2] found that neural networks can use various spurious correlations and shortcuts, and furthermore many popular explanation methods are not well-equipped to detect such usage. I would appreciate any discussion on this aspect. \n3. The manual feature alignment on the four-engine attribute seems a bit too ad hoc. There could be many different explanations, such as the color or style of the plane, that result in the heatmap focus. Such statements should be made more quantitative, and even better, falsifiable (e.g. does removing two engines affect the heatmap?). \n4. The description on the diversity loss function moves quite fast, especially for readers not familiar with MCL and FRL. The same goes with the feature selection, which assumes prior knowledge of glm-saga. \n\n[1] https://arxiv.org/abs/2004.07780\n\n[2] https://arxiv.org/abs/2104.14403",
            "clarity,_quality,_novelty_and_reproducibility": "See above. ",
            "summary_of_the_review": "Overall, I think this paper is on track for making a good contribution, but the interpretability analyses feel a bit thin. While I would empathize if the authors feel that they may not be necessary because the model is \"self-explaining\", I would push back with [3], which argues to the contrary. \n\n[3] https://aclanthology.org/2020.acl-main.386/",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1851/Reviewer_Vk6D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1851/Reviewer_Vk6D"
        ]
    },
    {
        "id": "0ILg_c2AuT",
        "original": null,
        "number": 3,
        "cdate": 1666658175299,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658175299,
        "tmdate": 1666658175299,
        "tddate": null,
        "forum": "9EcAsB7wgM",
        "replyto": "9EcAsB7wgM",
        "invitation": "ICLR.cc/2023/Conference/Paper1851/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to train a sparse neural network that only utilizes a small portion of all possible input features per class. The paper argues that due to a small number of input features, they are more interpretable. Another main highlight is that despite using only 10% of the features, the accuracy of the tested network remains relatively stable (97-100%). The paper shows that the small number of features that the model uses to make decisions can also be easily aligned with human-nameable concepts for additional explainability. ",
            "strength_and_weaknesses": "**Strengths:**\n\n*[S1]:* Surprisingly effective results: The main highlight of the paper for me is the fact that despite using very few features, the proposed method can achieve such good accuracy results. \n\n*[S2]:* Clearly presented: The paper is well-written and easy to follow. The experiments are described clearly and the main techniques used are adequately clear.\n\n\n**Weaknesses:**\n\n*[W1]:* Unclear why the proposed method is better for anything: The paper does a poor job of motivating exactly why a model using few features is more interpretable. I attempt to understand it from several angles, but do not find a satisfactory answer. \n\n- Possibility 1: Since there are few features, they can be visualized easier: I'm not satisfied because now we are talking about the efficacy of visualization techniques, many of which can map complex decisions to simple-to-understand visualizations (Grad-CAM, etc..)\n- Possibility 2: Fewer features can be easily aligned with the human-nameable concept: I do not understand why the analysis in Fig 3b and Fig 4 can also not be done for traditional networks. In the end, we would only be left with features that correlated with the concepts anyways; Why does it matter whether we run them on 5 or 50 features to start with? If more than one feature correlates with a single concept, they can then simply be visualized as a \"group\". \n- Possibility 3: Fewer features can show us what was really important: There are a lot of ways to rank the \"importance\" of features in neural networks. Why isn't ranking the features by feature importance enough? Why do we need to train a network to only make decisions using these features?\n\nFor all of those, there is a direct actionable item for the authors to show; i.e., show us the results of the alternatives and how the proposed method fares better. \n\n*[W2]:* Limited Novelty: The proposed work is a fairly straightforward adoption of glm-saga; While this would absolutely not be a problem on its own, the fact that there is also not sufficient analysis (W1 above) makes it harder to overlook the fact that the paper does not present \"previously unknown\" technique that others might use differently. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** To me, it is quite clear, See S2 above.\n\n**Quality:** See strengths and weaknesses above.\n\n**Novelty:** There is a limited novelty in both the techniques used and the empirical evidence (and discussion).\n\n**Reproducibility:** The method is built on existing models and a code is also promised, I think reproducibility is excellent. ",
            "summary_of_the_review": "While it was interesting to read, I found myself having critical questions about the main motivations of the paper by the end. The paper hinges on two main things: A network that is \"inherently interpretable\" is better than interpretability tools built on top of traditional networks. And the second is that fewer features lead to better interpretability. As mentioned above in W1, I am not sure about either of those claims and the paper does not show enough proof that it is true. So, despite interesting results (only using a few features gives good accuracy), this is a sub-par paper for me looking at it primarily from an interpretability angle. The paper isn't really written from a feature-compressive perspective, So, I am also unable to comment on that. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1851/Reviewer_9CdM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1851/Reviewer_9CdM"
        ]
    }
]