[
    {
        "id": "Esa8Iv1ycIn",
        "original": null,
        "number": 1,
        "cdate": 1666517375167,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666517375167,
        "tmdate": 1666517375167,
        "tddate": null,
        "forum": "jVDhQl8mPpx",
        "replyto": "jVDhQl8mPpx",
        "invitation": "ICLR.cc/2023/Conference/Paper3739/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper advocates a hybrid approach between inherent interpretability and post-hoc explainability for predictive models trained on image data. The aim is to blur the line between post hoc explanations of a Black-Box and constructing interpretable models, aiming to keep the high performance and flexibility of blackbox models while providing interpretability. To this end, the authors propose an iterative approach where at each iteration, an interpretable model is distilled from the blackbox model. Samples that cannot be routed through the interpretable model(s) at an iteration are routed through a residual network, which is again a blackbox. The method is repeated on the residual network until until the proportion of data explained by the residual network falls below a desired threshold.",
            "strength_and_weaknesses": "Strengths: The motivation is clear, the method is described well and the experiments conducted are extensive. The approach has been tested on a variety of datasets and the results have been discussed extensively and from various angles.\n\nWeaknesses: Section 2 on related work seems a bit short, especially the paragraph on concept- based interpretable models could be extended. What methods are used by others and how does the paper at hand differ from those? In line with that, the contributions of the paper in relation to previous work could be highlighted more. Similarly, there are no comparisons done with existing methods (except against the \u2018baseline\u2019). If there is a specific reason for that, it was not clear. If not, I would advocate comparing to current state-of-the-art methods for symbolic models for image data. Section 5 (discussion and conclusion) would benefit from a discussion of the strengths and limitations of the presented work. Future research trajectories could be discussed more detailed. \n\nHere are a few additional remarks and questions:\n\n- The readers would benefit from a brief explanation about neuro-symbolic interpretable models in Section 3.1.\n\n- The optimization problem (2) looks highly non-linear and non-convex. The authors have not mentioned difficulty of the problem, neither have they discussed the cases of obtaining multiple or only local solutions. They only propose one method to solve the problem in the appendix. However, this part is crucial as the method relies on the solution of this problem.\n\n- What is a good value for K to stop the algorithm? Is there a rule-of-thumb when to stop?\n\n- What is the effect of validation threshold (fixed to 0.7 in the paper)?\n\n- How would the method extend to non-image datasets?\n\n- What is a concept extractor in Section 4.1?\n\n- There are few typos that I spotted in the current version:\n   * In the beginning of Section 3, all $\\phi$ functions should be $\\Phi$.\n   * When referring to a figure, the authors sometimes use \"figure #\" instead of \"Figure #\" \n   * In equation (3), $c_j$ terms should be boldface",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality is acceptable. No code is available (yet) \u2014 reproducibility not assessable. I recommend to use anonymised git repositories for future submissions.",
            "summary_of_the_review": "The authors motivate their case and present extensive results. The aim of the proposed approach is described well but there are important missing details. The performance and results of the approach are discussed extensively. The paper could benefit from a more thorough discussion of related work on symbolic models and limitations of the proposed method. There are several typos in the manuscript.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3739/Reviewer_gZAS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3739/Reviewer_gZAS"
        ]
    },
    {
        "id": "i7G6qC8b83",
        "original": null,
        "number": 2,
        "cdate": 1666630592543,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630592543,
        "tmdate": 1666630592543,
        "tddate": null,
        "forum": "jVDhQl8mPpx",
        "replyto": "jVDhQl8mPpx",
        "invitation": "ICLR.cc/2023/Conference/Paper3739/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose an approach to explain black box classifiers by iteratively dividing the black-box into an interpretable model and a residual. They use first order logic on concepts for the interpretable models.\nA sample input is passed to the system and either routed (by a learnable gating mechanism) to the interpretable model or the residual, until it is interpreted or reaches the last residual.",
            "strength_and_weaknesses": "### strenghts\n- the explanations are relatively easy to read thanks to FOL\n- experiments are numerous and well described\n- the authors demonstrate that biases can be detected\n\n### weaknesses\n\n- a short explanation of the first order logic is missing\n- a short explanation of ``neuro-symbolic interpretable model'' is missing\n- no code\n- complicated approach, training required\n- no comparison with similar concept based explanation methods (like CAV)\n- no quantitative analysis of the produced explanations\n\n\n### questions\n- You write in section 3.1.1 that the selector routes a sample with a probability... Is this routing really probabilistic, so that I would potentially get different explanations when explaining the same image twice?\n- You are using the original black box classifier in the first iteration, if I understand correctly. Why is the performance in Figure 2 then not equal to the black box?\n- why do you not show explanations for the same samples in figure 4?",
            "clarity,_quality,_novelty_and_reproducibility": "### novelty\nThe iterative approach combining several neuro-symbolic interpretable models is novel. \n\n### clarity\nThe paper is, for the most part well written. Notation is fine. Some additional descriptions of concepts from other work that the authors use would have been nice.\n\n### Reproducability\nPseudo code is given but no github implementations, so reproducibility may be cumbersome.\n\n### Quality\nGiven that the approach is quite complicated, the motivation seems a bit poor.\nExperiments are of good quality. Formulas are a bit hard to understand.",
            "summary_of_the_review": "Overall I'm not quite convinced by the paper. As the procedure is relatively complicated and requires training, the motivation should be a bit clearer. Furthermore, there needs to be more quantitative evaluations and comparisons to state of the art concept based explanations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3739/Reviewer_1Nhn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3739/Reviewer_1Nhn"
        ]
    },
    {
        "id": "MeEkitLWTn8",
        "original": null,
        "number": 3,
        "cdate": 1666637894088,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637894088,
        "tmdate": 1666637894088,
        "tddate": null,
        "forum": "jVDhQl8mPpx",
        "replyto": "jVDhQl8mPpx",
        "invitation": "ICLR.cc/2023/Conference/Paper3739/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a method to iteratively extract a mixture of interpretable models from a trained Blackbox",
            "strength_and_weaknesses": "+ Interesting problem\n- Clear missing state of the art cf. below\n- Limited experiments - specially in the context of semantic segmentation",
            "clarity,_quality,_novelty_and_reproducibility": "+ Clear problem\n- No novelty as state-of-the-art is missing cf. (1), and specially (2) which is basically capturing AND/OR expression for annotating neurons, and then could used for computing what has been described in the paper.\n\n(1) David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba: Network Dissection: Quantifying Interpretability of Deep Visual Representations. CVPR 2017: 3319-3327\n(2) Jesse Mu, Jacob Andreas: Compositional Explanations of Neurons. NeurIPS 2020\n\n\n",
            "summary_of_the_review": "Overall the paper is well described, but novelty is not high given past works have achieved similar results",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3739/Reviewer_7Y2K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3739/Reviewer_7Y2K"
        ]
    },
    {
        "id": "dXZ8NZDgy-C",
        "original": null,
        "number": 4,
        "cdate": 1666812272402,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666812272402,
        "tmdate": 1670448325212,
        "tddate": null,
        "forum": "jVDhQl8mPpx",
        "replyto": "jVDhQl8mPpx",
        "invitation": "ICLR.cc/2023/Conference/Paper3739/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The manuscript proposes to bridge the gap between post-hoc explanation methods and interpretable-by-design methods.\n\nThis is achieved by a pipeline where given a black-box model, an interpretable component iteratively distills parts of the representation from the blackbox with uncovered (undistilled) parts forwarded to a residual component. By iterating on this process, the interpretable components achieves a higher coverage of the blackbox model while still preserving a competitive performance.\n\nExperiments on several datasets show the effectiveness of the proposed method on the image classification task for a variety of datasets. In addition, an alternative application of the method is presented on the task of bias detection.\n",
            "strength_and_weaknesses": "Strengths\n+ well-supported idea\n+ tested for bias removal\n+ Good presentation\n\nWeaknesses\n- Relevant aspects not evaluated\n- Limited novelty\n- Some open questions",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\nThe manuscript has a good flow and its content is relatively easy to follow. Overall it was a good read.\n\nQuality\nThe quality of the manuscript is good\n\nNovelty\nWhile the idea of using a surrogate model to enable the explanation of an existing model is not new, the way in which the proposed method implements that idea has several aspects that I find novel.\n\nReproducibility\nThe presentation of the method is relatively clear. Unless I overlooked something, I would not expect major challenges while trying to reproduce the reported results.",
            "summary_of_the_review": "The proposed idea is sound and most of the design decisions are well motivated. To the best of my knowledge, the idea of having this iterative combination of interpretable components with residual components is novel. \n\nI also found positive the fact that the proposed method aims at interpreting different levels of the blackbox model and does not limit itself to the last convolutional layer, as is commonly the case. \n\nThe presentation of the manuscript is good, and the structure followed for the presentation of its content is adequate.\n\nMy main doubts with the manuscript are the following:\n\n- Positioning with respect to Prototype-based methods. In Section 2, the paper discusses ProtoPNet as a representative of prototype-based methods. As part of this discussion, weaknesses from ProtoPNet are stressed. My concern here, is that ProtoPNet is nothing more than the first of these family of prototype-based methods. In this regard there are more recent variants, e.g. Proto-Tree (Nayuta et al., CVPR21) which have addressed the weaknesses of the original ProtoPNet model. Here I would expect the positioning to be made with respect to more recent prototype-based methods.\n\n- On the selection of the number of experts. When describing the training configuration, the number of experts used for each blackbox-dataset combination was indicated. However, it is not clear to me the basis on which the number of experts is selected. Is there a principle manner to decide on the number of experts to use for the proposed method?\n\n- On the semantic association of the detected concepts. In Section 4.2.2, local explanation produced by the MoIE are discussed. Several of these are described based on concepts with a clearly associated semantic meaning , e.g. IrregularStreaks, IrregularDG and Blue Whithish Veil. H \n\n- On the explanation capabilities of the proposed method. Independently of its inner-workings, at the end of the day the proposed method is an explanation method. In this regard, it would have strengthen the manuscript if the proposed method was tested under the sanity checks proposed by [Adebayo et al., NeurIPS'18]. This way there could be guarantees on whether the generated heatmaps/attribution maps do constitute valid explanations.\n \n- Quantitative Comparison. At the moment there is no quantitative comparison on the outputs of the proposed methods with respect to those from state of the art methods. In this regard, quantitatively speaking, it is hard to assess whether the proposed method is a better explanation method than existing ones.\n \n- Minor. There are typos in several places of the manuscript. \n \n-------------- \nReferences\n \n Meike Nauta, Ron van Bree, Christin Seifert; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 14933-14943\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3739/Reviewer_sRwr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3739/Reviewer_sRwr"
        ]
    }
]