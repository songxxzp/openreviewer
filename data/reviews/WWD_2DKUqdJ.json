[
    {
        "id": "OR3MkbwKjDV",
        "original": null,
        "number": 1,
        "cdate": 1666284006524,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666284006524,
        "tmdate": 1666773473986,
        "tddate": null,
        "forum": "WWD_2DKUqdJ",
        "replyto": "WWD_2DKUqdJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1221/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Proposes an out-of-core training approach for message-passing GNNs on very large graphs. Key idea is to partition the nodes + their features on disk, load a few partitions into memory, and then build batches on the subgraph induced by the currently loaded nodes as well as the most frequent nodes, and repeat. Also reports on a small experimental study.\n",
            "strength_and_weaknesses": "Strengths:\n\nS1. Can handle large graphs\n\nS2. I/O mostly sequential and interleaved with communication\n\nS3. Seems to provide reasonable results in study\n\nWeaknesses:\n\nW1. Novelty unclear; related work cited but otherwise ignored\n\nW2. Simplistic & purely heuristic, no analysis\n\nW3. Results far from SOTA\n\nW4. Experimental study setup shaky\n\nDetails:\n\nOn W1. The most relevant related work to this work is MariusGNN (cited here as Marius++). There is no discussion of how this work relates to MariusGNN, both technically and empirically. This is not acceptable. In fact, MariusGNN seems to use a more refined approach and perform experiments on (partly) larger graphs, smaller machines, with faster speed ups. (I coincidentally found a very short paragraph on Marius++ in the appendix after writing this review. I consider this discussion insufficient.)\n\nOn W2. The method proposed here is rather simplistic; e.g., is does not consider to exchange loaded partitions individually, but only all at once. More generally, the entire approach is driven by heuristics, but there is no analysis that justifies the approach. There is also no discussion of what could go wrong.\n\nOn W3. The test accuracies reported in Tab. 2 seem to be far away from SOTA. E.g., for ogbn-papers100M, this paper gives a test accuracy of 0.65 with SAGE. The OGB leaderboard lists an accuracy of .78 with GraphSAGE and goes up to >.85 in all of the top-10 approaches.\n\nOn W4. The experimental setup uses machines with more memory than used by the proposed methods. For evaluating out-of-core methods, this is not good: this memory may be used as a disk cache by the OS, for example.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The exposition is clear, but unnecessarily wordy and overly formal (e.g., Alg 1 is not needed). This approach is simple and can be described much more concisely.\n\nNovelty is unclear, see above.\n\nReproducibility is somewhat unclear, since details about the proposed methods are missing (e.g., how many partitions are used). This point can be addressed if the authors make the implementation available, though.\n",
            "summary_of_the_review": "Potentially useful approach, but falls short on novelty, comparison to related work, analysis, and quality of results. I recommend to reject this paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1221/Reviewer_p8K8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1221/Reviewer_p8K8"
        ]
    },
    {
        "id": "Rg6IRq6yeUH",
        "original": null,
        "number": 2,
        "cdate": 1666345822516,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666345822516,
        "tmdate": 1666345822516,
        "tddate": null,
        "forum": "WWD_2DKUqdJ",
        "replyto": "WWD_2DKUqdJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1221/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes HierBatching, a framework that conducts out-of-core training for GNNs on giant data on small main memory. HierBatching organizes the graph into closely connected partitions and loads some partitions to main memory to conduct training for locality. The nodes with the largest degrees are cached in main memory as they are accessed frequently. Pipelining is used to overlap disk read and computation. Experiment results show that HierBatching runs significantly faster than a na\u00efve solution that directly reads disk.",
            "strength_and_weaknesses": "Strengths\n\n1. Training GNNs on giant data is important, and out-of-core training is interesting as it makes training more accessible for practitioners.\n2. The paper is well-written with excellent clarity and logic flow.\n3. The codes are open-source\n\n\nWeakness\n1. The novelties of the proposed techniques are limited. Partitioning a graph into strongly connected clusters is a standard method to achieve spatial locality in graph processing, DistDGL also uses it for the distributed training of GNNs. Caching hot nodes in has been used by [a, b] to handle limited GPU memory. Moreover, overlapping computation and IO is a common technique in system design.\n\n2. HierBatching incurs accuracy loss, which may not be acceptable for some critical scenarios (e.g., recommendation, where marginal accuracy improvement translates into large revenue). \n\n3. The experiments can be improved. Figure 4 shows that HierBatching uses over 50% of the time for disk IO, which makes me wonder whether training with HierBatching is cheaper than using distributed in-memory solutions such as DistDGL. Note that DistDGL already conducts locality-aware graph partitioning, which reduces cross-machine communication. By using less time for IO, DistDGL may be cheaper than HierBatching is we purchase instances from AWS. Another interesting experiment is to show how the accuracy and training time of HierBatching changes with the amount of main memory. \n\n[a] https://arxiv.org/pdf/2111.05894.pdf\n[b] https://dl.acm.org/do/10.5281/zenodo.6347456/full/\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity excellent\nQuality moderate\nNovelty low\nReproducibility excellent",
            "summary_of_the_review": "I like that the paper tackles an important problem and conducts solid engineering. However, the utilized techniques are well-know and have limited novelty. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1221/Reviewer_Hx4Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1221/Reviewer_Hx4Y"
        ]
    },
    {
        "id": "W6r83KwTyiR",
        "original": null,
        "number": 3,
        "cdate": 1666568927694,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666568927694,
        "tmdate": 1666568927694,
        "tddate": null,
        "forum": "WWD_2DKUqdJ",
        "replyto": "WWD_2DKUqdJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1221/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In order to train GNN efficiently on large graphs with a single machine and external storage, this paper proposes a locality-aware training scheme to reduce the data movement time. First, it partitions the graph using METIS. The main idea is to save a mega-batch, which is randomly sampled from partitions in the main memory, and construct mini-batches from the mega-batch. To alleviate the performance from insufficient neighbors in mega-batch, it permanently saves nodes with the highest degree. Reusing each mega-batch several times and pipelining further reduces the total training time.",
            "strength_and_weaknesses": "## Strength:  \nThe experiments show up to 20 times speedup than in-memory DGL on several large graphs. The analysis of hyper-parameters is in detail.\n\n## Weakness:\n1. My main concern is the convergence of the training because both static cache and reuse of mega-batch can change the sampling distribution of nodes. \n2. I guess the performance including speedup and accuracy is also highly dependent on the parameters, such as mega-batch size and reuse times. Do you need to tune these parameters?\n\n## Questions:\n1. Table 5 provides the partitions (N_d) in mega-batch for each dataset, but I\u2019m not sure how the authors decide it. I think there is a trade-off between accuracy and data loading time when using different N_d.\n2. How many mega-batches are in the main memory? I assume the number is two so that the main memory can read the next mega-batch from external memory and construct minibatches at the same time. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The idea is not particularly novel. ",
            "summary_of_the_review": "Reducing the data movement of GNN training on CPU-GPU nodes has been extensively studied recently. The paper proposes a straightforward technique.  However, it is unclear whether the technique can be applied to more general GNN tasks. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1221/Reviewer_wEnk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1221/Reviewer_wEnk"
        ]
    },
    {
        "id": "r1LIwQJZKi",
        "original": null,
        "number": 4,
        "cdate": 1666753152845,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666753152845,
        "tmdate": 1666753152845,
        "tddate": null,
        "forum": "WWD_2DKUqdJ",
        "replyto": "WWD_2DKUqdJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1221/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors describe a hierarchical strategy for storing large batches of GNN training data in multiple levels of memory in a single system. By partitioning the storage of GNN training data between the extremely large out-of-core disk, smaller but faster CPU DRAM main memory, and much more limited but fastest GPU DRAM memory they are able to train GNNs on graphs that are far too large to fit into either GPU or CPU DRAM memory much faster than naive strategies. To achieve this the authors compare and contrast the training accuracy vs performance tradeoff of their approach vs the traditional neighbor sampling strategy used by default in DGL. A detailed strategy for detailing the high variability in the degree of nodes in a given graph is also given and the authors propose a decomposition of the cache into dynamic and static components to keep the highest utilized nodes throughout the training process in fast memory. An ablation study shows the utility of their approach and the impact on both training accuracy and speed.",
            "strength_and_weaknesses": "Strengths\n-------------------\n- The experiments support the author's motivation to provide a static cache for a small number of highly connected nodes and re-using nodes within a mega-batch while loading additional nodes from disk during training.\n- Ablation studies provide extensive data regarding the tradeoffs between the accuracy and performance of the different approaches.\n- As the graphs of interest for GNN training continue to increase in size it will be of imperative importance to make effective use of the memory hierarchy of the entire system. As demonstrated in Section 4.2 the authors are able to train extremely large graphs using a fraction of the DRAM memory and incurring a moderate amount of degradation in performance.\n\nWeaknesses\n-------------------\n- The major weakness is the lack of theoretical motivation to support the training scenario the authors describe in section 3.3. However, the authors acknowledge this weakness openly in the paper and pose it as an interesting area for future work.\n- Though the ablation studies were beneficial to support the author's claims regarding the use of a static cache to store the nodes with the largest degrees it may have been more straightforward to use a typical metric for cache effectiveness, such as hit-rate per minibatch.\n- The proposed scheme introduces yet another set of hyperparameters that much be tuned per input graph to yield the best results in terms of performance and accuracy, as illustrated in Figure 5.\n- The costs and impact of different graph partitioning strategies are not provided (ie Metis partitions may be of low quality depending on a number of factors, would that have any notable impact on the accuracy of the results?).",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is reasonably clear and provides a clear distinction regarding the problem, motivation, and novelty of the proposed solution. The experiments represent reasonable evidence to support the claims made by the authors. I believe the results could be reproduced based on the information provided in the text.",
            "summary_of_the_review": "Overall the paper addresses an important issue regarding the training of large graphs on machines with limited memory. The authors present a strategy to mitigate the limitations associated with training large GNNs and yield a training procedure that effectively utilizes the hierarchy of multiple memory systems of different capacities and performance characteristics. Though the authors mention the need to minimize the graph cut metric in Section 3.1, was Metis the easiest choice or the best? Also, in Section 3.1 the authors mention an out-of-core partitioner was used when the graph exceeded the memory capacity, please add a reference. It may be worth considering a simpler way to motivate the utility of the static cache size in terms of the hit rate over an epoch or similar metric of typical interest for studying the utility of caches. More information regarding the graph partitioning hyperparameters is provided in the Appendix, Section C.3, were the number of partitions chosen based only on memory considerations? It would be useful if the comment in section 3.1 regarding partitions that are too large or small could be supported by data.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1221/Reviewer_bDQL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1221/Reviewer_bDQL"
        ]
    }
]