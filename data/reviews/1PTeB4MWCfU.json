[
    {
        "id": "bF6UNP1VjMq",
        "original": null,
        "number": 1,
        "cdate": 1666561987746,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666561987746,
        "tmdate": 1666561987746,
        "tddate": null,
        "forum": "1PTeB4MWCfU",
        "replyto": "1PTeB4MWCfU",
        "invitation": "ICLR.cc/2023/Conference/Paper5550/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to tackle the problem of multi-document summarization through synthesis evaluation. Do summarization models synthesize large and varied inputs? The paper studies this question from two angles: movie reviews, where the meta-review is human-written and scores are given (roughly equivalent to sentiment scores); and biomedical systematic reviews of treatments, where multiple systematic reviews summarize clinical trials, resulting in statistical meta-analysis results that can be used as scores. The authors make a good choice of pre-trained models to analyze and show that they do not synthesize a large body of input as well. Then they further analyze how the models fare when we switch the input ordering, or the balance of the input (by ablation). Finally, they propose their own post-hoc method consisting of generating a diverse set of output candidates, and then select the set that best corresponds to an expected synthesis result obtained using an aggregate score predicted by an external pre-trained model.",
            "strength_and_weaknesses": "Strengths:\n- The paper tackles an important problem and proposes a simple post-hoc (inference time) method for improving synthesis in summarization models\n- The metrics provided are automatic and rely on human-written summaries, making the evaluation strong/fair\n\nWeaknesses:\n- The ordering analysis in section 5.2 could result in different results if we augmented the training data by switching the order of the reviews for example.\n- Likewise, the analysis in section 5.3 addresses something that the models were not trained for. The results could see a shift if there is data augmentation with the expected result as the label.\n- The abstention percentage is between 30 and 40%, which is quite high and shows the need for a change in training rather than inference.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well organized. The paper can be reproduced based on the proposed method is somewhat novel and empirically supported. The quality of analysis could be improved.",
            "summary_of_the_review": "While the authors propose an empirically-supported method to improve synthesis in summarization models, their analysis is lacking as they have evaluated models against input ordering and re-balancing, which the models could be trained for but were not. The result is a good contribution, but the analysis suggests that a training-time method can improve the scores further. The high rate of abstention is another flaw that hints at a required change in training rather than inference.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5550/Reviewer_YA3t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5550/Reviewer_YA3t"
        ]
    },
    {
        "id": "5ax5S4x_kJZ",
        "original": null,
        "number": 2,
        "cdate": 1666593183841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666593183841,
        "tmdate": 1666593183841,
        "tddate": null,
        "forum": "1PTeB4MWCfU",
        "replyto": "1PTeB4MWCfU",
        "invitation": "ICLR.cc/2023/Conference/Paper5550/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the synthesizing capabilities of multi-document summarization models and presents a method for increasing those capabilities in summarization. \nSynthesizing capabilities are measured with respect to an application-dependent latent factor, the model is required to generate outputs that align with that factor in the input. Two cases have been studied: movie reviews with the latent factor of sentiment, and medical study results with the latent factor of significant effects for interventions. The investigated models proved to be lacking in synthesizing capabilities, but the proposed method raised these to human level (with regard to the metrics).",
            "strength_and_weaknesses": "Strengths:\n[S1] The paper extends the conceptualization of desiderate for multi-document summaries to synthetization.\n[S2] The paper is well written and methodological sound.\n[S3] The proposed measures, given that factor information is available, could be used as evaluation metric for MDS tasks.\n\nWeaknesses: \n[W1] The novelty of the method is limited. \n[W2] Some questions in the design and results of the experiment remain open. \n[W3] A stronger baseline could have been used. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow. I especially liked the technical accuracy, being neither too complex nor sparse. The level of detail for the implementation is solid and generally seems reproducible. The authors don't provide the source code, which would greatly improve reproducibility (and speed up subsequent research).\nThe novelty is limited, the solution to address the weakness of the models is an application of existing methods.\n\nBaseline/Related Work\n- Shah et al. (2021a) solves a similar task, in a different application area. The authors need to state the technical relation to this paper in more detail. It seems to be a valid, stronger baseline than SoA summarization models not focusing on factors, and I am wondering why it hasn\u2019t been used in the experiments (source code is available).\n \nExperiments / Open Questions\n- How the cases when the model abstained from a decision (Table 5) counted in the evaluation (e.g., Figure 7)?\n- If the related work cited in footnote 2 is complementary, a remark on why that\u2019s the case and why it doesn\u2019t serve as a baseline would be interesting. Otherwise, this raises the question of whether the right baseline has been chosen.",
            "summary_of_the_review": "The paper addresses an important issues for multi-document summarization, and show empirical evidence that this issue persists in SoA summarization models. Conceptualisation and experiments are generally sound. It misses a detailed discussion of one highly relevant work, and it's inclusion in the comparison. The paper is generally reproducible from the given details, inclusion of source code would make more so. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5550/Reviewer_18KJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5550/Reviewer_18KJ"
        ]
    },
    {
        "id": "e1Ux9uU4UhG",
        "original": null,
        "number": 3,
        "cdate": 1666660836298,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660836298,
        "tmdate": 1672058550338,
        "tddate": null,
        "forum": "1PTeB4MWCfU",
        "replyto": "1PTeB4MWCfU",
        "invitation": "ICLR.cc/2023/Conference/Paper5550/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates the synthesis problem in multi-document summarization. Then, this paper proposes a simple method: Given a set of documents, the proposed method generates diverse candidates using the diverse beam search and selects one candidate that aligns with the expected aggregate property of inputs.",
            "strength_and_weaknesses": "Strengths:\n\n- This paper tackles one of the important issues in multi-document summarization: the existing multi-document summarizers can synthesize inputs properly.\n- This paper includes extensive analysis with statistical results on two data sets.\n\nWeaknesses:\n\n- The main criticism is that the diverse beam search (DBS) does not generate diverse candidates with various ``properties''. The DBS generates a set of candidates by penalizing the generation of strings similar to candidates on beams in other groups. In DBS, the similarity is only considered at the token level, not the property of interest level. DBS is not guaranteed to generate diverse candidates with various properties of interest.\n- The proposed strategy 'generate-diverse-then-select' should be compared with the strategy 'generate-then-select.' This paper has no empirical studies to show the superiority of the 'generate-diverse-then-select.'\n- There is no report of the ROUGE1 score on movie review experiments (Table 4). The main goal of the proposed strategy is to select a summarization that aligns with the expected property of inputs and the target summary. Only showing $R^2$, MSE, and Pearson's r results cannot prove the superiority of the proposed method because it selects the wrong summarization but aligns with the expected property.\n- The proposed method can do a cautious summarization, but there is no evaluation of the cautionary of the summaries. Human evaluations or correlations of automatic factuality metrics should be included.\n- There are two versions to predict sentiment on movie data set: BERT trained on the SST data set, and BERT trained on the IMDB data set. It is a wonder why this paper adopted different models.\n- Some evaluation settings are missing, like the number of beams. Furthermore, selecting the true sentiment in Table 4 is unclear. Please add the details to the appendix.\n- As mentioned in the introduction, the problem considered in this paper is only for some applications, such as movies or medical, not all applications for multi-document summarization, like news. The reviewer thinks the title is too generalized for what the paper considers.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper contains originality because this paper is the first study to investigate synthesis in multi-document summarization.",
            "summary_of_the_review": "The paper shows insights about the synthesis of the existing multi-document summarizers. However, the experiments lack to prove the superiority of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5550/Reviewer_Z9mN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5550/Reviewer_Z9mN"
        ]
    },
    {
        "id": "VSGPLjiTdq",
        "original": null,
        "number": 4,
        "cdate": 1666676246563,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676246563,
        "tmdate": 1666676246563,
        "tddate": null,
        "forum": "1PTeB4MWCfU",
        "replyto": "1PTeB4MWCfU",
        "invitation": "ICLR.cc/2023/Conference/Paper5550/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the capacity of summarization models to synthesize (potentially conflicting) information from multiple documents. It defines a model-based metric for the aggregate latent aspect of interest, in this case, the sentiment of movie reviews (Rotten Tomatoes dataset) and the treatment efficacy of medical trials. This metric is applied to reference summaries as well as summaries generated by Longformer, PEGASUS, PRIMERA, and T5. Then, the results are compared to gold labels from the datasets so that a higher correlation would be an indicator for better synthesis capacity. The experimental results indicate inconsistent synthesis for off-the-shelf summarization models. The authors then propose a technique for improving model synthesis by ranking candidate summaries according to the expected aggregated aspect.",
            "strength_and_weaknesses": "Strengths:\n1. The paper presents an interesting way to evaluate summarization that complements traditional ROUGE scores.\n2. The experiments are conducted in two quite different domains.\n\nWeaknesses:\n1. It is not clear why the authors decide not to fine-tune the measurement model on the datasets, especially for Rotten Tomatoes. The BERT and RobotViewer models were trained on human-written inputs and are now used on machine generated summaries, which might detrimental for its performance (and correlations to gold sentiments).\n2. The summarizers are the \"base\" models and the gap in quality with respect to the large versions could change some of the conclusions. At least one experiment with a large version (PEGASUS, for instance) could make the empirical evidence much stronger.\n3. Citation missing: Oved and Levy (2021) also used a strategy to generate several summaries and rank them according to a desired criteria. https://aclanthology.org/2021.acl-long.30/\n\nMinor comments:\n1. Figure 10 in the Appendix is cut.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and easy to follow. Experimental details and hyperparameters are well-defined for reproducibility. The proposed evaluation approach presents novelty but the idea of generating several candidate summaries is already explored in previous research.",
            "summary_of_the_review": "This work presents a novel way to evaluate multi-document summarization and its experimental methods are sound. I would recommend the authors to address the experimental improvements listed above to make the claims stronger.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5550/Reviewer_ACAC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5550/Reviewer_ACAC"
        ]
    }
]