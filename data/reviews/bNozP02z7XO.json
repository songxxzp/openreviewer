[
    {
        "id": "1Xi9KCuc9g",
        "original": null,
        "number": 1,
        "cdate": 1665926077779,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665926077779,
        "tmdate": 1667828915694,
        "tddate": null,
        "forum": "bNozP02z7XO",
        "replyto": "bNozP02z7XO",
        "invitation": "ICLR.cc/2023/Conference/Paper4895/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper claims in the abstract that they develop a method for exploration that is \"theoretically well motivated, and comes with zero computational cost while leading to significant sample efficiency gains in deep reinforcement learning training\". The claim about their experiments is that the \"technique improves the human normalized median scores of Arcade Learning Environment by 248% in the low-data regime\".",
            "strength_and_weaknesses": "Strengths:\nThe claims of the paper are ambitious.\n\nWeaknesses:\nThe paper is not sound.",
            "clarity,_quality,_novelty_and_reproducibility": "CLARITY\nThe paper has several elements that should be written differently. In the abstract only, here are three elements:\n- It is written that deep RL \"achieved high acceleration in its progress\". A constant progress would already mean that things improve steadily. An accelerating progress is even more specific and I think that using such terms is debatable and should be avoided in a technical abstract.\n- It is written that efficient exploration is \"a well-understudied and ongoing open problem\". Beyond the choice of the word \"well-understudied\", it feels a bit odd to say that exploration is \"understudied\" because exploration is known as being one of the top challenges in RL.\n- The algorithm is described as coming \"with zero computational cost\". It should be made clear that it is compared to another algorithm. In addition, the term \"zero\" is quite precise and I'm unsure you can actually claim this.\n\nAdditional elements lack clarity, for instance in the Algorithm 1: What is $e$? It seems that the replay buffer is not used in the learning phase?\n\nQUALITY, NOVELTY\n- Conceptually, my main concern is that the strategy of the algorithm does not sound reasonable. Imagine that you have initialized all Q-values at 0 for a given state. Imagine now that from that state, one of the actions gets a negative Q-value (e.g. negative reward followed by terminal state) and another one a postive Q-value (e.g. small positive reward followed by terminal state). In that case, it seems that the algorithm would never explore the other actions in that state.\n- The paper claims that it demonstrates \"theoretically that our method MaxMin Novelty based on minimization of the state-action value results in higher temporal difference loss, and thus creates novel transitions in exploration with more unique experience collection\". This claim is very strong (as it seems to be always true, including with deep learning as a function approximator). That is not what is see in terms of theoretical results in the paper.\n- From Table 3 in the appendix, the results are much less convincing as compared to what they are presented in the paper.\n\nREPRODUCIBILITY\nThe source code does not seem to be provided.",
            "summary_of_the_review": "The paper studies a potentially interesting setting but the claims are not well substantiated.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4895/Reviewer_3c9Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4895/Reviewer_3c9Y"
        ]
    },
    {
        "id": "51rntYYy25i",
        "original": null,
        "number": 2,
        "cdate": 1666616911985,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616911985,
        "tmdate": 1666616911985,
        "tddate": null,
        "forum": "bNozP02z7XO",
        "replyto": "bNozP02z7XO",
        "invitation": "ICLR.cc/2023/Conference/Paper4895/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "As an alternative to the usual $\\epsilon$-greedy exploration used in DQN and similar approaches, the authors propose taking the action with the minimum Q value with probability $\\epsilon$.",
            "strength_and_weaknesses": "Strengths\n- I found the chain MDP experiment to be quite interesting to transmit the issue the authors want to tackle. That is, the fixed noise that comes from initialization of parametric Q functions.\n\nUnfortunately, I have some major worries with this paper.\n\nWeaknesses\n- After some thought, I realized that the proposed method isn't guaranteed to converge to an optimal policy even in very simple examples. Take for example a 3-armed bandit problem with rewards 0, 1 and 2 per arm. An agent that learnt the Q-values 0, 1 and 0.5 for the arms (in the same order as before), will take the second arm (maximum Q value), except for when it's exploring that it will take the first arm (minimum Q value). Thus, it will never be able to explore the third arm, which is the most rewarding one. This is already a big red flag about this algorithm.\n- On top of the main issue with the theory of the paper, I don't find the experimental results convincing. In order to make a fair comparison HP-searches must be ran for each individual algorithm. At the very least, the value of $\\epsilon$ should be tuned differently, as it has a very different role in each algorithm. In fact, model-free approaches have been shown to achieve much higher scores in [1]. I suspect the 0.03 of $\\epsilon$-greedy from this paper comes mainly from a lack of proper hyper-parameters.\n\n[1] https://arxiv.org/pdf/1906.05243.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was easy to follow. The theory felt unnecessary in my opinion.",
            "summary_of_the_review": "The proposed approach isn't guaranteed to converge to an optimal policy, even in the tabular case. While this could be acceptable if the experimental part of the paper was very solid, that is not the case either. I expect the authors to clearly state that the approach doesn't converge to the optimal policy if they plan to submit this paper to any conference/journal.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4895/Reviewer_7Z6y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4895/Reviewer_7Z6y"
        ]
    },
    {
        "id": "wBfaQvNmjy",
        "original": null,
        "number": 3,
        "cdate": 1666873184055,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666873184055,
        "tmdate": 1666873184055,
        "tddate": null,
        "forum": "bNozP02z7XO",
        "replyto": "bNozP02z7XO",
        "invitation": "ICLR.cc/2023/Conference/Paper4895/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This method proposed to use minimal action with the lowest Q value to maximize novelty. The experiment demonstrates the effectiveness of the proposed methods. \n\n",
            "strength_and_weaknesses": "\n\nStrength:\n\nIt's interesting to see that using the minimum-valued action could lead to better exploration. The proposed method is very simple and requires minor modifications to existing methods. \n\n\n\nWeakness:\n\nGenerally, it's hard for me to follow the motivation for using minimum-valued action. The paper seems to claim the minium-valued action should be the one that maximizes the novelty. I didn't see the definition of novelty in Section 3. Although the paper provides several theorems in Sec 3, it's hard to get the main idea of these theorems. \n\nThe improvements in Figure 2 are not very significant. Moreover, on some tasks, the paper demonstrates results with 200 million frame training, while on other tasks, the paper demonstrates result with only 50 million frame training (e.g. Enduro). This is very misleading. Did the proposed method converge on these incomplete-reported tasks?\n\n\n\n\n\nSeveral Questions:\n\nAlgorithm 1 seems quite unclear to me.\n\nWhat do you mean by Populating the Experience Replay Buffer? \n\nDo you mean you need the MDP model (including the transition and reward function)?\n\nHow to obtain $s_{t+1}^{min}$ based on $a^{min}$ (and how to obtain $s_{t+1}$)?  \n\nWhat do you mean by $e$?",
            "clarity,_quality,_novelty_and_reproducibility": "\n\nThe main idea of the paper is hard to follow. And there is some confusing part in the theorems, experiments, and algorithms.\n\nThe code of this paper is not open-sourced. Supplying code could help the readers understand the details of the paper.\n\n",
            "summary_of_the_review": "\n\nThis paper tried to propose a new novelty maximization method with minor computation costs. However, the main motivation for using minimal-valued action is hard to follow. There are some confusing parts in the theorems, experiments, and algorithms that make it hard to assess the idea.\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": " ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4895/Reviewer_S1TB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4895/Reviewer_S1TB"
        ]
    },
    {
        "id": "0hyy19fxfGu",
        "original": null,
        "number": 4,
        "cdate": 1667196104738,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667196104738,
        "tmdate": 1667196104738,
        "tddate": null,
        "forum": "bNozP02z7XO",
        "replyto": "bNozP02z7XO",
        "invitation": "ICLR.cc/2023/Conference/Paper4895/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method for efficient exploration in discrete-action MDP environments. The intuition is to incentivize the agent to take actions that maximize \u201cnovelty\u201d, which is measured in terms of the temporal difference (TD) error from the transition. The authors show that under certain assumptions, the expected TD error achieved by the minimum-value action (the actions that minimizes the Q value) is higher than the expected TD error achieved under a uniform action distribution. Based on this motivation, a simple alteration to the $\\epsilon$-greedy exploration is proposed where, instead of sampling a uniform action with probability $\\epsilon$, the minimum-value action is taken with probability $\\epsilon$. Experiments on ALE and ALE 100K benchmarks show that the proposed algorithm compares favorably to the baselines ($\\epsilon$-greedy, NoisyNetworks) in terms of sample efficiency due to improved exploration. ",
            "strength_and_weaknesses": "**Strengths:**\n\nThe paper makes an interesting connection between the expected TD errors under the minimum-value action and the uniform action distribution (proposition 3.1). The proposed exploration strategy is computationally lightweight and, on the evidence of the evaluation section, seems to improve the sample efficiency in several benchmarks, especially in the limited interaction setting. \n\n**Concerns to address:**\n\nI have concerns about a couple of details mentioned in the paper and the overall presentation. I request the authors to address the following:\n1.  As a conclusion to Proposition 3.1, it is mentioned that the expected TD-error achieved by minimum-value action is higher than that achieved by uniform action distribution. Does this require the entity $D(s) \u2013 2\\delta - \\eta$ to be non-negative, and if yes, does that somehow follow from the definitions? \n2.  Propositions 3.1 and 3.2 are fairly similar in terms of the idea and the proof. I\u2019d recommending moving the Double-Q equations to the appendix to improve the reading experience. \n3.  Algorithm 1 box \u2013 could you clearly demarcate (with colors preferably) the difference between standard DQN with $\\epsilon$-greedy and MaxMin-Novelty? Are there differences both in the experience-generation and the gradient-calculation aspects? Also, there seems to be some notation errors in the TD definition in the box.\n4.  Motivating example (Section 4) \u2013 when there are just 2 actions, does the difference between $\\epsilon$-greedy and MaxMin-Novelty reduce to just the value of $\\epsilon$? Could you add $\\epsilon$-greedy with different values of $\\epsilon$ to Figure 1?\n5.  In Section 6, it\u2019s not clear if the TD values are computed for all transitions, or only for the transitions with the exploratory action (taken with probability $\\epsilon$).\n6.  It would be good to comment on the limitations of the current method, and/or future work. Any thoughts on extensions to continuous-action environments?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is there or thereabouts in terms of clarity and quality, though I\u2019d like to see resolution on at-least some of the aforementioned issues. Leveraging state-transition novelty or \u201csurprise\u201d for exploration is a well-researched area, but the paper\u2019s approach of using the minimum-value action and the supporting propositions are novel, to the best of my knowledge. In terms of reproducibility, the authors have included the hyperparameters and the architecture details, but I didn't find a link to the anonymized code. \n",
            "summary_of_the_review": "My initial rating is \u201c5: marginally below the acceptance threshold\u201d. The paper introduces a simple and interesting idea to facilitate exploration and improve the sample efficiency in deep-RL. The results are encouraging. However, I have some concerns about the material presentation and would be happy to reconsider after discussion with the authors.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4895/Reviewer_P9oC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4895/Reviewer_P9oC"
        ]
    }
]