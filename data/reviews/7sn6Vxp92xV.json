[
    {
        "id": "a2esSHneyu",
        "original": null,
        "number": 1,
        "cdate": 1666214442075,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666214442075,
        "tmdate": 1666214442075,
        "tddate": null,
        "forum": "7sn6Vxp92xV",
        "replyto": "7sn6Vxp92xV",
        "invitation": "ICLR.cc/2023/Conference/Paper1368/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper extends the Masked Auto-Encoders (MAE) with Vision Transformers by incorporating a Teacher-Student setup (asymmetric siamese model with exponential moving average update for one of the streams).  The proposal uses a reconstruction loss over the masked patches and adds a consistency loss over the reconstruction of both streams.  The experiments show an improvement over MAE in ImageNet-1K classification, and COCO detection and segmentation downstream tasks.\n\nThe paper also frames the Teacher-Student setup as a memory of past features that are used by the student to follow the gradient of the previous samples' features.  The analysis was done over a linear simplification of the Teacher-Student model, and an experiment shows the gradient responses on different cases of similarity and dissimilarity of the samples.",
            "strength_and_weaknesses": "Strengths:\n- The paper uses a simple extension to the mask autoencoders with ViT (MAE) by adding a consistency loss over an asymmetric siamese setup (Teacher-Student), following the success of recent asymmetric self-supervised models that rely on exponential moving average for the update of the other stream.\n- The results show improvement over the existing MAE, and two approaches for masking are explored and evaluated.\n- The role of the teacher as a memory bank of past gradients is explored.  A simple experiment is performed on a linear version of the Teacher-Student setup that shows the changes on the gradient w.r.t. the similarity of the features.  The same experiment is repeated with less explainability with more complex models.\n\nWeaknesses:\n- The claims of using the \"role of the teacher\" as basis to propose the extension of the model are not clear.  \n\n  In particular, the proposed loss of reconstruction and consistency is straightforward on the multi-stream setup.  In its current form, the paper reads as adding more complexity by trying to explain the bank of gradients as a basis for the extra consistency term in (6).  While the result is interesting, it doesn't seem to resonate with the proposal of updating the MAE.\n\n  Consider a simpler and cleaner approach of introducing the consistency and reconstruction loss, instead of adding unnecessary complexity to justify the claims.\n\n- It is not clear from the explanation in the Appendix how well the projection ($S\\hat{x}$) and the difference of the agreement between the teacher and student ($(S-T)\\hat{x}$) relate to the original data ($x$) also influence the final gradient.  \n\n  Wouldn't errors in the projection or difference between the streams also influence the gradients?  And that will also make a difference on the bank and what is stored in it?\n\n  Should the generic linear model also contain a term that shifts the vector $\\Theta \\tilde{x} -x$ depending on the difference of the streams as well? ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity.**  The paper proposes a simple extension of MAE, that gets complexified by the evaluation of the gradients.  The explanation of the expansion of the gradients and the memory bank are hard to follow from the main description.  After several reads of the appendix one can figure out the details though.  The readability could be improved if more explanations were provided in the main text.\n\n**Reproducibility.** The paper is heavily inspired by MAE (He et al., 2022).  The details of the implementation are left open, and it is described that He et al.'s implementation was followed.  It will be good for the sake of reproducibility to detail the paper setup, regardless of point the reader to He et al.  Moreover, it is not clear if the pre-training includes the decoder and the reconstruction of the image on top of the patches reconstruction.\n\n**Novelty.** The inclusion of a consistency and reconstruction loss over an asymmetric siamese network is not new.  The application to patches may be new.  The idea of masking and denoising as a source of supervision is not new as well.  The benefits seem to come from the ViT that helps to take advantage of the patches structure.  However, that was proposed before by He et al. \n\nThe expansion of the gradient and trying to understand the bank of gradients is interesting, and I'm not aware of others doing a similar exploration.  However, the link between that exploration and the proposed loss avoids me.",
            "summary_of_the_review": "The paper proposes a simple yet effective loss based on the reconstruction of patches using ViT as an encoder, and a two-stream asymmetric model with exponential moving average as update for one of the streams (teacher-student) that should agree in their reconstructions (consistency loss).  The claim of the authors of using the understanding of the teacher as a bank of gradients to propose the above mentioned setup is beyond me.\n\nThe setup is complicated, and tries to tie to topics and package them together.  Perhaps a simpler write-up with these two pieces presented individually could be easier to follow and allow them to shine.\n\nThe details about the implementation (encoders, decoders, and setup) is not presented.  The paper references the same implementation as He et al.'s setup, yet the differences for this proposal are not explained.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1368/Reviewer_5RHA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1368/Reviewer_5RHA"
        ]
    },
    {
        "id": "bTRImqVXw8",
        "original": null,
        "number": 2,
        "cdate": 1666361744238,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666361744238,
        "tmdate": 1666361744238,
        "tddate": null,
        "forum": "7sn6Vxp92xV",
        "replyto": "7sn6Vxp92xV",
        "invitation": "ICLR.cc/2023/Conference/Paper1368/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper firstly analyzes the properties of mean teacher in a masked data modeling task with a simplified linear model. The mean teacher contributes to the training by gradient correction, and it is similar to the memory queue. Several empirical findings about the mean teacher are introduced based on the experimental observation.    \nNext, the paper proposes RC-MAE (Reconstruction-Consistent Masked Auto-Encoder), which add a EMA teacher for MAE. The proposed model achieves better performance compared to the baseline on classification, detection and segmentation benchmarks. Besides, RC-MAE costs less computation resource and obtains more stability.  ",
            "strength_and_weaknesses": "**Strength**:    \n[1] The theoretical analysis of EMA teacher dynamics is solid.      \n[2] The reviewer appreciate that the RC-MAE objective does not introduce a new hyper parameter. The reconstruction term and consistency term simply add together without a balance.   \n\n\n**Weakness**:    \n[1] Although RC-MAE outperforms MAE on classification, detection and segmentation benchmarks, the improvement is not significant enough.     \n[2] With all the mathematical derivations, perhaps more insights and conclusions could be obtained.    ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well written and easy to follow. This paper makes the points clear in the math proofs. \n\nNovelty: The reviewer is confused about RC-MAE being a major contribution or just a validation for EMA teacher dynamics. For this simple method, greater experimental improvement are expected.  \n\nReproducibility: This paper provide sufficient details to reproduce the results. \n",
            "summary_of_the_review": "This paper explains the role of mean teacher in a MIM model and proposes RC-MAE that achieves better performance than MAE. With good quality and fair technical contributions, this paper is marginally above the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1368/Reviewer_JA1p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1368/Reviewer_JA1p"
        ]
    },
    {
        "id": "h6H5vIYmahw",
        "original": null,
        "number": 3,
        "cdate": 1666532886731,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666532886731,
        "tmdate": 1670810946738,
        "tddate": null,
        "forum": "7sn6Vxp92xV",
        "replyto": "7sn6Vxp92xV",
        "invitation": "ICLR.cc/2023/Conference/Paper1368/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a simple SSL method, the Reconstruction-COnsistent Masked Auto-Encoder (RC-MAE) by adding an EMA teacher to MAE. Experiments show that RC-MAE converges faster and requires less memory usage than state-of-the-art self-distillation methods during pre-training. Compared with MAE, RC-MAE consistently outperforms on various tasks, including image classification on ImageNet-1K, object detection and instance segmentation on COCO.",
            "strength_and_weaknesses": "Strength:\n+ This paper gives a analysis contributation of EMA teacher in self-supervised learning, and finds that the gradient provided by the teachers conditionally ajusts current gradient direction and magnitude conditioned on the similarity of features.\n+ EC-MAE achieves similarly performance for MAE with faster convergence speed, aversatial robustness.\n+ EC-MAE saves both memory and computation compared with self-distillation-based MIM methods.\n\n\nWeaknesses:\n+ The performance on ImagNet-1K is not impressive compared with recent concurrent works.\n+ Lack of downstream tasks like semantic segmentation tasks\n+ Why proposed method achieve better robustness, could you give some explanation?",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is not novel and there is no code for reproduction.",
            "summary_of_the_review": "See Strength And Weaknesses",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1368/Reviewer_jGgU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1368/Reviewer_jGgU"
        ]
    },
    {
        "id": "uKpnbYpJOJ",
        "original": null,
        "number": 4,
        "cdate": 1666584698671,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584698671,
        "tmdate": 1666584893175,
        "tddate": null,
        "forum": "7sn6Vxp92xV",
        "replyto": "7sn6Vxp92xV",
        "invitation": "ICLR.cc/2023/Conference/Paper1368/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This submission proposed a new simple yet effective method, named Reconstruction-Consistent Masked Auto-Encoder (RC-MAE), by equipping the latest ViT-based Masked image modeling (MIM) with mean teachers.\nThe authors derive some approximations (using a simple linear model) on MIM pretext setting to analyze the role of mean teachers. Some synthetic experiments are provided to support this mathematical analysis. Finally, experiments on classification (ImageNet-1k), and object detection (COCO) tasks show consistent improvement over MAE and with faster training time (RC-MAE, 800 epochs, 166.6 hours ~= MAE, 1600 epochs, 256.5 hours).",
            "strength_and_weaknesses": "Strengths\n\n- simple and effective\n- proved mean teachers technique is compatible with ViT-based MAE empirically\n- slightly stronger representation and much faster self-supervised training but requires not very much extra memory/computation cost compared to MAE, making RC-MAE more scalable compared to MAE and more practical than other self-distillation MIM methods (MSN, iBOT)\n\nWeaknesses\n\n- The final solution is exactly the combination of two well-known techniques, which makes sense but is too straightforward. Also, introducing some kind of memory mechanism to improve MAE has been demonstrated effective in BootMAE, which proposed a momentum encoder using EMA update. I think in-depth analysis and a thorough comparison with BootMAE are required.\n- The accompanied mathematical analysis and the synthetic experiments are good. But it is an extremely simplification of the actual model (ViT), which makes me wonder if the conclusions drawn from the equations are applicable to RC-MAE.",
            "clarity,_quality,_novelty_and_reproducibility": "The originality is OK. The writing is clear and easy to follow. The overall quality is good.",
            "summary_of_the_review": "Although the proposed method is effective, it is very straightforward and provides very much ``expected'' results. The analysis and conclusion from an extremely simplified linear model may not be the same as the actual ViT-based model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1368/Reviewer_rLZG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1368/Reviewer_rLZG"
        ]
    }
]