[
    {
        "id": "_XRr7wbohiC",
        "original": null,
        "number": 1,
        "cdate": 1666576615426,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576615426,
        "tmdate": 1666576615426,
        "tddate": null,
        "forum": "8re-nA0wDxW",
        "replyto": "8re-nA0wDxW",
        "invitation": "ICLR.cc/2023/Conference/Paper3016/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a novel diversity metric that measures the dissimilarity between few-shot learning tasks. The authors find that popular few-shot learning benchmarks - MiniImagenet and Cifar-fs, score low on the proposed diversity metric. The authors then compare the performance of MAML to transfer learning in these two benchmarks and find that under standardized hyperparameters, MAML and transfer learning are statistically similar in performance across many model configurations. Thus, the authors conclude that the similarity in performance between MAML and transfer learning is largely due to the low task diversity.\n",
            "strength_and_weaknesses": "Overall, I appreciate the thoroughness in the execution and presentation of the experiments in this work. The ablation over multiple probe networks lend credence to the robustness of the proposed metric over the choice of the probe network. The hyperparameters and training details of each model used in the experiments are well documented. As such, the paper states convincingly that the performance of MAML and transfer learning are statistically equivalent on MiniImagenet and Cifar-fs. \n\nThe writing quality of the paper is fairly strong. The authors went into great detail to explain background works, including formulations for the feature similarity metrics, in the appendix, which makes the work nicely self-contained. There is no noticeable ambiguity in any of the formulations/algorithms used in the paper. The proofs regarding similarity of a MAML-type algorithm and a \u201cfixed representation meta-learner without adaptation\u201d are correct, albeit straight-forward.\n\nThere are some problems that I notice in this work, which I list in the order of severity. \n\n1. The claim that MiniImagenet and Cifar-fs has low diversity is stated without context. While indeed the proposed diversity coefficient can range from 0 to 1, it is not clear where do real meta-learning datasets actually lie on this 0-to-1 scale. Without this context, one cannot claim that a value of 0.1 or 0.06 should be considered \u201clow\u201d. \n\n2. The authors suggest that their experiments point towards the following conclusion: \u201cif the task diversity is low, then transfer learned solutions might fail to outperform meta-learned solutions\u201d. In close examination, this claim is in fact vacuous (or unsupported, depending on the interpretation of if). Even if we assume the claim regarding \u201clow\u201d diversity to be correct, the experimental results (on real datasets) in this paper only indicates that transfer learned solutions fail to outperform meta-learned solutions in \u201clow\u201d diversity tasks, but this could be the case regardless of whether task diversity is low. The observations have only shown co-occurrence (\u201croses are red, sky is blue\u201d), but a non-vacuous claim at least requires correlation (\u201croses are more red when sky is blue\u201d), which requires negative examples (\u201croses are not red, sky is not blue\u201d). \n\n3. The synthetic experiment in this paper may refute the above issue as it shows transfer learning out-performing MAML (the Gaussian experiment) in \u201chigh\u201d task diversity settings. Yet, the control variable (task diversity) has a significant confounding variable that could directly explain the observation (USL beating MAML), the variable being task difficulty. The authors adjust the variance of the distribution of class means (sigma_m) to create a range of benchmarks with different diversity values. However, increasing the variance of class means also increases the average distance between the class means, which in turn increases the margin in classification boundary between each class for any given 5-way classification episode, thereby making the classification problem easier. This can be easily observed in figure 3, where the meta-test accuracy of both methods increases consistently with increased diversity, indicating a decrease in difficulty. We can then postulate that USL out-performs MAML in the 3 benchmarks on the right-end of the graph not due to diversity, but due to difficulty. A likely explanation is that the L-BFGS algorithm used for optimizing the last layer is fully converged, while MAML-5\u2019s 5 gradient steps are not fully converged. When the classes are so well separated (e.g. when inter-class variance is 20 times that of intra-class variance), a linear model (i.e. last layer of a mlp) can be trained to convergence on very few examples without fear of overfitting, as the likelihood of outliers is vanishingly small. This explanation predicts that using L-BFGS on the last layer with the feature-extractor learnt by MAML should close the performance gap, as the difference is in the meta-test time optimization, not MAML vs USL. In addition, this experiment highlights the fact that the proposed diversity metric is sensitive to task difficulty, which should be an unwanted side effect. The authors should conduct experiments in settings where the diversity varies while the difficulty is controlled, to illustrate how MAML and USL responds to various diversity levels.\n4. While the authors do not make any strong claims w.r.t their theoretical analysis, I want to point out that the analysis misses the point by analyzing the learner trained by equation 12 instead of equation 13. Without any task adaptation or conditioning, it is rather obvious that the decision rule can only learn the mean prediction over all tasks (as pointed out by the authors themselves). This decision rule is entirely useless, as it will only learn trivial solutions when the assignment between classes and labels is shuffled (e.g. all standard benchmarks). On the other hand, the result of when equation 11 is equivalent to 12 in the zero task diversity setting is weak in its current form - a much more useful result would be to show that the two learners tend towards the same when diversity converges to zero.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "In balance, I think the paper does not make a sufficiently novel claim to meaningfully contribute to our collective knowledge, thus I recommend rejection.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3016/Reviewer_NZ7N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3016/Reviewer_NZ7N"
        ]
    },
    {
        "id": "f0x12TL6sNU",
        "original": null,
        "number": 2,
        "cdate": 1666596209965,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666596209965,
        "tmdate": 1666596209965,
        "tddate": null,
        "forum": "8re-nA0wDxW",
        "replyto": "8re-nA0wDxW",
        "invitation": "ICLR.cc/2023/Conference/Paper3016/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Model agnostic meta-learning (MAML) and transfer learning (TL) are popular techniques for learning under data scarcity such as  few-shot learning. While the effectiveness of MAML over TL has been shown for medical image analysis tasks, the verdict on classical benchmark datasets in this domain is still unclear. This paper presents a systematic study comparing MAML and TL under fair conditions (architecture, optimizer, etc.) across diverse tasks. The authors introduce a diversity coefficient to measure task diversity using Task2Vec representations. Using this measure, the authors show that TL and MAML results are comparable outcomes when the task diversity is low. This observation holds even as the model size changes.\n",
            "strength_and_weaknesses": "Strengths\n\nThe earlier work by Raghu et al, did compare MAML against multi-class (USL) and multi-task learning. However, their analysis was restricted to studying the quality of features learned through different training regimes. The key difference between Raghu etal\u2019s work and this paper is the training of an additional classifier (adapt head only). The feature extraction backbone is not very influential in the low-task diversity setting as logistic regressors trained using MAML and USL feature extraction backbones result in comparable performance. This is an interesting observation and main strength of the paper (though I have some reservations as elucidated later)\n\n[Raghu et al] Aniruddh Raghu, Maithra Raghu, and Samy Bengio. Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML. Technical report, 2020. URL https://arxiv. org/abs/1909.09157.\n\nThe diversity metric based on the Hellinger distance of the Task2Vec representations is quite intuitive and sound.\n\nThe authors conduct experiments using benchmark datasets for few-shot learning. The variety of distance metrics such as SVCCA, PWCCA, and the likes reinforces the observations from the earlier experiments. \n\nWeaknesses\n\nThe diversity metric is computed using only the Resnet backbones (Table 1). I wonder if the similarity of the backbone architecture is a cause for the similar diversity results? Experimenting with diverse architectures (perhaps VGG and densenet variants) might strengthen the observation. \n\nMy primary concern is the task that the authors have used for quantifying diversity. The results in Table 1 and 2 are from 5-way 5-shot tasks (though I understand that 20 shots were used to estimate the diversity score in Table 1). I believe that a 5-way task is insufficient to draw out the diversity across tasks, especially when the tasks are sampled from miniimagenet or CIFAR-FS datasets. I would urge the authors to try out 10-way or even higher N-way tasks, with the same number of shots. If the authors were keen to stick to a 5-way task, I would have expected results from a 5-way 1-shot setting as well. The current set of results, though suggestive of a trend, are not convincing enough to draw an inference.\n\nI like the experiment on increasing the model size as a function of the number of filters. The results are presented in Figure 2. However, here too, I find that the authors have stopped the experiment at a critical juncture. I would have liked to see runs beyond 30 filters. There is an increasing trend in favor of USL. This should ideally get amplified with larger model size. It is difficult to conclude by stopping at 30.\n\nI also find results presented in Figure 3 to be quite strange. How can the meta-test accuracy increase with higher Task2Vec diversity coefficient? One would have expected the accuracy to actually reduce with harder tasks, the current trend seems to be quite counter intuitive as both MAML and USL have higher accuracies on difficult tasks. Further, similar to the previous observation, the authors stop at 0.5, when things start to get interesting - the difference between MAML and USL is apparent here. I would urge the authors to continue further (as indicated in the last subplot of Figure 3).\n\nWhile using synthetic data helps to control task diversity, I would urge the authors to try a more natural setting of higher task diversity - cross domain-few shot learning tasks. This should give better insights into behavior of both MAML and USL.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized. However, certain critical aspects are missing, or often relegated to the supplementary material. For example, MAML5 and MAML10 are defined in the caption for Figure 1, but appear in Table 1. \n\nThere are a few typos - Figure 2 caption MAM15 - MAML5.\n",
            "summary_of_the_review": "Overall, the paper is an interesting attempt to contextualize the behavior of MAML and USL algorithms. While I agree that task diversity is an important metric for evaluation, I find the work needing additional experiments. There are interesting patterns in the results, but I am afraid the experiments are not complete enough to draw conclusions. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3016/Reviewer_nDz4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3016/Reviewer_nDz4"
        ]
    },
    {
        "id": "Qa2A13v0Ak",
        "original": null,
        "number": 3,
        "cdate": 1666650212257,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650212257,
        "tmdate": 1666650212257,
        "tddate": null,
        "forum": "8re-nA0wDxW",
        "replyto": "8re-nA0wDxW",
        "invitation": "ICLR.cc/2023/Conference/Paper3016/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies when and how meta-learning algorithms should be used to solve few-shot classification tasks. The authors introduce a \u201cdiversity coefficient\u201d, which they use to measure how varied a set of tasks is. They show that (some) standard few-shot classification have low diversity \u2014 in other words, all tasks are easy so both MAML and transfer learning perform equally well. From those experiments, they conclude that meta-learning and transfer learning methods yield equivalent performance on few-shot classification tasks.",
            "strength_and_weaknesses": "- Strengths:\n    - The use of Task2Vec to compute diversity is novel in the context of few-shot classification (but Achille et al. already used it for similarity across vision tasks).\n    - I appreciated the experiments on the toy Gaussian experiments: it clearly shows a strong correlation between the proposed diversity coefficient, ground truth Hellinger distance, and the meta-test accuracy.\n- Weaknesses:\n    - More thorough validation: while the paper focuses on MiniImageNet and CIFAR-FS (which are well-known to have similar train and test tasks), I wonder if the same results would hold on FC100 and TieredImageNet. The latter benchmarks are specifically designed to increase task diversity, and I wonder if the proposed metric would be able to quantify this diversity. Second, I would like to see a comparison of methods to measure task diversity. Three come to mind:\n        - First, the authors could use the ones they mention (eg, CCA, CKA) to measure similarity between the obtained representations.\n        - Second, Dhillon et al., 2019 propose a measure of task difficulty, which can be used to measure diversity (along the axis of difficulty).\n        - Third, Ethayarajh et al., 2022 (Understanding Dataset Difficulty with V-Usable Information) also propose a measure of difficulty between datasets, but this measure actually builds on the information contained in a dataset (akin to the diversity in this paper).\n    - Missing discussion of prior art: several important prior works are missing from the discussion in this paper. In its current form, I am not sure that the paper offers novel insights.\n        - Dhillon et al., 2019 shows the same results as Tian et al., 2020, and so does Wang et al., 2019 (SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning).\n        - Arnold and Sha, 2021 (Embedding Adaptation is Still Needed for Few-Shot Learning) also show that transfer learning methods shine when train and test tasks are similar, but MAML has the upper hand whenever they are dissimilar. Collins et al., 2020 (How Does the Task Landscape Affect MAML Performance?) compare the settings under which MAML is competitive and can outperform a pretrained feature extractor.\n        - Several recent works (incl. at ICLR this year) have carefully studied MAML and drastically improved its performance on few-shot classification. See for example: Ye and Chao, 2022 (How to Train Your MAML to Excel in Few-Shot Classification).\n    - Presentation: the paper needs significant work in terms of presentation to be up to the ICLR standards. Some examples:\n        - Equation 2 and 3 have formatting issues.\n        - Citation: Achille UCLA et al., 2019 should not have UCLA.\n        - Figure 1 needs improvements to be legible. It is also misleading \u2014 in few-shot classification, we typically use 600 or 2000 tasks to compute confidence intervals. Doing so would significantly shrink the error bars.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity:\n    - The paper reads mostly OK but doesn\u2019t feel quite polished. See my remarks on presentation above.\n- Quality:\n    - This paper could also be improved in terms of quality, by discussing prior work and comparing to it.\n- Novelty:\n    - This is the biggest flaw of the paper: while the diversity coefficient is arguably novel (Achille and collaborators have also used Task2Vec to measure the distance across tasks), it is used to provide insights that are already known to the community.\n- Reproducibility:\n    - The authors use open-source software, and I believe most of their results are reproducible.",
            "summary_of_the_review": "- Main strength:\n    - Use of Task2Vec to measure task similarity in few-shot classification.\n- Main weakness:\n    - Lack of novelty \u2014 most insights in the paper are either well-known or obsolete by now.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3016/Reviewer_NP6p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3016/Reviewer_NP6p"
        ]
    },
    {
        "id": "HIQfioZlv6",
        "original": null,
        "number": 4,
        "cdate": 1666775020193,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666775020193,
        "tmdate": 1666775020193,
        "tddate": null,
        "forum": "8re-nA0wDxW",
        "replyto": "8re-nA0wDxW",
        "invitation": "ICLR.cc/2023/Conference/Paper3016/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the question of whether transfer learning (USL) is superior to meta-learning (MAML) for few-shot classification (multi-task), as suggested by recent studies.\nIt shows that the two methods have identical performance when task diversity is low, while their performance is different when task diversity is high.\nIt proposes a new measure of task diversity and argues that previous studies had not compared the two methods fairly.\nIt also shows that predictions of USL and MAML are similar although they learn different representations.\n\n",
            "strength_and_weaknesses": "Strengths\n\n- The debate around whether transfer learning or meta-learning should be used for few-shot classification is interesting and not resolved.\n- Experiments are convincing: they run with multiple architectures, model sizes, datasets, metrics and have several controls.\n\n\nWeaknesses\n\n- Showing differences between USL and MAML for high task diversity is crucial for testing the central hypothesis of this paper.\nYet, this is done only in synthetic Gaussian data in section 4.5.\nIt would be good to have a high diversity benchmark with real data, instead of Gaussians.\nThis could be done, for example, by separating the 100 classes of CIFAR-FS into hierarchies, with separate tasks for animals, vehicles, etc.\nOne way of doing this is described in https://ojs.aaai.org/index.php/AAAI/article/view/20590\n\n- I find it very surprising that USL is better than MAML for high task diversity. \nMAML is designed to deal precisely with this situation, while USL is not.\nThe authors should discuss this result in much more detail.\n\n\nMinor points\n\n- I don't understand why Eq.2 needs to be in this paper, it doesn't seem to be used anywhere.\n\n- \"distances based on Task2Vec are good approximations to the ground truth distance of task distribution.\" \nI don't understand this sentence, FIM is a metric in the space of parameters (or parametric distribution), not in the space of tasks.\nWhat is the \"ground truth distance of task distribution\" anyway?\nOverall, section 3.3 includes several unclear (and maybe wrong) statements, this section could be entirely removed.\n\n- Results in Table 2 are not entirely clear to me. \nHow can USL perform worse than chance (<20%) in any circumstance?\nWhy results depend so much on the adaptation method used?\n\n- \"the two methods are equivalent in a statistically significant way\".\nThis statement is repeated several times in the paper but is incorrect. \nThe correct statement is: \"differences in the two methods are not statistically significant\"\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear although it could be improved (see above).\nResults are novel and seem reproducible.",
            "summary_of_the_review": "Overall the strengths slightly overweight the weaknesses, but the paper could be improved a lot.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3016/Reviewer_Bxys"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3016/Reviewer_Bxys"
        ]
    }
]