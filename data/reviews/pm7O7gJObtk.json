[
    {
        "id": "pXmgmzh2a_",
        "original": null,
        "number": 1,
        "cdate": 1666581620468,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581620468,
        "tmdate": 1666581941104,
        "tddate": null,
        "forum": "pm7O7gJObtk",
        "replyto": "pm7O7gJObtk",
        "invitation": "ICLR.cc/2023/Conference/Paper863/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces NIERT which is a new framework for numerical interpolation for scattered data. The key idea of the framework is to use Transformers as a representation encoder and treats observed and target points in a unified fashion by embedding them into the same representation space. In the paper, the authors also propose a new partial self-attention mechanism that could escape from the unexpected interference of target points on observed points. Combining the partial self-attention mechanism,  masking mechanism, and pre-training technique, NIERT outperforms the existing approaches on both real-world and synthetic datasets. For example, NIERT gives the best mean absolute error on the NeSymReS dataset, the TFRD-ADlet dataset, the D30 dataset, and the PhysioNet. The authors also conduct ablation studies to understand the role of partial self-attention and the pre-training technique.",
            "strength_and_weaknesses": "## Strength\n* The proposed framework exploits the power of Transform in the numerical interpolation where traditional interpolation approaches for scattered data use explicitly pre-defined basis functions to construct interpolation functions.\n* The partial self-attention mechanism is new to the best of my knowledge.\n* The authors also discuss the connection between NIERT and the traditional approach by showing that partial self-attention is a general form of the RBF interpolation function.\n* The proposed framework beats all baselines on all datasets.\n\n## Weaknesses\n* The computational trade-off should be discussed e.g, the training time, and the interpolation time.\n* The authors should discuss the statistics about the number of observed and target points in each dataset. I wonder how the proposed framework performs in different settings of sequence lengths.\n* The authors only focus on the mean absolute error. There is no downstream task for showing that NIERT is better than previous frameworks.",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: The paper is well-written and easy to follow.\n* Quality: The proposed framework performs well in practice.\n* Novelty: Based on the claims of the authors, this is the first work that applies deep learning (Transformer) to the numerical interpolation problem and the partial self-attention mechanism is new.\n*  Reproducibility: The code is submitted. The experimental settings are reported in detail.",
            "summary_of_the_review": "The paper proposes the partial self-attention mechanism and applies it to the numerical interpolation problem. The framework achieves great prediction results however the performance in downstream tasks is missing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper863/Reviewer_uN6v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper863/Reviewer_uN6v"
        ]
    },
    {
        "id": "Aws1d8DLZn",
        "original": null,
        "number": 2,
        "cdate": 1666973968790,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666973968790,
        "tmdate": 1666973968790,
        "tddate": null,
        "forum": "pm7O7gJObtk",
        "replyto": "pm7O7gJObtk",
        "invitation": "ICLR.cc/2023/Conference/Paper863/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a learning-based approach using the encoder representations of Transformers for numerical interpolation of scattered data.",
            "strength_and_weaknesses": "Strength: this paper combines a set of existing (and indeed very popular) deep learning methods to deal with the numerical interpolation problem, and it achieves good results by experiments on real data sets.\n\n\nWeakness: there are several concerns, among which lack of novelty is the major concern.\n\n1. This paper combines a set of very popular techniques in deep learning to the numerical interpolation problem, such as transformer with self-attention and enhancement with pre-training. Every component of the proposed method is either existing or trivial, so it is not clear what is the novel method proposed by this paper.\n\n2. The claimed contribution, the partial self-attention mechanism, is mostly trivial which sets zero correlation weights to target points (eq. (2)).  While the authors argued that such partial self-attention mechanism is connected to Radial Basis Function (RBF interpolation), it is really trivial to me: if one sets zero weights to target points, then it is naturally written as a combination of functions on the observed points. In addition, while observed and target points are decoupled, I believe it should be helpful to boost the performance by modeling the correlation among target points by common principles of semi-supervised learning (the target points are given).\n\n3. Why are different metrics applied to different data sets, such as MSE for the NeSymReS, D30 and PhysioNet datasets, and MAE (CMAE,BMAE) for the TFRD-ADlet dataset? ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is mostly clear and well written. Source code is provided for reproducibility. Please refer to my above comments for the major concern about lack of novelty.",
            "summary_of_the_review": "This paper combines existing techniques with trivial modification to achieve good results for numerical interpolation. Due to lack of novelty, I believe this is a good technical report for practitioners to apply deep learning methods for numerical interpolation, but by its current form it is not ready to be published a paper in a top machine learning venue.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper863/Reviewer_qiuo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper863/Reviewer_qiuo"
        ]
    },
    {
        "id": "i9SBSKzVrs4",
        "original": null,
        "number": 3,
        "cdate": 1667508458713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667508458713,
        "tmdate": 1667508458713,
        "tddate": null,
        "forum": "pm7O7gJObtk",
        "replyto": "pm7O7gJObtk",
        "invitation": "ICLR.cc/2023/Conference/Paper863/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new partial self-attention layer to improve numerical interpolation for scattered data. The module was used in a modified Transformer to solve the interpolation task. The proposed approach treats observed and target points in a unified way by embedding them in the same representation space.",
            "strength_and_weaknesses": "Strength:\nA new mechanism to mask points in self-attention was introduced. \nThe proposed model achieves a better result on the synthetic and real-world dataset from the paper compared to the current state-of-the-art methods.\nExtensive analysis and ablation studies were conducted.\nThe Paper is written in an easy to read manner\nStrong evaluation and comparisons.\n\nWeakness:\nMistypes in the text.\nSome claims of the paper were not well-supported (see summary for more details).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, and the results are novel to the best of my knowledge. An anonymous link to the code is provided. There are a lot of experiments and ablation studies conducted both on synthetic and real-world datasets.",
            "summary_of_the_review": "In terms of text quality, the paper is easy to read. There are some minor issues with the text, but nothing major, except that the sentence abruptly ended at the end of page 4.\n\nExperiments and evaluation design are done nicely; the authors conducted many experiments and ablation studies. \n\nHowever, there are some issues with the paper:\n\nThe paper claims that one of the reasons why other methods perform poorly is because they process observed and target points separately. This fact prevents them from exploiting correlations between observed and target points (see page 2, end of paragraph 1, and page 5 before the experiments section). I do not see this claim proven in the paper, although I may be wrong.\n\nThere are a lot of variables (for example, different network parameter sizes) that can influence the outcome of this kind of experiment, so careful experiment design is needed to prove this.\n\nIn section 4.4 in figure 5, the fact that attention maps are imbalanced does not mean much. In my opinion, its area of affection for each point depends on its neighbors' count and their proximity placement. For example, if you have a lot of points in one area, it is natural that each point will contribute, or can either contribute a little to a wider area, or can contribute a lot to a local area. Which option is better is a good question. So the claim in the paper at the end of this section that NIERT can exploit the correlation between points observed and target points more effectively, in my opinion, is not well-supported. \n\nFinally, the simple use of the pre-training technique is not a contribution. \n\nQuestions:\nIs it correct that partly self-attention is just a particular case of self-attention, where observed and target points can\u2019t \u201clook\u201d at the target points(wij=0)? \nPlease clarify, what is input in each batch for your neural network in evaluation time? \n\n\nSmall suggestions:\n1. Section 3.4 is a good addition to the text but can be moved to supplementary.\n2. It would be nice to see a comparison of several hyperparameters for each model for the methods you compare with.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper863/Reviewer_rgW7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper863/Reviewer_rgW7"
        ]
    }
]