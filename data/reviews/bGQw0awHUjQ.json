[
    {
        "id": "GQOb4u17N_",
        "original": null,
        "number": 1,
        "cdate": 1666584788464,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584788464,
        "tmdate": 1666584788464,
        "tddate": null,
        "forum": "bGQw0awHUjQ",
        "replyto": "bGQw0awHUjQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3619/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the certified robustness of graph matching. Graph matching aims to find a matching between nodes of two graphs that maximizes the overall affinity score. This paper defines the certified robustness on graph matching by using randomized smoothing to the node classification stage. Specifically, they consider the randomized smoothing for the mapping function of each node while using a joint smoothing distribution matrix. They show the strictly certifies radius and two extra radii for evaluation. Finally, their experimental results show their certified robustness method performs better than previous methods on real-world datasets. ",
            "strength_and_weaknesses": "Strengths:\n1. This paper proposed a new setting for certified robustness of graph matching, which is a significant problem and can be used in many applications.\n2. They provided solid theoretical results on the strictly certified radius.\n\n\nWeaknesses:\n1. The techniques used to derive the theoretical results seem quite similar to the original randomized smoothing method (Cohen et al, 2019). The main difference is the noisy distribution here depends on the correlation between nodes, which is solved with standard linear algebra analysis. \n2. Figure 1(b) is hard for me to detect differences between methods.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. The certified robustness on graph matching considered in this paper is a new setting. I think the results in this paper are original and reproducible. ",
            "summary_of_the_review": "Overall I think this paper is very interesting. It provides theoretical results and convincing experimental results. My only concern is that the techniques used to derive the theoretical results seem quite similar to the original randomized smoothing method (Cohen et al, 2019). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3619/Reviewer_Sgyn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3619/Reviewer_Sgyn"
        ]
    },
    {
        "id": "px4aZUGVJjf",
        "original": null,
        "number": 2,
        "cdate": 1666637096567,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637096567,
        "tmdate": 1666637096567,
        "tddate": null,
        "forum": "bGQw0awHUjQ",
        "replyto": "bGQw0awHUjQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3619/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the problem\u00a0of certifying the robustness of structural graph matching, where the goal is to return a matching (or assignment) between the nodes of two input graphs G1 and G2, based on node and edge attributes. A robust graph matching mechanism is supposed to be resilient to adversarial perturbations of the input. The approach taken in this paper is to cast the graph matching problem as a collection of classification problems, where each node in G1 corresponds to a classification problem, and the nodes of G2 are the possible classes. Thus, if v_i is a node in G1 and u_j is a node in G2, then classifying v_i into the class u_j means that the matching matches v_i to u_j.\n\nThis formulation allows the paper to apply the generic randomizes smoothing (RS) technique to graph matching. RS is a general technique for turning non-robust classification mechanisms into certifiably robust ones, by adding some perturbation noise to the point that needs to classified, and classifying it to the most likely class (according to the input non-robust mechanism) of the perturbed point. Casting graph matching as a collection of classification problems as above allows applying RS, by adding noise to the nodes in G1. Furthermore, the covariance matrix of the Gaussian noise is chosen to reflect the structure of the input graphs, allowing for correlation between nodes, rather than adding independent noise, which according to the paper was the approach taken in prior work.\u00a0",
            "strength_and_weaknesses": "The paper appears to study a meaningful problem, and the proposed solution makes sense, up to some clarifications listed below. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written and I could largely understand and follow with almost no background on the subject, yet some parts were harder to follow. For example, I'm not sure why in section 4.1 the point attributes become 2-dimensional whereas the original formulation was with an arbitrary dimension d, and whether this matters for the results in the paper. In the definition of F (same section) it was not clear what is the range of the f_i's (meaning that r_j is a node in G2) until discerning this indirectly from the later derivations. I am not sure what I'm supposed to be seeing in figure 1(b), which looks like four near-identical figures. And finally, I was not sure if and how your method makes sure that the robust matching mechanism maintains the marginal constraints of the relaxed matching (the last two constraints in equation (1)).",
            "summary_of_the_review": "The paper ultimately makes sense to me in terms of the tackled problem and the proposed solution, without major issues that I could detect, and I think it can be accepted. I am, unfortunately, completely unfamiliar with almost all aspects of the topic of the paper (structural graph matching and certifiable robustness), so this should be taken as a non-expert review.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3619/Reviewer_n9K9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3619/Reviewer_n9K9"
        ]
    },
    {
        "id": "gMGj4T7aZN5",
        "original": null,
        "number": 3,
        "cdate": 1666660236350,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660236350,
        "tmdate": 1670288651382,
        "tddate": null,
        "forum": "bGQw0awHUjQ",
        "replyto": "bGQw0awHUjQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3619/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a new result on certified robustness for graph matching.  Its based on randomized smoothing (Cohen 2019), but the authors use a correlation matrix based on the graph information to construct a joint Gaussian distribution for smoothing (vs the standard single Gaussian distribution).  The experimental results seem to show some effect, but it's very hard to quantify with the provided results (basically guessing about one line vs another).  This heavily detracts from the work.",
            "strength_and_weaknesses": "Strengths:\n+ New results on the certified robustness for graph matching.\n+ Experimental results seem to show a benefit \n+ Generally good write up, but weak experimental analysis\n\nWeaknesses:\n- It's not obvious that there will ever be any application of these ideas (quite possibly low significance)\n- Information provided on experiments makes it hard to quantify the improvement.  Need metrics like AUC to quantize the performance better.  (is AUC even the right metric?)\n- Authors don\u2019t have a good explanation for interpreting results either\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Fairly clear for the material as far as the theory goes.  Experimental results need more.\n\nQuality: Generally high quality.\n\nNovelty:  Harder to judge, but I think there's some novelty.  Significance might be low though.\n\nReproducibility: Code provided.",
            "summary_of_the_review": "This work might have enough to get in with better analysis of the existing experimental results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3619/Reviewer_BhnR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3619/Reviewer_BhnR"
        ]
    },
    {
        "id": "ZRgLyPbBGe",
        "original": null,
        "number": 4,
        "cdate": 1667848845157,
        "mdate": 1667848845157,
        "ddate": null,
        "tcdate": 1667848845157,
        "tmdate": 1667848845157,
        "tddate": null,
        "forum": "bGQw0awHUjQ",
        "replyto": "bGQw0awHUjQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3619/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the problem of the probabilistic robustness of graph-matching (GM) algorithms against norm-based adversarial perturbations. The authors develop new algorithms for randomized smoothing for GM by exploiting the structure of the problem. They start by decomposing the robust matching problem defined over the whole graph into subproblems defined over the individual nodes. Next, the noise for building the smoothed classifier is sampled from a joint gaussian distribution which is set up in a way to capture correlations between the different nodes in the graph. Experimental evaluation is performed on the popular Pascal-VOC dataset showing the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n1. The technical contributions for building a probabilistically robust classifier for GM presented here are novel and solid. I wonder if structured distributions can also be used for improving the certified radius of vision classifiers.\n\n2. The experimental results show that the proposed method can certify larger radii than standard baselines.\n\nWeaknesses:\n1. The problem and the approach are not well-motivated. While the robustness of vision models is necessary when deployed in safety-critical scenarios such as self-driving cars, it is not clear what security implications the non-robustness of graph matching can cause. The authors do not motivate the need for solving their problem but instead choose to start describing their approach in the introduction. \n\n2. Randomized smoothing makes inference slow due to the need for sample multiple points and also makes its prediction non-deterministic. Why is this a good approach for proving certified robustness for the GM setting?\n\n3. The authors do not describe the impact of certified robustness on the accuracy of the classifier. Is there a tradeoff here?  For vision models, humans are the oracle and the goal is to achieve the same level of robustness as them. I do not see such an oracle for GM as even non-learned models are not robust. What level of accuracy and robustness is good in this setting?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality: Besides the issues with clarity mentioned above, I have the following comments for the authors:\n\n1. Randomized smoothing for neural networks was introduced by https://arxiv.org/abs/1802.03471 and not Cohen et al. The authors do not cite this. \n\n2.$\\sum$ is an important hyperparameter controlling the robustness/accuracy tradeoff but the authors do not elaborate on the principles behind its construction (e.g., what properties it should satisfy). Eq. (10) seems a bit random to me, why would any other equation not work? The authors should consider performing a study on the impact of different choices for constructing $\\sum$ on the certified robustness. \n\n3. In the equation for calculating $p_B$, the authors use $s_i \\neq p_A$ which is semantically not correct. \n\n4. In figure 2, how are the y-axis values computed? are they averaged over the whole dataset? \n\n5. Would the radii be larger if one considered the robustness of three nodes at a time instead of a single node as considered here? In general, considering the robustness of the full matrix should reveal more correlations right? which should yield larger radii. Did you choose single-node decompositions because it makes the problem tractable?\n\n6. The authors do not discuss the impact of sampling on the inference speed.\n\nNovelty: The technical ideas are sufficiently novel.\n\nReproducibility: Code is provided to ensure reproducibility, although I did not try to run it!",
            "summary_of_the_review": "In summary, this is a technically solid and novel work for developing smoothed graph-matching classifiers. However, I am not sure about the motivation for solving this problem and why randomized smoothing is a good approach for building robustness here.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3619/Reviewer_7S16"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3619/Reviewer_7S16"
        ]
    }
]