[
    {
        "id": "X6xT9mcM8nJ",
        "original": null,
        "number": 1,
        "cdate": 1666385699246,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666385699246,
        "tmdate": 1666385699246,
        "tddate": null,
        "forum": "zWy7dqOcel",
        "replyto": "zWy7dqOcel",
        "invitation": "ICLR.cc/2023/Conference/Paper3253/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new method to sample from a target distribution that is known up to a normalizing constant. \nIn essence, the proposed algorithm updates a set of particles by gradient descent and the loss is a smart pick that asses the energy of the pairwise interactions between the particles. When it's small, the particles follow the target distribution by design.\nAs I see it, the way to do it is to \"smooth\" the target distribution through some kernel with a bandwidth \\eps, for which the problem is tractable, and then see that the smaller the bandwidth get, the close we get to the target. This \"smoothing\" operation is called \"mollifying\".\n\nAll in all, the paper is nice to read, provides a lot of useful pointers and the proposed method looks surprisingly easy to implement. ",
            "strength_and_weaknesses": "Strengths of the paper include:\n* The method is well grounded theoretically and although I could not follow the proofs in detail, I trust the authors on this\n* The paper is a very good balance between motivations, background, a clear explanation of the method with all the hard maths in appendix, and a very nice part about how to implement the idea in practice\n* I personally found the paper very inspiring and it leaves me with many questions / ideas that I will just write down later here.\n* The method seems to work well and to offer a lot of ways of improvement.\n\nOn the downsides:\n* Since the method seems to work just as well as all others (see the experiments) on many aspects, I am wondering what would be the interesting features ? I must say that I like it that a paper is not bragging about how better it is everywhere, but I would appreciate it if the authors could at least provide some hints regarding when it is / is not appropriate. What about computing time ?\n* I am not used to this kind of experiments and I don't exactly know if such low dimensional problems may be considered state of the art. If so, it's all right. But my core concern is whether a particle-based system would really be able to scale to high dimensions, since it would maybe mean an extreme large number of particles ? How would that work ?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and nice to follow. Although it is challenging at times, I think the authors did a good job in balancing the main text and the appendix. I am not an expert in this particular field so I cannot say if they missed a paper that would really be relevant, but I feel they did their best to provide a good account for the state of the art.\n\nRegarding reproducibility, I must say that the proposed algorithm looks so easy to implement that I don't think it is a real issue. However, the authors also do provide their code.",
            "summary_of_the_review": "As a short summary, I think the authors do propose a principled and nice method to sample from a target distribution whose density is known up to a constant, that looks like an interesting alternative to MCMC. The paper is very inspiring to me, and leaves me with some comments. \n\nSince I don't exactly know where to write them, here I list them below in no particular order\n* 2/ pairwise interaction: you should maybe mention that $p$ is still the target here, or maybe it's obvious\n* looks to me you forgot |f(x)|^p instead of f(x)^p in the notation for ||f||_p\n* definition 3.2 looks super central. However, I was disapointed there was so few discussion about it, at least to provide some \"feel\" or rationale for it. It actually looked counter intuitive until I understood you were about to _minimize_ it, which was not obvious to me at first. Then I had to check appendix A.2 to see it in action.\n* As some food for thought and just to know your opinion on this (in this thread, not necessarily in the paper), I must ask whether you think you may come up with a way to define that kind of energy for any particular divergence between distributions ? Here we have chi^2, but it looks like maybe you may find some trick to generalize your idea to any other one, or at least ones with some features to be identified.\n* How restrictive is this assumption about ispd ? I am not used to it, and I wonder what it means in practice that may limit my choice of \"mollifier\"\n* you really don't want to write just kernel ? The historical reference is nice but do we need another name ? Is it used in some community ?\n* In equation (7), do we need N^2 terms ? Isn't there some symmetry that allows us using only N^2/2 ?\n* if N is too large, could you consider mini batches of pairs ?\n* the algorithm is nice, but do we know when to stop ? Do we have some clue whether when the samples actually match the target distribution ?\n* The \"special treatment of the diagonal terms\" is definitely not the most elegant part of the paper. Do you think you could handle the diagonal and off-diagonal terms in a different way, maybe by balancing them in some way ?\n* In this paragraph: \"which can dominant\", \"for mollifiers we consider\" -> \"for the mollifiers we consider\"\n* I have no clue about the \"dynamic barrier method\". For the paper to be self contained, it could help at least describing it a bit.\n* page 9 \"the mirror map has to chosen\" -> to be chosen\n* conclusion: \"the target mesaure\" -> measure\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3253/Reviewer_gruH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3253/Reviewer_gruH"
        ]
    },
    {
        "id": "nFoQctvoZNE",
        "original": null,
        "number": 3,
        "cdate": 1666679493605,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679493605,
        "tmdate": 1666679493605,
        "tddate": null,
        "forum": "zWy7dqOcel",
        "replyto": "zWy7dqOcel",
        "invitation": "ICLR.cc/2023/Conference/Paper3253/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new particle based probabilistic approximate inference method that (similar to Stein Variational Gradient Descent algorithm) is based on an optimisation task involving attractive and repulsive forces. In the present work, these forces are defined differently and via a function called mollified interaction energy (MIE). \nMinimising the proposed MIE function involves balancing a repulsive force to increase the distance between the particles and  an attractive force that pushes the particles to high-density regions. It is argued that the presented work might be suitable for constrained density functions. \n",
            "strength_and_weaknesses": "Strength: Up to my knowledge the presented algorithm is novel and correct. The paper is overall well-written (though, possibly the measure theoretic jargon could be explained simpler and more intuitively to increase the impact).\n\nWeaknesses: \n1. Most their experimental models are low-dimensional or toy models. It would add to the value of the paper if they could show some results on more complicated models (e.g. multimodal models) and high-dimensional models (to represent the role of the dimension on the attractive/repulsive forces.  \n2. The importance (and novelty) of their theoretical findings is not clearly explained.  \n3. The convergence of the proposed algorithm to the target distribution is not guaranteed meaning the algorithm is not asymptotically unbiased. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is reasonably clearly explained though, as I mentioned the significance and novelty of their theoretical findings is not well explained. \nThe algorithm is novel and sound. The experimental results (even though toy-ish and 2D) suggested that compared to the baselines such as SVGD, the particles are better spread throughout (e.g. the 2D uniform). \nAlso the time-complexity analysis is missing. \nI have not checked the code but it seems that the results are reproducible. \n\nQuestion: in page 3: $\\int_X f(x)^p dx < 0$ ? or >0?",
            "summary_of_the_review": "This paper presents a new particle based inference method that minimised a function called MIE. \nThe algorithm is interesting but the experimental results are not thorough (see the previous section).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no concern",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3253/Reviewer_oXmc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3253/Reviewer_oXmc"
        ]
    },
    {
        "id": "uBQrbld-0kp",
        "original": null,
        "number": 4,
        "cdate": 1666691050104,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691050104,
        "tmdate": 1666691050104,
        "tddate": null,
        "forum": "zWy7dqOcel",
        "replyto": "zWy7dqOcel",
        "invitation": "ICLR.cc/2023/Conference/Paper3253/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces algorithms for sampling constrained distributions. The authors introduce an algorithm MIED that transform this sampling problem into an optimization problem, and practically, to optimize the mollified interaction energy in a first-order particle-based solution. They also proved that by minimizing the energy function, the MIE coverges to the target measure in the sense of chi-square divergence. The authors experimentally validate their algorithms on simulated data to do unconstrained sampling and constrained samplign.",
            "strength_and_weaknesses": "Strengths:\n\n- The motivation of the paper is clear and interesting: sampling from a constrained domain either contain expensive numerical subroutines or requires explicit formulas for quantities, which are invalid in most of the cases. \n-The paper is overall well-written and the authors tries to make the main idea easy to follow.\n\nWeakness:\n- Since the performance in uncontrained sampling is close to the previous methods, the authors could put more emphasize on the contrained sampling part to show the effectiveness of the method. Some of the constrained sampling results could be moved to the appendix.\n- How to choose the hyperparameter s in practice? Is the method sensitive to s?\n\nTypo:  $\\int_x f(x)^p >0$.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The contribution is novel and solid with some potential applications. However, the main contributions are presented in a clear and understandable way.",
            "summary_of_the_review": "This paper addresses an interesting problem and its development looks very exciting. The work is very complete and addresses both theoretical and experimental aspects of the proposed methods. The theory proposed looks sound with a strong background and the experiments look convincing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3253/Reviewer_cXXg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3253/Reviewer_cXXg"
        ]
    },
    {
        "id": "aVte0WGeiE",
        "original": null,
        "number": 5,
        "cdate": 1666845327856,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666845327856,
        "tmdate": 1666845327856,
        "tddate": null,
        "forum": "zWy7dqOcel",
        "replyto": "zWy7dqOcel",
        "invitation": "ICLR.cc/2023/Conference/Paper3253/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of sampling from a target density without knowing its normalization constant. It focuses on one class of variational inference methods called Stein variational gradient descent (SVGD), which formulates the sampling problem into an optimization problem that minimizes the KL distance between the target distribution and a distribution that defines the mollified interaction energy. Based on it, the authors propose the mollified interaction energy descent (MIED) method to run gradient descent on the minimization problem. They proved that the exact version of MIED asymptotically converges to the target distribution. Empirical evaluations are provided to compare MIED with previous methods.\n",
            "strength_and_weaknesses": "This paper is well-written and clearly structured. The proposed method is more general than the previous ones compared in the paper. The experimental results also show the advantage of MIED in constrained and unconstrained sampling problems. The main weakness of the paper is its lack of discussion about the comparison of MIED with LMC-based algorithms both theoretically and empirically. The theoretical results in this paper are mainly asymptotic and it is not clear whether MIED will have good convergence properties in various empirical applications. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing in general is good. The clarity on the difference between MIED with previous methods such as SVGD could be elaborated more on the algorithm part.\n",
            "summary_of_the_review": "As described in the previous sections, I think this paper is studying a very interesting problem and the algorithm proposed here seems to be competitive over existing methods, especially SVGD. However, it is not discussed in full detail what the difference between them at an algorithmic level is. It would be better to have some discussion on the difference between the algorithm design in SVGD and MIED in the algorithm section (Section 4).\n \nMoreover, the theoretical results in this paper are all in the asymptotic sense. Moreover, the problem in (2) is not directly solvable and the paper proposes to minimize a discrete version of (2) in practice instead, which brings additional approximation error. It is not clear how large these errors could affect the sampling error in the final results. It is a natural question that does the proposed algorithm enjoy any non-asymptotic convergence? This differs from the LMC based approaches, where finite-time and fast convergence could be established for sampling from log-concave and non-log-concave densities (see [1, 2, 3] for example). Then what are the difference and advantage of using MIED instead of other LMC methods?\n\n[1] Dalalyan, Arnak S. \"Theoretical guarantees for approximate sampling from smooth and log concave densities.\" Journal of the Royal Statistical Society: Series B. 2017.\n[2] Zou et al. \"Faster convergence of stochastic gradient Langevin dynamics for non-log-concave sampling.\" Uncertainty in Artificial Intelligence. 2021.\n[3] Muzellec et al. \"Dimension-free convergence rates for gradient Langevin dynamics in RKHS.\" Conference on Learning Theory. 2022.\n\nIn Figure 3, why do the samples from SVGD collapse?\n\nAlthough the experiments show that MIED can deal with different constraints unlike SVGD, could you explain what causes the performance drop of MIED in Figure 6?\n\nWhat is the dataset used in the training of fair Bayesian neural networks?\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3253/Reviewer_FquR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3253/Reviewer_FquR"
        ]
    }
]