[
    {
        "id": "AnYgpOo3eFK",
        "original": null,
        "number": 1,
        "cdate": 1665673717186,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665673717186,
        "tmdate": 1669817521799,
        "tddate": null,
        "forum": "zqwryBoXYnh",
        "replyto": "zqwryBoXYnh",
        "invitation": "ICLR.cc/2023/Conference/Paper898/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method for prompt learning in vision and language models that aims at producing a variety of templates that can be used by the text encoder to provide better classification weights in a few-shot scenario. \n\nIn particular, the paper observes that learning a single prompt is not sufficient to represent a class, and proposes a variety of prompt candidates that can be used to cover a wider variety of concepts within the same image. The authors note that using a distribution of prompts that are learned independently using a standard contrastive learning will result in some of the prompts to collapse to the same centroid, and propose an optimal transport scheme to avoid such behaviour. \n\nThe methodology goes as follows: 1) a set of different visual features are computed for a given image (using different backbone exits); then 2) the scores for the pairs visual/text features are computed using the learnable prompts; then 3) a transport matrix is computed to minimize the total cost of assigning a feature vector a weighted combination of the text features (i.e. the transport matrix is the soft-extension of the permutation matrix that would assign vision/text features through the typical CLIP matching). Finally, for a fixed transport matrix 4) the prompts are learned using the standard cross-entropy where the class assignment is given by the computed combination of the weights produced by the prompts. \n\nThe use of a transport matrix allows to assign to each feature vector a weighted combination of the prompts, thus making the latter have a broader \"semantic\" capacity. The use of different feature vectors for each image rather than a global CLIP-based visual feature vector allows the prompt to describe different concepts that might co-occur in the same image, making the prompts having more ``local\" capacity. \n\nThe paper is validated in the same setting as CoOp for few-shot learning, using 11 visual recognition datasets, delivering consistently better results than the competing method. \n",
            "strength_and_weaknesses": "Strengths: \n\nThe paper is technically solid and contributes enough novelty to the domain of prompt learning which is of growing interest to the research community devoted to exploring pre-trained vision/language models. The use of a diverse set of prompts to generate different text classifiers that can all contribute to a single class prediction is of interest and the methodology is well developed. \n\nThe results seem to consistently outperform CoOp, showing the need of having a large set of prompts that require proper use to be able to generalize better to new classes.\n\nWeaknesses:\n\nWhile the core methodology is new, the idea of learning a distribution of prompts is not new, and indeed the authors do not include a discussion on why their proposed approach would be a better choice than CoCoOp which produces prompts for each image. Additionally, the authors need to consider, discuss, and compare, against \n\n[Lu et al.] Prompt Distribution Learning. CVPR 2022\n\nwhich is a method that learns a collection of prompts that produce a distribution of per-class weights, increasing their expressivity. Some analysis in terms of methodology and computation against [Lu et al.] should be included in the paper in my opinion.",
            "clarity,_quality,_novelty_and_reproducibility": "The method is novel and the code is provided, hence reproducibility and novelty are properly covered in the manuscript.\n\nThe paper could benefit from better writing and clarity in the presentation; in my humble opinion, writing the paper top to bottom (i.e. presenting first optimal transport and then the problem) is more confusing than writing it bottom to top (i.e. the problem of prompt learning and the intuition behind the optimal transport in ''assigning\" prompts to different visual features from the same image, there is barely discussion around it). ",
            "summary_of_the_review": "In summary, the paper proposes an interesting approach to prompt learning that is technically sound and well motivated. However, the clarity and the lack of an exhaustive comparison against state of the art works ([Lu et al.] and CoCoOp) both experimentally and conceptually is needed. Should the authors be able to bring this up would further increase the quality of the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper898/Reviewer_RWPs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper898/Reviewer_RWPs"
        ]
    },
    {
        "id": "NMym3dMSTqG",
        "original": null,
        "number": 2,
        "cdate": 1666422302386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666422302386,
        "tmdate": 1669103938819,
        "tddate": null,
        "forum": "zqwryBoXYnh",
        "replyto": "zqwryBoXYnh",
        "invitation": "ICLR.cc/2023/Conference/Paper898/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presented a new prompt learning approach for vision-language models, namely PLOT. The main idea of this paper is to learn multiple prompts per class and use optimal transport to match them with visual features of images. The whole optimization process contains inner and outer loops to optimize the matching and prompts, respectively. The experiment results show that the proposed approach can achieve state-of-the-art performance in the few-shot image recognition task with a slight increasement in training time.",
            "strength_and_weaknesses": "__Strength__\n\n1. This paper is overall well-structured and easy to follow.\n\n2. The idea of learning multiple comprehensive prompts to describe the image categories is reasonable to me, and using optimal transport for optimization prompt learning is novel.\n\n3. The ablation studies are comprehensive which clearly demonstrate the design choices of the proposed method.\n\n__Weakness__\n\n1. In Figure 3, the improvement achieved by the proposed method over CoCoOp is marginal. I wonder if the conditional formulation presented in CoCoOp can be easily extended to PLOT? If so, at least more detailed discussions should be provided by the authors.\n\n2. Figure 4 shows the visualization results of attended regions of images from different prompts. It is also interesting to see how learned inter-class and intra-class prompts are different from each other by using t-sne or PCA.\n\n3. It is unclear to see the differences among model variants in Table 2 just from the caption. It will be good if an individual section on introducing the detailed design/training setup of each model in the paper (or in the appendix). \n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I think the presentation of this paper is clear and easy to understand. The proposed method is novel and technically sound.",
            "summary_of_the_review": "This paper addressed prompt learning for vision-language models which is an interesting and important problem in few-shot image classification tasks. The proposed PLOT is reasonable and technically sound to me. However, I still have some concerns on the experiment part of the paper. Therefore, it leads me to borderline acceptance as my initial rating. I will be happy to increase my rating if the concerns could be addressed in the discussion period.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper898/Reviewer_1oAQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper898/Reviewer_1oAQ"
        ]
    },
    {
        "id": "ZJWrpmtRCLg",
        "original": null,
        "number": 3,
        "cdate": 1666994133758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666994133758,
        "tmdate": 1666994133758,
        "tddate": null,
        "forum": "zqwryBoXYnh",
        "replyto": "zqwryBoXYnh",
        "invitation": "ICLR.cc/2023/Conference/Paper898/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes PLOT, a prompt learning method for CLIP based on CoOp. PLOT aims to diversify the textual prompts to better depict the category for both its intrinsic and extrinsic contexts. By applying optimal transport, PLOT shows favorable performance over different few-shot benchmarks.",
            "strength_and_weaknesses": "Strength:\n1) The motivation of enriching prompts for different image regions is practical and well illustrated in the paper. The usage of optimal transport is also novel, which might be inspiring to future works.\n2) The ablation study with discussion clearly tackles most my confusion with interesting points.\n\nWeakness:\n1) The major concern is the classification performance. As shown in Figure 3, despite the better overall performance, PLOT performs worse than previous prompting methods or linear probe on some of the datasets. Considering the much more complicated training, PLOT is expected to achieve higher accuracy.\n2) The authors should provide complete comparisons between PLOT (adapter-based PLOT) and existing adapter-based methods, e.g. Tip-Adapter-F on all 11 datasets, not just ImageNet. As the adapter-based methods are SOTA for CLIP few-shot learning, this can better emphasize the contribution of PLOT.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of good quality concerning writing and figures. The idea is novel, but the experiments can be further improved as above.",
            "summary_of_the_review": "The reviewer expects the authors' response to the experiment weakness.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper898/Reviewer_qGjf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper898/Reviewer_qGjf"
        ]
    },
    {
        "id": "iqLVJnLeyox",
        "original": null,
        "number": 4,
        "cdate": 1667012403920,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667012403920,
        "tmdate": 1667012403920,
        "tddate": null,
        "forum": "zqwryBoXYnh",
        "replyto": "zqwryBoXYnh",
        "invitation": "ICLR.cc/2023/Conference/Paper898/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper focused on adapting large-scaled pre-trained vision-language model (i.e., CLIP) to downstream datasets under the few-shot setting. Compared to the recent related work CoOp, this work (1) learns multiple prompts, (2) uses both local features and global features, and (3) measures the similarity of prompts and visual features using optimal transport. Experiments on 11 benchmark visual recognition datasets show that the proposed method outperforms the baseline CoOp. Ablation studies further more analysis on the contribution of each component.",
            "strength_and_weaknesses": "Strengths:\n\n\\+ The problem of adapting CLIP under few-shot setting is recent. Compared to the baseline method CoOp, the improvement of the proposed method is significant.\n\n\\+ The motivation that (1) single prompt is insufficient to represent a class, (2) directly learning multiple prompts reduce to one single point, are straightforward and make sense. The proposed optimal transport based method is consistent with the motivation.\n\n\\+ The explanation of the comparison between optimal transport and other distances is clear, which supports the motivation.\n\n\\+ The ablation studies and analysis in Section 4.4 is well organized and clearly written. It is easy to follow the analysis and figure our the contribution of each component. Also, Figure 2 is well designed and clear to illustrate the pipeline.\n\n\\+ The experimental analysis is comprehensive. The analysis on computation time and inference speed is also provided.\n\nWeakness:\n\n(As I reviewed this paper before and all my previously concerns are addressed, I do not have much comments about the weakness.)\n\nThis work is built on top of CoOp. It would be interesting to know whether this proposed method can be extend to other prompt-learning-based methods, especially CoCoOp. If not, it would be great to discuss the reasons. Somehow, the method is only applied to CoOp, which is a limitation of this work.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality, clarity and originality of the work look good.",
            "summary_of_the_review": "Overall, considering both strengths (simple but effective pipeline, comprehensive experimental analysis) and weakness (generalizability), my initial rating is accept.\n\nReasons to accept:\n\n\\+ The motivation for this work is clear. Single prompts may lose the details of local attributes or contexts.\n\n\\+ The idea of applying optimal transport to prompt learning makes sense and meets well with the motivation.\n\n\\+ The experiments and analysis are comprehensive. The improvement is significant.\n\nReasons to reject:\n\n\\- This work is only applied to CoOp. It is not clear whether the method can be extended to other prompt-learning-based methods.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper898/Reviewer_FMp9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper898/Reviewer_FMp9"
        ]
    }
]