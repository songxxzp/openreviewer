[
    {
        "id": "Af7o107mjld",
        "original": null,
        "number": 1,
        "cdate": 1666009242520,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666009242520,
        "tmdate": 1666009242520,
        "tddate": null,
        "forum": "bH-kCY6LdKg",
        "replyto": "bH-kCY6LdKg",
        "invitation": "ICLR.cc/2023/Conference/Paper5576/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed a novel characterization of the \"edge of stability (EoS)\" phenomenon, referred to as the \"interaction-aware-sharpness\",  for both gradient descent (GD) and its stochastic variant SGD.\nThe proposed theory is of particular interest in its ability to provide novel and more precise insight into the EoS of SGD. Based on the theory, the authors further proposed a novel scaling rule (Linear and Saturation Scaling rule, LSSR) on the learning rate and SGD batch size, which extends the popular linear scaling rule (LSR).\nNumerical experiments on a few (not so deep) neural networks and across a few widely used datasets were provided to empirically support the theoretical results.",
            "strength_and_weaknesses": "**Strength**: the paper is in general well written and the main ideas are easy to follow. The problem under study is important and the proposed theory provides novel insights and is of interest. The numerical experiments look compelling.\n\n**Weaknesses**: the theoretical/technical depth of the paper is somewhat limited, the empirical contribution is rather significant, though. The limitations of the proposed theory/analysis are not sufficiently discussed.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: the paper is in general well written and easy to follow. (It is a bit awkward to first discuss SGD in Theorem 1 and then generalized momentum GD in Theorem 2, though)\n\n**Quality and Novelty**: novel and significant enough to be published, from both theoretical and empirical perspectives.\n\n**Reproducibility**: good",
            "summary_of_the_review": "The paper is in general well written and proposes an interesting and novel viewpoint of the EoS for SGD.\nI do not see any major flaws in the paper. \n\nDetailed comments:\n* Definition 2: the term \"concentration measure\" is used without providing more context. This can be misleading to some readers with probability or statistics background who may be more familiar with the term \"concentration of measure\".\n* It would be helpful to discuss the limitation of the proposed analysis (including the LSSR), from both theoretical and empirical perspectives possibly.\n* can one go beyond SGD to more involved settings? It would be of interest to have a short discussion on this point.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5576/Reviewer_bH22"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5576/Reviewer_bH22"
        ]
    },
    {
        "id": "qLTfzpNZ33",
        "original": null,
        "number": 2,
        "cdate": 1666182009907,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666182009907,
        "tmdate": 1668775340028,
        "tddate": null,
        "forum": "bH-kCY6LdKg",
        "replyto": "bH-kCY6LdKg",
        "invitation": "ICLR.cc/2023/Conference/Paper5576/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a fine-grained analysis of the edge of stability (EoS) phenomenon and in particular, their characterization also applies to SGD. In my understanding, the new characterization not only uses the largest eigenvalue of Hessian, which corresponds to the sharpest direction but also considers the curvature along other directions. To this end, an interaction-aware sharpness is defined. The numerical experiments show that the characterization is quite \"accurate\" and the authors provide an interesting application in explaining the linear and saturating scaling rule in large-batch training. ",
            "strength_and_weaknesses": "### Strong point\n\nThe derivation of new characterization is simple but powerful. The new sharpness also seems to capture the EoS phenomenon more precisely. The application of explaining the learning rate rescaling in large-batch training is consistent with empirical observations, which might be practically relevant. In general, I think this paper makes some interesting observations and I believe they are useful for further study on relevant topics. \n\n\n----\n### Weak point\n\nHowever, I feel that this paper is written in a hurry and the work is still somewhat preliminary at this stage. Listed below are a few concerns in my mind. \n\n1. **The discussions of prior works are unfair**\n\n   The phenomenon of  EoS was first observed by (Wu et al., 2018) at the end of training; (Jastrze \u0328bski et al., 2020) further observed the progressive sharpening phenomenon in training neural nets. (Cohen et al., 2021) followed these works by providing a systematical investigation of these phenomena and also additionally found the non-monotonic convergence at EoS.  All three works are crucially important in this line of research. Only attributing the discovery of EoS to (Cohen et al., 2021) is unfair (at least in my opinion). \t\t\n\n2.  **The explanation of implicit interaction regularization is quite hand-waving and not convincing**\n   - First, the definition of $\\rho_b$ (a concentration measure of batch gradient)  is the same as the gradient diversity defined in (Yin et al., 2018)  and is never mentioned. \n   - I find equation  (16) misleading since both sides depend on where the iterate locates. In iterations, both sides keep changing. The statement that *$\\|H\\|_{S_b}$ is implicitly regularized and bounded to be less than $2\\rho_b/\\eta$* is technically correct. But it implies nothing since both sides keep changing. For example, we cannot claim that SGD tends to find solutions with smaller $\\|H\\|_{S_b}$ since $\\rho_b$ is not a constant. \n\n \t3. In Section 5, I do not understand why keeping $2\\rho_b/\\eta$ to be constant is a good choice for generalization. This is never explained. \n\n\n\n### Other comments\n\n1.  In the second paragraph, the authors ask three questions. But it seems that this paper only answers the third one. Therefore, claiming \"we provide a new characterization of the EoS for SGD, which can serve as an answer to the above questions\" is not appropriate. \n\n2.  In Section 2, I suggest focusing on the case of sampling with replacement for simplicity. Otherwise, the statement is much more complicated and no new insights are brought. \n\n3.  In Section 3, the authors claim that existing works using tr(HS_b) make some additional assumptions, but they do not. In my opinion, this claim is quite misleading, since this paper also needs to make some assumptions, e.g., the validity of local quadratic approximation. In my opinion, the real difference is that this paper uses this quantity to analyze EoS, which is not carried out previously.  \n\n4.  In Figure 1, I do not understand why the authors only mark the endpoint with 'x' for cross-entropy loss. Also, it should be clarified in the caption the meaning of MSE and CE. \n\n5. In Theorem 1, the batch gradient never follows a normal distribution, thus the Gaussian assumption is too strong. Also, I do not understand why  (8) and (9) are interesting.\n\n6. In the definition of the new edge (11), I think the author should explain the intuitive difference between $||H||_{S_n}$ and $||H||$. In my understanding, the former considers the curvatures in typical directions but the latter only considers the curvature in the sharpness direction:\n\n$$\n\\qquad ||H||_{S_n} = \\frac{\\theta^TH^2\\theta}{\\theta^T H \\theta}, \\qquad ||H|| = \\sup \\frac{\\theta^TH^2\\theta}{\\theta^T H \\theta},\n$$\nwhere the supremum is taken wrt the direction theta.\n\n8. In Figure 2, what do you mean by **overestimation**? In addition, are the \"iteratre enter the EoS\" and \"in the unstable region\" the same thing?\n\n9. In Equations (13) and (14) and Theorem 2, suddenly momentum GD is studied. I do not understand why? Do the authors want to say the EoS is the same as vanilla (S)GD? Theorem 2 is also repeatedly referred to argue the convergence of directions, but it is stated for momentum GD.  In addition, in Theorem 2, what is $\\lambda_i$? What do you mean by quadratic $L$? Is each sample loss also quadratic?\n\n\n\n\n---\n### Reference\n\n[1] Yin et al., Gradient diversity: a key ingredient for scalable distributed learning\n\n[2] Wu et al., How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective\n\n[3] Jastrzebski et al.,The break-even point on optimization trajectories of deep neural networks",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is hard to read and the material is not well-organized. The analysis is interesting but not well explained ",
            "summary_of_the_review": "The more precise characterization of EoS and the empirical findings are interesting, but this work is still somewhat preliminary and more work is needed. \n\n\n=====\nThe author's response partially addressed my concern. I generally think that the analysis and numerical experiments are useful for future study on relevant topics. Hence, I increased my score to 6. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5576/Reviewer_9x87"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5576/Reviewer_9x87"
        ]
    },
    {
        "id": "Xj6ch-1frZp",
        "original": null,
        "number": 3,
        "cdate": 1666313568622,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666313568622,
        "tmdate": 1670506899529,
        "tddate": null,
        "forum": "bH-kCY6LdKg",
        "replyto": "bH-kCY6LdKg",
        "invitation": "ICLR.cc/2023/Conference/Paper5576/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims at gaining more understanding of the Edge of Stability phenomenon. Some theoretical understanding was obtained via Theorem 1, which assumes a quadratic loss function. Although EoS does not manifest for quadratic loss, this theorem was then used to motivate empirical claims. Such claims are backed by experiments.",
            "strength_and_weaknesses": "Strengths:\n\n(1) Problems studied in this paper are very interesting.\n\n(2) Empirical investigations are detailed.\n\nWeaknesses:\n\nThe demonstration is still largely empirical, and I wish the theoretical component could be stronger. Some reasons are: \n\n(1) some of the claims appear to be rather general, but it is actually known that EoS does not occur for all loss functions. I\u2019d appreciate it very much if the authors could be more specific about their conjectures (which I nevertheless found interesting), such as the hypothesized conditions under which they could be true.\n\n(2) The abstract suggests the question of \u201cwhy the sharpness is somewhat larger than 2/(learning rate)\u201d will be answered. Could the answer be more explicitly and concisely provided? Is there any quantification of how much larger it is than 2/(learning rate)? Is it loss function dependent? In fact, I wonder how the authors relate this claim (and other empirical observations) to the recent line of analytical work based on matrix factorization, e.g., [Wang et al. Large Learning Rate Tames Homogeneity. ICLR 22], [Zhu et al. Understanding Edge-of-Stability Training Dynamics with a Minimalist Example. arXiv]. The latter is concurrent so it is not a missing reference, but I\u2019m curious if there is any contradiction: for example, these results seem to give sharpness not larger than 2/(learning rate). Although the authors seem unaware, theories in these papers do seem to be aligned with many of the empirical observations made here. However, do they align with all observations? It could be that neither is wrong, and instead it is just a matter of different loss functions. I wish these aspects could be explored more.",
            "clarity,_quality,_novelty_and_reproducibility": "Empirical results are of good quality and originality, although related work from 2021 on might have been missed. Clarity overall is okay, but I recommend the authors to itemize and specify their main findings and contributions in a concise but quantitative fashion.",
            "summary_of_the_review": "Overall, this is an interesting paper and a helpful contribution toward an understanding of EoS. I have some concerns at this moment, but my score can be changed.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5576/Reviewer_mqQV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5576/Reviewer_mqQV"
        ]
    }
]