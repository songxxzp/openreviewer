[
    {
        "id": "Rvwi1m_yHO-",
        "original": null,
        "number": 1,
        "cdate": 1666664551296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664551296,
        "tmdate": 1666664551296,
        "tddate": null,
        "forum": "OVbY-QCCjAh",
        "replyto": "OVbY-QCCjAh",
        "invitation": "ICLR.cc/2023/Conference/Paper4557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors of this work propose SAGE, a data mining method to discover global explanation rules for black box text classifiers using semantic augmented features in a post-hoc fashion.  The method first leverages FP-growth (Han et al 2000) to generate candidate implication rules of the form (t1, t2, .. tn) -> y  ( where t1, t2, etc delineate the presence of tokens and semantic features within a window and y is a class for a given task ).  They then show how to filter the generated set of rules and select a final concise set of rules.  They evaluate SAGE on the NER task, by training BERT on the task and using SAGE to explain its predictions.  They compare SAGE against SP-Lime-k which is global submodular based variant of LIME and an ablated version of SAGE which removes semantic augmentation.  They show SAGE outperforms the other methods on the CoNLL03 and OntoNotes datasets, semantic augmentation increases accuracy and additionally provide some examples of rules the system learns which highlights the utility of adding semantic augmentation.   ",
            "strength_and_weaknesses": "**Strengths:**  \nAlthough the use of rule sets is quite common in tabular data, its less prevalent in NLP and I don\u2019t recall seeing the construction of rule set classifiers for explaining sequence level tasks before so its quite novel ( though the authors mention Bayesian Rule Lists which is less efficient method ).  \n\n**Weakness: **  \nThe paper's argument could be made stronger.  The baseline they compare against seem pretty weak ( they allude to its limitations in the related work section ) and it seems like comparing SAGE with additional methods such as Decision trees would have more sense as well ( since SP-LIME performs very poorly in the task ).  More robust comparison of baselines/alternative methods  ( bayesian rules, global explain models, decision trees for NLP etc ) would benefit this paper along with some ablations including whether if instead of filtering as aggressively and using all rules if they'd get you 91% F1 on CoNLL?  If so, it seems perfectly acceptable as a solution if only a sparse amount of rules are used at each's instance explanation level.  \n\nDiscussing what it means for overlapping spans ( unigram, bigram, etc ) of the same word to be present in rules is also important.  Also qualitative analysis of rules extracted including human judgement on whether they are good/interpretable, because the only rules shown in the paper are of simple unigram words/features implying a NORP class.\n\nFinally, the accuracy of the BERT NER system on both tasks should be provided because thats really the upper limit of how well any explanation model could do.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clearly written and easy to follow though there are few minor things that could help such as (1) explicitly mentioning Y are the blackbox predictions and not gold truth labels ( this is apparent in Fig 1, but not in section 3.1 ), (2) the last sentence of Section 3.1 right before Algorithm 1 seems cut off (3) defining minimum support and minimum confidence explicitly, etc.  The work could be reproduced for the most part.",
            "summary_of_the_review": "Its an interesting idea overall, but feels under explored in the paper and could be made stronger and more compelling if it had more robust comparison of baselines/alternative methods, ablations and qualitative analysis which really highlighted why the method should be used.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_RoTP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_RoTP"
        ]
    },
    {
        "id": "5HL3VsSp2Z-",
        "original": null,
        "number": 2,
        "cdate": 1666682144549,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682144549,
        "tmdate": 1666682144549,
        "tddate": null,
        "forum": "OVbY-QCCjAh",
        "replyto": "OVbY-QCCjAh",
        "invitation": "ICLR.cc/2023/Conference/Paper4557/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present SAGE or SEMANTIC-AWARE GLOBAL EXPLANATIONS model specifically for handling named entity recognition problems. They present a method to produce highly interpretable global rules to explain NLP classifiers. ",
            "strength_and_weaknesses": "The authors have provided motivation to the problem, which is certainly important. They present their algorithm and their experiments on 2 datasets, compared against the SP-Lime baseline model. \n\nThe paper could have been improved by comparison to other baselines. Given that LIME and SP-Lime were released in 2016, more work has gone into the field of XAI. It is not clear why only LIME was chosen as the baseline. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly easy to read. However, most terms used in the algorithms are not defined before using them, and one has to refer to subsequent sections to understand their meaning. This reduces the readability of the paper.\nThe approach is seems relatively novel, but it is hard to appreciate given the lack of baselines.\nThere is limited details about the implementation and it is not clear if one can reproduce the results in the paper.",
            "summary_of_the_review": "The authors present SAGE specifically for handling named entity recognition problems and present a method to produce rules to explain NLP classifiers. The paper could have been improved by comparison to other baselines. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_rFXT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_rFXT"
        ]
    },
    {
        "id": "1bWnoxK-bD",
        "original": null,
        "number": 3,
        "cdate": 1667374040830,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667374040830,
        "tmdate": 1667374580761,
        "tddate": null,
        "forum": "OVbY-QCCjAh",
        "replyto": "OVbY-QCCjAh",
        "invitation": "ICLR.cc/2023/Conference/Paper4557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Paper presents a post-hoc method to produce interpretable global rules to explain NER classifiers. Rules are extracted with a data mining approach that gathers labeling rule patterns by FP-growth algorithms, prunes the rules by removing soft-duplicated rules, and selects rules that maximizes the F1 score. Selected rules serve well as a post-hoc global explanation for the NER model and show its better explanation quality than LIME-based global explanation.",
            "strength_and_weaknesses": "Proposed approach shows its effectiveness in generating a global explanation for the NER model. I believe this approach can be useful to the future research of weakly-supervised learning and neural-symbolic learning for NER. However, I have a minor concern that the paper needs more analysis to convince readers. For example,\n\n(1) Could find that the F1 score between model prediction and rule-based prediction is over 0.4 with only one rule. It could be good to show qualitative examples of which rules are extracted \u201cglobally\u201d, not just for the specific entity type (e.g., NORP).\n\n(2) Comparison of qualitative explanation examples between SAGE and LIME. Seems the word itself contributes a lot to labeling decision (e.g., Chinese \u2192 NORP), compared to the surrounding words or context. Then, I think word importance generated by LIME can also be a good explanation but it seems not. It would be good to show qualitative explanation examples between SAGE and LIME to show the effectiveness of SAGE.\n\n(3) How does each semantic/syntactic information (POS tag, entity types, is_digit, \u2026 etc.) contribute to the performance? Seems entity prediction gives a lot of information to create rules.\n\n(4) What is the performance of the fine-tuned BERT model as it is?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and the proposed approach is classic but its application is novel. \nIt demonstrates an effective approach to generating a global explanation for the NER model, which is unexplored. \nI have a minor concern about the reproducibility.",
            "summary_of_the_review": "Paper presents a post-hoc method to produce interpretable global rules to explain NER classifiers and show its effectiveness.\nAlthough the paper needs more analysis to convince readers, I believe this approach can be useful to the future research of weakly-supervised learning and neural-symbolic learning for NER. I'm willing to increase the score when analysis is provided.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_WCcH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_WCcH"
        ]
    },
    {
        "id": "-PmEP2b3Rik",
        "original": null,
        "number": 4,
        "cdate": 1667533276144,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667533276144,
        "tmdate": 1667533276144,
        "tddate": null,
        "forum": "OVbY-QCCjAh",
        "replyto": "OVbY-QCCjAh",
        "invitation": "ICLR.cc/2023/Conference/Paper4557/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes SAGE, a method that extracts semantic-aware global explanations that can be applied to NLP in general and specifically experiments on name entity recognition.",
            "strength_and_weaknesses": "### Strengths:\n* The proposed method, SAGE, is potentially useful.\n\n### Weaknesses:\n* The paper may need major revision in its writing.\n  * The paper is not easy to read for me, especially the method section and its notations without complete definitions, e.g., mingensup, mingencon, maxrulelen, maxnrules, SORT-SUPPORT-AND-ABSTRACTION, etc. I need to guess their meanings.\n  * The paper can be further proofreading. For example,\n    * in Section 2, \u201cFinally, Performing\u2026\u201d has a wrong uppercase.\n    * In Section 3.1, \u201c..explanation method, the In Algorithm 1 the \u2026\u201d\n  * Most of the citations in this paper are not in a correct format. Please change \\cite{} to \\citep{}.\n* The paper lacks discussion about other explanations for NLP works while this paper claims to be a general method for NLP but test on NER, such as (I'm not asking you to cite them but would like to see its position in explanation for NLP field):\n  * David Alvarez-Melis and Tommi Jaakkola. \u201cA causal framework for explaining the predictions of black-box sequence-to-sequence models.\u201d In EMNLP 2017.\n  * Hanjie Chen and Yangfeng Ji. \u201cLearning variational word masks to improve the interpretability of neural text classifiers.\u201d In EMNLP 2020.\n  * Yi-Lin Tuan, Connor Pryor, Wenhu Chen, Lise Getoor, and William Yang Wang. \"Local explanation of dialogue response generation.\" In NeurIPS 2021.\n  * Hanjie Chen, Song Feng, Jatin Ganhotra, Hui Wan, Chulaka Gunasekara, Sachindra Joshi, and Yangfeng Ji. \u201cExplaining neural network predictions on sentence pairs via learning word-group masks.\u201d In NAACL 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "#### Clarity:\nThe paper is not easy to read for me.\n\n#### Quality:\nThe paper needs major revision in its delivery. Many details are skipped or just refer to figures, tables, or algorithm blocks. I\u2019m not sure how the F1 metric is computed in the end. Does it mean that the authors only feed the black-box classifier the explanations and use the NER F1 score to evaluate explanations?\n\n#### Novelty:\nThe paper misses two lines of papers, (1) papers about explanation in NLP while claiming the method is general to explanations for NLP tasks (as listed in the weaknesses), (2) papers about another line of works often also considered as local explanation, Shapley value.\nLloyd S Shapley. \u201cA value for n-person games.\u201d\nErik Strumbelj and Igor Kononenko. \u201cAn efficient explanation of individual classifications using game theory.\u201d The Journal of Machine Learning Research, 11:1\u201318, 2010\n\n#### Reproducibility:\nThe paper may not be easy to reproduce in its current status.",
            "summary_of_the_review": "My major concerns are that (1) the paper is not written well to deliver its ideas and results, and (2) according to the paper\u2019s statement to be general in NLP, I would anticipate to see discussion about other works on explanations of NLP in this paper, which are not mentioned. I would raise my score if these are clarified.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_4Q6v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_4Q6v"
        ]
    },
    {
        "id": "mONXjlug8NL",
        "original": null,
        "number": 5,
        "cdate": 1667535648583,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667535648583,
        "tmdate": 1667535648583,
        "tddate": null,
        "forum": "OVbY-QCCjAh",
        "replyto": "OVbY-QCCjAh",
        "invitation": "ICLR.cc/2023/Conference/Paper4557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper the authors introduce a new post-hoc method to create explanations for a black box classifier. They explain the steps in the method using the task of Named Entity Recognition as example. They turn the data into baskets of information , by extracting the contextual window around each word as well as some semantic features like pos tags, synsets etc and then use a data mining algorithm to extract patterns of co-occurrence corresponding to each classification label. Pruning is done over these patterns or candidate explanations based on thresholds and explanations are then selected greedily using f1 score maximization. The perform evaluation on this method by measuring the F1 score vs complexity defined by the number of explanations and they compare this to variations of LIME as well as a version of their model without semantic information\n",
            "strength_and_weaknesses": "Strengths:\n\n\t1. They support the necessity for each step in the algorithm\n\t2. They provide global explanations\n\t3. They are task agnostic , method can be applied to many NLP tasks\n\nWeaknesses:\n\t\n\t1. In section 3.1 they do not specify what certain notations mean  , eg the difference between the two transaction tables on the right of figure 2. \n\t2. Jump from section 3.2 to 3.3 is big especially for people who are unfamiliar with algorithms they point to such as FP-growth Han et al. (2000) and apriori Agrawal et al. (1994). They use an example for section 3.1 but then they drop the example for subsequent sections in the algortihm . \n\t3. Other evaluation metrics employed by other papers eg, fidelity to the model and comprehensibility could have been explored . Human evaluations might make a more compelling case .\n\t4. They don\u2019t perform any study about which semantic features help and which harm the f1 score.\n\t5. Visualizaition is an important part of explainable models which this paper lacks\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity of the paper is a bit rocky.  They use some phrases repeatedly in multiple sections but do not elaborate them . Eg.They claim one caveat of LIME is that explanations must be positional. But their method is also dependent on a fixed contextual window .\nNovelty exists , in the transforming of the data into a data mining task and incorporating semantic information.\nReproducibility is good since they list the hyperparameters used for the datasets mentioned in the paper. But it is unclear how much of finetuning of hyperparamers took place and how resilient the method is to any change in these values and its effectiveness for a different dataset or different task.",
            "summary_of_the_review": "Although the fundamental idea is good, it fails to convince that the method is robust across different datasets or NLP tasks.\nThey employ a contextual window of +/-3 tokens and that might not be transferrable to other NLP classification tasks like sentiment classifier or recommendation systems.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_3Fgr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4557/Reviewer_3Fgr"
        ]
    }
]