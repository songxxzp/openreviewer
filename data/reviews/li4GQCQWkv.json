[
    {
        "id": "yGQl6MnwJB",
        "original": null,
        "number": 1,
        "cdate": 1665843284431,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665843284431,
        "tmdate": 1666515310264,
        "tddate": null,
        "forum": "li4GQCQWkv",
        "replyto": "li4GQCQWkv",
        "invitation": "ICLR.cc/2023/Conference/Paper581/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles the problem experiments reproducibility with specific reference to inferential reproducibility which proposes to interpret the variation of performance values as due to the following factors; data characteristics, meta-parameter settings, includng also their interactions. The paper starts from the rationale that variance in performance values of different deep learning models and/or machine learning models in general is an intrinsic and interesting effect of non-determinism and a bug to be methodologically solved by researchers. A statistical model, i.e., linear mixed effects model, tailored to analyze performance data from paired experiments (models in this case) is developed to compare performances of state of the art models to that achieved by baseline models. The paper develops the argument of inferential reproucibility and the linear mixed effects model together with numerical experiments on natural language data that show how the proposed approach for inferential reproducibility can be effectively used to state whether the achieved performances by the baseline and the state of the art model are different or not.\n",
            "strength_and_weaknesses": "Strengths:\n- the problem addressed is a relevant one, I would say a fundamental problem for good scientific discussion in this times where almost each new pulished paper achieves better performances than well established existing baselines\n- the proposed model extends or let say replaces existing models aimed to the same goal\n- the proposed approach is elementary and can be easily interpreted and understood, it's assumptions and its' limitations\n- the paper is well organized and structured and it read easily and quite well\n\nWeaknesses:\n- numerical experiments are limited to only to two datasets, even if this is a weackness of the paper, I think it is a minor one\n- I personally do not like the strict statement that reproducibility is the wrong question to ask raher than inferential reproducibility, reproducibility is of great urgency and relevance while inferential reproducibility is a relatively new and fresh concept which deserves attention\n- stating that deep learning is non deterministic is a little misleading in my humble opinion, the outcome of learning deep learning models due to many hidded factors turns out to be stochastic but there are no theoretical reason that deep learning models are sthocastic.\n- at a give point of page 2, reading the paper, it seems that the authors could mention the no free lunch theorem while they did not, thus I would like them to check whether a link between that important theorem and what they wrote \"On the one hand, a successful duplication\nof a SOTA result on a benchmark does not guarantee generalization to new data. On the other hand, a model might generalize well to new data even if the exact SOTA result cannot be duplicated because of differences in computational budget.\"\n- maybe providing some more details about experimental design methodology for those who are not aware could improve the effectiveness of the paper, I personally know about about that but I do not think design of experiemnts (DoE) is a methodology that spread in the macine learning community.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is well structured, organized and written, at the same time its' quality is very good in my opinion.\nThe paper is novel in terms of the fact it proposes linear mixed effect models that suit much better than standard linear models and AVOVA models to answer the research questions tackled in the research study.\nAssessing reproducibility of a paper which is about reproducibility, in particular inferential reproducibility, can seen ilarious, but ... having say that I answer the paper is in principle reproducible by making availabe data, cose and meta-learning parameters used by numerical experiments.\nHowever, I think it would be clearer to refer to the comparison of the performance of two algotihms, rather than using the term SOTA and baseline, I think it would be clearer to talk about SOTA compared to a new algrithm.\nThe quality and readability of Figure 1 can be improved by making more clear to the interested reader the message it conveys.\n",
            "summary_of_the_review": "The paper is about a fundamental and urgent issue in scintific research, specially under the current setting where it seems that each new method is better and sometimes much better that existing baseline methods.\nThe paper also introduces and describes the use of linear mixed effect models to compare and futher analyze the perfoamnce achived by state of the art methods and that achieved by baseline methods. \nThe paper motivates the advantage of the proposed approach in theoretical terms and in a conving manner in my humble opinion.\nFurthermore, some numerical experiments are presented to support the statements made by the paper. However, I would have liked to see more numerical experiments to better explain the different aspects of the proposed approach.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper581/Reviewer_V9qk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper581/Reviewer_V9qk"
        ]
    },
    {
        "id": "-_XzE8DtKvK",
        "original": null,
        "number": 2,
        "cdate": 1667140606858,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667140606858,
        "tmdate": 1667140606858,
        "tddate": null,
        "forum": "li4GQCQWkv",
        "replyto": "li4GQCQWkv",
        "invitation": "ICLR.cc/2023/Conference/Paper581/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose the use of nested linear mixed effects models to compare performance of machine learning models. This approach is proposed to incorporate the effects of random initialization, hyper-parameter values, and other non-deterministic elements of model evaluation in the comparison. The authors then detail a reliability coefficient that compares the amount of variance in model performance due to the above aspects to the total variance. This enables the quantification of the notion of reliability in machine learning model evaluation.",
            "strength_and_weaknesses": "The authors approach attempts to address criticisms in reproducibility in the machine learning field. By using standard statistical analyses that separate out sources of variation, the authors are able to better characterize in what way the performance of two models may differ. This can increase the understanding in the community of the strengths and weaknesses of different models.\n\nThe perspective of the authors is valuable in that it addresses overly simplistic understanding of performance of machine learning models. Namely, one model (SOTA) being categorically better than another (baseline), is an overly simplistic view. Further, the authors' perspective calls into direct question the role of hyper-parameter tuning, random weight initialization, and the availability of computational resources, most of which are not explicitly considered in performance comparisons.\n\nUnfortunately, I do not think the authors have presented enough study of their point of view for a viable paper. While the authors propose to improve upon the comparison of models, their enumeration of the many dimensions of flexibility in their approach makes it clear that such an approach could easily exacerbate issues of uncertainty rather than resolve them. While being flexible enough to include many different sources of variation is useful, a high degree of standardization of the comparison is needed. Further, the authors do not address the power of their proposed test. \n\nIn the end, one could argue that model comparison has more to do with the ability of a model to perform well on novel problems, beyond the test set used for the comparison. One chooses to use a SOTA model not because it performed well on some historical test set, but because one hopes that it will perform well on one's problem at hand. The method proposed by the authors does address this in some capacity by quantifying the variation due to observations (sentences) separately from that due to hyper-parameters and random initialization. \n\nMore standardization and demonstration of the proposed approach are needed.\nWhile the authors mention that code is available, they do not really specify what that code does. Libraries that make it easy for authors to use the proposed approach, would be welcome.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well written and clear. \nThe approaches proposed are not novel, yet application of them to machine learning model comparison is a novel departure from the current state of practice. ",
            "summary_of_the_review": "A great start, but this paper needs more.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper581/Reviewer_yHxA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper581/Reviewer_yHxA"
        ]
    },
    {
        "id": "lW0wMfAooqf",
        "original": null,
        "number": 3,
        "cdate": 1667177396732,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667177396732,
        "tmdate": 1667177474104,
        "tddate": null,
        "forum": "li4GQCQWkv",
        "replyto": "li4GQCQWkv",
        "invitation": "ICLR.cc/2023/Conference/Paper581/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an approach to reproducibility that focuses on assessing and reporting variability in performance rather than focusing on replicating precise performance values.  The authors propose using a linear mixed-effects model to analyze the factors that influence performance.",
            "strength_and_weaknesses": "\\+ The paper discusses an extremely important issue around generalization and statistical variability. This issue has a long history and a central role in science (see below), but that understanding appears to be missing form many current ML researchers. This paper is a useful remedy for that problem.\n\n\u2013 The authors make a high-level point about characterizing and comparing variability which is a longstanding issue in science generally and statistics specifically. Then they recommend the use of several known methods in statistics. Thus, it is unclear what is novel about the paper. The authors appear to contend that researchers in deep learning are not making sufficient use of these ideas and methods (and I would tend to agree). However, there is little evidence of that point in the current version of the paper.\n\n\u2013 The paper does not cite some relevant scholarship in this area, including Andrew Gelman's work on \"the garden of forking paths\" (Gelman & Loken 2014) and David Hand's work on \"the illusion of progress\" (Hand 2006) in the context of classification. Others (e.g., Clary et al. 2019; Cobbe et al. 2019; Agarwal 2021) have raised issues of variability in reporting of results of deep learning methods.\n\n\u2013 The authors present their view as \"heretical\" when a long tradition of work exists on how to make inferences in the face of statistical variability. That some researchers in machine learning appear to have missed these basic methods of science hardly make the authors' viewpoint heretical. \n\n\u2013 The authors claim to present \"...a new type of reproducibility called inferential reproducibility...\" They note that \"It asks the following question: Can qualitatively similar conclusions be drawn from an independent replication of a study?\" Here, the authors are identifying the difference between what is sometimes called \"reproducibility\" (different team; same experimental setup) and \"replicability\" (different team; different experimental setup) (ACM 2020). They, themselves, note some prior scholarship on this issue, so it is unclear why they refer to it as \"a new type of reproducibility.\"\n\n\u2013 The authors describe a general problem (a focus on replicating precise values of performance rather than assessing and understanding the effects of variability in performance), and then jump immediately to specific statistical methods. An intermediate topic appears to be entirely missing: What sorts of generalization are being evaluated in a given study, what sorts of generalization are to be reasonably expected in practical uses, and how do those two match up? For example, Hand (2006) explicitly identifies explicit differences of the joint distribution between training and test as likely in realistic cases, and he conjectures that newer methods for classification are likely to be more sensitive to such differences. Similarly, work in concept drift, transfer learning, and related fields have assessed the robustness of specific types of machine learning models to specific types of differences in the distributions and when those sorts of differences are to be expected in practice. The paper would be improved by discussing this sort of middle-ground issue that connects the high-level points of the introduction and the specific statistical methods proposed for use.\n\nReferences\n\nAssociation for Computing Machinery (2020). Artifact Review and Badging. Version 1.1.\n\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34, 29304-29320.\n\nClary, K., Tosch, E., Foley, J., & Jensen, D. (2019). Let's Play Again: Variability of Deep Reinforcement Learning Agents in Atari Environments. arXiv preprint arXiv:1904.06312.\n\nCobbe, K., Klimov, O., Hesse, C., Kim, T., & Schulman, J. (2019, May). Quantifying generalization in reinforcement learning. In International Conference on Machine Learning(pp. 1282-1289). PMLR.\n\nGelman, A., & Loken, E. (2014). The statistical crisis in science data-dependent analysis\u2014a \u201cgarden of forking paths\u201d\u2014explains why many statistically significant comparisons don\u2019t hold up. American scientist, 102(6), 460.\n\nHand, D. J. (2006). Classifier technology and the illusion of progress. Statistical science, 21(1), 1-14.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and appears to be reproducible. What is less clear is novelty. See above comments for details.",
            "summary_of_the_review": "An important topic, and lots of interesting exposition about the high-level problem and specific statistical methods. However, the novelty is less clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper581/Reviewer_994g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper581/Reviewer_994g"
        ]
    }
]