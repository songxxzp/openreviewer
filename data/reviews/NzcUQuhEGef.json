[
    {
        "id": "i6At54ZqsY",
        "original": null,
        "number": 1,
        "cdate": 1666507573613,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666507573613,
        "tmdate": 1666507573613,
        "tddate": null,
        "forum": "NzcUQuhEGef",
        "replyto": "NzcUQuhEGef",
        "invitation": "ICLR.cc/2023/Conference/Paper2087/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method for graph contrastive learning, specifically optimized for heterophily graphs. The idea of HLCL is simple: to generate contrastive pairs, HLCL uses two branches of encoders. One encoder is the vanilla GCN which performs low-pass filtering (smoothing); the other encoder is uses the Laplacian matrix as the propagation matrix so that each layer preserves high-pass filtering. The low-pass embeddings are contrasted with the high-pass ones with a standard contrastive loss (InfoNCE). The additional high-pass encoder helps preserve information for heterophilous graph. Experiments on both homophilous and heterophilous graphs show that HLCL achieves significant accuracy improvements compared with various baselines. ",
            "strength_and_weaknesses": "## Strengths\n\n+ The paper is well-written and very easy to read. \n+ The idea to use high-pass filters for high heterophily is reasonable. \n+ Experiments have been performed extensively to demonstrate the accuracy gains as well as ablations. \n\n## Weaknesses\n\n- The novelty of the design is limited. There have been many existing works combining low-pass and high-pass filtering in GNN layers. Using high-pass filtering to address heterophily is also known in the literature. The designs of the constrastive learning architecture and loss function follow a standard protocol.  \n- The overall design is simple. It would be better to include more analysis to deepen the understanding. \n- It is mentioned that the \"high-pass-low-pass\" encoder pair is used only in contrastive learning. Only the low-pass encoder is used during inference. It is not clear how such a design can handle heterophily during inference: regardless of the training process, the inference architecture only performs smoothing and is unable to preserve high-frequency components. \n- Experiments would be more convincing if larger graph are used. The currently included benchmarks are all very small. However, one of the main motivations for CL is that labels are expensive to obtain on *large* graphs. Achieving accuracy gain on small graphs do not fully justify the proposed CL method. \n- Experiments should follow a standard setup. For example, each dataset should come with a standard data split commonly accepted by the community. However, the authors \"randomly select 10% nodes for training, 10% nodes for validation and 80% nodes for testing\". As a result, some accuracy number of the baselines do not match those in the original papers (e.g., DGI on CiteSeer). \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written. The motivation is clear and valid; the architecture design is illustrated with sufficient details. \n\nFor the quality, I think the design is sound. But the analysis and in-depth understanding is limited. \n\nThe novelty of the paper is limited. Essentially, the method is to transfer a known architecture (GNN with different filtering functions) into the contrastive learning setup. \n\nThe paper has included some details in the experimental setup. However, detailed hyperparameter settings as well as the tuning procedure are not described for HLCL and the baselines. Source code has not been provided. ",
            "summary_of_the_review": "Overall, the paper presents a simple constrastive learning method that shows some accuracy gains on small heterophilous datasets. The paper shows some potential from its evaluation. However, at its current stage, I think the paper is still below the bar of acceptance due to its lack of in-depth analysis, limited novelty and lack of evaluation on realistic, large graphs. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2087/Reviewer_vWUj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2087/Reviewer_vWUj"
        ]
    },
    {
        "id": "F4ljMJ-Dpy",
        "original": null,
        "number": 2,
        "cdate": 1666650674495,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650674495,
        "tmdate": 1666652037867,
        "tddate": null,
        "forum": "NzcUQuhEGef",
        "replyto": "NzcUQuhEGef",
        "invitation": "ICLR.cc/2023/Conference/Paper2087/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes taking leverage of graph filters to directly generate augmented graph views for graph contrastive learning. The model tries to maximize the agreement between a low- and high-pass encoder of the same graph. The experiments show that this approach improves the graph CL results in heterophily graphs. ",
            "strength_and_weaknesses": "**Strength**:\n- The paper is well-written and clear.\n- Heterophily graphs have gained more attention recently, and the problem they are trying to tackle is relatively novel.  \n- The method is straightforward and the results are promising.\n\n**Weaknesses**:\n- My main question is how much of the reported performance gains in heterophily graphs are coming from the contrastive part and how much from the high-pass filter. What would happen if we just replace the encoder in the other graph CL methods in table 2, with a high-pass encoder?\n- Minor:\\\ntypo: \"We follows\" in line 1, section 4.2 \\\nPage 6, the paragraph above equation 3 has 2 repetitive sentences. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned, the paper and method are clear and simple.\\\nThe novelty lies in the idea of using graph filters for heterophily graphs.\\\nI believe it wouldn't be a problem to reproduce this method. ",
            "summary_of_the_review": "Most of the points are mentioned in the strength and weaknesses section but in summary, to the best of my knowledge, this is the first paper that addresses graph CL for heterophily graphs.\\\nThe paper is clear and the method is straightforward.\\\nHowever, I believe the scope is very limited since the significant gains are seen only in heterophily graphs. Also, calculating Laplacian for very large graphs can be troublesome. Besides, we don't know the effect of just replacing the encoders in the current graphCL methods with a high-pass one.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2087/Reviewer_KBX6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2087/Reviewer_KBX6"
        ]
    },
    {
        "id": "xEHsnD7L--6",
        "original": null,
        "number": 3,
        "cdate": 1666735050725,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735050725,
        "tmdate": 1666735050725,
        "tddate": null,
        "forum": "NzcUQuhEGef",
        "replyto": "NzcUQuhEGef",
        "invitation": "ICLR.cc/2023/Conference/Paper2087/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "They argue that current graph contrastive learning methods are inadequate for graphs with \u2018heterophily\u2019 in which nodes have diverse signals. They argue this is partially due to the nature of GNNs over-smoothing node signals in neighborhoods. They propose a method of view generation for contrastive learning on graphs in which a typical GNN-encoded graph is contrasted with a graph in which differences between nodes are magnified. They do this by using a \u2018laplacian matrix\u2019 or the degree diagonal matrix minus the adjacency matrix in place of the typical augmented adjacency matrix when \u2018aggregating\u2019 signals in the typical GNN way. The laplacian matrix is used in one branch and the other uses the adjacency matrix as a typical GNN, they are in my opinion analogous to a sharpening kernel v.s. a blurring kernel. The results of these two branches are used for contrastive learning as a positive pair, with other samples being negative. They show results against benchmarks on datasets with homophily (similar nodes) v.s. heterophily, and have particularly impressive results with the latter, and studies on how augmentations compare with their method in graphs with heterophily, and how their method changes with augmentation schemes. ",
            "strength_and_weaknesses": "The strengths of the paper are that it is very clear and the results seem impressive. I don\u2019t have particular actionable criticism that I can think of. ",
            "clarity,_quality,_novelty_and_reproducibility": "I believe it is clearly written and there should be no trouble in reproducing their experiment. I haven\u2019t seen this particular idea before and believe it is novel enough, at least in application to graph contrastive learning. ",
            "summary_of_the_review": "I believe the paper is clear and concise. The technical contribution / deviation from related work is very simple, and it is hard to say it\u2019s entirely novel, but is put to good use, and I believe it is novel in its application to graph contrastive learning at the least. The results are impressive. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2087/Reviewer_TBxC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2087/Reviewer_TBxC"
        ]
    },
    {
        "id": "hQRi4cTD5O",
        "original": null,
        "number": 4,
        "cdate": 1666804235835,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666804235835,
        "tmdate": 1666804235835,
        "tddate": null,
        "forum": "NzcUQuhEGef",
        "replyto": "NzcUQuhEGef",
        "invitation": "ICLR.cc/2023/Conference/Paper2087/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a graph contrastive learning method called HLCL, which performs better than SOTA methods on heterophily graph datasets. HLCL uses a high-pass and a low-pass graph filter to generate two graph representations and then contrasts the two representations. The high-pass filter uses renormalized Laplacian matrix left, multiplying the product of the node embeddings matrix and weight matrix, while the low-pass filter uses a renormalized version of the adjacency matrix. Some experiments analyze the influence of different graph augmentation methods.",
            "strength_and_weaknesses": "Strengths:\n1. In the experiment part, this paper discussed the effect of different graph augmentation methods very thoroughly.\n\n2. The problem of heterophily is worth exploring, and there are not many works analyzing this problem very deeply. This paper provides a contrastive learning method for this problem, which has great potential.\n\nWeaknesses:\n1. Lack of explanation about the influence of the low homophily ratio on the low-pass filter, and about why HLCL works in high homophily ratio graphs. Since this paper claims that HLCL can obtain good performance, there should be some explicit explanation about why HLCL is supposed to do better on heterophily graphs. Explanation with only words is not convincing. Some toy examples or math descriptions of the influence of the graph\u2019s homophily ratio and high- and low-pass filters are strongly suggested.\n\n2. Confusing math notations. In 3.1 PRELIMINARIES  High-pass and Low-pass graph filters, several transpose donations are missed for vector ui.\n\n3. Outdated SOTA result. SOTA results in Table 2 are not up to date. For example, Chameleon, Squirrel, and Texas datasets. For example, GGCN in Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks (accepted in ICDM 2022), whose code is open-sourced, outperforms HLCL on both Chameleon and Squirrel datasets, two datasets with heterophily graph. Only comparing HLCL with Contrastive learning methods cannot prove its high accuracy. \n\n4. The datasets used in the experiments are small and outdated. There are only two types of networks, i.e., small citation networks and small web page networks. This makes the results not sufficiently convincing.\n\n5. Lake of novelty. In Interpreting and Unifying Graph Neural Networks with An Optimization Framework (www2021), GNN-LF (GNN with Low-pass Filtering Kernel) and GNN-HF (GNN with High-pass Filtering Kernel) are proposed, GNN-HF-closed has SOTA performance on Cora and CiteSeer. This paper uses contrastive learning with these two GNNs, so the novelty is marginal. Besides, GNN-LF contains a linear combination of normalized adjacent matrix and identity matrix, while GNN-HF contains the sum of normalized Laplacian matrix and identity matrix. The details of the high- and low-pass filters are very similar to this paper.\n\n6. Bad algorithm expression. The pseudocode illustrated in Alg. 1 is meaningless. It is better to add some unique details of HLCL in Alg. 1. The current Alg. 1 seems to fit every graph contrastive learning algorithm.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper\u2019s framework is very clear.\n\nThis paper lacks novelty. In Interpreting and Unifying Graph Neural Networks with An Optimization Framework (www2021), GNN-LF (GNN with Low-pass Filtering Kernel) and GNN-HF (GNN with High-pass Filtering Kernel) are proposed, GNN-HF-closed has SOTA performance on Cora and CiteSeer. This paper uses contrastive learning with these two GNNs, so the novelty is limited. Besides, GNN-LF contains a linear combination of normalized adjacent matrix and identity matrix, while GNN-HF contains the sum of normalized Laplacian matrix and identity matrix. The details of the high- and low-pass filters are very similar to this paper.",
            "summary_of_the_review": "Although this paper is not so novel in specific GNN, it still proposed a contrastive learning frame for heterophily graphs, and also discussed the influence of different graph augmentation methods in its frame.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2087/Reviewer_32Ub"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2087/Reviewer_32Ub"
        ]
    }
]