[
    {
        "id": "n0p4b46kHD",
        "original": null,
        "number": 1,
        "cdate": 1666353098521,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666353098521,
        "tmdate": 1670841115776,
        "tddate": null,
        "forum": "X4DOJ-wL2I",
        "replyto": "X4DOJ-wL2I",
        "invitation": "ICLR.cc/2023/Conference/Paper3876/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method for safe RL or more general constrained RL. Constraints are modelled by the mean-std method. As RL algorithm the authors use TRPO and use a multi-step distributional critic.\nApart from combining these methods, the paper seems to have three independent contributions: \n1. The authors extend the distributional RL approach based on quantile regression and Wasserstein distance to the multistep setting.\n2. Then replace the advantage function with the q-function in estimating the trust region.\n3. Finally, they propose a gradient-based method to find starting policies in the trust region.\nExperiments are conducted on the safety Gym environment and compare to other safe RL approaches w.r.t. constraint violations and performance.",
            "strength_and_weaknesses": "strong points:\n* The paper combines and extends several SOTA techniques.\n* Results indicate a superior trade-off between constraint violations and performance\n\nweak points:\n* The paper's contributions are not easily distinguishable from the combined basis methods.\n* it is unclear how the contributions relate to each other. Some of them seem disconnected and not really specific to safe RL. In particular, using a distributional critic seems valid for any AC method. \n* Some of the methods are not well-motivated. For example, I did not get why replacing the advantage function with the q-function is beneficial. \n\nAfter reading the author's comments and the revised paper, explaining why these methods address open problems of constrained RL with trust regions is more precise. \n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the paper is clear. However, it is advisable to have a look at the papers the proposed SDAC method is built on top of.\nThe technical quality is high, and the authors seem to have the ambition to use sota-methods for any part of their method.\nThe paper does provide code and provides hyperparameters and settings in the appendix. Thus, reproducing the results should be no major problem,\nThe novelty of the approach is kind of hard to assess. The authors have engineered a well-performing safe RL method. However, the contributions do not seem to follow a single general idea but are tailor-made to make the combination to trust-region-based safe RL and distributional methods work.",
            "summary_of_the_review": "The paper proposes a well-performing safe RL-method which combines several recent approaches such as trust-region-based safe RL and distributional critics. Experimental results are also quite convincing. \nOn the other hand, some of the contributions are not well-motivated. For example, the authors claim that it is a good idea to use q-values instead of the advantage function. However,  as the advantage function is the q-function minus the value function and it was introduced to lower the variance, it is kind of unclear why this is done. Later, they add entropy regularization which makes more sense, but it is not very exciting as it is a well-known technique. \nIn general, many improvements seem to be a relatively straightforward combination of existing techniques and a better explanation on why the proposed extension yield a major technical contribution would be desireable.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3876/Reviewer_FqLh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3876/Reviewer_FqLh"
        ]
    },
    {
        "id": "CWHNyShBdO",
        "original": null,
        "number": 2,
        "cdate": 1666696777245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696777245,
        "tmdate": 1666696777245,
        "tddate": null,
        "forum": "X4DOJ-wL2I",
        "replyto": "X4DOJ-wL2I",
        "invitation": "ICLR.cc/2023/Conference/Paper3876/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The author develop an efficient trust region-based safe RL algorithm with multiple constraints.\n1. introduce a TD(\u03bb) target distribution combining multiple-step distributions to estimate constraints with low biases\n2. present a memory-efficient method to approximate the TD(\u03bb) target distribution using quantile distributions\n2. to increase the efficiency of the trust region method, we propose novel surrogate functions which use the reparameterization trick with Q-functions",
            "strength_and_weaknesses": "Strength\n1. introduce a TD(\u03bb) target distribution combining multiple-step distributions to estimate constraints with low biases\n2. present a memory-efficient method to approximate the TD(\u03bb) target distribution using quantile distributions\n3. propose novel surrogate functions which use the reparameterization trick with Q-functions to increase the efficiency of the trust region method,\n\nWeaknesses\n\n1. I don't understand what figure 1 is trying to say, could the author be more clear?\n2. The authors believe that Figure 3 can be clearly represented, \"the result can be interpreted as excellent\", but from this figure, I have no way to see the cost limit, as well as fine metrics, intelligent roughly, the method of this paper is roughly put in the upper left, but Point Goal and Car Goal seem to overlap in the upper left.\n3. I carefully read through the experimental results of the article, and in Appendix D.1, Figure 8, in Point Goal and Point Button, WCSAC is significantly better than the SDAC proposed in this paper in terms of score, cost rate, and Total CVs, and this seems to be consistent with the Safe RL perspective. From the Safe RL perspective, this seems to be in great conflict with what is discussed in the abstract, \"From extensive experiments, the proposed method shows minimal constraint violations while achieving high returns compared to existing safe RL methods.\" \n    I suggest that the authors should rework Figure 3 or put a more refined result, Figure 8, in the text, because the ambiguous and unrefined Figure 3 leads to a misunderstanding of the performance of the algorithm.\n4. In Figure 8, why is there if a big difference between cvpo in Button's task and Goal's task? cvpo's reward in Goal's task is even negative, while it does have a considerable reward in Button's task.\n5. As far as I know, \\alpha is the discount factor corresponding to cost, so why the comparison in Figure 9 and Figure 10 lack CPO and CVPO.\nBased on the above doubts, I have doubts about the contradictory claims in this article and the results of the experiments, and the authors don't seem to disclose their code in the appendix material. Also based on the list of hyperparameters provided by the authors (and no other baseline details of the experimental parameters are provided), I think the reproducibility of the experimental results of this work is very low.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality, Novelty, and Reproducibility can be seen above.",
            "summary_of_the_review": "I have many doubts about the experimental results and opinions in this article, and I hope the authors can explain them in detail.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3876/Reviewer_97Q2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3876/Reviewer_97Q2"
        ]
    },
    {
        "id": "SjOQOvAxO_",
        "original": null,
        "number": 3,
        "cdate": 1666903506814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666903506814,
        "tmdate": 1666903506814,
        "tddate": null,
        "forum": "X4DOJ-wL2I",
        "replyto": "X4DOJ-wL2I",
        "invitation": "ICLR.cc/2023/Conference/Paper3876/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper concerns safe (deep) reinforcement learning. In particular, the authors present an approach that take the constrained MDP setting, where on top of a reward function, a cost function is given that conveniently reflects safety concerns. The learning goal is then to maximize the reward, while the cost needs to stay below a certain thresholed. In this work, the authors employ actually a set of cost functions. The actual approach presented here entails a distributional RL method that uses trust region optimization, like in other prior works. Here, however, the multipe constraints cause problems, and the authors use (1) a target distribution to estimate constraint biases, and (2) something called the \u2018parameterization trick\u2019 to develop a dedicated actor-critic techniqe. The approaches are evaluated by means of several environments from the Safety Gym, and further robotics experiments.\n",
            "strength_and_weaknesses": "++ Important problem fitting for ICLR\n++ Novel approach with multiple cost functions\n++ Interesting actor-critic technique\n\n\u2014 The paper is at parts very hard to follow\n\u2014 Unclear utility of multiple cost functions in the Safety Gym environments\n",
            "clarity,_quality,_novelty_and_reproducibility": "The biggest weakness of the paper is, at least to me, the clarity of writing. While I believe that the approach is sensible (good quality), and that it is novel, I find the paper extremely hard to follow. I had to read the introduction at least three times to have an idea of the approach, and the tense technical description does not help me. I think with more structure and more intuition, the paper can definitely be improved. \n\nI have some specific questions, and I am happy to re-evaluate my judgement after the discussion.\n\n\u2014 computing the target distribution in an off-policy setting sounds a bit like \u2018safe policy improvement\u2019 (SPI), coined by Thomas et al. This offline RL technique estimates a better policy than the data-collecting behavior policy from a limited amount of data. Some clarification or comparison there would be nice.\n\n\u2014 Is the overall methods \u2018safe\u2019 or reliable, that is, are we guaranteed to get a final policy that is safe? Can anything be said about a trade-off between safety and rewards?\n\n\u2014 In the first part of the experiments, I do not understand the multiple constraint setting. Are there even multiple constraints? And if not, why is the method performing so much better than state-of-the-art baselines, who are supposed to perform well on the single constraint setting. \n\n\u2014 Could it be an interesting experiment to shape a cost function that aims to capture the multiple constraints, and then show the differences in safety?\n",
            "summary_of_the_review": "Good approach, clarity can be improved. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3876/Reviewer_SPhH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3876/Reviewer_SPhH"
        ]
    },
    {
        "id": "g6mVeZcPAS",
        "original": null,
        "number": 4,
        "cdate": 1666963466005,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666963466005,
        "tmdate": 1669724789563,
        "tddate": null,
        "forum": "X4DOJ-wL2I",
        "replyto": "X4DOJ-wL2I",
        "invitation": "ICLR.cc/2023/Conference/Paper3876/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose SDAC, a safe RL method based on trust regions that uses TD($\\lambda$) for the distributional critic and SAC-style updates.\nThey provide an elegant and memory-efficient way to approximate TD($\\lambda$).\nThey propose to perform gradient integration in order to avoid getting stuck due to unmet constraints.\nThe proposed method was tested on Safety Gym with very nice results.\n",
            "strength_and_weaknesses": "### Strengths\n\nThe paper involves technical elements that the authors explain in a clear way.\nFor instance, the introduction is super clear and provides a nice overview of the existing methods. In the background, the notations are clear and precise. The figures do a great job at facilitating the comprehension. The related work is exhaustive and well-organized.\n\nThe results on Safety Gym are quite nice, showing increasing or equal scores while reducing the number of violated constraints, which illustrates the empirical pertinence of the proposed method.\n\n### Weaknesses\n\nThe paper is quite dense and I think this aspect could be improved. Some proofs might appear in the appendix instead.\n\nOn the title: use the term low-bias instead of low-biased?\n\nOn the RL method:\n* It should be made clearer that the recursive formula (eq.7) is particularly useful to implement the method\n* Fig.1 is nice, but a clarification as to why a projection is needed, i.e. different supports for distinct time-steps, would be welcome in the text \n* why the off-policy version of TRPO? This choice should be motivated.\n* Why TRPO over PPO? PPO enforces a trust region by modifying the policy gradient, while TRPO requires backtracking. If I understand correctly this is because backtracking includes the linearized constraints as well? This choice could be better motivated in the text.\n* What is the cost of gradient integration in practice?\n\nOn experiments:\n* There are many hyperparameters for the resulting method, how hard is it to hyperoptimize? I think a discussion on that aspect would benefit the work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: clear but quite dense, some design choices are not well explained in the current state\n\nQuality: good\n\nNovelty: several elements are novel or their combination is novel (efficient TD($\\lambda$), distributional critics, SAC-style surrogates, gradient integration).\n\nReproducibility: no code is available as far as I know, a computation cost analysis is missing",
            "summary_of_the_review": "This is a nice paper, featuring good writing and a genuine effort at making technical points digest to the reader.\n\nThe experiments are varied and convey a lot of information. They do a good job at validating the claims of the paper.\n\nThe proposed algorithm seems super heavy (k-distributional critics + backtracking + quadratic programming for gradient integration) but the computational cost is not quantified in this work, which I think is regrettable. \n\nIn the current state, it is somehow hard to see how the community will use this work if no open-source implementation and/or not more comments on computational cost and sensitivity of the hyperparameters.\n\nFor all these reasons, though the proposed method is inventive and sound, I only borderline recommend for acceptance. There are low hanging fruits that the authors could choose to address to increase the potential impact of the work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3876/Reviewer_QThN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3876/Reviewer_QThN"
        ]
    }
]