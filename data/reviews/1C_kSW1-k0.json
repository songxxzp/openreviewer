[
    {
        "id": "ivs_HrIjIJ",
        "original": null,
        "number": 1,
        "cdate": 1665788825365,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665788825365,
        "tmdate": 1666709854944,
        "tddate": null,
        "forum": "1C_kSW1-k0",
        "replyto": "1C_kSW1-k0",
        "invitation": "ICLR.cc/2023/Conference/Paper409/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work introduces a new benchmark (STREET) to evaluate the reasoning capacity of language models. The benchmark contains 5 different reasoning datasets (ARC, SCONE, GSM8K, AQUA-RAT, AR-LSAT) requiring on average 7.8 reasoning steps per answer.\nThis benchmark evaluates 3 metrics: Answer Accuracy, Reasoning Graph Accuracy, and Reasoning Graph Similarity.\nThe paper evaluates a t5-large model fine-tuned on individual tasks and a GPT3 model in a few-shot procedure. Results show that large language models still struggle in these multi-step reasoning tasks.\n",
            "strength_and_weaknesses": "**Strengths**:\nThe paper is clear and easy to read. The motivation of the work is well defined.\nThe introduction of an additional multi-step reasoning benchmark is very welcomed and can have a large impact as many previous multi-hop datasets were either only a few hops (mainly 2), or lacking the natural language steps to explain complex answers.\n\n**Weakness (W1)**: However, some previous datasets propose multi-step reasonings with logical explanations in natural language: CLUTRR (Sinha et. al, 2019) and AI2 ProofWriter (Tafjord et. al, 2020). The paper must compare their benchmark to these previous work.\n\n**Proposition (P1)**: It would be nice to include CLUTRR and ProofWriter in the STREET benchmark or at least document how the community can contribute to STREET and add new relevant multi-step reasoning datasets in time.\n\n**Proposition (P2)**: Can the authors compare models\u2019 performance when the candidate answers are present and missing from the input to the model.\n\n**Recommendation (R1)**: The STREET benchmark should also propose a split based on the number of reasoning steps per dataset. That would give users the opportunity to evaluate (1) a multi-task setup, and (2) a compositional generalization setup: can a model trained on k reasoning steps perform well on k+-n reasoning steps. \n\n**Question (Q1)**: The reasoning graph\u2019s steps are ordered according to a valid topological order. However, there can exist multiple topological orders. For instance if we have A+B->D followed by B+C->E, followed by D+E->Final Answer, we can also first prove B+C->E then A+B->D then E+D->Final Answer. When linearizing the graph into a sequence of tokens, did the authors shuffle the premise order? Otherwise trained models could pick on some weird premise order biases. Something to consider.\n\n**Recommendation (R2)**: The text similarity function for GSM8K and AQUA-RAT should probably include the operator characters. If a reasoning step contains 48*2=24 and the golden step was 48/2=24 the current evaluation will consider the predicted step correct.\n\n**Weakness (W2)**: The paper should include previous best performance on each individual dataset. This can be done by adding a \u201cbest model\u201d line to the Answer Accuracy section of Table2, and detailing in the text of the paper or the caption of the table which \u2018best\u2019 model refers to for each of the dataset.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. The proposed work is novel and very interesting.\nBelow are some clarification questions for the authors:\n\n**Clarification Question (CQ1)**: In the text example shown in Section 3.1, why do you need to have (3) in the proof steps that does (1)+(3)+(4)->(5) ? (1) and (4) should give enough information to infer (5): 72 clips in April and May.\n\n**Clarification Question (CQ2)**:  In the Appendix A.1, in the SCONE (Alchemy) example, how is the agent supposed to know that mixing 1 yellow and 1 green chemical gives 2 brown chemicals? Same question for (17)+(12)->(19).\n\n**Clarification Question (CQ3)**: In the Appendix A.1, in the GSM8k example I believe conclusion (8) should have incoming edge (6) in addition to (7). If not, can you explain why the proof has a (5) -> (6) step since (6) is not used anywhere in the proof.\n\n**Recommendation (R3)**: This makes me think about another statistic that would be interesting to have: how many proof steps in the graph are not used to arrive at the final answer? These can be identified as leaf nodes that are not the answer in your graph.",
            "summary_of_the_review": "This is a good paper, however some things need to be added: see **W1**, **W2**, and **R1**, **R2**, **R3** paragraphs in the above sections. In addition, after looking at the few examples of the dataset in the paper, I question the quality of the reasoning graphs automatically generated (see **CQ1**, **CQ2** and **CQ3** above.)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper409/Reviewer_KKP2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper409/Reviewer_KKP2"
        ]
    },
    {
        "id": "P6am4pfXcfL",
        "original": null,
        "number": 2,
        "cdate": 1666654202904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654202904,
        "tmdate": 1666726897492,
        "tddate": null,
        "forum": "1C_kSW1-k0",
        "replyto": "1C_kSW1-k0",
        "invitation": "ICLR.cc/2023/Conference/Paper409/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents STREET, a multi-task benchmark for structured reasoning and explanations in NLP.  \n* STREET is composed of 5 existing datasets: ARC, SCONE, GSM8K, AQUA-RAT and AR-LSAT. \n* The authors introduced a unified reasoning formulation involving textual logical units (TLUs) and reasoning graphs.\n* The authors used expert annotation or automatic programatic annotation to annotate the reasoning process on a subset of the 5 datasets mentioned above. \n* The authors introduce three metrics (answer accuracy, reasoning graph accuracy, and reasoning graph similarity) to evaluate a system trained on STREET.\n\nEmpirically, the authors evaluate T5-large fine-tuning and GPT-3 in-context few-shot learning performance on STREET, and explain errors on a per-dataset basis.",
            "strength_and_weaknesses": "Strength:\n* The paper studies an important problem: reasoning capabilities when answering complex questions.\n* The STREET benchmark, especially the reasoning graphs that are newly annotated, is a useful resource for studying structured reasoning and explanations. (Thanks to the authors for these huge efforts!)\n* Paper is written clearly and is easy to follow. \n\nWeaknesses:\n* Lack of comparisons and justifications\n  * Since multi-tasking and a unified reasoning formulation is one of the main contribution, it is necessary to include non-multitask performance of the five datasets in order to justify whether multi-tasking is useful.\n  * To justify the proposed unified reasoning formulation is useful, it is necessary to include a multi-task baseline with no reasoning and proofs. \n  * To justify that the proposed reasoning graph is more flexible, it is necessary to compare performance (on ARC, where annotations are available for both methods) of reasoning graph and entailment tree. Also please consider including examples that entailment trees are not able to represent but reasoning graphs are.\n* Findings are mostly comparing performance on the surface-level. \n  * In sec 4.2.1, error analysis is conducted on each dataset _separately_. As a multi-task benchmark, it is important to show what is lacking from current best methods with a holistic view, and what are the potential directions for future research.\n  * I will be very interested in a case study on reasoning graphs generated by the models. How often do they generate a valid reasoning graph in the output?\n  * I will also be very interested in quantifying performance by grouping test set with reasoning steps. Does the model struggle more on test cases with more reasoning steps?\n* More details needed on annotation process\n  * Since the resource of reasoning graphs is one main contribution of the paper, it is important to provide more information about how expert annotators are selected, whether inter-annotator agreement is measured (as mentioned, \"there are multiple ways one can explain the answer\"). \n  * This is less important but please provide more information on the user-interface mentioned in Sec 2.3.\n\nQuestions:\nI have some questions regarding some choices in STREET.\n* In Table 3, different similarity function is used for different dataset. I suspect that extracting and comparing numbers or using BLEURT > 0.25 may be over-simplified, but I may be wrong. Do you observe any false negative/positives? How often does this happen?\n* Is there any reason that an adapted version of HotpotQA (e.g., using the gold context sentences, so that information needed to answer the question is present) is not included in STREET. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has good clarity and quality. \nThe authors promise to release data, code and a leaderboard, so reproducibility is good. ",
            "summary_of_the_review": "The reasoning formalism and resources introduced in the paper will be useful to the research community. \nHowever, the paper could benefit from more detailed empirical analysis on the proposed benchmark. \nAlso, some experiment settings and designs need to be further justified or explained (e.g., what's the motivation of multi-tasking?)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper409/Reviewer_xxyE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper409/Reviewer_xxyE"
        ]
    },
    {
        "id": "vWuFpeYOT6",
        "original": null,
        "number": 3,
        "cdate": 1666679454208,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679454208,
        "tmdate": 1666724411228,
        "tddate": null,
        "forum": "1C_kSW1-k0",
        "replyto": "1C_kSW1-k0",
        "invitation": "ICLR.cc/2023/Conference/Paper409/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThe authors define a reasoning task, that of providing a graph that\ncorresponds to the reasoning steps leading to the correct answer (from\npremises provided in the question), and build an annotated dataset\nwith question and answers and the graphs from several domains. The\nauthors present several properties of the dataset (histograms of\nnumber of steps required, number of premises, etc). They explain how they use existing\ntechniques to produce their reasoning, how they evaluate (graph\ncomparisons), and report that existing techniques appear to\nsubstantially lag behind human performance (even though, the correct\nanswer rate may be relatively high).\n\n",
            "strength_and_weaknesses": "Strengths:\n\n-- A challenging task with much annotations in several domains\n\n-- An analysis of performance of several advanced techniques\n\nWeaknesses:\n\n-- A few minor clarity issues\n\ntypos, etc:\n\nPage3: \"The TLUs for the rationale L_R can be thought of .. \" is\nconfusing ..  (I don't think L_R is defined yet, though L_T is, and\nperhaps you meant to say: 'The TLUs for the rationale R, L_R, can be\n...\", that is introduce the notation L_R pehraps ..) (L_R is used in\nthe next paragraph.. ) (On re-reading, I now understand T can be any of\nL, O, etc.. perhaps stress that, early, when you start with for any T\nin \\bf{T}..  )\n\npage2: why not use 'correct' instead of 'expected' answer.\n\nA couple of questions:\n\nquestion 1: [Assessing the quality of graph matching] Especially when\nthe system gets the answer right, and the graph generated is perhaps\nsomewhat similar but not perfect, how often are the steps actually\ncorrect or close to correct (is it near 0%?).  It would be good to\nunderstand the calibration of your graph matching, or to get a\nfunction mapping from similarity score to correctness or degree of\nconnectedness, by manually examining a few such cases, perhaps 10s to\n100s is sufficient to get an estimate (you may have done this\nanalysis, but I missed it) (I looked at appendix C but didnt find\nsuch). It is possible that a partial graph similarity score is almost\nnever correct (with probability 90% or 99% depending on how many you\nmanually evaluate).\n\nquestion 2: Is there any need for negation (does that happen, or is\nthe percentage of such basically 0)?  For instance, a part of the\nquestion or context negating one of the multiple choice answers,\nthereby rendering it incorrect.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and there is strong novelty in defining a\nchallenging and useful task.  The dataset will be available, and the\ntechniques used can be reproduced.\n",
            "summary_of_the_review": "\nA useful challenging task that will help advance language understanding and research on reasoning/explanations.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper409/Reviewer_3BMb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper409/Reviewer_3BMb"
        ]
    },
    {
        "id": "p1Fgj3KmQO",
        "original": null,
        "number": 4,
        "cdate": 1666895100599,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666895100599,
        "tmdate": 1669042446962,
        "tddate": null,
        "forum": "1C_kSW1-k0",
        "replyto": "1C_kSW1-k0",
        "invitation": "ICLR.cc/2023/Conference/Paper409/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "1. This paper presents STREET, a new question answering benchmark that also contains step-by-step structured explanation annotations.\n2. Established a number of baselines on this dataset including GPT3 few-shot prompting.",
            "strength_and_weaknesses": "Strengths:\n1. explainable NLP is an active research area, and this benchmark can help the community push forward the progress\n2. Some baseline results are estalished\n\nWeaknesses:\n1. There is a lack of detail in annotation process.  Specifically, AR-LSAT seems like an extremely difficult reasoning dataset, and yet in section 2.2, very limited information about how the annotation process is carried out.\n2. There is a lack of dataset quality verification process / dataset analysis.  There are many papers showing that the agreement rate could be low for intermediate reasoning steps annotations.  An in-depth study in this regard is important.  Examples include sec 3.4 in [1] and and sec 4 [2].\n3. As a multi-task dataset, there is a lack of analysis in the multi-task aspect.  At the bare minimum, it would nice to see how fine-tuning on all of the tasks improves or degrades the performance. See [3] for an example of what people do to analyze multi-task models.\n\nQuestions / suggestions:\n1. What is the rationale of not including previous multi-hop reasoning dataset such as ProofWriter?  Are the authors only selecting datasets that contain pure natural language but not synthetically generated language?\n2. It would be nice if the authors could discuss what more can this paper offer than [4], which has a more comprehensive survey on the structured explanation dataset.  I guess the unifying metrics established in this benchmark is valuable for future work to evaluate different NLP systems under the multi-task setting.\n\n[1] WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge\n[2] HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\n[3] UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark\n[4] Teach Me to Explain: A Review of Datasets for Explainable NLP",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good, but Quality, Novelty And Reproducibility needs improvement.",
            "summary_of_the_review": "This paper presents a valuable benchmark for pushing forward research in structured explainability, and I would love to see it accepted in the end.  In the current version, 1) authors spent most effort in collecting annotations alone and have done little analysis for the annotations, and 2) although being a multi-task benchmark, this is a lack of analysis in the multi-task aspect because experiments have mostly been done on each task individually. For these two reasons, I don't feel this paper is ready to be presented.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper409/Reviewer_4X1J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper409/Reviewer_4X1J"
        ]
    }
]