[
    {
        "id": "0OW3QXnf0Y",
        "original": null,
        "number": 1,
        "cdate": 1666597340581,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597340581,
        "tmdate": 1670519216373,
        "tddate": null,
        "forum": "xWFguIF_hG",
        "replyto": "xWFguIF_hG",
        "invitation": "ICLR.cc/2023/Conference/Paper2286/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the parametric landscape in terms of the loss on clean data. Inspired by the phenomenon in the landscape, the authors propose to use natural gradient fine-tuning only to the last classifier layer for defense and also introduce clean data distribution-aware regularizer to keep the natural accuracy. Empirical results demonstrate the effectiveness of the proposed method.\n",
            "strength_and_weaknesses": "**Strength**\n1. The paper demonstrates a side effect of backdoor attacks. When calculated only on benign data, the loss landscape of the minimum point becomes sharper. This might inspire further studies.\n2. The proposed method focuses on fine-tuning of last linear layer, which influences fewer parameters and consumes less computation. The empirical results also show notable improvements compared to previous SOTA methods, which verifies the effectiveness of the proposed method.\n3. Overall, the paper is easy to follow.\n\n**Weakness**\n1. What does $p$ in $\\mathcal{L}_p$ mean? Considering the author only provides an approximation of $\\mathcal{L}_p$, \nwhat is the exact formulation?\n\n2. In my opinion,  fine-tuning only to the linear layer has very limited ability, which makes me worried the performance. Although the authors show that the triggered data (images with trigger pattern) are not classified as the target class after defense (i.e., low ASR),  I am curious whether these triggered test data are classified into their ground-truth label. \n\n3. I think the success of the proposed method relies on the separation between (1) the cluster of benign data from the target class and (2) the cluster of poisoned data. Since they are separated, we can use a linear classifier to distinguish them.\nHowever, it might be easy to propose an adaptive attack, i.e., If the adversary is aware of the presence of the proposed defense, he can conduct an adaptive attack during backdoor attacks to escape. Specifically, he can introduce an extra linear discriminator on the top of the backbone in parallel with the classifier. The discriminator tries to predict whether the sample from the target class is benign or poisoned, while the classifier still predicts which class the sample belongs to. During training, he maximizes the discrimination loss while minimizing the classification loss. \nAfter training, since the discriminator cannot predict whether the sample is benign or poisoned anymore, the cluster of benign data from the target class is heavily mixed with the cluster of poisoned data. At this time, does the proposed method still works?\n\n4. Why does SAM fails in defense? The proposed method is inspired by the sharpness of the landscape caused by the additive poisoned data during training, while SAM is the most natural choice to recover the flatness. If SAM fails, does the success of the proposed method come from something else instead of the flatness?\n\n5. The natural gradient descent (NGD) contributes the most improvements, and the proposed regularization only introduces little improvements in natural accuracy (see Table 5). Considering NGD has been widely applied, the technical novelty in this paper is limited.\n\nMinor:\n- The $\\theta$ should be $\\theta = [ W_{0, 1}, W_{1,2}, W_{L-1, L} ]$ above Equation (3), i.e., starting from $0$. Otherwise, there are only L-1 layers in total.\n\n- In Equation (1) and Equation (3), the comma should follow the formula in the same line, rather than appearing in the front of the following sentence.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Why does the natural choice, SAM, fails in defense, during the natural gradient descent success? This makes me still concerned about the correctness of the motivation (See weakness 4). Is the success of the proposed method genuinely related to the sharp loss landscape caused by poisoned data?",
            "summary_of_the_review": "This paper introduces an interesting side effect caused by poisoned data and proposes a simple but promising method to defend against backdoors attacks. However, there are still some concerns about adaptive attacks and the correctness of the motivation. If the authors could address my concern, I will reconsider my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2286/Reviewer_1UnP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2286/Reviewer_1UnP"
        ]
    },
    {
        "id": "8QX7hq5VBd",
        "original": null,
        "number": 2,
        "cdate": 1666648048432,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648048432,
        "tmdate": 1669943626034,
        "tddate": null,
        "forum": "xWFguIF_hG",
        "replyto": "xWFguIF_hG",
        "invitation": "ICLR.cc/2023/Conference/Paper2286/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new method to recover the model after an attack by finetuning only one layer. Moreover, it has assumed that the backdoor model tends to be trapped in the local minimum. Therefore, a new purification technique named NGF is invented based on the loss surface curvature matrix, i.e., Fisher Information Matrix. The proposed method is evaluated on multiple benchmarks across 11 backdoor attacks with promising results in ASR.",
            "strength_and_weaknesses": "Pros:\n+ It is interesting to relate the model attack with optimization. The basic assumption of trapping in local minimum is convincing. \n\n+ The proposed NGF is inspired by the Fisher Information Matrix, which has solid theoretical foundations to support it.\n\n+ The entire manuscript is well written, with clear logic to follow.\n\n+ There are solid and comprehensive experimental analyses throughout the paper. The proposed method has achieved competitive results in ASR over many benchmarks against multiple backdoor attacks.\n\n+ The proposed method only needs one-layer finetuning which is efficient.\n\nCons:\n- The motivation of NGF does not fully convince me. The authors are suggested to explain more of geometric awareness in Fisher Information Matrix and why it is helpful for smoother optimization. Is NGF applicable for general optimization where the smoother surface is supposed to be helpful for many tasks?\n\n- The experimental results on ACC seem not good. Is that true? What is the reason for this, and how to solve it?\n\n- The baseline methods are not very recent (ANP in 2021). Since this is for ICLR 2023, is it any baseline in 2022?\n\n\nQuestions/Other Comments:\n\n1. Please consider changing the color of the references. The black references are hard to distinguish, and they may interrupt reading.\n\n2. Could authors explain more about ASR and ACC? How to compute ASR and ACC?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality, clarity and originality are around average.",
            "summary_of_the_review": "Based on the comments above, this submission is around the borderline. I will consider changing the score after discussion.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2286/Reviewer_6Zjq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2286/Reviewer_6Zjq"
        ]
    },
    {
        "id": "RrnNPYr9s6e",
        "original": null,
        "number": 3,
        "cdate": 1666753873091,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666753873091,
        "tmdate": 1669768693296,
        "tddate": null,
        "forum": "xWFguIF_hG",
        "replyto": "xWFguIF_hG",
        "invitation": "ICLR.cc/2023/Conference/Paper2286/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed NGF, which uses clean data to remove the backdoor from a model.  The proposed method works by fine-tuning the last layer of the model such that the false local minima are smoothed out and the effect of removing the backdoor from the model is achieved. Finally, this paper gives very extensive and comprehensive experiments to demonstrate the performance of their proposed model\n",
            "strength_and_weaknesses": "Strength:\n1. The motivation of this paper is clear, and the experiments are effective in proving the proposition of removing Backdoor from the model by smoothing the loss surface of the model.\n2. This paper demonstrates the effectiveness of the proposed approach through extensive and comprehensive experiments. The experimental results under various Backdoor Attack methods and models are included, and the effectiveness and superiority of the proposed approach is demonstrated by comparing various mainstream defense methods.\n \nWeaknesses:\n1. Not much intuition and discussion on how the proposed method will help eliminate the non-smoothness and why it is better than other solutions. \n2. lack comparison with new defense baselines \n3. technical contribution is not much aside from a regularization term in loss and the use of NGD\n",
            "clarity,_quality,_novelty_and_reproducibility": "The implementation is well supported by experiments.  \n\nThere are some typos in the text, such as \"Eigenvlaue\" should be \"Eigenvalue\" in Figure 1, the last \")\" in eqn(3) is missing, and so on.  ",
            "summary_of_the_review": "1. There lacks sufficient intuition on why smoothness is the key to remove the backdoor.  Essentially smoothness measures the changes in gradients wrt to changes in inputs. Then the authors\u2019 conjecture is that triggers in input could lead to large changes in gradients. But why it must be a backdoor? It seems to me that the lipshitz pruning method (CLP, Zheng et al. 2022) is telling a more convincing story that if triggers in input could lead to large changes in loss, it might be backdoored.\n \n\n2. For runtime comparison, the author should compare with other baselines as well as NGF without regularization term. Also for ablation study, I would like to see the results for fine-tuning on all layers. \n\n3. The proposed method requires to have clean training data in order to function properly. The authors did not test the situation when the number of clean training data is not sufficient. To be comprehensive, I would suggest the authors to include the number of training samples as an ablation study.\n\n4. The performance reported for data-free backdoor removal by Zheng et al. 2022 seems a bit strange as the original paper actually gives quite a good performance on Blend, TrojanNet. Did the author tune the hyperparameters?\n\n5. There is also some recent work on backdoor removal:\n\n    \"One-shot Neural Backdoor Erasing via Adversarial Weight Masking.\" NeurIPS 2022.\n\n    The authors may want to comment on/compare with the above work.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2286/Reviewer_uefD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2286/Reviewer_uefD"
        ]
    }
]