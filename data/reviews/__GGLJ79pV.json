[
    {
        "id": "nzFX_TB6g8u",
        "original": null,
        "number": 1,
        "cdate": 1666241875960,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666241875960,
        "tmdate": 1666333890297,
        "tddate": null,
        "forum": "__GGLJ79pV",
        "replyto": "__GGLJ79pV",
        "invitation": "ICLR.cc/2023/Conference/Paper3872/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a privacy-preserving federated learning protocol. Unlike prior works, it avoids running the training in a privacy-preserving manner but instead uses local training on sample-answer pairs that have been obtained using private inference.\n",
            "strength_and_weaknesses": "This is an interesting idea, but not enough attention is given to privacy issues regarding the dataset used for local training (line 13 in Algorithm 1). If this dataset is useful to the model it intuitively must leak something about the other parties' datasets, and if it doesn't, how can it improve the model? In light of that, I don't think it's entirely fair to compare to prior work where the training happens completely in private.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I don't understand the notion of the auxiliary querying dataset. Where does it come from? I can't be entirely randomly generated, as that wouldn't make any sense in many of the considered applications (say MNIST, ImageNet). So it must be somewhat real data, which comes with privacy implications.\n\nThe caption of Table 3 should make it clear that the times are for inference (I assume).\n\nI think it's not fair to highlight that this work outperforms the state of the art by two orders of magnitude. Cheetah improves more than ten-fold over CryptFlow2 in the same security setting, so the latter is clearly not the state of the art. Furthermore, the setting of two parties plus an auxiliary party is more comparable to the three-party setting of CryptGPU than the pure two-party setting of CryptFlow2 and Cheetah, so only CryptGPU can be considered an appropriate baseline. Furthermore, CryptGPU itself has been outperformed by the work of Keller and Sun (ICML'22) at least when it comes to training.\n",
            "summary_of_the_review": "Interesting idea with too many open questions in the execution\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3872/Reviewer_q3RN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3872/Reviewer_q3RN"
        ]
    },
    {
        "id": "Vm3ZrB2-fYD",
        "original": null,
        "number": 2,
        "cdate": 1666669988144,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669988144,
        "tmdate": 1666671466285,
        "tddate": null,
        "forum": "__GGLJ79pV",
        "replyto": "__GGLJ79pV",
        "invitation": "ICLR.cc/2023/Conference/Paper3872/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a practical framework for heterogeneous federated learning (HFL). Existing HFL frameworks suffer from privacy leakage through query data communication. Compared to prior work in HFL or adaptation with privacy-preserving feature, the proposed framework 1) is lightweight and efficient, 2) provides formal privacy guarantees. The key idea is to use several cryptographic primitives to perform secure query-data sharing, secure model prediction and secure result aggregation. This paper demonstrates the efficiency through extensive experiments. ",
            "strength_and_weaknesses": "Strengths: 1. Compared to prior works with no privacy protection or adaptation of existing frameworks, this proposed framework is efficient and customized. \nWeaknesses: 1. Secure model predictions requires customized design for particular model architectures. I am wondering how to deal with some privacy-sensitive layers, like for example, batch normalization.\n2. It would be better if this paper could provide some attacks from an adversary perspective to demonstrate privacy.",
            "clarity,_quality,_novelty_and_reproducibility": "I think this paper is well-written. I am not an expert on this topic so I could not provide a fair judgment on the novelty. ",
            "summary_of_the_review": "I think this paper provides an efficient and privacy-preserving framework. The effectiveness/ efficiency is demonstrated through extensive experiments",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3872/Reviewer_Gysq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3872/Reviewer_Gysq"
        ]
    },
    {
        "id": "mTMUKwnm7YF",
        "original": null,
        "number": 3,
        "cdate": 1666919236581,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666919236581,
        "tmdate": 1666919236581,
        "tddate": null,
        "forum": "__GGLJ79pV",
        "replyto": "__GGLJ79pV",
        "invitation": "ICLR.cc/2023/Conference/Paper3872/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a secure querying scheme for heterogeneous federated learning (HFL). HFL is a setting where clients in collaborative learning will train a model with different model architectures. Hence the global model cannot be directly aggregated/averaged from local client models. The proposed GuardHFL can query models on other clients to get predictions of their private data without publicly sharing the query data by multi-party encryption between Query Client, Answer Client and the Server. Experiments on r SVHN, CIFAR10, and Tiny ImageNet show the efficiency of GuardHFL. \n",
            "strength_and_weaknesses": "\nImproving the privacy protection techniques in FL when clients have different model architectures is an important and timely topic. \n\nThat being said, the motivation of the proposed method for federated learning does not seem to be super strong. The local training, querying and retraining approach is taken as granted. However, it is not clear to me that this should be the default, or even a popular algorithm for so-called heterogeneous FL. The authors cited (Li & Wang, 2019; Zhu et al., 2021), but the approaches in the two papers seem to be different from Figure 1 (without considering the encryption). The experiments compared with other encryption methods in collaborative learning. I would encourage the authors to think about ways to justify the training paradigm, and make stronger connections between the training paradigm and the proposed method. \n \nI would also encourage the authors to discuss more about the application setting, and limitation of the proposed method.GuardHFL assumes the communication between clients and server are stable with possibly multiple communication targeted specific clients (query and answer), which seems to be only applicable to the  cross-silo FL, not cross-device FL. Algorithm 1 line 4-14 also seems to suggest a large communication cost. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. I would encourage the authors to release their code. \n",
            "summary_of_the_review": "I unfortunately lack the expertise to evaluate the encryption method itself. The HFL motivation is interesting, but can benefit from more justification. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3872/Reviewer_VwDD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3872/Reviewer_VwDD"
        ]
    }
]