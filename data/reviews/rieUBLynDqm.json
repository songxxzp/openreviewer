[
    {
        "id": "h-wKGdzuzCl",
        "original": null,
        "number": 1,
        "cdate": 1666001870775,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666001870775,
        "tmdate": 1669131974266,
        "tddate": null,
        "forum": "rieUBLynDqm",
        "replyto": "rieUBLynDqm",
        "invitation": "ICLR.cc/2023/Conference/Paper5875/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper uses image translation models to generate diverse biased views of images to debias models. For this, the paper observes that generative models find it easier to latch onto biases instead of the signal, which is consistent with the observation made by previous works for classification models. As such, when asked to translate, they change the bias features, while not altering the core features, thus providing views with different biases.",
            "strength_and_weaknesses": "**Strengths (S):**\n\n[S1] The approach is neat and sensible. It converts the weakness of DNNs (that image translation models latch onto biases) to a strength (use that to generate diverse biases).\n\n[S2] The results show benefits over existing methods on the datasets tested. However, I have some concerns about the choice of datasets (see weaknesses section).\n\n[S3] The method functions well despite not having bias-free samples in the training set, which is an important result.\n\n**Weaknesses (W):**\n\nI think the experiments leave many open questions about the true generalization abilities of the system:\n\n[W1] The paper claims that it doesn\u2019t require presumptions about the data, but it assumes that the biases are easier to exploit than the signal. What if they are of similar complexities to exploit/learn? Wouldn\u2019t the generative model then alter the core features too and not just the bias?\n\n[W2] ColoredMNIST consists of single source of bias. But how would this method handle multiple types of biases e.g., BiasedMNIST [1]? Does the image translation model uncover all the spurious factors regardless of their \u2018ease of exploitation\u2019?\n\n[W3] Waterbirds, BAR, ImageNet-9 all test against foreground-background spurious correlations. Apart from completeness, do they add additional value? \n\n[W4] It is unclear how the method would tackle real world distributional shifts. Examples,  datasets from WILDs benchmark [2] or the different renditions of ImageNet (https://github.com/hendrycks/imagenet-r) could be used, which are not tested here.\n\n[W5] The original submission does not include references or the appendix which is referenced in the main text.\n\n[1] Shrestha, Robik, Kushal Kafle, and Christopher Kanan. \"OccamNets: Mitigating Dataset Bias by Favoring Simpler Hypotheses.\" arXiv preprint arXiv:2204.02426 (2022).\n\n[2]  Koh, Pang Wei, et al. \"Wilds: A benchmark of in-the-wild distribution shifts.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Hendrycks, Dan, et al. \"The many faces of robustness: A critical analysis of out-of-distribution generalization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\nThe paper is clearly written except for some sections e.g., Sec 5.3 (Ablation Study) which needs to be revised heavily due to usage of terms such as \n\n\u2018there is remarkable improvement\u2019/ \u2018obvious improvement\u2019 \u2013 better just provide a range of gains and remove terms such as remarkable/obvious\n\n\u2018bias-transform generator G is not always a perfect model\u2019? \u2192 what is a \u2018perfect G\u2019? Did you mean to say, it may alter signal/core features too?\n\n**Awkward writing:**\n\u2018... address the bias problem not a single component \u2026 \u2018\n\nThe subsection needs to be re-written, but this poorly written paragraph did not affect my score.\n\n**Novelty:** As noted in the paper, the observation that DNNs find it easier to latch onto biases compared to the signal  (at least for the types of datasets studied in this line of work) is a known observation. However, the application to image translation models is novel.\n\nReproducibility: It lacks supplementary materials (appendix/code), so reproducibility is questionable.\n",
            "summary_of_the_review": "I think the paper cleverly exploits the bias-prone nature of generative models to debias methods, which is an important contribution. \nWhile I am leaning toward acceptance, I cannot see this as a clear accept mainly due to the experimental limitations.\nI would also need confirmation if the authors have fixed Sec 5.3, and included references and the appendix.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5875/Reviewer_Xnts"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5875/Reviewer_Xnts"
        ]
    },
    {
        "id": "kUcUX3mqJP",
        "original": null,
        "number": 2,
        "cdate": 1666676885663,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676885663,
        "tmdate": 1666676885663,
        "tddate": null,
        "forum": "rieUBLynDqm",
        "replyto": "rieUBLynDqm",
        "invitation": "ICLR.cc/2023/Conference/Paper5875/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a contrastive learning-based debiasing framework to train a unbias classifier under the assumption that there are NO unbias samples existing in the training data. To achieve this goal, the authors utilize a conditional StarGAN approach to ``adjust'' the images and augment the samples. The experiment demonstrates that the method can outperform the existing approaches when combing with LfF debiasing framework.",
            "strength_and_weaknesses": "Strength\n1. the paper discusses an interesting debiasing case: when there is no unbias data among the data, how can we train an unbias classifier\n2. the experiment is very solid\n\nWeakness\n1. Detailed experiment setting is not described. For example, how to combine the proposed work with LfF?\n2. Missing reference.\n3. Missing detailed information about StarGAN, for example, how to train StarGAN on the bias data? What is the structure of StarGAN?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Here are several comments:\n\n1. Where is the reference? It seems the article missing the reference.\n2. How you trained StarGAN? Did you train it on Color-MINIST data?\n3. How to combine the proposed framework CDvG with LfF? Is the training embedding of CDvG used for LfF?\n4. Lacking explanation about StarGAN. Please include a detailed explanation of StarGAN in the supplemental materials.\n\n",
            "summary_of_the_review": "Overall, the paper is well-written and the experiment is solid. But authors should address the comments I mentioned above.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5875/Reviewer_SxSC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5875/Reviewer_SxSC"
        ]
    },
    {
        "id": "qNmbxLdqRj",
        "original": null,
        "number": 3,
        "cdate": 1666677387250,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677387250,
        "tmdate": 1666677387250,
        "tddate": null,
        "forum": "rieUBLynDqm",
        "replyto": "rieUBLynDqm",
        "invitation": "ICLR.cc/2023/Conference/Paper5875/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses image dataset bias by using GANs and contrastive learning techniques. By training an image-to-image translation model (StarGAN) on a biased dataset, the model learns the implicit biases on that dataset. This is used by the debiasing model to generate new samples with different types of biases. Then, a debiased classifier can be trained using contrastive learning on {original (and biased) sample} - {generated sample} pairs. The model is evaluated on 5 different image datasets and ablation studies are reported.",
            "strength_and_weaknesses": "Strength:\n1. The idea of the paper is simple and can be potentially applied to any dataset. Even if the biases are known, the GAN will supposedly uncover them, so no annotations on biased samples are required.\n2. Experiments are conducted on very diverse datasets, including synthetic and real-world images. \n3. It is very interesting that the proposed method outperforms previous methods by a large margin when no unbiased samples are available.\n\nWeaknesses\n1. Paper clarity should be improved. Details:\n- The paper is not very clear about the proposed method. I did not understand what the proposed method was until page 6 (Section 4.2). The paper would benefit from making the proposed model clearer from the beginning.\n- It is not clear to me if the image-to-image augmentations are also applied at test time.\n\n2. Looks like the method needs the existence of a very strong bias to be useful. The following points need clarification:\n- Why Table 1 only has results for low ratios of unbiased samples? Specially BFFHQ dataset goes up only to 0.5%, while results on other datasets are reported to be up to 5%.\n- It should be clarified whether the method works if the dataset bias is not very strong. What if only 50% of the samples are biased? It may be that StarGAN cannot capture this bias well, making the whole method fail. The paper should be clear and transparent about this point.\n- In Table 1, except when the ratio is very small, some results do not seem to be statistically significant.\n\n3. Given that this method needs to train a StarGAN model and generate various images per sample in the dataset, computation cost and time should be reported and compared against previous work.\n\n4. Other details:\n- Section 5.1 says the model is evaluated on BAR dataset, but results are not reported.\n- Why is the Waterbird dataset the only dataset not reported over 3 runs?\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper should be improved in terms of clarity. The details about the proposed method do not appear until page 6.\n- Quality, novelty, and reproducibility look reasonable. The method is simple, and it may be able to be reproduced with the details provided in the paper.",
            "summary_of_the_review": "The proposed method is simple and targets an important task. However, there are some concerns with respect to the limitations that should be clarified during the rebuttal. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5875/Reviewer_F8oL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5875/Reviewer_F8oL"
        ]
    },
    {
        "id": "fuapIVp-oZO",
        "original": null,
        "number": 4,
        "cdate": 1666874076428,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666874076428,
        "tmdate": 1666874076428,
        "tddate": null,
        "forum": "rieUBLynDqm",
        "replyto": "rieUBLynDqm",
        "invitation": "ICLR.cc/2023/Conference/Paper5875/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper tackles the problem of a classifier being biased to nuisance factors (shortcuts). The proposed method uses image-to-image translation to synthesize different biases in the dataset to remove the bias without collecting bias-free dataset.",
            "strength_and_weaknesses": "Strengths\n\n(+) The translation model does not require bias supervision for training.\n\n(+) Using translated images with bias for reducing bias of the dataset is plausible.\n\n(+) Experiments cover many datasets over different classes (numbers, faces, objects, birds), modalities (images and videos) and biases (synthetic, real).\n\n(+) The proposed method can be used in any setting.\n\nWeaknesses\n\n(-) The bias of a discriminative model is not guaranteed to be in the same bias of translation model.\n\n(-) Outdated translation model (stargan 2017). Biased translation may not exist in recent translation models: stargan v2 2020, style-aware \ndiscriminator 2022. Is stargan 2017 chosen among other alternatives to produce biases? Please discuss the trade-off between existence \nof bias and image quality.\n\nEtc.\n\nHow does it work when there is no bias in the dataset?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\n(-) Bird-eye view of the proposed framework is missing. Please refer to my summary.\n\n(-) The sentences should be more compact.\n\n(-) The sentences have many redundant descriptions. E.g., \"auxiliary information which is rarely obtainable in practice\" and \"such presumptions about the availability of the auxiliary information or bias-free samples are not always guaranteed\"\n\n(-) Sentence flow can be improved. Especially in 3.2 before 3.2.1.\n\n(-) Inconsistent statements:\n- \"We find that image-to-image translation models favor learning malignant biases over task-relevant signals.\"\n- \"We experimentally observe that certain image translation models are also prone to consider conspicuous but incidental information.\"\n\n(-) It is difficult to guess the expected result in each column in Figure 3. I can see the 2nd and  3rd columns should be female and male. I cannot tell on other datasets.\n\n(-) What do checks and crosses mean in Table 1?\n\nNovelty / Originality\n\nThe logical flow is just the same as \"Imagenet-pretrained cnns are biased toward texture\". Instead of texture, this paper is tackling the biases that can be captured as the domain specific attributes by image-to-image translator.\n\nQuality\n\nThis paper is technically sound except that the translators are not always biased.\n",
            "summary_of_the_review": "This paper builds up upon somewhat true (true in some cases) intuition (bias in translation) and provides reasonable de-biasing technique. However, the presentation and the logical flow should be improved. Furthermore, the idea is incremental in that this paper works with image-to-image translation instead of style transfer in \"imagenet-pretrained cnns are biased toward texture\".",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5875/Reviewer_Fqi9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5875/Reviewer_Fqi9"
        ]
    }
]