[
    {
        "id": "3dKHCmnvsV",
        "original": null,
        "number": 1,
        "cdate": 1666176885239,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666176885239,
        "tmdate": 1666176885239,
        "tddate": null,
        "forum": "aoDyX6vSqsd",
        "replyto": "aoDyX6vSqsd",
        "invitation": "ICLR.cc/2023/Conference/Paper4180/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors consider the task of improving the scalability of the popular linearised Laplace method for which they develop a sampling-based EM approach. The value of the proposed approach is then demonstrated in several experimental settings.\n",
            "strength_and_weaknesses": "The authors tackle an important task as the linearised Laplace is a popular approach, especially in the Bayesian Deep Learning literature. The experimental evaluation is extensive and considers both small/large scale classification tasks as well as an image reconstruction task. A minor weakness is the lack of downstream evaluation of the quality provided by the method's predictive uncertainty. However, that can be justified by the necessary limit of the breadth of the work given the page constraints.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in a clear and comprehensible manner. The proposed approach combines several known approaches into a joint new one and provides a novel solution to the task. The discussion in the appendix together with the provided code seems to provide full reproducibility.\n",
            "summary_of_the_review": "The paper sets out to solve a well-defined task and delivers both concerning the theoretical as well as empirical side.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4180/Reviewer_ZeYJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4180/Reviewer_ZeYJ"
        ]
    },
    {
        "id": "Es8I17wzZ4S",
        "original": null,
        "number": 2,
        "cdate": 1666600792647,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600792647,
        "tmdate": 1666600792647,
        "tddate": null,
        "forum": "aoDyX6vSqsd",
        "replyto": "aoDyX6vSqsd",
        "invitation": "ICLR.cc/2023/Conference/Paper4180/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a scalable linearized Laplace procedure which can be applied to realistic BNN models.  The algorithm can be viewed as an extension of the sample-then-optimize procedure that allows for normalization layers, and hyperparameter selection is also implemented a an E-step.  The authors evaluated the method on ResNet-18 and U-Net models, demonstrating improvements on predictive performance and quality of uncertainty estimates.",
            "strength_and_weaknesses": "The authors proposed a scalable approximation to the Laplace method, which is always of interest.  The proposed method is easy to implement, and demonstrates competitive performance.  The authors provided new connections to the sample-then-optimize formulation.\n\nAs for limitations, it is easy to nitpick (e.g., the limitation of Laplace method, the conjugate Gaussian model, or the linearized approximation), but I don't think these are important concerns that must be addressed in a single paper, or a single thread of literature, and the applicable scope of the proposed method is quite clear.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written; the algorithm appears easy to implement; the modifications are novel to my knowledge.",
            "summary_of_the_review": "Nice work with methodological and empirical contributions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4180/Reviewer_ejzP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4180/Reviewer_ejzP"
        ]
    },
    {
        "id": "TI4m9Oez63",
        "original": null,
        "number": 3,
        "cdate": 1666739182957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666739182957,
        "tmdate": 1668553653663,
        "tddate": null,
        "forum": "aoDyX6vSqsd",
        "replyto": "aoDyX6vSqsd",
        "invitation": "ICLR.cc/2023/Conference/Paper4180/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a number of tricks to scale linearization + Laplace approximations to large neural networks with the goal of performing uncertainty quantification.  The standard approach is to train a neural network and then post-hoc linearize the network, and use a Laplace approximation to obtain that the posterior should be Gaussian.  In this posterior, a regularization term (corresponding to the prior) appears, and this term should be fit using maximum likelihood.  To do so requires some computationally expensive numerical matrix algebra, and so the present paper instead proposes some sampling approaches (which in turn can be turned back into optimization problems) to more efficiently fit the prior and obtain samples from the posterior.  The authors apply this approach to a number of networks and datasets, finding that it can obtain better uncertainty estimates faster than existing methods (particularly in the case of simultaneously estimating uncertainty at several points).",
            "strength_and_weaknesses": "Strengths:\n* The approach is interesting and combines a lot of disparate ideas to make this the linearization + Laplace approximation scalable.  Some of these ideas may be of (minor) independent interest.\n* The empirical runtime and uncertainty quantification results are impressive, and show a benefit to this approach for real networks.\n* The writing was usually clear (but see below), and the results were presented clearly.\n\nWeaknesses:\n* In a few places the presentation was hard to follow.  For example, the rightmost plot in Figure 3 shows different layer-wise precisions, but the exposition only discusses learning a single precision (controlled by $\\alpha$).  I can imagine how it is straightforward to extend to different layer-wise $\\alpha$s, but that should be clarified in the main text.  In Section 4 , $\\bar{w}$ is used without definition (presumably the optimized weights).  Then, $\\bar{\\theta}$ is used (e.g., in equation (15)) without definition.  Finally, the acronym MCDO is introduced _after_ references to Figure 6 and Table 2, where that acronym is used.  Similarly \"DIP\" is used in that Figure and Table, but never defined.\n* The reported metrics are all good, but one of the most interpretable measures of uncertainty quantification is simply coverage, and it would be nice to report empirical coverage at different posterior credible levels on some held out data.  \n* I found the results in Figure 5 difficult to interpret.  Namely, it is not obvious to me how any Bayesian uncertainty quantification should perform when some datapoints are corrupted.  Would the true posterior (perhaps with a learned prior) outperform approximate posteriors in terms of log likelihood on test data?  I'm not sure that that is true, and so I'm not sure what the relative ordering of the different uncertainty quantifications methods in Figure 5 says about their \"quality\".",
            "clarity,_quality,_novelty_and_reproducibility": "I am not an expert in neural network uncertainty quantification, so I cannot comment on the novelty of the proposed approach in that space.  Overall, the proposed approach feels like a collection of clever tricks, each of which may not be groundbreaking, collectively allow the linearization + Laplace approximation to scale to very large datasets and neural networks.  For the most part the writing is clear, aside from some notational issues described above.  The code is available on an anonymous github repo.  I did not run the code, but it looks well-commented and well-documented.",
            "summary_of_the_review": "Overall, I found the paper to be a collection of interesting tricks that scale an existing approach to larger neural networks and datasets.  The presentation was mostly clear, and the presented approach seems like it will be useful.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4180/Reviewer_MCAU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4180/Reviewer_MCAU"
        ]
    },
    {
        "id": "m7ZD61ofFh",
        "original": null,
        "number": 4,
        "cdate": 1666786846534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666786846534,
        "tmdate": 1666786846534,
        "tddate": null,
        "forum": "aoDyX6vSqsd",
        "replyto": "aoDyX6vSqsd",
        "invitation": "ICLR.cc/2023/Conference/Paper4180/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The computational cost associated with Bayesian linear models constrains this method\u2019s application to small networks, small output spaces and small datasets. Moreover, the method is sensitive to the choice of regularisation strength for the linear model. This paper overcomes these limitations by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hyperparameter selection. The experimental results demonstrate the contributions. ",
            "strength_and_weaknesses": "Strengths:\nThe paper is well-written.\nThe approach is easy to understand and presented well in the paper. \nThe authors applied their method to some tasks to demonstrate the validity of the proposed method. \nThe theoretical analysis is strong.\n\nWeaknesses:\n1. Novelty is somehow limited. The main contribution is that the authors simply apply EM algorithm to scale inference and hyperparameter selection in Gaussian linear regression. It would be more novel if the EM algorithm was improved. \n2. Experiments across multiple datasets and with larger models would make the results more convincing. \n3. The ResNet-18 is also a very small model. Why not try the method on larger models? Is it because of the difficulty to implement?\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. Novelty is somehow limited. The main contribution is that the authors simply apply EM algorithm to scale inference and hyperparameter selection in Gaussian linear regression. It would be more novel if the EM algorithm was improved. \n\n2. The paper is well-written.",
            "summary_of_the_review": "I am concern about the novelty of this paper. \n\nThe experiments are also suggested over larger models.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4180/Reviewer_d9Fu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4180/Reviewer_d9Fu"
        ]
    }
]