[
    {
        "id": "e0OMH4NKpYi",
        "original": null,
        "number": 1,
        "cdate": 1666611013692,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611013692,
        "tmdate": 1666611013692,
        "tddate": null,
        "forum": "dNdOnKy9YNs",
        "replyto": "dNdOnKy9YNs",
        "invitation": "ICLR.cc/2023/Conference/Paper3301/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces and studies a class of functions called seq2seq function with knowledge, whose form is inspired by typical seq2seq learning problems . This new class of functions explains why  network structures similar to self-attention are the right structures to represent the target function of many seq2seq problems. The paper introduce several concrete illustration to verify the hypothesis proposed in this paper. ",
            "strength_and_weaknesses": "Strength:\nThe paper reveals that one possible reason behind the success of self-attention based language model, is because self-attention is close to \u201cseq2seq function with knowledge\u201d,  a function class that is guaranteed to be orthogonal equivariant. The paper shows that functions with such equivariance enjoy a representation which takes a similar but more general form as self-attention. This idea is fresh and novel to me. I enjoy the clear illustration of the math examples supporting the authors main idea much.  \n\nWeakness:\nHowever, I guess the main weakness of the paper is the lack of empirical study. The paper seems does not provide any empirical evidence supporting the hypothesis present in the paper. Take for example, is the seq2seq function with knowledge with equivariance performs better than the conventional self-attention? Is it possible to verify the superiority of the class of function with equivariance empirically? I sincerely appreciate it if authors may kindly comment on the missing empirical study during rebuttal. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper delivers the hypothesis that the success of self-attention based language model, is owing to the fact that self-attention is close to \u201cseq2seq function with knowledge\u201d,  a function class that is guaranteed to be orthogonal equivariant. This idea is very interesting and novel to me. \n\nThe illustrations are also clear and convincing to me. ",
            "summary_of_the_review": "The idea present in the paper, i,e., the success of self-attention based language model, is because self-attention is close to \u201cseq2seq function with knowledge\u201d,  a function class that is guaranteed to be orthogonal equivariant is interesting and novel to me. \n\nHowever, I am concerned that there is no empirical evidence supporting the hypothesis proposed in the paper. I think, w.r.t. the content present in this paper, empirical evidence might be a valid justification supporting the hypothesis in the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3301/Reviewer_KVpw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3301/Reviewer_KVpw"
        ]
    },
    {
        "id": "AVQZT4D2aEJ",
        "original": null,
        "number": 2,
        "cdate": 1666688326203,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688326203,
        "tmdate": 1666688843548,
        "tddate": null,
        "forum": "dNdOnKy9YNs",
        "replyto": "dNdOnKy9YNs",
        "invitation": "ICLR.cc/2023/Conference/Paper3301/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "The paper studies the representations of seq2seq functions with certain symmetries, and show that such functions have forms similar to the self-attention. ",
            "strength_and_weaknesses": "The paper introduces an interesting idea that orthogonal equivariance in the embedding space is natural for seq2seq functions with knowledge, and under such quivariance the function must take the form close to the self-attention. It also gives a thorough derivation on the findings. I like this part.\n\nHowever, my main concern is that the paper is lack of empirical studies on the findings. For example, their theoretical study shows that there could be other design of self-attention, such as high-order matrix products or RBF kernels. It would be interesting to see some experimental results on these.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and is novel, giving a theoretical analysis on self-attention.",
            "summary_of_the_review": "This is a nice theoretical paper but lack of empirical study.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3301/Reviewer_AZo2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3301/Reviewer_AZo2"
        ]
    },
    {
        "id": "3QJucZCn5p",
        "original": null,
        "number": 3,
        "cdate": 1666792132905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666792132905,
        "tmdate": 1666792132905,
        "tddate": null,
        "forum": "dNdOnKy9YNs",
        "replyto": "dNdOnKy9YNs",
        "invitation": "ICLR.cc/2023/Conference/Paper3301/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper showed that structures similar to self-attention are natural to learn many sequence-to-sequence problems from the perspective of symmetry. It studied the representations of sequence-to-sequence functions with certain symmetries, and showed that such functions have forms similar to the self-attention. Hence, self-attention seems to be the natural structure to learn many seq2seq problems. \n",
            "strength_and_weaknesses": "Strength:\n\n+ Study the representations of sequence-to-sequence functions with certain symmetries, and show that such functions have forms similar to the self-attention. Hence, self-attention seems to be the natural structure to learn many seq2seq problems. \n\n+ Moreover, except the inner product based attention mechanism widely used nowadays, the paper revealed more possibilities that may be picked in the design of attention mechanisms, such as higher-order matrix products or the RBF kernels.\n\nWeaknesses:\n\n- The paper only provided pure theoretical analysis. Its real-world relevance should be justified through different Seq2Seq problems.\n\n- Many of the analysis and derivations seem to be not very straightfoward and loss connection to some extent. The paper provides the analysis of symmetry from orthogonal and permuation while there are lots of other possibility of symmetry.\n\n- The paper did not provide an in-depth discussions on the limitations.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is presented in a relatively mathematical dense manner.\n\nQuality & Novelty: The main novelty lies in the perspective of analyzing the success of self-attention from symmetry.\n\nReproducibility: The paper is a pure theoretical analysis paper.\n",
            "summary_of_the_review": "The paper showed that structures similar to self-attention are natural to learn many sequence-to-sequence problems from the perspective of symmetry. The main limitaitons lie in 1) theoretical justification; 2) experimental validations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3301/Reviewer_KdaA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3301/Reviewer_KdaA"
        ]
    }
]