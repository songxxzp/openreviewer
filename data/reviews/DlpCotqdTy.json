[
    {
        "id": "yixWEXnIxmM",
        "original": null,
        "number": 1,
        "cdate": 1666564944598,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666564944598,
        "tmdate": 1666564944598,
        "tddate": null,
        "forum": "DlpCotqdTy",
        "replyto": "DlpCotqdTy",
        "invitation": "ICLR.cc/2023/Conference/Paper3348/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides algorithms for estimating stability of OLS to dropping a small fraction of samples. The theoretical results focus on low-dimensional regime and give algorithms with run-time exponential in $d$. ",
            "strength_and_weaknesses": "The paper is clearly strong at a technical level. The results are sound and complete. The upper bound for exact estimation, though somewhat unsatisfying in terms of run-time, is complemented by the faster approximation algorithm and the lower bound. \n\nThe author(s) provide an extensive set of experiments, which is great for a work that is theoretical by nature. \n\nThat said, I can see that the paper can be strengthened in a few ways. \n\n1. Can any of these results by extended to regularized (ridge) regression? \n2. Is it possible to identify certain distributional assumptions so that the run-time lower bound can be bypassed? It would be great to demonstrate poly-time algorithms even though it has be to under further conditions. \n3. The prior work of Broderick et al., 2020 (https://arxiv.org/abs/2011.14999) considers a broader setting, where there's a general quantity of interest $\\phi : \\mathbb R^d \u2192 \\mathbb R$ (mapping from the $d$-dimensional parameter $\\beta$ to a real) and the algorithm is asked to check whether $\\phi$ is stable wrt the dataset. The setting studied in this work is strictly more restricted, since it's only concerned with a single coordinate.  I am wondering if the results can be extended further. ",
            "clarity,_quality,_novelty_and_reproducibility": "I appreciate the technical merits of the work.  The paper is well presented, and the algorithms are sufficiently explained at an intuitive level first in the main paper then formally described and analyzed in the appendix. The main results, to the best of my knowledge, are novel. ",
            "summary_of_the_review": "Given the strengths of the paper, I recommend accept. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3348/Reviewer_BXKi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3348/Reviewer_BXKi"
        ]
    },
    {
        "id": "wrAHBgkVOr",
        "original": null,
        "number": 2,
        "cdate": 1666647115762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647115762,
        "tmdate": 1666647115762,
        "tddate": null,
        "forum": "DlpCotqdTy",
        "replyto": "DlpCotqdTy",
        "invitation": "ICLR.cc/2023/Conference/Paper3348/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Given a matrix $X\\in\\mathbb{R}^{n \\times d}$ and vector $y \\in \\mathbb{R}^{n}$, we are interested in understanding the _stability_ of the least squares problem $\\min_{\\beta} \\|\\|X \\beta - y\\|\\|_2$. In the prior work before this paper, _stability_ is defined as the smallest number of rows of $X$ that need to be remove to make the (wlog) first entry of the least squares solution $\\beta$ flip sign. This is computationally hard, and so the prior work relies on heuristic or brute-force algorithms.\n\nThis paper considers a fractional notion of stability, where we are able to reweigh each row of $X \\beta - y$ by some $w_i \\in [0,1]$, where $w_i=0$ means entirely removing the row and $w_i=1$ means keeping the row, and ask what is the smallest sum of $(1-w_i)$ needed to make the first entry of the least squares solution flip sign. The paper focuses on the computational hardness of this problem, and shows three key results:\n1. Exactly computing fractional stability is possible in $n^{O(d^3)}$ time.\n1. Exactly computing fractional stability requires $n^{\\Omega(d)}$ time, by reduction to the Exponential Time Hypothesis.\n1. Under a \"mild anti-concentration\" assumption that few rows of $[X \\beta]$ are nearly linearly independent from other rows, a $\\tilde{O}(n + \\frac{d}{\\varepsilon^2})^{d + O(1)}$ time algorithm approximates fractional stability to additive error\n1. Under a stronger near-incoherence assumption, a $O((\\frac{\\sqrt d}{\\varepsilon})^d \\text{poly}(n))$ time algorithm approximates fractional stability to additive error\n\nCompelling experimental results show that these algorithms can be practical on low-dimensional datasets, giving provable stability in potentially tractable runtime.",
            "strength_and_weaknesses": "I like this paper. I recommend it be published.\n\nAt a high level, it takes an intractable problem (the prior work's notion discrete of stability), keeps the intuitive underlying structure, and extends it to a fractional problem that's relatively tractable.\n\nThe paper is decently well written, with the high level idea of the algorithms being broadly clear, but some parts of the proof sketches being hard to understand. The results form a decently satisfying narrative: a terrible runtime without any assumptions, a lower bound to boot, a mild assumption to improve the terrible runtime with some approximation error, and a stronger assumption to get a potentially tractable runtime with some approximation error.\n\nThe results seem significant. The lower bound from the exponential time hypothesis makes it clear that this problem is hard, so fighting to get a runtime that's just polynomial in $n$ makes sense. Further, since verifying the stability of models is high-stakes in high-stakes applications, we really don't want to rely on heuristic algorithms. Provably correct algorithms are important. So, while the fastest runtime presented is pretty horrendous for even small $d$, it's nevertheless a good win.\n\nThe results certainly have weaknesses though, though I'm inclined to be lenient towards these weaknesses given the hardness of the problem. The key weaknesses are:\n1. The exact algorithm runs in $n^{O(d^3)}$, the lower bound for exact algorithms is $n^{\\Omega(d)}$, and the approximate algorithm with mild-anticoncentration assumption runs in $\\approx n^{O(d)}$ time. In order to just match the lower bound, the paper both relaxes the exactness of the answer and assumes structure on the input.\n\n    $\\phantom{.}$\n\n    It's not clear _where there's looseness_. Can an exact algorithm run faster, at $n^{O(d)}$? Does mild-anticoncentration suffice to beat the $n^{\\Omega(d)}$ lower bound, just using a different algorithm? Does approximation error allow us to beat the $n^{\\Omega(d)}$ lower bound? Is there a $n^{\\Omega(d)}$ lower bound for approximate algorithms given inputs that satisfy mild-anticoncentration? There's some gap here that I don't really understand, nor have any intuition for.\n\n    $\\phantom{.}$\n\n2. The proof sketches are a bit unclear. I can follow 90% of the intuition, but then some language about _demarcating regions_ or _connected component of varieties_ appears to resolve a conceptual technical block, and I don't really get how this work. I'm sure these are elaborated upon in the appendix, but that shouldn't be necessary. This hurts my confidence of correctness of the algorithms.\n\n    $\\phantom{.}$\n\n3. The final approximation algorithm, with it's stronger assumption on the input, is similar to the _incoherence_ assumption made in the matrix completion literature, which intuitively says that sampling rows from a matrix uniformly-at-random suffices to preserve $\\ell_2$ norm information (see [here for a formal paper](https://arxiv.org/pdf/1408.5099.pdf) and [here for a more approachable writeup](https://randnla.github.io/leverage-subspace-embedding/#uniform-sampling)). It's certainly weaker than a real incoherence assumption, since an $\\varepsilon$-fraction of rows are allowed to violate incoherence, but we can still contrast the assumptions.\n\n    $\\phantom{.}$\n\n    For an incoherent matrix, any uniform subsampling of $O(d \\log d)$ rows preserves the least squares solution with high probability. So, we should intuitively find that such matrices have to be stable. Admittedly, this notion of \"preserving\" a solution is not obviously related to stability, so my intuition could be really betraying me here. At the same time though, it remains unclear how much of the weight of this result is owed to the algorithm, and how much is owed to the incoherence-like assumption. Either way, it's a nice result to have out there, but I think there's a nice discussion to be had about how this differs from incoherence.\n\n4. Both assumptions on the input data, the mild-anticoncentration and the near-incoherence, read sorta like assumptions on stability. It's not clear if we should be assuming some kind of stability to \n\n\n---\n### Appendix: Formalizing the Incoherence Relationship\nFor the authors, I formalize what I see at the relationship between incoherence and the assumption used for the last algorithm of their paper. First the _leverage score_ of row $i$ of a matrix $\\bar{X}$ is\n$$\n\\tau_i[\\bar{X}] = \\max_{\\beta} \\frac{([\\bar{X}\\beta]_i)^2}{\\|\\|\\bar{X}\\beta\\|\\|_2^2}\n$$\nThen $\\tau^* := \\max_i \\tau_i$ is the incoherence of the matrix. In other words,\n$$\n\\forall \\beta,\\ \\forall i\\in[n],\\ \\frac{([\\bar{X}\\beta]_i)^2}{\\|\\|\\bar{X}\\beta\\|\\|_2^2} = \\tau^*\n$$\nAssumption B in the paper, when $\\varepsilon \\rightarrow 0$, says that $\\tau^* = \\frac{\\delta^2}{n}$, so that (intuitively) uniform $\\tilde{O}(\\delta^2 \\log(d))$ rows is needed to preserve $\\ell_2$ norms (assuming we pick $\\delta$ to be as small as possible for $\\bar{X}$).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of good quality and novelty, but is not perfectly clear.\nBelow, I ask some questions about intuitions in the proof sketches, since I am not convinced of correctness when I don't understand proof sketches. Afterwards, I list some mild typos or writing suggestions.\n\n## Proof Sketch Questions\n\n1. [Page 4, first paragraph] Appendix F.1 seems to give polynomially small mild anti-concentration, not exponentially small mild anti-concentration.  What's the relationship between Proposition F.1 and $(\\varepsilon, e^{-\\Omega(n)})$?\n1. [Page 5, remark 3.1] Why do such $w'$ have to lie in a linear subspace of bounded codimension? How does this argument follow?\n1. [Page 6, second-to-last paragraph] I don't see where the hyperplanes come in from. I can assume that the residuals are bounded, but how does this impose some sort of linear geometry? How does $\\varepsilon$ appear in this picture?\n1. [Page 6, paragraph after equation 2] This is altogether okay, since it seems to just be appealing to the prior work, but is there a clearer intuition you can give for what parts of this geometry go into appealing to the prior work? The sentence \"By classical results...\" seems to carry a crux of the proof, but I don't get what geometry it's even using.\n\n## Minor suggested edits and typos\n\n1. I'd recommend changing the language a bit from \"finite-sample stability\" to \"fractional stability\" and \"integral stability\" to \"discrete stability\" or \"binary stability\". _Finite-sample_ doesn't sound fractional, and _integral_ can sound continuous since integrals are continuous sums. _Finite-sample_ is definitely worse than _integral_ though.\n1. Instead of the $\\star$ operator, consider the standard notation for the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)), or using a diagonal matrix to contain $w$, since that's pretty common in the weighted least square (and broader linear algebra) community.\n1. WLS is a standard acronym for weighted least squares, so I'd recommend always saying either WLS or OLS, where the former has a weight vector and the latter does not have a weight vector.\n1. [Page 3, paragraph after theorem 1.3, sentence that starts with \"In practice\"] Say \"model\" or \"dataset\" instead of \"conclusion\"\n1. [Page 8, Isotropic Gaussian Data paragraph] Say \"this is possible\" instead of \"this is the case\".\n1. [Page 8, last paragraph] \"Just 27% of the data\" -- isn't that pretty large? Why should we expect or especially want more than a quarter of the data to control just the sign of a single variable from OLS?\n1. [Pages 8,9; the plots] The labels of the axes should be __much__ larger, so they're easily legible. Also, figure 2(b) is missing error bars.\n1. [Page 9, All-features-pairs analysis] \"and never performs much worse\" -- does it ever perform any worse? It seems to lie entirely below the line $y=x$ on the plot. Would be good to clarify in the text.",
            "summary_of_the_review": "The setup and results are cool, but I don't completely understand the technical proof sketches for correctness. I understand a lot of it though, so I'm still confident enough to give the paper a pass.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3348/Reviewer_hNxp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3348/Reviewer_hNxp"
        ]
    },
    {
        "id": "GeeTdxgsPM",
        "original": null,
        "number": 3,
        "cdate": 1666657227758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657227758,
        "tmdate": 1666657476801,
        "tddate": null,
        "forum": "DlpCotqdTy",
        "replyto": "DlpCotqdTy",
        "invitation": "ICLR.cc/2023/Conference/Paper3348/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a fast algorithm to estimate a stability metric of ordinary least squares problems (OLS). The stability metric was introduced as\nthe minimum number of samples that need to be removed so that rerunning the analysis overturns the conclusion. Naive computation of the stability metric is computationally prohibitive, whereas the proposed algorithm can efficiently obtain an approximation in the low-dimensional regime.\n",
            "strength_and_weaknesses": "[Strengths]\n- Evaluating the stability of OLS could be useful for robust regression and model interpretation.\n- The paper presents polynomial-time algorithms for estimating a particular stability metric introduced in (Broderick et al., 2020). Theoretical guarantees are also provides for the approximation quality.\n- The authors illustrated the utility of the method on Boston Housing Data.\n\n[Weaknesses]\n- The proposed method seems to be limited to only linear models/OLS problems. Generalizations to non-linear models or more advanced models are not obvious.\n- The proposed method targets one particular stability metric (Broderick et al., 2020). This metric might be sensitive to noise in the data or resampling of the data. There are widely-studied candidate stability metrics such as the Cook's distance in regression analysis that could afford cheaper computations.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: the presentation of the paper is mostly clear. Question: what is the significance of zeroing out the first coordinate of the coefficient vector in the stability definition?\n- Quality: theoretical analysis is provided to justify the quality of the approach.\n- Novelty: the proposed approximation algorithm is novel; however, generalization of the method to other stability metrics are not straightforward.\n",
            "summary_of_the_review": "The authors propose an efficient approximation algorithm for computing the stability metric introduced in (Broderick et al., 2020). Theoretical analysis is provided to justify the robustness of the algorithm.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3348/Reviewer_LbgB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3348/Reviewer_LbgB"
        ]
    },
    {
        "id": "Tr9iAOSZUwz",
        "original": null,
        "number": 4,
        "cdate": 1666677344478,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677344478,
        "tmdate": 1666677344478,
        "tddate": null,
        "forum": "DlpCotqdTy",
        "replyto": "DlpCotqdTy",
        "invitation": "ICLR.cc/2023/Conference/Paper3348/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the stability of linear regression problem by defining it as the minimum samples (in $\\ell_1$-norm) to be removed to zero out the first coordinate. The paper provides a $O(n^{d^3})$ algorithm to exactly solve the problem, and also provide a lower bound stating that the computation complexity cannot be $O(n^{o(d)})$. Further, approximate algorithms are provided and experiments are carried out on real dataset to identify stability.",
            "strength_and_weaknesses": "Unlike the previous results using influence functions to approximate the local stability, the paper study the exact problem--which means it is able to identify perturbation of the solution beyond local regime. The theory and experiments are sound. However, even with the relaxed approximate algorithm, the computation cost seems still very big.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and provide good insight into studying stability of optimization problem.",
            "summary_of_the_review": "The paper is well written and organized. The problem is clearly formulated, and the authors tackle the exact stability problem instead of using local approximations from influence functions, which is novel and interesting to me. By exactly auditing linear regression, the authors provide theoretical guarantees for such identification and the algorithm works well on real datasets. Although the complexity of the algorithm seems to suffer from curse of dimensionality, but it is a good starting point for study stability problem in its exact form. I think this is a good paper and would like to accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3348/Reviewer_Hpqm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3348/Reviewer_Hpqm"
        ]
    }
]