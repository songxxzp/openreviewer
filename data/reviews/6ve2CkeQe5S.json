[
    {
        "id": "esOkB1HMQp",
        "original": null,
        "number": 1,
        "cdate": 1666344778962,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666344778962,
        "tmdate": 1668653088506,
        "tddate": null,
        "forum": "6ve2CkeQe5S",
        "replyto": "6ve2CkeQe5S",
        "invitation": "ICLR.cc/2023/Conference/Paper3161/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper propose a framework, named MEDFAIR, to benchmark the fairness of machine learning models for medical imaging. Through extensive experiments, the authors find that the under-studied issue of model selection criterion can have a significant impact on fairness outcomes. The recommendations are made for different medical application scenarios.",
            "strength_and_weaknesses": "The topic of this paper is significant and valuable. But there are many issues.\n\n1. Page 1 (bottom), \"label noise (e.g., Zhang et al. (2022) find that label noise in...\".\n2. Figure 1, why is this figure not mentioned in the text?\n3. Page 2 (bottom), \"(E.g., data collected at hospital A is used to train a model deployed at hospital B). \" I have never seen it written like this before.\n4. Page 2 (bottom) \"Bias widely exists in ERM models... \" and Page 3 (top) \"...the empirical risk minimization (ERM)\nwith statistical significance...\",  why does the latter ERM have the full name and not the preceding one?\n5. Page 3 (middle), \"...e.g., there are data of patients\u2019 age and sex. \", statements are not concise enough.\n6. Page 3 (bottom), \"Our framework can be easily extended to non-binary classification.\" There is no explanation in the full text or even in the supplementary material on how to extend to non-dichotomous classification.\n7. Page 3 (middle), \"We train models for each combination of algorithms \u00d7 datasets \u00d7 sensitive attributes.\" What role does the model selection in Section 3.3 play?\n8. Section 3.2 lacks the reasons for choosing these algorithms.\n\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing of this paper needs further improvement.\n\nQuality: High and easy to follow.\n\nNovelty: High.\n\nReproducibility: High.",
            "summary_of_the_review": "As an empirical evaluation paper, this paper has conducted a lot of experiments and obtained some valuable conclusions, but the writing needs to be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3161/Reviewer_sgSV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3161/Reviewer_sgSV"
        ]
    },
    {
        "id": "-9-mqd74eB",
        "original": null,
        "number": 2,
        "cdate": 1666569144469,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666569144469,
        "tmdate": 1666569144469,
        "tddate": null,
        "forum": "6ve2CkeQe5S",
        "replyto": "6ve2CkeQe5S",
        "invitation": "ICLR.cc/2023/Conference/Paper3161/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper performs an analysis of fairness training methods on multiple medical imaging tasks. They conclude that no method is significantly better than basic ERM training.\n",
            "strength_and_weaknesses": "The paper is well written and articulates existing methods of fairness very well. \n\nDetails about the experiments are not clear. Such as how the age was divided into subgroups and how many subgroups exist for each category are not mentioned. Knowing the number of subgroups where the min and the max performance are calculated would help to interpret the significance of the spread in Figure 3.\n\nThe main weakness is that there is not much of an actionable conclusion. The experiments contain runs of models on many datasets without a deep focus on any one of them, not focusing on the specifics of each one. The breakdown of biased variables is very limited to sex and age which doesn't really expose subtle issues. \n\nI find the paper spends more time discussing fairness concepts than studying it on medical data. The conclusions drawn from the experiments on medical data seem limited and not detailed enough to reflect deeply on the specifics of each domain.\n",
            "clarity,_quality,_novelty_and_reproducibility": "> All of the datasets we use are publicly available\n\nI am not aware that the CheXpert and MIMIC datasets have race publicly available. Was this information privately obtained? Can you add a breakdown of these subgroups in the paper?\n\nIn the discussion the authors mention sources of bias include \"subgroup size, imbalance in subgroup disease prevalence, difference in imaging protocols/time/location, spurious correlations\" but these are not included in the evaluation or explored in this work. \n\nThe experiments don't appear to have multiple trials so it is hard to draw conclusions from these experiments, even negative conclusions.\n",
            "summary_of_the_review": "The paper is well written and clearly discusses fairness but does not provide significant insight into the fairness issues of medical datasets. For this reason I do not recommend acceptance.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3161/Reviewer_q4yd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3161/Reviewer_q4yd"
        ]
    },
    {
        "id": "FWkXzskNZ36",
        "original": null,
        "number": 3,
        "cdate": 1666653110741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653110741,
        "tmdate": 1666653110741,
        "tddate": null,
        "forum": "6ve2CkeQe5S",
        "replyto": "6ve2CkeQe5S",
        "invitation": "ICLR.cc/2023/Conference/Paper3161/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors focus on benchmarking the fairness issue in the field of medical images, taking different algorithms, datasets and models into consideration. They first choose 9 different datasets, covering the different categories of medical images, and then implement 11 different algorithms that aim to mitigate the bias/fairness issue in machine learning systems, and finally conduct model selection for evaluation. The main contributions and conclusions are:\n1. By extensive experiments, they show that the bias issue widely exist in the area of medical images;\n2. Model selection introduces significant difference in the performance for different subgroups;\n3. Current bias mitigation algorithms fail to address the fairness issue in medical images",
            "strength_and_weaknesses": "The main strength of this paper:\n1. The problem setup is interesting and of significant importance. Fairness issue in medical image systems can be harmful when put into practical use;\n2. The experiments are extensive and well-designed. The authors choose different datasets and algorithms to finish the benchmarking, which makes it more reliable;\n3. The paper is well-written and easy to follow;\n4. The results are significant and the limitations are clarified. In the results shown in the paper, there does exist bias issue in various tasks related to medical images and cannot be ignored.\n\nDoes not have to be weakness, but some concerns are:\n1. For now this paper only considers one sensitive attribute at a time, I believe it's important to also understand the correlation within the sensitive attributes, since in most datasets that's also imbalanced. Therefore some future work may also include a multi-factor analysis, for example adding a regression;\n2. Maybe I am missing it -- what is the diagnose label used in the experiments reported in the main body? Were other label/tasks also considered?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of good quality and should be easy to reproduce, and is an original work.",
            "summary_of_the_review": "This paper provides a benchmarking of fairness in medical image systems, with thorough experiments and significant results, I recommend this paper to be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3161/Reviewer_zqn8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3161/Reviewer_zqn8"
        ]
    },
    {
        "id": "K_MCV-ehwP7",
        "original": null,
        "number": 4,
        "cdate": 1666706328245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666706328245,
        "tmdate": 1666706328245,
        "tddate": null,
        "forum": "6ve2CkeQe5S",
        "replyto": "6ve2CkeQe5S",
        "invitation": "ICLR.cc/2023/Conference/Paper3161/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces MEDFAIR, a framework to benchmark fairness in machine learning models for medical imaging. It provides a reproducible environment for developing and evaluating bias mitigation algorithms for deep learning applied to medical imaging. The authors use it to evaluate fairness in multiple scenarios and make recommendations for different medical cases.",
            "strength_and_weaknesses": "*Strengths\n- The background on fairness research was deeply evaluated, not focusing only on medical papers but also on other fairness work applied to computer vision, which were included in the proposed framework.\n- The proposed framework has the potential to standardize comparison among different debiasing strategies in medical imaging\n- Aside from the contribution of the framework, the experiments and discussion highlight several key points and challenges for fairness awareness. \n- The appendix provides an extensive and explained report of the experiments, metrics, hyperparameters, as well as instructions for adding new algorithms to the proposed framework.\n\n*Weaknesses\n- The proposed framework is still missing several fairness strategies such as the self-supervised ones, as well as datasets from other medical data modalities, and also does not include segmentation, which is an essential task in medical imaging.",
            "clarity,_quality,_novelty_and_reproducibility": "The experiments have been described appropriately. Code and instructions were provided for proper reproducibility.",
            "summary_of_the_review": "The framework contribution for fairness in medical imaging is relevant and an active concern in the field. Experiments are extensive, detailed, and possibly reproducible. I am overall quite happy with this contribution as it may help other researchers in the field, all code is available and it is possible to add new algorithms, metrics, and datasets into the framework.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3161/Reviewer_aeZu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3161/Reviewer_aeZu"
        ]
    }
]