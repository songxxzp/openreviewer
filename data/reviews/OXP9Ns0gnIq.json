[
    {
        "id": "Yn5IqJ2oBqM",
        "original": null,
        "number": 1,
        "cdate": 1666695712977,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695712977,
        "tmdate": 1669983594177,
        "tddate": null,
        "forum": "OXP9Ns0gnIq",
        "replyto": "OXP9Ns0gnIq",
        "invitation": "ICLR.cc/2023/Conference/Paper779/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a method to reparameterize and gaussianize the latent tensors for deep generative models such as StyleGAN2 and Glow. This idea was evaluated in a number of inverse problems such as compressive-sensing MRI, image deblurring, and eikonal tomography. \n\n",
            "strength_and_weaknesses": "Strength:\n- Unless this work clearly resolved the first issue in the weakness, it is hard to write down any strength since it is unclear if those strengths are belonging to this work or other works.\n\nWeakness:\n- This work looks very similar to the following prior work: C Meng, Y Song, J Song, S Ermon, Gaussianization Flows, AIStats 2020. The key idea of this work is to reparameterize latent vectors using learnable orthogonal matrices (Cayley parameterization) while the work of Meng et al. also used \"trainable orthogonal matrix layer.\" This is really critical in my assessment for this work and it is really unfortunate that this work missed this prior work. Thorough comparison with this prior work seems critical for this work to be justified as a new work.\n- It is unclear if all comparisons in this work were proper. For example, (Wulff & Torralba 2020) also investigated on Gaussianization in the latent space for StyleGANv2 and this work should be compared with the proposed method with StyleGANv2 in terms of performance and the level of Gaussianization. \n- It is unclear if the proposed methods worked as designed or not. All comparisons show that the proposed methods outperformed other selected methods, but there is no investigation on how much Gaussianization was achieved with the proposed methods. It will be helpful for many readers to see some concrete quantitative results on Gaussianization itself.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- It seems that MRI experiments are realistic or not. Many works ignore that MRI measurements are complex, not real, and this work also seems to make the same mistake. More realistic simulations for MRI including multi-coils (all modern MRI scanners are using multi-coils!) and complex Gaussian noise. \n- Glow is an important work, but it seems like an outdated method as a compared method and there are a number of related recent works with better performance. Comparing with more recent methods will make this work stronger. For StyleGANv2, comparing with (Wulff & Torralba 2020) will help this work to be demonstrated as a SOTA method. Comparing with score matching based works will be surely a plus.\n- Gaussianization technique is also used in other image generation works. See Yen-Chi Cheng et al., InOut : Diverse Image Outpainting via GAN Inversion, CVPR 2022. This work should survey various image generation works with Gaussianization not only for compressive sensing MRI, deblurring and eikonal tomography, but also for other related inverse problems.\n",
            "summary_of_the_review": "This work introduced a Gaussianization layer for image generation models and then used it for solving a few inverse problems. This work was compared with the works not using Gaussianization. However, using Gaussianization for image generation is not new as discussed in the review and some of them look critical. Moreover, it is unclear if the intended Gaussianization really happened in all problems - no quantitative results for Gaussianization. Thus, in its current form, without justifying its novelty over (C Meng et al., AIStats 2020) and other works using the idea of Gaussianization, it will be very hard not to recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper779/Reviewer_k6hw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper779/Reviewer_k6hw"
        ]
    },
    {
        "id": "aPTRhtTQw2s",
        "original": null,
        "number": 2,
        "cdate": 1666970971693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666970971693,
        "tmdate": 1666970971693,
        "tddate": null,
        "forum": "OXP9Ns0gnIq",
        "replyto": "OXP9Ns0gnIq",
        "invitation": "ICLR.cc/2023/Conference/Paper779/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper improves the deep generative models used for constraining solution spaces in inverse problems by making distributions of latent variables obey a Gaussian. Deep generative models such as StyleGAN2 and Glow are widely employed for the regularizes in inverse problems. Given latent variables that obey a Gaussian, these generative models generate realistic images. The authors point out that the latent variables deviate from the desired Gaussian distribution during inversion lead to poor results, and the proposed method reparameterizes and Gaussianizes the latent variables during the optimization for solving inverse problems. The proposed method was applied to compressive sensing MRI, image deblurring and eikoonal traveltime tomography, and the experimental results show the proposed method works well.",
            "strength_and_weaknesses": "Strong points:\nThe authors point out that when the latent variables of deep generative models deviate from the desired Gaussian distribution, the solutions of inverse problems would be poor, and propose a method that makes the latent variables obey a Gaussian distribution. Using ICA, Yeo-Johnsonn transformation, and Lambert WxFx transformationo, the methd improves the independency, skewness, and heavy-tailedness of the distributions. The method is novel and the experimental results show that the proposed method can constrain the solutions more correctly. \n\nWeak points:\nThe algorithm for the update of v is not clearly described in the text. Appendix F describes the gradient computation of the optimization-based differentiable layers but the reviewer believes the contents shown in Appendix F should be described in the text.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized. The method is novel and the experimental results show that the method is promising. ",
            "summary_of_the_review": "The reviewer recommends to accept this paper because of the strong points described above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper779/Reviewer_YtuR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper779/Reviewer_YtuR"
        ]
    },
    {
        "id": "qJdLALlesF",
        "original": null,
        "number": 3,
        "cdate": 1667243956403,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667243956403,
        "tmdate": 1667243956403,
        "tddate": null,
        "forum": "OXP9Ns0gnIq",
        "replyto": "OXP9Ns0gnIq",
        "invitation": "ICLR.cc/2023/Conference/Paper779/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tries solving inverse problems using generative models as regularizers. It is known that when doing MAP / Maximum Likelihood optimization, the optimization variable deviates from its expected statistics, which is standard Gaussian in most cases. \n\nThis paper proposes an algorithm that can enforce Gaussian statistics on the latent variable $z$ during optimization. This is done by adding a KL divergence regularizer to the optimization objective, where the KL is between the distribution of $z$ and a standard Gaussian. The authors then decompose the KL divergence as a sum of two non-negative terms, and minimization is done by alternating ICA / whitening on the two terms in the decomposition.\n\nThe algorithm is novel and the experiments are interesting, although I have some concerns about the baselines used for comparison.",
            "strength_and_weaknesses": "**Strengths**:\n\n- I think the proposed solution is very practically reasonable. The algorithm requires minimal ``training'' to force Gaussian statistics on the solution. Previous works have considered using the vanilla $\\ell_2$ norm of $z$ (as in Bora et al 2017), while algorithms like PULSE[3] forcibly project to the Gaussian sphere. This solution is much better.\n\n- The decomposition in Eqn 6 is quite clever, as is the algorithm for iteratively minimizing the two terms in the decomposition -- the ICA keeps the LHS of Eqn 6 constant and increases the second term in the RHS, which implies the first term in the RHS must decrease; the next step is a whitening step that forces the second term to decrease while leaving the first term constant.\n\n- The experiments look really good, but it's hard to judge because the baselines considered are not the strongest. \n\n**Weaknesses**:\n\n- The motivation behind this problem is that an optimization problem as in Eqn (3) is unstable to choice of $\\beta$, if the regularizer is $\\beta || z ||^2$, as $z$ is supposed to be Gaussian distributed. But this doesn't make much sense in theory, as eqn (3) solves for the MAP in $z$, [which is not the same as searching for the MAP in $g(z)$ space, since $- \\log p(G(z)) = ||z||^2 - \\log det( \\nabla_z G(z) ) $]. This inconsistency in performing MAP in $G(z)$ or $z$ space could be the reason for the artefacts, as opposed to issues in optimization / distribution of the latent vectors.\n\n- An additional point about MAP in $z$ space: the phenomena described in this paper can also be circumvented when using GLOW by using something like Langevin dynamics to approximate Posterior Sampling [1]. In the case of StyleGAN, score-guided optimization[2] has been useful. In both cases the statistics of $z$ are respected. Your argument would be much stronger if you can offer a direct advantage (methodologically or experimentally) over these techniques. Does your optimization offer advantages over these?\n\n- The related work section is not very accurate -- score models can be used for non-linear inverse problems as well. The cited references [Jalal et al, Song et al] considered compressed sensing MRI and CT, which are linear inverse problems. I am not familiar with work questioning their applicability to non-linear inverse problems.\n \n- The baselines used for comparison are very weak, especially in Fig 4. To clarify, I'm not blaming the authors, it's well known that inverting a StyleGAN is empirically challenging. Comparing to compressed sensing experiments in [2] on the celebA dataset + StyleGAN would be a much more convincing and necessary experiment. (code and models: https://github.com/giannisdaras/sgilo )\n\n- The optimization procedure makes sense, but is there any guarantee that ICA and whitening leads to a strict decrease? I don't know what the theoretical guarantees for ICA are, and it would be helpful if you could add some references in the appendix.\n\n- Figure 5 has clear artefacts for the proposed method. What's going on? Can you provide a comparison to Langevin dynamics[1] on the same experiment?\n\n**Clarifications**:\n\n- The experiments section is a little confusing -- why do you have separate experiments for the Orthogonal Reparameterization and Gaussianization layers? I thought they're two components of one algorithm? What are these things in relation to Figure 2?\n\n-\n\n**References**:\n\n[1] Jalal, Ajil, Sushrut Karmalkar, Alex Dimakis, and Eric Price. \"Instance-Optimal Compressed Sensing via Posterior Sampling.\" In International Conference on Machine Learning, pp. 4709-4720. PMLR, 2021.\n\n[2] Daras, Giannis, Yuval Dagan, Alex Dimakis, and Constantinos Daskalakis. \"Score-Guided Intermediate Level Optimization: Fast Langevin Mixing for Inverse Problems.\" In International Conference on Machine Learning, pp. 4722-4753. PMLR, 2022.\n\n[3] Menon, Sachit, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. \"Pulse: Self-supervised photo upsampling via latent space exploration of generative models.\" In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pp. 2437-2445. 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**:\n\nI found the paper to be quite clear, but perhaps a little verbose. An algorithm environment that describes the final updates, as well as inputs, parameters, and outputs, would be very useful. The information is very spread out in the current format. \n\nAnother nitpick is that the authors use the term ``Gaussianization'' to sometimes refer to Step 2 (whitening), and sometimes to refer to both steps of ICA + Whitening. Some consistency would be useful to avoid confusion about what sub-component of the algorithm is being described.\n\n**Quality and originality**:\n\nI think the paper meets the standard for ICLR. The proposed solution is sufficiently novel, and the problem of inverse problems using generative models has significant practical and theoretical significance.",
            "summary_of_the_review": "The algorithm is novel. The experiments are sort of convincing, but have clear issues. I am happy to raise my score if the authors can compare to the additional baselines I mentioned.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper779/Reviewer_VNJA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper779/Reviewer_VNJA"
        ]
    },
    {
        "id": "joTL5YeLseT",
        "original": null,
        "number": 4,
        "cdate": 1667476448780,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667476448780,
        "tmdate": 1667476448780,
        "tddate": null,
        "forum": "OXP9Ns0gnIq",
        "replyto": "OXP9Ns0gnIq",
        "invitation": "ICLR.cc/2023/Conference/Paper779/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Recent work has shown how deep generative models (such as GANs or VAEs) can be used to regularize inverse problems by enabling optimization in the latent space of a pre-trained generative model\u2014with a regularisation term to encourage latent code gaussianity. However, low-fidelity solutions whose latent codes strongly deviate from gaussian still occur. To address this issue, the authors propose a new approach: (i) perform optimization on unconstrained latent code, (ii) leverage a novel gaussianization layer to map the unconstrained latents to gaussian ones, which are then input to the generative model.\n\nThe authors apply their approach to three different inverse problems (compressive-sensing MRI, image deblurring, and eikonal tomography) and compare their approach to different methods for inverse problems (including methods that use generative models but different strategies to enforce gaussian latent codes). Across the three applications, the proposed approach achieves state-of-the-art performances.",
            "strength_and_weaknesses": "Strengths:\n* The new Gaussianization layer is a novel technical contribution, which combines several sublayers (ICA layer, power transform layer and lambert layer). This layer and its component can be applied to various deep generative models, inverse problems and deep variational inference models, thus being of potential interest to a broad community.\n* Results are convincing. Additional experiment/results are provided in the supplement. The authors also considered an ablation study to investigate which parts of the proposed Gaussianization layers were responsible for the observed performance improvement.\n\nWeakness:\n* I think the presentation of the paper could be improved. The authors may consider discussing a specific example when introducing inverse problems, regularization with generative models and the gaussianization layer. The deblurring example would be the easiest for this (the authors may also consider discussing this as the first experiment for consistency).\n* The authors do not comment on (future) code availability. I think that including Pytorch/Tensorflow implementations of the proposed Gaussian layer and its subparts would make this work more impactful. If the paper is accepted, the code to reproduce experiments should also be released.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: OK. It could be improved (see comments above).\nNovelty: The contribution seems novel. However, a version of this manuscript has been available on arXiv since Dec 2021. Can the authors comment on this?\nReproducibility: The authors should realise code if the manuscript is accepted.",
            "summary_of_the_review": "The authors introduce a new strategy for solving inverse problems using generative models by introducing a new layer enabling latent code gaussianization. This is potentially an important contribution for inverse problems, generative models and deep variational inference. The authors could improve paper readability providing a guiding example (see comments above) and release code for the proposed layers for broader impact. If the paper is accepted, the authors should provide code to reproduce their experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper779/Reviewer_NCs6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper779/Reviewer_NCs6"
        ]
    }
]