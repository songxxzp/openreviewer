[
    {
        "id": "pcEr8QG6Ca",
        "original": null,
        "number": 1,
        "cdate": 1666379655556,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666379655556,
        "tmdate": 1670761394372,
        "tddate": null,
        "forum": "9dFQcu9vmX",
        "replyto": "9dFQcu9vmX",
        "invitation": "ICLR.cc/2023/Conference/Paper2151/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an image-goal navigation (ImageNav) method on top of VGM (Kwon et al. 2021) which builds a topological graph. The proposed method, MemoNav considers VGM as the short-term memory (STM) and removes some nodes based on thresholding on attention score (selective forgetting). The modified STMs are fed into the GATv2 encoder followed by a decoder. The final output of the decoder is used as an input of the policy network. The authors tested MemoNav in Gibson and Matterport3D environments and showed that the proposed method improves performance.\n",
            "strength_and_weaknesses": "**[Strength]**\n\n1. The paper is well-written and easy to understand.\n\n2. The authors include the failure trajectories in Figure 10 for a better understanding of the behaviors of the proposed method. \n\n\n**[Weakness]**\n\n1. Basically, the proposed MemoNav is a combination of existing methods, VGM (topological mapping, decoder) + forgetting module + GATv2 (across time). Although I do not degrade the contribution of merging together and achieving good performance, the novelty is somewhat limited. The authors argue that the forgetting module is novel in Section 2. However, attention-based dropout already exists [Ref 1].\n\n2. According to Table 2 (1-3, 2-4), the forgetting module is not that helpful and even degrades the performance in terms of SPL/PPL. The authors should specify the reason why the performance s decreased. Moreover, considering Table 3, I wonder effectiveness of using the forgetting module. I read Section A.5 but it compares with random forgetting, not the effectiveness of forgetting itself. One possible explanation might be to reduce the computation by removing the node. The authors briefly mentioned that the proposed method employs 20% fewer node features. However, further explanation is needed.\n\n3. In Figure 4, the performance is consistently decreasing in 3-goal and 4-goal settings as p grows. The authors argue that \u201cexcluding an excessive fraction forces the agent not to utilize what it has explored.\u201d This statement should be also applicable to the smaller number of goals tasks. However, the performance is getting better in 1-goal and 2-goal tasks. The authors should analyze this trend more clearly.\n\n\n**[minor]**\n\n1. In Section 5.3, please unify the notation. The second paragraph uses absolute difference while the third paragraph uses relational difference.\n\n2. There is one paper [Ref 2] that achieves better performance than the proposed method but I will not count on this since it is arxiv paper.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned in **[Strength]**, the paper is well written and includes many ablation studies and visualization. However, some analysis regarding results or explanation (e.g., A4.2. what is special linear projection layers) are missing. \n",
            "summary_of_the_review": "As mentioned in above, the idea and achievement are good but the further analysis of experimental results is needed and the novelty is somewhat limited.\n\n- Post Rebuttal - \nAlthough I appreciate the authors dealt with my concerns, I agree with other reviewers' concerns and G+L+F is not significantly different from G+L. Hence, I maintain the original rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2151/Reviewer_aALd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2151/Reviewer_aALd"
        ]
    },
    {
        "id": "K3LVYFEWcjS",
        "original": null,
        "number": 2,
        "cdate": 1666638925672,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638925672,
        "tmdate": 1671220929814,
        "tddate": null,
        "forum": "9dFQcu9vmX",
        "replyto": "9dFQcu9vmX",
        "invitation": "ICLR.cc/2023/Conference/Paper2151/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel graph-memory network for visual navigation. Inspired by working memory from cog-sci literature, the proposed method consists of short-term, long-term and working memories to perform the ImageNav task. A forgetting module is proposed to prune the goal-irrelevant parts of the short-term memory. Results are demonstrated on Gibson and Matterport3D datasets.",
            "strength_and_weaknesses": "\n# Strengths\n* The idea is interesting and novel to the best of my knowledge. \n* The clarity of the paper is good and the paper is easy to understand.\n* The experiments have been well designed, especially the ablations and subsequent analyses. \n\n\n# Major weaknesses\n## The design of selective forgetting module is suboptimal\n* The module deletes nodes which are not scored highly by the target decoder. But the graph also contains other valuable information which are not directly related to the goal: (1) the context around the agent's current position in the environment, and (2) the graph connectivity information from agent's current position to a node close to the target --- useful for path planning. This information is lost when selective forgetting module is used.\n* The design of the LTM module appears to be focused on undoing some of the information loss from selective forgetting, but since it is just a single node, it may not completely reverse the loss. \n* Is it necessary to perform hard removal of nodes instead of soft attention?\n* The results in Tab. 2 indicate that L+F (row 4) is better than row 1 primarily for the 1-goal task. For the multi-goal tasks, the results are slightly better, but possibly within the margin of error. Can the forgetting module be better designed to avoid losing the above information? It can lead to more significant gains across tasks.\n\n## Experiments do not sufficiently account for statistical significance\n* In Table 1, bolding does not account for statistical significance b/w VGM and MemoNav. Are the bolded numbers statistically better than the next-best values?\n    * In Gibson: 1-goal SPL, 2-goal PR / PPL, 3-goal PPL, 4-goal PPL\n    * In MP3D: 1-goal SPL, 2-goal PPL\n* In Table 2, {2,3,4}-goal results are fairly close based on standard deviation even though the averages show progression. Need to account for statistical significance of the increase in performance across rows.\n* Figure 4 - no standard deviation markings to show significance of the differences.\n\n## Unfair comparison to VGM \n* VGM uses the inferior GCN encoder instead of the GAT-v2 used for Memonav. The particular architecture of encoder used is orthogonal to the contributions of Memonav. Does Memonav outperform VGM with GCN replaced by GAT-v2?  \n\n## Missing state-of-the-art baselines --- ZER, OVRL, Last-Mile, SSL-sparse\n* VGM is no longer the state-of-the-art for ImageNav. There are a lot more recent works which show significant improvements on ImageNav and need to be compared with [R1,R2,R3,R4].\n\n\n# Minor weaknesses\n* Tab. 3 -- need average STM features baseline instead of randomly sampled STM feature\n* Figure 4 --- unclear how to pick a good forgetting threshold without having complete test results --- no single number is good across settings.\n* Tables 1 and 2 - why limited or no gains on SPL / PPL?\n* Clarification needed in paragraph below Figure 3 - is forgetting module not used during training?\n* Sec 4.1 1st paragraph - incorrect reference to Anderson et al., 2018 ? \n* Missing broader related work comparison to navigation and representation learning literature\n* Working memory graphs has been cited as a closely related work, but it has not been experimentally compared with.\n\n# References\n[R1] Al-Halah, Ziad, Santhosh Kumar Ramakrishnan, and Kristen Grauman. \"Zero experience required: Plug & play modular transfer learning for semantic visual navigation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. \n\n[R2] Yadav, Karmesh, et al. \"Offline Visual Representation Learning for Embodied Navigation.\" arXiv preprint arXiv:2204.13226 (2022). \n\n[R3] Wasserman, Justin, et al. \"Last-Mile Embodied Visual Navigation.\" 6th Annual Conference on Robot Learning. 2022. \n\n[R4] Majumdar, Arjun, et al. \"SSL Enables Learning from Sparse Rewards in Image-Goal Navigation.\" International Conference on Machine Learning. PMLR, 2022. \n\n\n# Post-rebuttal update\nI thank the authors for their extensive experiments and detailed responses. However, my concerns with the forgetting module remain, and the experiments are incomplete since the models are not trained to convergence and results are statistically similar to baselines. It is hard to overlook these glaring concerns. This work is promising, but incomplete. I maintain my rating.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper clarity is good. The idea is novel. The code has been provided along with instructions for reproducibility.",
            "summary_of_the_review": "The idea is interesting and novel, the paper is clearly written, and the experiments are well designed. I have concerns about the design of the forgetting module, insufficient accounting for statistical significance, unfair comparison to VGM and missing state-of-the-art baselines for ImageNav. I also have some minor concerns which I hope that authors should be able to address easily.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2151/Reviewer_LEke"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2151/Reviewer_LEke"
        ]
    },
    {
        "id": "qihEoEg9aR",
        "original": null,
        "number": 3,
        "cdate": 1666659991811,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659991811,
        "tmdate": 1666659991811,
        "tddate": null,
        "forum": "9dFQcu9vmX",
        "replyto": "9dFQcu9vmX",
        "invitation": "ICLR.cc/2023/Conference/Paper2151/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper builds upon the VGM (Kwon et al., 2021) for image navigation. By forgetting some less informative nodes in the topological map through the attention mechanism, the short-term memory (STM) can be updated dynamically. The selected STMs are fused to obtain the scene-level feature with Long-term memory (LTM), and which are encoded through graph attention to generate the Working Memory (WM). The WM is finally used to generate the actions for navigation. The experiments of both 1-goal and multi-goal tasks are conducted in two public datasets Gibson and Matterpot3D in Habitat simulator.",
            "strength_and_weaknesses": "Strengths:\n1.\tThe paper is generally well-written and easy to follow.\n2.\tThe experiments and analysis are extensive with the ablation experiments and baselines.\n3.\tThe selective forgetting module seems capable of retaining the informative nodes in the topological map and forget the redundant nodes, which may be helpful for improving the SR/PR for 1-goal and multi-goal tasks.\n\nWeaknesses:\n1.\tThe MemoNav seems to be trained in the 1-goal setting, while used in both 1-goal and multi-goal tasks, is it suitable to use this model to evaluate the multi-goal task? It is worth discussing training different models (under multi-goal settings) to validate for multi-goal tasks.\n2.\tThe proposed MemoNav follows the VGM method of constructing the topological maps of the environments. Considering the topological maps can somehow memorize the observations during navigation as well, the advantages of proposed memory based modules are not that effective compared to the baseline, also the close SPL/PPL results are obtained compared the baseline VGM under 1-goal and multi-goal tasks. \n3.\tIt seems the regularity and relevance are not that significant in Figure 4. The 2-goal task shows a sharp drop in PP at p=0.4, and a dramatic increase at p=0.6, what are the reasons for this phenomenon?\n \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the contributions of the paper are easy to follow. Compared with the current methods, this paper contributes to node selection and encoding, but less in constructing topological maps. The code is submitted, the paper should be reproducible. ",
            "summary_of_the_review": "I think this is a practical work, the motivation and idea are interesting, and the experimental results are also sufficient. However, I worry about the that novelty of the technical contribution over baseline (VGM) seems limited. And compared to the results of VGM, the improvement of the proposed method seems marginal in some parts. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2151/Reviewer_EedW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2151/Reviewer_EedW"
        ]
    }
]