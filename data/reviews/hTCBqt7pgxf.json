[
    {
        "id": "QkU74_S-i-w",
        "original": null,
        "number": 1,
        "cdate": 1666566083392,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666566083392,
        "tmdate": 1666566083392,
        "tddate": null,
        "forum": "hTCBqt7pgxf",
        "replyto": "hTCBqt7pgxf",
        "invitation": "ICLR.cc/2023/Conference/Paper2485/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to utilize the meta-node (average of node embeddings within a cluster) for constructing negative samples in contrastive learning. It claims that this approach is more efficient than previous sampling based methods and it leads naturally to block contrastive learning which could be beneficial. Experiments on various datasets demonstrate the performance of the proposed method on different modalities including image, text and graph.",
            "strength_and_weaknesses": "## Strength\n1. The direction of more efficient negative sample mining is important for contrastive learning.\n2. The idea of utilizing the clustering centers as negative samples saves the sampling cost.\n3. Extensive experiments demonstrate comparable performance with baseline methods while achieving good speedup ratio for training time.\n\n## Weaknesses\n1. Discussion on related works is not enough, which makes the novelty a little unclear. Is this work the first to utilize averaged embeddings as negative samples for contrastive learning? What are the closest related works? \n2. It is not clear how the proposed method can save inference time\n3. How long does it take to evaluate the soft label? Is it included in the training time?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: fair, see questions above\nQuality: reasonable\nNovelty: needs clarification\n",
            "summary_of_the_review": "Overall the proposed method is well motivated and achieves reasonable speedup for training. I have some questions about the novelty and experiments. I may consider adjusting my rating after discussing with the authors.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2485/Reviewer_6uyq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2485/Reviewer_6uyq"
        ]
    },
    {
        "id": "jSMkjl53YD",
        "original": null,
        "number": 2,
        "cdate": 1666575274975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575274975,
        "tmdate": 1666873675650,
        "tddate": null,
        "forum": "hTCBqt7pgxf",
        "replyto": "hTCBqt7pgxf",
        "invitation": "ICLR.cc/2023/Conference/Paper2485/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extends the contrastive learning on graph by replacing the contrastive between negative samples with that between class clusters with theoretical analysis. It possesses the attractive characteristic of reducing complexity. Experimental evaluations demonstrate its effectiveness and efficiency.",
            "strength_and_weaknesses": "Strength\n- It makes sense to replacing the contrastive between negative samples with that between class clusters to reduce the complexity.\n- The experimental evaluations are sufficient.\n- The writing and organization are good.\n\nWeakness\n- My main concern is the novelty may not be significant. Although authors call the cluster center as meta-nodes, they are actually the prototype of the cluster. There are many efforts have be paid on prototypical contrastive learning, such as [1] and its variants, which also takes cluster centers into considerations and reduce the complexity.\n\n[1] Junnan Li, Pan Zhou, Caiming Xiong, Steven C. H. Hoi: Prototypical Contrastive Learning of Unsupervised Representations. ICLR 2021\n2020\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The clarity and reproducibility are good.\n- The novelty and originality may be not significant.",
            "summary_of_the_review": "This paper possesses clear motivation, good writing and organization, and sufficient evaluations. My main concern is the novelty, since existing prototypical contrastive learning also reduce complexity by considering cluster centers.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2485/Reviewer_cHFr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2485/Reviewer_cHFr"
        ]
    },
    {
        "id": "q6US_t72iMC",
        "original": null,
        "number": 3,
        "cdate": 1666661787630,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661787630,
        "tmdate": 1666661787630,
        "tddate": null,
        "forum": "hTCBqt7pgxf",
        "replyto": "hTCBqt7pgxf",
        "invitation": "ICLR.cc/2023/Conference/Paper2485/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the task of representation learning for graph data. The proposed objective is a contrastive learning objective that is based on using clusters in negative sampling. Empirical analysis compares the proposed approach to a wide variety of representation learning and clustering approaches for graphs. ",
            "strength_and_weaknesses": "**Strengths** \n* The proposed approach seems to be widely empirically effective and is accurate. The approach is very computationally efficient compared to baseline approaches. \n* The proposed approach uses an interesting clustering-based objective that seems well motivated. \n\n**Weaknesses**\n* Clustering objectives have been studied in other contrastive learning approaches e.g. [1, 2, 3] among a wide array of other approaches in deep clustering. It would greatly improve the paper to consider how these approaches relate to the proposed approach. \n* Could the authors clarify the statement\n>We provide theoretical proof and show that real world graphs always satisfies the necessary\nconditions, and that PamCGC is block-contrastive, known to outperform pair-wise losses\n* While the proposed approach offers empirical advantages and is an interesting idea, I am wondering if the methodological depth is sufficient for the ICLR \"bar\". It would seem that the novelty is the combination of clustering ideas in graph representation learning. I think this is nice, but I feel the authors could clarify how their contribution investigates deeper questions in this area than past work. For instance, efficiency is a major advantage of the proposed approach. Understanding tradeoffs here is in Figure 2. It seems like more emphasis could be placed in this understanding.\n\n[1] Caron, Mathilde, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. \"Unsupervised learning of visual features by contrasting cluster assignments.\" Advances in Neural Information Processing Systems 33 (2020): 9912-9924.\n\n[2] Zhang, Chunyang, Hongyu Yao, C. L. Chen, and Yuena Lin. \"Graph Representation Learning via Contrasting Cluster Assignments.\" arXiv preprint arXiv:2112.07934 (2021).\n\n[3] Li, Junnan, Pan Zhou, Caiming Xiong, and Steven CH Hoi. \"Prototypical contrastive learning of unsupervised representations.\" arXiv preprint arXiv:2005.04966 (2020).",
            "clarity,_quality,_novelty_and_reproducibility": "Please see weaknesses for additional clarifications. ",
            "summary_of_the_review": "This paper presents an empirically effective approach for graph representation learning and clustering. The approach, which uses clustering  is more efficient than previous work. The paper could be improved with more complete description of the landscape of related work on representation learning and clustering. The paper could be further improved with more clarity on the technical depth and revealed performance tradeoffs.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2485/Reviewer_X8V7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2485/Reviewer_X8V7"
        ]
    },
    {
        "id": "8VksCy66JZ",
        "original": null,
        "number": 4,
        "cdate": 1666869635209,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666869635209,
        "tmdate": 1666869635209,
        "tddate": null,
        "forum": "hTCBqt7pgxf",
        "replyto": "hTCBqt7pgxf",
        "invitation": "ICLR.cc/2023/Conference/Paper2485/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Proxy approximated meta-node Contrastive (PamC) for contrastive representation learning on graphs. PamC is motivated by the computational burden of vanilla contrastive loss (i.e., InfoNCE), and to deal with this problem, it proposes a meta-node based approximation technique which proxies all negative combinations in quadratic cluster size time complexity. Empirical reuslts sho that the proposed method demonstrates promising accuracy gains over sota graph clustering methods on 6 benchmarks with better efficiency.\n",
            "strength_and_weaknesses": "Strenghts:\n1. The motivation of this paper is good and solid. The computation complexity of InfoNCE loss is quadratic with respect to the number of nodes, which severly prevent full-graph training.\n2. The proposed method does show better efficacy than traditional contrastive methods.\n\nWeaknesses:\n  1. The authors claim that contrastive learning on graphs require a large number fo negative samples while subsampling is suboptimal. Is there any empirical support for this claim as according to my experience, subsampling will not severly degrade the model\u2019s performance.\n \n2. Compared with data augmentation based contrastive learning  (InfoNCE loss) methods which ony requires two shared GNN encoder to generate node embeddings, the proposed method looks much more complicated (e.g., pretraining to get soft clusters). Although the overall complexity is linear, I doubt whether it can lead to efficacy when really applied to these datasets (especially when the graph is not that large, e.g., the datasets used in experiments)\n3. The paper focus on contrastive learning on graphs, however, a lot of important related works are missing in both related works and experiments. For example, [1] and [2] are two data augmentation-based contrastive methods using InfoNCE loss. [3] avoids negative samples using asymmetric structures. [4] avoids negative samples through feature-level decorrelation. The complexity of [3] and [4] are both linear to the graph size and thus they are scalable. However, this paper never consider these important baselines.\n4.  I am also confused about the tasks and datasets used in experiments. According to my knowledge, most self-supervised learning methods (including contrastive ones) foucs on node classifcation tasks (e.g., [1-4]). Why you consider graph clustering tasks instead of more commonly used node classication tasks.\n5. Although the most imporant claimed advantage is scalability, the datasets used for evaluation are really small. The authors should consider use larger graphs.\n\nReferences:\n[1] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131, 2020b.\n[2] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In WWW, 2021.\n[3] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, R\u00e9mi Munos, Petar Velickovic, and Michal Valko. Bootstrapped representation learning on graphs. arXiv preprint arXiv:2102.06514, 2021.\n[4] Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation analysis to self-supervised graph neural networks. In NeurIPS, 2021.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the propsoed method is novel and is clearly presented. ",
            "summary_of_the_review": "Generally, I think the motivation of this paper is good. However, I think the propsoed method is over-complicted while does not show prominently better performance than simple methods. Besides, I believe the proposed method is not properly evaluated, in terms of tasks, datasets and baselines. I am leaning on rejection.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2485/Reviewer_GN9c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2485/Reviewer_GN9c"
        ]
    }
]