[
    {
        "id": "oSSesUqus6",
        "original": null,
        "number": 1,
        "cdate": 1665694868480,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665694868480,
        "tmdate": 1665694868480,
        "tddate": null,
        "forum": "53FyUAdP7d",
        "replyto": "53FyUAdP7d",
        "invitation": "ICLR.cc/2023/Conference/Paper3946/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents ODIS, a new approach to learning policies for multi-agent, multi-task RL problems via skill-discovery and conditioning. Leveraging transformer models, the proposed approach can learn state-action-space invariant policies from offline data, and through additional loss functions the policies are encouraged to learn distinctive \"coordination skills\" that can be used to aid in action generation. Experimental evaluations show ODIS often outperforms simple BC baselines as well as other MARL approaches using comparable optimization steps.",
            "strength_and_weaknesses": "Strengths:\n* ODIS is shown to perform better than baselines in multi-agent problems.\n* ODIS generalizes to new state-action space configurations, such as adding more Marines in the StarCraft II micro-control domains.\n* The appendix has a wealth of detail on the experimental design and results, additional results and experiments, and network architecture and training details.\n* The approach is relatively clear and the loss functions are well-introduced and explained.\n* Contributions are evaluated with ablation studies across new objectives, showing the importance of the consistent loss that is introduced as part of the method.\n\nWeaknesses:\n* The experimental setup and results are jumbled together in a somewhat confusing way. Section 5.1 introduces the baselines and describes how they relate to ODIS, and at the same time it is delivering results and discussion. There is a lot of information for the reader to make sense of all at once, without having a full-picture of the results or baselines.\n* The experiments and introduction mention an additional navigation task, but it is not present in the main body of the paper.\n* The main body of the paper references and discusses a result that is only present in the appendix (Figure 7).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, although the appendix is doing a lot of heavy lifting for architecture design, training details, extra experiments, dataset details, etc. The main body of the paper is self-contained for describing the methods and the experiments, although the appendix is frequently referenced and used.\n\nThe work appears to be original/novel and high quality. Comparisons to recent work are highlighted by the authors, and differences are clearly discussed and experiments are conducted to highlight performance gaps.",
            "summary_of_the_review": "The paper presents an interesting approach that is clearly described and shows strong empirical performance relative to baselines that are given in the paper. The main weakness of the paper is that many of the relevant details of the method (network architecture, dataset/training details, experimental design/results, comparison to contemporary work, etc.) are relegated to an appendix. However, the main body of the paper does not often suffer for moving so much information to the appendix, and overall the submission is strong.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3946/Reviewer_1rgQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3946/Reviewer_1rgQ"
        ]
    },
    {
        "id": "NI9Vo6YTDvN",
        "original": null,
        "number": 2,
        "cdate": 1666630671955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630671955,
        "tmdate": 1666630671955,
        "tddate": null,
        "forum": "53FyUAdP7d",
        "replyto": "53FyUAdP7d",
        "invitation": "ICLR.cc/2023/Conference/Paper3946/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Although there have been studies that extended MARL into offline learning concepts and multi-tasking learning, no studies have attempted both simultaneously. Thus, the current research is meaningful in that it tries to tackle both offline and multi-task learning simultaneously in MARL. The principal methodology that the current study uses is inferring the latent vector that can possibly represent the essential skills for inducing coordination among agents and using these inferred skills in independent decision-making.",
            "strength_and_weaknesses": "To conduct multi-task learning, the current study utilizes a kind of high-level action called a skill that co-exists in multiple tasks. In addition, to conduct offline learning, the current study uses imitation learning with a conservative Q-value estimation technique. Each technique is already an existing methodology, and it seems this study used these two techniques simultaneously to achieve two purposes. \n\nHowever, this study did not formalize or discuss additional issues that may arise when both cases occur simultaneously. Both problems have a common issue of distribution shift, but no means to overcome this problem is specifically suggested.",
            "clarity,_quality,_novelty_and_reproducibility": "\n<Methodology>\n\n1. The process of skill discovery seems to be nothing but imitation learning with latent factor modeling. Although current studies claim z to be one of the possible skills, there is no evidence that z_i implies a specific thawing pattern. \n\n2. The skill discovery module and Q-mix style MARL algorithms seem to be trained separately. What is the reason for this, and is there any issue arising from this separate training?\n\n3. It is ambiguous where $\\hat{q}_i$ is used. It seems that $\\hat{q}_i$ amortizes the global skill discovery encoder. The amortized encoder may have lower generalization capability, especially when the distribution shift occurs during multi-task transfer.\n\n\n<Experiments>\n\nWhen the trained model from the source data is tested with unseen tasks, the unseen task of a similar scale, the performance drops sharply. For example, when a model trained at 9m10m is tested at 10m11m, the performance drops by more than 50%. It may be hard to say that the proposed model can effectively transfer to unseen tasks. \n",
            "summary_of_the_review": "Although it is a very challenging study that combines offline learning and multi-task learning with MARL, the methodological novelty is not great. In addition, too much overstatement of the latent variable z.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3946/Reviewer_PuN1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3946/Reviewer_PuN1"
        ]
    },
    {
        "id": "P6d4EBxK2n",
        "original": null,
        "number": 3,
        "cdate": 1666670631214,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670631214,
        "tmdate": 1666670631214,
        "tddate": null,
        "forum": "53FyUAdP7d",
        "replyto": "53FyUAdP7d",
        "invitation": "ICLR.cc/2023/Conference/Paper3946/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to discover generalizable multi-agent coordination skills from multi-task offline data (multi-task marl with offline data). The insight is that we can extract universal skills for coordination from offline multi-agent multi-task data. The authors propose the ODIS algorithm which consists of two stages. Firstly, task-invariant coordination skills are extracted from the offline multi-task data, and it learns to delineate different agent behaviors with the discovered coordination skills. Secondly, a coordination policy is trained to choose optimal c coordination skills under CTDE. The authors conduct extensive experiments based on several maps from SMAC to validate the effectiveness of ODIS.",
            "strength_and_weaknesses": "### Strengths\nThe paper tackles an important problem in MARL - generalization to tasks with varying agents and targets based on the recent success of offline MARL algorithms that leverage the offline dataset (compared to a fully online way). The setting is novel and new. The proposed method is also reasonable and clearly written, which is motivated well. Figure 1 does a good job in motivating the problem and the proposed method, where ODIS is responsible for generalizing to the unseen 10m task given offline data from source tasks like 5m and 8m via discovering coordination skills and learning a coordination policy. The paper is clearly written and easy to follow.\n\n### Weaknesses\nOne minor concern for the paper is to evaluate ODIS in maps that is more difficult (like corridor and 2c_vs_64zg), as the experiments are mainly focused on easy maps.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, with clear motivation for the problem and the method, and is also easy to follow. The authors conduct extensive experiments on several maps from SMAC in comparison to recent baselines to validate the effectiveness of ODIS. The setting is new and the proposed method is also reasonable.",
            "summary_of_the_review": "In all, the paper studies an intersting and new setting, which aims to improve generalization in MARL via offline multi-task multi-agent data. The proposed method is also reasonable, with significant improvements over baselines.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3946/Reviewer_e7iL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3946/Reviewer_e7iL"
        ]
    },
    {
        "id": "OO33oyPe0u5",
        "original": null,
        "number": 4,
        "cdate": 1666992043942,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666992043942,
        "tmdate": 1666992043942,
        "tddate": null,
        "forum": "53FyUAdP7d",
        "replyto": "53FyUAdP7d",
        "invitation": "ICLR.cc/2023/Conference/Paper3946/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel algorithm, ODIS, for learning multiple multi-agent tasks from an unlabeled offline dataset. The algorithm is broken up into two phases. In the first phase, a skill extractor is learned from global states, and the encoded skills are fed to per-agent action decoders. In the second stage, per-agent observation encoders ",
            "strength_and_weaknesses": "Strengths: \n\n1) This paper addresses the intersection of three important areas of research - multi-agent, multi-task, and offline RL. It is the first paper to directly address these areas and notably outperforms single-task offline MARL methods. \n\n2) The paper is well written, with thorough experiments, appropriate baselines, and a variety of tasks across two distinct environments.\n\nWeaknesses:\n\n1) The method requires access to global state to learn the skill extractor. It's not clear to me why learning observation encoders and a mixing network, as is done in the coordination policy, is not sufficient, and some intuition would be helpful. \n\n2) The authors state that the distribution of skills is prevented from collapsing by maximizing KLD with a uniform skill distribution. While Figure 3 seems to support this, learning diverse skills automatically has historically been a difficult problem [1, 2], and I would appreciate some further discussion or ablations around this. \n\n3) In most tasks, the results are only slightly better than behavioral cloning, even on the lower quality datasets. \n\n4) The transformer-based architecture is closely related to [3], and I would recommend it be added to the literature review. \n\nQuestions:\n\n1) How many evaluation trajectories are run for each result?\n\n2) Is any parameter sharing used in the observation encoder or action decoder?\n\n3) Are there any results comparing IQL or QPLEX on other datasets besides medium? \n\n4) Does freezing the last layer of the skill encoder affect the performance of the skill classifier? \n\n5) Is there a reason to not standardize the size of the medium-replay dataset, using the same construction described? This would make the results more comparable to the other datasets.\n\n[1] Eysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2018, September). Diversity is All You Need: Learning Skills without a Reward Function. In International Conference on Learning Representations.\n\n[2] Haarnoja, T., Hartikainen, K., Abbeel, P., & Levine, S. (2018, July). Latent space policies for hierarchical reinforcement learning. In International Conference on Machine Learning (pp. 1851-1860). PMLR.\n\n[3] Iqbal, S., & Sha, F. (2019, May). Actor-attention-critic for multi-agent reinforcement learning. In International conference on machine learning (pp. 2961-2970). PMLR.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper presents a novel method in an important and relatively unexplored area. It is clearly written and well organized, and has the experiments and ablations necessary for a high quality submission\n\n",
            "summary_of_the_review": "Overall, this paper proposes a novel idea with promising results in an important field, and it is well written and well supported. While I have some minor questions, I would recommend this paper for acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3946/Reviewer_E6Nd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3946/Reviewer_E6Nd"
        ]
    }
]