[
    {
        "id": "opt0uTqfzKS",
        "original": null,
        "number": 1,
        "cdate": 1666745265567,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666745265567,
        "tmdate": 1670109374685,
        "tddate": null,
        "forum": "Lt8bMlhiwx2",
        "replyto": "Lt8bMlhiwx2",
        "invitation": "ICLR.cc/2023/Conference/Paper4204/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work aims to utilize the encoded vision-and-language correlation in the trained CLIP embedding space to propose a DeCap method to perform zero-shot captioning. In implementations, this work first constructs an auto-encoder language model to learn to generate the given sentence based on the text embedding encoded by the off-the-shelf pre-trained CLIP. Then, this work further proposes a training-free projection mechanism to project the CLIP's visual embedding into the CLIP's text embedding space, which will be used in the trained auto-encoder language model to generate the final captions. The experiments on the image captioning and video captioning benchmark datasets show that the DeCap can significantly outperform previous methods under comparable settings.",
            "strength_and_weaknesses": "Strengths:\n1. The explored problem is valuable. The proposed approach is sound. The proposed training-free projection mechanism is interesting.\n2. The proposed approach can significantly outperform previous zero-shot captioning methods.\n\nWeaknesses:\n1. The idea of the proposed approach is similar to the existing work [1]. Meanwhile, the performances achieved by [1] are better than in this work. However, this work neither cites nor compares with [1]. Although it's acceptable that the proposed approach underperforms the previous work [1] in some scenarios, I still recommend the authors discuss the differences between this work and previous work [1] in Introduction and Related Work. It can help the readers better understand the contribution of this work.\n\n2. The fairness of the experiments should be clarified.\n- Compared with previous works, the approach proposed in this work adopts an external strong pre-training model, i.e., CLIP, and several external large-scale datasets, e.g., CC3M, to perform the zero-shot tasks. So I am wondering how many contributions of the achieved best results are brought by the proposed approach, instead of the CLIP and the additional datasets.\n- Could you report the performance of the proposed approach without the CLIP and/or CC3M?\n\n3. Some experiments are missing. \n- Could you perform a detailed ablation study of Equation (2)? That is, what are the performances without m_i^T v, i.e., \u2211w_i m_i? What are the performances of P(v_proj) instead of P(v_proj/||P(v_proj)||_2)\n- What are the performances of the fully-supervised DECAP model?\n- Could you visualize the learned representations, e.g., the visual representations, the projected visual representations, and the textual representations? It can better intuitively prove your arguments that the proposed approach can alleviate the modality gap problem.\n\nReference:\n[1] Aligning Source Visual and Target Language Domains for Unpaired Video Captioning. TPAMI, 2021",
            "clarity,_quality,_novelty_and_reproducibility": "The overall clarity and quality are good. The novelty is limited, an important reference should be discussed. The reproducibility of this paper is good: key resources (i.e., code and data) will be available upon publication and sufficient details of the experimental setup are described such that the researchers could be able to reproduce the main results. However, the fairness of the experiments should be clarified.",
            "summary_of_the_review": "Overall, the paper has provided extensive experiments to show that the proposed approach is effective in performing zero-shot captioning. However, the fairness of the experiments should be clarified.\n\n----After Rebuttal---\n\nI've read the rebuttal and the reviews of fellow reviewers. Many thanks for the detailed response. I have no further questions regarding this paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_CANm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_CANm"
        ]
    },
    {
        "id": "mFG-zJxdt-p",
        "original": null,
        "number": 2,
        "cdate": 1666837880328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666837880328,
        "tmdate": 1666837880328,
        "tddate": null,
        "forum": "Lt8bMlhiwx2",
        "replyto": "Lt8bMlhiwx2",
        "invitation": "ICLR.cc/2023/Conference/Paper4204/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the problem that the usage of GPT-2 for zero-shot captioning is not reasonable due to the task discrepancy between captioning and language modeling. To this end, this paper proposes the DeCap, which aims to decode sensible visual descriptions from the CLIP multi-modal embedding space. Specifically, the DeCap contains a pretrained contrastive model and a lightweight visual-aware language decoder taking the CLIP embedding as input. Expensive experiments on the MSCOCO, Flickr30k, MSR-VTT, and ActivityNet-Captions datasets show that DeCap can outperform previous state-of-the-art methods.",
            "strength_and_weaknesses": "Strength:\n* This paper is well-written and easy to follow. The paper provides sufficient technical details for readers to understand.\n* The results are very promising. The experiments on both zero-shot image captioning and zero-shot video captioning settings show that DeCap can outperform previous state-of-the-art methods.\n* The text reconstruction module in decoder training is simple and effective. And the projection-based decoding is alleviating the modality gap phenomenon.\n\nWeakness:\n* Lack of important references and comparations. The idea of text reconstruction is not the first work in the captioning area. The authors should compare the differences between the DeCap and [1][2].\n* The introduction claims that \u201cthe inference speed of these methods is slow because each word generation involves a CLIP text encoder forward\u201d. It is necessary to conduct inference speed experiments to make a claim more convincing.\n* It is not clear how the training-free projection mechanism can be beneficial to the modality gap. I suggest that use T-SNE to visualize the distribution of the v_proj, M, and v, it will be very interesting.\n\n\n[1] Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation. NeurIPS 2021.\n[2] Aligning Source Visual and Target Language Domains for Unpaired Video Captioning. TPAMI 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* This paper is well-written and easy to follow. \n* This work is simple but effective.",
            "summary_of_the_review": "Overall, I think this work is simple but effective, and shows promising results for zero-shot captioning. However, this paper missed some important references and some claims are imprecise. So, I tend to not fully accept the current version of this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_1suB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_1suB"
        ]
    },
    {
        "id": "uJXNBxKRIBo",
        "original": null,
        "number": 3,
        "cdate": 1667090667340,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667090667340,
        "tmdate": 1667090667340,
        "tddate": null,
        "forum": "Lt8bMlhiwx2",
        "replyto": "Lt8bMlhiwx2",
        "invitation": "ICLR.cc/2023/Conference/Paper4204/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors proposed a zero-shot captioning framework.  To enable the language-pre-trained decoder to generate captions based on visual inputs, the authors project the visual embedding into the text embedding space of CLIP. The authors perform projections with the help of support memory. The projected embedding is then considered as the prefix inputs to the decoder, and ask the decoder to generate captions.\n",
            "strength_and_weaknesses": "Strengths:\n1. The idea is good and interesting. The use of support memory is interesting.\n2. The paper is well-written, and it addresses practical problems of zero-shot captioning.\n3. The authors demonstrate good results on both image and video captioning benchmarks. \n\nWeaknesses:\n1. The decoder is actually trained using the text from caption datasets. In my opinion, as the model have seen caption-related training data, strictly speaking, it may not be appropriate to call it zero-shot captioning. It would be more convincing if the decoder is pre-trained on generic text corpus, but not image-caption related text corpus.\n2. Although the projection with support memory is encouraging, there is still a noticeable performance gap between supervised and unpaired trained results. These results might indicate that the projections may not keep the original visual information.\n3. While the method could be seen as training-free mechanism, the decoder seems need to be pre-trained using text data. \n4. It will add more values to the paper if the authors provide more in-depth studies of the support memory. Now the authors only discussed about the size of the support memory. For example, what type of embeddings are required as the support memory?\n\nOthers:\n1. There are some latex typos in page 9.\n2. I wonder if the authors can test their method on VATEX, which is a newer, larger video captioning dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear.",
            "summary_of_the_review": "The paper presents an interesting idea for \"zero-shot\" captioning. The key idea is to use the text embeddings as the support memory to help project visual embedding to the text embedding space of CLIP. The projected embedding is then considered as the prefix inputs to the decoder for generating caption. \n\nOverall, I think the paper is relative positive, and it could be a good reference for future research. The authors present a clever idea to enable such caption capability without training the encoder. Results on multiple benchmarks show that their method achieves better results than other zero-shot or unpaired baselines. \n\nMinor concern is about the term \"zero-shot\". I am not sure if the proposed method is strictly zero-shot, since the decoder have already seen the caption-related text during training. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_Sz68"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_Sz68"
        ]
    },
    {
        "id": "P7E_WeAFCuA",
        "original": null,
        "number": 4,
        "cdate": 1667141520879,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667141520879,
        "tmdate": 1670257818029,
        "tddate": null,
        "forum": "Lt8bMlhiwx2",
        "replyto": "Lt8bMlhiwx2",
        "invitation": "ICLR.cc/2023/Conference/Paper4204/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied how to use CLIP model for zero-shot captioning, i.e., no human-annotated image-text pairs. Previous works use large language models or pretrain the encoder-decoder network, which may not generate task-specific descriptions or data/computation consuming. This paper proposed a visual-aware language decoder, which (1) uses text data rather than image-text paired data for training, (2) use a memory module to project visual hint to prefix language embedding for inference. Experiments are conducted on image captioning (MSCOCO, Flickr30K) and video captioning (MSR-VTT, ActivityNet-Captions) datasets.",
            "strength_and_weaknesses": "Strengths\n\n\\+ The proposed method is simple but effective. Instead of end-to-end training/finetuning or leveraging additional large-scale language model, this work only fine-tune a language decoder on text-only data, which is more efficient.\n\n\\+ Experiments are conducted on both image captioning and video captioning datasets to validate the effectiveness of the proposed method, which is comprehensive and general. The proposed method achieves SOTA performance.\n\n\\+ The paper is well written and easy to follow.\n\nWeaknesses & Questions:\n\n\\- [Major concern] The technical contribution is not strong. Compared to the baseline CLIPRe, the main difference is to include a trainable language decoder. It is not surprising that using an addition module can improve the performance.\n\n\\- [Major concern] An important contribution of this paper is only using text-only data for language decoder training. However, the text data comes from CC3M and SS1M, while the images in the datasets are discarded. It is wired to discarded the images as the image-text pairs can be collected from the web. Also, collecting text-only task-specific descriptions is the same cost/time-consuming as collecting image-text descriptions from the web. It is interesting to know whether the proposed method can achieve better performance using the images to see the performance gap. This helps to clarify the importance of the motivation.\n\n\\- According to Figure 2, the captioning generation relies on a memory bank, whose capacity is over 10^5. Is this volume too large to affect the inference speed? Also, is there any visualization or analysis on the diversity of memory bank? More detailed explanation and analysis on the memory bank are expected. (Typo: \"Figure ??\" in \"The size of the support memory\")\n\n\\- In Section 3.1, the claim that \"adjusting the source of the training data, we can control the style of the generated sentences\" is questionable. Any model can change the generation style by adjusting the training data. It would be great to explain more on this claim.\n\n\\- Should $\\sum w_i * (m^T_i v) m_i$ in Eq. (2) be $\\sum w_i * m_i$?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The technological novelty of the module is not strong. Please see \"Strengths and Weaknesses\".",
            "summary_of_the_review": "Overall, this paper is a good application paper to adapt CLIP for zero-shot captioning. Considering the strengths and weaknesses, I am leaning towards borderline reject.\n\nThe strengths and contributions are (1) its strong performance on zero-shot captioning, (2) the writing quality, (3) the simple but effective idea.\n\nThe weaknesses and concerns are (1) the technical contribution is limited to extra language decoder and text-only data, (2) the analysis of memory bank is not comprehensive, (3) the concern of discarding image data in the image-text paired training data.\n\nI would increase my score if my concerns can be well addressed.\n\n\n================== After rebuttal ======================\n\nAfter reading the authors' rebuttal and other reviewers' comments, I decided to increase my score to 6.\n\nThe authors addressed most of my concerns well, especially the analysis of inference speed, the number of support embeddings, updated claims of motivations and contributions. I hope that the authors could further revise the paper to include these discussion to make the paper more well-motivated and clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_oCpD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_oCpD"
        ]
    },
    {
        "id": "0GRMYrxlOO4",
        "original": null,
        "number": 5,
        "cdate": 1667146221112,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667146221112,
        "tmdate": 1669933970698,
        "tddate": null,
        "forum": "Lt8bMlhiwx2",
        "replyto": "Lt8bMlhiwx2",
        "invitation": "ICLR.cc/2023/Conference/Paper4204/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a method named DeCap, for zero-shot captioning. The method firstly utilize the CLIP text encoder, to train from-scratch a new text decoder that can reconstruct the text sentence input. Thus, the trained text decoder can be attached to a CLIP image encoder to generate captions from visual input. Due to the modality gap between CLIP image encoder and text encoder, the embedding space of images and texts are actually far away. Another contribution from this work is to mitigate the modality gap, by using projection-based decoding. It uses a support memory with text embeddings, and project image embeddings to the support text embeddings and sum up as the condition text embedding for the text decoder. Extensive experiments are conducted on image captioning tasks and video captioning tasks. ",
            "strength_and_weaknesses": "Strength:\n\n(1) This paper proposed a very simple and intuitive idea of making use of pre-trained CLIP image and text models for captioning tasks. CLIP was not trained for generative tasks, which has been bridged by a very lightweight text decoder in this work. The demonstration of this idea in the paper is very easy to follow.\n\n(2) Extensive experiments are conducted on both image captioning tasks and video captioning tasks.\n\nWeakness:\n\n(1) The major concern is about the slowness when adopting a large support memory. In section 4.1, the authors mentioned they used one million descriptions randomly sampled from the 3M descriptions to construct the support memory. According to equation 2, this indicates one million calculations for the v_proj embedding vector. It sounds too expensive to be useful in practice. The ablation test in Figure 2 is appreciated, but that doesn't address the concern about the slowness. The speed analysis of the proposed DeCap model is missing.\n\n(2) There lacks a few baselines as discussed in Section 3.2.2 in a few tables. For example, it would be nice to add VD and NND to Table 1, 3 and 4. The reason is that VD baseline doesn't require (expensive) visual embedding projection, if it achieves sometimes competitive results as DeCap, that could be a very strong and solid baseline.\n\n(3) Although the experiments are very broad and cover diverse tasks, the analysis of the proposed method is not thorough. A few questions are still remaining after reading this paper: (a) what would be the results without projection-based decoding (PD)? I can imagine it's not doing great according to Figure 2, but it's nice to add this missing number to make it complete. (b) Are there any other methods you have tried other than PD, to bring the image and text embeddings closer? (c) How does DeCap perform in a supervised or few-shot setup? For example, could DeCap benefit from additional supervised paired image and text (i.e. in the support memory)?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed idea is simple, the paper is well written and it is very easy to follow. There are few equations and all of them are well discussed. It should not be too hard to reproduce the idea proposed in this paper, but however the inference speed with support memory may be too slow to become useful in practice. The idea of adapting CLIP models to image captioning tasks have been explored in the past, while the exact DeCap idea proposed in this paper is original.\n\nSmall typo: Section 4.4: Figure ?? (right) -> Figure 2 (right)",
            "summary_of_the_review": "This paper proposed a simple idea named DeCap to apply CLIP models to captioning tasks. Due to a few concerns (not useful in practice, lacking in-depth discussions) as mentioned in the previous sections, my initial rating is below the threshold. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_MHPt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_MHPt"
        ]
    },
    {
        "id": "CfxBQIZvx9",
        "original": null,
        "number": 6,
        "cdate": 1667557186296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667557186296,
        "tmdate": 1669829764173,
        "tddate": null,
        "forum": "Lt8bMlhiwx2",
        "replyto": "Lt8bMlhiwx2",
        "invitation": "ICLR.cc/2023/Conference/Paper4204/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new pipeline for zero-shot captioning. It first establishes a text decoder to inverse the text embedding from the CLIP to sentences. The authors further develop an embedding projection technique to project the image embedding to a weighted sum of the memorized text embeddings, which can significantly reduce the modality gap. ",
            "strength_and_weaknesses": "Strengths:\n\n1. The proposed method is compact yet effective.\n\n2. DeCap outperforms existing work in a zero-shot setting with a noticeable margin. The authors compare algorithms in four different settings: zero-shot captioning, in-domain captioning, out-domain captioning, and video captioning. The results from these experiments prove the superiority of the proposed method.\n\n3. Ablation study provides some useful and interesting discoveries.\n\nWeaknesses:\n\n1. The proposed method seems to cost a lot of time during inference. Authors should provide the memory cost and inference time and compare them with other existing works.\n\n2. The paper should be further proofread.\n\n    - In the ablation study \"The size of the support memory\", the crossref for Figure 2 is missed.\n    - \"does not requires\" -> \"does not require\"\n    - \"ESPER-Free use reinforcement\" -> \"ESPER-Free uses reinforcement\"\n\n3. Is Decap sensitive to the hyper-parameter $\\tau$ is the proposed method? I hope the authors can discuss more the choice of this hyper-parameter.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: well\n\nQuality: well\n\nNovelty: well\n\nReproducibility: looks OK",
            "summary_of_the_review": "An interesting study with some novel designs and extensive experiments. Although I appreciate that authors have evaluated their method on a variety of different tasks, I think it still lacks some important experiments, mainly about inference efficiency and sensitivity check. I'll adjust my score after the discussion.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_9VQ6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4204/Reviewer_9VQ6"
        ]
    }
]