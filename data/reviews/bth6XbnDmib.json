[
    {
        "id": "ExP2PRQ0M85",
        "original": null,
        "number": 1,
        "cdate": 1666617011084,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617011084,
        "tmdate": 1666617137553,
        "tddate": null,
        "forum": "bth6XbnDmib",
        "replyto": "bth6XbnDmib",
        "invitation": "ICLR.cc/2023/Conference/Paper5254/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduced a coreset that provably approximates any function that can be represented by an RBFNN architecture, and use the coreset to suggest a provable data subset selection algorithm for training deep neural networks. The better performances of their algorithms are verified by some experiments. \n\n",
            "strength_and_weaknesses": "Although the experiment results seem promising, the technical parts may not seem very solid, which makes me have some concerns about the correctness of the results. For example,\n\n1. In Theorems 4 and 6 (and somewhere else), the authors write something like $f(n)\\in O(n)$, this is not the standard notation, it should always be $f(n)= O(n)$ in mathematics.\n\n2. In Theorem 4, actually the author prove that $\\sum s(p) \\geq n/2$. In this case, the results should be $\\sum s(p) = \\Omega (n)$ (the big Omega notation), not $\\sum s(p) \\in O (n)$. \n\n3. The formula in Claim 5 is not correct since letting $R=1$ will give $e/2\\leq 1$, this can not be true.\n\n4. The right hand side of formula in Claim 5 could be $1$, since $e^{-|p^T \\cdot x|} \\leq 1$. I don't understand why it writes $e^{-|p^T \\cdot x|} \\leq 1 + |p^T \\cdot x|$.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: This paper is not very technically sound.\n\nClarity: This paper is well organized. I find it easy to follow.\n\nSignificance: I think the results in this paper are not very significant. ",
            "summary_of_the_review": "Due to many confusions in the technical parts, as explained above, I tend not to accept this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5254/Reviewer_zVhj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5254/Reviewer_zVhj"
        ]
    },
    {
        "id": "kE7Fn-vkoJd",
        "original": null,
        "number": 2,
        "cdate": 1666683364861,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683364861,
        "tmdate": 1666684116685,
        "tddate": null,
        "forum": "bth6XbnDmib",
        "replyto": "bth6XbnDmib",
        "invitation": "ICLR.cc/2023/Conference/Paper5254/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an algorithm for coreset selection with low computation time. The key idea is to approximate a function using RBFNN on a large dataset and then construct coresets for radial basis and Laplicain loss function. Empirically, the authors show that the proposed method can find a small coreset with competitive performance with the existing baseline methods on MNIST, CIFAR-10, and CIFAR-100 datasets.\n",
            "strength_and_weaknesses": "As detailed below, the paper is well-written and I believe that the idea presented in this paper is novel. Note that I am not familiar with the background and did not fully understand the mathematical justification. Hence, my most concerns lie in the experiment section:\n\n- While the paper is generally easy-to-follow, the motivations in Section 1 are unclear.\n- The paper does not sufficiently describe the limitation of the proposed approach. What are additional hyperparameters that need to be tuned (e.g., $R$) and how robust are they? The paper briefly mentions that the algorithm is robust to these hyperparameters, but did not empirically verify this. \n- The performance seems to be limited for CIFAR-100 and MNIST datasets. \n- The paper does not describe the experiment details and it would be challenging to reproduce the experiment. Moreover, the details on how the hyperparameters (e.g., learning rate) are selected are missing.\n- The evaluations were done on three relatively small datasets and architectures (e.g., MNIST, CIFAR-10, and CIFAR-100) and it is unclear if the method will scale to large settings. ",
            "clarity,_quality,_novelty_and_reproducibility": "I believe that the idea of selecting the coreset with RBFNN is interesting and novel. Moreover, the paper is generally well-written although some parts of the paper are not easy-to-follow due to organization. I believe that the quality of the paper meets the standard of the ICLR. \n\nThe authors did not provide a code or details of the experiment set-up and I believe that it would be difficult to reproduce the experiments at the current state. \n\nBelow, I left some additional comments:\n- I found the description in Section 1 - coresets to be confusing at first. A more description of the query selection before showing the equation would be useful. \n- In addition, I think that the reorganization of the materials presented in Section 1 would be helpful. At the moment, the motivation for the work is not clear when first reading the introduction. \n- While I believe that Figure 1 is extremely useful to capture the core contribution, there needs to be a detailed description. At the moment, the main text does not mention Figure 1.\n- In my current understanding, the algorithm proposed in the paper could be split into two parts: RBFNNs and coreset selection. Would it be possible to set up an experiment that confirms the accuracy of each method (in step)? \n\nMinor comments:\n- While I believe that Figure 1 is extremely useful to capture the core contribution, there needs to be \nIn Section 5, \u201c... after each gradient update to support there theoretic\u2026\u201d \u2192\u201c... after each gradient update to support their theoretic\u2026\u201d.\n- There are several places where \\citet and \\citep are misused (e.g., Section 6, competing methods). \n- In Table 1, it would be helpful if the fastest model training time is bolded. The same applies to Table 2 and Table 3. \n- The resolution for figure 3 is bad. \n",
            "summary_of_the_review": "Overall, I believe that the idea presented in the paper is interesting and the paper is overall well-written. There are some concerns about the experiment analysis (e.g., the small number of baseline methods, evaluation on relatively small datasets and architectures, limited performance on CIFAR-100 and MNIST, and no ablation studies). I recommend 6. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5254/Reviewer_NHbT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5254/Reviewer_NHbT"
        ]
    },
    {
        "id": "TSCZWR-qgv",
        "original": null,
        "number": 3,
        "cdate": 1666709829075,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666709829075,
        "tmdate": 1666709829075,
        "tddate": null,
        "forum": "bth6XbnDmib",
        "replyto": "bth6XbnDmib",
        "invitation": "ICLR.cc/2023/Conference/Paper5254/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose the first coreset construction algorithm for RBFNNs, i.e., a small weighted subset which approximates the loss of the input data on any radial basis function network. This is achieved by constructing coresets for radial basis and Laplacian loss functions. The coreset is then used to develop a provable data subset selection algorithm for training deep neural networks, since the coreset approximates every function. Experimental results on function approximation and dataset subset selection on popular network architectures and data sets are presented to demonstrate the effectiveness of the algorithm in certain cases.",
            "strength_and_weaknesses": "Strength\n\n- This paper develops the first coreset construction algorithm for RBFNNs and Laplacian cost function. These properties of RBFNNs are further leveraged to approximate the gradients of any deep neural networks.\n\n- The authors also develop provable theoretical guarantees for the proposed coreset construction algorithm. \n\nQuestions:\n\n- In the abstract, `notoriously known' should be `well known'\n\n- The authors study RBFNNs and use them to help data selection as they can approximate any continuous function on a closed bounded set with arbitrary precision given enough hidden neurons. But many other neural networks are also universal approximators, I am wondering what is the advantage to use RBFNNs and bridge them with general deep neural networks.\n\n- In the experiments, the authors compare with some SOTA baselines with warm start, i.e., training on the whole data for 50% of the training time before training the other 50% on the coreset. It is shown that the proposed algorithm outperforms these SOTAs without warm start, but worse than with warm start. As warm start is a good strategy used in practice, I suggest the author also add the proposed algorithm + warm start to the comparison.\n\n- How does this approach compare to the clustering-based data selection as discussed in the recent work https://arxiv.org/pdf/2206.14486.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "Good",
            "summary_of_the_review": "see above",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5254/Reviewer_A6tT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5254/Reviewer_A6tT"
        ]
    },
    {
        "id": "R3PxaVlvJ01",
        "original": null,
        "number": 4,
        "cdate": 1666738996482,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666738996482,
        "tmdate": 1666738996482,
        "tddate": null,
        "forum": "bth6XbnDmib",
        "replyto": "bth6XbnDmib",
        "invitation": "ICLR.cc/2023/Conference/Paper5254/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a coreset construction algorithm for RBFNNs. The coreset corresponds to a data subset selection algorithm for training deep neural networks. Such a coreset can provably approximate any function that can be approximated by a given RBFNN. The proposed algorithm is efficient and the results are empirically verified.",
            "strength_and_weaknesses": "Strength:\n\n1. The authors propose a coreset construction algorithm for the RBF and Laplacian cost functions, which is efficient and in theory can be used to approximate the gradients of any deep neural networks.\n\n2. The proposed algorithm construction leverages existing theoretical results; and the authors provide further theoretical analysis for RBF and Laplacian loss functions.\n\n3. The proposed algorithm is efficient - the coreset can be computed before each epoch with negligible time by sampling according to the sensitivity distribution.\n\n4. Extensive empirical evaluation that demonstrates the efficiency and effectiveness of the proposed algorithm.\n\nQuestions and weaknesses:\n\n1. I'm curious about the experiment results - from Tables 1 and 2, the proposed RBFNN Coreset seems to be inferior to some other methods (e.g., GradMatchPB-WARM) for both accuracy and training time. Is this difference mainly coming from the warm start? If so, would warm start help to improve the performance of the RBFNN Coreset?\n\n2. Why is the training time with a warm start shorter than the one without (e.g., Table 1)? Shouldn't train on the whole dataset increase the training time?\n\n3. The standard deviation of the empirical results is not reported - I'm not fully convinced the differences are statistically significant.\n\nMinor issue:\n\nIn definition 2 where the function $U(p)$ is defined, the $P$ should be $p$?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the proposed idea seems interesting and novel.",
            "summary_of_the_review": "This paper presents an interesting coreset construction for RBFNNs, and the authors provide solid theoretical analysis and empirical evaluation for the proposed algorithm.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5254/Reviewer_zk6H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5254/Reviewer_zk6H"
        ]
    }
]