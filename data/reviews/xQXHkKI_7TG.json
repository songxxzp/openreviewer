[
    {
        "id": "XrJpna1FVH",
        "original": null,
        "number": 1,
        "cdate": 1665959107615,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665959107615,
        "tmdate": 1665959107615,
        "tddate": null,
        "forum": "xQXHkKI_7TG",
        "replyto": "xQXHkKI_7TG",
        "invitation": "ICLR.cc/2023/Conference/Paper1564/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper deals with a specific setting of Federated Learning where the goal is to find an optimal weight between all the local objectives so that the learned parameter can be optimal for some different global task. To do so, this paper first formulates the above setting as a bi-level optimization problem over the weight parameter and the model parameter. Then, to solve this bi-level optimization problem, the paper reformulates the bi-level problem into an optimization problem with equality constraint using convexity assumption. Then the paper introduces a primal-dual algorithm and gives convergence guarantee under the case where each local objective function is strongly convex. Since it is possible to encounter a constrained optimization problem with non-linear equality constraints, the paper proposes a new Lagrangian formulation with an augmented Lagrangian term. They shows that due to this new term, the primal-dual optimizer can converge to the KKT point of the original bi-level optimization problem.",
            "strength_and_weaknesses": "Strength:\n\n1. The presentation is good and easy to follow.\n\n2. The mathematics seems correct (though I did not check all the proof details).\n\n3. The problem itself (i.e. equation 1) is meaningful.\n\n4. Extensive experiments are conducted to support the theory. \n\n---\nWeakness: \n\nNot well-motivated: I indeed believe problem eq (1) is a very meaningful problem. But I do not think there is enough motivation in this paper. \n\ni) the original motivation is that one would like to train a model parameter that is good for some global task $f_0$. This makes sense but why not directly optimize for $f_0$? The reason is (my understanding) there is not enough data for optimization w.r.t. $f_0$ directly and therefore we need some pre-training using all local clients\u2019 data. Motivation like this should be emphasized. \n\nii) under the same motivation, equation 1 is only one of the many ways to formulate the problem. It would be necessary to discuss alternative problem formulation as well. For example, why not first pretrain on local clients data using federated learning and then fine-tuning on few-shot data from task $f_0$? \n\niii) When I first read, it was not immediately unclear to me why the estimation of Hessian in the previous methods is hard to implement under this paper\u2019s setting due to the large amount of data. My understanding is one cannot get an unbiased estimator of the Hessian. Issues like this should also be discussed in more depth because it motivates the paper. \n\n2. The paper considers the case where all the $f_i$\u2019s are strongly convex, which is the most simplified case. That is fine. But the paper did not explicitly discuss the unique technical challenges under this setting. It is hard to justify the technical novelty given the current writing. I do believe the augmented Lagrangian term is novel but it is hard to judge given the current writing. Besides, the algorithmic design section is also hard for me to spot what is the novelty. I think the authors should emphasize the technical challenges and novelty. \n\n---\nQuestions: \n\n1. Motivate problem formulation given by equation (1). Especially, what is the relation between this problem and meta-learning? In Meta-learning, one would like to find a model parameter that is good for (possibly unseen) downstream tasks. This is clearly very related with this problem considered in this paper. \n\n2. In the paragraph below equation 5, the paper mentions that the term $\\tilde{\\nabla}^2 f_i(w_{t,k})$ is calculated with auto-differentiation. Explain why we need to calculate this term and why it is easier than the calculation of Hessian in previous approaches. \n\n---\nMinor comments/suggestions/typos:\n1. Typo in Alg 1 line 7.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: \nThe mathematics and experimental results are very clear. The motivation of the paper is not clear. I think more words need to be added to discuss the motivation of the problem.\n\nQuality: \nThe theoretical results and empirical results have high quality. The proof seems correct to me, though I did not check them all. \n\n\nNovelty: \nGiven the current writing, it is hard for me to detect enough novelty. I do think the augmented Lagrangian term is a novel design. However, I did not see enough discussion on the technical challenges and novelty. \n\nReproducibility: \nThe reproducibility is good.\n",
            "summary_of_the_review": "My current recommendation is reject. \nMy reasons are the following:\n\n1. The paper has some good theoretical results and empirical results. However, the current paper seems more like a rough composition of these results and does not have a very clear story for me to follow. I did not see a very clear motivation of the problem.\n\n2. I do not think problem 1 is a novel problem formulation. Therefore the paper should discuss its relation with some closely related problems like meta-learning. \n\n3. Technical novelty is not clear to me.\n\nOverall, I think the current draft requires a big amount of modifications to make it reaching the acceptance level. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1564/Reviewer_KXgD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1564/Reviewer_KXgD"
        ]
    },
    {
        "id": "qXG8s5cAAX",
        "original": null,
        "number": 2,
        "cdate": 1666272988271,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666272988271,
        "tmdate": 1666272988271,
        "tddate": null,
        "forum": "xQXHkKI_7TG",
        "replyto": "xQXHkKI_7TG",
        "invitation": "ICLR.cc/2023/Conference/Paper1564/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The author(s) formulated the objective of the federated learning problem as a bi-level optimization problem. The author(s) proposed an algorithm that solve the bi-level optimization problems with stochastic gradient oracle. Convergence analysis of the proposed algorithm is given and experiments on toy and real-world datasets are conducted to evaluate the proposed method.",
            "strength_and_weaknesses": "The major issue of this paper is the writing and presentation, I list some writing issues I found when reading the paper:\n- The first sentence is exactly the same as the first sentence of the abstract.\n- The citation format is not very careful, should use \\citep in some places, for example in Remark 1.\n- What is $w$ and $x$ in equation 1? Although these information can be recovered in the following context, it is still better to explain equation more clearly after equation 1.\n- Second paragraph of introduction: propose and algorithm -> propose an algorithm\n- Second paragraph of introduction: gradient calculate -> gradient calculation\n- Second paragraph of introduction: will involved -> will involve\n- Section 2.2, One Franceschi et al.., the sentence is informal\n- Section 2.2, Different from above work -> works\n- The paragraph below Proposition 1, For given x -> For given $x$.\n- What is \"LICQ\" in Proposition 2.\n- The paragraph below Proposition 3, constraint set ad $\\Lambda$ -> constraint set $\\Lambda$.\n- (A1), should be $f_0, \\ldots, f_N$ have $L_1$ gradient and are lower bounded by $\\underline{f}$.\n- (A4) looks weird, it looks like a conclustion instead of an assumption.\n- (A5) Better to express it with a math formula.\n- Proposition 4, when ..., then the stationary point....\n- The paragraph below Theorem 1, theorem1 -> theorem 1, Then, We -> Then, we\n- Remark 2, bilevel works, too informal, should be previous works on bilevel optimization.\n....\n\nSome other issues:\n- It is not clear to me why the bilevel formulation (problem 1) is important. Seem like an intuitive problem formulation without much theory behind it?\n- The author(s) stated that $f_0$ depends on the validation set, which is weird. In ML, validation set usually used for hyper-parameter tunning and should not be involved in the training.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity should be improved, the idea of the paper is novel to my knowledge, reproducibility is not clear.",
            "summary_of_the_review": "The writing and presentation of the paper should be improved. It is not clear if the bi-level formulation is important, evidences (either theoretical or empirical) for the problem formualtion is not given. The current version of the manuscript seems not good enough to appear in ICLR.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1564/Reviewer_Yrj6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1564/Reviewer_Yrj6"
        ]
    },
    {
        "id": "h1vmPyWLmZ",
        "original": null,
        "number": 3,
        "cdate": 1666838332184,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666838332184,
        "tmdate": 1666838332184,
        "tddate": null,
        "forum": "xQXHkKI_7TG",
        "replyto": "xQXHkKI_7TG",
        "invitation": "ICLR.cc/2023/Conference/Paper1564/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers training a global model on the server using updates receive from clients. The paper proposes that clients and the server collaborate to solve a bi-level optimization problem. This enables the server to learn the global model using the optimal parameter and combination of the local objectives.",
            "strength_and_weaknesses": "Strength:\n\n1. The paper proposes a new bi-level optimization algorithm for federated learning to find the optimal parameter with respect to local objectives.\n2. The paper provides the convergence guarantees for the proposed algorithm.\n\nWeaknesses:\n\n1. Using the proposed algorithm, the server has to perform more computations than other well-known existing algorithms. One of the benefits of federated learning is to push the computations from central servers to edge devices with good computational capability. This can adversely affect the applicability of the proposed algorithm compared to fedavg-based existing ones especially when the number of clients is large.\n2. Experimental results presented in the paper is not enough to show the effectiveness of the proposed algorithm. The paper mainly compares the proposed algorithm with bi-level optimization methods. However, I believe that it is better to compare the proposed algorithm with other fedavg-based algorithms in terms of both training time and test data accuracy. Also agnostic federated learning and personalized federated learning are closely connected to the study of the paper and it would be interesting if the paper compares the proposed algorithm with some personalized algorithms as well as agnostic federated learning.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper can be improved. Specifically it was difficult for me to understand 5 lines before problem (1) when I read that part first. I recommend that authors express that part in a simpler and easier to understand way. Also, there are some typos in the paper. For example in line 2 of section 2 on page 2, there is a typo. The proposed algorithm is novel. However, I could not check the proofs in the paper carefully.",
            "summary_of_the_review": "In summary, the paper proposes a novel federated learning algorithm with theoretical guarantees. However, the empirical study in the paper is not enough to show its effectiveness.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1564/Reviewer_BDrt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1564/Reviewer_BDrt"
        ]
    },
    {
        "id": "IvRcLeDmCF",
        "original": null,
        "number": 4,
        "cdate": 1667196954379,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667196954379,
        "tmdate": 1667196954379,
        "tddate": null,
        "forum": "xQXHkKI_7TG",
        "replyto": "xQXHkKI_7TG",
        "invitation": "ICLR.cc/2023/Conference/Paper1564/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on solving the local coefficient learning in federated learning as a bi-level optimization problem. Instead of directly solving the bi-level problem, they reformulate it as a convex-concave problem where they propose a double loop algorithm to solve it. The authors show that the proposed algorithm achieves the sample complexity of $\\mathcal{O}(\\varepsilon^{-4})$ to arrive at a $\\varepsilon$-stationary point. Experiments on synthetic and vision datasets are conducted to illustrate the performance of the new primal-dual appoach versus existing bi-level algorithms.",
            "strength_and_weaknesses": "Strengths:\n- The primal-dual approach presented in the paper appears to be new compared to existing work.\n- Theretical analyses are provided. I have gone through the key steps in the proof and find no problem but there's a chance I miss something.\n- Numerical experiments on synthetic and vision datasets show that the new appoach obtain promising performance.\n\nWeaknesses:\n- I have one major concerns which is the applicability of the proposed method in federated learning. I am not sure if the authors design the new primal-dual algorithm as a federated learning algorithm or not. If it is, then the local devices should send back their local models instead of the stochastic gradient/hessian. Therefore, I believe their algorithm instead is somewhat a distributed learning algorithm, not specifically for federated learning. I notice that only in this paper the bi-level optimization is applied to federated learning while others do not mention it.\n- Another concern is about the metric of evaluation. In federated learning, the main challenge is the communication efficiency (or the number of bits communicated between clients and server). As a results, other federated learning methods use performance over communication round or communicated bits as the metric of comparison. I think the metric in section 5 should be adjusted to illustrate this. Also, the payload of the primal-dual apporach can be twice as much as others such as BSA in 1 iteration so it might not be fair to use iteration as in the current paper.\n\nMinor comment:\n- Typo on page 9: when all local devices participant in -> participate in\n\nSuggestion for improvement:\n- I think it would be good to discuss the convergence rate obtained by the primal-dual approach vs existing work as I need to go back to their original paper to retrieve their rates.\n- I hope to see clarification whether the new algorithm is for federated learning since it does not follow federated learning framework where clients only send back local model instead of local gradients.\n- The metric of comparison for numerical experiments may need to be revised to match standard comparison in federated learning.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the writing of the paper is good as it is easy to follow. The theoretical results in the paper do have novelty compared to existing works. However, I need to see more discussion on how the new results stand compared with existing one, and whether the new approach is applicable in federated learning setting. The paper is associated with code for reproducibility which is a plus.",
            "summary_of_the_review": "As mentioned above, I believe the paper does have contribution in algorithmic design to solve the bi-level optimization but it is not clear whether the new algorithm is suitable for federated learning. Also, the comparison metric using performance in iteration may not be fair as the communication payload is different among the algorithms used in Section 5.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1564/Reviewer_Ycsn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1564/Reviewer_Ycsn"
        ]
    }
]