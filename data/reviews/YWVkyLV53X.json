[
    {
        "id": "srVgUI5MUV",
        "original": null,
        "number": 1,
        "cdate": 1666121989140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666121989140,
        "tmdate": 1669747815693,
        "tddate": null,
        "forum": "YWVkyLV53X",
        "replyto": "YWVkyLV53X",
        "invitation": "ICLR.cc/2023/Conference/Paper4080/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors demonstrate that performance on\nGLUE/Commongen/CommonsenseQA can be improved by selectively\nincorporating textual CLIP representations. This method outperforms\nprior vision-for-language methods like Vokenization. Ablations in the\nappendix substituting random/T5/etc. features for the CLIP features\nsuggest that this effect is not due to architecture choices, but\nsomething unique about the CLIP textual features.",
            "strength_and_weaknesses": "The question the authors address is one of the \"holy grails\" of\nvision-and-language research --- it seems like visual information\nshould be able to augment a text only model's capacity (sometimes\ncalled \"vision-for-language\"). Yet, the results to date haven't been\ndefinitive. Here, the authors results suggest that\nthe visually grounded textual representations of CLIP can do just that.\nThis method outperforms Vokenization and iACE, which are two prior\nvisual knowledge transfer to text-only tasks works, which is\nimpressive.\n\nMy biggest concern is that, despite the presented experiments, I am\nnot convinced that visual information is the reason why performance\nimproves. The authors presented a compelling case that CLIP textual\nfeatures uniquely improve performance vs. text-only alternatives. But\n--- does this really mean that visual knowledge transfer is occurring?\nFor example, what visual knowledge is required for SST-2, i.e., which\ninstances does the model improve performance over, and are those one's\nextra \"visual\"? The authors allude to color/shape/size information\nbeing what is transferred, but the experiments do not complete that\nargument.\n\nIt's a very difficult argument to make, and I commend the authors for\ntheir ambition in providing additional results in this sometimes\ndifficult-to-analyze domain. I appreciate the ablations the authors\nran in the appendix. And yet, despite these efforts, the core\nconclusion of the paper requires that visual knowledge is why CLIP\ntext representations do better --- I am still somewhat unconvinced.\n\nAlternate explanations that are not possible to rule out include:\n\n- CLIP's pretraining dataset ImageWebText, contains different text\n  compared to the usual language-only models. Maybe the text-only\n  information there is why they help? Prior work on commongen suggests\n  incorporation of additional textual information can improve performance:\n\n@inproceedings{wang-etal-2021-retrieval-enhanced,\n    title = \"Retrieval Enhanced Model for Commonsense Generation\",\n    author = \"Wang, Han  and\n      Liu, Yang  and\n      Zhu, Chenguang  and\n      Shou, Linjun  and\n      Gong, Ming  and\n      Xu, Yichong  and\n      Zeng, Michael\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021\",\n    month = aug,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.findings-acl.269\",\n    doi = \"10.18653/v1/2021.findings-acl.269\",\n    pages = \"3056--3062\",\n}\n\n- CLIP's contrastive objective at the level of sentences is different\n  enough from language modeling objectives such that any contrastive\n  model's features might add predictive accuracy.\n\n- The authors ran a large number of permutations of their model ---\n  could it be that the performance gains of a few accuracy points are\n  due mostly in part to better hyperparameter optimization?\n\nWhile definitively making the visual knowledge transfer case is\nperhaps beyond the scope of this work --- I nonetheless could envision\na number of missing additional experiments in this setup:\n\n- Which words does the model select as \"visually hungry\"? Do they\n  align with the syntax-based method? Do they align with human judgments\n  of visual-ness, e.g., from:\n\nDouglas L. Nelson, Cathy L. McEvoy, and Thomas A.\nSchreiber. 2004. The University of South Florida free association,\nrhyme, and word fragment norms.  Behavior Research Methods,\nInstruments, & Computers, 36(3):402\u2013407.\n\n- Which instances across the NLP tasks does performance improve most\n  for? Are these cases where we would expect visual information to\n  help?",
            "clarity,_quality,_novelty_and_reproducibility": "Selective incorporation of CLIP features into a text-only model is an interesting idea that I haven't seen before. The experiments are extensive and straightforward to understand.",
            "summary_of_the_review": "\nOverall --- the authors make some promising experimental steps towards\nan exciting goal: showing that \"visual knowledge\" can be transferred\nto text-only models with performance improvements for downstream\ntasks.  However, the argument is incomplete: there are plausible\nalternative hypotheses that could explain CLIP features helping\nlanguage models beyond \"visual knowledge:\" and, some experiments that\ncould quantify additional aspects of the potential visual knowledge\ntransfer are missing.\n\n=== after response:\n\nThe new experiments the authors ran to address my core concern are interesting initial steps, and I raised my score accordingly. However, I am still not 100% convinced that visual information is the reason why CLIP base features are better. 400M image captions is quite a lot of novel data, which might be the bottleneck in the scaling regime the authors consider in their additional creative experiment (first one described in response to YcbG). It's a great initial type of experiment to run, but, because the authors are quite ambitious in their work, the extraordinary claim of visual knowledge transfer, for me, requires more evidence than just this initial result on ALBEF.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4080/Reviewer_TuCV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4080/Reviewer_TuCV"
        ]
    },
    {
        "id": "CnbbwNIBqqu",
        "original": null,
        "number": 2,
        "cdate": 1666509854495,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666509854495,
        "tmdate": 1666509854495,
        "tddate": null,
        "forum": "YWVkyLV53X",
        "replyto": "YWVkyLV53X",
        "invitation": "ICLR.cc/2023/Conference/Paper4080/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to integrate visual information into pre-trained language models during fine-tuning when applied to pure-language tasks. Different from previous work that either uses retrieved or generated images, they propose to leverage the CLIP text encoder to obtain the image-aligned text representations of certain input words, and then feed the language models with these additional representations when fine-tuned on language tasks. They demonstrate improvements over baselines on text classification, commonsense classification/generation, and visual entailment tasks.\n\nSpecifically, they propose three strategies to extract \"visually-hungry words\" (VH words): 1) using spacy to extract the POS of each word and treating nouns and adjectives as VH words; 2) using the attention scores of the CLIP text encoder and selecting the top-k words with the highest attention scores with the EOS token; 3) training a neural network with Gumbel-Softmax to select the VH words automatically.\n\nAfter they extract the VH words, they use the CLIP text encoder with prompts to get the image-aligned representations of these words and feed them to their language models.",
            "strength_and_weaknesses": "Strengths:\n1. The paper focuses on how to improve the model performance on pure-language tasks by introducing visual information, which is an important research direction yet has not been widely studied yet.\n2. The idea of not using either retrieved or generated images for integrating visual information is novel and interesting, as it has been pointed out by the paper that sometimes the original text representations can be polluted using previous methods.\n3. They conduct experiments across various tasks and demonstrate improvements over their baselines.\n\nWeaknesses:\n1. Their ways of extracting \"visually-hungry\" words are not technically sound and sometimes even arbitrary to me. For example, for their syntax-based strategy, they simply use an external tool to extract all the nouns and adjectives from sentences. As mentioned in the paper, not all nouns and adjectives are related to visual semantics, and using external tools can cause error propagation issues, yet this is their default strategy for some tasks as the other methods is computationally more costly or achieves similar performance. More technically sound approaches should be proposed, and the authors should conduct qualitative and/or quantitative analyses on the extracted VH words.\n2. Their evaluation settings are questionable. It is unclear why they chose to report numbers on 6 GLUE tasks whereas most previous work would report results on 8 or 9 GLUE tasks. In addition, for GLUE tasks, they use a relatively weak baseline from Tan et al. [2020], but they should also include the baselines from the original BERT/RoBERTa papers and build their models on top of them so that the readers can be more clear about how their models perform.\n3. There are few analyses on why their model can achieve improvements over pure-text models. They should include more analyses in this regard. For example, they can include ablation studies on whether the improvements are because of ensembling different text encoders, on what kinds of sentences/tasks their model can outperform the baselines the most, what if there are no visually-hungry words or their models extract wrong VH words, etc.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow as their proposed methods are relatively simple.\n\nThe idea of not using either retrieved or generated images for integrating visual information is novel and interesting, as it has been pointed out by the paper that sometimes the original text representations can be polluted using previous methods.\n\nThey claim that their code, data, and models will be publicly available.\n\n",
            "summary_of_the_review": "Overall, while the idea of using an image-aligned text encoder to provide visual information for language tasks is interesting and may have potential, the currently proposed methods are not technically sound and the evaluation settings are questionable (the details of these are listed in \"strength and weakness\"), therefore, I am leaning towards a rejection of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4080/Reviewer_A4xX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4080/Reviewer_A4xX"
        ]
    },
    {
        "id": "y_EYuyzuSzZ",
        "original": null,
        "number": 3,
        "cdate": 1666682677741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682677741,
        "tmdate": 1666682677741,
        "tddate": null,
        "forum": "YWVkyLV53X",
        "replyto": "YWVkyLV53X",
        "invitation": "ICLR.cc/2023/Conference/Paper4080/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed to inject visual information into pre-trained language models without retrieved or generated images. Instead, it detects visually-hungry words and generates their visual representation by CLIP text encoder, and injects them back into a pre-trained language model. Experiments are done in various datasets and recent methods are compared.",
            "strength_and_weaknesses": "Strengths:\n1. Compared to recent works of Visually-augmented PLM, this work doesn't need to retrieve or generate images, which is much faster in training/inference and more straightforward in methodology.\n2. Experiment section covers many different datasets and previous methods. The proposed method can outperform others and baselines by a satisfactory margin.\n\nWeaknesses:\n1. Some detail of the visually-hungry word extraction is not clear to me. \n- a) In the learning-based strategy, when back-propagation, will the VL-PTM (CLIP in your case) and PLM all be updated? Or just the MLP is updated?  \n- b) In those three strategies, will the visual bi-gram or phrase be detected one unit? For example, in the case of 'He is eating a green apple', 'green apple' should be detected as one phrase instead of two words. In the following CLIP text encoding, 'green apple' should also be encoded as in one sentence instead of encoding 'green' and 'apple' separately. \n2. In experiments, why T5-3B model is missing in NLU tasks while it's used in other tasks?\n3. The details about the experimental setting of `+retrieved images` in Tab2, 3 are missing.  What's the image database? How many images are retrieved for each VH token? How is it different from other retrieving-based methods such as iACE?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and easy to understand.\nNovelty is good and it should be easy to reproduce.",
            "summary_of_the_review": "Overall, I like the straightforward idea and it also works well. Some details and experiments are missing but I still lean to accept it.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4080/Reviewer_4pzW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4080/Reviewer_4pzW"
        ]
    },
    {
        "id": "OPR7VvtG74y",
        "original": null,
        "number": 4,
        "cdate": 1667183219616,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667183219616,
        "tmdate": 1667183219616,
        "tddate": null,
        "forum": "YWVkyLV53X",
        "replyto": "YWVkyLV53X",
        "invitation": "ICLR.cc/2023/Conference/Paper4080/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed to leverage pretrained vision-language models like CLIP to augment the pretrained language models with visual knowledge by injecting embeddings generated from the text encoder in CLIP for words that may be lack of visual knowledge. Effectiveness of the propopsed method is verified on various benchmarks on natural language understanding, commonsense reasoning, text generation and visual entailment.",
            "strength_and_weaknesses": "Pros:\n1. The proposed idea is quite intuitive and straightforward.\n2. The evaluation is extensive on ten NLP tasks.\n\nCons:\n1. Insufficient analysis to identify the true contribution of performance gain.\na. The baselines in Table 5 should be actually included for the other evaluations as it is important to know whether the proposed method is only significantly effective on other tasks/datasets.\nb. It is unclear whether stronger VL models lead to better performance when used in the proposed method.\nc. Most importantly, despite the performance gain of CLIP text encoder as reported in Table 5, it is unclear whether this really comes from \"seeing the visual world\". What if the knowledge source actually comes from the captions? Therefore, it is important to add baselines to justify this problem. For example, using image captions to pretrain a language model as the knowledge source; retrieve some image captions as the knowledge source.\n2. Additionally, could the augmented embeddings be interpreted? Are there any evidence from the interpretation really also supporting the claims?\n3. The extra computation and latency introduced by the proposed method should be also provided to help the audience understand better the trade-off between visual augmentation and efficiency.\n\n\nMinor:\nSecond to last paragraph in introduction,  \"improveing\".",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, quality and reproducibility look overall good. The technical part is not entirely novel but certainly interesting.",
            "summary_of_the_review": "Although the reviewer thinks the proposed idea might be interesting to the community, the analysis on the real source of performance gain is not enough to really support the claim.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4080/Reviewer_YcbG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4080/Reviewer_YcbG"
        ]
    }
]