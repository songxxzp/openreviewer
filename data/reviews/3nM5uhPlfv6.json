[
    {
        "id": "NE8RR8nZf",
        "original": null,
        "number": 1,
        "cdate": 1665717952060,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665717952060,
        "tmdate": 1665717952060,
        "tddate": null,
        "forum": "3nM5uhPlfv6",
        "replyto": "3nM5uhPlfv6",
        "invitation": "ICLR.cc/2023/Conference/Paper346/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a fair learning algorithm with DP guarantees. The algorithm owns a provable \u201cconvergence\u201d guarantee and can deal with multi-class, multi-sensitive attributes classification tasks.",
            "strength_and_weaknesses": "The problem studied in this paper (i.e., fair learning algorithm with DP guarantees) is important and significant. Overall, this paper is easy to follow. The new algorithms introduced in this paper look interesting. \n\n\nMy main concerns are listed below:\n\n**Privacy mechanism**: it is unclear to me why the DP is defined only in terms of the sensitive attribute S. This may be vulnerable: when S is correlated with the rest of the features, an adversary can potentially reconstruct it from the rest of the features. \n\n**Convergence guarantee**: The convergence guarantee (e.g., Theorem 3.2) is different from the typical convergence results. First, it only guarantees the existence of T (number of iterations), eta_theta, and eta_w (step size) such that the results hold. Hence, it is very likely that the (T, eta_theta, eta_w) a data scientist chooses does not have any convergence guarantee. Second, it is unclear to me why the result is only derived for the gradient of Phi. This is not even the gradient used for updating the parameters. Ideally, the convergence results should tell us the convergence of theta and w. Finally, the authors did not provide any convergence rate so it is unclear how the algorithm converges w.r.t. the number of iterations. \n\n**Estimation error** it seems that the algorithm introduced by the author depends on estimating the joint distribution of \\hat{Y} and S. It is unclear to me how the estimation error influences the performance of the algorithm. In particular, since fairness is defined through chi-square divergence and the accuracy of estimating chi-square divergence relies on the support set, I am a bit worried about how the estimation error propagates to the learning algorithm. This problem is important because this paper considers the setting of multi-class, multi-sensitive attributes (I assume this is one main contribution of this paper). Hence, the support of \\hat{Y} and S can be very large. \n\n**Misleading statement** the authors wrote \u201cif sex is the sensitive attribute and a data set contains all men, then any classifier is trivially fair with respect to sex, so fairness with respect to sex is meaningless for that data set.\u201d I find this sentence very misleading! The authors should revise this statement.\n\n**Novelty** It seems that many results in this paper rely on prior work [Lowy et al., 2021]. \n\nOther comments:\n\nThe authors may consider discussing what group fairness measures can be covered in this paper. For example, can the learning algorithm cover false discovery rate and calibration error?\n\nMinor comment: D_R(\\hat{Y}, S) is chi-square information. Calling it Renyi mutual information may not be accurate\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is easy to follow. Some results in this paper rely on prior work [Lowy et al., 2021]. It is also unclear if the convergence guarantee derived in this paper is useful in practice.",
            "summary_of_the_review": "Interesting paper but many concerns need to be addressed",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns",
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "Some statements in this paper may be misleading.\nQuote from the paper: \"if sex is the sensitive attribute and a data set contains all men, then any classifier is trivially fair with respect to sex, so fairness with respect to sex is meaningless for that data set.\"\nThis statement makes me uncomfortable. A data set contains all men DOES NOT mean any classifier trained from this data set will be fair. In fact, lacking sample diversity could be one big issue in practice. ",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper346/Reviewer_8e2X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper346/Reviewer_8e2X"
        ]
    },
    {
        "id": "tX3PKJG-ylc",
        "original": null,
        "number": 2,
        "cdate": 1666107192296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666107192296,
        "tmdate": 1670338121671,
        "tddate": null,
        "forum": "3nM5uhPlfv6",
        "replyto": "3nM5uhPlfv6",
        "invitation": "ICLR.cc/2023/Conference/Paper346/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies how to learn models that satisfy various fairness desiderata (e.g., demographic parity and equalized odds) while satisfying differential privacy. Like prior work, it writes down a loss function encoding some weighted combination of accuracy and fairness constraints and aims to develop a private method for optimizing it. The proposed \"DP-FERMI\" solution does so using DP-SGD. The claimed novelty lies in its convergence analysis, which appears to be the first of its kind for DP-SGD applied to a fairness-constrained loss. The key technical step relies on the \"exponential Renyi mutual information\" loss developed by (Lowy+ 2021) and used in that paper to derive a similar convergence result for stochastic/mini-batch (non-DP) SGD. While other papers have studied similar DP-SGD-based approaches to optimizing a fairness-constrained loss, those works did not attempt to theoretically analyze convergence. This paper concludes with some experiments demonstrating that DP-FERMI obtains better empirical tradeoffs between fairness and accuracy loss at the same privacy levels as existing baselines.",
            "strength_and_weaknesses": "Strengths: The DP-FERMI formulation seems like a neat idea for getting to a convergence analysis. The paper is generally easy to follow (though there is a sudden drop-off in clarity when explaining technical bits, see below). The empirical performance looks like a nontrivial improvement over existing baselines.\n\nWeaknesses: The application of FERMI is not obviously a large improvement over its introduction in (Lowy+ 2021): we want to optimize a weird fairness-constrained loss, so we instead optimize an upper bound on it, which admits a stochastic convergence analysis, and also handles non-binary classification. The added contribution here is, in the paper's phrasing, \"a careful analysis of how the Gaussian noises [necessary for DP-SGD] propagate through the optimization trajectories\". I don't have much feel for what constitutes an \"interesting\" convergence analysis, but the conceptual novelty here is unclear, and the introduction is a bit slippery about what is novel and what is borrowed from the FERMI paper.\n\nThe paper also struggles to explain its technical contributions in terms between a very high level summary and a long, opaque theorem statement. I suggest changing the focus of the paper to 1) reduce, relegate to the appendix, or eliminate the discussion of demographic parity (an extremely coarse notion of fairness that, IMO, the fairness literature needs to move past, and has only been discussed this long because it's very simple), which takes up over a page of the main body without meaningfully adding to the story told by the equalized odds results alone, 2) extending the discussion of how Theorem 3.1 works and what it accomplishes (the current statement is a blizzard of notation with little explanation -- I still don't know what W is doing), along with Theorem 3.2, and 3) extending the equalized odds results to more datasets (why are Parkinsons and Retired Adult results only reported for demographic parity? it seems like equalized odds should also apply here, and an empirical story built on 2 datasets seems thin). I think 2) would help provide a clearer explanation of the paper's improvement over (Lowy+ 2021) and 3) would make a stronger empirical case separate from the convergence analysis.\n\nOther questions/comments:\n1) I'd appreciate a table in the appendix attempting to concisely explain all of the relevant variables -- by my count, Theorem B.1 has well over a dozen.\n2) Why is Tran 2021b a single point where the other methods have curves? More generally, perhaps I missed the explanation of this in the text, but what is varied to generate the curves?\n3) As far as I can tell, the paper does not discuss the tightness of the upper bound offered by ERMI, nor does it explicitly write out the expression for equalized odds. This makes it hard to contextualize the convergence guarantee in terms of the underlying loss we actually want to optimize.\n4) Figure 4 \"priavte\"",
            "clarity,_quality,_novelty_and_reproducibility": "I think the previous section answered this. I appreciate that the authors have submitted their experiment code. I did not attempt to check the derivations in the appendix.",
            "summary_of_the_review": "Overall, I think the current submission has some promising ideas and results that are hindered by an unclear explanation of technical or conceptual novelty and a sparse empirical evaluation on meaningful notions of fairness. As a result, my current recommendation is weak reject. However, I think a modified version of this paper, perhaps implementing the suggestions above, could be acceptable at ICLR or a similar venue, especially since the DP fairness literature has (IMO) still not featured much compelling work.\n\nEDIT: After reading the author responses and discussing with some of the other reviewers, I've increased my score to a weak accept. Other reviewers have expressed stronger misgivings about 1) the assumptions required to set the parameters to satisfy the utility guarantee, 2) the difference between the \"random iterate\" version analyzed and the \"final iterate\" version implemented, and 3) the division between sensitive and non-sensitive attributes. IMO, these aren't serious problems, but after discussing with the other reviewers, I'm not planning to champion this paper further. I suggest that the authors attempt to address these issues and re-submit a modified version of this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper346/Reviewer_yWF7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper346/Reviewer_yWF7"
        ]
    },
    {
        "id": "ntV2_ZzKHT4",
        "original": null,
        "number": 3,
        "cdate": 1666132953353,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666132953353,
        "tmdate": 1669059014654,
        "tddate": null,
        "forum": "3nM5uhPlfv6",
        "replyto": "3nM5uhPlfv6",
        "invitation": "ICLR.cc/2023/Conference/Paper346/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "An SGD-style algorithm is proposed for fair, private learning, with provable convergence even on non-convex loss.",
            "strength_and_weaknesses": "Strength: It is maybe of theoretical interest that SGD-style algorithm is proposed for fair, private learning, with provable convergence even on non-convex loss.\nWeaknesses:\n1) The input of each training example is partitioned into sensitive features s and nonsensitive features x. The algorithm is private and fair with respect to s, while the trained classifier only uses x as input. It is not clear to me why one would want to partition features in this way. If I'm training a language model on text input I want my credit card number to be private, but I'm not worried about fairness with respect to credit card numbers. Conversely, I want my face recognition system to be fair with respect to skin tone, but I don't necessarily need that to be kept private. Finally, in principle it seems like it should be possible to use sensitive features as model input while retaining privacy and/or fairness.\n2) The final model parameters are chosen by drawing uniformly at random from all intermediate models during training. Won't that often destroy model utility, if an early iterate is chosen?\n3) The experimental results are not impressive. Other than Figure 3a, the results are not significantly stronger than the Tran et al. (2021b) baseline, although they do allow an interpolation between fairness and model utility. I would like to see non-private and/or non-fair baselines. How much model utility are we losing due to weakness #2 above?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear, novel, and reproducible. What does the width of the curves in the experimental figures represent? If it is a confidence interval, I am surprised that it is not wider due to the randomness of the final model choice. There is no baseline for the DCNN experiment. I understand that the Tran (2021b) doesn't work, but I would still like to see non-private or non-fair baseline performance.",
            "summary_of_the_review": "The described algorithm has the practical weaknesses I mentioned above. Unless I am mistaken, it seems unlikely that this method would be useful in practice. But as far as I know the technical contribution is novel and could be a useful stepping stone toward a more practical algorithm with the strengths of the proposed approach.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper346/Reviewer_bxYf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper346/Reviewer_bxYf"
        ]
    },
    {
        "id": "YEmcsFEfTx",
        "original": null,
        "number": 4,
        "cdate": 1666790771609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666790771609,
        "tmdate": 1670598223105,
        "tddate": null,
        "forum": "3nM5uhPlfv6",
        "replyto": "3nM5uhPlfv6",
        "invitation": "ICLR.cc/2023/Conference/Paper346/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a differentially private and fair learning algorithm, satisfying two common group fairness notions: demographic parity and equalized odds. The proposed algorithm relies on augmenting the standard empirical risk loss component with an exponential Renyi mutual information term between the protected group feature and the label. The latter term requires fine-tuning to fit the specific fairness notion adopted. \nThe claimed benefit of the proposed procedure, which relies on approximating the resolution of a min-max problem through a classic stochastic gradient method, is its convergence property even when this optimization procedure operates over mini-batches.\n",
            "strength_and_weaknesses": "*Settings and contributions*:\n\nThe paper tackles an interesting and significant problem. I believe this one posed is a highly relevant question and am happy the authors studied this problem. \n\nI have also, however, found the claimed contributions to be a bit overstated: It is nice that the proposed method allows to handle non-binary sensitive attributes and that multiple fairness definitions, but this is also what is currently assumed in the recent literature, including in some of the work against this paper compare. \n\nI also believe that the claimed guarantees are overstated. From the abstract all the way to section 3 the paper claims that this proposal allows convergence for any fairness level even when mini-batches of data are used in each iteration of training. \nThis is, however, misleading. In section 3 and Theorem 3.1, in fact, after learning the assumptions adopted, I am left with the impression that claimed results are much less general than what is implied in the abstract and introduction: The restrictions taken on the loss function (strongly concave in the parameter space, global L-Lipschitz property in both the parameter of the primal and the dual space and convexity of the {\\cal W} set.).\n\nThe claimed mechanism properties thus, need to be tuned down substantially in the eyes of this reviewer. \nI encourage the authors to be very clear from the abstract on which assumptions the claimed properties are satisfied. \n\nNotice that, related to this overstatement, the last sentence before section 4 \"no existing work with convergence guarantees are able to handle non-binary classification\" is misleading. I don't think it does good to our community. \n\n*Experiments*:\n\nI really like the way the authors have displayed the experiments. They show a good Pareto boundary and can clearly see the superiority of the proposed method w.r.t. Tran et al.\n\nHowever, I also need to point out that there is no need to \"adapt\" Tran et al algorithms to non-binary cases, as their paper clearly reports experiments on non-binary cases too. I also found the statement about the instability of these methods to be possibly unjustified. Has a hyperparameter tuning over these methods been performed? Or did the authors use the hyperparameters adopted by Tran et al in their paper? If the latter is true, note that the datasets and architectures adopted for the experiments in this paper differ from what was used by Tran et al. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The novel contributions are specified, but, as stated above, the contributions need to be toned down substantially. \n",
            "summary_of_the_review": "The paper develops a technique for an important much-needed problem: guaranteeing both privacy and fairness while providing convergence guarantees. \nSome of the contributions of the work are however overstated and some clarity over the experimental setting adopted is needed. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper346/Reviewer_oW3z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper346/Reviewer_oW3z"
        ]
    }
]