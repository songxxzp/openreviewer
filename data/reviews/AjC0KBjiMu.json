[
    {
        "id": "Pqyp1CE1-B",
        "original": null,
        "number": 1,
        "cdate": 1665712789055,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665712789055,
        "tmdate": 1670643738519,
        "tddate": null,
        "forum": "AjC0KBjiMu",
        "replyto": "AjC0KBjiMu",
        "invitation": "ICLR.cc/2023/Conference/Paper3364/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "1. This paper proves that multiple contrastive learning methods can find a minimax-optimal representation for linear predictors assuming the target functions satisfy approximate view-invariance property (Assumption 1.1). To prove this, they first show that minimizing the contrastive loss is equivalent to solving Kernel PCA for the positive-pair kernel. Then they show that solving the Kernel PCA is equivalent to finding the eigenfunctions of the Markov chain over positive pairs, which allows them to prove the minimax-optimal property. They also give generalization bounds for the performance of the learned representations for downstream supervised learning. \n2. In the experiments, in two synthetic datasets where the positive kernel has a closed form, the authors verified that multiple contrastive learning methods could indeed approximately recover the eigenfunctions of the kernel. The experiments also showed that the constraints on the kernel approximation and the weaker augmentation can have a negative impact on the recovery of the eigenfunctions. ",
            "strength_and_weaknesses": "Strengths:\nThis is a very solid and well-written theory paper that proves multiple contrastive learning methods can find a minimax-optimal representation for linear predictors assuming positive pairs have similar labels. This theory offers a unifying explanation for the success of multiple contrastive learning methods in practice. In the proof, they also build a very interesting connection between contrastive learning, Kernel PCA, and the Markov chain over positive pairs. \n\nWeaknesses:\nThis paper builds a connection between contrastive learning and Kernel PCA, which is very interesting from a theoretical perspective. However, what's the implication of this theory? How does this knowledge further guide us to build better contrastive learning methods in practice? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThis paper is written very well. All the assumptions and theorems are clearly stated and lots of intuitions and explanations are provided. The limitations are also well discussed at the end.\n\nQuality: \nThis is a solid theory paper that proves the minimax-optimality of contrastive learning methods by identifying their connection with Kernel PCA. \n\nNovelty:\nAs far I know, this is the first paper that proves multiple contrastive learning methods can find a minimax-optimal representation for linear predictors assuming the target functions satisfy approximate view-invariance property.\n\nReproducibility:\nI think both of theoretical analysis and the experimental results are reproducible. \n\n",
            "summary_of_the_review": "This is a solid theory paper that proves the minimax-optimality of contrastive learning methods in finding representations for linear predictors assuming the view-invariance property of the target functions. The analysis also builds an interesting connection between contrastive learning, Kernel PCA, and the Markov chain over positive pairs. I believe this is a good step toward explaining the empirical success of various contrastive learning methods. My only concern is that it's unclear to me how this theory can guide us to design between contrastive learning methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3364/Reviewer_7QsK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3364/Reviewer_7QsK"
        ]
    },
    {
        "id": "DbBC92MOZQ",
        "original": null,
        "number": 2,
        "cdate": 1666614777764,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614777764,
        "tmdate": 1666614963552,
        "tddate": null,
        "forum": "AjC0KBjiMu",
        "replyto": "AjC0KBjiMu",
        "invitation": "ICLR.cc/2023/Conference/Paper3364/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work claims that contrastive learning can find a minimax-optimal representation for linear predictors when the prediction function is approximately view-invariant. \n\nMore precisely, the authors demonstrate that learning a representation via contrastive losses such as NT-XEnt, NT-Logistic, and Spectral can be seen under some assumptions as learning a positive definite kernel between pairs of samples. Then, a finite-dimensional representation can be extracted via kernel PCA using the learned kernel. Under some assumptions, it is possible to demonstrate that the resulting representation is minimax optimal for linear prediction for approximately view-invariant functions. This demonstration is done by bridging the vectors obtained via kernel PCA to eigenfunctions of a Markov chain over positive pairs. \n\nThe authors then proceed to experimentally validate their theory on two synthetic tasks (overlapping regions and MNIST with specific augmentations).",
            "strength_and_weaknesses": "Strength:\n- This paper tackles understanding and unifying self-supervised learning methods, which is an important problem as this framework becomes increasingly popular and many apparently different methods are in fact doing the same thing.\n- The theory is both simple and provides insights that follow the practice of SSL such as the importance of stronger augmentations or the relevance of linear evaluation, as well as new (at least to me) ones such as the negative effects of constraints on the output heads, and is validated experimentally. \n- The main Assumption 1.1 is clearly stated and discussed. \n\nWeakness: \n- I think Assumption 1.1 does not apply to important downstream tasks for evaluating visual representations such as object detection and segmentation. Moreover, visual representations are evaluated via linear probing, but also via fine-tuning or knn, which is not covered here. Hence, the extent of this theoretical analysis seems at first sight limited. In my opinion it would be very interesting to discuss these limitations, as another possible valuable conclusion is that object detection / segmentation / fine-tuning / knn are not relevant when it comes to evaluate contrastively learned representations.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- The paper is generally well-written except in my opinion in one part: at first sight, it is not obvious why this work relies on the Markov Chain over positive pairs and it would be useful to provide more intuition on this. \n\nQuality:\n- The theory seem sound and is validated by detailed experiments.\n\nNovelty: \n- The interpretation of contrastive SSL as kernel learning seems new to me.\n\nReproducibility: \n- The code is not provided but Appendix contains enough experimental details.",
            "summary_of_the_review": "This work offers a sensible kernel interpretation of contrastive self-supervised learning, whose understanding is an important problem today. The scope of this work may be limited since it differs from practical SSL in different ways: \n- Models are not always linearly evaluated (knn and fine-tuning are also used).\n- Augmentations distributions when learning downstream tasks often differs from training time.\n- Important downstream tasks such as segmentation or object detection may not follow Assumption 1.1\n\nI think these points could be at least discussed in the manuscript.\n\nOverall, the pros outweight the cons and would tend to recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3364/Reviewer_76h8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3364/Reviewer_76h8"
        ]
    },
    {
        "id": "jclkY3aqPQa",
        "original": null,
        "number": 3,
        "cdate": 1666707052160,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666707052160,
        "tmdate": 1666707052160,
        "tddate": null,
        "forum": "AjC0KBjiMu",
        "replyto": "AjC0KBjiMu",
        "invitation": "ICLR.cc/2023/Conference/Paper3364/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed a unified framework for interpreting a few existing contrastive learning methods as positive-pair (see Definition 2.1) kernel learning problems, see Table 1 for a summary. \nUnder a mild assumption for the target function g in Assumption 1.1, the authors showed:\n* in Theorem 3.2 that standard kernel PCA under the proposed positive-pair kernel has a few interesting theoretical properties (to identify eigenfunctions of some Markov transition of interest); and \n* in Theorem 4.1 that the obtained representation (1) maximizes the view invariance of the least-invariant unit-norm predictor and (2) minimizes the (quadratic) approximation error for the worst-case target function.\n\nSome (somewhat toy) numerical experiments were provided in Section 6 to support the theoretical results.",
            "strength_and_weaknesses": "**Strength**: the paper focuses on an important problem and the proposed viewpoint is interesting. The paper is in general well written.\n\n**Weaknesses**: a few concepts as well as their connections are discussed in the paper. And (in addition) the notations are somewhat heavy. It would be helpful to have a few figures or tables for the readers to have a better understanding of the contribution of this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: the paper is in general clearly written. The major contribution should be highlighted more, though.\n\n**Quality and Novelty**: the paper studies an interesting problem and the proposed viewpoint adds value to the existing theory.\n\n**Reproducibility**: satisfactory.",
            "summary_of_the_review": "I think the paper is in general interesting. It would be great if the authors could (1) find some way to better illustrate the contribution of this work and (2) address the following detailed comments:\n\n* more discussion on the positive-pair kernel is needed: the probability ratio and the positive-pair kernel seem to be one key point in the paper, and it is a Mercer kernel. Do we know anything more about it? can we have some more insight into the task, etc.?\n* it remains unclear to me how the proposed theory is of interest from a practical standpoint. Can that be used to design some novel methods/objectives beyond those listed in Table 1?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3364/Reviewer_Ry9G"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3364/Reviewer_Ry9G"
        ]
    }
]