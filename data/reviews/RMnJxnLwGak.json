[
    {
        "id": "FO4_jYh0jZ",
        "original": null,
        "number": 1,
        "cdate": 1666645684594,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645684594,
        "tmdate": 1666645684594,
        "tddate": null,
        "forum": "RMnJxnLwGak",
        "replyto": "RMnJxnLwGak",
        "invitation": "ICLR.cc/2023/Conference/Paper4166/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new efficient transformer model for time series forecasting \u2013 which mitigates the typical O(N^2) forecast time using a approximation based on vector quantisation. The model demonstrates outperformance over other O(N) RNN methods across a variety of datasets.",
            "strength_and_weaknesses": "Strengths\n---\n\nWhile transformer models have demonstrated strong performance on time-series datasets, the slow performance of the attention mechanism have made applications difficult in low-latency settings. Methods to improve performance are important, and vector quantisation is an interesting approach. The paper is also fairly well-written and motivated, and the arguments are clearly lined out.\n\nWeaknesses\n---\n\nWhile the approach is promising, I do have several key concerns:\n\n1.\tResults for the model are rather weak, with outperformance demonstrated on 4 out of the 6 dataset only when compared to fairly simple neural network baselines. Are there any specific dataset characteristics that cause the approach to underperform?\n\n2.\tHow does the model compare against other transformer approaches? The true test of the model should be against methods with the full attention mechanism to evaluate how much performance is sacrificed for efficiency. How does the model stack up against the Pyraformer (ICLR 2022) which also has linear time/space complexity?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written and clear. However, for full reproducitbility/evaluation additional details on training would be required -- specifically with regards to how feature engineering and hyperparameter optimisation are performed.",
            "summary_of_the_review": "On the whole the approach does appear to hold promises, although additional comparable benchmarks are required to fully demonstrate the value proposition of the approximation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4166/Reviewer_TMte"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4166/Reviewer_TMte"
        ]
    },
    {
        "id": "PEk3E4fqfsk",
        "original": null,
        "number": 2,
        "cdate": 1666650798708,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650798708,
        "tmdate": 1666650798708,
        "tddate": null,
        "forum": "RMnJxnLwGak",
        "replyto": "RMnJxnLwGak",
        "invitation": "ICLR.cc/2023/Conference/Paper4166/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduced a `VQ-TR` method that maps larger sequences to a discrete set of latent representations to be used as part of the attention module. In this way, larger context windows can be attended with linear complexity w.r.t sequence length. Experiments show that this method performs competitively against the baseline methods.\n\nThe contributions of the paper come from incorporation of ths vector quantization attention module in Transformer to get linear computation and memory use w.r.t. sequence size, theoretically analyze how the vector quantization method approximated the vanilla attention mechanism, and demonstrate competitive its performance.\n",
            "strength_and_weaknesses": "Overall, this paper follows up on the `VQ-AR` model and further incorporates the vector quantization module as part of the approximate attention block.\n\nStrengths of this paper:\n1. The presentation of `VQ-TR` model is clear, with rich details in demonstrating how it approximates the attention mechanism and how the linear complexity is derived.\n2. Figure 1 was well generated and informative for understanding this model.\n3. Experiments are extensive, covering multiple domains of time series and different baselines.\n4. The idea seems generally applicable to more applications in the sequence representation learning area.\n\nWeaknesses and how to improve:\n1. The paper claimed to improve the theoretical complexity of Transformer's attention mechanism. This should potentially bring benefit from using a longer input length and outperforming Transformer based models with that. More experiments with long input length against vanilla Transformer based models would make this point more valid.\n2. Similar to the previous point, the improvement over theoretical complexity can be quantified by benchmarking against the vanilla Transformer model. Would be great to see numbers on how it improves inference speed or memory occupation.\n3. The performance seems competitive but there's a lack of analysis of why the model doesn't work well for some of the datasets. Some case study or analysis would fix this problem.\n4. The hyper-parameter `J` of `VQ-TR` model was claimed to control the `trade-off of computation and memory use`, and would like to see some ablation study on how this hyper parameter is selected and the actual impact on performance.\n5. In the Experiment section, the use of different emission heads is a bit confusing. Can you please add some explanation on how the emission head is selected for certain tasks, especially when `VQ-AQ` and `VQ-TR` use different ones?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper was written in good clarity. The introduction of vector quantization and `VQ-TR` method was very detailed, figure 1 is very informative for understanding model architecture. Training & inference details are also covered. On the other hand, some details in the Experiment section seem a bit unclear. The point of proposing this method is to achieve linear complexity w.r.t sequence length in the Transformer model but whether this benefit is achieved or useful remains unclear, which impacts the quality of this paper. On the originality side, this paper followed the setting of probabilistic time series forecasting, and the concept of vector quantization was already used in `VQ-AR`. The creative part lies in the design of VQ based attention mechanism and how that fits in Transformer, which is still interesting and novel. \n",
            "summary_of_the_review": "To summarize, this paper proposed this `VQ-TR` design that uses vector quantization to approximate the attention mechanism in Transformer, and utilized that for probabilistic time series forecasting. The paper presented its model design clearly and provided details for theorem proof for the usage of vector quantization in attention. On the other hand, even though the experiments demonstrated the method is competitive, it does not take advantage of the complexity advantage of linear complexity and show how that could be leveraged for better prediction and how that improves efficiency in a quantified way. Therefore, I recommend this paper to be rejected.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4166/Reviewer_PERd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4166/Reviewer_PERd"
        ]
    },
    {
        "id": "0-8M3c0gMY",
        "original": null,
        "number": 3,
        "cdate": 1666692178856,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692178856,
        "tmdate": 1666692178856,
        "tddate": null,
        "forum": "RMnJxnLwGak",
        "replyto": "RMnJxnLwGak",
        "invitation": "ICLR.cc/2023/Conference/Paper4166/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel Transformer architecture for extremely long-range time series forecasting. The key component of the proposed method is VQ, where the number of possible output features is limited. As a result, the computation only happens between quantized vectors, and the heavy quadratic cost of self-attention can be greatly reduced. Experimental results show that the proposed VQ-TR achieves comparable and sometimes better performance than other competitors.",
            "strength_and_weaknesses": "Strengths:\n- As I know, this is the first work to compute cross-attention and self-attention between VQ codewords. This approach greatly reduces the computation cost, which is the main obstacle to long-range prediction.\n- The paper properly compared previous competitors for various benchmarks. Also, the performance seems good enough.\n\nWeaknesses:\n- Although the authors mention that Transformers \u201cscale poorly to the size of the sequence length\u201d, there is no empirical evidence to support this claim.\n- Are there other metrics that are related to computation cost/inference time/parameter size, etc.? I am not sure that the compared models have experimented using a similar budget (i.e., fair comparison).\n- Just a suggestion; There exist some Transformer models that incorporate (K-means) clustering for query/key computation. Although not directly related, comparing those works can be helpful.\n\n[1] Fast Transformers with Clustered Attention (NeurIPS 2020)\n\n[2] Efficient Content-based Sparse Attention with Routing Transformers (TACL 2020)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is somewhat difficult to follow, but it seems that the idea is novel and potentially can inspire many future studies. For reproducibility, some operations (latent self-attention, latent cross-attention, etc.) could be more explained in detail. Although the VQ process takes an important part of the paper, there is not much explanation (i.e., VQ training loss, VQ regularization, hyper-parameters, etc.). I hope the code release can solve these.",
            "summary_of_the_review": "Overall, the paper tackles an important problem. Using VQ for computation reduction itself may not be new, but the realization/implementation is novel enough. However, some parts can be improved (see weaknesses above).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The authors addressed potential concerns related to the paper.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4166/Reviewer_miFG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4166/Reviewer_miFG"
        ]
    },
    {
        "id": "8t7CRCvo_eS",
        "original": null,
        "number": 4,
        "cdate": 1667189635014,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667189635014,
        "tmdate": 1667191028819,
        "tddate": null,
        "forum": "RMnJxnLwGak",
        "replyto": "RMnJxnLwGak",
        "invitation": "ICLR.cc/2023/Conference/Paper4166/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an approach to speed up transformer based forecasting / enable it to scale better for longer context (history) windows feeding into the transformer forecasting model.  Typically in space and time complexity transformer forecasting would scale quadratically with this context window (sequence) length.  Here they propose to adopt a quantization approach used elsewhere for the encoder part of the forecast model, which encodes each input in the sequence to a finite dimension codebook, applies cross-attention on the encoded vectors, and then applies self attention on the reduced set of vectors to transform them before mapping back to the input sequence positions - in this way enabling linear scaling with context window length, for deriving the output of the encoder portion of the forecast model.  The decoder part of the model is left to handle the causal temporal modeling with the regular self attention approach across the full output sequence (so it is quadratic in the forecast horizon).\n\nThe authors compare the proposed approach with different output distributions on a number of datasets to a variety of methods, showing competitive performance.",
            "strength_and_weaknesses": "Strengths:\n\n1. The quantization idea is interesting and I don't believe has been applied to transformer forecasting before.  \n\n2. The authors provide theoretical analyses on approximation errors to help motivate the idea.\n\n3. Extensive experiments are performed to compare forecast error using point and probabilistic forecast metrics on multiple datasets.\n\n\nWeaknesses:\n\n1. The biggest weakness is that the method is motivated as a way to speed up transformers for forecasting for long context windows.  However, surprisingly, no transformer forecast methods were compared with, and no analyses or comparison or results of any kind are reported for runtimes and how it varies with context length.  Ideally the recent state of the art transformer forecast methods should be compared with (e.g., Fedformer, Informer, etc.) both in terms of accuracy, but also in terms of run times, especially for varying context length.  At the very least, showing the runtime variation of the basic transformer forecasting approach vs the proposed one should be provided in experiment results, as context and forecast horizon length vary.  It's also important to show how forecast error changes with varying context length - to validate the motivating claim that longer context window length improves forecasts and thus speeding up this part of the model is important.\n\n\n2. Furthermore, there are a lot of techniques for speeding up transformers, and many have been applied to transformer forecasting as well - these should be compared to (and be part of related work too to set the context).  For example, in \"Long-Range Transformers for Dynamic Spatiotemporal Forecasting\" they use a Performer style approach to enable scaling the attention - which uses a linear approximation to the attention using a random kernel approach.\n\n\n3. The method only addresses speeding up the calculation of the encoder part of the forecast model - which seems insufficient to enable general scalability - especially since in practice the forecast horizon can be just as long as the context window (or even longer).  I.e., in Section 3 - it is stated \"Since P << C for the datasets we train on...\" - but C is essentially a modeling choice or hyper parameter (not a feature of the dataset) and it seems rarely the case for it to be chosen much larger than P.   I.e., it is often not the case - in fact a lot of forecasting work in the literature has found using smaller context windows to work best in many cases (that could be even less than the forecast horizon) as long as appropriate features / exogenous series are used.\n\n\n4. The model formulation is not provided with sufficient detail and clarity - many specifics and even variables used in equations are not defined or explained.  Overall it feels like an incomplete, \"hand-wavy\" description of the approach and the model.  Some specific comments / examples:\n    -  In 2.1, C used in multiple equations is never defined, and it should be defined.\n    - Also here the description is confusing and not very clear or precise - making it hard to follow.  E.g., in 2.1 it's stated \"Afterwards M layers of a causal or masked decoding Transformer can be used to model the P future time points as:\" but the following equation does not at all show how M layers are applied and doesn't even include M or any coefficient related to it.  Right after this the output model is introduced with \"For example...\", when it has nothing to do with the preceding sentence and the output model and this equation are not fully explained, again along with nomenclature used not being explained, such as the \"N\" being used but not defined or explained.\n    - Section 3 is not explained clearly - shorthand operations are given without any complete and clear description of what they translate to (either mathematically or via textual description).  I would suggest moving the proof to the appendix (and adding more detail as they are difficult to follow), and possibly the formulation of the optimization objective following that, which is also not clearly explained.  In this way, more space would be made for providing full mathematical details of the model.\n\n\n5.  There are some unsubstantiated statements / claims made - these should supported with references or sufficient arguments (with explanation that they are just opinions are hypotheses).  \n    - \"Transformers offer good inductive bias for the forecasting task...\" - this seems like a strong and unsubstantiated statement - is there some reference to back this claim up, or anything beyond a heuristic argument?\n    - \"...and consider some fixed sized context window sampled randomly from the full time series history to learn some historical representation and use this representation in the decoder to learn the distribution of the next time point...\" - I disagree - the majority of approaches use a fixed sized context window not sampled randomly, but immediately preceding the current time points / the future time window being predicted.  For instance, if one is predicting t+1 through t+h, we use a context time window from t-c to t - not some random window from the history.\n\n\n6. The experiment comparison may not be fair.  In particular, in section 3.4 the authors describe a window scaling approach they use for their method.  However, a similar window scaling approach was shown to be effective in ICLR 2021 paper \"Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift\" across a wide range of models.  I.e., when applied to a wide variety of different models it significantly improved their forecast accuracy across multiple datasets (I believe the same set used here).  However, only the proposed model here uses the window-scaling normalization approach - so it's hard to say if the other methods wouldn't work as well or better if also using the window scaling - i.e., any observed benefit may just be from using window scaling.\nAdditionally particulars of how all configurations / hyper parameters are chosen should be given - in particular in the reported results - why are only some output distributions shown for some datasets - how were these selected - and why weren't results shown for all.\nFinally, std. dev. should also be reported over multiple random runs and shifted test windows.\n\n\n\nMinor:\nIn section 2 you use the term \"data-time\" without definition - a term which I have never seen before.  Is this perhaps meant to be \"date-time\" (i.e., a combination of data and time and commonly used in computing and libraries for time series and for referring to particular point in time)?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality should be improved - as mentioned above.  The idea does seem interesting and novel (at least in its application to time series forecasting - as the idea is borrowed from an existing approach for other applications of transformers to temporal data).  The authors state they will release the code which will help with reproducibility, but otherwise the method would need to be described with greater detail, preciseness and clarity to enable reproducing.  ",
            "summary_of_the_review": "This work has potential, but overall feels like a work in progress - a lot more is left to be done before it is ready for publication in a top conference.  This includes the experiments needed to validate the main claims and motivations for the proposed method, which are currently lacking.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4166/Reviewer_ZX4K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4166/Reviewer_ZX4K"
        ]
    }
]