[
    {
        "id": "2VzaMfL20X4",
        "original": null,
        "number": 1,
        "cdate": 1665977063855,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665977063855,
        "tmdate": 1665977176157,
        "tddate": null,
        "forum": "gUZWOE42l6Q",
        "replyto": "gUZWOE42l6Q",
        "invitation": "ICLR.cc/2023/Conference/Paper2194/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new machine learning model, and training objective, for classifying signal data from speech, wearables, EMG, and sensors. I don't fully understand the proposed model (see questions for clarification below). But it outperforms previous methods in the field.\n\n\"While most efforts have predominantly resided in image classification, OOD in time series remains underexplored and more challenging.\" I don't think it's underexplored, there's entire research fields dedicated to sequential extrapolation. Look up DYCK languages or most of the papers published by Michael Hahn in 2019-2020. Moreover, if the OOD is in reference to signal processing, I am sure the field of reinforcement learning and robotics are dealing with similar challenges of OOD.\n\nAfter reading the introduction, I still don't understand what \"DIVERSIFY\" does, other than it's adversarial and models the underlying distribution.\n\n\"Surprisingly, we even find temporal distribution shifts in our experiments (Figure 6) that distributions of one person can also change at various times\" Do you collect your own dataset? What kind of experiments are you referring to?\n\n\"a time series may consist of K2 unknown latent domains3 rather than a fixed one\" The way the citation works make it seems like it says K squared.\n\nWhat are Hc, Hb, and Hf - do you have more detailed explanations of them somewhere? Also, please give a more elaborate explanation of (2).\n\n\"At the first iteration, there is no domain label d\u2032 and we simply initialize d\u2032 = 0 for all samples. We treat per category per domain as a new class with label s \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , S}. We have S = K \u00d7 C where K is the pre-defined number of latent distributions that can be tuned in experiments. We perform pseudo domain-class label assignment to get discrete values for supervision: s = d\u2032 \u00d7 C + y.\"\nI don't understand this. Do you choose a set of sub-sequences by hand, and then force your model to iteratively use more finegrained subsequences?\n\n\"Latent Distribution Characterization\" Is this some type of hard attention?\n\nFigure 7 is not covered enough, I don't have a strong intuition for why your model works. I would like to see specific examples of data where your model performs better.\n\n",
            "strength_and_weaknesses": "Strength\nImproved performance\n\nWeakness\nUnclear how the model works\nLack of visualizations\nMotivation is not completely correct\nUnclear if improved performance is state-of-the-art",
            "clarity,_quality,_novelty_and_reproducibility": "The math is not completely clear to me, I don't fully understand the method or loss function proposed.",
            "summary_of_the_review": "I am not familiar with this field and the paper does not make it easier to understand. The authors argue that their focus on OOD on sequences is novel, however, I don't see how OOD signal processing is a new field of research. I need a better motivation. It is not clear to me from the text if the models they are testing against are state-of-the-art, or if they are reimplementations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_MfQf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_MfQf"
        ]
    },
    {
        "id": "8mfIYR_TcX2",
        "original": null,
        "number": 2,
        "cdate": 1666713231373,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713231373,
        "tmdate": 1666713265960,
        "tddate": null,
        "forum": "gUZWOE42l6Q",
        "replyto": "gUZWOE42l6Q",
        "invitation": "ICLR.cc/2023/Conference/Paper2194/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a representation learning model for OOD timeseries data. By building on top of DANN, it applies clustering in order to identify domains along with pseudo domain-class labels and adversarial self-supervised pseudo labeling to obtain the pseudo domain labels. Extensive experiments are conducted on various datasets with different OOD settings showing that the proposed framework outperforms two closely related baselines and other baselines from the domain generalization literature.",
            "strength_and_weaknesses": "S\n+ Strong results in an array of diverse timeseries tasks\n+ Extensive experimentation with different settings across people/positions/datasets\n+ Intuitive explanation of why the components of the method should be there (along with ablations)\n\nW\n- Lack of comparisons with other backbones like Transformers",
            "clarity,_quality,_novelty_and_reproducibility": "The work is of high quality. However, there's no provided code for reproducibility. ",
            "summary_of_the_review": "This is a very well designed paper with strong results. My only concerns are about exploring different backbone architectures such as Transformers and lack of code.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_EJKA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_EJKA"
        ]
    },
    {
        "id": "CbdilkANxy",
        "original": null,
        "number": 3,
        "cdate": 1666747501153,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666747501153,
        "tmdate": 1666747501153,
        "tddate": null,
        "forum": "gUZWOE42l6Q",
        "replyto": "gUZWOE42l6Q",
        "invitation": "ICLR.cc/2023/Conference/Paper2194/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors propose DIVERSIFY, an OOD representation learning algorithm for time series classification. It has two iterative steps: (i) learn to segment the time series data into several latent sub-domains by maximizing the distribution gap and (ii)  learn domain-invariant representations by reducing the distribution divergence between the obtained latent domains. The experiments on several time series classification tasks showed that the proposed method outperforms other baselines.",
            "strength_and_weaknesses": "* Strength\n\n(i) The work is clearly written and easy to follow.\n\n(ii) The design is sensible, as it is widely seen from other OOD work beyond time-series data.\n\n(iii) Sec 2.3 is insightful, esp. linking Eq. (7) to the step 3 of the proposed algorithm.\n\n* Weakness\n\n(i) The novelty is quite limited, as the techniques used here are largely adapted from existing work.\n\n(ii) For the experiment, it is not clear why such a simple feature net (two-conv) is used, given that stronger backbones exist.\n\n(iii) Why there is an inconsistency over methods for different datasets? i.e., some methods are not evaluated for certain datasets/tasks. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality is good, while originality is limited. Reproducibility may be hard as no code is given.",
            "summary_of_the_review": "Overall I think it is a good work, though there are some flaws in experiment section should be addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_oHGw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_oHGw"
        ]
    },
    {
        "id": "mxwGoX-RAN",
        "original": null,
        "number": 4,
        "cdate": 1666981736155,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666981736155,
        "tmdate": 1666981736155,
        "tddate": null,
        "forum": "gUZWOE42l6Q",
        "replyto": "gUZWOE42l6Q",
        "invitation": "ICLR.cc/2023/Conference/Paper2194/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper propose a method for representation learning robust to out of distribution cases for the times series classification cases.",
            "strength_and_weaknesses": "## Strength\n- consider worst-case scenario and develop the algorithm and modeling end to end.\n- extensive real-data experiments to demonstrate the superiority of the method.\n\n\n## Weakness\n- lack of motivation and weak examiniation on the method \n - why OOD is important in time series? \n - how was OOD defined in the time series formally? at least reference? these definition can be different from static data \n - What if all real dataset do not fall in OOD categories the authors defined? any synthetic experiments to support the necessity of the method?\n - the theoretical results is adopted from existing one, why is this specially applicable to time series setting beyond static setting? Without it, it is just OOD method in image classification and need more thoroughly to be compared with those literatures? \n - similary, why existing methods in static classification are not applicable to time series setting?",
            "clarity,_quality,_novelty_and_reproducibility": "the domain invariant feature is quite common in domain adaptation literature beyond DANN, e.g., DAF https://arxiv.org/abs/2102.06828,  and thus be suspicious on the novelty of the method. In addition, worst-case scenarios is considerd in ADA-RNN method and suspicious of the orignality of the concept.",
            "summary_of_the_review": "Even with the propose method end2end and extensive real data experimetns, the contribution is marginal, not strong connection of OOD motivation and supporting evidences. Many things the author claimed 'novel', like perspetive and methodology is variant of existing works and thus marginal, or if not, authors should more thoroughly examine and point out the differences. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_x6zY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_x6zY"
        ]
    },
    {
        "id": "XeVSUB8b9Dv",
        "original": null,
        "number": 5,
        "cdate": 1667352362648,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667352362648,
        "tmdate": 1670134335621,
        "tddate": null,
        "forum": "gUZWOE42l6Q",
        "replyto": "gUZWOE42l6Q",
        "invitation": "ICLR.cc/2023/Conference/Paper2194/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a domain generalization method for time series classification to\nimprove the generalization on new unseen target domain without using domain labels.\nMoreover, an identification approach of the sub-distributions inside the data is\nproposed. Last, a min-max adversarial approach is presented to find domain invariant\nfeatures among the predicted domains and sub-domains. Experimental results on four\ndifferent time series datasets show the superiority of the proposed approach.",
            "strength_and_weaknesses": "Strength:\n\nThe problem of domain generalization for time series classification is significant\nand important\n\nA new perspective for the inherit sub-distributions inside the time series data is\ninsightful\n\nTheorical insights are also provided.\nWeaknesses:\n\nThe technicality of the idea, while assuming there exists sub-distributions\nwithin each well-defined domain is plausible, it is not clear how the method\ndifferentiates between the class-distributions within each domain (which is also\ncan be detected as sub-domains) and the original sub-distributions.\n\nOverclaims and Novelty:\n\nThe authors claimed that domain generalization for time series\nclassification is that \u201cunder explored\u201d without mentioning in time series\ndomain generalization methods. However, the exist some domain\ngeneralization methods for time series data such as [1,2]\n\nThe authors claim novelty for working and using pseudo domain labels.\nHowever, some domain generalization methods exist that use peudo\ndomain labels such as [3].\n\nThe authors claim \u201cFinally, our work is not a direct application of DG due\nto the non-existence of domain labels \u201d to distinguish their methods from\ndomain generalization approach. Nevertheless, there exist some\napproaches that also doesn\u2019t require domain labels [1, 2, 3]\n\nExperiments:\n\nOne key motivation for the paper is the detection of sub-distributions\nwithin each domain. However, all the tested domain generalization\nscenarios are clearly usually mapping across clearly separated domains\nsuch as different humans where each human is one domain. Thus, I\ncannot see any experiment that support their claim of the existence of\nsub-domains.\n\nMissing Time series domain generalization baselines [1,2].\n\n[1] Zhang, Wenyu, Mohamed Ragab, and Ramon Sagarna. &quot;Robust domain-free domain generalization\nwith class-aware alignment.&quot;\u00a0ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2021.\n\n[2] Ragab, Mohamed, et al. &quot;Conditional Contrastive Domain Generalization for Fault Diagnosis.&quot;\u00a0IEEE\nTransactions on Instrumentation and Measurement\u00a071 (2022): 1-12.\n\n[3] Matsuura, Toshihiko, and Tatsuya Harada. &quot;Domain generalization using a mixture of multiple latent\ndomains.&quot;\u00a0Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 07. 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is clearly written, with a detailed supplementary material.\n\nQuality: the paper is technically sounds, though some claims are not well supported.\n\nNovelty: The assumption of sub-domains and the ability to detect and adapt these sub-\ndomains is somewhat novel. However, the ideas of using pseudo domain labels and the\nassumption of no domain label existing are existing in the literature\n\nReproducibility: The paper is reproducible",
            "summary_of_the_review": "The paper addresses a challenging yet an important problem of distribution shift for time\nseries data . The main weaknesses are the incomplete support of some claims, while\nmissing some important baselines in the related works and experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_SWyN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2194/Reviewer_SWyN"
        ]
    }
]