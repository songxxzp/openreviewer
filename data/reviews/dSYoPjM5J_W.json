[
    {
        "id": "DBa2nUAsna",
        "original": null,
        "number": 1,
        "cdate": 1666641396119,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641396119,
        "tmdate": 1669572047931,
        "tddate": null,
        "forum": "dSYoPjM5J_W",
        "replyto": "dSYoPjM5J_W",
        "invitation": "ICLR.cc/2023/Conference/Paper1573/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explains the reason behind the effectiveness of Gradient-based attacks for poisoning and evasion on semi-supervised node classification tasks on the graph. ",
            "strength_and_weaknesses": "Strength:\n- It is an important and interesting problem to investigate. \n- Addressing this problem from a distribution perspective is interesting\n- Provided tips for attacks and defenses that can be useful for other research works are very interesting. \n---------------------------------------------------\nWeakness: \n- This paper claims to have found out that the adversarial attack is successful because it enlarges the shift between test and training. This is, however, a well-understood result from image adversarial attack and it is obvious. \n- This paper emphasizes that the gradient-based attack causes this shift. Could authors please justify this a bit further? Can these results only be applied to gradient-based attacks or is it applicable to different types of attacks?\n- it seems like their analysis of distribution shift is limited to the choice of data and the attack. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear enough and one can follow the train of thought. Novelty is enough. Their experimental results may NOT be verified. There is no code provided. ",
            "summary_of_the_review": "This paper has some merits, some results are taken from image adversarial attacks but overall the idea is novel enough. The experimental results are not verifiable. It is recommended that the author submit their code so that reviewers can make sure their results are accurate. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1573/Reviewer_Vakj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1573/Reviewer_Vakj"
        ]
    },
    {
        "id": "rfCh_dayE3W",
        "original": null,
        "number": 2,
        "cdate": 1666667027307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667027307,
        "tmdate": 1666667027307,
        "tddate": null,
        "forum": "dSYoPjM5J_W",
        "replyto": "dSYoPjM5J_W",
        "invitation": "ICLR.cc/2023/Conference/Paper1573/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper discovers a phenomenon that the gradient-based graph structure adversarial attack aims for increase the train-test distribution shift. The authors provide some theoretical proofs and then give explanations to some graph attack observations based on the prove. What\u2019s more, this paper gives some practical tips on robust GNNs and design their attack and defense algorithm based on some tips.",
            "strength_and_weaknesses": "S1: This paper explains the structural adversarial attack from a distribution perspective, which is seldom discussed in graph adversarial attack.\nS2: This paper both theoretically and empirically proves this \u2018distribution shift\u2019 phenomenon and revisiting classical attack methods like PGD and MetaAttack.\nS2: This paper gives explanations to the existing graph attack observations from a data distribution view.\n\nW1: The distribution analysis from PGD and MetaAttack is similar to that in [1]. Besides, the usage of pseudo labels in the semi-supervised node classification task is also similar. Maybe the authors could give more explanations to the difference of these two works and how this paper improves from [1].\nW2: The baseline set of both attack and defense algorithm comparison in Table 10 and Figure 6 is relatively small, which may degrades the effectiveness of proposed methods.\n\n[1] Zhan H, Pei X. Dealing with the unevenness: deeper insights in graph-based attack and defense[J]. Machine Learning, 2022: 1-33. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The core idea of this paper is clear and easy to understand, the authors also give theoretical and empirical proofs. Besides, this paper provides the experimental detail in the appendix for reproducibility. ",
            "summary_of_the_review": "This paper revisits gradient-based graph structure adversarial attack methods from a distribution view. The authors give some theoretical proofs and explanations to graph attack observations. However, the empirical analysis for pgd and meta-attack from a distribution view is somehow being discussed in the previous work and the authors have not mention relevant literature or compare with these works. As a result, I think this paper is marginally above the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1573/Reviewer_Lqax"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1573/Reviewer_Lqax"
        ]
    },
    {
        "id": "gwlGC3fBNvG",
        "original": null,
        "number": 3,
        "cdate": 1666717104401,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717104401,
        "tmdate": 1668687778448,
        "tddate": null,
        "forum": "dSYoPjM5J_W",
        "replyto": "dSYoPjM5J_W",
        "invitation": "ICLR.cc/2023/Conference/Paper1573/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper thoroughly studies two gradient-based attack methods on graph-structured data, namely MetaAttack and PGD, and reveals their behavior patterns from a data distribution shift perspective. Based on the theoretical analysis regarding training and test data distribution, the authors propose several practical tips from both attack and defense perspectives.\nExperiments that are specifically designed for these tips align with the theoretical foundations and show insights for future designs.\n",
            "strength_and_weaknesses": "Strengths:\n* The paper contains multiple original ideas that are also well-executed and theoretically well-grounded.\n* The attack analysis on the behaviors of gradient-based attack methods is novel and could provide insights for further graph robustness research.\n* The empirical observations are interesting and the practical tips seem useful for future graph learning attack and defense designs.\n\nWeaknesses:\n* The assumption that $p_{train}(\\tilde{x}, y)$ and $p_{test}(\\tilde{x}, y)$ are the same on clean graphs might be too strong as there are many works on the out-of-distribution analysis on the graphs recently[1]. Perhaps the authors should narrow down the analysis to graphs with specific types to meet this assumption. \n* While the empirical results on the impact of $L_{train}$ and $L_{self}$ are interesting, how the pseudo labels affect the theoretical results seems unknown. Meanwhile, why there are no results for MetaAttack for the impact of $L_{train}$ and $L_{self}$?\n* Some of the related works are missing. For example, a similar observation on the unevenness distribution is made in [2] and the authors are suggested to discuss the difference with [2] in detail. Meanwhile, some other literature[3,4,5] that analyzes the behaviors of adversarial examples from the spectral perspective are also suggested to be included in the related works.\n\n[1] Out-Of-Distribution Generalization on Graphs: A Survey, ArXiv 2022\n\n[2] Dealing with the unevenness: deeper insights in graph-based attack and defense, Machine Learning 2022\n\n[3] Adversarial Attacks on Node Embeddings via Graph Poisoning, ICML 2019\n\n[4] Not All Low-Pass Filters are Robust in Graph Convolutional Networks, NeurIPS 2021\n\n[5] Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge, TKDE 2022\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is well-written and relatively clear with interesting organizations. Note that there is a typo in Figure 2(b), where the first Testing set should be Training set. \n\nQuality:\nThe quality of the theoretical analysis is high but the assumptions need further clarification. \n\nNovelty:\nSome of the observation is also captured in previous works, but the theoretical foundation has novelty.\n\nReproducibility:\nAs the authors claimed, the proposed algorithms are easy to implement, though no codes provided weak the reproducibility.\n",
            "summary_of_the_review": "In summary, the paper has merits from the theoretical analysis and shows insights for future attack and defense designs on graph learning with practical tips. Meanwhile, my concorns are mainly from two aspects:\n* The assumption for train and test data distributions need further clarification. The discussion on the impact of pseudo labels versus real labels can be improved.\n* Some of related works should be included.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1573/Reviewer_Gcf5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1573/Reviewer_Gcf5"
        ]
    },
    {
        "id": "Sq1dWVDm3H",
        "original": null,
        "number": 4,
        "cdate": 1666964492981,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666964492981,
        "tmdate": 1666964492981,
        "tddate": null,
        "forum": "dSYoPjM5J_W",
        "replyto": "dSYoPjM5J_W",
        "invitation": "ICLR.cc/2023/Conference/Paper1573/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tries to have a better understanding of adversarial attacks on graphs. They revisit graph adversarial attack from a data distribution perspective and formulate the distribution shift: the adversarial edges are not uniformly distributed on the graph.\nThen, they provide an explanation for the effectiveness of the gradient-based attack method from a data distribution perspective and revisit both poisoning attack and evasion attack. Next, they provide several practical tips on both attack and defense and meanwhile leverage them to improve existing attack and defense methods. ",
            "strength_and_weaknesses": "\nStrength\n1. The research problem is very important and this paper provides different aspects to understand how does attackers work on graph data. \n2. They summarize some useful tricks to conduct adversarial attacks and defense. \n \n\n\n\nThere are some concerns regarding this paper.\n1. Distribution Shift is unclear to me. It seems they mainly focus on edges among train-train, train-test, test-test. It would be better if you could motivate this well and provide better explanations. \nIt's still hard to understand the distribution shift for attacking. \n\n\n\n2. This paper focuses on adversarial attacks on structure attacks, which limit their applications in various tasks and can not be well generalized to many tasks. \n\n\n\n3. As for the tips on both attack and defense, some of them have been found in Adversarial examples on graph data: Deep insights into attack and defense-2019.\nIt would be better if this paper could provide more insights into graph data from both structure and feature aspects. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper can be improved. \n\nThis paper provides different aspects to understand how does attackers work on graph data. ",
            "summary_of_the_review": "\nStrength\n1. The research problem is very important and this paper provides different aspects to understand how does attackers work on graph data. \n2. They summarize some useful tricks to conduct adversarial attacks and defense. \n \n\n\n\nThere are some concerns regarding this paper.\n1. Distribution Shift is unclear to me. It seems they mainly focus on edges among train-train, train-test, test-test. It would be better if you could motivate this well and provide better explanations. \nIt's still hard to understand the distribution shift for attacking. \n\n\n\n2. This paper focuses on adversarial attacks on structure attacks, which limit their applications in various tasks and can not be well generalized to many tasks. \n\n\n\n3. As for the tips on both attack and defense, some of them have been found in Adversarial examples on graph data: Deep insights into attack and defense-2019.\nIt would be better if this paper could provide more insights into graph data from both structure and feature aspects. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1573/Reviewer_3B87"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1573/Reviewer_3B87"
        ]
    }
]