[
    {
        "id": "O7-5-gIvdN2",
        "original": null,
        "number": 1,
        "cdate": 1666618657653,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666618657653,
        "tmdate": 1669970527648,
        "tddate": null,
        "forum": "Io0mSpdqnHJ",
        "replyto": "Io0mSpdqnHJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5701/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new latent action model based on Neural Householder Transform, where context dependent linear actuation subspaces are the outputs of an encoder-like neural network. The encoder outputs can be transformed via the exponential map on the sphere to form Household reflectors, that can be used to compute the context-dependent subspace. These local subspaces when interfaced with standard RL approaches can lead to more efficient and robust (w.r.t hyperparameters) learning in some environments.",
            "strength_and_weaknesses": "The paper is well written and the introduced approach is quite nice in my opinion.\n\nStrengths:\n\n* The Neural Householder Transform and the way it is computed, can yield a flexible way to compute locally well approximating linear subspaces, as indeed shown by the paper. \n\n* By bounding the network with a Lipschitz bound, the authors can also show that the computed transform is smooth. This result can perhaps be used computationally as well, however the authors have not done so (e.g. during training).\n\nWeaknesses:\n\n* The claims of the paper about the contributions are not rigorously demonstrated, the authors themselves use \"tend to\", to describe the effect of the framework (when compared with competing approaches). This can be remedied by either more detailed experiments, and/or by a more stringent analysis of the failures/benefits of the approach. \n\n* It is not clear if some of the NHT is the author's contribution, e.g. the particular output representation utilizing the exponential map.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is unclear at times about the NHT and the author's contribution, please see above. Otherwise the paper is quite well written and the method is well introduced. \n\nSome minor comments:\n* In the introduction: is the last paragraph meant specifically for NHT?\n* Section 2.3: in some works -> please cite\n* How do you fix k? Can it be estimated/learned? Any works that do so? \n* Is section 3.1.1. part of the contribution?\n* In Figure 3, what happens after 100 epochs? Does the alternative approach catch up?\n* How do you decide what is the right observation/context for the environments? (e.g. WAM wipe and grasp)\n* Why would/should agents trained with NHT be more robust w.r.t. different hyperparameter configurations?",
            "summary_of_the_review": "I would recommend the acceptance of the paper, see above.\n\n=== POST-REBUTTAL EDIT ===\n\nThanks to the authors for providing a careful rebuttal. As mentioned before, the method proposed in the paper is quite nice and novel in my opinion. However upon deeper reading the paper, the comments and the rebuttal, I am not convinced about the results of the paper. In particular relating to the hyperparameter-robustness, I am not convinced of the link between the NHT transform and increased robustness to hyperparameters. It's not clear to me why the context dependent linear subspaces emitted by the NHT transform would lead to such robustness, as opposed to the nonlinear dimensionality reduction achieved by LASER. I would rather expect improved learning curves, and better generalization (as I noted in my reply to the authors). \n\nAs hyperparameter-robustness seems to be the main claim of the paper (as an improvement), I'm lowering my score to 5, but I'm happy to reconsider if the reviewers would like to give some feedback on my concerns.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5701/Reviewer_2zLC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5701/Reviewer_2zLC"
        ]
    },
    {
        "id": "j79YFuWQ1o",
        "original": null,
        "number": 2,
        "cdate": 1666677731797,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677731797,
        "tmdate": 1666677731797,
        "tddate": null,
        "forum": "Io0mSpdqnHJ",
        "replyto": "Io0mSpdqnHJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5701/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed neural household transforms (NHT) for learning a low-dimensional action representation for RL problem. Prior work in this area usually learns an autoencoder-style action representation using neural networks. In this work, the authors propose to learn a network that maps to an orthogonal matrix instead of the original action space. By doing this, it enables them to avoid the need of encoding the action into the latent space as it can be obtained by an analytical least square solution. To obtain a well-behaved mapping function, exponential map is used to map the neural network output onto a unit sphere, which is then formed into the orthogonal projection matrix. The authors provided theoretical justifications for the approximation error as well as the smoothness of the proposed method. The algorithm is evaluated on two manipulation tasks and a locomotion task in simulation and is demonstrated to outperform baseline methods.",
            "strength_and_weaknesses": "Strengths:\n1. The proposed NHT algorithm is intriguing, with theoretical justifications, and without much compromise to make the method practical.\n2. Compared to alternative methods that learn a latent action space, the proposed method appears to perform better.\n\nWeaknesses:\n1. The manipulation tasks were trained with DDPG, while newer methods like SAC or TD3 has shown notably better learning performance. It\u2019s not clear how much impact the algorithm would still make given a stronger algorithm.\n2. The comparison to using the raw action space is not entirely fair. For example in the half-cheetah case an optimal policy is already trained to collect data for learning the action representation. I think a more proper way of comparison is to separate the tasks for training the action space representation and evaluating the representation performance. For example for the two manipulation tasks one can learn the action space with one task and test on the other. For the half-cheetah task one may learn the action space with a running backward policy and test with a running forward policy.\n",
            "clarity,_quality,_novelty_and_reproducibility": "good",
            "summary_of_the_review": "As discussed in the weaknesses, my main concerns for the work is the comparison fairness to the baseline as well as using a stronger learning algorithm for the manipulation tasks. I think the algorithm itself is novel and interesting in my knowledge and if the concerns were addressed I believe this would be a good one for the venue.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5701/Reviewer_1kbF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5701/Reviewer_1kbF"
        ]
    },
    {
        "id": "0yPo1dUGKZ7",
        "original": null,
        "number": 3,
        "cdate": 1667255202949,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667255202949,
        "tmdate": 1667255202949,
        "tddate": null,
        "forum": "Io0mSpdqnHJ",
        "replyto": "Io0mSpdqnHJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5701/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to learn a linear subspace for actions to control agents. It is well-known in the RL community that choice of action space has a dramatic effect on the learning process. For example, end-effector control in robotic manipulation is much easier than direct torque control for several tasks. Thus, learning better action spaces is an important research problem. This work specifically aims to learn local linear subspaces for actions, and utilizes NHT for efficient learning and inference. The paper also demonstrates that subspaces learned using NHT have desirable local smoothness properties.",
            "strength_and_weaknesses": "The paper proposes a nice algorithmic approach for the problem studied -- i.e. finding local linear sub-spaces for actions. However, the paper does not do a good job of motivating why this is an important problem. What are the specific advantages of linear subspaces? Why is it preferable to nonlinear per-timestep action-space mappings (nonlinear latent actions) and trajectory-level action-spaces (motor primitives)?",
            "clarity,_quality,_novelty_and_reproducibility": "- The main baseline, besides the simple SVD baseline, is LASER. However, the paper does not review or explain what this baseline is. I would encourage the authors to review this method to situate their work in better context. \n- Per my quick read, LASER seems to be a representative algorithm for the latent actions framework. The paper can compare against a representative algorithm from the motor program framework.\n- The paper can better motivate the importance of the research question they study and specifically the advantages of a linear subspace for actions.\n",
            "summary_of_the_review": "It is unclear why this problem is important and relevant to the NeurIPS community. The authors can either motivate the problem statement better or submit to a more specialized conference where it might better appeal to the audience. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5701/Reviewer_XySg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5701/Reviewer_XySg"
        ]
    }
]