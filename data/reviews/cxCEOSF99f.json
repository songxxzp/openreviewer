[
    {
        "id": "S_cL9YKa2E",
        "original": null,
        "number": 1,
        "cdate": 1666344702397,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666344702397,
        "tmdate": 1666344702397,
        "tddate": null,
        "forum": "cxCEOSF99f",
        "replyto": "cxCEOSF99f",
        "invitation": "ICLR.cc/2023/Conference/Paper5414/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper revealed the connection of influence function and the linear datamodel using harmonic analysis.\nLinear datamodel is a model fitted with an $N$-dimensional binary vector as its input and with the outcome (or loss) of a model trained on a subset of $N$ training points indicated by the binary vector as its regression target.\nA recent study observed that the influence function (more formally, discrete influence function) can be approximated by this linear datamodel.\nThis paper provides a unified view of this observation why such an approximation is possible.\nLet $f$ be a training algorithm that maps the $N$ dimensional binary vector to the target.\nFor the analysis, the authors considered expanding $f$ using an orthonormal basis in $N$ dimensional space.\nThis expansion allows us to view $f$ as a function on the $N$ dimensional grid rather than the training algorithm.\nWith this expansion, we can see that discrete influence function corresponds to the coefficient of the degree-1 basis. \nThis finding explains why the linear datamodel can approximate discrete influence function.\nIf the coefficients of $f$ in the higher-degree basis are small, the basis of degree-1 can well approximate $f$.\nThe authors also analyzed a case when this approximation is possible by introducing the notion of noise stability.\nThe authors also provided an efficient algorithm for estimating the approximation error.\n",
            "strength_and_weaknesses": "### Strength\nThe major strength of the paper is on the characterization of the connections between influence function and the linear datamodel.\nThis characterization successfully explained the finding of the linear datamodel.\nThe authors also found some counter cases when the connection fails.\nThe authors further provided an efficient way to estimate the fitting error so that one can assess whether the linear datamodel is valid on the dataset at hand.\n\n### Weakness\nI do not find any crucial weakness.\nAs a minor suggestion, I think the paper becomes much easier for the readers if there is a brief overview of the linear datamodel.\nThe linear datamodel is not that popular compared to influence function.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nI think the paper is written clearly and the main idea and the results are easy to follow.\nAs a minor suggestion, I think the paper becomes much easier for the readers if there is a brief overview of the linear datamodel.\nThe linear datamodel is not that popular compared to influence function.\n\n### Quality, Novelty\nThe problem considered in this paper, characterizing the connection between influence function and the linear data model, is an interesting problem and the finding would be novel.\nThe finding explains the success of the linear datamodel.\n\n### Reproducibility\nThe experimental setups are provided in appendix.\nThe results will be reproducible.",
            "summary_of_the_review": "This paper provide the characterization of the connections between influence function and the linear data model.\nThis characterization successfully explained the finding of the linear datamodel.\nThis is an interesting problem and the finding would be novel.\nThe paper also provides some counter cases when the connection fails, and also an efficient way to estimate the fitting error so that one can assess whether the linear datamodel is valid on the dataset at hand.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5414/Reviewer_cauj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5414/Reviewer_cauj"
        ]
    },
    {
        "id": "xmLaAdeMJZw",
        "original": null,
        "number": 2,
        "cdate": 1666538593007,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538593007,
        "tmdate": 1666538593007,
        "tddate": null,
        "forum": "cxCEOSF99f",
        "replyto": "cxCEOSF99f",
        "invitation": "ICLR.cc/2023/Conference/Paper5414/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on a very interesting and important research topic, predicting the predictions from training data. Built on the datamodels proposed in [1], this paper seeks to provide a better theoretical understanding of why a linear regression method can predict the effect of training data. More importantly, this paper designs a new algorithm for estimating the approximated linear datamodel with much less training cost. This paper provides an exciting view to the counter-factual explanations of training data.\n\n[1]Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data, ICML 2022",
            "strength_and_weaknesses": "Strength:\n1. The research problem, analyzing the influence of data points, is very interesting and important for explainable AI.\n2. The paper provides a theoretical analysis of the linear datamodel, and explained why the effects of number of points deleted is observed in practice as linear rather than exponential.\n3. Experimental results verified the proposed method\n\nWeaknesses:\n1. This paper is not friendly to most beginners, and the main idea is hard to follow. The authors also need to explain more detailedly how the residual estimation can predict the performance.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is novel and interesting while hard to read.",
            "summary_of_the_review": "This paper is generally well-written and novel, and the authors focus on an interesting research problem. I'd like to accept this paper and appreciate if the author can revise the paper more clearly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5414/Reviewer_rQ2x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5414/Reviewer_rQ2x"
        ]
    },
    {
        "id": "6svst48VQO",
        "original": null,
        "number": 3,
        "cdate": 1666567557902,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666567557902,
        "tmdate": 1666567557902,
        "tddate": null,
        "forum": "cxCEOSF99f",
        "replyto": "cxCEOSF99f",
        "invitation": "ICLR.cc/2023/Conference/Paper5414/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper uses theoretical tools from harmonic analysis and noise stability to explain the effectiveness of data-models introduced by Ilyas et. al (2022). In addition, the same theoretical components are used to analyze group influence and identify conditions when first-order influence (linear in terms of training data influence) can be used to characterize group influence. ",
            "strength_and_weaknesses": "Strengths:\n\n\t- While the framework of datamodels empirically approximate training data influence, the original paper does not show any understanding of why it works in the first place. This paper, provides a starting point to analyse datamodels and further understand why it works. This is one of the major strengths of the paper. \n\n\t- The noise stability estimator is a good contribution to bypass the expensive process of training the datamodels (though see Weakness for a follow-up question). \n\n\nWhile I am not an expert in harmonic analysis, there are certain questions regarding the paper:\n\nWeakness / Questions:\n\n\t- While Ilyas et. Al (2022), show that datamodels can approximate group influence (via linearity), as shown in previous works [1], this linearity assumption is not always true, especially when a large number of points are deleted. In such cases, how does datamodels relate to higher-order variants of influence which seem to be a better approximation for group influence? \n\n\t- The paper has a good theoretical contribution, however I would like to see some more empirical analysis on how the noise stability estimator is a good proxy for predicting the fit. One way could be to run similar experiments on datasets beyond CIFAR-10 (e.g., Imagenet derivatives) to strengthen it's effectiveness.\n\nThe paper is difficult to read for someone who has limited background on harmonic analysis; Considering influence functions is used by a mix of empirical + theoretical researchers, it would be good to provide a small background on the topic in the Appendix or in the main paper (e.g., a brief background of the theoretical tools used in your analysis). \n\n\n[1]. https://arxiv.org/abs/1911.00418\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. However, as mentioned in the previous section, it would be beneficial to give a more informative background about the theoretical tools used in the paper.",
            "summary_of_the_review": "Overall,  I feel that the theoretical contribution is a good initial step towards understanding data-models and finding links between influence estimation and datamodels. While the authors connect the theory to empirical findings (e.g., prediction of the fit), this section can be strengthened with more experiments on datasets beyond CIFAR-10. \n\nOverall, the paper can be a good addition to iclr as it provides some theoretical footing connecting datamodels to  influence functions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5414/Reviewer_TeFi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5414/Reviewer_TeFi"
        ]
    },
    {
        "id": "_kLP_UdQBX",
        "original": null,
        "number": 4,
        "cdate": 1666572257076,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666572257076,
        "tmdate": 1666572257076,
        "tddate": null,
        "forum": "cxCEOSF99f",
        "replyto": "cxCEOSF99f",
        "invitation": "ICLR.cc/2023/Conference/Paper5414/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides some mathematical understanding on influence functions and datamodels via harmonic analysis including\n1. reasons for existence of datamodels.\n2. exact characterizations of \\theta_i for datamodels without/with abbitrary regularization.\n3. providing a new algorithm to estimate the degree whether a linear datamodel is well-approximated without having to train the datamodel per se.\n4. studing group influence which quantifies the effect of adding or deleting a set.",
            "strength_and_weaknesses": "Strength:\n1. This paper introduces harmonic analysis into the discussion of influence functions and datamodels, which is a novel idea.\n2. This paper gives a new algorithm to estimate the degree whether a linear datamodel is well-approximated without having to train the datamodel per se.\n\nWeakness:\n1. To my knowledge, all the analysis in this paper is based on a fixed test point, which means that we need to repeat the algorithm proposed in this paper with $m$ times when we need to estimate $m$ test points.\n2. In Section 3.1, the author claimed that normalized noise stability should be high, and perhaps close to its maximum value of $1$ when the number of training samples grows, intuitively. Then Theorem 3.2. gives a bound on the best linear approximation in terms of the magnitude of the residual error because $1- \\overline{h} (\\rho)$ is close to $0$. But if I understand correctly, $\\overline{h} (\\rho)$ is close to $1$ when $\\rho$ is close to $1$ intuitively. The bound given in Theorem 3.2 is meaningless at this point. In consequence, I think this intuition lacks theoretical or experimental evidence. I am willing to raise my score if the author can explain this question theoretically or experimentally.\n3. It becomes worthless to estimate the quality of the best linear datamodel when it becomes a question whether datamodel is a good approximation.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is written clearly and well organized. There are a few flaws. For example, in page 2 line 2, author used symbol \u2018ith\u2019 but \u2018i-th\u2019 in other parts to represent the same thing.",
            "summary_of_the_review": "This paper introduces harmonic analysis into the discussion of influence functions and datamodels and gain some novel results. But the main theorem proofs based on the intuition. Generally, I think harmonic analysis is a useful and relatively novel tool for machine learning.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5414/Reviewer_ECUW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5414/Reviewer_ECUW"
        ]
    }
]