[
    {
        "id": "nXV-HOfDzjm",
        "original": null,
        "number": 1,
        "cdate": 1665726428829,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665726428829,
        "tmdate": 1668669486847,
        "tddate": null,
        "forum": "eGm22rqG93",
        "replyto": "eGm22rqG93",
        "invitation": "ICLR.cc/2023/Conference/Paper228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper diagnosis two shortcomings of existing approaches to fully test-time adaption and proposes two methodological extensions to overcome these shortcomings. The first shortcoming is that estimates of means and variances in batch normalization (BN) have relatively high error on a per-batch level; this is addressed by replacing BN by batch renormalization (Ioffe, 2017). The second shortcoming identified is that self-learning in test-time adaptation can reinforce imbalanced class distributions; this is addressed by a dynamic online re-weighting of test samples.",
            "strength_and_weaknesses": "Strength:\n * Introduction and Related Works are well written.\n * The presentation and motivation of the methods is nicely structured (I like the split into diagnosis and treatment)\n * Using Batch Renormalization (BRN) instead of Batch Normalization is a simple but very good idea\n * The experimental evaluation is extensive and supports the proposed methods\n\nWeaknesses:\n * Confusing use of terminology: \n    + Using IID to refer to a setting with covariate shift ($p^{train}(x|y) \\neq p^{test}(x|y)$) because samples are drawn independently is misleading since IID typically refers to situation where the test data is drawn IID from the training distribution. A better term might be \"Independent Samples + Covariate Shift\" (IS-CS) vs \"Dependent Samples + Covariate Shift\" (DS +CS)\n    + Similarly, denoting class distribution shift ($P^{train}(y) \\neq P^{test}(y)$) as \"Class Imbalance\" is confusing. This indicates that the author assume that $P^{train}(y)$ is uniform over classes (and $P^{test}(y)$ is not) - the experiments are also conducted in this setting but it is a more special setting than dealing with $P^{train}(y) \\neq P^{test}(y)$. Thus, the paper should either make more clear that $P^{train}(y)$ is assumed as the uniform distribution over classes or the term \"Class Shift\" should be used instead of \"Class Imbalance\".\n    + Section 3.2 confuses the \"bias\" and the \"variance\" of an estimator: judging from Figure 2, \"BN adapt\" has an empirical mean very close to \"Global\" and thus provides a (nearly) unbiased estimate of the global mean/standard deviation. However, as the authors not, \"BN adapt\" has high fluctuations and thus provides a high _variance_ estimate of the global mean/standard deviation. Accordingly, Batch Renormalization is a variance reduction technique here and not a debiasing technique.\n   + Also, the usage of the term \"bias\" in Section 3.3 is unfortunate: TENT etc. do not have an intrinsic _bias_ towards certain classes and it is also no a priori necessary to \"overweight\" rarely predicted classes; only if one assumes that  $P^{test}(y)$ is uniform, an intervention like overweighting is recommended (and such an intervention actually _biases_ the method to predicting classes uniformly in expectation).\n   + In summary, the methods discussed in this paper are not \"debiasing\" but rather a variance reduction technique and a useful inductive biases (uniform class frequency). It is unfortunate that the term \"debiased\" even made it into the title of the paper and the name of the method.\n * Doubts about the method DOT: \n   * The authors motivate DOT with \"[the] common constraint of making the output distribution uniform Liang et al. (2020) is no longer reasonable.\". However, it seems that DOT is doing exactly that: assigning weight to samples inversely proportional to the frequency of their predicted label encourages the output distribution to be uniform. Can the authors comment on that?\n   * Using a \"hard label\" in DOT (line 5 in Algorithm 1) seems suboptimal. Why not use the soft labels?\n   * The weight in line 6 can in principle become arbitrary large; it seems more appropriate to define it as $1 / (z_t[k^*_{m_t + b}] + \\epsilon)$ for some small $\\epsilon$\n   * Comparison to other approaches for controlling the output distribution are missing. For instance, Mummadi et al. (https://arxiv.org/abs/2106.14999) proposed a KL-based diversity regularizer for matching the empirical output distribution to a prior output distribution. ",
            "clarity,_quality,_novelty_and_reproducibility": "The overall structure and exposition of the material in the paper is good. There is, however, a confusing misuse of terminology like \"bias\" or \"IID\" (as discussed above) that is even contained in the title and reduces the overall clarity of the paper. The first main contribution (using BRN for test-time adaptation) is novel and well supported. The second contribution DOT has some questionable design choices that require clarification (see above).\n\nMinor points:\n * The authors should clarify how hyperparameters like $\\alpha$ and $\\lambda$ have been selected.\n * Figure 3 is confusing: sometimes methods correspond to lines and different settings go into different plots (a and b) and sometimes methods go to different plots and settings are different lines (c and d). Could this be presented more consistently?\n * Table 3: Standard deviation could be presented instead of variance because this is essential linear in the range and not quadratic. \n * Could the authors clarify why SHOT's applicability is limited? (as of the related works)\n * In Figure 1, the CI setting is still relatively balanced. For clarity of exposition, it could be helpful to have a more imbalanced class distribution here.\n * The sentence 'LAME (Boudiaf et al., 2022) is a relatively \u201cconservative\u201d method, which does not rectify the model\u2019s parameters but only the model\u2019s output. ' does not well explain what LAME does.",
            "summary_of_the_review": "The paper contains novel contributions that are well motivated and have sufficient empirical support. However, the paper in its current form, lacks clarity since some basic terminologies are used inconsistently with their definitions in statistics/machine learning. Moreover, there remain some doubts regarding DOT that need to be addressed. While the paper has sufficiently novel contributions, is needs a major revision before it can be accepted in my opinion.\n\n## Update after author response ##\nThe authors have largely revised the manuscript and addressed my main concerns. I am no leaning towards recommending acceptance of this paper and have improved my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper228/Reviewer_MFwz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper228/Reviewer_MFwz"
        ]
    },
    {
        "id": "zKlsqTLD5m",
        "original": null,
        "number": 2,
        "cdate": 1666526950079,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666526950079,
        "tmdate": 1667445565124,
        "tddate": null,
        "forum": "eGm22rqG93",
        "replyto": "eGm22rqG93",
        "invitation": "ICLR.cc/2023/Conference/Paper228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "**Summary:**\n\nThis paper presents a new Test-time adaptation (TTA) method called DELTA for debiased fully TTA. To be specific, the authors 1) introduce batch renormalization to alleviate the bias in normalization statistics, 2) propose dynamic online re-weighting (DOT) to address the class bias within optimization.",
            "strength_and_weaknesses": "**Positive Points:**\n\n - The authors point out the normalization statistics may be biased with the current test mini-batch and the optimization gradient may be biased with the dominant class.\n\n - The proposed methods including TBR and DOT are simple and effective.\n\n **Negative Points:**\n\n - In Section 3.2, I suggest the authors can provide more descriptions of stopping gradient operation. In my opinion, the word \"stopping gradient\" is confusing and I can not understand the meaning of this. Some readers may be unfamiliar with this and have to check for the orginal batch renormalization paper (Ioffe, 2017). Beside, the batch renormalization seems to be different from the original one. Thus I strongly recommend the authors can provide a detailed Algorithm of TBR like Algorithm 1 (DOT) of the paper.\n\n - About TBR, some details are missing. What is the output of batch renormalization? $v^*$ or $\\gamma v^* + \\beta$? And do $\\hat{\\sigma}^{ema}$ and $\\hat{\\mu}^{ema}$ need to be updated?\n\n- I noitice the authors mentioned \"TBR discards the warm-up and truncation operation\". Thus, in this case, $v^{*} = (v - \\hat{\\mu}^{batch}) / \\hat{\\sigma}^{batch} \\cdot r + d = (v - \\hat{\\mu}^{batch}) / \\hat{\\sigma}^{batch} \\cdot (\\hat{\\sigma}^{batch} / \\hat{\\mu}^{ema}) + (\\hat{\\mu}^{batch} - \\hat{\\mu}^{ema}) / \\hat{\\sigma}^{ema} = (v- \\hat{\\mu}^{ema}) / \\hat{\\sigma}^{ema}$. I found that it is the same as the original BN because the authors remove truncation operation, right? The differences between the proposed TBR and the regular BN are unclear.\n\n - In the analysis of the results in Table 2, the authors state \"Different from the TEMA, TBR is well compatible with gradient-based adaptation methods\". Could the authors give more explanations?\n\n - In Table 3, what are variance and range? Do the authors calculate them in some features of a layer?\n\n - In Table 3, what are the differences between the baselines efficient test-time adaptation (ETA) and entropy-based weighting (Ent-W)?\n\n  - Would the authors release the code upon acceptance?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-writen and novel. The proposed TBW is not described clearly and I can not reproduce it.",
            "summary_of_the_review": "The proposed method alleviates the bias resulted from the small mini-batch and dominant class, which is simple and effective. Thus I vote for accept. But some importance details in TBR are missing or unclear. I hope the authors can address my concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper228/Reviewer_BBcu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper228/Reviewer_BBcu"
        ]
    },
    {
        "id": "AIekOYWkFQa",
        "original": null,
        "number": 3,
        "cdate": 1666649957832,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649957832,
        "tmdate": 1669303678137,
        "tddate": null,
        "forum": "eGm22rqG93",
        "replyto": "eGm22rqG93",
        "invitation": "ICLR.cc/2023/Conference/Paper228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes two improvements to test time adaptation in the streaming data setting: one to address the possibility of non IID test data, and one to address the possibility of class imbalance in the test distribution. These improvements are relatively plug-and-play with existing popular adaptation approaches, e.g., adaptation via batch normalization and entropy minimization. Taken together, these improvements seem to consistently improve adaptation performance on CIFAR and ImageNet distribution shift test sets in which non-IID-ness and class imbalance are artificially introduced.",
            "strength_and_weaknesses": "Strengths\n---\n\n- The paper provides a number of experiments and ablations which demonstrate the effectiveness of the proposed approach.\n- The paper is reasonably well written and well motivated.\n- The paper studies a potentially important scenario that has received less attention in prior papers studying test time adaptation to distribution shift.\n\nWeaknesses\n---\n\n- There are no error bars in the experimental results.\n- Use of the term \"bias\" seems incorrect -- see next section for more detail.\n- There are no experiments studying a real world scenario in which the problems of non-IID-ness and class imbalance occur naturally.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality\n---\n\nOverall, the paper is of relatively high quality. There are two main ways to improve the paper's quality, both of which are important. First, without some notion of error bars in the experimental results, it is not always straightforward to tell whether a result really is significant. The most obvious example of this is Table 7, in which the proposed method is claimed to improve performance on the in distribution CIFAR-100 test set by 0.1%, but this seems within the range of noise. Certainly some aspects of the evaluation pipeline are random, e.g., the order in which test points are presented, and all randomized aspects should be run with multiple seeds and the standard error of the final results should be reported in all tables.\n\nSecond, the paper would benefit greatly from having experiments on a real world application exhibiting non-IID-ness and class imbalance. Video prediction datasets would be a natural fit here. Right now, it is up to the reader to decide whether the studied setting is even of practical relevance, and while personally I believe it is, it is best to not leave this up to the reader. Furthermore, experiments in such a setting would simply serve to determine whether the proposed method truly works as advertised, on naturally occurring problems rather than synthetically created ones.\n\nClarity\n---\n\nOverall, the paper is well written. My only major concern is that I do not believe \"bias\" is the correct term to describe the fact that the BN statistics \"are far from the ideal global values and fluctuate dramatically during adaptation\" (page 4). Is this not variance?\n\nOriginality\n---\n\nAs stated, the problem setting considered by this paper is relatively understudied in the test time adaptation literature and of potential importance. My main concern related to originality is still that there is no clear example of a real world application exhibiting the properties of the proposed problem setting.",
            "summary_of_the_review": "In summary, the paper is reasonably well written and the experiments are sound, but there are several important improvements that can be made as laid out above. I am recommending weak reject and am happy to discuss further.\n\nEdit after author response\n---\n\nI apologize for my late response. I believe that the authors have adequately addressed my concerns and I am updating my recommendation to weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper228/Reviewer_Hnwy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper228/Reviewer_Hnwy"
        ]
    },
    {
        "id": "ojL16JigFlt",
        "original": null,
        "number": 4,
        "cdate": 1666685494531,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685494531,
        "tmdate": 1666685762969,
        "tddate": null,
        "forum": "eGm22rqG93",
        "replyto": "eGm22rqG93",
        "invitation": "ICLR.cc/2023/Conference/Paper228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method named Debiased Fully Test-time Adaptation (DELTA) to address the biased issue in test-time adaptation. Specifically, the authors conduct experiments to verify the claims that 1) the normalization statistics tend to fit the current test mini-batch, and 2) the test-time adaptation optimization would bias to some dominant classes. The authors first adopt the renormalization technique to alleviate the biases in the normalization statistics of batch normalization, and then devise a re-weighting module to assign different weights for test samples with different pseudo labels to address the optimization issue. Extensive experiments on the ImageNet-C, ImageNet-R, and CIFAR 100-C demonstrate the effectiveness of the proposed method. However, I have some concerns about this paper. My detailed comments are as follows. ",
            "strength_and_weaknesses": "Strength:\n\n1.\tThis paper introduces a new setting, namely the class imbalance issue during the test-time process, in the test-time adaptation.\n\n2.\tThe authors dig out the issues of existing test-time adaptation methods (i.e., the bias issue) from a new perspective.\n\n3.\tExtensive experiments on the ImageNet-C, ImageNet-R, and CIFAR 100-C demonstrate the effectiveness of the proposed method.\n\nWeakness:\n\n1.\tThe technical contribution is not very significant. For example, the TBR just adopts the batch renormalization technique from the initial Batch Normalization paper, and the re-weighting technique is derived from class-wise re-weighting.\n\n2.\tAs referred to the Treatment II, some components of DOT have multiple options. However, the ablation studies about the components of DOT and the selection of the components of DOT may be missing.\n\n3.\tIn Table 8, when conducting experiments on the real-world out-of-distribution dataset ImageNet-R, the improvement of DELTA is marginal compared with that on the ImageNet-C dataset, more discussions are required.\n\n4.\tIn Table 5, how about the performance of DELTA with \\rho<0.1 (e.g., 0.01?), i.e., totally concentrate on one same class during a period. Similarly, could the authors provide more results regarding smaller \\pi<0.05 (e.g., 0.001)? I am curious about these results since any value of \\pi and \\rho may appear in practice.\n\n5.\tIn Table 10, could the authors further provide the results of Div-W+Fisher regularization (namely EATA, this is the full version of ETA in Niu et al, 2022)?\n\n6.\tFigure 1 is somewhat confusing. It is hard to distinguish the difference between IID and CI, non-IID and CI & non-IID.\n\n7.\tIn Section 3.1, it would be better to detail describe each scenario and the difference between the previous TTA setting.\n\n8.\tIn Treatment II of Section 3.3, it would be better to extend the \u201cL x\u201d to \u201cLine x of Algorithm 1\u201d to improve the readability of the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The class imbalance settings of TTA and the perspective of addressing the issue of TTA are novel. ",
            "summary_of_the_review": "This paper proposes a new setting of test-time adaptation, namely class imbalance in the mini-batch, and proposes to address this problem from the de-bias perspective, which is interesting. Though the pure technical contribution is not very significant, it is new in the area of TTA. Some results are still missing to convince me regarding its effectiveness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper228/Reviewer_E7Gz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper228/Reviewer_E7Gz"
        ]
    }
]