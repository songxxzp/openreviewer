[
    {
        "id": "cAiYTkJmQvr",
        "original": null,
        "number": 1,
        "cdate": 1666652295245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652295245,
        "tmdate": 1666697648396,
        "tddate": null,
        "forum": "B8_T6-8-tCU",
        "replyto": "B8_T6-8-tCU",
        "invitation": "ICLR.cc/2023/Conference/Paper2805/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper attempts to study the generalisation properties of one-dimensional ReLU networks directly, instead of via the usual VC-dimension or Rademacher complexity-type bounds. \n\nTo this end, they *geometrically* characterise the space of one-layer ReLU networks with a single linear unit, which *fit* a given dataset $D \\subset \\mathbb{R}^2$ *while achieving the minimum possible $\\ell_2$ norm among the neuron weights*.  \n\nMain Results: \nA geometric characterisation of all one-layer ReLU networks that exactly fit a univariate dataset $D \\subset \\mathbb R^2$. Since ReLU networks in this context are just piecewise linear functions, this turns out to be described by the constraint that the network fits the data while achieving as few inflection points as possible. \n\nCorollary: Interpolating ReLU networks with $\\ell_2$ regularisation achieve best possible $\\ell_\\infty$ generalisation error for one-dimensional Lipschitz functions. \n\nTheir work builds on the observation due to Neyshabur(2014) that $\\ell_2$ regularisation in the setting considered is equivalent to penalising the TV distance of the derivative of the network function. This observation applies to any dimension, but the complete characterisation in this paper is only for one-dimensional neural networks. \n",
            "strength_and_weaknesses": "Weaknesses:\n1. The paper does not study optimization, only the generalisation guarantees that are achieved by the minimiser. \n2. Only the one-dimensional setting is studied. \n\nStrengths: \n1. The authors describe an algorithm which is a consequence of the theorem, that allows them to describe all elements that might minimise the $\\ell_2$ norm of the weights while interpolating the dataset. \n2. I personally like the attempt at a geometric characterisation of the min-cost interpolating function, and generalisation guarantees that might arise from that. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written clearly. \nQuality: I think this is a good paper. \nNovelty: The geometric characterisation is novel. \nReproducibility: This is not an empirical paper. \n",
            "summary_of_the_review": "This paper gives a complete characterisation of the $\\ell_2$ minimizing univariate ReLU network. \n\nI think this is a complete result for a simple setting that provides interesting insights into the generalisation as well as geometric properties of the minimiser, and vote to accept it. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2805/Reviewer_oSFb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2805/Reviewer_oSFb"
        ]
    },
    {
        "id": "cBDi4-XkoL",
        "original": null,
        "number": 2,
        "cdate": 1666678217665,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678217665,
        "tmdate": 1666678217665,
        "tddate": null,
        "forum": "B8_T6-8-tCU",
        "replyto": "B8_T6-8-tCU",
        "invitation": "ICLR.cc/2023/Conference/Paper2805/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers a one-layer ReLU network that only considers 1-D input. The authors study a set of solutions RidgelessReLU(D) that has the smallest norm of weights among all interpolators of the training samples. The authors show that the functions in such a set satisfy certain requirements by examining the second-order discrete derivative. Based on these requirements, this paper provides a generalization upper bound on Lipschitz functions when the 1-D input is uniformly spaced (deterministically) in [0, 1], and argues that a similar result also holds if the 1-D input is drawn independently at random from [0, 1].",
            "strength_and_weaknesses": "Strength: The presentation is relatively clear.\nWeakness: The setup is too restrictive and not justified properly. See my comments below.",
            "clarity,_quality,_novelty_and_reproducibility": "The setup is too restrictive (e.g., needs 1-D input) and not justified properly. The following are some detailed comments.\n\n1. Minimizing the l_2-norm of all neuron weights is not weight decay. The authors confirm this at the end of Page~3. While the authors argue that they do not study optimization and do not care whether such a setup has any relationship with the actual gradient descent with weight decay, the title of this paper still highlights the weight decay, which in my opinion is totally misleading and confusing.\n\n2. The requirement of Corollary 1.1 is restrictive, as it needs all training inputs uniformly spaced (deterministically) in [0, 1]. Although the authors argue that a similar result also holds if the 1-D input is drawn independently at random from [0, 1], no rigorous proof is provided and thus such a claim is less convincing.\n\n3. Figs. 1~7 take too much space (almost 2.5 pages) and are repetitive. No simulations on the test error are provided.",
            "summary_of_the_review": "I don't think this paper will be helpful in understanding the performance of existing neural networks, because of 1) an oversimplified setup, and 2) no connection/similarity with the actual gradient descent with weight decay used in the neural network.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2805/Reviewer_LdBk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2805/Reviewer_LdBk"
        ]
    },
    {
        "id": "r9yjPhOWZW",
        "original": null,
        "number": 3,
        "cdate": 1666684542348,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684542348,
        "tmdate": 1666684740876,
        "tddate": null,
        "forum": "B8_T6-8-tCU",
        "replyto": "B8_T6-8-tCU",
        "invitation": "ICLR.cc/2023/Conference/Paper2805/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the implicit bias of univariate one-hidden layer ReLU networks with infinitesimal weight decay. In this setting, the authors derive a geometric description for all the networks that interpolate some given training data and attain the minimal value of the $\\ell_2$ norm of the neuron\u2019s weights. Then, the authors use this result to obtain a generalization guarantee for learning $1d$ Lipschitz functions.",
            "strength_and_weaknesses": "**Strengths:**\n- The paper is well written and explains clearly how the results relate to previous works.\n- The generalization guarantee for ReLU networks is an interesting and novel result, to the best of my knowledge.\n\n**Weaknesses:**\n* The main weakness is that the setting is quite restricted. The results only apply to univariate one hidden layer ReLU networks which interpolate the training data and minimize a $\\ell_2$ penalty. As the authors mention, the penalty does not regularize the linear terms in the ReLU network, which is not standard practice. I think that the results will be much stronger if the authors either:\n    + Supply some optimization result that motivates studying this setting.\n    + Extend the results to the multivariate setting using the results from Ongie et al (2019).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The assumptions and results are clearly stated, and the results given in the paper are new, to the best of my knowledge.",
            "summary_of_the_review": "Overall, I think that the paper's contributions are not sufficient for acceptance, as discussed under \u201cweaknesses\u201d. Supplying some additional results, as suggested above, will significantly strengthen the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2805/Reviewer_Y8Pp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2805/Reviewer_Y8Pp"
        ]
    },
    {
        "id": "A8mKCfwfCy",
        "original": null,
        "number": 4,
        "cdate": 1667090011165,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667090011165,
        "tmdate": 1667090011165,
        "tddate": null,
        "forum": "B8_T6-8-tCU",
        "replyto": "B8_T6-8-tCU",
        "invitation": "ICLR.cc/2023/Conference/Paper2805/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper asks a very interesting question about implicit bias of neural training - in particular about interpolating depth 2 nets. Instead of trying to ask which of these interpolating nets would some training algorithm find, this paper tries to come up with a geometrical description of a subset of these nets which have a certain minimum norm condition on its parameters - when the input and the output dimension of the net is 1. \n\nAlso the paper chooses to work with an unconventional class of nets which have an additional linear unit. But then one is left to wonder why the authors' didn't frame it as a result about R -> R depth-2 ResNets - which is what this looks like. There are a few  more ambiguities like this which I shall point out in the later parts of the review.  ",
            "strength_and_weaknesses": "The framing of the question is quite interesting and its generalization implications are intriguing. The paper zones into a very special case and solves it in details. The approach is very appreciable.  \n\nThe weakness of the paper is in its organization of the main proof.",
            "clarity,_quality,_novelty_and_reproducibility": "The main part of the paper is quite clear and well-motivated. But the proof of the main theorem 1.3 is somewhat hard to piece together. It seems scattered through the Appendix. There needs to be a better organization of this - maybe a summary of the proof in the main paper which walks the reader through the steps.",
            "summary_of_the_review": "Its a very nice piece of work and I think it can lead to some very interesting further work. My main complaint is with the writing/organization of the main theorem's proof - and some claims which seem to have feet of clay - like the following line at the end of page 3,  \n\n\"The elements of RidgelessReLU(D) can intuitively be thought of as all univariate one layer ReLU networks that minimize a weakly penalized loss...and the strength lambda of the weight decay penalty is infinitesimal.\"\n\nEither this is a claim that the authors should be be able to put in the proof for - or they should formally state this as a conjecture. \nCurrently the wording is quite dodgy. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2805/Reviewer_izUL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2805/Reviewer_izUL"
        ]
    }
]