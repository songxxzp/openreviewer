[
    {
        "id": "T5sEIdtXS4",
        "original": null,
        "number": 1,
        "cdate": 1666535319328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535319328,
        "tmdate": 1666535319328,
        "tddate": null,
        "forum": "GGx4EVTZU5r",
        "replyto": "GGx4EVTZU5r",
        "invitation": "ICLR.cc/2023/Conference/Paper1023/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes FedDS to improve the performance of the global model with distillation-based model aggregation. The key idea is to suppress the contributions of unreliable clients when performing multi-teacher ensemble distillation. Compared to FedDF, the authors added two additional components, the entropy-weighted pseudo label and the self-supervised learning (SSL) loss. The former calculates the uncertainty of each teacher's prediction score for weighting a teacher\u2019s output. The latter can be an arbitrary SSL method. The experimental results verify the effectiveness of the FedDS to some extent.",
            "strength_and_weaknesses": "Strengths:\n1. The motivation is good. It is intuitive to suppress the contribution of unreliable clients.\n2. The overall paper is well organized and easy to follow.\n\nWeaknesses:\n1. The novelty of the proposed method is limited. The overall method is FedDF + entropy-weighted + SSL loss. For \u2018entropy-weighted\u2019, the authors should explain more why entropy can be used to measure the uncertainty of classification. For \u2018SSL loss\u2019, in learning with unlabeled data, SSL loss is commonly used and can be added in any distillation-based model aggregation method. These two components added to FedDF are incremental.\n2. The experiments are unconvincing. In most experimental settings, there are only four clients. Unless in the cross-silo setting, there should be at least 50 clients in the cross-device setting. \n3. The proposed method needs to be tested on a larger dataset, the conclusion on CIFAR is not convincing. \n4. The results shown in Figure 3 seems meaningless. The relationship between \u2018softmax value for true class\u2019 and \u2018entropy-weight\u2019 is inherent (because the former is calculated using the latter). How does this result relate to the motivation or performance of FedDS? \n5. It is mentioned in 4.1 EXPERIMENT SETUP that \u2018the total client training dataset is sampled in such a way that it has the same class distribution as the entire training dataset\u2019, which means that the server dataset has the same distribution as those of all the clients. Under such a setting, it is not surprising that the scheme based on weighted distillation is effective, because the difference between the dataset used in distillation and the client datasets is too small.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is easy to follow, but some details are missing or unclear. \n- The novelty of the paper is limited.\n- The code is not provided.",
            "summary_of_the_review": "The paper has a good motivation. However, the novelty (FedDF + entropy-weighted + SSL loss) is limited. The experiments are unconvincing that in most experiments there are only four clients and the dataset is small.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1023/Reviewer_CynN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1023/Reviewer_CynN"
        ]
    },
    {
        "id": "NckmeVdKBhI",
        "original": null,
        "number": 2,
        "cdate": 1666683913443,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683913443,
        "tmdate": 1666683913443,
        "tddate": null,
        "forum": "GGx4EVTZU5r",
        "replyto": "GGx4EVTZU5r",
        "invitation": "ICLR.cc/2023/Conference/Paper1023/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, authors explore how to utilize unlabeled data on server to improve upon current Federated Learning methods. Authors propose (1) using entropy-weighted ensemble of client models to create pseudo labels for distillation training of global model (2) directly using SSL method with unlabeled data to help to train global model. Authors show compared with previous baselines such as FedAVG, FedDF and FedAVG-S, their proposed method shows significant increase in final accuracy. Also, authors show that their method demonstrates more robust performance with unreliable client-side data conditions. Finally, authors show small unlabeled datasets on server is still useful.  ",
            "strength_and_weaknesses": "Strength:\n\n1: The paper is well motivated. It is true that in realistic FL tasks, we might be able to collect a lot of unlabeled data on central server. Thus, it is a very important question on how we could utilize such unlabeled data.\n\n2: The proposed method is simple but seems to be effective in the presented experiments. It is interesting that it is shown that small unlabeled dataset will still be useful. \n\nWeakness:\n\n1: The proposed method might not be novel enough. As stated by authors in related work section, both distillation training and SSL method have already been applied to federated learning. Authors only propose additional entropy weighted ensembles for distillation, which also might not be new in ensemble learning. \n\n2: In experiment sections, if I understand correctly, authors only use 4 clients in general (and upto 20 clients in some experiments). This might not be enough number of clients to be a meaningful federated learning experiment (which should have at least 100 clients). \n\n3: Authors should also include many other FL baselines: (1) authors should include algorithms that focus on solving non-iid FL, such as FedProx and Scaffold, in order to show the effectiveness of using additional unlabeled data (2) author should also compare with many other FL+SSL algorithms (some of which are already mentioned by authors in related work section).\n\n4: Lack of theoratical analysis of the proposed method. But I understand this is minor point for the emperical paper.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality:\n\nOverall clear and well written paper.\n\nNovelty:\n\nI am a little concerned about novelty as stated above.\n\nReproducibility:\n\nSeems to be reproducible but no appendix or code is provided.",
            "summary_of_the_review": "I recommend weak rejection mainly due to lack of overall novelty and some questions on the experiment settings. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1023/Reviewer_qQ7S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1023/Reviewer_qQ7S"
        ]
    },
    {
        "id": "0MSTRdT5Ca6",
        "original": null,
        "number": 3,
        "cdate": 1667317071071,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667317071071,
        "tmdate": 1667317336901,
        "tddate": null,
        "forum": "GGx4EVTZU5r",
        "replyto": "GGx4EVTZU5r",
        "invitation": "ICLR.cc/2023/Conference/Paper1023/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a knowledge distillation based federated learning method by utilizing unlabeled data on the server. A global model, including a feature extractor and classifier, is trained to approximate entropy-weighted pseudo-label on the server. Further, a self-supervised learning framework is applied on top to learn generic feature representation better. \nThe method is suitable when the amount of data per client or the number of clients is scarce. Empirically, it beats strong baselines such as FedAVG and FedDF. ",
            "strength_and_weaknesses": "Pros:\n1. The usage of entropy to denote prediction uncertainty of each client\n2. Ablation comparison of different components of the framework (EED and SSL) is well done\n\nCons:\n1. The entropy-induced uncertainty is at the client level, but could it be that a local model performs well on part of the unlabelled data only?  Can this pseudo label be calculated on a datapoint level?\n2. The choice of $\\gamma$ (balance parameter for SSL loss) is not well justified\n3. The claim when $N_c \\leq N_s$, settig $B_{\\text{AVG}}$ = false would give higher test accuracy is not well justified: when $N_c =N_s$, how could $B_{\\text{AVG}}$ affect test accuracy? Would be nice to show it empirically.\n4. How the train-test split is done on each client? Do they have the same distribution? This needs to be detailed about.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is organized in an easy-to-read way and clarifies the main results well with empirical experiments. Novelty is a bit lacking.",
            "summary_of_the_review": "This paper proposed FedDS, which is a continuation and extension of FedDF. The empirical results are well presented. More comparisons to related SOTA results would be better though. Some theoretical support for the convergence of the proposed method is lacking. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1023/Reviewer_fQQY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1023/Reviewer_fQQY"
        ]
    }
]