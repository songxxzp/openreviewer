[
    {
        "id": "N0VRzPO6Ofv",
        "original": null,
        "number": 1,
        "cdate": 1666332750575,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666332750575,
        "tmdate": 1666332750575,
        "tddate": null,
        "forum": "zEn1BhaNYsC",
        "replyto": "zEn1BhaNYsC",
        "invitation": "ICLR.cc/2023/Conference/Paper156/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of learning trace-class operators between Sobolev reproducing kernel Hilbert spaces. Information theoretical lower bound of convergence rates are derived. The machine learning problem is characterized by bias contour and variance contour on spectral spaces. Three regularization schemes are proposed inspired by the bias and variance contours. All three schemes are proven to achieve the minimax lower bound subject to a logarithmic factor.",
            "strength_and_weaknesses": "Strength\n1. Clear insights provided by bias and variance contours\n2. Important mathematical problem well motivated by many applications, including generative modeling, functional linear models, causal inference, multi-agent reinforcement learning, and so on.\n3. Analysis achieves minimax optimal rates\n\nWeaknesses:\nNo significant weakness has been identified.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written with a very clear structure, and the idea of bias and variance contours are clearly illustrated with figures. The results are novel and of high scientific quality.\nSince this paper is on theoretical research, the reproducibility of theory is guaranteed by the provision of proof, and numerical reproducibility is not applicable.",
            "summary_of_the_review": "Inspiring characterization of bias and variance contours. Neat convergence analysis with the estimations achieving minimax optimal rate.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper studies machine learning theory, and there is no ethics concern.",
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper156/Reviewer_v7T2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper156/Reviewer_v7T2"
        ]
    },
    {
        "id": "TRnEZVAJHQ",
        "original": null,
        "number": 2,
        "cdate": 1666604291485,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604291485,
        "tmdate": 1668595399821,
        "tddate": null,
        "forum": "zEn1BhaNYsC",
        "replyto": "zEn1BhaNYsC",
        "invitation": "ICLR.cc/2023/Conference/Paper156/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the problem of deriving theoretical learning rate for learning mappings between infinite-dimensional function spaces. As noted by the authors, it is an important topic to understand how much training data is needed to approximate linear operators within a prescribed accuracy. After reviewing existing theoretical works on operator learning connected to PDEs, the authors derive a lower bound for learning linear operators between infinite-dimensional Sobolev reproducing kernel Hilbert spaces.",
            "strength_and_weaknesses": "### Strengths\n\n1. The problem of deriving theoretical learning rates for Hilbert-Schmidt operators addressed by this paper is a significant topic with applications to understand performance of state-of-the-art neural operators approach for learning solutions operators of PDEs.\n2. The authors derive a novel lower bound for learning linear operators between Sobolev reproducing kernel Hilbert spaces, generalizing existing works by Li et al (2022) which takes into account the output space (as opposed to prior works).\n3. The lower bound on the learning rate is optimal as shown by the authors who introduces a scheme achieving a near-optimal learning rate.\n\n### Weaknesses\n\n1. The paper is very technical and dense which makes its understanding very difficult beyond section 1. More backbround material should be provided in section 2 on reproducing kernel Hilbert spaces and Hilbert-Schmidt operators along with concrete examples connection to operator learning associated with PDEs.\n2. The lower bound provided by the authors in Theorem 3.1 is the main result for the paper but little explanation and interpretation is provided in the section and no comment on the proof techniques.\n3. The multilevel kernel operator learning scheme of section 5 seems interesting (based on the theoretical results in Theorem 5.1) but I don't understand the method, which is essentially described in one sentence by Eq. (5). I would suggest rewriting the section to describe the method extensively, eventually adding a simple illustrative examples.\n\n### Minor comments\n\n1. End of page 3: \"kernl\" -> \"kernels\"\n2. Could the authors add some interpretation on the assumptions for the kernel in section 2.2?\n3. Some equations are missing punctuations, e.g. Lemma A.1, A.3...",
            "clarity,_quality,_novelty_and_reproducibility": "The results obtained by the authors seem novel and high quality but the density of the paper prevents is clarity and is the main weakness.",
            "summary_of_the_review": "While the paper contains novel and significant results on the sample complexity of Hilbert-Schmidt operators, its technicality and density prevent the readability. I believe that a revised version of the paper, including the proofs in the main text and expanding on the background material and interpretation of the results, would be more suitable for a top machine learning journal like Journal of Machine Learning Research (with a longer review time allowing for a careful check of correctness) rather than a short conference paper with a long supplementary material.\n\n--\nI have updated my score following the authors' response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper156/Reviewer_7Ap7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper156/Reviewer_7Ap7"
        ]
    },
    {
        "id": "0XFORg-Orb",
        "original": null,
        "number": 3,
        "cdate": 1666612466127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612466127,
        "tmdate": 1666612466127,
        "tddate": null,
        "forum": "zEn1BhaNYsC",
        "replyto": "zEn1BhaNYsC",
        "invitation": "ICLR.cc/2023/Conference/Paper156/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper analyses the problem of constructing maps netween infinite dim function spaces.  The paper claims 3 contributions:\n\n1. An information-theoretic lower bound of learning a linear operator between two infinite-dimensional Sobolev RKHSs consisting of two polynomial rates, the first of which depends on the input space and is consistent with known results, the second of which depends on the output space and is novel.\n\n2. A study of the shape of regularization needed to obtain the optimal learning rate.\n\n3. A setting where a multilevel training procedure is needed to achieve the optimal learning rate, which is distinct from the finite-dimensional case where a single-level estimator suffices.",
            "strength_and_weaknesses": "The paper is soundly written and to the best of my knowledge the theoretical contributions are novel and provide insight into the problem.  Examples are given to motivate the mathematical development and there is commentary describing salient points.\n\nMy one complaint is the lack of any summary of proofs in the body of the paper.  While obviously it is not practical to include the actual proofs, it is useful to include some indication of the important steps or interesting methods required in the body of the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "As far as I can tell the paper is clear, and the results would appear to be novel.  I did struggle with some of the mathematics in the paper but I assume this has more to do with unfamiliarity on my part than any problems with the presentation itself.",
            "summary_of_the_review": "The paper presents novel results in what appears to be a thorough and informative manner.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper156/Reviewer_EtQp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper156/Reviewer_EtQp"
        ]
    },
    {
        "id": "rHOGdJ912f",
        "original": null,
        "number": 4,
        "cdate": 1666631557316,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631557316,
        "tmdate": 1666631557316,
        "tddate": null,
        "forum": "zEn1BhaNYsC",
        "replyto": "zEn1BhaNYsC",
        "invitation": "ICLR.cc/2023/Conference/Paper156/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the problem of learning a linear operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces, which includes learning differential operators and condition mean embedding as special examples. A novel information-theoretical lower bound is derived, which implies that the minimax learning rate depends on both the smoothness of input and output spaces. This paper proposes a multi-level kernel operator learning algorithm that can achieve the optimal learning rate.",
            "strength_and_weaknesses": "$\\textbf{Strength}$\n\n1. This is a novel contribution that provides insights into the understanding of operator learning between two infinite-dimensional spaces, a topic with various applications in scientific computing, machine learning, and statistics.\n\n2. The minimax lower bound for learning a linear operator is interesting and novel, which characterized the difficulty of this problem.\n\n3. The proposed multi-level kernel operator learning algorithm also makes a solid contribution and the insights are valuable for the design of other operator learning algorithms.\n\n4. The mathematical analysis of this paper is rigorous and sound, which extends the previous results for learning differential operators and conditional mean embedding.\n\n$\\textbf{Concerns}$\n\n1. Can the analysis (or lower bound) be extended to nonlinear operator learning?\n\n2. The definitions of $\\hat{C}_{LK}$ in (3) and (5) are not given. It is also a little confusing how $\\hat{A}$ can be calculated in practice as $\\rho_j$ and $f_j$ are not easy to obtain.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written with a sufficient literature review. The results are novel, significant, and of broad interest.",
            "summary_of_the_review": "The paper studies the optimal convergence rate of learning a linear operator between two infinite-dimensional spaces, which should be of interest to a broader audience in the field of machine learning and scientific computing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper156/Reviewer_Zt4i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper156/Reviewer_Zt4i"
        ]
    },
    {
        "id": "v66IDkczqQ",
        "original": null,
        "number": 5,
        "cdate": 1666660463138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660463138,
        "tmdate": 1668793427464,
        "tddate": null,
        "forum": "zEn1BhaNYsC",
        "replyto": "zEn1BhaNYsC",
        "invitation": "ICLR.cc/2023/Conference/Paper156/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the problem of learning a linear operator from one RKHS to another RKHS (hereafter from an input RKHS to an output RKHS). In this literature, it is common to study a certain kind of mis-specification: what if the ground truth belongs to an interpolation space between the RKHS and L2. Whereas previous works on the problem consider the mis-specification with respect to the input RKHS, this work considers mis-specification with respect to both the input RKHS and the output RKHS, which is a generalization. The authors characterize the learning lower bound in generalized Hilbert-Schmidt norm and analyze a new multilevel estimator that attains it.",
            "strength_and_weaknesses": "See below",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nOverall, the paper was written in a very clear manner for kernel experts. It would be nice to at least point to other works that interpret the various abstract assumptions of Fischer and Steinwart\u2019s paradigm in a way that more readers can understand.\n\nThe remarks do a great job explaining the similarities and differences with prior work. Two additional references which I would like to be included in the \u201clearning with kernel\u201d paragraph of the related work are\n1. \u201cA measure-theoretic approach to kernel conditional mean embeddings\u201d (NeurIPS 2020), which was a relatively early work about learning rates, though not in Sobolev norm\n2. \u201cKernel methods for causal functions\u201d (arXiv 2022), which obtains optimal rates in Sobolev norm but only considers beta=1 and gamma=1\nNeither of these references diminish of the contributions of this work in any way, but would make the related work section more complete.\n\nQuality\n\nI only skimmed the proofs due to time constraints. The main results and main steps in the proof seemed reasonable.\n\nNovelty\n\nBy studying the harder learning scenario in which both the input and output RKHSs are mis-specified, the authors break new ground. The second rate in the bound, based on the output RKHS, seems to be interesting and novel.\n\nReproducibility\n\nGiven the proposal of a new multilevel estimator, I was a bit surprised that there were no simulations. The works in this literature that study well known estimators sometimes skip the simulations, but this paper seems to propose a new estimator and to claim that in certain \u201chard\u201d scenarios it is preferable. It raises the question of how difficult the new multilevel estimator is to implement.",
            "summary_of_the_review": "This paper joins a mature literature, but I think that its study of the output RKHS mis-specification is significant. The lack of simulations raises the question of how feasible the multilevel estimator actually is. If the authors implement the minor improvements suggested above, I will raise the score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper156/Reviewer_B1fn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper156/Reviewer_B1fn"
        ]
    }
]