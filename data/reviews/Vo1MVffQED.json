[
    {
        "id": "lonVPwAFSg",
        "original": null,
        "number": 1,
        "cdate": 1666642407266,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642407266,
        "tmdate": 1669734775293,
        "tddate": null,
        "forum": "Vo1MVffQED",
        "replyto": "Vo1MVffQED",
        "invitation": "ICLR.cc/2023/Conference/Paper3407/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper deals with the question of learning Stackelberg equilibria -- games with a leader-follower structure -- an important topic with many relevant applications. The main contribution is to show that, given access to a best-response oracle for the follower, this problem can be reduced to learning in a POMDP consisting of two phases. In some simple games, the authors then implement this reduction experimentally. A crucial requirement is that in the first phase of the POMDP, the leader does not adapt their policy; the authors also experimentally test this point.",
            "strength_and_weaknesses": "Strengths: The paper presents a very clear paradigm which unifies many existing approaches. This is an important topic, as shown by the cited prior work that involves using RL-style approaches in Stackelberg games, but much prior work has somehow lacked the clarity of perspective which this paper provides.\n\nWeaknesses: the main theorem is not very exciting in some sense. And the experiments are not particularly impressive (they mainly look at very toy matrix games, when there are many more interesting problems that in principle can be described by this framework).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the work is well-written and easy to follow.\n\nNovelty: in some sense, the work is novel, as it presents an original viewpoint, but it also is just building on/generalizing previous work e.g. the cited Brero et al. paper.\n\nReproducibility: reproducibility seems fine, the code included looks after looking through it briefly.\n\nQuality: the work is high quality although the main theorem and experiments, while they are enough to support the argument, are not too impressive.",
            "summary_of_the_review": "I recommend weak acceptance simply because in the end, I was glad to read this paper and think it is a positive contribution to the community.\n\nUpdate after reading author's rebuttal: due to the presence of new experiments (and clarification of comparison to previous methods) which are indeed more convincing, I've increased my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3407/Reviewer_3yUN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3407/Reviewer_3yUN"
        ]
    },
    {
        "id": "GiG226BrpK0",
        "original": null,
        "number": 2,
        "cdate": 1666681738267,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681738267,
        "tmdate": 1670359972060,
        "tddate": null,
        "forum": "Vo1MVffQED",
        "replyto": "Vo1MVffQED",
        "invitation": "ICLR.cc/2023/Conference/Paper3407/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper combines recent multi-agent RL approaches for finding stackelberg equilibria into one framework, with conditions on convergence. Also, ideas from meta-RL are used to improve the follower. \n",
            "strength_and_weaknesses": "This paper provides a nice overview of the existing literature for solving sequential Stackelberg games. I especially like table 2. I think that a version of this paper that does a better job experimentally comparing against all the existing approaches and with better theory characterizing when one would expect approaches to not work could be useful to the community. \nHowever, as it currently is, the theory seems to give a result that is already known, the experiments do not sufficiently compare across all the baselines, and the novel algorithmic contribution does not have good enough experiments to back it up.\n\nThe experiments should be a lot more extensive and compare against all the related works shown in the table\n",
            "clarity,_quality,_novelty_and_reproducibility": "The conclusions from theorem 1 seem too strong. Just because some method doesn\u2019t satisfy the conditions of the theorem doesn\u2019t mean it isn\u2019t principled. I could see a lot of approaches working where the leader updates while the follower learns\nTheorem 1 also seems pretty obvious and already known from the literature. I think the work should be pretty reproducible. I thought the writing was clear and concise. ",
            "summary_of_the_review": "Overall, I think this paper needs a bit more work, with better theory that proves existing methods won't work, with extensive experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3407/Reviewer_RZA6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3407/Reviewer_RZA6"
        ]
    },
    {
        "id": "6hS1Aowd1MF",
        "original": null,
        "number": 3,
        "cdate": 1667253445410,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667253445410,
        "tmdate": 1669983239830,
        "tddate": null,
        "forum": "Vo1MVffQED",
        "replyto": "Vo1MVffQED",
        "invitation": "ICLR.cc/2023/Conference/Paper3407/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a framework for convergence to Stackelberg equilibria in multi-agent RL. The main contribution of the paper can be summarised by Theorem 1 in the paper. Theorem 1 states 3 conditions for convergence so that the solutions for the leader and the follower form a Stackelberg equilibrium. Additionally, 2 more conditions are stated that are required to consider the problem of the leader as a POMDP. Finally, the authors experimentally evaluate when Theorem 1 holds in several matrix games and scenarios.",
            "strength_and_weaknesses": "Strengths:\n- The paper is clearly written, and the concepts are nicely described.\n- The background information is described concisely.\n\n------------------------------------------------------------\nWeaknesses\n\n- Page 2, last paragraph: \"We focus here mainly on this memory-less case.\" Why do you only consider memory-less cases?\n- Page 3, first line: \"total reward\" ---> this should be \"total return\"\n- The paper is heavily based on three works of Brero et al, which however only one of those is published in an archival venue.\n\n- I would recommend including the payoff matrices of the games in the appendix.\n\n- Page 6, 3rd paragraph: Could you please elaborate more on this sentence: \"The leader should receive reward only when followers are playing their best-response equilibrium, and the entire follower learning process should be one episode for the leader.\" Why do we need such a constraint? \n\n- There are small typos and incorrect use of articles in the paper, which, however, do not make the paper hard to read. For example, on page 8 in the last paragraph: alllow ---> allow\n\n- I would like to see the baselines of Balaguer et al. (2022) included in Figures 1 and 2. Otherwise, the reader has to go to the paper of Balaguer et al. (2022) to see the achieved returns of the baselines.\n\n- I feel that ICLR is not a suitable venue for this paper. ICLR, as its name indicates, focuses on learning representations, and I do not see how this paper fits in this scope. I think conferences such as AAMAS/AAAI/IJCAI/ICML/NeurIPS/COLT are more suitable for this paper.\n\n- Why are the matrix games POMDPs?\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The paper is very well-written. Every piece of background is clearly and concisely described. \n\n**Novelty**: I believe that the concepts that are mentioned are not very novel. The main idea behind the paper is to propose a unified framework and show that it converges to Stackelberg equilibria. However, I think that the ideas are \n\n**Reproducibility**: The source code to reproduce the experiments is provided in the supplementary materials.",
            "summary_of_the_review": "I recommend a weak reject for this paper. I do not feel that it significantly alters our understanding of Stackelberg equilibria. Additionally, I believe that ICLR is not a suitable venue for this paper. I would expect higher dimensional experiments.\nMy ranking reflects my current understanding of the paper. I would be happy to change my score if the authors satisfactorily refute my main points of criticism.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3407/Reviewer_FBtF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3407/Reviewer_FBtF"
        ]
    },
    {
        "id": "WvHPyX_F6Y",
        "original": null,
        "number": 4,
        "cdate": 1667653607556,
        "mdate": 1667653607556,
        "ddate": null,
        "tcdate": 1667653607556,
        "tmdate": 1667653607556,
        "tddate": null,
        "forum": "Vo1MVffQED",
        "replyto": "Vo1MVffQED",
        "invitation": "ICLR.cc/2023/Conference/Paper3407/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies Stackelberg equilibria and presents a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, which encapsulates both previous approaches and some new design spaces such as contextual policies. Some numerical experiments are also provided.",
            "strength_and_weaknesses": "Strengths:\n* Propose a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem.\n* Numerical experiments are provided to demonstrate the usefulness of the proposed framework.\n\nWeaknesses:\n* The notation is confusing and inconsistent. For example, in Definition 2, it is unclear how these notation are related to the Markov Game notation in the previous page. Also, it seems that the authors are confusing strategies $s$ with actions $a$. In particular, if ${\\bf s}_F$ is the follower strategies, then what is ${\\bf a}_F$? Similarly, if $s_L$ is the leader strategy, then what are $s_t$ and $a_L$?\n* Theorem 1 is either trivial or very hard to follow, making the claim about a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem unconvincing. The general case seems to be a simple restatement of the problem formulation, and the proof in the appendix also seems to suggest this. The special case is hard to understand. and also $s_L(o)$ and $\\mathcal{O}_L$ are undefined. The authors should try to make these statements and proofs more formal, instead of using informal statements like \"acting the same during queries as they would in $\\mathcal{M}$\". \n* The authors seem to only compare with Balaguer et al. (2022) as the benchmark. However, there are tons of Stackelberg RL algorithms that the authors fail to compare with. See [A, B] for example.\n\n[A] Tianmin Shu and Yuandong Tian. M3RL: Mind-aware multi-agent management reinforcement learning. In ICLR, 2019.\n\n[B] Runsheng Yu, Xinrun Wang, Youzhi Zhang, Rundong Wang, Bo An, ZhenYu Shi, and Hanjiang Lai. Learning expensive coordination: An event-based deep RL approach. In ICLR, 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is hard to follow, and neither the theoretical nor the numerical results are convincing. The reproducibility is okay given the provided codes.",
            "summary_of_the_review": "Given the comments above, I think the paper is not qualified for publication in ICLR due to the drawbacks in both writing quality and solidity of the results.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3407/Reviewer_671a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3407/Reviewer_671a"
        ]
    }
]