[
    {
        "id": "6LoWnzqisK",
        "original": null,
        "number": 1,
        "cdate": 1666562079118,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666562079118,
        "tmdate": 1666562079118,
        "tddate": null,
        "forum": "3F6I-0-57SC",
        "replyto": "3F6I-0-57SC",
        "invitation": "ICLR.cc/2023/Conference/Paper1520/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper investigates the improvement from plain ViT to hierarchical Swin-Transformer step-by-step and proposes a new hierarchical vision transformer that only uses hierarchical patch embedding at the beginning and keeps the global attention at 14x14 level instead of window attention. It shows similar or better performance than Swin transformers on Imagenet classification, segmentation and detection. ",
            "strength_and_weaknesses": "Strength:\t\n1.\tThe paper provides a detailed study regarding prevail Swin-Transformer architecture, identifying the key parts for its superior performance v.s. plain ViT, e.g increasing depth and decrease channels, add relative position embedding. \n2.\tThe paper proposes an alternative way to enjoy the performance gain from hierarchical ViT which is simpler and friendly for techniques that are designed for plain ViT, i.e. Mask Image Modeling.\nWeaknesses:\n1.\tThere are several part of descriptions not clear enough. \na.\tTable 2, no explanation for RPE, Low Att, Mid Att, etc in table caption. \nb.\tStage definition missing, though can be referred, but better have clear definition. \nc.\tCan authors also add Param and Flops to Table 2, as changing components will change computation as well.\nd.\tIncomplete sentences: Page 5, Step (c\u2019) \u2018causes a significant accuracy\u2019;  Sec 3.3 \u2018all tokens in Stage ? are symmetric\u2019\ne.\tMerging operation details? One can only infer it from figure 1, can authors add some descriptions about it? \n2.\tRegarding the proposed hierarchical patch embedding, can authors give more explanation about the specifical design as 2 consecutive MLP with ratio 3, how authors reach this design. As this is a critical module proposed, can authors provide some ablation studies about it, e.g. why uses a different MLP ratio of 3 as latter stages uses 4 instead? Will allocate computations differently to 56x56, 28x28 influence the final performance? What computation percentage should be used in patch embedding v.s. latter stage (14x14) for better performance? \n \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has some minor clarity issues describing the details of the method. Overall quality is good. Novelty and contributions are enough for the community as it introduces some key findings, and the method can be widely adapted. Reproducibility is good as method is simple and code is provided. ",
            "summary_of_the_review": "\nOverall, the paper presents a good study of plain ViT and hierarchical ViT, and provides a simpler hierarchical ViT with similar or better vision task performance and is friendly to many follow-up techniques of plain ViT. The proposed method can be widely adapted with its simple design and good performance. It would be much better if authors can provides further studies regarding the proposed patch embedding module. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1520/Reviewer_vrWv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1520/Reviewer_vrWv"
        ]
    },
    {
        "id": "pJZkYfV0zt",
        "original": null,
        "number": 2,
        "cdate": 1666573494090,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573494090,
        "tmdate": 1666573494090,
        "tddate": null,
        "forum": "3F6I-0-57SC",
        "replyto": "3F6I-0-57SC",
        "invitation": "ICLR.cc/2023/Conference/Paper1520/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors study what makes hierarchical ViTs (in particular SwinTransformers) work, and based on the insights, propose a new vision transformer architecture called HiViT. The authors show that HiViT works better than Swin and vanilla ViT on ImageNet and ADE20K. Moreover, HiViT doesn't use window attention, makes it compatible to MAE style efficient SSL pre-training. The authors show that masked pre-training on HiViT obtains better accuracy than MAE on vanilla ViTs.",
            "strength_and_weaknesses": "Strengths:\n+ Swins, ViTs, and MAE are state-of-the-art and widely-used tools in computer vision. The three main contributions, (1) understanding what makes Swin works well, (2) advancing architecture design, and (3) showing that HiViT works with masked-pretraining, are practically useful. I can see that many computer vision practitioners will be interested in this paper.\n+ The ablations in Table 2 are clear and convincing. \n\nWeaknesses:\n- While HiViT works well with masked-pretraining, without masked pre-training it is not very strong; For example, MViTv2 is more accurate and uses only 2/3 of FLOPs. This makes HiVIT's usefulness in practice limited.\n- Many ablations are not entirely new. For example, the effectiveness of RPE and hierarchical resolutions have been shown in the original Swin paper. The finding that I find novel is that C512 x 24L works quite a lot better than C768 x 12L. However, this is technically a small change. I thus overall would not rate the technical contributions high.\n- Adding runtime, flops, and params into Table 2 would makes the trade-offs clearer.\n\n- minor points on writing:\n1) why use the word \"merging\" instead of \"pooling\", which is more concretely defined?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good. The illustration in Figure 1 along with ablations in Table 2 makes the main message clear. The quality is reasonable, too. The authors test on state-of-the-art settings and compare with state-of-the-art methods with extensive experiments. The novelty isn't particularly high, but can be a useful reference for future researchers.",
            "summary_of_the_review": "While the technical novelty of this paper is somewhat limited, I still think this paper can be a useful reference for practitioners and future researchers on architecture design. Overall, I rate this paper as marginally above the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1520/Reviewer_5NXC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1520/Reviewer_5NXC"
        ]
    },
    {
        "id": "sqsHPUKPQW",
        "original": null,
        "number": 3,
        "cdate": 1666665252748,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665252748,
        "tmdate": 1666665302827,
        "tddate": null,
        "forum": "3F6I-0-57SC",
        "replyto": "3F6I-0-57SC",
        "invitation": "ICLR.cc/2023/Conference/Paper1520/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a hierarchical ViT, by analyzing the effective design in Swin compared with the plain ViT. The model contains a hierarchical patch embedding with only MLP used in early stages, using transformer blocks with global attention in the 14x14 stage, and removing the last stage. Experiments on ImageNet, COCO and ADE20K have shown quite promising results. The architecture is also compatible with MAE, which can achieve higher accuracy with the mask pretraining.",
            "strength_and_weaknesses": "Strength:\n- The paper is well written, the contributions are clearly stated and supported by empirical experimental results and analysis.\n- The paper gives a step-by-step analysis on the differences between ViT and Swin, which \n- The proposed model addresses the problem of incompatibility between MAE and hierarchical vision transformers, by simply removing the 7x7 stage and the self-attention in early stages.\n- The authors performed rich experiments to validate the effectiveness of the proposed HiViT. The training and design details are provided, and the overall method seems reproducible.\n\nWeaknesses:\n- This paper claims that early attentions can be replaced by hierarchical patch embedding only when the model is deep enough, with an attention variance comparison on the deepened plain ViT. However, as the proposed model removed the self-attention on hierarchical ViT (Swin) instead of plain ViT, does the statement still hold on Swin?\n- In the analysis section, the authors proposed a variant of Swin without stage4 and does not hurt performance. However, the model without stage4 also used global window in stage 3 (as illustrated in Fig1, bottom, 3rd & 4th figures), which may also have a significant impact on the performance and computational cost of the model. Therefore, the experiment may not well support the claim that stage4 is not helpful.\n- The overall architecture of HiViT comprises an overlap patch embedding that embeds the image to 14x14 and use continuous transformer blocks like plain ViTs. Yet, using hierarchical patch embedding have been proposed by several papers, e.g., papers use overlapping patch embed [1-4], and another similar work that also removed the self-attention in early stages, and only use MLP in 56x56 and 28x28 [4]. \n[1] Xiao, Tete, et al. \"Early convolutions help transformers see better.\" Advances in Neural Information Processing Systems 34 (2021): 30392-30400.\n[2] Guo, Jianyuan, et al. \"Cmt: Convolutional neural networks meet vision transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[3] Ali, Alaaeldin, et al. \"Xcit: Cross-covariance image transformers.\" Advances in neural information processing systems 34 (2021): 20014-20027.\n[4] Pan, Zizheng, et al. \"Less is more: Pay less attention in vision transformers.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 2. 2022.\n- Since an important claim of this paper is enabling hierarchical architectures to trained with MAE and save pretraining cost, a more detailed comparison of the training time or GPU usage, shall be given for a better reflection of the capacity of the architecture for pretraining, as an example, the Table2 in [5].\n[5] Huang, Lang, et al. \"Green Hierarchical Vision Transformer for Masked Image Modeling.\" arXiv preprint arXiv:2205.13515 (2022).\n- The performance gain in Table 4 may not well reflect whether HiViT has \u201cstructural advantage over plain vision transformers\u201d. ViT-B has 77.9% acc while HiViT has 83.8% acc in the supervised training, while MAE+ViT-B has 83.6%(+5.7%) acc and MAE+HiViT has 84.6%(+0.8%) in the pretraining setting. Therefore, it seems HiViT do not gain much from the pretraining, even with 1600 epochs. This phenomenon should be further explained.\n- There are some other minor issues, such as mistakes in writing. For example, \u201cAs an alternative solution (used by Swin) to Step (c), it causes a significant ADE20K segmentation accuracy and Compared to Step (c)\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presents the ideas clearly and the work is original. The quality can be improved if the concerns in the weaknesses part are addressed.",
            "summary_of_the_review": "Overall, I think this paper is well organized. It studies an important task, where the hierarchical models are not well compatible with the powerful MAE. The analysis of the plain and hierarchical ViTs are quite impressive. \n\nHowever, this proposed model is a simple plain transformer with hierarchical patch stem, which has been used in several prior works. Hence, the technical novelty is limited. The key contribution of using MAE for hierarchical model pretraining, does not give impressive results. The experiments do not present the actual training cost saved with HiViT, meanwhile the improvement with 1600epoch MAE pretraining only bring limited performance improvement compared with ViT.\n\nTherefore, the reviewer votes for the borderline in the first round. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1520/Reviewer_mVGH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1520/Reviewer_mVGH"
        ]
    }
]