[
    {
        "id": "CV7c6EVLHW",
        "original": null,
        "number": 1,
        "cdate": 1666628657299,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628657299,
        "tmdate": 1666628657299,
        "tddate": null,
        "forum": "GVSf7Z7DbYL",
        "replyto": "GVSf7Z7DbYL",
        "invitation": "ICLR.cc/2023/Conference/Paper2861/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper targets solving knowledge transfer problems. Specifically, the authors distill large models to compact models for efficient machine learning. Teacher-guided training (TGT) framework is proposed for training high-quality compact models which learn knowledge from pretrained generative models. Experiments show the effectiveness of TGT.",
            "strength_and_weaknesses": "Strengths:\n+ This paper is well-written. It is enjoyable to read.\n+ The proposed framework is simple and effective. The overall idea is neat.\n+ Experiments are comprehensive and show the effectiveness of TGT.\n\nWeaknesses:\n+ The contributions are a little limited. Knowledge distillation is an existing approach.\n+ More insightful analyses should be provided. How the knowledge is transferred in TGT?\n+ Is the conclusion in 4.2 still hold true in NLP dataset?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. This paper is well-written. It is enjoyable to read.\n\nQuality: Good. Experiments are comprehensive and show the effectiveness of TGT.\n\nNovelty: A little limited. Knowledge distillation is an existing approach.\n",
            "summary_of_the_review": "Strengths:\n+ This paper is well-written. It is enjoyable to read.\n+ The proposed framework is simple and effective. The overall idea is neat.\n+ Experiments are comprehensive and show the effectiveness of TGT.\n\nWeaknesses:\n+ The contributions are a little limited. Knowledge distillation is an existing approach.\n+ More insightful analyses should be provided. How the knowledge is transferred in TGT?\n+ Is the conclusion in 4.2 still hold true in NLP dataset?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2861/Reviewer_PrCv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2861/Reviewer_PrCv"
        ]
    },
    {
        "id": "xFhgzKuFJ56",
        "original": null,
        "number": 2,
        "cdate": 1666702703292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666702703292,
        "tmdate": 1666793139211,
        "tddate": null,
        "forum": "GVSf7Z7DbYL",
        "replyto": "GVSf7Z7DbYL",
        "invitation": "ICLR.cc/2023/Conference/Paper2861/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a general teacher-guided training (TGT) framework for efficient model distillation. TGT explicitly leverages the low dimensional representations extracted by the pretrained teacher generative models, which is then theoretically shown to be able to improve the generalization of the student model. TGT does not need to go through a large volume of data, thus and can better work in limited data regime or long-tail settings. ",
            "strength_and_weaknesses": "Strength:\n\n1. I think this paper will be valuable to the community. For the large pretrained models, even distilling to a lighter proxy can be unaffordable. Cleverly utilizing the teacher model and distilling the student efficiently is a very important problem. I like the theoretical analysis showing the benefit of utilizing TGT on generalization error bounds. These are good contributions.\n\n2. The paper appropriately cites related works and discusses their relationship to the teacher-guided training method. \n\n3. The paper is well-motivated and clearly written. Experiments are properly conducted to validate the effectiveness. \n\nWeakness:\n\n1. A minor disadvantage of TGT is that it relies upon an encoder-decoder style generative model as a teacher, which limits its practicability as most of the models trained in the industry are deterministic and do not have an encoder-decoder structure.  \n\n2. I am not sure if all three terms in Equation (2) are utilized in the experiments. If so, will the first supervised learning term dominate and nullify the better generalization applying TGT?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the proposed method is novel.",
            "summary_of_the_review": "None",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2861/Reviewer_Qnjd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2861/Reviewer_Qnjd"
        ]
    },
    {
        "id": "bVr00aMs3D",
        "original": null,
        "number": 3,
        "cdate": 1666775484223,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666775484223,
        "tmdate": 1670152256944,
        "tddate": null,
        "forum": "GVSf7Z7DbYL",
        "replyto": "GVSf7Z7DbYL",
        "invitation": "ICLR.cc/2023/Conference/Paper2861/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies an important problem, i.e., given a pre-trained large teacher model, how to transfer its knowledge to a compact low-compute smaller model. The proposed method is developed on top of the standard knowledge distillation approach. The authors assume that there is an available encoder and decoder, such that any training sample can be encoded into the latent space. Then one can find the maximally disagree point in the data manifold by exploring the latent space of the encoder, and train the student network by minimizing this disagreement. Extensive empirical results on both image and text data are provided.",
            "strength_and_weaknesses": "Strength:\n- The problem is important. The proposed TGT framework is well-motivated.\n- The experiments are extensive.\n- A theoretical analysis is provided.\n\nWeaknesses:\n- It may be difficult to say that the proposed method is 'novel'. TGT is based on a standard knowledge distillation framework. The idea of 'minimizing the disagreement' is not new (e.g., [*1]). In general, I'm not surprised that this idea can improve the performance.\n- From Table 1, one can see that TGT (gradient-based) only marginally outperforms the random baseline, i.e., TGT (random). In addition, the random baseline is not provided in other experiments. Therefore, I'm not fully convinced by the effectiveness of the proposed 'Gradient-based exploration' mechanism, which is an important contribution of this paper.\n- In Table 1, the teacher model is pre-trained on a much larger dataset (i.e., JFT-300M) than the baselines. The comparison may be unfair. In addition, JFT-300M is not publicly available, which may hurt the reproducibility of this paper.\n\n## Post-rebuttal\nThe responses from the authors have addressed most of my concerns (e.g., novelty and reproducibility). I have updated my score. However, I still think that adding more results of TGT (random) is important.\n\n\n[*1] Miyato, T., Maeda, S. I., Koyama, M., & Ishii, S. (2018). Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8), 1979-1993. ",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of this paper is of high quality. The theoretical analysis is insightful. However, I have some concerns on the novelty and reproducibility of this paper.",
            "summary_of_the_review": "Personally, I think that this paper is around the borderline. Currently, I'm slightly leaning reject. My rating may change after seeing the comments from other reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2861/Reviewer_hz78"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2861/Reviewer_hz78"
        ]
    },
    {
        "id": "JVSmRHIWcd",
        "original": null,
        "number": 4,
        "cdate": 1666863588995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666863588995,
        "tmdate": 1666863588995,
        "tddate": null,
        "forum": "GVSf7Z7DbYL",
        "replyto": "GVSf7Z7DbYL",
        "invitation": "ICLR.cc/2023/Conference/Paper2861/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a novel technique for producing compact versions of large models using a new approach to distillation named Teacher Guided Training. The authors develop a sound mathematical framework based on maximizing the match between the teacher and student model distributions.  The authors establish some useful performance bounds. The main insight of this work is in operating in the latent space which allows handling both discrete and continuous domains seamlessly. The technique also significantly improves upon the state of the art on a variety of test domains both in the reduction of parameters and the accuracy. ",
            "strength_and_weaknesses": "Strengths:\n\n1. Innovative teacher-student distillation approach that is well motivated and explained.\n2. Sound and insightful mathematical framework that enables application to both discrete and continuous domains.\n3. Consistent and significant improvement over the state of the art in both model size reduction and accuracy. \n\nWeaknesses\n1. No significant weaknesses.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThe paper is well written and easy to understand.\n\nReproducibility\n\nThe method is well explained and can certainly be reproduced.\n\nQuality and Novelty\n\nThis paper represents a noteworthy advance over the state of the art. Good paper.",
            "summary_of_the_review": "The paper nicely motivates the main techniques proposed. The literature survey is thorough and insightful. The proposed technique is backed up by a sound mathematical framework that is systematically established and well explained. The technique is shown to work well on a variety of domains and yields noteworthy improvement over the state of the art. It also advances the state of the art in the architecture and mathematics of its approach to the problem. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "The authors present a new technique for compaction of large models. Large models are known to have various biases. Presumably the proposed compact version will also have those biases. The authors do not have an ethics section so it is not clear that they have taken ethical concerns into account. Perhaps it is just an inadvertent oversight that could be rectified easily with an added ethics section.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2861/Reviewer_u9os"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2861/Reviewer_u9os"
        ]
    }
]