[
    {
        "id": "f4Sx-6frCI",
        "original": null,
        "number": 1,
        "cdate": 1665832720608,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665832720608,
        "tmdate": 1665832720608,
        "tddate": null,
        "forum": "u_-XxuTcnJ7",
        "replyto": "u_-XxuTcnJ7",
        "invitation": "ICLR.cc/2023/Conference/Paper572/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the possibility of using multiple generators in a GAN setting. The idea is to let each generator specialize to a certain set of modes -- this \"assignment\" is currently assisted by a classifier, and encouraged via a novel total variation distance (TVD) loss. This idea may well have some merit, although the use of a classifier seems quite limiting. Unfortunately, the paper has pretty low information density and reads like a course work rather than an ICLR paper. The only dataset is MNIST, which is woefully inadequate in 2022. Realistically the multimodality problem should not exist in MNIST, as even in ImageNet StyleGAN-XL has SOTA results implying that modern GANs scale pretty far. So, while I agree that the problem still exists in the limit of highly diverse datasets, it surely should not exist in MNIST (using a StyleGAN family, for example). ",
            "strength_and_weaknesses": "Pros:\n- TVD loss may be a novel idea, seems promising.\n- Clear exposition.\n\nCons:\n- The existence of the problem is not defended adequately.\n- MNIST is not enough. I don't believe it actually needs multiple generators.\n- SOTA GANs not used, conclusions may be incorrect.\n- Low information density for an ICLR paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good, easy to follow. Unusual start for a technical paper to talk about 7Cs. I'm not opposed to this per se, but I would have liked to read a bit more why the problem exists in practice. Having worked a lot with GANs, I'm sure there is no need for multiple generators in MNIST, even though the authors manage to find support for that in their context. A single (StyleGAN-XL) generator can deal even with ImageNet's 1000 classes after all.\n\nThe background of GANs is also a bit too verbose for my taste, and may confuse things a little bit. The best current GANs continue to use the (non-saturating) loss for Goodfellow's original paper. WGAN was a very interesting step, but its loss was never used in SOTA methods -- but its suggestion of a Lipschitz regularizer stuck (in the form of R1). \n\nTVD has some novelty and seems quite interesting.",
            "summary_of_the_review": "Basically the scientific novelty, tested datasets and thus the breadth of conclusions, as well as the information density are all clearly below ICLR standards. A resubmission to a workshop might be viable option. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper572/Reviewer_YBt6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper572/Reviewer_YBt6"
        ]
    },
    {
        "id": "_qn6VH-5SN6",
        "original": null,
        "number": 2,
        "cdate": 1666769954998,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666769954998,
        "tmdate": 1666769954998,
        "tddate": null,
        "forum": "u_-XxuTcnJ7",
        "replyto": "u_-XxuTcnJ7",
        "invitation": "ICLR.cc/2023/Conference/Paper572/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents collaborative GAN, which aims to better balance the workload between individual generators. Specifically, a pre-trained classifier is introduced, and the training strategy is modified by adding a total variation distance based on WGAN-GP. Some experiments were conducted on MNIST dataset.\n ",
            "strength_and_weaknesses": "1) The novelty of the proposed method is very limited. It simply adds additional TVD loss on the existing WGAN-GP. And it is not clear why adding such TVD loss to the WGAN-GP can optimize the workload of the individual generators.\n\n2) The organization of this paper is poor. The paper spends too much space on introducing existing models and works, while the introduction of the proposed model is too concise.\n\n3) The experiments are only conducted on the simple MNIST datasets. The results are not convincing at all.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and novelty of this paper is poor",
            "summary_of_the_review": "The novelty and experiments of this paper are in doubt.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper572/Reviewer_f1XE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper572/Reviewer_f1XE"
        ]
    },
    {
        "id": "-Cjvkc6vov",
        "original": null,
        "number": 3,
        "cdate": 1666771915227,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666771915227,
        "tmdate": 1666771915227,
        "tddate": null,
        "forum": "u_-XxuTcnJ7",
        "replyto": "u_-XxuTcnJ7",
        "invitation": "ICLR.cc/2023/Conference/Paper572/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors target to learn a set of collaborative generators and a single discriminator in the generative adversarial network (GAN) framework. They are motivated by the belief that collaboration among multiple skill-specific generators can reach general and complex performance. They prove the concept on the MNIST dataset with the help of Total Variation Distance (TVD).",
            "strength_and_weaknesses": "*************************\nStrengths\n*************************\n+ Clear writing, except for the equations and the algorithm.\n+ Easy to reproduce.\n\n*************************\nWeaknesses\n*************************\n\n- The name \u201cCoGAN\u201d has been used in [r1].\n\n- The motivation is meaningless.\n  - It is only applicable to data with finite discrete modes. And the complexity (number of generators) is proportional to the number of modes. This is one of the major reasons their experiments only involve the toy-like MNIST dataset, and impossible for Stacked MNIST [r2] with 1000 modes nor any other natural images like FFHQ human faces.\n  - The reviewer suggests authors stop thinking along this direction, which is deviated from the standard prototypes of research, has zero practical usage and zero new theoretical thoughts.\n\n- Technical details are vague and inaccurate.\n  - In Eq. 6, what are the definitions of P and Q? What is the distance metric used to measure |P(x) - Q(x)|? It has a summation over a set of x and, for each summation term, P and Q take the same x. This differs a lot from the formulation of delta, where x_i and x_j are different single samples not a set. Please correct and articulate your formulations.\n  - In Algorithm 1, k is not reflected in the summation terms when calculating L_g.\n\n- Experiments are far from convincing.\n  - The dataset includes only the toy-like MNIST dataset. Actually the method itself cannot be applicable to other realistic datasets.\n  - The WGAN-GP backbone is out-of-date. Experiment with multiple backbones including the recent SOTAs like StyleGAN-XL [r3]. Please also discuss the recent progress in GANs, to convince readers that the authors are knowledgeably ready to research in the GAN regime.\n\n[r1] Liu, Ming-Yu, and Oncel Tuzel. \"Coupled generative adversarial networks.\" NeurIPS 2016.\n[r2] Metz, Luke, et al. \"Unrolled generative adversarial networks.\" NeurIPS 2017.\n[r3] Sauer, Axel, Katja Schwarz, and Andreas Geiger. \"Stylegan-xl: Scaling stylegan to large diverse datasets.\" SIGGRAPH 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Clear writing, but vague formulations for the equations and the algorithm.\n\nZero practical usage and zero new theoretical thoughts.\n\nEasy to reproduce.\n\n",
            "summary_of_the_review": "See above.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper572/Reviewer_WzHB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper572/Reviewer_WzHB"
        ]
    }
]