[
    {
        "id": "ZcGJWTbvHs",
        "original": null,
        "number": 1,
        "cdate": 1666355878231,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666355878231,
        "tmdate": 1669977786705,
        "tddate": null,
        "forum": "Frt6LTRFhui",
        "replyto": "Frt6LTRFhui",
        "invitation": "ICLR.cc/2023/Conference/Paper5212/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper \"General Policy Evaluation and Improvement by Learning to Identify Few But Crucial States\" proposes to consider fingerprints based on actions distributions on states, rather than using all policy parameters, for building value functions that generalize over all possible policies. Such policy evaluation functions are very useful for discovering efficient policies, by gradient ascent of policy parameters through the value function, as they do not require interactions to improve policies whenever the value function is general enough. Authors show interesting results.  ",
            "strength_and_weaknesses": "Strengths:\n   - Interesting results : only few states are enough to get useful value functions and policy improvements in many classical settings\n   - Well written paper\n\nWeakness: \n    - I feel the approach is a very straightforward extension of [Harb et al., 2020], as the only novely comes down to simply alternating the evaluation and improvement algorithms from that paper (except a very minor difference on the way of considering rewards - as a regression target rather than classification in discretized bins). \n    - Thus the interest of the paper is mainly experimental. However I feel that not enough analysis is given to well understand the dynamics of the proposal.   \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clearly written paper, but novelty is limited",
            "summary_of_the_review": "I already gave my main concerns above, I have now some questions for the authors: \n    - Is there no catastrophic forgetting during learning, since V has to embed every encountered policies ? Is there not a risk of oscillation between two parameters areas ?  \n    - From my point of view, the experimental part miss a study of probing states dynamics. It would be veryinsightfull to understand how they behave during learning. Do they first collapse in a single state while the value is not well learned for spreading on more useful areas afterwards ? \n   - As PVN is a very related approach, I would have expected to have results of this approach as a baseline, maybe with a version that alternate evaluation and policy improvement\n    - Did authors experiment a version with reward discretization as in PVN ?  \n   - Algorithm1 only learns from feedback over full trajectories. Thus the variance must be high. How would it perform on more difficult stochastic environments ? I suspect that considered tasks are rather simple and this kind of approach would be difficultly transferred on more complex environments. \n\nIn summary, a very easy to read and interesting paper, but from my point of view the proposal is not enough innovative for ICLR, and the experimental analysis not enough insightful. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5212/Reviewer_eb1Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5212/Reviewer_eb1Q"
        ]
    },
    {
        "id": "H8XI2EiBjbO",
        "original": null,
        "number": 2,
        "cdate": 1666372328151,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666372328151,
        "tmdate": 1666385970105,
        "tddate": null,
        "forum": "Frt6LTRFhui",
        "replyto": "Frt6LTRFhui",
        "invitation": "ICLR.cc/2023/Conference/Paper5212/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors get the parameter-based state-value function (PSVF) Faccio et al., 2021 to work. The motivation is simple, if there's a value function $V:\\mathcal S\\times\\Theta\\to\\mathbb R$ which maps both a state and the parameters of a policy to an accurate cumulative discounted reward estimate, then the policy can be improved by maximizing $V(s,\\theta)$ w.r.t. $\\theta$. The only problem is that for any class of policies to be useful, the parameter space of that class of polices $\\Theta$ is generally quite large (thousands of dimensions) making learning the PSVF V impractical.\n\nThe authors propose to map this large $\\mathcal S\\times\\theta$ to a smaller space with two simple tricks. First, they don't worry about the discounted return will be from a specific state, peruse a general policy evaluation framework to learn the sum of discounted rewards from an average starting state. This isn't novel. The novel and significant step they take is to learn K \"probing states\" $s_1,\\dots,s_K$ and instead of learning the PSVF they estimate (this notation only works for deterministic policies, but the idea works for certain stochastic policies as well) the value of a policy with $V_\\phi(\\pi_\\theta(s_1),\\dots,\\pi_\\theta(s_K))$. Since $K$ is small (generally 200 or less) and the action space is also small, this addresses the curse of dimensionality.\n\nThe authors then do two sets of experiments, one on MNIST and the other on MuJoCo outlining interesting characteristics of their PSSVF method, showing that the probing states learned are meaningful and that the learned value estimation is useful for constructing good policies.\n\n\n\n\n",
            "strength_and_weaknesses": "I'll compress \"Strength And Weaknesses\" and \"Clarity, Quality, Novelty And Reproducibility\" into one review, as there's a lot of overlap.\n\n### Strengths:\n- the paper has a clear goal which is quite relevant: learn a single value function for evaluating any policy, and the authors clearly explain how they go about working to this goal, compare to past work on how this goal was achieved, and what they do different, and why this leads to better results.\n- the paper's notation is generally clear, well introduced and easy to follow\n- The experiments show what the method is doing, not just that it gets good results but that the probing states are significant, and that a value function learned even on random policies can be used to generate a good policy.\n- I like that the authors don't claim their method is the best, but compare against more sample efficient methods like SAC.\n\n### Major Concerns:\n\nMy only major concern is that the authors seem to claim to do more than they actually achieve. The claim is to \"learn a single value function for evaluating...any policy\" But they don't do this, they learn an estimate $J(\\theta)$ of the expected return. Worse, they really dress the paper up to make it look like they learn a value function, using $V(\\theta)$ notation for example, and discussing value functions and Q-functions at length in the introduction. While I'm sure their methods could be modified to learn an actual parameter-based state-value $V(s,\\theta)$ I bet it's tricky (since most of the time the estimate of the value function depends on an estimate of the value function).\n\nSo I'd really wish the author's are honest with their readers and say something like \"in order to focus on the main point, the probing state method and not get distracted by the complexities of learning a value function, we focus on the easier task of learning estimate $J(\\theta)$ of the expected return on finite horizon MDPs. And drop anything that distracts from this simpler yet significant goal.\n\nThe authors discuss a tiny bit the step to learning a real value function at the top of page 7, but it's a bit of a farce, since they only learn the value function over initial states, if I remember correctly in MuJoCo initial states are all quite similar, so it's unsurprising that the authors state \"The results were very similar to those we presented in this section\"\n\nFor this reason, I have to say on the correctness that \"Several of the paper\u2019s claims are incorrect or not well-supported,\" but I don't see any reason why this can't be fixed.\n\n### Minor Concerns:\n- I don't find Figure 1 helpful at all. Why are some lines solid, why are some dashed? What's $U_\\phi$? The algorithm has a replay buffer (or storage of policy parameters / rewards) that's not in Figure 1. Either have a helpful figure or no figure at all but not a figure for figure's sake.\n- I think that Eq. 2 is incorrect, but since I think it's a distraction and should be dropped I won't go into detail.\n- in eq. 4, $\\pi_\\theta(s_1)$ is a distribution over actions. You discussed \"parameters of the output distribution of the policy in such states\" but left that out in eq. 4. For eq. 4 to be correct, this mapping from $\\theta$ to distribution parameterization must be included.\n- Please also tell the readers that the estimation of $V(\\theta)$ only works since in MuJoCo the starting states are all very similar, and the MNIST experiment is stateless.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "see above for clarity, quality and novelty.\n\nThe experimental setup is nicely explained in the appendix, and I think reproducible. ",
            "summary_of_the_review": "Overall, when reading the paper I felt I learned a significant new tool, one that I feel others in the RL community should learn",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5212/Reviewer_zMK3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5212/Reviewer_zMK3"
        ]
    },
    {
        "id": "ADL1bkFiQ6",
        "original": null,
        "number": 3,
        "cdate": 1666513736949,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513736949,
        "tmdate": 1666513736949,
        "tddate": null,
        "forum": "Frt6LTRFhui",
        "replyto": "Frt6LTRFhui",
        "invitation": "ICLR.cc/2023/Conference/Paper5212/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis paper proposes using the policy evaluation networks architecture\n(policy fingerprinting) with the algorithm structure of parameter-based\nvalue functions to learn a single value of the start state that\nconditions on the policy representation. This value function is learned\nthrough online interaction, like parameter-based value functions, but\nscales to larger policies because of policy fingerprinting. The\neponymous finding is that, for some mujoco environments, the number of\nprobing states for policy fingerprinting is surprisingly low while\nfacilitating zero-shot policy learning on different architectures.\n\n",
            "strength_and_weaknesses": "# Strengths\n\n-   The main result is very interesting and surprising. The result being\n    that policies in mujoco environments can be represented by observing\n    the actions at a small number of states. It is further surprising\n    that a strong policy can be learned by regressing on the actions at\n    those few states.\n\n-   While there are a few clarity issues, which I outline in the\n    detailed comments below, the overall paper is well-structured and\n    easy-to-follow. Not only is the motivation clear, the contribution\n    is clear as well.\n\n# Weaknesses\n\n-   The main weakness is that the novelty is incremental.\n    Algorithmically, there is little novelty: the proposed algorithm is\n    a combination of the architecture of policy evaluation networks and\n    online algorithm structure of parameter-based value functions. The\n    eponymous finding, that policy evaluation and improvement can be\n    done by identifying few but crucial states, is limited to only two\n    mujoco environments. While the experiments encompass more than this\n    finding, this seems to be the major contribution.\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Detailed Comments (Clarity, Quality, Novetly, Reproducibility)\n\n-   Section 2 (Stochastic vs deterministic): Relatively minor but, I do\n    not think these two cases should be treated separately, as the\n    objective written in equation 1 is still the objective being\n    maximized. The policy-gradient theorem merely provides an\n    alternative update for the policy directly. The deterministic\n    objective can still be thought of as an integral, where the\n    distribution is a dirac delta on the deterministic action.\n\n-   Section 3 (Probing states and policy visitation distributions): The\n    idea of having a single value function for many policies is\n    appealing. The parameter representation is a global description of\n    the policy. But, the probing states are local and depend on the\n    policy and the visitiation distribution it induces. For example, two\n    deterministic policies travelling in separate directions will never\n    visit the same states, and hence the probing states may not be\n    shareable amongst these two policies.\n\n    Is there a theoretical justification or intuition for evaluating all\n    the policies on one distribution of probing states (even if these\n    states are learned)?\n\n-   Section 3 (Policy Improvement and Probing states): Because the\n    probing states are learned, it may be the case that the probing\n    states do not correspond to any real state in the environment.\n    Improvements on these probing states benefit the policy globally\n    only through generalization. Returning to the \"two direction MDP\"\n    example from my earlier point, the action on the learned probing\n    state could be sufficient for evaluation/prediction and yet may not\n    provide a useful gradient.\n\n-   Section 4.1 (Offline policy improvement): It is quite surprising\n    that, after constraining the loss to be 12\\%, the value function is\n    able to extrapolate and provide a gradient that improves the CNN to\n    65\\%. MNIST, however, is a relatively simple dataset which can be\n    classified with a linear decision boundary. I wonder if similar\n    results hold on harder datsets, such as omniglot, fashion MNIST and\n    CIFAR.\n\n# Minor Comments\n\n-   Section 2: Markovianity -> Markov Property\n-   Section 4.3: \"trough\" -> through\n\n",
            "summary_of_the_review": "It is difficult to rate this submission because, while lacking in\nnovelty, the eponymous finding is rather surprising and interesting. The\nlack of novelty in the algorithm is unavoidable, as it builds on two\ndistinct works that are quite similar. However, I think the experiments\nhave potential to provide more insight. While the results are mostly\npositive, the submission could be strengthened by outlining the limits\nthe approach in negative results. For example, is it more or less\ndifficult to learn to predict the losses when the dataset is more\ncomplicated (omniglot, fashion MNIST, CIFAR). Does this difficulty\ntranslate to difficulty to extrapolate to new architectures, or to\nzero-shot train an architecture to a performance level beyond what was\nseen? There is plenty of opportunity to contextualize the very\ninteresting main finding. As the submission currently stands, I am\nleaning towards weak reject. I am open to increasing my score of some of\nthese points are addressed.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5212/Reviewer_yJcN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5212/Reviewer_yJcN"
        ]
    },
    {
        "id": "69iPOj8jQ1_",
        "original": null,
        "number": 4,
        "cdate": 1666640480036,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640480036,
        "tmdate": 1666640480036,
        "tddate": null,
        "forum": "Frt6LTRFhui",
        "replyto": "Frt6LTRFhui",
        "invitation": "ICLR.cc/2023/Conference/Paper5212/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work combines two existing approaches: representing a policy based on its behavior in a set of probing states and finding a successful policy via a critic that estimates the return of any input policy.\n\nThe ability to automatically learn probing states is shown via MNIST.\nThe performance on a few continuous control tasks is demonstrated.\nThe ability to use a trained critic to find a successful linear policy is also shown.\n",
            "strength_and_weaknesses": "This work identifies a promising combination of two existing methods.\nIf probing states are a more efficient representation for complex policies, then this method is of interest.\n\nThe authors claim that \"Flattening the policy parameters ... is difficult to scale to larger policies,\" but do not show the new method scaling.\nUnfortunately, the proposed method is not shown to scale to larger domains. Empirical evaluation is required to confirm favorable scaling, but all of the domains considered can be solved with simple policies.\nI am skeptical that this method scales better than using the policy parameters: a strength of policy gradient methods is that the policy complexity generally scales more slowly than the environment complexity. However, this method requires coverage of the state space, so the number of required crucial states may scale quickly.\nThe key experiment is shown in the appendix (Fig 10) and is quite limited in scope. It specifically uses the environments where few probing states are needed to learn a policy.\n\nMinor Comments:\n- The \"Policy fingerprinting\" paragraph should be divided to more clearly partition novel contributions from existing work.\n- By changing the rewards for Hopper, Walker, and Ant, those environments are no longer being solved (only modified versions of them). Likewise, gamma is part of the MDP; reporting total return (gamma=1) when gamma is 0.99 for only some evaluated methods is incorrect.\n- Sec 4.3: \"trough\" -> \"through\"\n- The limitation of the proposed method to deterministic policies is used to motivate baseline selection, but this limitation is a choice rather than part of the problem setup. To use this line of reasoning, one should also express why deterministic policies are advantageous.\n\nQuestions:\n- How were domains selected for evaluation? Why are only two environments used for Fig 10?\n- What is the behavior of your method with stochastic policies?\n- Can your approach perform well when the policies have different architectures during training?\n- Can vanilla PSSVF be used with weighted sampling?",
            "clarity,_quality,_novelty_and_reproducibility": "The combination of methods is novel, but the utility is not demonstrated.\n\nIn addition, the experiments are oddly limited:\n- only continuous control problems are used for main experiments\n- only deterministic policies are used for main experiments\n- only environments where few crucial states are needed are used for evaluation\nTo support the claim that probing states are a better representation, thorough comparisons between different representations should be performed.\nIn particular, to demonstrate that probing states scale better, evaluations should be performed with more complex environments.\n\nThe prioritization of content inclusion needs to be improved. Key experiments are in the appendix while lots of content in the main body can be cut or moved to the appendix.\nNote that \"reviewers are not required to read the appendix\" (ICLR 2023 CFP). I have still read the appendix, but I urge the authors to move key experiments to the main body.\nExamples:\n- Sec 2 and 3 contain lots of non-novel content that is not needed within this work.\n- Likewise, \"a demonstration that fingerprinting can learn interesting states in MNIST\" is of limited interest (no hypothesis is being tested; no baselines are used for comparison).\n- Also, learning a near-optimal linear policy does not show that this method has an advantage over directly using policy parameters (i.e., this ability is due to PBVF).\n- Evaluating the contribution of policy fingerprinting is crucial, yet it is in the appendix.",
            "summary_of_the_review": "The proposed method is motivated by the difficulty scaling a policy-parameter-based approach. However, this benefit is not supported.\nAdditionally, the overall experiments are limited without sufficient justification.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5212/Reviewer_xK3x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5212/Reviewer_xK3x"
        ]
    }
]