[
    {
        "id": "nBVi9rDSB-a",
        "original": null,
        "number": 1,
        "cdate": 1666609433472,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609433472,
        "tmdate": 1666983696563,
        "tddate": null,
        "forum": "SNgLnzFQeiD",
        "replyto": "SNgLnzFQeiD",
        "invitation": "ICLR.cc/2023/Conference/Paper373/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes use of entropy of speech-text alignment distribution as a regularizer during model training or as an additional supervision signal in the model distillation process.  As a regularizer the alignment entropy attempts to prevent the alignment distribution that\u2019s induced without any supervision from becoming too concentrated on some, possibly erroneous, alignments.  And during model distillation the alignment entropy is intended to provide additional supervision to the student model.  The key idea in the proposed approach is to utilize dynamic programming with entropy semi-ring to efficiently compute the alignment entropy.  Authors propose and open-source a numerically stable and parallel implementation of CTC and RNN-T within the semi-ring framework.",
            "strength_and_weaknesses": "Pros:\n* A novel regularizer and distillation objective based on entropy of alignment distribution achieving state of the art performance on Librispeech in streaming scenario.\n* Open-source stable and parallel implementation for CTC and RNN-T\n\nCons:\nN/A\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Very clear, pleasure to read\nNovelty: Novel, significant contribution\nReproducibility: empirical results should be reproducible",
            "summary_of_the_review": "Paper makes a novel and significant contribution to enable use of alignment entropy as part of model training process.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper373/Reviewer_dC8C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper373/Reviewer_dC8C"
        ]
    },
    {
        "id": "nyRwmkGQ5Yv",
        "original": null,
        "number": 2,
        "cdate": 1666671096274,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671096274,
        "tmdate": 1666671096274,
        "tddate": null,
        "forum": "SNgLnzFQeiD",
        "replyto": "SNgLnzFQeiD",
        "invitation": "ICLR.cc/2023/Conference/Paper373/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a numerically stable and parallelizable version of the entropy semi-ring for ASR training. For numerical stability, the paper introduces log entropy semiring with $<\\log p(e), \\log(-p(e)\\log p(e))> $. For the knowledge distillation, which is based on the hard and soft labels, the paper proposes one more semi-ring called log reverse-KL semiring by concatenating log and log entropy semi-rings. This way, the paper tries to incorporate alignment information into the distillation loss. Experiments show the effectiveness of the method on Librispeech in two settings: (1) adding entropy regularization to CTC/RNNT LSTM/Conformer combinations reduce the WER especially when the base model is not very strong. (2) In the RNN-T distillation experiments add Libri-Light as the semisupervised data which are pseudo labeled by a strong supervised model, then a teacher is learned using a combination of these LibriLight and LibriSpeech data sets. The student model is trained on LibriLight using the teacher. The results show that the proposed distillation approach performs better than hard and soft distillation alone. Comparison with streaming models also show the success of the proposed approach. The paper will also be accompanied with an open-source implementation of the proposed approach.",
            "strength_and_weaknesses": "Strengths:\n- The paper brings the mathematically backed up idea of semi-rings back to ASR such that they can be used in CTC or RNNT systems. \n- The paper is clearly written.\n- Experimental results show good WER performance in the Librispeech setting.    \n\nWeaknesses: \n- In Table 3, does any of the papers utilize the LibriLight data? If yes, it might be better to denote that in the table. Current semiring knowledge distillation proposal might be benefiting from that external data and the comparison might not be fair enough. \n- The appendix might be improved by giving some more details on the gradient back propagation over the semiring.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. Clearly written except some typos. \nMinor suggestions: \n- $x_3 $ should be $ v_3$ in Example 2.10.\n- Motvation -> Motivation in Section 5.1. \n\nQuality: Good. Mathematical expressions are given correctly. Experiments show gains in both entropy regularization and in knowledge distillation experiments. \n- In the regularization experiments, the batch size is 2048. How would this choice affect the regularization performance? Is the proposed model efficient memory-wise so that one can train with such a large batch size?    \n- A further analysis of the performance on sequences with different (short and long) lengths might have provided some further intuition for one of the motivations behind the proposed methods. \n\nNovelty: Sufficiently novel. It revives the FST framework in the case of neural network training by building on the earlier studies on semi-rings. \n\nReproducibility: Even though some details such as the exact architecture of the RNNT model is missing (e.g. the structure of the predictor network), the paper is probably going to be accompanied by the open-source code (based on the claims in the text). Hence, it should become reproducible.  ",
            "summary_of_the_review": "The paper brings the mathematically backed up idea of semi-rings back to ASR such that they can be used in CTC or RNNT systems. The paper provides both mathematical description and experimental evaluation of the proposed semirings. Experiments on entropy regularization and knowledge distillation show low enough WERs. The paper is well-written. Open source implementation will be provided and the paper should become reproducible. It is a good paper in general.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper373/Reviewer_NK1H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper373/Reviewer_NK1H"
        ]
    },
    {
        "id": "47CzqGjLyn",
        "original": null,
        "number": 3,
        "cdate": 1666686036886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666686036886,
        "tmdate": 1666686036886,
        "tddate": null,
        "forum": "SNgLnzFQeiD",
        "replyto": "SNgLnzFQeiD",
        "invitation": "ICLR.cc/2023/Conference/Paper373/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper authors propose to leverage entropy semiring and alignment entropy to improve the performance of neural speech recognition, via regularization or distillation. Experimental results show its effectiveness. There are also open-source contributions based on this work.",
            "strength_and_weaknesses": "Strength: The idea of regularization or distillation based on alignment entropy is technically sound. Overall, this paper is well structured, with detailed background introduction on semiring. Experiments are well-designed for validation purpose. Open-source contribution with this work makes it easier for speech community to experiment with and integrate this approach. \n\nWeakness: I think experimental section could be made stronger with more details added to explain design and results. See questions in the summary section below.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this paper is clearly written, with well structured sections on background, methodology, experiments etc. It proposes new approaches to improve neural speech recognition, and it is reproducible with open-source implementation.",
            "summary_of_the_review": "The methodology presented is technical sound, with convincing experimental design and results. Open-source implementation makes it easier for speech community to leverage this work. I think the experimental section could be made stronger by adding more details to justify design and results, including:\n\n1. Adding std to those results in Table 1-3 to show if improvements are statistically significant.\n2. How to determine model hyperparameters, e.g. those listed in the \"Experimental Setup\" paragraphs in Sec 5.1 & 5.2.\n3. As shown in Table 3, prior work has different model sizes. Do authors also experiment with smaller models (e.g. similar as 30M or 80M #Params)?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper373/Reviewer_3ceX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper373/Reviewer_3ceX"
        ]
    },
    {
        "id": "1OCp1xyCHFR",
        "original": null,
        "number": 4,
        "cdate": 1666915920212,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666915920212,
        "tmdate": 1666915920212,
        "tddate": null,
        "forum": "SNgLnzFQeiD",
        "replyto": "SNgLnzFQeiD",
        "invitation": "ICLR.cc/2023/Conference/Paper373/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new speech recognition formulation based on the entropy semiring, which is used as an entropy regularization and knowledge distillation. First, the paper motivates hard alignment-based speech recognition issues based on overconfidence and the solution based on entropy regularization. Then, the paper provides an elegant and easy-to-understand formulation of the semiring from the basic one to their novel entropy semiring by caring for the backpropagation. The experiments also show the expected result of using entropy regularization and knowledge distillation experiments for the well-known Librispeech benchmarks with state-of-the-art performance in a streaming setup. \n\n",
            "strength_and_weaknesses": "Strength\n\n- clear descriptions of each semiring operations\n- novel entropy semiring formulation by considering the backpropagation capability and efficiency.\n- reasonable experimental results showing the mitigation of overconfidence and the knowledge distillation\n- archives the state-of-the-art performance in the Librispeech streaming setup\n\nWeaknesses\n\n- it could have more discussions about other applications than ASR (although they are briefly mentioned in the conclusion section).\n- lack of the analysis\n- there are some duplicated descriptions (e.g., the first paragraph in Section 3 and the first paragraph in Section 3 are highly overlapped with Section 1). These parts can be improved. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\n- the paper is well written.\n- semiring explanations are easy to understand\n- some parts (e.g., Example 2.8) are difficult to follow, but later I found the solution in the appendix. It is better to add some pointers to the appendix.\n- the motivation of corresponding experiments is also evident.\n\nQuality\n\n- The elegant formulation and reasonable experimental results make this paper's quality very high.\n\nNovelty\n\n- Although entropy semiring itself is not novel, I think the authors' contributions to the backprobable algorithm have enough novelty.\n\nReproducibility\n\n- The method will be open-sourced. Together with the public Librispeech benchmarks, the paper has high reproducibility\n",
            "summary_of_the_review": "The paper's novelty, clearness, experimental effectiveness, and reproducibility significantly contribute to the machine-learning community. I strongly recommend this paper be accepted.\n\nOther suggestions\n- It would be better to provide some more concrete examples for the semiring explanation (e.g., Example 2.6 corresponds to the forward computation, right? We can provide such an example)\n- Can we visualize whether the overconfidence is mitigated or not in Section 5.1? I think such analysis (visualization) strengthens the claim of this paper.\n- Section 5.1: It might significantly improve the performance in the out-of-domain conditions (e.g., noisy speech recognition, multilingual speech recognition). I would recommend the authors apply such tasks to prove further the effectiveness of mitigating overconfidence by the proposed method.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper373/Reviewer_jsaa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper373/Reviewer_jsaa"
        ]
    }
]