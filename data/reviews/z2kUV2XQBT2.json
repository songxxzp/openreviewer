[
    {
        "id": "taSAOyyjwD",
        "original": null,
        "number": 1,
        "cdate": 1666295193033,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666295193033,
        "tmdate": 1666295193033,
        "tddate": null,
        "forum": "z2kUV2XQBT2",
        "replyto": "z2kUV2XQBT2",
        "invitation": "ICLR.cc/2023/Conference/Paper2593/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers learning a quantisation of colour space, and the parallels to how humans learn colour names. \nA novel deep network architecture for colour quantisation is proposed, which is learned by optimising accuracy on a task of interest (image classification or object detection) when using images with quantised colours as input, but also takes into account several regularisation terms. \nExperiments are conducted on 4 image classification datasets (CIFAR-10/100, STL10, and TinyImages-200) and one object detection dataset (MS-COCO). Comparisons are made to an upper bound of non-quantised colours, and 4 existing methods for colour quantisation.\nGenerally, the proposed method performs well compared to others on image classification when few colour quantisation levels are used (4-16) and worse otherwise. For object detection the proposed method is compared to only three alternative methods, and compares consistently better or comparable to those, yet substantially worse than the upper bound obtained by not applying any color quantisation. \n",
            "strength_and_weaknesses": "Strengths\n\n+ The paper considers a relatively little studied problem, which might interest a (limited) subset of the ICLR audience. \n\n+ Experiments are conducted on two different tasks (image classification and object detection), and using 4 different datasets for the classification task. \n\n+ The proposed colour quantisation method seems more effective that existing alternative techniques. \n\n\nWeaknesses\n\n- It is not clear how the results for the existing methods were obtained. Were experiments conducted by the authors of the current paper? Was code provided by the authors of the existing methods? Please clarify the process. \n\n- It was not clear why an input-dependent approach is used to determine the color palette, while an input-independent network is used to map the input image to quantisation indices in the Annotation branch. What happens if we remove the \"Palette branch\"  and just learn a set of centroids in color space?\n\n- Parts of the paper are hard to follow, in particular the sections 3.4 and 4.4 and 4.5. \n",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\n I found the clarity to vary across sections of the paper. Most of the paper is is clear. but some technical presentations and experiments were hard to follow.\nFor example:\n- Section 3.4 the paragraph \"colour probability map embedding\" was not clear. Is there any fine-tuning of the model involved here, or just an adaptation of an already trained model? I would suggest a more formal description of the method to clarify the exposition.\n- Similarly the paragraph \"Colour evolution\" in the same section was unclear to me. Please add a more formal description of the approach here. \n- The same goes for the last paragraph in section 3.4 \"Central colour embedding\".  It was not clear to me whether this is an alternative regularisation method used during training, or a post-hoc adaptation of the trained model. \n\nThe captions of Figure 4 and Figure 5 are not separated from the main text, probably by applying negative vspace in the latex source, which is hurting readability of the text, and might be in conflict with the author guidelines.\n\nIt was not clear to me whether the image classification and detection models are trained jointly with the color quantisation model, or whether the classification/detection models are pre-trained without color quantisation, and the used as is without further training them when learning the colour quantisation model. \n\nThe purpose of Section 4.4. is not clear. \nFor example, the first sentence was unclear to me: \"Similar to task centered colorCNN [ref], we are also inferior to the traditional method under a large space.\" \nThe section does not describe any experiment, but rather seems to be an interpretation of the previous experiments. \nThe text itself should be clarified by clearly pointing to the experimental results on which it is based, and integrating it in the corresponding section. In its current state it did not bring me any additional insights. \n\nThe colour evolution experiment in section 4.5 it not clearly described. It seem that in the first stage the proposed colour quantisation model is trained to align with the Nafaanra color naming in three colours, and to perform well on the CIFAR-10 classification task. The second stage learns a fourth colour bin, starting from the three colour quantisation. But it seems the design of the experiment already dictates which of the three initial colour bins will be split in the second stage, which seems to strongly direct the outcome of the second training stage. \nMoreover, it is not clear if the alternative color quantisation methods would exhibit similar behaviour, or whether this is unique to the proposed method. \n\n# Quality\n\n- The paper refers to studies of color naming in the Nafaanra language, but does not explain why these studies are of particular interest as compared to color naming in other languages, nor how the findings for Nafaanra differ from those for other languages. When referring to analysis of Nafaara in the first paragraph of page 2, or elsewhere, there is no reference to the corresponding study.\n\n- The discussion of related work in section 2 is rather limited, and does not succeed in clearly placing the current work in the context of related work, ie which problems of existing methods are solved? What additional insight does the current work provide over others?\n\n- The technical approach lacks a clear motivation. Why is it insufficient to learn several centroids in color space, and assign pixels to the nearest color centroid? What is the motivation for the Palette and Annotation branches of the proposed approach over simpler alternatives? \n\n- Purity regularisation: it was not clear to me why a max over pixel positions is taken, this seems to ensure that at least one pixel is strongly assigned to each one of the quantisation bins, rather than that *all* pixels are assigned to just one color. Did the authors consider, for example, summing over the pixels the entropy of the assignment distribution?\n\n- When considering image classification, and increasing the number of colour quantisation levels, the proposed method saturates to an accuracy level  that is lower than that for some of the other methods to which is compared (OCTree, MedianCut, MedianCut+dither). A discussion to interpret this result is missing. Is this an inherent limitation of the proposed method? It is due to what? Can it be overcome? \n\n\n#  Novelty\n The proposed technique to address color quantisation seems novel to me. The topic of study is relatively little studied, and the findings seem to make a contribution to this literature. The technical tools / architectures used in the work are not novel as such: combining existing architectures for the classification and detection tasks, and presenting a new color quantisation network built using attention blocks and a ResNext component. The paper does not go into detail on how the proposed technique differs from the ColorCNN baseline model to which comparison is made. What shortcomings does the prior work have, and how does the current work address these?\n\n\n# Reproducibility\nthe authors say code will be released upon publication. Standard datasets are used. \nClassification experiments are run on a single GPU, for detection models a setup with 4 GPUs is used. \nThe replication of the results for the existing methods is less clear: the paper does not discuss how these were obtained.  \nIt is not clear how the color index map for Nafaanra in section 4.5 is obtained, and whether it is accessible to other researchers. \n\n\n# typos \nThe manuscript contains quite a few typos, here a are a few:\n- Page 3 and 7: \"asteroid\" --> \"asterisk\"\n- Page 6: in Eq 9 L_High is used, which in the line below L_H is used, these should be the same as I understand it. \n- Page 7: in paragraph \"evaluation metrics\" repetition in \"...top-1 classification accuracy and top-1 classification accuracy...\"\n",
            "summary_of_the_review": "The paper presents a novel color quantisation technique, and experiments that compare it to a number of baselines. \nThe main concerns that I have with the paper is a lack of clarity in some parts describing the methods and experiments, and the positioning wrt related prior work. Overall I'm not convinced that the current paper would make a significant impact in the ICLR community.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2593/Reviewer_SHap"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2593/Reviewer_SHap"
        ]
    },
    {
        "id": "VXy6WE2Bia",
        "original": null,
        "number": 2,
        "cdate": 1666561584980,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666561584980,
        "tmdate": 1666561584980,
        "tddate": null,
        "forum": "z2kUV2XQBT2",
        "replyto": "z2kUV2XQBT2",
        "invitation": "ICLR.cc/2023/Conference/Paper2593/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a color quantization transformer with the goal of reduding the number of bits needed to allocate the color information, but, at the same time, trying to look at the question: Do machine learning methods make evolve the different color terms as human civilizations?",
            "strength_and_weaknesses": "The paper has a positive side, on the study of how can we quantize an image without losing a large accuracy in downstream tasks. This said, this is not a novel research line, as it was already presented in ColorCNN. The main novelty of this work is therefore, trying to include perception into this task.\n\nIn terms of results (Figure 5), the proposed method seems to beat ColorCNN is all the cases, and other perception based approaches in low-bin level (1-4). This is positive, but I believe the for number of bins in which there exist an advantage, the accuracy is still not acceptable for the downstream tasks.\n\nRegarding the analysis on the order of appearances of the color names, I believe that the analysis is not of real help; as I am quite convinced the authors are forcing that order of appearance due to the enforcement of the perceptually similarity regularisation. That loss guides the net to actually predict exactly the order of colours in thw WCS, as the probabilities in WCS exactly represent that order of appearance.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is mostly clear, although some parts can be improved, for example, the paragraph after equation 2.\n\n- As explained in the previous comment, there is some novelty in combining both perception and machine learning techniques for color quantization, but the results are not enough good in my opinion.\n\n- There are some typos here and there, for example:\n               \u00ba Missing citation to the papers in section 2.1.\n               \u00ba would generates (page 5)\n               \u00ba we locates (page 5)\n\n- Authors comment on the realising of code after acceptance, so it should be reproducible.",
            "summary_of_the_review": "The paper has interest and that idea might be promising; however, I believe the results of this approach are still not yet comprisng with pure perception approaches, and also, the analysis of the appearance of the color names is perfectly guided by one of the losses applied in the experiment, and therefore cannot be really explained out of the quantization effect. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2593/Reviewer_bh8b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2593/Reviewer_bh8b"
        ]
    },
    {
        "id": "OcoKnAnNL_9",
        "original": null,
        "number": 3,
        "cdate": 1666627250105,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627250105,
        "tmdate": 1666627250105,
        "tddate": null,
        "forum": "z2kUV2XQBT2",
        "replyto": "z2kUV2XQBT2",
        "invitation": "ICLR.cc/2023/Conference/Paper2593/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an end-to-end color quantization transformer that is able to discover the color naming system under the need of perception and machine. The framework contains two branches, respectively for structure mining and color localization. The proposed CQFormer quantizes the color space of the input image and the generated quantized image can maintain recognition accuracy, which is of practical application such as images compression. Besides, the proposed method can provide consistent evolution pattern between the color system and the basic color terms across human languages. The experiments also demonstrate the effectiveness of the method.",
            "strength_and_weaknesses": "Strength:\n(1)\tThe paper is well-written and description of the proposed method is clear and easy to follow.\n(2)\tThe core problem is novel and has potentially practical usage in vision field.\n(3)\tThe proposed modules are reasonable and the whole experiments indeed support the effectiveness of the method, which shows strong performance improvements compared with previous methods.\n\nWeaknesses:\n(1)\tFrom Table 3, we can see that the CQFormer indeed shows superiority under extremely low bits, such as 1-bit and 2-bit, and the performance maintains relatively stable when the color bit decreases. But it fails to compete with traditional methods like MedianCut and OCTree when the color bit is large, such as 5-bit and 6-bit. Could you provide explanation for this phenomenon, and if there are any improvements that can help CQFormer keep consistent performance superiority across all color bits?\n(2)\tFor the experiments part, the value of C is missing in the Training Settings part.\n(3)\tThe description for the Palette Branch is not enough. For example, the detailed structure of Cross Attention and FFN is not specified or formulated. And I do not know if there is a shortcut alongside or a normalization along with Cross Attention and FFN.\n(4)\tSome typo such as \u201cto to\u201d in the 3rd paragraph of the Introduction, \u201ca explicit\u201d in the last but one paragraph of Page 4. And the author should leave some blank between Figure 5 and the following half paragraph.\n(5)\tI am doubt with the name \u201ctransformer\u201d because it seems that the author only implement one \u201cCross Attention + FFN\u201d layer, which is only part of Decoder of standard Transformer. In my opinion, only the stacks of multiple Encoder layers or Decoder layers could be named as \u201cTransformer\u201d. The implementation in this paper seems like only adopting Cross Attention Mechanism. So I suggest change the name to the other one without \u201cTransformer\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written generally but with some minor typo and less clear in some parts. The quality and novelty meet the requirement of the conference. And the method is able to be reproduced.",
            "summary_of_the_review": "This paper is generally of good quality and the novel method is supported by the sufficient experiments. I point out some weaknesses of the paper and ask for paper revision. In conclusion, this paper meets the requirement of the conference and I vote for Accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2593/Reviewer_4LP2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2593/Reviewer_4LP2"
        ]
    },
    {
        "id": "y5-Ez9hYRz",
        "original": null,
        "number": 4,
        "cdate": 1666688215032,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688215032,
        "tmdate": 1666688215032,
        "tddate": null,
        "forum": "z2kUV2XQBT2",
        "replyto": "z2kUV2XQBT2",
        "invitation": "ICLR.cc/2023/Conference/Paper2593/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents CQFormer: a color quantization transformer. The goal is to artificially discover the color naming system so that even with less bit representation acceptable performance on core vision tasks such as classification and detection can be achieved. Experiments show that the proposed method significantly outperforms related approaches for detection and classification tasks on public benchmarks. \n\n",
            "strength_and_weaknesses": "Strength:\n+ The ideas presented in this paper (i.e. CQFormer, loss formulation, and regularizers) are original to my knowledge. I found Perceptually Similarity Regularisation intuitive and useful. \n\n+ The experimental results show the effectiveness of the proposed approach for both detection as well as classification task. It is impressive that the proposed quantization with just a few bits is able to achieve a decent AP on MS-COCO object detection. The performance is clearly better than the related publications.\n\nWeakness: \n- Upper bounds are misleading. For example, why MS-COCO detection upper bound is this low? Is it the SOTA object detection? I do not think so. If it is the upper bound for the method then it has to be stated and compared against SOTA.\n\n- Are the weights of regularization terms (R) empirically decided? How sensitive is the model for a different alpha, beta, and gamma? Why alpha is chosen as 0 for the detection task? \n\n- Terms in equation 7 are not clearly defined. Such as what is c, (u,v)? Without these definitions, it is difficult to assess the technical correctness of this equation. \n\n- How sensitive is the model to different noises such as color jitter or Gaussian blur? Such studies are important for assessing the robustness of the proposed approach. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: The ideas presented in this paper (i.e. CQFormer, loss formulation, and regularizers) are original to my knowledge. I found Perceptually Similarity Regularisation intuitive and useful. \n\nReproducibility: I did not check for reproducibility very carefully. There are some technical details that need clarity. The datasets used are publicly available. The paper promises to release the code.\n\nClarity: The paper is not well-motivated. What is the impact of solving this task? Why someone would work on this problem? The motivation should be such that a reader from an outside area is able to appreciate the work. In the current write-up, it is highly unclear. Even the abstract does not make clear sense. The same applies to the title. It is much later possibly after going through the other related works such as colorCNN and the experiment section the motivation became somewhat clear. \n",
            "summary_of_the_review": "Though the method has some originality and achieves superior performance to related work, there are several weaknesses such as limited ablations, experimental choices, missing technical details, and poor presentation of the paper.\n ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2593/Reviewer_hYs4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2593/Reviewer_hYs4"
        ]
    }
]