[
    {
        "id": "6mn--CG_od",
        "original": null,
        "number": 1,
        "cdate": 1666580619941,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580619941,
        "tmdate": 1666580619941,
        "tddate": null,
        "forum": "KRLUvxh8uaX",
        "replyto": "KRLUvxh8uaX",
        "invitation": "ICLR.cc/2023/Conference/Paper5507/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes four additional task for validating whether current large VL models trained with contrastive losses are sensitive to detailed attributes and relationships of the objects. The tasks include genome attributes and relations and COCO and Flickr captions. The paper found that most large VL models can not respond to the changes applied to the attributes and captions. In order to address this issue, the paper propose to add  composition-aware hard negatives during training. The resulting model obtains similar results on the original downstream task and good results on the 4 proposed new tasks. ",
            "strength_and_weaknesses": "Strengths: (1) Bring up an interesting and critical issue of not sensitive to compositional order of the current large VL models \n(2) Four new tasks based on existing dataset that do not need human annotations to validating the aforementioned issue\n(3) a simple yet effective treatment to the issue by adding compositional-aware hard examples\n(4) Good results on the newly proposed tasks\n\n\nWeakness: \n(1) Hard to control each example in the 4 tasks, for example, swapping adj for the caption \"a cute girl is walking a little dog\", it would be good to have human check if the semantic is actually not valid \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of good quality and the author promise to release code",
            "summary_of_the_review": "The paper proposes 4 new tasks to measure whether a large VL model trained using contrastive loss are sensitive to compositional orders. After finding that most of them perform bad on these 4 new task, the paper propose a simple fix by adding compositional aware hard negative. These validation of 4 new tasks and the compositional hard negatives help the model be more compositional aware without much suffering on the original  downstream tasks. My largest concern is that when using the 4 tasks as validation dataset, the dataset should eb further validated by human to check if the semantic is valid or not.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5507/Reviewer_y4fM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5507/Reviewer_y4fM"
        ]
    },
    {
        "id": "kLw12yaZcOr",
        "original": null,
        "number": 2,
        "cdate": 1666638067589,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638067589,
        "tmdate": 1666638067589,
        "tddate": null,
        "forum": "KRLUvxh8uaX",
        "replyto": "KRLUvxh8uaX",
        "invitation": "ICLR.cc/2023/Conference/Paper5507/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a benchmark for evaluating the ability of vision-language models with regard to relations, attributions and order information. By testing existing visual-language models, they show that the exiting models have poor relationship understanding. They further propose a simple modification contrastive loss to help models become more sensitive to orders and compositionality.",
            "strength_and_weaknesses": "Strength:\n1) This paper is well-written and easy to follow.\n2) The discovery that existing visual-language models could not handle complex relationship understanding seems to be well-supported.\n\nWeakness:\n1) The proposed benchmark seems to be very similar as the Winoground, which is also cited in the related work. This, to some extent, limits the novelty of this work to the community. Could authors test the proposed composition-aware hard negatives on the Winoground to see if the method can improve these natural images capturing object relationship?\n\n2) The proposed composition-aware hard negatives needs to know the targeted composition beforehand, which greatly limits the generalization and the potential effectiveness of this work.\n\n3) \"Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation\" by Qin et al is closely related to this work and should be cited in Related work.",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "This work investigates the ability of existing visual-language models on relationship understanding and constructed a new complementary benchmark for this task. The proposed composition-aware hard negatives have major limitations but good to see they have effects on the targeted compositionality. I am leaning towards accepting this work and curious about the results on Winoground.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5507/Reviewer_LKvk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5507/Reviewer_LKvk"
        ]
    },
    {
        "id": "aLvYnTnBIs",
        "original": null,
        "number": 3,
        "cdate": 1666654599382,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654599382,
        "tmdate": 1666654599382,
        "tddate": null,
        "forum": "KRLUvxh8uaX",
        "replyto": "KRLUvxh8uaX",
        "invitation": "ICLR.cc/2023/Conference/Paper5507/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper probes the compositional generalization ability of large pre-trained vision-and-language models, such as CLIP. They introduce a benchmark dataset of synthetically generated caption perturbations, called ARO. They show that out-of-the-box performance on ARO is poor, and that existing models do not seem to distinguish captions with ordering perturbation. They also propose a method for training these models to be more robust compositionally, by mining and generating hard negative examples to fine-tune with.",
            "strength_and_weaknesses": "Strengths:\n- The discussion of the problem and presentation of the dataset and solution is extremely clear; the paper is very well-written. \n- The paper presents a clear problem with existing systems and the drawback of using retrieval as an evaluation method. The presented benchmark is offering a \"bare minimum\" sort of evaluation for such systems.\n\nWeaknesses:\n- The fonts in some of the figures is really small. I also strongly suggest putting numbers on the actual bar charts; it's difficult to interpret the results otherwise.\n- The numbers for the experiment in 2.3 (evaluating COCO/Flickr30k on perturbations of captions) should be in the main paper, not the appendix.\n- It is suggested that the reason these models ignore word ordering so much is that they really are trained as keyword identifiers, as required for image retrieval, and there's no reason to learn ordering. However, what happens if you incorporate better priors on the captions? I would imagine that a large language model would place very low probability on most of the perturbed captions (I could be wrong, though), and a VLM that uses features from a general large language model would be able to distinguish the obviously grammatically incorrect examples from the true caption.\n- I was hoping the dataset would be a bit more of a scaled-up Winoground dataset, because Winoground directly tests all four settings (perturbation of relations in text and perturbation of relations in image). However, this evaluation set only seems to test perturbation of relations in text.\n\nQuestions:\n- Why are there so few relations and attribute pairs in ARO, as described in Section 2.1? \n- Practically, how are some of the perturbations done? With operations on top of the parse tree?\n- Does the experiment in Section 4 backprop through both text and image features in CLIP? ",
            "clarity,_quality,_novelty_and_reproducibility": "Quality:\n- Experimental setup is thorough. My main concern would be that this becomes a new benchmark to achieve such that people interpret success on it to imply good compositional understanding. I think it works as a \"bare minimum\" evaluation, but performing well on it doesn't necessarily confirm a model is performing correct compositional reasoning (partially because I imagine using a LLM prior will downweight the grammatically incorrect or semantically implausible perturbations, for example).\n\nClarity:\n- Paper is very clearly written, with a few small comments on readability of figures.\n\nOriginality:\n- Relatively original; the experiments are new as far as I can tell, as is the proposed training method. However, in terms of a dataset, there are many that evaluate compositionality with synthetically generated data (GQA, CLEVR), or could be adapted to evaluate more semantically plausible alternatives for compositionality. Missing citation for the NLVR(2) corpora (Suhr et al. 2017/2019) which also evaluate compositionality with both true and false image pairs.",
            "summary_of_the_review": "This paper introduces a new benchmark and training method for evaluating compositionality of VLMs. The compositionality is mostly evaluated by perturbing captions of existing datasets in several ways. Experiments find that SOTA VLMs perform poorly (i.e., they cannot easily distinguish between perturbed and true captions). The paper is very well written. My main concern is that the kinds of perturbations in the evaluation set may be easy to reject simply by considering priors from an LLM, something which is not evaluated in this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5507/Reviewer_U4Mo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5507/Reviewer_U4Mo"
        ]
    },
    {
        "id": "4fjdfnFYwB",
        "original": null,
        "number": 4,
        "cdate": 1666821120639,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666821120639,
        "tmdate": 1666821120639,
        "tddate": null,
        "forum": "KRLUvxh8uaX",
        "replyto": "KRLUvxh8uaX",
        "invitation": "ICLR.cc/2023/Conference/Paper5507/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper reveals an interesting and important failure pattern of the pre-trained vision-language models (VLMs): they are insensitive to object attributes, relations, and even word orders. They created several tests from Visual Genome, COCO caption, and Flickr30K, and show several representative VLMs (BLIP, CLIP, etc.) are insensitive to:\n\n1. Relation (prepositions and verbs): e.g., differentiate between \u201cthe **man** is behind the **tree**\u2019 vs \u201cthe **tree** is behind the **man**\u201d\n\n2. Attribute: e.g., differentiate between \u201cthe **crouched man** and the **open door**\u201d and \u201cthe **open man** and the **crouched door**\u201d\n\n3. Word order: whether the model can tell if a sentence differentiates a sentence with shuffled words\n\nThe paper goes on discussing what causes the models to be insensitive to such compositional structures in the images and captions. They provide an intuitive hypothesis: the contrastive (retrieval) objective in the pre-training does not encourage the model to learn such compositional structures, because \u201cthe datasets are not designed to contain many images with captions containing similar words that must be differentiated\u201d (or from my understanding, the probability of being able to sampling such \u201chard negatives\u201d is very small). They also provided an experiment backing it up: models do not need order information to do well on current image-caption retrieval benchmarks.\n\nThey further show that if we purposely generate negative captions by swapping the word orders and sample hard negative images using clip similarity, the model does much better on the proposed evaluation benchmarks.",
            "strength_and_weaknesses": "There is no major weakness. I will just note two issues I noticed which do not affect my rating of the paper.\n\n1.  In terms of the three challenges presented in the paper, it seems they can be addressed by explicitly considering how the challenge is formed (e.g., adding shuffled captions during training as hard negatives). However, this does not imply that the problem is solved. This reminds me of what is discussed in Teney et al., 2020, where once we know how the \u201cchallenge\u201d dataset is constructed, we could use such information to train a model that performs well on the challenge dataset; but the model will likely fail on cases not foreseen during dataset creation.\n\n   I wonder what should be the intended use of the benchmark. If simply adding shuffled captions can solve the challenges pretty well, what should be the next move?\n\nTeney et al., On the Value of Out-of-Distribution Testing: An Example of Goodhart's Law.\n\n2. I would have appreciated more discussions on relation to negative mining and contrastive learning. \n\n---\n\nMinor comments:\n\n1. Most tables are in the appendix. While figures send a strong message, I would appreciate having a few tables in the main paper.\n\n2. NegCLIP has two improvements: generating neg captions and sampling hard-neg images. Is there an ablation study on this?\n\n3. What\u2019s the batch size of fine-tuning CLIP?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow.",
            "summary_of_the_review": "Overall, this is a well-presented paper with an easy-to-follow yet important finding: performance on the current VL benchmarks could be misleading (even though these benchmark data contain images and captions of rich compositional structures) and current VLMs are very close to bag-of-word models.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5507/Reviewer_J2a5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5507/Reviewer_J2a5"
        ]
    }
]