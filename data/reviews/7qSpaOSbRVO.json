[
    {
        "id": "Nvg5V5sq6D",
        "original": null,
        "number": 1,
        "cdate": 1666653429300,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653429300,
        "tmdate": 1666822120138,
        "tddate": null,
        "forum": "7qSpaOSbRVO",
        "replyto": "7qSpaOSbRVO",
        "invitation": "ICLR.cc/2023/Conference/Paper1976/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores the poisoning attacks on CLIP. Compared to the previous work that focused on pre-training attacks on the image-modal encoder, this paper focuses on attacking the fine-tuning process and evaluating it using text-image retrieval, a downstream task that shows the attack's effect on both image and text-modal encoder. The attack is also generalized to a class-class poison rather than a target-class poison. The paper analyzes the differences between poisoning effects on both modalities and proposes post- and pre-training defenses against these attacks. ",
            "strength_and_weaknesses": "Strength:\n1. To my knowledge, this is the first paper that tries poisoning attacks on a multimodal model during the fine-tuning phase. The scenario is realistic and easier to expand further for future experiments compared to training CLIP from the scratch. Using Image-Text retrieval is also a novel evaluation task for analyzing multi-modal attacks with more evaluation metrics like hit@k and minrank. \n2. This paper isolates the linguistic and visual modalities and analyzes the attack's effect on them separately, providing an interesting insight that may lead to effective attacks with more focusing goals (lower minrank or higher hit@k rate).\n3. The paper gives simple but effective defenses against the proposed attacks.\n4. The paper considers a more general attack scheme compared to the previous works.\n\nWeakness:\n1. I do not see significant differences between attack 2 and attack 3. both datasets seem to have consistent Hit@K ratios and MinRank values. The only values inconsistent are \"ours-2\" under the Flickr-PASCAL dataset, but I think it can be attributed to the specific dataset selections rather than the attack 3 itself being more harmful. There is an analysis in the ablation study on different combinations of classes during the attacks, but the consistency issue is not analyzed fully during the main experiment part.\n\n2. I also cast a doubt on the dataset used, specifically Flickr-PASCAL. Even though the combined dataset has a relatively large size, only 20 categories from the PASCAL datasets could be selected to be poisoned and that may influence the scope of the attack. Namely, even though the overall poisoning rate is low, the effect may be stronger due to the data imbalance issue. \n\n3. Combing the freezing experiment with the main experimental result, I can have a clear understanding of the linguistic modality being attacked. However, the analysis of which modality is most vulnerable is not satisfactory. From the experiment, each image is matched to more than one caption, rendering an imbalance in encoding representations. As stated in previous work, when there are more diverse caption sets for each image, the model is more likely to change the image encoder compared to the text encoder. Although the statement is unjustified theoretically, It makes sense intuitively and a more considerate experiment concerning this issue or an explanation of why this concern is unnecessary could be very helpful to the overall argument. There is also a lack of explanation and extension on their result with the difference in attacking different modalities. \n\n4. Both the pre-training defense (removing less correlated data pairs) and the post-training defense sound a bit weak. For the pre-training, it is possible to have harder training tasks with larger cosine distances, and setting a threshold is difficult to do and could be harmful to the performances. For the post-training, the paper does not explore the situation when the fine-tuning datasets are of different scopes.\n\n5. Overall, I think there is a lack of analysis and expansion on the experiments discussed",
            "clarity,_quality,_novelty_and_reproducibility": "This work is easy to follow and explains its reasoning in a clear manner. The focus is interesting and the experiments are very thorough. ",
            "summary_of_the_review": "The paper gives many new insights into analyzing attacks on multi-modal models. The training scheme on fine-tuning tasks makes future experiments more manageable and easier to transfer. However, I think there is a lack of explanation for many of the findings. I will raise my rating if there are more explanations on the points I mentioned. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1976/Reviewer_ygKY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1976/Reviewer_ygKY"
        ]
    },
    {
        "id": "zKqKA-CRE5",
        "original": null,
        "number": 2,
        "cdate": 1666661401503,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661401503,
        "tmdate": 1666661427055,
        "tddate": null,
        "forum": "7qSpaOSbRVO",
        "replyto": "7qSpaOSbRVO",
        "invitation": "ICLR.cc/2023/Conference/Paper1976/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the data poisoning task on multimodal encoders. This paper investigates three types of poisoning attacks first. After that, it studies the effectiveness of attacking visual and linguistic features. In addition, it explores two types of defense mechanisms for defending against the attack on multimodal encoders.",
            "strength_and_weaknesses": "Strength:\n1. This paper investigates a new problem about whether attacking the linguistic modality is also effective for the poisoning attack task. \n2. The experiments are extensive and provide a deeper understanding of the vulnerability of multimodal encoders.\n\nWeaknesses: \n1. There is a lack of baselines in this paper. Although the paper proposes three types of attack, the comparison does not include other state-of-the-art methods. As mentioned by the author, there are existing methods in this domain, but they are not introduced in the experiment to show whether the attacking method is not redundant. \n2. There is a lack of discussion on whether it\u2019s worth of attacking linguistic modality. Although it can be as effective as attacking linguistic modality, it is not clear whether it changes a lot of linguistic data compared to visual data. If not, based on the principle of not causing much difference in the data, it does not make sense to change the linguistic data.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. As for its novelty, it is new to see work on the poisoning attack domain to conduct a study on the linguistic modality. As for reproducibility, although the authors provide a reproducibility statement, it is still better to provide codes for other readers to implement the experiments.",
            "summary_of_the_review": "In total, this paper is novel for providing a study on attacking linguistic modality for multimodal encoders. The experiment is comprehensive and provides an easy-to-follow structure for readers. However, this paper lacks an illustration of why linguistic modality matters for poisoning attacking and whether the used attack method is better than the baselines. Thus, the reasonableness for conducting this type of attack needs more explanation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1976/Reviewer_zCXF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1976/Reviewer_zCXF"
        ]
    },
    {
        "id": "TQQTEPrCEp",
        "original": null,
        "number": 3,
        "cdate": 1666699323698,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699323698,
        "tmdate": 1666699758050,
        "tddate": null,
        "forum": "7qSpaOSbRVO",
        "replyto": "7qSpaOSbRVO",
        "invitation": "ICLR.cc/2023/Conference/Paper1976/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper examines data poisoning attacks and defense for joint language-vision model (CLIP) in a retrieval setting. Expanding on Carlini et al. 2022, the authors propose attacks and defenses on both modalities instead of just vision signal and show vulnerability in both.\n\nThe paper proposes three attacks, all try to lead the model to mis-associate the text signal to the image signal. The first attack targets a class of text, swapping out the image from an <image, text> pair for another image of a different class. The second attack generalizes the target of the first into a class of image, and the third attack generalize the second attack into multiple classes of images. All three attacks work well during the fine-tuning process on COCO and Flickr-PASCAL even with very low poisoning rate and with virtually no loss in term of original test data utility. \n\nThe authors then analyze the difference in the effect of data poisoning on the text/image encoders by freezing each and calculate the performance (Hit and Minimum rank) and show that the image and text encoder lead to slightly different forms of poisoned behavior. While the poisoned image encoder generally leads to worse behavior overall (generally higher rank of the poisoned image in retrieval), the poisoned text encoder leads to higher probability of top retrieval results being the poisoned class.\n\nIn the defense, the author proposes two methods, the first is data filtering with distance from the original encoders and the second is retraining with clean dataset. Both method improves the robustness substantially.",
            "strength_and_weaknesses": "Strength:\n+ The paper is one of the first works to explore data security in both modalities of a joint vision-language model\n+ Both the attacks and defense methods are simple but effective, with strong experimental results.\n\nWeakness:\n+ There has been numerous works on adversarial attacks on text models, many of them also about data poisoning, so it has been quite clear that linguistic modality is also vulnerable to data poison attack [1,2]\n+ As Carlini et al. and others have shown in their previous work, CLIP is quite vulnerable to adversarial attacks, and they have also shown that CLIP can be poisoned with a very limited number of poisoned data points. \n+ The attacks and defense method proposed in the paper is different compared to previous work of Carlini, but the novelty is limited.\n\n[1] https://aclanthology.org/2021.naacl-main.13.pdf\n[2] https://aclanthology.org/2021.naacl-main.13.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "To my knowledge, this is one of the first work to explore attack/defense from both modalities in joint vision-language model. However, the attack and defense methods are simple and not entirely novel. Previous works have also shown vulnerability of CLIP and text model in data poisoning attacks, which also reduce the novelty and potential impact of this work.",
            "summary_of_the_review": "My main concern is about the potential impact of the paper. The previous work of Carlini has shown the vulnerability in CLIP model while the attack and defense method is not novel enough.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1976/Reviewer_KeYj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1976/Reviewer_KeYj"
        ]
    }
]