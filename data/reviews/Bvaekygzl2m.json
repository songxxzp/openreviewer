[
    {
        "id": "B-wd5riJwZ6",
        "original": null,
        "number": 1,
        "cdate": 1666148693551,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666148693551,
        "tmdate": 1666148693551,
        "tddate": null,
        "forum": "Bvaekygzl2m",
        "replyto": "Bvaekygzl2m",
        "invitation": "ICLR.cc/2023/Conference/Paper1328/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes adversarial training with a threshold of minimum adversarial loss, i.e., while the training adversarial example cannot reach the minimum loss, expand its attack radius.",
            "strength_and_weaknesses": "Strength:\n1. Well written and easy to follow\n2. Idea is straight forward\n\nWeaknesses:\n1. A little novelty. For me, setting a minimum adversarial loss during adversarial training is not exciting.\n\n2. Weak experiments.   \n(a) There is just a trade-off between adversarial accuracy (under auto attack) and clean accuracy in the experiments, i.e., increase (adaptive) adversarial attack strength during training -> improve test adversarial accuracy and damage test clean accuracy.  \n(b) In the current development of adversarial training, Wide ResNet-34-10 on CIFAR-10/100 is necessary, where TRADES-AWP can get a better performance under auto attack. I wonder the performance of SAAT under these settings. (i.e., the results of Tab. 1 with WRN-34-10 and TRADES-AWP).\n\n3. Experiment-based work without deep analysis.",
            "clarity,_quality,_novelty_and_reproducibility": "The manuscript is well written and clear.",
            "summary_of_the_review": "This is an experiment based work without deep (theoretical) analysis, it only presents weak empirical results with a straight-forward idea, i.e., a trade-off between clean accuracy and adversarial accuracy (under AA) with SAAT. For this reason, I think it is below the bar of ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1328/Reviewer_AoDK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1328/Reviewer_AoDK"
        ]
    },
    {
        "id": "MWZzgkiV9Ww",
        "original": null,
        "number": 2,
        "cdate": 1666626365226,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626365226,
        "tmdate": 1666626365226,
        "tddate": null,
        "forum": "Bvaekygzl2m",
        "replyto": "Bvaekygzl2m",
        "invitation": "ICLR.cc/2023/Conference/Paper1328/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Strength-Adaptive Adversarial Training (SAAT) to improve adversarial training. Specifically, SAAT adopts a dynamic and adaptive strategy to control the adversarial attack budget. Instead of setting a fixed adversarial budget, this paper proposes to enlarge the budget while attacking. The proposed SAAT is evaluated on CIFAR-10 with Resnet-18. Some marginal improvements are observed.",
            "strength_and_weaknesses": "**Strengths:**\n\n1. The proposed idea is very straightforward and clean, although I have some questions regarding the proposed pseudo-codes.\n2. Experimental results show that SAAT can achieve consistent yet marginal improvements over vanilla\nadversarial training and AWP.\n\n**Weaknesses:**\n1. Adopting adaptive attacking budget during adversarial training has been highlighted by many previous\nwork. The proposed SAAT is just a simple early stop (loss value) based on some empirical observations,\nwhich in my personal view is not novel.\n2. In Alg.1, considering that step size $\\alpha$ is the same as perturbation budget step size $\\tau(2/255)$, what is\nthe meaning of enlarging epsilon by $\\tau$ every attacking step? The real adversarial perturbation will be\nbounded by the step size (if ignoring [min, max] clipping). So in my view, the adaptive attack budget is\njust the normal PGD using $\\epsilon = \\epsilon_{max}$. Besides, the break in Line 12 only jumps up of the inner for-loop.\nDoes this mean that attacking will continue after enlarging the epsilon for this adv. example whose loss\nhas already been greater than $\\rho$?\n3. By setting SA-PGD step K = 3, the proposed SAAT might adopt much more attack steps to train\ntheir models. For example, with $\\epsilon_{max}$ = 8/255, $\\tau$ = 2/255, the actual attack steps might be at most\n$\\epsilon/T*K = 12$. When $\\epsilon_{max}$ = 14/255 (common setting in this paper\u2019s experiments), the attack steps\nmight be at most 21. A lot of computational costs will be introduced in SAAT.\n4. The proposed SAAT doesn\u2019t show any improvements under common adversarial evaluation settings\n(eps=8/255) and only marginal improvements can be observed on vanilla adversarial training and AWP\nunder very specific settings. Besides, SAAT hurts clean accuracy very much. Only experiments on\nCIFAR-10 and Resnet-18 are provided. The authors should conduct experiments on other architectures\nwith larger capacities and benchmarks (CIFAR-100).",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and clear. The proposed method is very naive. This paper sounds technical and\ncan be reproduced.",
            "summary_of_the_review": "The proposed method is straightforward, clean, and naive. Performances are my biggest concern. SAAT\ndoesn\u2019t show any improvements under common adversarial evaluation settings but hurts a lot of baseline\nperformances.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1328/Reviewer_snup"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1328/Reviewer_snup"
        ]
    },
    {
        "id": "K9JeEWlxiq",
        "original": null,
        "number": 3,
        "cdate": 1666779145063,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666779145063,
        "tmdate": 1666779145063,
        "tddate": null,
        "forum": "Bvaekygzl2m",
        "replyto": "Bvaekygzl2m",
        "invitation": "ICLR.cc/2023/Conference/Paper1328/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper works on adversarial training, a well-known defense for adversarial samples. It aims to address the problem of robustness over-fitting, which refers to the phenomenon that training with a fixed budget degenerates model performance. The authors propose a method called Strength-Adaptive Adversarial Training, which can update the strength of the attacks during the training process. \n",
            "strength_and_weaknesses": "Strength\n1. The algorithm is fit for networks of various model capacities.\n2. Figure 2 (d) shows robustness differences between the \"best\" and \"last\" checkpoints, which is interesting.\n\nWeakness\n1. About motivation: I question the novelty of your motivation that using a dynamic perturbation budget can alleviate robustness over-fitting. In fact, Cai et al. [1] adjust the iteration numbers of PGD to control the adversarial strength. Similarly, Kim et al. [2] directly manipulate the step size of FGSM to overcome the question. At least, you should cite these works in the introduction part of your paper.\n\n2. About methods: Your work introduces a new parameter $\\rho$ and uses a heuristic method to adjust it. However, why not apply the same heuristic method to $\\epsilon$ directly? So you can adjust the adversarial budget during training. I hope you can do some additional experiments to demonstrate the advantage of introducing $\\rho$.\n\n3. About methods: You claim that your method can cope with different model capacities. However, I do not see such an experiment in your paper. Could you train some models of different architectures adversarially with the same $\\rho$?\n\n4. About Experiments: Your paper bears a lack of baselines. Please add some evaluation on related works (like [1,2]) to Table 1. Note that you should evaluate these methods under the same budget. \n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. Please increase the size of your images. \n2. Some other questions: Researchers set the perturbation budget to avoid the adversarial samples being recognized by people. I find your method loosens the constraints of the perturbations in Table 1. Can you visualize some perturbations obtained? \n\n[1] Cai, Qi-Zhi, et al. \"Curriculum adversarial training.\" arXiv preprint arXiv:1805.04807 (2018).\n[2] Kim, Hoki, Woojin Lee, and Jaewook Lee. \"Understanding catastrophic overfitting in single-step adversarial training.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 9. 2021.",
            "summary_of_the_review": "Although the paper has some interesting findings, it bears some severe weaknesses, like a lack of novelty, ignoring related work, and few baselines.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1328/Reviewer_e7uD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1328/Reviewer_e7uD"
        ]
    },
    {
        "id": "FRdGsUuHMag",
        "original": null,
        "number": 4,
        "cdate": 1667443084412,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667443084412,
        "tmdate": 1667443084412,
        "tddate": null,
        "forum": "Bvaekygzl2m",
        "replyto": "Bvaekygzl2m",
        "invitation": "ICLR.cc/2023/Conference/Paper1328/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper discusses the issue of adversarial training over-fitting, pointing out that a pre-specified perturbation budget is not optimal as the training progresses the perturbation budget should be adjusted accordingly. Based on this intuition, the author proposes a new adversarial training method that generates adversarial examples by maintaining a minimum adversarial loss instead of searching for the point that maximizes the loss. Experiments on CIFAR10 show that the proposed method outperforms standard adversarial training and when combined with AWP, it also outperforms standard adversarial training with AWP.",
            "strength_and_weaknesses": "Strength:\n\n- The paper is clearly written with intuition and finding discussed in detail. \n\n- Experiments show that the proposed method outperforms standard adversarial training on CIFAR10\n\nWeakness:\n\n- The proposed method lacks theoretical and empirical support. Even though, some experiments are done on CIFAR10, but one dataset is not enough to show that the method works well in general. Besides, there have been many new adversarial training methods proposed since 2018 (the year standard adversarial training was proposed). The author also discusses some in the paper, but why not compare with those methods in the experiments? There have also been works about adaptive adversarial training. Lack of experiments is the key issue of the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\n- The paper is clearly written and easy to follow.\n\nQuality:\n\n- The paper is well written but to support the claim more experiments need to be done.\n\nNovelty:\n\n- The idea is novel and interesting. As far as I know, no one has proposed similar adaptive ideas, but there are some works about adaptive epsilon in adversarial training. \n\nReproducibility:\n\n- Though some implementation details are mentioned in the paper, code is not provided.",
            "summary_of_the_review": "Due to the lack of experiments support, the paper is below the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1328/Reviewer_HbG6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1328/Reviewer_HbG6"
        ]
    }
]