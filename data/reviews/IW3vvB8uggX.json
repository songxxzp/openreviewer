[
    {
        "id": "PkDsL4PqX5",
        "original": null,
        "number": 1,
        "cdate": 1666482577781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666482577781,
        "tmdate": 1669198981446,
        "tddate": null,
        "forum": "IW3vvB8uggX",
        "replyto": "IW3vvB8uggX",
        "invitation": "ICLR.cc/2023/Conference/Paper4878/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes ROLLIN, and algorithm to reduce the complexity of learning an optimal policy in a sequence of contextual MDPs, i.e. a set of MDPs sharing everything but the reward function in this work. The main idea of the paper is that if the two contexts are close enough, we can modify the initial state distribution in a way to reduce the total computational complexity of a policy gradient method.\n",
            "strength_and_weaknesses": "\n### Strengths\n\n* The paper proposes a neat way to reduce sample efficiency in the setting providing the theoretical proof (I did not check all the derivations in the appendix).\n\n### Weaknesses\n\n* The paper chooses a non-standard way of evaluating the algorithm, i.e., providing the learning progress value instead of the policy returns. As far as I understand, we have an ordered set of MDPs, and we are eventually interested in solving the target one. I believe, the chosen evaluation protocol favours the proposed method, and I would like to see the comparison: curriculum vs baseline on the target task only.\n* The proposed algorithm requires knowing the optimal parameters for the first MDP in the set. If I get it correctly, if we remove this quite limiting assumption, we will get the exponential complexity again, though it will still be reduced by the consequent application of ROLLIN.\n* The paper misses important connection to the related work, which is more empirical, but very similar in spirit:\n    * REvolveR: Continuous Evolutionary Models for Robot-to-robot Policy Transfer, ICML 2022\n    * Curriculum Learning with a Progression Function, Bassich et al.\nIn addition, I think the paper misleads the reader by its title. Usually, by multitask RL, we mean a single model being able to solve multiple tasks at once, whereas this paper looks at the linear curriculum with a single target task.\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nThe paper is mostly clearly written, but it assumes knowing a lot of context from the reader. Some of the concepts used in the introduction\n get defined only later in the 'Preliminaries' section. \n\n### Quality\n\nI am not a theoretical RL researcher, but the theoretical part looks of high quality to me. My concerns about the assumption on knowing the optimal policy for the first MDP are outlined in the 'weaknesses' section. As for the empirical part, I am concerned with the reported metric, as I also mentioned in the 'weaknesses'\n\n### Novelty\n\nThe theoretical part is novel to the best of my knowledge. Empirical part has some missing related works as I pointed out in the 'weaknesses'.\n\n### Reproducibility\n\nThe appendix contains detailed pseudocode and the hyper parameters to reproduce the experiments.\n",
            "summary_of_the_review": "My current recommendation 'below acceptance' is mostly caused by the empirical evaluation protocol weaknesses. I am ready to increase the score if my concerns from the 'weaknesses' are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4878/Reviewer_haUz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4878/Reviewer_haUz"
        ]
    },
    {
        "id": "zzdOWS5LxF",
        "original": null,
        "number": 2,
        "cdate": 1666725583529,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725583529,
        "tmdate": 1666725583529,
        "tddate": null,
        "forum": "IW3vvB8uggX",
        "replyto": "IW3vvB8uggX",
        "invitation": "ICLR.cc/2023/Conference/Paper4878/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a theoretically-motivated framework that reformulates a single-task RL problem as a multi-task RL problem defined by a curriculum for computationally-efficient policy learning. Their framework and other baseline are demonstrated in a goal-reaching task with a Mujoco dynamical system. The results show improved learning performance compared to traditional methods. ",
            "strength_and_weaknesses": "Strengths:\n- A novel approach for efficient RL policy learning.\n- Algothrim seems sound with theoretical backing.\n- Experiments demonstrate that the proposed approach leads to better performance. \n\nWeaknesses\n- The experiment section is relatively weak. It is unclear how the proposed approach will scale to other Mujoco robots, such as Swimmer and Humanoid. \n- Is the idea of a warm start to policy learning also applicable across different environments? For instance, using parameters from u-maze to some other maze environment could aid in investigating that.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presented approach is novel, with results demonstrating improved performance over prior methods.\n",
            "summary_of_the_review": "Overall, the presented approach is novel with theoretical support. Experiments demonstrate better performance than the prior methods. However, test cases with different dynamical systems and investigation on the applicability of the proposed approach in speeding up learning across different environments can further strengthen the paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4878/Reviewer_cM8j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4878/Reviewer_cM8j"
        ]
    },
    {
        "id": "7jzicDBx7AF",
        "original": null,
        "number": 3,
        "cdate": 1667372755383,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667372755383,
        "tmdate": 1667372755383,
        "tddate": null,
        "forum": "IW3vvB8uggX",
        "replyto": "IW3vvB8uggX",
        "invitation": "ICLR.cc/2023/Conference/Paper4878/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Under the assumptions of 1) The reward function is lipschitz continuous w.r.t the context and 2) maximum distance between two contexts, this paper proposes an algorithm \"Rollin\" to learn an optimal policy for the final context and show that it can be achieved in polynomial iteration complexity instead of exponential. The empirical results were shown on a toy domain.",
            "strength_and_weaknesses": "Strengths:\nThe paper provides novel polynomial time iteration complexity and sample complexity bounds for learning a single-task given a curriculum of contexts. \n\nWeaknesses:\n1) Throughout the paper, the authors carry the tone of learning a single-task policy by recasting it as a multi-task problem. (e.g, \"In this work, we provide a theoretical framework that reformulates a single-task RL problem as a multi-task RL problem defined by a curriculum..\" in the abstract) But, the proposed algorithm \"Rollin\" assumes that the curriculum (contexts) are given as input. \n2) The assumption of having a curriculum that satisfies the assumptions 3.1 and 3.2 is a very strong assumption. In most algorithms involving goal-conditioned RL or curriculum learning, such a curriculum is not provided. \n3) They also assume that the optimal policy for the initial context is known.\n4) The empirical results are shown on a very simple toy domain.\n5) There are multiple grammatical errors / typos throughout the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "As listed in the weaknesses, this paper makes very strong assumptions that makes the proposed algorithm impractical to use. The authors should come up with an algorithm to automatically generate the curriculum of contexts. In its current form, I recommend the paper as a weak reject. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4878/Reviewer_5mDQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4878/Reviewer_5mDQ"
        ]
    }
]