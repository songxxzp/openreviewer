[
    {
        "id": "IrtSSax3Pj",
        "original": null,
        "number": 1,
        "cdate": 1666353065730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666353065730,
        "tmdate": 1668674639096,
        "tddate": null,
        "forum": "3VKiaagxw1S",
        "replyto": "3VKiaagxw1S",
        "invitation": "ICLR.cc/2023/Conference/Paper1716/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "  \n        This paper analyzes gradient boosting ensembles and shows that they can be understood as a kernel method that is indeed finding the solution to an optimization problem that converges to the posterior mean of a Gaussian process. Using the technique known as sample-first-then-optimize the considered method can be used to generate samples from the posterior distribution of a Gaussian process. This means that output uncertainty can be readily obtained and used, for example for out of distribution detection. The proposed method is evaluated on several datasets from the UCI repository.\n",
            "strength_and_weaknesses": "Strengths:\n\n        - Nice theoretical results showing that gradient boosting can be understood as a kernel method.\n\nWeaknesses:\n\n        - The main result is a bit obvious since gradient boosting is minimizing objectives in functional space.\n\n        - The fact that KGB performs better questions why not using a Gaussian process as the learning method.\n\n        - The experimental section is weak. No error bars are given and therefore it is not possible to assess the significance of the results.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "        The paper is overall difficult to follow. It is written in an obfuscated way. For the reader non-familiar with Gaussian process nor Gradient boosting it is completely unreadable.\n",
            "summary_of_the_review": "   I believe this is a nice paper showing a nice theoretical results. However, it is impaired by the questionable applications. In particular, I wonder why not using directly a Gaussian process if KGB is precisely approximating that. The experimental section is also weak and does not allow to check if the method proposed is significant. In particular, no error bars are given.\n\nMinor:\n\nEquations are not numbered and cannot be referred. There is a typo in the Eq. for the posterior variance of the GP.\n\nN (f (x), \u03c3 2 (x) is missing ).\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1716/Reviewer_1ARD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1716/Reviewer_1ARD"
        ]
    },
    {
        "id": "iacnM6uvoWH",
        "original": null,
        "number": 2,
        "cdate": 1666669419124,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669419124,
        "tmdate": 1666669419124,
        "tddate": null,
        "forum": "3VKiaagxw1S",
        "replyto": "3VKiaagxw1S",
        "invitation": "ICLR.cc/2023/Conference/Paper1716/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "A method for posterior inference with a variant of GBDT is proposed. The weak learner used in this paper is a variant of the standard decision tree, where decision rules are oblivious (same split used at all nodes of the tree at the same level), the splitting procedure is randomized by use of the max-Gumbel trick to sample the decision rule. Shrinkage is applied throughout the training process as a regularizer.\n\nThe proposed inference technique is a form of sample-then-optimize, wherein a posterior sample is acquired by first sampling from a prior on the function space, then applying GBDT to the residuals of the sample from the prior and the training data.\n\nThe paper provides theoretical justification for their method, including finite-sample convergence of the gradient boosting algorithm, and convergence of the sampler to the posterior distribution.\n",
            "strength_and_weaknesses": "Strengths\nThe paper presents a compelling connection between gradient boosting and Gaussian Process inference.  It presents a tree-based kernel, which is used to define a RKHS and the definition of the Gaussian process.  The paper presents experimental results that show impressive performance both for prediction and uncertainty quantification on a range of datasets. The paper is addressing an important and unsolved problem in a principled way.\n\nWeaknesses\nThe paper would benefit from editing and condensing to make it clearer and to provide more intuition to the reader. The mathematical exposition is provided without much explanation or connection to empirical results, other than ensuring convergence. For example, the authors show some empirical improvement on bagged SGB and SGLB, but it\u2019s not clear where the relative weakness of those methods stem from.\n\nThe experiments could be further strengthened in a way that both bolsters the case for the method, and improves the paper\u2019s arguments.  For example, the authors do not compare to GP with standard kernels. This would be interesting to see, given the close connection to these methods. What is the dimensionality of these problems?  Does the method work well in high-dimensional problems, especially with sparsity?\n",
            "clarity,_quality,_novelty_and_reproducibility": "I would suggest the paper needs some editing to improve its readability, to communicate the main ideas more intuitively, without excessive mathematical baggage, and to clarify the particular theoretical contribution.\n\nThe relationship to other work could be expanded in the main text. The \u2018related work\u2019 section in the appendix does mention a few other kernel interpretations of tree-based methods, but it is fairly cursory.\n\nThe mathematical detail does not always differentiate between novel theory and restatements of standard results in GP learning / kernel methods.  It is unclear why symmetric trees are used. Are they essential ingredients of this procedure, or could other tree-building procedures work well, but just less readily admit mathematical analysis?  The definition of the RKHS inner product is not the standard one, is not cited, is highly technical, and its relevance is unclear. What are the key novel technical developments in the proof, for those of us not intimately familiar with this branch of the literature?  How can we understand the different terms in the convergence theorems?\n\nThere are a few places where unclear notation is used. For example, in section 2.3, the term $\\min_{f\\in H}$ appears without explanation in defining the loss function.\n\nFinally, the title is a bit misleading. Standard gradient boosting does not perform Gaussian process inference, but can be adapted to do so.\n",
            "summary_of_the_review": "The connection between boosting and GP inference is a compelling unification of different approaches to uncertainty quantification.  The paper is theoretically grounded and shows strong empirical results. The presentation could be made clearer, with more explanation of the significance of the theory, and intuition behind the performance of the method. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1716/Reviewer_HjFB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1716/Reviewer_HjFB"
        ]
    },
    {
        "id": "brHOY4RyVbW",
        "original": null,
        "number": 3,
        "cdate": 1667254904280,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667254904280,
        "tmdate": 1667254904280,
        "tddate": null,
        "forum": "3VKiaagxw1S",
        "replyto": "3VKiaagxw1S",
        "invitation": "ICLR.cc/2023/Conference/Paper1716/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is a nice theoretical one which shows an equivalence between two seemingly unrelated ML methods: gradient boosting decision trees on hand, and kernel methods/Gaussian processes on the other. On top of this equivalence, it is shown that model uncertainty can be obtained according to the GP variance.",
            "strength_and_weaknesses": "Strengths: the paper strikes a is a nice balance between theory and practical relevance of the proposed method. In particular, there are nice non-asymptotic results in section 4.1.\nExperiments show interesting results in posterior variance estimation and OOD detection.\n\nWeaknesses: I find it hard (even for a Bayesian) to fully get the methods proposed in section 4, esp. regarding Bayesian terms as prior, posterior, on Monte Carlo sampling. \n-Providing transitions between subsections would help. The discussion after corollary 4.2 is difficult to read, probably because it is essentially made of equations/complexities. What do you conclude from it?\n- Usually, Bayesian have a handle on their priors, eg with hyper parameters. Is it the case here? I find this subsection difficult to get.\n- SOme diagram to relate the different quantities from the prior/posterior/asymptotic posterior would be helpful.",
            "clarity,_quality,_novelty_and_reproducibility": "I found the text overall clear and well-written. Legend of Fig 1 should be changed (there\u2019s something wrong with the colors).\nAs an emergency reviewer, I couldn\u2019t check the related literature, and cannot evaluate precisely the novelty wrt the previous works referred to as SGB and SGLB.\nI did not check the github repository.",
            "summary_of_the_review": "In summary, I find that the paper is making well-grounded contributions which I think should be of interest for the ML community working on uncertainty quantification.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1716/Reviewer_iZU9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1716/Reviewer_iZU9"
        ]
    }
]