[
    {
        "id": "gpWWfuE8T05",
        "original": null,
        "number": 1,
        "cdate": 1666575098419,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575098419,
        "tmdate": 1666577068231,
        "tddate": null,
        "forum": "YnkGMIh0gvX",
        "replyto": "YnkGMIh0gvX",
        "invitation": "ICLR.cc/2023/Conference/Paper2762/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "With the advancements of machine learning and boosting availability of compatible hardware as well as cloud services, machine learning-based classification systems have been deployed in numerous scenarios. Regardless of the specific application \u2014 some more critical than others \u2014 it is essential to detect the failure cases in a timely manner in order to quickly iterate on targeted improvements.\n\nThe authors pointed out that currently the dominant means for such failure detection is through checking some type of confidence scores, either directly produced by the model as an output, or indirectly calculated through some other heuristics (e.g., recall the notion of uncertainty in Focal Loss). The authors claimed that these different approaches are aiming to solve the same failure detection problem yet through distinctive methods which led to diverging protocols of various subfields (misclassification detection, out-of-distribution detection, selective classification, predictive uncertainty quantification). The authors then systematically analyzed the existing methods to reveal their pitfalls, and proposed a holistic and realistic approach for failure detection.\n\nThe proposed metric is the Area under the Risk-Coverage Curve (AURC), which is proposed in Geifman et al. This metric assess the rate of silent failures of a classifier across different failure filtering thresholds. The authors claimed that the metric avoids all pitfalls they revealed, and recommended wide adoption across all subfields of research within classification failure detection. The authors further delineated the modification of protocols needed to adapt to the new evaluation approach from these aforementioned subfields.",
            "strength_and_weaknesses": "Strengths\n1. I strongly encourage the intention stated by the authors: to articulate a call to various interrelated yet isolated communities to acknowledge the shortcomings of current practices and adapt to a more reasonable practice that not only better fit their alleged purpose but also help bring the isolated fields together.\n2. Section 2 is a deliberate and eye-catching section. The authors discussed three pitfalls of the existing methods and derived the corresponding point-by-point requirements for a desired evaluation protocol for failure detection. Among them, I particularly like (1) the second pitfall where the authors drew out attention towards different sources of classification failure and pointed out that existing methods often cover a subset of them (also illustrated in Figure 1). (2) the third pitfall where most relevant research claim failure detection as their purpose but in fact are evaluated on outlier detection, causing problems due to the mismatch (also illustrated in Figure 2).\n3. I also like that the authors pointed out an easily neglected issue: rounding errors during softmax operation may largely affect the resulting evaluation metrics.\n\nWeaknesses\n1. Personally I believe reorganizing section 4.2 would be beneficial. For example, consider organizing the CSFs to be compared by their subfields (MisD, SC, PUQ, OoD-D) if possible?\n2. Table 1 is a bit difficult for me to quickly process \u2014 I understand what is displayed but my brain is not well trained to instantly make sense of this representation. Is this just me or is it a common inconvenience? If it\u2019s that latter, I would suggest adding a column for each dataset showing the \u201csum of rank order for the current method across all evaluations in this dataset\u201d.\n\nMinor Things\n1. Typo? Section 2, Pitfall 2, last sentence. \u201c\u2026it is not realistic to exclusively assume classification failures from label-altering shifts **an** no failures caused by label-preserving shifts.\u201d\n2. Typo? Section 4.3, Under heading \u201cDifferent types of uncertainty are empirically not distinguishable.\u201c 3rd line. \u201c\u2026we are interested in the **extend** to which such relations can be confirmed\u2026\u201d\n3. Is there any reason $y_{f,i}$ in Eq. 24 is highlighted?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is of high clarity and quality. Although it does not propose a new approach/technique/method, I still believe it has considerable novelty.",
            "summary_of_the_review": "This paper encouraged researchers to view multiple subfields (misclassification detection, out-of-distribution detection, selective classification, predictive uncertainty quantification) under the umbrella of classification failure detection through a unified perspective, and proposed improvements to the problem statements and walked through the design of a generic evaluation protocol. More remarkably, the authors showed that a simple baseline can match or beat the state-of-the-arts in these subfields on the unified benchmark, since these SOTA techniques only perform in their selective subfield and miss the generalizability in the bigger picture. I like the scope, vision and insight from the authors, and would recommend for acceptance of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2762/Reviewer_7dx4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2762/Reviewer_7dx4"
        ]
    },
    {
        "id": "Kep6kesNts",
        "original": null,
        "number": 2,
        "cdate": 1666648392669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648392669,
        "tmdate": 1666648392669,
        "tddate": null,
        "forum": "YnkGMIh0gvX",
        "replyto": "YnkGMIh0gvX",
        "invitation": "ICLR.cc/2023/Conference/Paper2762/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper offers a set of criteria (and a metric area under risk coverage curve) to compare different flavors of failure detection. It gives experiments  that compare different methods according to these criteria.",
            "strength_and_weaknesses": "(strengths)\n\nThe paper evidently springs from a substantial understanding of and experience with the topic.\n\nThe proposed criteria are thoughtful and offer, agree or disagree, a good point of departure for discussion.\n\nThe \"step back and look in the mirror\" approach is welcome.\n\nThe findings section is super interesting.\n\n(weaknesses)\n\nSometimes the paper covers ground too fast for me (too condensed).",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the work is high quality and well informed. I do not know the literature. \n\nThe perspective and approach are thoughtful.\n\nThe authors indicate full reproducibility via open codebases.",
            "summary_of_the_review": "The proposed criteria are thoughtful and offer, agree or disagree, a good point of departure for discussion.\n\nThe \"step back and look in the mirror\" approach is welcome.\n\nThe paper combines many methods that are usually silo'ed.\n\nThe findings section is super interesting.\n\nThe Conclusions paragraph is wonderful.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2762/Reviewer_JKia"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2762/Reviewer_JKia"
        ]
    },
    {
        "id": "ABazAe-mHzO",
        "original": null,
        "number": 3,
        "cdate": 1666673382521,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673382521,
        "tmdate": 1668806441855,
        "tddate": null,
        "forum": "YnkGMIh0gvX",
        "replyto": "YnkGMIh0gvX",
        "invitation": "ICLR.cc/2023/Conference/Paper2762/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper summarizes various types of classification failures and confidence scoring metrics and identifies their shortcomings with respect to the softmax baseline. Extensive experiments on public image benchmarks (CIFAR-10, CIFAR-100, SVHN, CAMELYON-17-Wilds, iWildCam-2020- Wilds, BREEDS-ENTITY-13) indicate that confidence scoring techniques are sensitive to the dataset choice, integer precision, and calibration.\n",
            "strength_and_weaknesses": "The key strength of the paper is the thorough discussion of evaluation of model classification failures, which is an important and often overlooked part of ML research. The key conclusion is concisely stated in the conclusion: a need for greater introspection and possibly less novelty. \n\nThe key weakness is that the paper is difficult to follow. It is not clear how the pitfalls discussed in Section 2 are evaluated or studied in the remainder of the paper. A number of relevant method definitions are found in the appendix, rather than the main paper. In addition, definitions of techniques is often found after the fact. The paper would benefit from greater organization and extraction of key findings and results that support these conclusions. In Table 2, a number of results are reported, however, it is difficult to parse the metrics, color code, and main results from the paper. One of key findings is that MSR outperforms other techniques, but (a) this is not obvious from the results as its not clear what trends to look for in the metrics (a simple up/down arrow would help) (b) no analysis and discussion of other techniques is provided.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper offers a number of valuable insights into failure analysis, confidence scoring and evaluation practices of classifiers. An extensive evaluation  is generated over a number of public benchmarks. I believe the paper would be very valuable to the research community because it brings up overlooked points in evaluation protocols. A detailed code repository is included. \n\nHowever, the manuscript is relatively difficult to follow and understand. \n",
            "summary_of_the_review": "A useful paper offering insightful, novel analysis, but very difficult to follow.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2762/Reviewer_poSv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2762/Reviewer_poSv"
        ]
    }
]