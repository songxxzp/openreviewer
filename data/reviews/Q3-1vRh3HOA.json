[
    {
        "id": "ZScK7az7OJw",
        "original": null,
        "number": 1,
        "cdate": 1666777539283,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666777539283,
        "tmdate": 1666777539283,
        "tddate": null,
        "forum": "Q3-1vRh3HOA",
        "replyto": "Q3-1vRh3HOA",
        "invitation": "ICLR.cc/2023/Conference/Paper268/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Dilated or \"\u00e0 trous\" convolutions allow increasing the receptive field of convolutional networks by orders of magnitude, at a constant time and space complexity. While a standard approach is to define the dilation rate as an hyperparameter, to define a regularly spaced grid, authors propose and adaptive alternative which, given a budget of kernel positions, learns where to place these positions on the grid.\n\nThey show improved or identical performance on computer vision tasks (classification, segmentation, detection), improved at constant parameter number and identical with fewer parameters.",
            "strength_and_weaknesses": "Strengths:\n* the dilation rate of dilated convolution layers requires a careful cross-validation, with sometimes very little intuition to guide the exploration, which becomes prohibitively expensive in networks that involve many dilated layers as they interact globally. Having 1) a way to replace cross-validation by backpropagation 2) handle irregular grids are very desirable features for such layers, to the potential impact is large.\n* the method of authors is a drop-in replacement to standard convolutions and comes at a light increased computational cost, which makes it useful in practice and not just in an exploratory setting.\n* the method is conceptually simple (reparametrization + bilinear interpolation) which is a clear plus\n* section 3 is a great idea, it's rare to see a paper detail as much the various explorations of authors, including unsuccessful ones.\n\nWeaknesses:\n* the main weakness of the paper is that it completely ignores the groundbreaking advances that dilated convolutions brought to audio, where the scale of dependencies to handle is much higher than in images and where dilation rates are typically increase in an exponential fashion along the network to go up to receptive fields of 10^3 to 10^6 values. The most famous success of this approach is [Wavenet](https://arxiv.org/abs/1609.03499) which inspired many works in tasks such as [speech separation](https://arxiv.org/abs/1809.07454) or [audio coding](https://arxiv.org/abs/2107.03312). While I understand that extensive experiments in computer vision are already costly in time and resources, it is unfortunate that authors did not consider (or even mention) the huge opportunity that their method could represent for audio understanding and generation.\n* the second big weakness is the model section is very austere and does not guide the reader at all. Not only notations are not detailed (one needs to go to appendix to get the definition of p1 and p2) but the reader is left with a matrix definition and two algorithms without further explanation or high-level description of the framework. I strongly recommend refactoring this section to make it more readable, as a high-level of the concepts, and pseudo-code if necessary belong more in the main text (in my opinion) than the exact backpropagation algorithm which takes half a page.\n* as far as I understand, the authors use a standard convolution algorithm regardless of the level of sparsity of their kernel, which explains why they maintain a high computational cost that does not correlated with the reduced number of parameters. This is unlike standard dilated convolutions that exploit their regular grid to reshape inputs before applying the standard convolution with a kernel of size `k` rather than a kernel of size `s1*s2` (cf. this [tensorflow function](https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd)). It would be nice if authors discussed this issue.\n* There is a lack of discussion. Do the authors consider the problem of learning the spacings now solved? If not, what would they like to improve in their method and how?\n\nQuestions:\n* I assume the FLOPs reported for DCLS are at inference. It would be interesting to know how training time is affected by the forward and backward passes through the DCLS layers.\n* As explained by authors, dilated convolutions allow increasing the receptive field with a limited parameter budget and computational cost. Recent approaches such as [CKConv](https://arxiv.org/abs/2102.02611) parametrize kernels of the size of the input with a small neural network, and compute this convolution in the Fourier domain. Such an approach offers both the limited parameter budget and computational cost without requiring dilation, and I would be interested to read (in the related work or as an answer) the author's view of such approaches.\n\nTypos:\n* \"FlOPs\" in Table 5 should be \"FLOPs\"",
            "clarity,_quality,_novelty_and_reproducibility": "*clarity: as explained above, while the motivation and usefulness of such a method are straightforward, the model section could be significantly improved.\n* novelty: I found at least [one reference](https://dl.acm.org/doi/abs/10.1016/j.patcog.2021.108369) not mentioned in the related work and that tries to address the same problem, so it may be that authors forgot to mention close work.\n* reproducibility: the code provides direct alternatives to standard convolutional layers and can thus be used very easily.",
            "summary_of_the_review": "Anyone who ever worked with dilated convolutions probably thought at some point \"what if I could learn this pattern instead of cross-validating it?\", and dilated convolutions are a fundamental building block of so many architectures that the potential impact of such a method is very significant. The main weakness of the paper is that they completely ignore the audio domain which is probably the field where dilated convolutions had the biggest impact. While this would be acceptable in computer vision conference, it's unfortunate in a machine learning conference to not consider such a field that would have been the perfect testbed for the author's method. In particular, the experimental setting chosen by the authors remains limited to very small kernels, which limits the understanding of the method's potential in more general settings.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper268/Reviewer_2fMY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper268/Reviewer_2fMY"
        ]
    },
    {
        "id": "QKHwsWcO0F",
        "original": null,
        "number": 2,
        "cdate": 1666829610583,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666829610583,
        "tmdate": 1670009694478,
        "tddate": null,
        "forum": "Q3-1vRh3HOA",
        "replyto": "Q3-1vRh3HOA",
        "invitation": "ICLR.cc/2023/Conference/Paper268/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an advanced version of dilated convolution. In particular, this would mean learning the exact (float) spacing of the convolution locations in addition to the weights, instead of treating the former as a hyper-parameter. Learning of this spacing was enabled through bilinear interpolation as well as deriving the backpropagation rules. Empirically, the paper found that this additional degree of freedom can improve image recognition performance as well as downstream performance, even on very competitive CNN architectures such as ConvNeXt. The authors also provided important techniques that are needed to train this model successfully and analyses such as the empirical receptive field size.",
            "strength_and_weaknesses": "Strength:\n- The paper shows that the convolution coordinates in dilated convolution can be more flexible than a fixed, grid structure. In this sense, this work showcases the middle point between dilated convolution and deformable convolution (the latter has the added flexibility being input-dependent). \n- Though this paper is framed as an extension to dilated convolution, in my opinion it really answers the broader question, which is whether or not the coordinates in convolution can be learned instead of being pre-determined. In this sense, it brings the concept of neural architecture search to a very elementary neuron level (i.e. convolution coordinates).\n- The performance improvement is solid, considering that DCLS is building on competitive architectures (e.g. ConvNeXt) on competitive datasets / tasks (e.g. ImageNet classification). \n- The majority of the paper is well-written and easy to follow, minus a few places which I will mention in the weaknesses below.\n\nWeaknesses:\nQ1 - The clarity of Section 2 can be greatly improved. $p^1$ and $p^2$ are not clearly defined as the x, y float location among $m$ convolution coordinates, and it is up to the reader to connect the dots. Also it wasn't until Section 3 when \"three additional learnable parameters\" is explicitly mentioned. Explicitly mentioning towards the beginning of Section 2 that \"our extension entails that we learn the float coordinates ($p^1$, $p^2$ in the 2D case) for convolution input in addition to the true weight\" would make things a lot easier to understand. Finally, the explanation here can be done in conjunction with Figure 2. Maybe add a big (e) panel which zooms in on four adjacent pixels and draw what $p^1$, $p^2$, $r^1$, $r^2$ mean.\nQ2 - I understand that the length of Section 2 is limited. But I think one important question that needs to be answered in the main text is a clear intuition and explanation why the bilinear interpolation formulation unlocks the learning of the coordinates. How does the derivation change / where does the derivation break,  when $p$ must be integers or if $K$ only considers the closest element instead of the four neighbors? Empirically, will 'nearest' perform much worse than 'bilinear'? The conclusion section writes \"circumvented by interpolation\" but I did not find its justification in the main text.\nQ3 - Why set a limit to the dilated kernel size? Ignoring all model capacity related concerns, does removing this limit always result in better performance?\nQ4 - Deformable convolution was discussed in Section 5, which is very good because it is indeed very related. However, I think one angle the authors did not explicitly mention is the fact that deformable convolutions are *input-dependent*, and dilated convolution / DCLS are not. As such, the deformable convolution paper was able to show visualization where the kernel looks at different (but key) areas when presented with different inputs. Though DCLS also tries to be flexible in its convolution coordinates, I feel the intuition here may be less clear. I wonder if the authors have any comment on this, or if can show visualization of 10 randomly selected learned convolution input patterns?\nQ5 - I appreciate the authors describing the important techniques needed to train DCLS. It is also mentioned in Section 6 that \"in particular sharing the positions within stages was key\". However, I do not see ablation study results / tables that support these claims. Having some of them, or at least the key ones, will strengthen the paper. \nQ6 - Appendix 10, the text writes \"the ERF of ConvNext-T-dcls is larger\", but the ERF in Figure 9 seems smaller than that in Figure 11. Were the captions flipped?\n\nNit:\n- Add space after \"impractical.\" on page 7\n- Cai & Vasconcelos (2018) should be \\citep towards the end of page 7",
            "clarity,_quality,_novelty_and_reproducibility": "I think the majority of the paper has very good clarity.\n\nPending my questions, I would place the quality and novelty pretty high, as I think it sits right in between several different important topics.\n\nThe reproducibility should also be high, as the code was provided. ",
            "summary_of_the_review": "Overall I feel positive about the paper at this stage. The problem is an interesting one; not addressed in an overly-complicated manner; and the experimental advantage is solid. But I also feel very strongly about my questions. I feel having concrete answers to those would make the paper even stronger. \n\n=======================================\n\nPOST-REBUTTAL UPDATE\n\nI have read the authors' rebuttal, which I think addresses my questions reasonably well. I also appreciate the authors' efforts in actively improving the paper throughout the process. I have increased my score which shows up as an 8, but that is because there is no 7 to choose between 6 and 8. I would really give this paper a \"7 - Good paper, accept\" rating, and personally vote for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper268/Reviewer_N17u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper268/Reviewer_N17u"
        ]
    },
    {
        "id": "kgUi1-AVyi",
        "original": null,
        "number": 3,
        "cdate": 1666880379929,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666880379929,
        "tmdate": 1670009946024,
        "tddate": null,
        "forum": "Q3-1vRh3HOA",
        "replyto": "Q3-1vRh3HOA",
        "invitation": "ICLR.cc/2023/Conference/Paper268/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a technique to learn the shape of a dilated convolution kernel to allow via backprop the learning of the positions of the non-zero elements of the kernel. They do this by reparameterizing it as a series of weights (the \u201ckernel count\u201d), and a kernel window size (the \u201cdilated kernel size\u201d), and use bilinear interpolation to perform the convolution. They show modest gains on ImageNet image classification, Mscoco Object Detection, and ADE20k Semantic segmentation, at the cost of modestly decreased throughput. \n",
            "strength_and_weaknesses": "Strengths:\n\n- The paper itself is well structured, well written, and very easy to follow.\n- The experiments are thorough and well done, and include most of the basic data I would request as a reviewer (e.g. they have throughput, though they probably should also have it for object detection and semantic segmentation, and have appropriate baselines with modified receptive fields). \n- The method is relatively simple to understand, and the paper correctly focuses on one single idea throughout, holding everything else constant. I didn't have to guess where their gains came from.\n\nMain Weaknesses:\n\n- The paper, and associated project, feel poorly motivated to me. I don't get the why of this project from several angles: Why do we want or need to learn the kernel locations? When do we expect this to help? And why would people use this? \n  - The intro states `Our own investigation on ResNet and ConvNeXt with standard dilated convolution (Section 4.2) will lead to a similar conclusion. The failure of this method for classification tasks could be attributed to the great rigidity imposed by its regular grid as discussed in Wang & Ji (2018).`\n    - I don't buy this motivation. The main use case of dilated convolutions was to grow the receptive field of models when applied to large images, like in semantic segmentation or other dense prediction tasks. For smaller images, like those used in classification like the authors do the dilated convolution study on, it makes no sense, as the receptive field is already the whole image. \n- While the authors spend a lot of their related work contrasting the methods, it seems the situations (if any) you may want to use DCLS you may instead use deformable convolutions (https://arxiv.org/abs/1703.06211) which are popular in the object detection community. The authors should have compared directly to deformable convolutions.\n- The results just aren't that strong. e.g. the authors state: `From Table 2, we highlight the fact that ConvNeXt with DCLS convolutions always surpasses the ConvNeXt baseline in accuracy (gains ranging from 0.3 to 0.6) with the same number of parameters and only a little cost on throughput`\n  - I unfortunately don\u2019t see these gains as meaningful at all. 0.3 - 0.6 accuracy improvement (e.g. 82.1 -> 82.5) on ImageNet is really small at the cost of ~7% lower throughput, but even more importantly, this would require researchers/users to use non-standard convolution operations (including compiling and packing custom CUDA operations with their models). \n  - Furthermore, using non-standard convolution operations limits the platforms you can run this model on efficiently, and prohibits the advantages of different optimizations people may want to use (e.g. more efficient conv kernels for inference). \n\n\nOther feedback:\n- `\u201cThe problem with dense convolutional layers having large kernel sizes is that the number of trainable parameters is huge, which makes learning impractical.Finally, we observe that after training, the DCLS position density is higher around the center of the RF (see Appendix 8), suggesting that the central region is the most important one.\u201d`\n  - I don\u2019t think the first claim here is really supported. The ConvNeXt-T-ker17 seems to do reasonably well. Perhaps it just needs a bit more regularization (which you can think of DCLS as a version of). \n  - To the second point, the DCLS position density being higher around the center of the RF to me points to the fact that you probably don\u2019t need that large of kernels. \n- Re: Semantic Segmentation / Object Detection Results\n  - Thank you for including these! These two tasks are a much better application of dilated convolutions than ImageNet classification. That said, in my opinion, we end up at the same place that the results, though a small improvement, are not really meaningful enough to justify using this method.\n  - Throughput is also dropped here from the tables, and I suspect the small gains in accuracy (e.g. AP increases of 0.3-0.9 or mIoU increases/decreases of -0.3-1.1) are overshadowed by the increase in actual runtime.\n- Re: Robustness: Is there a hypothesis driving this set of experiments? Why would these kernels lead to more robust networks? The gains here are also relatively minor.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the paper itself as an artifact is high. The writing is good, the experiments well done, the baselines mostly appropriate (see deformable convolutions).\n\nThe work appears reasonably original, though the idea itself is very similar to Deformable Convolutional Networks (https://arxiv.org/abs/1703.06211) which have been widely used in the object detection community for awhile. The authors do spend a large portion of their related work contrasting the methods, and they are different, but not very different.\n\nThe paper has available code that presumably can be run to reproduce the results. I did not verify myself. ",
            "summary_of_the_review": "While the paper itself as an artifact feels high quality, the experiments done are (mostly) appropriate and well executed, and the project requiring significant impressive engineering work, I just don't see this paper as impactful or meaningful. The results simply don't justify the extra complexity (in method, in code, in platform lock in) or run time hit. As a result, I don't see it as being something that gets built upon, used by others, etc. \n\nBeyond the empirical and practical tradeoffs, the paper/project lacked a convincing underlying hypothesis of why we would want to do this at all. As a result, from reading the paper, the main takeaway of a reader will be \"this specific idea, when implemented this specific way, works reasonably\". There aren't generalizable insights that other researchers can build upon, learn from, or discuss. \n\nMy score is a result of a combination of these things. Because the paper feels pretty polished and finished, but I don't find the motivation or results themselves convincing, I see myself unlikely to raise my score.\n\n**Update**: After conversations with other reviewers, I've tried to re-calibrate what I think the bar for acceptance should be. As a result, I've updated my score to a 6. After reading the rebuttal, I stand by my assessment that the actual paper quality (in writing, experiment design, code availability, etc) is high, but still have concerns about how well motivated or impactful the work will be. As a result, I am OK to see this accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper268/Reviewer_8tHP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper268/Reviewer_8tHP"
        ]
    },
    {
        "id": "0-DWPuZLC8",
        "original": null,
        "number": 4,
        "cdate": 1666890500304,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666890500304,
        "tmdate": 1666890500304,
        "tddate": null,
        "forum": "Q3-1vRh3HOA",
        "replyto": "Q3-1vRh3HOA",
        "invitation": "ICLR.cc/2023/Conference/Paper268/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper describes a method for learning sparse convolution kernels (like dilated convolutions) that consists of a weighted sum of shifted pixels. I.e. the kernels can be understood to be expanded of a basis of shifted dirac-deltas, and the learnable parameters are the shifts for each \"pixel\" and its corresponding weight. The layers/kernels are differentiable w.r.t. the shift parameters via bilinear interpolation. The paper then explores what the effect is of changing the standard dense convolution kernels, with the sparse kernels that are able to grow to bigger receptive field sizes while training. Results show that learning the kernel sizes during training is beneficial, whilst typically being more parameter efficient (due to the sparse parametrization), at the cost of only a moderate computational overhead.",
            "strength_and_weaknesses": "**Strengths**\n1. The paper proposes a relatively simple, but effect idea\n2. The method is extensively tested on 2D image tasks and results are clearly presented.\n3. The method has practical value in that it seems to consistently improve performance at only a moderate reduction of througput. Moreover, the authors published their code with the paper.\n4. The paper is transparent in also reporting negative findings; it reports things that worked and things tried, but didn't work \n\n**Weaknesses**\n1. The presentation of the bilinear interpolation is really confusing and possibly incorrect.\n    * Equation 1 defines the kernel in terms of a function f but the function f is nowhere explicitly defined. The definition is somehow implicit in equation 3.\n    * p^1 and p^2 are not defined. I suppose these are the \"x\" and \"y\" coordinates (centers) of the pixels. In the start of section 2, $n$ is not defined, nor is $p$. This is in some sense OK because the labels are only used to indicated the # rows and columns in a matrix, but it confuses with the notation of p for the positions.\n    * In equation 1 however, index i is used to iterate over the weights/centers of the kernel, wherease in (3) it is used to iterate over the rows of K. This is somewhat confusing.\n    * Even more so as in (3) the p^1 and p^j no longer have indices, which p are are talking about?\n    * After a while I figured out that indeed this is just bilinear interpolation where each of the rows in (3) correspond to each of the 4 nearest corner points, but I feel like the presentation could have been much more effective.\n2. Recent works on learnable receptive field sizes seem to be missing. In particlar the works of Romero et al. which show the benefit of (extremely) large receptive field sizes:\n    * Romero, D. W., Kuzina, A., Bekkers, E. J., Tomczak, J. M., & Hoogendoorn, M. (2021). Ckconv: Continuous kernel convolution for sequential data. arXiv preprint arXiv:2102.02611.\nand learnable receptive field sizes:\n    * Romero, D. W., Bruintjes, R. J., Tomczak, J. M., Bekkers, E. J., Hoogendoorn, M., & van Gemert, J. C. (2021). Flexconv: Continuous kernel convolutions with differentiable kernel sizes. arXiv preprint arXiv:2110.08059.\n\n    The idea of using kernels parametrized by a set of shifted weights has been done before by Dai et al. (already discussed in the paper) by shifted dirac deltas, as in this paper, or shifted Gaussians as in:\n    * Jacobsen, J. H., Van Gemert, J., Lou, Z., & Smeulders, A. W. (2016). Structured receptive fields in cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2610-2619).\n    * Shelhamer, E., Wang, D., & Darrell, T. (2019). Blurring the line between structure and learning to optimize and adapt receptive fields. arXiv preprint arXiv:1904.11487.\n    * Pintea, S. L., T\u00f6men, N., Goes, S. F., Loog, M., & van Gemert, J. C. (2021). Resolution learning in deep convolutional networks using scale-space theory. IEEE Transactions on Image Processing, 30, 8342-8353.\n\n    The idea of sparse localized basis functions with learnable centers has also already been extended to group convolutional neural networks in\n    * Bekkers, E. J. (2019, September). B-Spline CNNs on Lie groups. In International Conference on Learning Representations.\n3. It should be discussed what the current work has to add to the above works. I believe that the emperical study presented in this paper is valuable in itself, but I believe some of the obtained insights have also already been discussed in the above works.\n\nMinor details:\n* It would be nice to give a bit more detail on what depthwise implicit gemm convolution entails. It is mentioned several times and a high level description could help the reader without them having to look up the original paper.\n* Regarding the position initialization (section 3). \"In an attempt to facilitate learning, we chose an initial distribution close to the one obtained at the end of training\", is this reasonable? What if the initial distribution was chosen poorly, then also the one at the end of training would be right? \"Yet in practice, ...\" so would you recommend using uniform initialization then instead?\n* On \"Dilated kernel size tuning\" I can imagine that the overhead could be independent from the constrained kernel size. Since convolution and kernel construction are linear, one could do the interpolation on the input feature maps instead (for the m different shifts) and then simply combine the results with a linear layer afterwards.\n* In related work the effective RF size is discussed and the reader is referred to appendix 10. The idea of looking at the ERFs is interesting, however, app 10 does not give much detail on how the \"heatmaps\" are obtained.\n* When discussin the method in comparison to deformable convolutions, it mentions \"Firstly, in deformable convolutions, the offsets ...\" I do not consider this to be a difference. Parametrizing the locations as offsets rather then absolute points (which are offsets to an origin) boils down to the same thing. Also, the \"secondly\", it indeed describes a difference, but I consider dilated convs to be strictly more general than the proposed work (i.e. one could decide with deformable convs not to make the kernels dependent on the input)",
            "clarity,_quality,_novelty_and_reproducibility": "Apart from the mathematical description of the bilinear interpolation, the paper is clearly written.\nThe quality is good, mostly from the experimental point of view.\nThe work is not particularly novel, but it's experimental insights are valuable.\nTheir code is made available.",
            "summary_of_the_review": "Not particularly novel, but the value of the paper is in its experimental work and code release, which seems to be properly documented. I cannot recommend accept as the first part of the paper is sloppy and it isn't very clear what the paper adds relative to existing works on kernels in adaptive bases.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper268/Reviewer_2NcJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper268/Reviewer_2NcJ"
        ]
    },
    {
        "id": "41O2fg8Lqf",
        "original": null,
        "number": 5,
        "cdate": 1667166291905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667166291905,
        "tmdate": 1670102982399,
        "tddate": null,
        "forum": "Q3-1vRh3HOA",
        "replyto": "Q3-1vRh3HOA",
        "invitation": "ICLR.cc/2023/Conference/Paper268/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper considers the problem of how to allow for larger receptive fields for better accuracy with affordable compuitational complexity. The authors propose an algorithm based on learnable spacing for dilated convolutions.",
            "strength_and_weaknesses": "The motivation of the paper is clear. It is well-understood both from the previous experience, from the related work and from the results presented in the paper, that larger receptive fields allow for more accurate CNNs in some scenatrious. \n\n**Strengths**\n1. The paper is well-written. The story is coherent and it is easy to follow. The main results are presented in an understandable form. \n2. The authors share the implementation of the algorithm which is a huge plus as it allows one to understand it better.\n\n**Weaknesses**\nThe main weakness of the current manuscript is that it does not consider a huge part of the related work. I am sure that the following papers should be taken into consideration. At least, they should be mentioned. It will be a huge plus to my current rating if the proposed method is somehow compared to these papers\n1. In [1] the authors propose to decompose filters as gaussian derivatives and vary their scales. Thus the model is able to utilize larger filters with the same number of trainable parameters. Their results demonstrate that such a method significantly improves the models' performance.\n2. In [2, 3] the authors propose to reparametrizer convolutional filters and learn their parameters. Which is very close to the proposed method in its spirit\n3. In [4, 5, 6, 7, 8, 9] the authors buid scale-scale equivariant neural networks which focus aroung the scale problem and propose everal interesting methods for a problem similar to the considered\n4. In [4, 6, 7] the authors use dilation and an alternative of dilation for fractional spacing, which is close to the proposed idea, although the initial motivation may vary.\n\n- [1] Jacobsen J. H. et al. Structured receptive fields in cnns. \n- [2] Romero D. W. et al. Flexconv: Continuous kernel convolutions with differentiable kernel sizes\n- [3] Romero D. W. et al. Ckconv: Continuous kernel convolution for sequential data\n- [4] Worrall D., Welling M. Deep scale-spaces: Equivariance over scale\n- [5] Sosnovik I., Szmaja M., Smeulders A. Scale-equivariant steerable networks\n- [6] Sosnovik I., Moskalev A., Smeulders A. Disco: accurate discrete scale convolutions\n- [7] Sosnovik I., Moskalev A., Smeulders A. How to Transform Kernels for Scale-Convolutions\n- [8] Bekkers E. J. B-spline cnns on lie groups\n- [9] Zhu W. et al. Scaling-Translation-Equivariant Networks with Decomposed Convolutional Filters\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear. The quality of writing is high. And the authors did everything to facilitate reproducibility",
            "summary_of_the_review": "The current version of the manuscript misses several very important papers which have a similar spirit while the motivation may slightly vary. Understanding the significance of paper without comparing it to other approaches is not possible.\n\n===================\n\nWhile the intiial rating was 6, I adjusted it to 8. The authors improved the paper according to my recommendations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper268/Reviewer_zKnb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper268/Reviewer_zKnb"
        ]
    }
]