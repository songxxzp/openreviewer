[
    {
        "id": "qYaSAVxoxf",
        "original": null,
        "number": 1,
        "cdate": 1666394800986,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666394800986,
        "tmdate": 1666394800986,
        "tddate": null,
        "forum": "woa783QMul",
        "replyto": "woa783QMul",
        "invitation": "ICLR.cc/2023/Conference/Paper276/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Traditional representation learning methods (for example, contrastive learning), while making significant progress, have potential fairness issues. This paper proposes training the image attribute editor to generate contrastive sample pairs for each sample in the original dataset, which share the same visual information except for sensitive attributes (e.g., male and female). To enable the model to learn powerful and fair representations, it reduces the distance between representations of contrastive sample pairs. The key idea is to first build the debiased training set and then learn the debiased representation. The proposed method outperforms existing unsupervised learning methods in terms of classification accuracy and fairness, according to experimental results on the CelebA and UTK-Face datasets.\n\n",
            "strength_and_weaknesses": "Advantages:\n- It is worthwhile to investigate the problem of fair unsupervised representation learning with only partially annotated sensitive attributes.\n- The paper is well written.\n- The proposed FairCL framework is generally sound, and experimental analysis could validate the framework's effectiveness to some extent.\n\nDrawbacks:\n- My main concern is that the proposed method is limited to face recognition-based applications. It is now possible to generate contrastive samples for face images thanks to the advancement of GAN-based techniques. However, doing so for other applications is challenging. Without high-quality augmentations, it is doubtful that this method can be applied in other applications.\n- Another major concern is that the authors only show one point on the fairness-accuracy trade-off curve. The majority of existing literature assesses the effectiveness of fairness mitigation algorithms using a fairness utility curve to determine which algorithm achieves the best fairness accuracy trade off. It would be difficult to determine which of the two is superior if only one point on the curve was considered: 1) EO: 39.6, and ACC: 77.7, and 2) EO: 24.5, and ACC: 74.1.\n- The evaluation is limited to the two datasets of the face recognition tasks.\n- An important baseline is missing. That is, we first train the sensitive attribute classification model using the partial sensitive attribute annotations. Following that, this model can be used to generate pseudo sensitive labels for other unlabelled samples. This method may not work well with difficult real-world datasets. However, for the two datasets used in this study, CelebA and UTK-Face, even a low ratio of labeled sensitive labels (e.g., less than 5%) are sufficient to train high-quality sensitive attribute classification models. The main reason is that the sensitive attribute annotation (male and female) task is much easier than the underlying task, such as Attractive.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed idea is sound, the paper is well written.",
            "summary_of_the_review": "Although this work has some merits, I am concerned about the experimental evaluations and the proposed method's real-world applicability.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper276/Reviewer_G4GB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper276/Reviewer_G4GB"
        ]
    },
    {
        "id": "G8rKcuAsvo",
        "original": null,
        "number": 3,
        "cdate": 1666535846880,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535846880,
        "tmdate": 1666535846880,
        "tddate": null,
        "forum": "woa783QMul",
        "replyto": "woa783QMul",
        "invitation": "ICLR.cc/2023/Conference/Paper276/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this manuscript, author attempt to employ the supervision of limited sensitive labels in images for learning fair representations. To address this new task, they refer to the idea of contrastive learning and propose a novel fairness-aware framework. Specifically, to ensure this fairness in contrastive learning, author adopt a generator to obtain some positive samples with various sensitive attributes, which could avoid the effect of sensitive attributes to ensure impartial learning process.",
            "strength_and_weaknesses": "Strength:\n1)The first strength of this article is the proposed new problem: FURL-PS, which aims to settle the key issue that sensitive attribute label is high cost to obtain and even suffer privacy policy. It has well practicability and worth to generalize.\n2)Besides, a major strength in this method is to implement the dataset adjustment to adapt the contrastive learning in self-supervised manner, thereby design a general framework FairCL. The design solution for the absence of sensitive label is also technically feasible to generate some positive samples that support the fair representations learning. In this case, after identifying the qualities of representations, it makes sense to implement this method and realize the process of fair learning.\n3)The corresponding experiments in two datasets are enough to validate its effectiveness and generalizability.\n\nWeakness:\nAccording to my understanding, the performance of this method relies heavily on the quality of generated images as positive sample, which is implemented by off-the-shelf framework AttGAN. In order to verify the generalization of your method, I think you should further discuss the effect of different frameworks of image generation. If possible, you could also provide some failure cases to observe the stability of your model.",
            "clarity,_quality,_novelty_and_reproducibility": "The article is well written and easy to understand. Author proposed a new problem of learning fair representation. They clearly pointed out the obstacle of acquiring the sensitive labels and refer to the idea of contrastive learning, in which generated image as positive samples to solve this problem. The various sensitive attributes in these generated samples ensure the model to implement fair representation learning, which is very novel and effective idea. Though with a good motivation and framework, the comprehensiveness of experiments are barely satisfactory and related explanations are lack of further analysis detailed, especially some hyper-parameters and the pseudo-code.",
            "summary_of_the_review": "The highlight of this paper falls on their motivation and solution, which has well practicability and worth to generalize. It provides a new direction to implement unbiased and fair representation learning under the limited annotated information.Besides, although the number of experiments is enough, the related analysis is barely satisfactory. Some experiments result still need to further analysis. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper276/Reviewer_Vv8C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper276/Reviewer_Vv8C"
        ]
    },
    {
        "id": "dGZo84I09c",
        "original": null,
        "number": 4,
        "cdate": 1666552139598,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666552139598,
        "tmdate": 1666552139598,
        "tddate": null,
        "forum": "woa783QMul",
        "replyto": "woa783QMul",
        "invitation": "ICLR.cc/2023/Conference/Paper276/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The author aims to bring attention to a known problem by giving it a formal name FURL-PS and definition, and proposes a practical solution, FairCL/FairCL* which is a novel combination of known theory and algorithms previously not having been applied to this particular problem of ML fairness. In particular, the paper demonstrates how to tackle the problem of training a face image encoder with a good ratio between the equalized odds and the face attribute classification accuracy, given a dataset partially annotated with sensitive labels, such as gender and age, and without the use of the target labels of the downstream tasks (in this case, face attribute classification). The paper is backed with appropriate model performance comparison tables and graph to show the effectiveness of the proposed solution FairCL/FairCL* along with some of its limitations.",
            "strength_and_weaknesses": "Dealing with a dataset partially annotated with sensitive labels is a known and common problem in the real world. However, as far as I know, this paper seems to be the first to give the problem a formal name FURL-PS to be able to refer to it with an ease. In addition, the choice of using AttGAN for the generative model is clever because AttGAN was already demonstrated in the original paper with the same CelebA dataset to modify only the intended face attribute. So, in this case, just a sensitive face attribute can be modified according to the already approved paper's demonstration. This paper demonstrated the effectiveness and limitation of using negative samples generated by an image editor with two different versions of the solutions FairCL and FairCL*. It seems like an extra mile the authors took that made the paper's findings even more interesting and useful for the reader. The theory used to balance between utility and the fairness using feature re-weighting in the absence of the target labels, is also simple and clever. The experiments themselves seem to be a high quality in that the authors chose downstream task face attributes that have high Pearson correlation with the sensitive attributes and in that the authors honestly wrote the limitation of the proposed solution in the presence of the \"extreme spurious correlation\" like in the case of Heavy Makeup and Lipstick. Figure 5 is also very helpful to better understand the effectiveness of the proposed model compared to other prior arts.\n\nNevertheless, there were some lingering questions/doubts about the approach described. The process of training the image editor G and the sensitive label classifier C with unlabeled data feels like a self-feeding/self-reinforcing loop that can introduce some bias. There wasn't much discussion about this to satisfy my doubts about the proposed model's fairness limitation that may come from this self-feeding training loop. It would have been also interesting if the author listed the Pearson correlation numbers of the face attributes that the author defined as \"extremely spurious correlation\" vs \"spurious correlation\" in order to translate the potentially qualitative and intuitive description into a more quantitative description.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was easy to follow and clearly written, backed by appropriate model comparison tables, graph, and model architecture diagrams along with appropriate equations. The proposed method is novel as described above.",
            "summary_of_the_review": "I recommend to accept this paper. Despite some lingering doubts mentioned before, the theory seems sound, and the empirical results seem comprehensive and promising. The paper is coherently written with clear language backed by appropriate tables and figures to assist easier reading and understanding. The proposed method is novel and tackles an important and practical ML Fairness problem faced in the real world. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper276/Reviewer_MhQc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper276/Reviewer_MhQc"
        ]
    },
    {
        "id": "-_aNm_YPeR",
        "original": null,
        "number": 5,
        "cdate": 1666612098926,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612098926,
        "tmdate": 1670473170934,
        "tddate": null,
        "forum": "woa783QMul",
        "replyto": "woa783QMul",
        "invitation": "ICLR.cc/2023/Conference/Paper276/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a variation of contrastive learning that learns fair representation with partially annotated sensitive attribute labels. It combines several components together to achieve the goal: generative model that can generate \"counterfactual\" examples with different sensitive labels for making the positive and negative samples for contrastive learning, pseudo-label prediction model to estimate the sensitive attributes, and feature re-weighting scheme for balancing utility and fairness. The resulting method is shown to achieve positive results on benchmark dataset for (unsupervised) learning fair representations with partially annotated sensitive labels. ",
            "strength_and_weaknesses": "Strength:\n- As mentioned above, the method combines multiple methods to achieve the goal, which is to learn fair representation with partially annotated sensitive label and without target label. It seems to be the first work to consider this specific setting. \n- The generative model example and t-sne visualization provides necessary qualitative results. \n- The proposed method is compatible to several contrastive learning methods. \n- The method can generate similar quality of fair representation compared to the methods with target labels. \n\nWeakness:\n- There are many hyperparameters (*thr* for generative model, $\\alpha$, $\\tau$), and the effect of them is not considered or analyzed. There is only one table on $\\alpha$. Particularly, I think the effect of *thr* should be very important since the generative models are typically sensitive to the hyperparameter choice. \n- The method is highly dependent on the GAN-based generative model, and when the dataset is not face-related images like CelebA or UTKFace, it is not clear how the method would perform. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- Exact details are not clearly presented and it is not clear how to reproduce the result.\n   - Line 176: how is *thr* chosen? \n   - Line 268: \"remove some low-confidence samples\" --> this is not rigorous. \n   - In fact, the whole paragraph line 171~182 is not very rigorous. It is not clear how to reproduce based on the description. Also, it is not clear whether the iterative steps described in the paragraph would always work. \n\n- The components of the method are careful combination of existing techniques rather than novel development of new schemes. The setting seems to be new, but it is not clear whether the proposed method is generalizable to other datasets other than face-related image datasets. ",
            "summary_of_the_review": "As mentioned above, while the problem setting seems to be new (but, it is also inspired by recent work (Jung et al, 2022) and is not very hard to think of) and there are some positive aspects in the paper, I think the overall method simply combines existing methods (which undermines the novelty) and the description of the method is not rigorous/general enough for a publication at ICLR. \n\n**** Post-rebuttal\nI think some limitations still exists since the proposed method looks highly dependent on the examined dataset regarding face images, for which synthetic data is relatively easy to generate via GAN. But, the proposed problem setting indeed can be regarded as novel and the paper also has some value of tacking it for the first time. The experimental results for the considered dataset is promising, too, so I raised my score to 6. I think the authors need to at least describe the limitation of their method as well. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper276/Reviewer_kKgZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper276/Reviewer_kKgZ"
        ]
    }
]