[
    {
        "id": "RzvwBkXnuUh",
        "original": null,
        "number": 1,
        "cdate": 1666331522627,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666331522627,
        "tmdate": 1666331522627,
        "tddate": null,
        "forum": "JZRBSoJv7lb",
        "replyto": "JZRBSoJv7lb",
        "invitation": "ICLR.cc/2023/Conference/Paper3867/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes using the attack transferability of adversarial examples to measure neural architecture similarity. The authors analyze the design choices leading to better model diversity measured by the proposed metric. The paper also demonstrates the application of the proposed similarity measure in ensemble learning and knowledge distillation.",
            "strength_and_weaknesses": "__Strength__\n\n__[S1] Novel model similarity measure.__ Using attack transferability of adversarial examples to measure model similarity is relatively novel. As a data-oriented approach, the similarity measure can be easily applied to different model architectures.\n\n__[S2] The analysis reveals some interesting findings.__ I appreciate the comprehensive analysis of the model diversity. The analysis identifies some factors that lead to more diverse models measured by attack transferability. The findings may be of interest to some data partitioners in the community.\n\n__[S3]__ Overall, the paper is easy to follow.\n\n__Weakness__\n\n__[W1] Some findings are mostly expected or have been studied before.__\n- Intuitively, an adversarial example generated for a specific model is more effective on the same base architecture. For example, using the adversarial examples generated from ResNet to attack VGG is more difficult than attacking other ResNet models. So, it is somewhat expected that the proposed method considers the differences in model architecture as the most diverse change.\n- The observation that using more diverse models in ensemble learning leads to better performance has been explored in many previous works.\n- As suggested by the paper, the observation that using a similar model architecture for student and teacher models results in better performance is studied in previous KD works.\n\n__[W2] Limitation of using attack transferability as similarity measures.__ As using the adversarial attack transferability naturally gives a higher similarity score for the same model architecture, is it possible that the proposed similarity measures give an incorrect similarity score in terms of the predictions:\n- Given two models with the same base architecture, the models make very different predictions; however, the adversarial examples transfer well across the two models.\n- Given two models with different base architectures, the models make very similar predictions; however, the adversarial examples do not transfer well across the two models.\n\n__[W3] Does not discuss and compare with other diversity measures.__ Although the proposed similarity measures can improve ensemble learning, the paper does not compare the approach with previously suggested model diversity metrics in ensemble learning literature, e.g. [1]. It is difficult to judge whether the proposed method is a better measurement in selecting the models in ensemble learning.\n\n__Other comments__\n\n__[O1]__ If the set of candidate models contains defended models that are trained to be robust to adversarial samples, does the proposed similarity measure also work?\n\n__[O2]__ The paper uses PGD as the adversary. Is the method sensitive to the choice of attacks?\n\n__Ref__\n\n[1] Kuncheva, Ludmila & Whitaker, Chris. (2003). Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy. Machine Learning. 51. 181-207. 10.1023/A:1022859003006.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The analysis is sound and reasonable. I did not identify obvious obstacles in reproducing the results, however, providing the code can improve the reproducibility of this work.",
            "summary_of_the_review": "This paper is well-written and proposes a simple and practical similarity measure for neural architecture. However, this paper lacks the discussion and comparison with the model diversity metric proposed in previous works. The paper should show and explain why the attack transferability metric is a better measurement of model similarity.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_kjGm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_kjGm"
        ]
    },
    {
        "id": "TPuRIyx-FK7",
        "original": null,
        "number": 2,
        "cdate": 1666551394597,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666551394597,
        "tmdate": 1666551394597,
        "tddate": null,
        "forum": "JZRBSoJv7lb",
        "replyto": "JZRBSoJv7lb",
        "invitation": "ICLR.cc/2023/Conference/Paper3867/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new similarity function to quantify the similarity of the DNNs in image classification tasks based on transferability of adversarial examples. For a pair of similar DNNs, authors investigate which types of DNN components contribute most to the similarity and their implications to practical scenarios. Using a collection of pre-trained image classifiers on ImageNet, authors conclude that the base architecture component (i.e., convolution-based CNN vs. self-attention-based Transformer) plays a most important role in measuring similarity and cluster DNNs according to the 13 categories of DNN components.       ",
            "strength_and_weaknesses": "The problem of measuring DNN similarity sees a broad range of applications in model diversity, such as choosing diverse models for ensemble, and similar models for knowledge distillation. The categorization of DNN into 13 different components provides a useful guideline for future studies. \n\nA major weakness to the reviewer lies in the rationale of defining the similarity score, i.e., transferability of adversarial examples generated by PGD. The underlying assumption states if the adversarial examples are transferrable between a pair of DNNs, then the similarity is high. It is more reasonable if the similarity is measured between a pair of CNNs but not so between a pair of Transformers nor between a pair of CNN and Transformer. This is because the adversarial examples generated by PGD on CNN may not be transferrable to Transformer, evident by several recent publications on transferability of adversarial examples (e.g., Mahmood et al, ICCV-2021, Fu et al, ICLR-2022). In fact, the algorithms to generate adversarial examples for Transformer can never be the same as those for CNNs. This is because Transformer operates on image patches to extract global features via self-attention whereas CNN operates on pixels to extract local features via convolution feature maps. As such, the reviewer believes the similarity score is limited to CNN on image classification tasks.               \n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear, logically coherent and related works are comprehensively covered. It starts with motivation and problem statement, followed by literature review and the proposed similarity score. Then it continues with experimental evaluation and outline of practical scenarios. The perspective of using transferability of input gradients via adversarial examples are novel and the decomposition of DNN into 13 components provide useful guideline for future studies. The reproducibility appears to be good as well. The paper would be much strengthened if authors propose the similarity score for CNN and Transformer separately based on different mechanisms of generating adversarial examples.    ",
            "summary_of_the_review": "This paper tackles an important problem of measuring DNN similarity in terms of image classification task. The similarity score is well motivated by the input transferability and practical application in model diversity, either increase diversity in generating ensembles or decrease diversity in selecting teacher and student in knowledge distillation, are also promising. However, the major technical concern is the similarity score may be more suitable for measure CNN similarities. For Transformer similarities, a different mechanism to generate adversarial are required.      ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_yYKW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_yYKW"
        ]
    },
    {
        "id": "F_crudxougD",
        "original": null,
        "number": 3,
        "cdate": 1666565315781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666565315781,
        "tmdate": 1666565315781,
        "tddate": null,
        "forum": "JZRBSoJv7lb",
        "replyto": "JZRBSoJv7lb",
        "invitation": "ICLR.cc/2023/Conference/Paper3867/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper present a DNN model similarity measure using input gradient transferability. The basic hypothesis is that if two neural networks are similar, adversarial attack transferability will be high. Additionally two topics are investigated: (1) Which network component contributes to the model diversity? (2) impact of model diversity in practice.",
            "strength_and_weaknesses": "Strength\n\n+ Similarity of DNN architecture is an important research topic.\n\n\nWeaknesses\n\n- What does \"If A and B are similar and assume an optimal adversary, then accA\u2192B will be almost zero\" mean?\n\n- There are numerous syntax errors. e.g., please revise \"We expect our analysis tool helps a high-level understanding of differences between various neural architectures as\nwell as practical guidance when using multiple architectures.\"\n\n- What is \"destructive success\"?\n\n- Please elaborate \"distinct properties\".\n\n- How is similarity score defined?\n\n- What is input gradient?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThis paper is often confusing. The writing and organization need to be significantly improved, e.g. \"then accA\u2192B will not be dropped significantly\"\n\nQuality\n\nThe paper is technically weak, and it is hard to tell what contributions it makes.\n\nNovelty\n\nThe novelty of this paper is not clear. For example, \"we found that more diversity leads to better ensemble performance\" has been recognized in ensemble learning for a long time.\n\nReproducibility\n\nGood.",
            "summary_of_the_review": "This paper is lack of technical contribution and novelty.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_Bk5e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_Bk5e"
        ]
    },
    {
        "id": "ZgKYfJCGFbk",
        "original": null,
        "number": 4,
        "cdate": 1666609498204,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609498204,
        "tmdate": 1666609498204,
        "tddate": null,
        "forum": "JZRBSoJv7lb",
        "replyto": "JZRBSoJv7lb",
        "invitation": "ICLR.cc/2023/Conference/Paper3867/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a quantitative similarity score between different neural architectures based on the adversarial attack transferability. This smiliarity helps to understand the component-level architecture design, and leads to better understanding of the relationship between model similarity of model ensemble performance and model distillation performance. Several interesting observations are obtained based on the proposed similarity function.",
            "strength_and_weaknesses": "Strength:\n1. This paper proposes a quantitative similarity score between different neural architectures based on the adversarial attack transferability. \n2. The proposed smiliarity helps to understand the component-level architecture design, and leads to better understanding of the relationship between model similarity of model ensemble performance and model distillation performance. \n3. Several interesting observations are obtained based on the proposed similarity function.\n4. Extensive experiments and analysis are conducted, to lead to better understanding.\n\nWeaknesses:\n1. Could the authors provide more motivation and insights for why  adversarial attack transferability can serve as the similarity function?\n2. In Figure 6 (c), most teacher models are clustered together with quite similar slimilarity. More teachers with large variety of similarity should be provided to better justify the conclusion that using a more dissimilar teacher leads to better distillation performance.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and easy to understand. The proposed method is of good novelty.",
            "summary_of_the_review": "This paper is interesting and leads to several observations that may be beneficial to architecture design, model ensemble and knowledge distillation. More analysis and insights for the proposed similarity function should be provided. Several experiments related to Figure 6 are encouraged to be conducted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_UduE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_UduE"
        ]
    },
    {
        "id": "RriXwBzNI1",
        "original": null,
        "number": 5,
        "cdate": 1666684331522,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684331522,
        "tmdate": 1666684331522,
        "tddate": null,
        "forum": "JZRBSoJv7lb",
        "replyto": "JZRBSoJv7lb",
        "invitation": "ICLR.cc/2023/Conference/Paper3867/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method to compute pairwise similarities between two architectures. This is done by evaluating the change in prediction of first model caused by the adversarial prediction w.r.t. second model and viceversa, with the intuition being that if two models are similar then their adversarial perturbations would have similar effect. Paper then uses the proposed similarity measure to analyze existing model architecture choices and effect on ensembling performance.",
            "strength_and_weaknesses": "Strengths\n- Well written and easy to understand paper\n- A detailed analysis of the similarity measure w.r.t. a fairly large (69) menu of models with different architectural components.\n- Interesting insights into effect of component choices on similarity and ensembling performance.\n\nWeaknesses\n- Proposed method is computationally expensive and the approximation proposed in 5 is rather ad-hoc and not well evaluated. This limits the applicability of the method.\n\nMinor nitpick\n- What do authors mean by \"destructive success of DNNs\"? The phrase seems to refer to negative effects of DNN success. Perhaps, rephrase is that is not intended, otherwise clarify. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: Paper is well written and easy to understand\n- Quality: Paper includes a fairly detailed study of the proposed similarity metric, with a few interesting insights (e.g. choice of stem has a large impact on similarity).\n- Novelty: Reasonable. Paper repurposes adversarial perturbations to define a similarity and presents novel conclusions on choice of teacher.\n- Reproducibility: The proposed metric is fairly simple. If the images used to compute similarity are shared, then result should be reproducible.\n",
            "summary_of_the_review": "Overall, I feel that the technical contribution is fairly limited. However, the paper presents an interesting an fairly detailed study that provides interesting insights and would be beneficial to a large audience. I vote for accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_ynKk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3867/Reviewer_ynKk"
        ]
    }
]