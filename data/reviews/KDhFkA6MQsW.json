[
    {
        "id": "crzIFuscEgo",
        "original": null,
        "number": 1,
        "cdate": 1666255415814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666255415814,
        "tmdate": 1666255415814,
        "tddate": null,
        "forum": "KDhFkA6MQsW",
        "replyto": "KDhFkA6MQsW",
        "invitation": "ICLR.cc/2023/Conference/Paper2555/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the problem of escaping from saddle points, a basic problem in nonconvex optimization. Specifically, this paper considers function evaluations, i.e., zero-order inputs, and is able to give algorithms with ~O(1/eps^1.75) iteration complexity and ~O(d/eps^1.75) query complexity. Although this 1/eps^1.75 dependence has been achieved by gradient-descent based methods, this paper is first one to achieve the same dependence for gradient-free methods. Finally, numerical experiments are conducted to corroborate the theory results.",
            "strength_and_weaknesses": "From my perspective, this paper has the following strengths:\n\n- This is the first work which gives zeroth-order methods for escaping saddle points with matching eps dependence compared to first-order methods. This is a great contribution to me and significantly improves our understanding of zeroth-order methods.\n\n- Two algorithms are proposed to achieve the result of ~O(1/eps^1.75) iteration complexity and ~O(d/eps^1.75) query complexity. The second one based on negative curvature finding only has one logarithmic term in d.\n \n- The theory results are corroborated by numerical experiments on two instances: cubic regularization and quartic functions. Codes are also provided in the supplementary material.\n\nNevertheless, the paper may still have space to improve from the following aspects:\n\n- It would be very helpful if the authors can illustrate more on the practical cases where both escaping from saddle points is a vital problem and we have to use zeroth-order methods instead of first-order methods. In the introduction, the authors mentioned the situations where the calculation of explicit gradients is expensive or even infeasible, such as black-box adversarial attack on deep neural networks. This is still a bit vague to me; would be helpful to explain how we have the function evaluation access in such problems.\n\n- The writing in this paper is a bit sloppy in general and has space to further polish. From a relatively superficial reading, I can already spot the following glitches:\n\n- - Second paragraph in Page 1: non-convex problem 1 -> non-convex problem (1). This also applies to a few following places.\n\n- - Third paragraph in Page 1: O(1/eps^2) -> Theta(1/eps^2) (Here, the authors are talking about the optimality of gradient descent, so a lower bound or tight bound should be reflected here.)\n\n- - Second half of Page 2: proposed a first-order Negative curvature finding framework -> \u2026 negative (should be small letter instead of capital letter here)\n\n- - End of Page 2: with fewer function query complexity -> with smaller function query complexity, or with fewer function queries\n\n- - Beginning of Page 3, second bullet point in contributions: Due to the efficiency of the negative curvature finding for finding the most negative curvature direction near a saddle. We further study -> \u2026, we further study (currently, the first sentence is not complete)\n\n- - Beginning of Page 4: By the property Hessian Lipschitz -> By the Hessian-Lipschitz property \n\n- - Middle of Page 5: The second part of the Algorithm 1 is the Nesterov\u2019s accelerated gradient descent steps with its gradients estimated by 2 -> \u2026 with its gradients estimated by Eq. (2) (otherwise it reads like the gradient is estimated by the value 2)\n\n- - Page 5, after Eq. (6): Specifically, when 6 doesn\u2019t hold -> Specifically, when Eq. (6) does not hold (in formal English writing, abbreviations are supposed to be open). Same change shall be applied to the statement of Lemma 3: when 6 does not holds -> when Eq. (6) does not hold\n\nIn addition, I also have a couple of questions for the authors:\n\n- It would be helpful to give detailed comparison between Algorithm 1 and Algorithm 3. Algorithm 1 gives slightly worse query complexity results (extra poly-log d overhead); so why is this of general interest over Algorithm 3?\n\n- In Eq. (2) in Section 2.3, a central difference coordinate-wise gradient estimator is applied. In fact, in numerical analysis, higher-order central difference formulae have been studied, see for instance Li https://www.sciencedirect.com/science/article/pii/S0377042704006454?via%3Dihub It would be of general interest to discuss whether using a higher-order central difference formula could further improve the results.",
            "clarity,_quality,_novelty_and_reproducibility": "From my perspective, the clarity and quality of this paper is in general good, though the paper still has space to improve as I mentioned above. Novelty is excellent -- this is the first paper proving the 1/eps^1.75 convergence rate for escaping from saddle points using zeroth-order methods, matching that of first-order methods. Reproducibility is also excellent -- theoretical results are stated with detailed proofs and the numerical experiments are provided with codes and detailed explanations.",
            "summary_of_the_review": "Overall, I think this is a novel and outstanding work in studying zeroth-order methods for escaping from saddle points, and I\u2019m happy to recommend acceptance for ICLR 2023. Nevertheless, the paper still has space to improve as mentioned above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2555/Reviewer_kuK4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2555/Reviewer_kuK4"
        ]
    },
    {
        "id": "G1TYSo09k_V",
        "original": null,
        "number": 2,
        "cdate": 1666419959735,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666419959735,
        "tmdate": 1668800906712,
        "tddate": null,
        "forum": "KDhFkA6MQsW",
        "replyto": "KDhFkA6MQsW",
        "invitation": "ICLR.cc/2023/Conference/Paper2555/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors provide to their knowledge the first two variants of Nesterov's accelerated gradient-free algorithms for escaping saddle points of minimization problem, with comparable convergence rate to their first-order counterparts and smaller oracle complexity compared to prior derivative-free methods for finding second-order stationary points.\n",
            "strength_and_weaknesses": "Pros: This paper studies a significant novel research problem. Theorems and experiments look solid. The presentation looks generally clear, brief and neat. \n\nCons: There are some points that need to be clarified, especially those about complexity results, See \"Clarity, Quality, Novelty And Reproducibility\" for detail. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The contribution looks clear and brief. This paper is generally clear and well organized. Some points need to be clarified as shown below.\n\n(1) What do $c$ and $\\Delta_f$ mean at the end of page 4? Though $\\Delta_f$ is defined in Theorem 1, it is better to define it at its first appearance. \n\n(2) At the beginning of Lemma 6, should the second $\\widehat{\\nabla} f(\\mathbf{x}_t)$ be $\\nabla f(\\mathbf{x}_t)$? \n\n(3) Just double check, Do Lemmas 3-5 hold almost surely instead of high probability? \n\n(4) In Theorem 1, by \"one of the iterates will be an $\\epsilon$-approximate SOSP\", do you mean $\\mathbf{x}_t$ or $\\mathbf{y}_t$? The lemmas seems to imply $\\mathbf{x}_t$ but it is better to clarify it in Theorems 1 and 2. \n\n(5) In Lemma 7, $\\lambda_{\\min}(\\nabla^2 f(\\mathbf{x}_t))\\le ??$.\n\n(6) In Algorithms 1 and 3, variables like $\\mathbf{x}_t$, $\\tilde{\\mathbf{x}}$ have been assigned multiple times. Is it better to use $\\leftarrow$ instead of $=$ for such variables, which includes all the assignment statements in Algorithms 1 and 3? \n\nAlso, do $\\mathbf{x}_t$ in Theorem 1 and Lemmas 3-6 refer to its value right after implementing line 4, 6 or 9 of Algorithm 1? Similar issue exists in Section 3.2. You may clarify it in these Lemmas and theorms, or add variables such that each variable is assigned only once?\n\n(7) In Line 15 of Algorithm 3, should the first $\\tilde{\\mathbf{x}}$ be $(\\tilde{\\mathbf{x}}, \\tilde{\\mathbf{x}})$ to match dimensionality? \n\n(8) In Theorem 2, you could substitute the value of $\\delta_0$ into the complexity, since $\\delta_0$ relies on $\\epsilon, \\delta, \\rho$. \n\n(9) The complexities in Theorems 1 and 2 match oracle complexity in the abstract, but the iteration complexity is not found in the theorems. Could you add them? Also, you may say \"The total number of function queries (oracle complexity)\" in the theorems to be consistent with abstract.\n\n(10) I did not find out the complexity value expressed in $\\mathcal{O}$ in the proof of Theorem 1. In the proof of Theorem 2, $T=\\widetilde{O}\\big(\\frac{\\Delta_f}{\\epsilon^{1.75}}\\log d\\big)$ does not match the complexity in Theorem 2. Could you explain that? Usually when proving complexity, it is recommended to express the complexity in terms of hyperparameters and then substitute the hyperparameter values. \n\n\nQuality: \n\nThis work is complete and generally solid. However, the claim in the abstract that \"our methods achieve a comparable convergence rate to their first-order counterparts and have fewer oracle complexity compared to prior derivative-free methods for finding second-order stationary points.\" is not well supported, unless you compare all these complexities in terms of $\\mathcal{O}$, either in the introduction or after Theorems 1 and 2. \n\nIn the experiment, why is RSPI not implemented in Figure 1? \n\n\nNovelty:  This work gives positive answer to a novel research problem: can Nesterov's momentum accelerate the convergence of gradient-free methods for finding second-order stationary point? \n\n\nReproducibility: To ensure reproducibility of the experiments, it is better to tell the values used for all the inputs (hyperparameter choices). For example, what are the candidate values in the coarse grid search for $\\ell$ and $\\rho$? \n\nMinor comments: \n\n(1) In the abstract, I think \"smaller oracle complexity\" is better than \"fewer oracle complexity\". \n\n(2) At the beginning of Section 3.1, use \"three parts\" instead of \"three part\". \n\n(3) Use bolded $\\mathbf{x}_t$ in line 5 of Algorithm 1.\n\n(4) Line 8-9 of Algorithm 1 could be moved to right before line 6. \n\n(5) In and right after Lemma 3, you could use \"eq. (6)\" instead of \"(6)\". This also applies to equation citations elsewhere.\n\n(6) You could use big { } in Lemma 4. Similarly, use big ( ) on the left side of the centered equation in Lemma 8. \n\n(7) In Theorem 1 and 2, \"query\" could be changed into \"queries\". \n\n(8) A typo in Theorem 2: \"set set\".\n\n(9) You could also cite \nZhang, H., Xiong, H., & Gu, B. (2022). Zeroth-Order Negative Curvature Finding: Escaping Saddle Points without Gradients. arXiv preprint arXiv:2210.01496.",
            "summary_of_the_review": "Based on \"Strength And Weaknesses\" above, I recommend borderline accept for this paper. After my major concerns above about the complexities are solved by the authors, I would like to raise my rating. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2555/Reviewer_sdTo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2555/Reviewer_sdTo"
        ]
    },
    {
        "id": "XFIi5q-JFrf",
        "original": null,
        "number": 3,
        "cdate": 1666795624148,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666795624148,
        "tmdate": 1666795697733,
        "tddate": null,
        "forum": "KDhFkA6MQsW",
        "replyto": "KDhFkA6MQsW",
        "invitation": "ICLR.cc/2023/Conference/Paper2555/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes two zeroth-order Nesterov\u2019s accelerated gradient descent based algorithms that can escape saddle points and converge to second order stationary points. The main idea of the work is to use the central finite difference gradient estimator to approximate the gradient. The theoretical results show that the proposed can converge to second-order stationary points with fewer function queries. \n",
            "strength_and_weaknesses": "Strength \n\nOverall, the paper is well written and the related work are well cited. The introduction is clearly organized. The derived convergence results match the state-of-the-art one and are faster than previous zeroth-order algorithms for escaping saddle points. Numerical results illustrate the effectiveness of the proposed algorithms.\n\nWeaknesses\n\n1. Since the work focused only on the deterministic optimization, this may restrict the application scenarios of the algorithm. It'd be nice if the authors have further insights for stochastic  optimization as well.\n\n2. The algorithms use $\\mathcal{O}(d)$ function evaluations per iteration to enable a high accuracy approximation of the gradient,  which may also not\nbe practical in realistic applications when the dimension of the problem is large. I wonder if such a gradient estimator is necessary?\n\n3. Since the algorithms are similar to the Jin et al. and Zhang et al., it would be helpful to explain what novel theoretical analysis techniques are used in this work.\n\n4. In theoretical results, the value of parameter $\\mu$ is set to be very small, is it the same in the experiment? Since in practice, too mall $\\mu$ may cause system error. It would be better for the authors to explain how to choose the parameter u in the experiment.\n\n5. Minor issues:\n\nit seems that $\\mathcal{H}_f$ only appear in section 2.4, where is it used?\n\nAlgorithm 2 is from Jin et al. 2018, you should add a cite here or delete it and add a footnote.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written, and the techniques are effective and well-supported by the empirical results.",
            "summary_of_the_review": "Overall, this is a good paper, and I would recommend an \"accept\".",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2555/Reviewer_Y99G"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2555/Reviewer_Y99G"
        ]
    },
    {
        "id": "BY14k69YAnD",
        "original": null,
        "number": 4,
        "cdate": 1667220465327,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667220465327,
        "tmdate": 1667220465327,
        "tddate": null,
        "forum": "KDhFkA6MQsW",
        "replyto": "KDhFkA6MQsW",
        "invitation": "ICLR.cc/2023/Conference/Paper2555/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper shows convergence to an \u03b5-second-order stationary point for zeroth-order methods. They substantially improve existing bounds, achieving \u03b5^-7/4 dependence on \u03b5 and quasi-linear dependence on problem dimension. They present two algorithms: the first algorithm and its analysis are based on the Jin et al\u201918. The second algorithm, based on the Hessian-vector product idea, substantially improves the logarithmic factors. One of the main component in their analysis is better estimation of discrepancy between the true gradient and its approximation, based on the Lipschitz Hessian property.\n\n",
            "strength_and_weaknesses": "My main concern about the paper is the lack of explanations:\n-- The algorithms should be explained in more details. I\u2019m familiar with Jin et al\u201918, so I had a good idea what happens in Algorithms 1 and 2. However, while I could parse Algorithm 3, I can\u2019t say I understood what happens there.\n-- Similarly, it would be great to explain the choice of parameters (I mean, why their values make sense), especially \u03bc in Theorem 1.\n-- Proof outline for Section 3 should be added.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Minor issues:\n-- The title of Table 1 should be changed\n-- Page 6: \u201cHessian-vector product estimator in 3.\u201d -> \u201cEquation (3)\u201d.\n",
            "summary_of_the_review": "Solid paper, accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2555/Reviewer_85d1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2555/Reviewer_85d1"
        ]
    }
]