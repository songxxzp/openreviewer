[
    {
        "id": "aJAmE4Qvkc",
        "original": null,
        "number": 1,
        "cdate": 1666047349058,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666047349058,
        "tmdate": 1666224918111,
        "tddate": null,
        "forum": "4-aEhZnvNnk",
        "replyto": "4-aEhZnvNnk",
        "invitation": "ICLR.cc/2023/Conference/Paper2017/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a text embedding that deeply represents semantic meaning,\nand calls it a neural embedding.\nWhile the conventional text embeddings use the vector output of a pre-trained language model,\nthe neural embedding learns the text and literally pick its brain, and takes the actual weights of the model\u2019s neurons to generate a vector.\nMore precisely, it allows to learn model\u2019s neurons from text, and may reflect deeper semantic features of the text.\nAuthors compare GPT sentence (SGPT) embedding over  several datasets and show that 1) the neural embeddings are comparable to SGPT embeddings, and 2) its errors and qualities are often different.\n",
            "strength_and_weaknesses": "Strength\\\n*Authors take an empirical and theoretical approach\\\n*Compare the smaller models with the larger model, SGPT\\\n*Human analysis\n\nWeaknesses\\\n*The theoretical background of the proposed approach is weak.\\\n\u3000The theoretical background and reasons why tuning and masking strategies can acquire semantic representation of texts are unclear. \\\n*Authors' improvement is unclear in applying the micro-tuning\\\n\u3000What is the new rationale for the application of a periodic masking strategy in BERT? \\\n\u3000Why you use BERT instead of GPT? \\\n*Insufficient comparison of technical aspects \\\n\u3000Authors apply only the product of the embeddings to compare the semantic similarity  in Eq (2). \\\n\u3000Is the product of embeddings more common than the difference in distance in triplets? \\\n*No ablation analysis \\\n\u3000Is it fair to compare GPT-base (decoder) and Bert-base (encoder) models in the applied task? \\\n    If we don't apply  a periodic masking strategy to GPT-base model,  \\\n    isn't BERT-base models, not the masking strategy, affecting the outcome?",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of manuscript is not low, and is easy to understand.\nWhile the reproducibility is also not low,\nboth its quality and novelty are not high.\nThat leads to the following review decision.",
            "summary_of_the_review": "Although author's motivation is interesting,\nthey do not provide its theoretical basis and show that comparisons are insufficient.\nThis is the reason why I can not appreciate it.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2017/Reviewer_JM7i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2017/Reviewer_JM7i"
        ]
    },
    {
        "id": "AdBJSdonvf",
        "original": null,
        "number": 2,
        "cdate": 1666652428320,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652428320,
        "tmdate": 1666652428320,
        "tddate": null,
        "forum": "4-aEhZnvNnk",
        "replyto": "4-aEhZnvNnk",
        "invitation": "ICLR.cc/2023/Conference/Paper2017/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new way of representing a text as a single fixed-size vector. While most text embedding methods use model activations to represent the input text, the proposed method computes the difference in model parameters that is induced by finetuning on a single text input towards. The finetuning task is a custom masked language modeling objective.\nThe new method is compared to a recent SOTA model that uses activations as embeddings. Through exhaustive text similarity evaluations, the new model is shown to be qualitatively different from the baseline, exceeding it in performance in several regards.",
            "strength_and_weaknesses": "Strengths:\n* sensible and interesting idea, easy to understand\n* comparison on a lot of text similarity benchmarks\n* promising results, as the proposed method seems complementary to the baseline in many regards\n\nWeaknesses:\n* no evaluation of the time complexity in comparison to the baseline. Only the per example time of the proposed method is evaluated (in appendix), but a reference point is missing. This makes it difficult to judge the practicality of the approach, and (probably) hides its high computational cost.\n* no controlled experiment / ablation: BERT is used for creating the neural embeddings, so it would be logical to use averaged BERT embeddings as a baseline, which would constitute a controlled experiment. While comparison to SOTA is nice (as an upper bound), you cannot rule out that the observed qualitative differences between embedding methods are actually due to the underlying model architecture or training data, rather than the way embeddings are created. This weakens the support for the main claim.\n* writing could be improved. There are many places that appear a bit sloppy (but could be fixed easily); details in the Clarity section below.\n* the method includes a masking scheme that deviates from the original BERT masking objective. As no ablations or explanations are provided, it remains unclear how this specific training objective influences the performance of the model. Could it be responsible for the observed qualitative differences?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presents a novel idea, and no obvious mistakes were made in the evaluation. It would be fairly easy to repeat the experiments with the provided code and exhaustive descriptions in the paper.\n\nThe writing could use improvement in many places:\n* The description of L_1 ... L_3 are based on their naming conventions in the Huggingface Transformer library. First, this is not understandable to people who are not familiar with this library. A description of what these weights refer to is needed. Second, the choice of layers seems arbitrary. Why use the bias vector of the linear projection layer but not the weight matrix? Why are the weights of LayerNorm included? A justification and explanation of this selection would be helpful.\n* Table captions need to go above the table according to the ICLR style guidelines.\n* At the end of Section 2, there is a reference to a lower quality neural embedding model (ablation on the training objective) supposedly evaluated in Section 3. However, in Section 3, no such evaluation is to be found.\n* Table 1: The columns total and wrong seem to add no value, but only contain large, uninformative writers. For all practical purposes, isn't the 'error' column more than enough?\n* You say that for 'text-summ-g' neural embeddings are sensitive to the random seed. Why do you only report the sensitivity here? Shouldn't it be senstitive to the random seed on all datasets?\n* Figures are no vector graphics, bad quality when zooming in\n* Section 3.4: The quality-controlled paraphrase generation model is not described in enough detail. The description is not exhaustive enough to fully understand what Section 3.4. is measuring exactly.",
            "summary_of_the_review": "The paper presents a worthwhile and novel idea that shows promising initial results against a state-of-the-art baseline. However, only a controlled experiment can support the main claim that neural embeddings learn something qualitatively different from conventional embeddings. Moreover, the reader learns only little about what makes the model perform due to a lack of ablations on e.g. the training objective. I therefore recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2017/Reviewer_gPmr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2017/Reviewer_gPmr"
        ]
    },
    {
        "id": "2yh8i-6Tjk",
        "original": null,
        "number": 3,
        "cdate": 1666665122301,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665122301,
        "tmdate": 1666665122301,
        "tddate": null,
        "forum": "4-aEhZnvNnk",
        "replyto": "4-aEhZnvNnk",
        "invitation": "ICLR.cc/2023/Conference/Paper2017/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work proposes a new type of embeddings of text. Instead of relying on the output of the neural network, the proposed embedding is generated by the actual weights of the network, when it is tuned with the specific content/text. The work shows that the embeddings generated this way capture semantic differences between text -- texts with similar meanings are closer to each other in the embedding space. \n\n",
            "strength_and_weaknesses": "Strength:\nIt is an interesting idea to represent text by the actual neural network weights (or the change of it after tuning in specific). \n\nWeakness:\nThe paper fails to establish why this is a desired method, for the following reasons.\n\n1. The benchmarks (ranking triplets) used are quite elementary. Tasks close to real world scenarios should be used (e.g. retrieval on MS MARCO and other tasks in SGPT and related works).\n\n2. It shows that the proposed embeddings method (110M parameters) performs comparably to SGPT (5.8B parameters), but this does not answer the question whether a smaller SGPT model can achieve similar performance, and whether a larger model with the proposed method can outperform SGPT.\n\n3. Conventional embeddings are defined as the output of a trained neural network. So the embeddings can be efficiently generated when running inference. However the proposed embedding requires running training. This can be far more expensive and hard to compute even if the network is much smaller in size. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarify: The idea presented in this work is clear to understand.\nQuality: Low. See above.\nNovelty: The idea appears to be new, but the work does not justify why the proposed method is desired among alternatives.\nReproducibility: Code will be released according to the authors.",
            "summary_of_the_review": "It is an interesting idea to consider the change of neural network weights in the training process as embeddings. Understanding of the semantics represented by the neural network weights is also a fundamental problem of interests to the community.  However the work fails to establish why the proposed method is superior in comparison with conventional embedding methods. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2017/Reviewer_SjTe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2017/Reviewer_SjTe"
        ]
    },
    {
        "id": "rtYkYhbRr-",
        "original": null,
        "number": 4,
        "cdate": 1666699527874,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699527874,
        "tmdate": 1666699527874,
        "tddate": null,
        "forum": "4-aEhZnvNnk",
        "replyto": "4-aEhZnvNnk",
        "invitation": "ICLR.cc/2023/Conference/Paper2017/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on developing a new type of embeddings for texts (sentences or documents, but not words). \nThe proposed method's basic idea is to consider actual weights in the trained models (seem to assume language models).\nThen, the method computes the weight difference between the original and micro-tuned ones (fine-tuning one sample only).\nThe method is evaluated on several benchmark datasets for semantic computing similarity among texts.\nThe experimental results show comparable results with embeddings obtained from GPT-3.\n\n\n\n\n\n",
            "strength_and_weaknesses": "Strength:\n* The proposed method seems novel.\n* The task this paper tackles is an essential technology in the NLP field.\n\n\nWeaknesses:\n* I agree with the importance of developing a better text representation. However, the paper does not describe the merits of using the proposed method compared with other existing methods, such as SGPT. Therefore, I am wondering what the advantages of the proposed method are.\n* If I do not miss something, there is no explicit discussion of theoretical justification for the proposed method. Therefore, I am not convinced of the effectiveness of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality:\nIt is a bit hard for me to find actual usefulness or effectiveness of the proposed method.\nHowever, it may be possible that I miss something.\n\nClarity:\nIt seems that there are several notations that are not clearly explained in this paper.\nFor example, || in Equation 1.\nMoreover, I guess W represents the matrix. In this case, what do the absolute value of the matrix represent, such as |W| represents, and what is the meaning of E/|E| ?\n\nOriginality:\nThe originality seems very high since, to the best of my knowledge, I have never seen a similar method before.\n\n",
            "summary_of_the_review": "Overall, while the originality of the proposed method is high, the advantage of the proposed method seems to be limited.\nThis is because, as shown in the experiments and ponted out by the authors, the performance of the proposed method does not clearly surpass that of SGPT.\nThe authors claim that the proposed method can be compared using a smaller model but seems to require additional computational costs for micro-tuning.\nThis property seems to limit the usefulness of the proposed method.\nTherefore, I lean toward not recommending this paper to be accepted to the conference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I found no ethical concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2017/Reviewer_fZyy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2017/Reviewer_fZyy"
        ]
    }
]