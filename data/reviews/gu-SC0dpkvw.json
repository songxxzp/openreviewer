[
    {
        "id": "4mBswUbJU49",
        "original": null,
        "number": 1,
        "cdate": 1666694875565,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694875565,
        "tmdate": 1666694875565,
        "tddate": null,
        "forum": "gu-SC0dpkvw",
        "replyto": "gu-SC0dpkvw",
        "invitation": "ICLR.cc/2023/Conference/Paper3993/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents oblivious sketches for logistic regression with significantly improved sketching sizes both in terms of $d$- the dimension of the data and $\\mu$ the measure of complexity of compressing the data. The paper also gives sketches for the $\\ell_1$ regression and variance regularized logistic regression. Theoretical claims are supported by empirical evaluations on both real and synthetic data.",
            "strength_and_weaknesses": "Strengths:\n1. Vast improvements in oblivious sketching dimension for logistic regression compared to previous works.\n2.  Sketch for variance regularized logistic regression\n3. Empirical evaluations\n\nWeakness:\n\nThe only weakness I feel is that the paper may be difficult to read for those who are unfamiliar with the related works. There are a few points I mention in the section below which might help improving the clarity",
            "clarity,_quality,_novelty_and_reproducibility": "It would perhaps be more useful to describe the entire process of sketch and solve as an algorithm in place of describing it in a paragraph. There does not seem to be any empirical validation for the $\\ell_1$- regression problem. I could not find details of how synthetic data is generated.\n\nAlthough sketches have been applied to logistic and $\\ell_1$ regression in earlier works, the improvement obtained here is vast. I did not check the proofs, but the technical ideas given in the main paper appear convincing. ",
            "summary_of_the_review": "Overall, the paper is more of theoretical nature and will be of great interest to the researchers working in the \"sketch and solve\" paradigm. Also, logistic regression being a very important problem in machine learning, the paper would be of interest to broader community too. I recommend an accept",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3993/Reviewer_mC84"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3993/Reviewer_mC84"
        ]
    },
    {
        "id": "EyIPGOoFYp",
        "original": null,
        "number": 2,
        "cdate": 1666728359843,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666728359843,
        "tmdate": 1666728359843,
        "tddate": null,
        "forum": "gu-SC0dpkvw",
        "replyto": "gu-SC0dpkvw",
        "invitation": "ICLR.cc/2023/Conference/Paper3993/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper improves upon previous oblivious sketching and turnstile streaming results for $\\ell_1$ and logistic regression, achieving constant factor approximation, and an efficient algorithm in the sketched space. They demonstrate:\n\n1. Based on a modification of Munteanu et al. (2021) (with significant change of the analysis), they are able to achieve an $O(1)$-factor approximation algorithm with sketch size $\\tilde{O}(\\mu d^{1+c})$ for any $c > 0$, for the problem of logistic regression. Here $\\mu$ is a data-dependent parameter which captures the complexity of compressing the data for logistic regression. \n\n2. To obtain a $1+\\epsilon$ guarantee, they demonstrate that a sketch size of $(\\mu d \\log(n)/\\epsilon)^{O(1/\\epsilon)}$ is sufficient. \n\n3. They show that their sketch can also approximate variance-based regularized logistic regression\nwithin an $O(1)$ factor if the dependence on $n$ in the sketching dimension is increased to $n^{0.5+c}$ for\nany $c > 0$.\n\nThe authors then experimentally demonstrate the performance of their algorithms in comparison to prior work, with performance usually better, and just a little worse than prior work on one benchmark dataset. \n\n\n\n",
            "strength_and_weaknesses": "Strengths: \n1. This is an interesting paper with a clear improvement over prior work. \n2. The analysis involves several new ideas.   \n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality and Clarity: The paper is well-written and clear. \nNovelty: The paper introduces a new analysis for an existing algorithm, slightly modified. \nReproducibility: The authors point the reader to prior work for code that can be modified to run the experiments. ",
            "summary_of_the_review": "I think this is a good paper and vote to accept it. The paper produces a clear improvement over prior work, building off the work while complementing it with a significant change of analysis. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3993/Reviewer_bsu6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3993/Reviewer_bsu6"
        ]
    },
    {
        "id": "iOsqsvkOJC",
        "original": null,
        "number": 3,
        "cdate": 1667252383610,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667252383610,
        "tmdate": 1667252383610,
        "tddate": null,
        "forum": "gu-SC0dpkvw",
        "replyto": "gu-SC0dpkvw",
        "invitation": "ICLR.cc/2023/Conference/Paper3993/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the problem of using sketching techniques for logistic regression. It builds on the previous work of Munteanu et al.'21 which developed an approach to sketch n datapoints in d dimensions to poly(\\mu,d,log n) weighted datapoints (in d dimensions) such that solving the logistic regression problem on the smaller dataset gives a log(n) approximation to the function value obtained by the solution on the original dataset. Here, \\mu is a data-dependent parameter which captures the complexity of sketching. Though Munteanu et al.'21 obtain a poly(\\mu,d,log n) sized sketch, the polynomial dependence on d is not optimal. The current paper closes this gap and shows that a sketching dimension only slightly superlinear in d suffices to obtain a constant approximation to the logistic regression problem. Similar approximation results are also obtained for the L1 loss. The improvement comes from a small modification in the algorithm, and a much tighter analysis. The modification in the algorithm is to hash each element to multiple buckets instead of a single bucket in the CountMin Sketch.\n\nThe paper validates the algorithm via similar experiments as in Munteanu et al.'21. The proposed algorithm generally performs better than the previous approach in the experiments.\n\n",
            "strength_and_weaknesses": "Strengths:\n\nThe proposed algorithm is a simple modification of the previous approach, but comes with much stronger guarantees and achieves better results in practice.\n\nWeaknesses:\n\nMy main concern is that the paper is not written in a way such that it can really be self-sufficient without reading and understanding Munteanu et al.'21 and related literature. This makes the contribution harder to evaluate and makes it appear to be not as significant. There are several issues in the writing and discussion of the setup:\n\n1. To begin with, a much better job needs to be done to motivate the problem. Since the goal is to solve the logistic regression problem without storing all the datapoints, why is SGD/online gradient descent not the right approach? It would only require memory linear in dimension, even better than the proposed approach. It comes with regret guarantees too, are those sub-optimal compared to the approximation ratios which the paper obtains?\n\n2. Various approaches have been proposed to sketch solutions to linear classification problems, such as \"Linear and Kernel Classification in the Streaming Model: Improved Bounds for Heavy Hitters\", \"Sketching Linear Classifiers over Data Streams\", \"Random Projections for Classification: A Recovery Approach\", \"Finding needles in compressed haystacks\", \"Compressed Classification from Learned Measurements\", how do the sketches in these paper compare with the proposed approach? It appears to me that these papers compress the dimensionality d of the datapoints, whereas the current paper compresses the number of datapoints n, but some more discussion of the pros and cons would be useful to situate the work better.\n\n3. The discussion of related work is often that as well organized. For e.g. Sec 1.2 on L1 regression combines various discussions of the current paper with previous approaches in a way which makes it a bit hard to follow the thread.\n\n4. The paper would benefit from an algorithm box with a detailed description of the algorithm (perhaps also pointing out the part where the algorithm differs from Munteanu et al.'21).\n\n5. Similarly, it is nice that the authors included Section 3 on the proof overview, but I found it hard to follow without first going through Munteanu et al.'21.",
            "clarity,_quality,_novelty_and_reproducibility": "Most of this is discussed in the weaknesses above.",
            "summary_of_the_review": "In summary, I think this paper has a good contribution and achieves tight bounds for a natural problem. But especially for a venue like ICLR where much of the audience may be unfamiliar with the developments for this problem, it is important for the paper to be written and presented such that familiarity with prior work is not as essential. The paper falls short on this regard, and I would place it below the acceptance threshold for this reason.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3993/Reviewer_yfhf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3993/Reviewer_yfhf"
        ]
    }
]