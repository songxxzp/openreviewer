[
    {
        "id": "1238XyWQKx",
        "original": null,
        "number": 1,
        "cdate": 1666545967793,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666545967793,
        "tmdate": 1666545967793,
        "tddate": null,
        "forum": "RPVgoRFYWHB",
        "replyto": "RPVgoRFYWHB",
        "invitation": "ICLR.cc/2023/Conference/Paper5694/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed neural activation patterns as a replacement for adversarial robustness specifications to reason about the robustness of the network. The paper argues that this resolves the problem that test samples don't fall into certified regions of an adversarially robust verified network.",
            "strength_and_weaknesses": "# Strengths:\n- The paper highlights an important issue in adversarial robustness certification (the adversarial robustness verification only covers a minuscule part of the entire input space).\n- The paper introduces a novel set of specifications and shows that it covers a larger part of the test inputs.\n\n# Weaknesses:\nThe main issue with the submission is the lack of proper evaluation of the proposed specifications. Particularly, the paper validates the specification on a 2D example and MNIST. The MNIST dataset is severely outdated and known to have artifacts that are not present in other datasets, such as prototypical samples or high certifiable accuracy (compared to, for instance, CIFAR where the top accuracy and the top certified accuracy have a huge gap).\nAdditionally, the paper only assesses MLP networks. \nConsequently, the paper provides insufficient experimental validation of whether the proposed specifications are actually useful and reasonable in practice. It is vital to check if the hypothesized NAP specification works on larger and more diverse datasets, different types of data, and different structures of networks \n\nMoreover, neural activation patterns have already been used in the verification literature of neural networks [1], making the concept less novel.\n\n\n[1] Henzinger et al. Outside the Box: Abstraction-Based Monitoring of Neural Networks. 2020",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is clear. The parts and motivation about why dominance may matter is neural networks could be explained a bit better (e.g., with examples) as it seems to be the main premise of the NAP specifications.",
            "summary_of_the_review": "Interesting concept of overcoming the problem of certified adversarial regions having little relevance for the test data. Insufficient experimental validation of whether the proposed specifications are actually reasonable in practice.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5694/Reviewer_rBXu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5694/Reviewer_rBXu"
        ]
    },
    {
        "id": "Xwc1tNKZh2m",
        "original": null,
        "number": 2,
        "cdate": 1666735862930,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735862930,
        "tmdate": 1666735862930,
        "tddate": null,
        "forum": "RPVgoRFYWHB",
        "replyto": "RPVgoRFYWHB",
        "invitation": "ICLR.cc/2023/Conference/Paper5694/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a novel specification for neural network verification,\nwhere instead of checking whether sets of inputs are classified equivalently by\na network, it concerns checking the robustness of networks when their behaviour\nis restricted to certain neural activation patterns.",
            "strength_and_weaknesses": "+ The specifications introduced present in my opinion a step towards obtaining\nstronger neural network robustness guarantees when used in conjunction with\nexisting specifications.\n\n- Unsound claim that standard specifications are impractical for real world\n  applications. I think that checking small perturbations radiuses which do not\n  alter the semantics of the input is important for any application.\n\n- Not surprising empirical results. As the specifications proposed are\n  restricted to certain activation patterns, it is expected that the\n  perturbation radiuses that can be verified will be bigger when compared to\n  standard specifications. \n\n- The evaluation is limited in that it only concerns one benchmark.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and presented. The contribution is not particularly\nnovel as the main contribution restricts specifications for neural network\nverification, thus providing a small increment to previous work. Still, the\nspecifications proposed are significant to check as in conjunction with\nstandard specifications for small perturbation radiuses can give stronger\nrobustness guarantees. The significance of this is  however often\noveremphasised by what I think are unsound claims pertaining to the\nimpracticality of existing specifications.",
            "summary_of_the_review": "Whilst the paper introduces novel neural network specifications that are\nimportant to check in practice, the overall novelty and significance of the\nempirical evaluation are weak.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5694/Reviewer_epj3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5694/Reviewer_epj3"
        ]
    },
    {
        "id": "q6GPsFeom2D",
        "original": null,
        "number": 3,
        "cdate": 1666837865690,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666837865690,
        "tmdate": 1666837918693,
        "tddate": null,
        "forum": "RPVgoRFYWHB",
        "replyto": "RPVgoRFYWHB",
        "invitation": "ICLR.cc/2023/Conference/Paper5694/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a novel robustness specification for neural networks. Unlike canonical specifications where pre-conditions are defined as constraints on the neural network input,  the specification used in this paper are defined as particular activation patterns. The paper presents a strategy to extract activation patterns, shows how the specification can be checked with existing verifiers, and how this specification achieves better coverage of the test set. ",
            "strength_and_weaknesses": "Strength:\n\nI really like this work. Adding activation patterns to the pre-conditions of the specification is a simple but clever extension. It seems to be able to cover more test images than the canonical way does, where the constraint is only put on the input. Instead of only having the activation patterns as the pre-condition, still specifying a (now larger) epsilon ball in the input region not only makes the specification more relevant (as it focus the verification on valid images of digits instead of some noise input) but also likely to make the verification task easier. \nThis work might also motivation development of verification techniques that can handle constraints on NAP more effectively. \n \nWeakness:\n\nThe experiment is performed only on small neural networks. The verification is in general challenge, and using tools other than Marabou is unlikely to suddenly allow us to scale to SoTA perception networks. However, I would still be curious to see how scalable the NAP extraction procedure is and what the neural specifications look like on more complex image dataset and networks.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and very clear. \nThe contribution is original to my knowledge. Could the author discuss the relation of their work to [a,b]?\n\n[a] Gopinath et al, \"Property Inference for Deep Neural Networks.\"\n\n[b] Sabour et al, \"Adversarial Manipulation of Deep Representations.\"",
            "summary_of_the_review": "Overall, I think the paper contains several good and novel ideas and could motivate future work both in application and verification techniques.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5694/Reviewer_uPcc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5694/Reviewer_uPcc"
        ]
    },
    {
        "id": "nSSS_92C3TU",
        "original": null,
        "number": 4,
        "cdate": 1666955428926,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666955428926,
        "tmdate": 1666955622568,
        "tddate": null,
        "forum": "RPVgoRFYWHB",
        "replyto": "RPVgoRFYWHB",
        "invitation": "ICLR.cc/2023/Conference/Paper5694/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "###  Problem\nThe paper tackles the problem of neural network verification, which aims to give guarantees of whether a trained NN follows certain specifications.\n\n###  Proposed method\nThe authors claim that the current adversarial robustness specification used by the verification community is improper. Their claim is that this specification does not allow verification of test set inputs as they lie outside the verified region.\n\nAuthors propose neural representation as specification. Essentially, they propose to use neural activation patterns as specifications.\n\n### Experimental Validation\nExperiments are conducted on the MNIST dataset, where it shows that with the proposed specification, the certified region can go well beyond adversarial examples and even verify test set inputs.\n\n### My one line understanding\nAuthors claim to have found a condition which:\n1. adversarial examples don't follow, so verification won't stop at them and will be able to go beyond\n2. test examples from same class follow, so test set examples can be verified",
            "strength_and_weaknesses": "## STRENGTHS\n1. Finding alternative specifications is a good direction to explore\n\n## WEAKNESSES\n\n1. Writing needs to be improved\n- Not put into context properly: Doing adversarial verification is not necessarily an *overfitted* specification, as you call it. There are many applications where we also want to verify against adversarial examples. I don't think you need to belittle previous work. Your work is complementary, and that is how you should write the paper.\n- I don't agree with the entire 'data as specification' vs 'neural representation as specification' angle.  Because if one looks at equations in Sec3.2, you have the same formulation, but an extra constraint/specification. So you can write we add a new specification, instead of saying one vs the other.\n- Many equations are not numbered. How should I refer to them?\n\n2. Strong and probably incorrect assumptions\n- What is the guarantee that test examples will definitely follow same NAP?\n- What is the guarantee that adversarial examples will not follow NAP\n- Isn't such an assumption against the whole point of verification?\n\n3. Weak experimental validation\n- One main limitation of adv verification methods is their inability to scale beyond small epsilons. This is especially critical for your work because for going upto the test inputs, you would require large epsilon. Current methods don't allow that. So your method might not work on bigger datasets at all. Whereas, adv verification will still work on big datasets with small epsilon. \n- The experiments are only conducted on the MNIST dataset. I feel that MNIST is too easy and that is why your assumptions hold there. Since your assumptions are so strong, can you validate them on a bigger dataset?",
            "clarity,_quality,_novelty_and_reproducibility": "The work is Novel.\n\nBut I doubt the correctness of the claims and lack of proper experimental validation. Please see my detailed answer in the previous question.\n\nQUESTION:\n\nIf you find a specification which all same class test examples follow, have you not solved ML?  If you have such a constraint, then just use that to make class prediction and your accuracy will be 100% ($- \\delta$)?",
            "summary_of_the_review": "It is important to look for alternate specification.\n\nBut the assumptions made in this work feel incorrect and are not properly validated, so I cannot draw a conclusion from the current version.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5694/Reviewer_F47b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5694/Reviewer_F47b"
        ]
    }
]