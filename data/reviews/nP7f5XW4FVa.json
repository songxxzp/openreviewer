[
    {
        "id": "LmE4AL38Jf",
        "original": null,
        "number": 1,
        "cdate": 1666389656742,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666389656742,
        "tmdate": 1669249042068,
        "tddate": null,
        "forum": "nP7f5XW4FVa",
        "replyto": "nP7f5XW4FVa",
        "invitation": "ICLR.cc/2023/Conference/Paper2513/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the transfer attacks in the context of federated learning (FL) where the attackers don't act maliciously during training but use the information they obtained to perform transfer-based adversarial attacks later on. The paper provides quantitative and qualitative results on the factors that affect the attack success on a simple FL setting.",
            "strength_and_weaknesses": "+ Covert attackers using FL training for reconnaissance and performing attacks later on is a totally valid threat model that is significant enough to study.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Poorly motivated threat model, why would the attacker perform a transfer attack when FL protocol when there are malicious clients who observe the victim model at every FL round?\n\n- Experiments are insufficient, vanilla PGD is not particularly transferable.",
            "summary_of_the_review": "1) The proposed threat model doesn't make too much sense to me. Malicious clients don't do anything but they observe the model at every round of FL to be able to send the server the models updates on their data. So, these clients have access to the actual victim model (or something very close to it from the later rounds of FL). Why would they then train another model to launch transfer attacks on the victim model? Can't they just performing an almoist white-box attack? Please tell me if I'm missing something, I'm willing to update my review.\n\nIf the setting is that the actual attacker knows the data of some of the clients (without being able to observe the FL models these clients train), then we can't call the clients malicious anymore, if anything they're compromised. In this case, I still can't say the contributions of this paper are significant enough because there's plenty of work studying how training set knowledge affects transferability [1,2]. Although these papers study centralized training, I don't think the results will be different enough to justify a new paper.\n\n2) A more interesting setting would be that the malicious clients are not used throughout the FL training. They participate in the first N round and become inactive afterwards. So, the malicious clients don't have access to final victim model but an intermediate model from the Nth round. Can the attacker leverage this intermediate model + data from the clients to improve the trainsferability to the final victim model? How does the attack success change as a function of N?\n\n3) I would recommend using attacks other than PGD. There's a lot of work on designing transferable attacks in various settings [3,4]. The risk of using PGD is that it probably underestimates the transfer success.\n\n\n[1] https://www.usenix.org/conference/usenixsecurity18/presentation/suciu\n[2] https://www.usenix.org/conference/usenixsecurity19/presentation/demontis\n[3] https://openaccess.thecvf.com/content_CVPR_2019/papers/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.pdf\n[4] https://openreview.net/forum?id=BJlRs34Fvr",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2513/Reviewer_bp6y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2513/Reviewer_bp6y"
        ]
    },
    {
        "id": "BNMRK-Nk__A",
        "original": null,
        "number": 2,
        "cdate": 1666581915545,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581915545,
        "tmdate": 1666582022337,
        "tddate": null,
        "forum": "nP7f5XW4FVa",
        "replyto": "nP7f5XW4FVa",
        "invitation": "ICLR.cc/2023/Conference/Paper2513/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper investigates black-box adversarial attacks in FL. The paper assumes the attackers control some clients, thus have access to some training data. The attackers can launch transfer-based black-box attack with the training data. The paper has implications for understanding the robustness of federated learning systems and poses a practical question for federated learning applications.",
            "strength_and_weaknesses": "Strength: \n- The research topic is important. Defending adversarial attacks in FL is useful in reality.\n\nWeakness\uff1a\n- The setting is unreasonable. The paper assumes the attacker controls some clients that can take part in the FL training and considers black-box attack where the attacker cannot access the model (i.e., does not know the model parameters). Since the malicious clients can take part in the training, they definitely know the model parameters, which contradicts with black-box attack.\n\n- Table 1 is confusing.\n    - (1) The paper says \u201cwe early stop at 85% for adversarial training\u201d, but the reported acc of R50 (adv) is 81.23 (centralized) and 80.05 (federated).\n    - (2) Since same-acc shows the early-stopped results, it should be lower than regular. However, in federated CNN (adv), the adv. acc of same-acc (25.5) is higher than regular (24.35). Can you explain why?\n    - (3) The paper conducts a \u201cfair\u201d comparison (same-acc) and says \u201cwhen both models reach a comparable clean accuracy, federated model shows greater robustness against white-box attack compared with the centralized model.\u201dI don\u2019t think this experiment is fair: no evidence proves that when both models reach a comparable clean accuracy, they reach the same training stage. Previous studies [1] also show robustness may be at odds with accuracy. Thus, there is no strong relationship between acc and adv. acc.\n\n\n- The paper conducts extensive experiments but lacks theoretical analysis. \n\n\n[1] Tsipras D, Santurkar S, Engstrom L, et al. Robustness may be at odds with accuracy[J]. arXiv preprint arXiv:1805.12152, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not novel.\nThe code is not given, thus I don\u2019t think it is easy to reproduce.\n",
            "summary_of_the_review": "The paper considers transfer-based black-box adversarial attack in FL. However, the setting is unrealistic. The experiments are confusing. Thus\uff0c I think the paper is below the bar of ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2513/Reviewer_wDFS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2513/Reviewer_wDFS"
        ]
    },
    {
        "id": "T0BKZL28HF",
        "original": null,
        "number": 3,
        "cdate": 1666832536900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666832536900,
        "tmdate": 1666874979845,
        "tddate": null,
        "forum": "nP7f5XW4FVa",
        "replyto": "nP7f5XW4FVa",
        "invitation": "ICLR.cc/2023/Conference/Paper2513/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper delivers an empirical assessment of adversarial transferability between FL trained model and centralized models, between limited data trained model and full data trained model. Based on the empirical evidence, it conjectures that FL models are more robust and their attack possesses high transferability, and believe it is caused by two factors: the decentralized training on distributed data and the averaging operation.",
            "strength_and_weaknesses": "Strength: The paper discusses an interesting and important matter. It conducts detailed simulation studies, aiming to reveal how transferability is affected by FL learning.\n\nWeakness: It seems that the results from CNN and ResNe50 are not consistent, hurting the credibility of the author's arguments. Such inconsistency somehow suggests that architecture plays a role.\n\nDefinition of transfer accuracy (T.Acc): what is ||x||? Although all simulation presents the results about the T.Acc, there is no discussion on the trend of T.Acc. Then what is the purpose of introducing this notation?\n\nPlease double check the legend in figure 2. The paper claims that \"T.Rate against the federated model increases till 20 users and decreases.\", but the figure shows that T.Rate against the centralized model increases till 20 users and decreases.\n\nAs an empirical work, I suppose the current scale of the experiment is not big enough to convince readers.\n\nFigures in Section 5, i.e. Fig 3 & 4, display a complicated trend rough than the claimed monotone changing. It seems some in-depth messages are missing.",
            "clarity,_quality,_novelty_and_reproducibility": "The experiment details are clear enough for reproducibility.\nThe writing is in general good.",
            "summary_of_the_review": "I appreciate this attempt to understand adversarial transferability in FL setting. However, I sense the current empirical evidence is not convincing enough.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2513/Reviewer_zdkS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2513/Reviewer_zdkS"
        ]
    }
]