[
    {
        "id": "2TtIfXF_nR",
        "original": null,
        "number": 1,
        "cdate": 1666273300533,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666273300533,
        "tmdate": 1666273300533,
        "tddate": null,
        "forum": "-i73LPWa3bD",
        "replyto": "-i73LPWa3bD",
        "invitation": "ICLR.cc/2023/Conference/Paper1284/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a hyper-network for FNO\u2019s that allow it to predict intermediate future states. These are used to augment the loss for a better trained model. \n",
            "strength_and_weaknesses": "\ns: the result analysis is interesting and quite insightsful.\n\ns: the method and loss seem to be well derived, and are sensible.\n\ns/w: The results are not strong enough. The method does roughly hover around the baseline, getting either marginal improvements or similar results.\n\nw: the writing is ok but could be improved. The introduction could do a better job at describing the problem and the issues in earlier works. This paper is quite incremental by focusing largely on FNO, and making it more powerful. The paper could try to position its contributions on the wider context better.",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity adequate, but could be improved. Quality adequate. Novelty adequate. Reproducibility high. \n",
            "summary_of_the_review": "This is an ok paper with a sensible idea, but ultimately I feel this is not enough and nothing really stands out. The method is simple and incremental, and the results are not impressive enough.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_ykCW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_ykCW"
        ]
    },
    {
        "id": "Md9_sTYpf2s",
        "original": null,
        "number": 2,
        "cdate": 1666312101190,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666312101190,
        "tmdate": 1666312101190,
        "tddate": null,
        "forum": "-i73LPWa3bD",
        "replyto": "-i73LPWa3bD",
        "invitation": "ICLR.cc/2023/Conference/Paper1284/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Fourier Neural Operator (FNO) is a data-based approach for learning an infinite-dimensional mapping, which is helpful for numerically solving PDEs. The paper extends FNO to an evolutional setup. FNO can be used in an evolutional setup too, for which one can learn a mapping that maps the solution at time t to that at time t+dt, and then iterate this mapping. This paper however aims at directly learning the evolutional semigroup $\\Phi(t): \\mathcal{H} \\to \\mathcal{H}$, as a function of both the solution/initial condition and $t$. For this, properties of a semigroup are encouraged by the construction of loss and the time-dependence was accounted for by a specific hyper-network architecture. Interesting results are empirically shown.",
            "strength_and_weaknesses": "Strength: \nOverall, this is an interesting paper. I especially enjoy the construction of the loss, which prompts what is learned to be like a semigroup.\n\nWeakness: \nI wish more empirical results are provided for a more comprehensive evaluation of the performance. I hope I didn\u2019t read incorrectly, but at this moment I have several questions: \n\n(1) Has the method been compared to iterations of vanilla FNO? That is, FNO for characterizing the evolution map over time-step dt, and then iterated T/dt times for reaching time T? I hope the new results still outperform FNO used in this way, for both small dt and large dt. \n\n(2) What kind of Navier-Stokes problem is being considered? What is the initial condition, what should the exact solution look like, and what does the learned solution look like?\n\n(3) In existing methods like NODE, time dependence was often just used as a part of the input of a neural network. Is it possible to justify, experimentally, why the new hierarchical hyper-network structure is better? Is there any performance/computational-cost trade-off?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The loss seems innovative. The architecture is not new but its application to evolutional PDE solve seems new.\n\nRegarding quality, perhaps it is better to weaken some of the claims. For instance:\n\n* Abstract writes \u201cThe evolution of dynamical systems is generically governed by nonlinear partial differential equations\u201d. But it is not \u201cgenerically\u201d. An example is https://en.wikipedia.org/wiki/Tent_map .\nAnother example is non-local \u201cPDE\u201d.\n\n* Introduction paragraph 2: \u201call analytical techniques fail\u201d. This is false and disrespecting an entire field (e.g., [Foias, Manley, Rosa, Temam. Naiver-Stokes equations and turbulence. 2001]).\n\nThese are easy to revise though.\n",
            "summary_of_the_review": "Overall I feel this is an interesting paper. Simple but good idea, which can be a solid step toward an important problem (ML for PDE solve). I still have some reservation at this moment due to comments made above about the results, but I can be convinced.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_rv2Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_rv2Z"
        ]
    },
    {
        "id": "aHsqC2BJit",
        "original": null,
        "number": 3,
        "cdate": 1667022231586,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667022231586,
        "tmdate": 1667022231586,
        "tddate": null,
        "forum": "-i73LPWa3bD",
        "replyto": "-i73LPWa3bD",
        "invitation": "ICLR.cc/2023/Conference/Paper1284/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work considers neural operators in the semisupervised setting. It extends Fourier neural operator with a hypernetwork structure, which allows composition in time. The paper provides numerical experiments on 1d Burger equation and 2d and 3d Navier-Stokes equation. The proposed method has a slightly improved error rate but better interpolation performance in time.",
            "strength_and_weaknesses": "Strength:\n- extend operator learning to semisupervised learning\n- new hyper-network architecture\n- continuous interpolation in time\n\nWeakness:\n- learning accuracy at the supervision time-points is not very impressive (vs FNO+)\n- need more justification for a semi-supervised learning setting.\n\nComment:\n- Is this model data efficient? Maybe the proposed method will show an advantage at a smaller amount of data sets.\n- How does the continuous-time learning compare with the standard autoregressive model (t ->t+1, e.g. Markov neural operator)? The autoregressive automatically enforces composition in time. Since the Navier-Stokes is markovian, one probably does not need history. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall understandable, but the clarity can be improved. The novelty is not very impressive.",
            "summary_of_the_review": "Overall, I think the paper provides a concrete study, but its contribution and novelty may not meet the threshold. Therefore, I think this work is marginally below the bar for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_2qHC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_2qHC"
        ]
    },
    {
        "id": "PRHiIYGF8M",
        "original": null,
        "number": 4,
        "cdate": 1667230281110,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667230281110,
        "tmdate": 1667230281110,
        "tddate": null,
        "forum": "-i73LPWa3bD",
        "replyto": "-i73LPWa3bD",
        "invitation": "ICLR.cc/2023/Conference/Paper1284/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a hyper-network to solve partial differential equations. ",
            "strength_and_weaknesses": "The strengths of this paper are:\n\n$\\mathbf{1}.$ The novelty of separating time and space by the hyper-network architecture. \n\n$\\mathbf{2}.$ This paper is clearly written and well organized, with detailed explanation of theories and experiments. \n\nThe weakness of this paper are:\n\n$\\mathbf{1}.$ The proposed approach is a combination of known methods, although it might be novel to use hyper-network.\n\n$\\mathbf{2}.$ The baseline used in the experiments might not be strong enough. For example, why using FNO and/or MGNO in the experiments? Why other network-based methods were not used? Also, is it possible to use some numerical methods as a baseline? The main concern is whether or not the chosen baseline is good and comprehensive to use as references. \n\nOther concerns (minor if compared with the above weakness):\n\n$\\mathbf{1}.$ The literature review claims the model base method, such as (Raissi 2019), does not require training dataset. Physics informed network does require training the network with data, as most of the network-based method do. \n\n$\\mathbf{2}.$  The literature review also claims that the methods referred do not predict intermediate time, which could be false. Even if it is right, there exists methods which can predict solutions at intermediate time. \n\n$\\mathbf{3}.$ The loss function presented in Eq (17) is a simple addition, which could be improved by tuning hyper-parameters as a convex or other types of combinations. ",
            "clarity,_quality,_novelty_and_reproducibility": "$\\cdot$ This paper is clearly written and organized. \n\n$\\cdot$ The novelty of this paper is not obvious as it merely combines known methods. It might be ok to qualify a good novelty as long as there are significant contribution from other aspects, such as empirical results, etc. However, $\\mathbf{1}.$ It is not clear how innovative improvement on the architecture of hyper-network; $\\mathbf{2}.$ it is not convincing enough the proposed method outperforms other methods as the chose of baseline is limited; $\\mathbf{3}.$ predicting intermediate time point is not a novel contribution. Based on the above three points, along with the combo of known methods, the novelty is not obvious. \n\n$\\cdot$ The reproducibility is feasible. ",
            "summary_of_the_review": "As stated in the above comments, this paper might not deliver a solid contribution, and the novelty could be limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There is no ethics concern in this paper.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_cCio"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_cCio"
        ]
    },
    {
        "id": "NzAxzhaF93",
        "original": null,
        "number": 5,
        "cdate": 1667474388855,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667474388855,
        "tmdate": 1667474388855,
        "tddate": null,
        "forum": "-i73LPWa3bD",
        "replyto": "-i73LPWa3bD",
        "invitation": "ICLR.cc/2023/Conference/Paper1284/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents an approach to modeling PDE solutions using a trainable hyper-network that generates the parameters of an FNO network. The hyper-network gets as an input only the desired continuous time t (>0) and the FNO transforms an initial condition under the dynamics specified by the output of the hyper-network. While the performance of this method at the discrete training time steps already improves on FNO, the main innovation is allowing for interpolated solutions at any continuous time. ",
            "strength_and_weaknesses": "The main strength of the paper is the pursuit to learn the underlying dynamics over the continuous time domain. As the authors mention in the discussion section (Sec. 6), this might indeed be very useful as a reduced-order model e.g. of the NSE. \n\nComments:\n- Given that the main comparison baseline is FNO and FNO is also a major building block in this architecture, I would have expected a brief introduction to the used FNO formalism. \n- Where are CNNs and GNNs? Given the recent popularity of GNNs for the solution of PDEs (e.g. https://arxiv.org/abs/2202.03376, https://arxiv.org/abs/2002.09405) as well as CNNs (e.g. https://arxiv.org/abs/1810.08217), one should at least mention these lines of research in the \"Related Work\" section and justify why they are not (or are) relevant here. \n- I see the intention to learn meaningful interpolated solutions of PDEs via the hyper-network, but given the lack of results on extrapolation over longer horizons, e.g. t=10...100, I would speculate that the results are not promising. Any comment on that?  Given the practical relevance of extrapolating over longer times, this aspect should be addressed somewhere. And this regime is something that has already been studied by the so-called direct-time methods, e.g. in this workshop paper by Meyer et al., 2021 using GNNs: https://arxiv.org/abs/2112.10296. I understand that all the PDEs you consider are dissipative, thus too long trajectories wouldn't make sense, but why not look at some inviscid equations (e.g. Burgers, Euler) and discuss e.g. the survival time as done in https://arxiv.org/abs/2202.03376.\n- I like the loss terms defined in the paper quite a lot: they incorporate all possible types of interpolation. However, I know other papers that do similar things (ignoring L_final and L_initial for a second): The construction of L_comp^(P) is very closely related to the \"temporal bundling\" presented here https://arxiv.org/abs/2202.03376, and L_inter reminds me of the \"pushforward training\" from the same paper. I'm sure that there are more analogies from other papers.\n\nTwo minor problems I detected are:\n1. some formatting issues with the equations leading to \"Equation equation [...]\"\n2. \"Miltiple Graph Neural Operator\" -> Multipole Graph Neural Operator\"",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to digest. As mentioned in the main review, some more information on FNO could be useful.\n\nI haven't had the chance to work with FNO-based approaches yet, thus I cannot evaluate the figures and tables more than what I see in the paper. The presented results look ok to me and the improvement over vanilla FNO in the interpolation regime seems quite clear.\n\nRegarding novelty, the paper seems to introduce the combination of hyper-networks and FNO for the first time. And although the hyper-network and FNO networks seem to be \"off the shelf\", given the rather specialized loss makes for a novel approach.\n\nThe supplementary material seems to do the job in terms of reproducibility. The 1D data is downloadable, and the 2D and 3D data can be generated using PhiFlow. I successfully started a training run on the 1D data.",
            "summary_of_the_review": "The idea of the paper is very interesting, but out of the 21 mentioned references, I didn't find a single one from 2022. This suggests that the authors lack knowledge of the recent literature, which includes e.g. GNNs. This issue and the weak justification of the chosen methods and experiments (see above) are the reason why I believe that this paper is not yet ready for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1284/Reviewer_NVyD"
        ]
    }
]