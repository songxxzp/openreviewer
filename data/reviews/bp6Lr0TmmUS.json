[
    {
        "id": "wwk3gUt2Rl",
        "original": null,
        "number": 1,
        "cdate": 1666699303198,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699303198,
        "tmdate": 1668345786527,
        "tddate": null,
        "forum": "bp6Lr0TmmUS",
        "replyto": "bp6Lr0TmmUS",
        "invitation": "ICLR.cc/2023/Conference/Paper4317/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of data heterogeneity in federated learning. It suggests mathematical analysis based on the \u201cworst-case margin\u201d theory to measure the generalization contribution of clients\u2019 FedAvg updates. Based on theoretical results authors propose decoupling the neural network into 2 parts: feature extractor and classifier in a desire to decrease gradient dissimilarity. Experimental evaluation shows the benefits of the proposed method on (synthetic) heterogeneous federated datasets.",
            "strength_and_weaknesses": "## Strengths\n\n1. The paper is trying to address a critical problem in federated learning.\n\n2. Experimental evaluation is quite extensive and done accurately.\n\n3. Theoretical concepts suggested for analysis seem novel in this setting.\n\n\n## Weaknesses\n\n1.  Problem origin: it is said that FedAvg suffers from client drift caused by heterogeneous data. The issue is that the so-called \u201cclient drift\u201d is a consequence of the optimization method which relies on local training. In the distributed case without multiple local SGD steps, the variance can be eliminated via known control variate techniques, and even with local steps, it is not a problem from an optimization viewpoint. The current theory is not able to explain the bad performance of these methods in the setting of training non-convex deep neural networks. So, it is hard for me to agree with the original problem formulation.\n\n2. The authors make an assumption that feature distributions can be approximated by a Gaussian referring to some previous works. Why is it the case? Was this assumption tested in practice e.g. via a statistical test? Is it the case only for a particular dataset and model architecture? Can the proposed method be combined with standard federated learning techniques like differential privacy and secure aggregation?\n\n3. Experiments were conducted only on synthetically created small federated versions of centralized datasets. It is not clear whether the conclusions can be generalized to natural federated datasets like Federated Extended MNIST, Stackoverflow, etc.\n\n4. Experimental results are obtained for single runs which limits their credibility. In addition, the curves in Figure 2 (b) are almost indistinguishable and the benefits of the proposed approach are not clear.\n\n5. Important baselines like FedAdam/FedAdagrad are missing. They were shown to tackle well the natural heterogeneity in federated datasets. Can the method from this work be combined with adaptive optimizers?",
            "clarity,_quality,_novelty_and_reproducibility": "Section 4 is hard to follow and not clear. The ideas look original in the federated learning context. However, I think that this part needs a lot of clarification to make it ready for publication. Next, I try to list and unriddle some of my concerns.\n\n- What do $\\rho(\\cdot)$ and $y_m$ look like mathematically? These objects (their dimension and the space they come from) were not properly defined. Is the guess that $y_m$ is a one-hot encoded vector right? Then, I have a question for definition 1: how $\\rho(\\theta; \\xi\u2019)$ can possibly be equal to $y$ in practice except in the case when the network is absolutely sure about the prediction and the scores for all other classes equal to zero. \n\n- Definitions 1, and 2 need much more detail and proper explanation. As far as I understand they are not standard in the community and I have not found these exact objects in the referenced papers, (Koltchinskii & Panchenko, 2002, Franceschi et al., 2018). \n\n- What does notation $\\Delta : \\mathbf{L}(\\mathcal{D}_m)$ in equation (2) mean? What is $\\mathcal{J}$ from definition 2? How $\\hat{f}$ on (page 5) is defined? How is equation (6) obtained?\n\n\n- The sentence on page 6\n>\u201cIntuitively, we prefer the client where the generalization contribution can be lower bounded.\u201d \n\nis not clear to me. Why is it actually the case?\n\n- The client drift is defined as $\\frac{1}{\\left|\\mathcal{S}^{r}\\right|} \\sum_{i \\in \\mathcal{S}^{r}}\\left\\|\\bar{\\theta}-\\theta_{i}\\right\\|$\n and referred to the work of Karimireddy et al., 2019. However, in the original SCAFFOLD paper, this term is defined differently.\n\n- Conclusion on \u201creducing the high-level gradient dissimilarity is more important than the low-level\u201d on page 9 is drawn from one experimental example (ResNet-18 trained on CIFAR-10) and needs more evidence.\n\n- In Appendix Table 6 shows improved results for baseline methods when using momentum. It was previously shown (Hsu et al., 2019) that momentum can greatly help in the more synthetically heterogeneous case and parameters need to be adjusted for various heterogeneity levels. I would like to ask why the authors did not compare their approach to the stronger baselines.",
            "summary_of_the_review": "The main part of the paper lacks a lot of details which makes it very hard to understand clearly the theoretical contributions. That is why it is hard to judge their significance based only on text claims.\n\nThe empirical section lacks important baselines and relies on a very restrictive assumption of gaussian features distribution.\n\nAlthough, if my concerns are addressed and questions are clarified I am open to raising my recommendation.\n\n\n**Minor comments and typos**\n\n- Is the problem setting limited only to classification?\n- Typos: \u201cvaraince\u201d -> variance on pages 6, 16\n    - \u201cLearnign\u201d -> learning on page 20\n- Looks like a missed word on page 7 in the sentence \u201cWe conjecture that CIFAR-100 datasets \u2026 , **leading to the results**\u201d.\n- SCAFFOLD work was already published and could be referred to as an ICML paper.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4317/Reviewer_UyvV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4317/Reviewer_UyvV"
        ]
    },
    {
        "id": "WfyspMtSZ_",
        "original": null,
        "number": 2,
        "cdate": 1667155708807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667155708807,
        "tmdate": 1667167359799,
        "tddate": null,
        "forum": "bp6Lr0TmmUS",
        "replyto": "bp6Lr0TmmUS",
        "invitation": "ICLR.cc/2023/Conference/Paper4317/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the generalization contribution of local training in Federated learning. Their results show the key to promote generalization (conditional Wasserstein distance) and therefore this paper proposes decoupling the deep models for harnessing client drift and protecting privacy. Experimental results show FL with decoupled gradient dissimilarity is robust to data heterogeneity.\n\n",
            "strength_and_weaknesses": "\nStrength: This paper is well written and the topic of client drift is important and interesting. The theoretical result of local training generalization is sound and answers the question related to local training. \n\nWeakness: The communication cost should be reduced (e.g., quantizations) (although authors talked in the limitations.)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper talks about interesting and important topics in FL with good quality and clarity. The idea is novel as far as I know. \n",
            "summary_of_the_review": "Overall, I think that the studied local training generalization contribution in this paper is well-motivated. The idea behind the proposed algorithm is novel and effective.  Therefore, I recommend \u201caccept\u201d.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4317/Reviewer_fFS7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4317/Reviewer_fFS7"
        ]
    },
    {
        "id": "ciEOp7bZ72",
        "original": null,
        "number": 3,
        "cdate": 1667387538852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667387538852,
        "tmdate": 1670942271713,
        "tddate": null,
        "forum": "bp6Lr0TmmUS",
        "replyto": "bp6Lr0TmmUS",
        "invitation": "ICLR.cc/2023/Conference/Paper4317/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper addresses the problem of client drift in Federated Learning, a problem well-known in the literature to diminish the efficacy of FL methods. The authors propose to use a decoupling of features into low and high-level ones, they show that by grouping and sharing the low-level features among clients, gradient dissimilarity can be reduced, and thus performance gains can be obtained.\nIn practice, the low-level distribution is approximated by a Gaussian distribution to avoid high communication costs and make the algorithm feasible.\n\nThe authors also proclaim that this process preserves privacy.",
            "strength_and_weaknesses": "Strengths: the idea of decoupling features is not 100% new, but sharing the low-level features in the fashion proposed in the paper in addition to approximating the low-level feature distribution is a nice idea. Also, the experiments (I haven't gone through all the details of them) seem thorough, and most importantly they show that this method works better than other already-known methods.\n\nWeaknesses: The writing of the paper is not clear sometimes, and some statements are just wrong, for example, the remark after Th 4.1 the authors say that Th4.1 implies \"the generalization contribution of the training distribution is\nexpected to be large on the training set\", this is not implied by Th4.1 you simply expect from any good algorithm to perform well on data similar to what it was trained on, the second implication is also not implied by Th4.1.  The notations are hard to follow (the Gaussian distribution mean is denoted by sigma and the variance by mu is an example, another example is $\\zeta,y$ I would say $\\zeta = x,y$ but you keep using both, so in the distance do you include the label or it is just the distance between inputs). Also in theorem 4.1, I  would expect the learning algorithm to depend on $\\tilde{D}_m$ not on $D_m$, because if I understood correctly, you will be training on the sampled distribution $\\tilde{D}_m$ (it would have been better to introduce this before the theorem and explain exactly what you are doing: we sample $\\tilde{D}_m$ from $D_m$ and use it for training and so on...). One last thing is in the definition of the worst-case margin, you should explain why this would be a good measure of generalization (I would say you can make a link with SVMs, but you should explain it).\n\nAlso, the theory, in my opinion, is not very strong (maybe because of the notation, but there is also $\\tilde{D}_m$ and $D_m$). you are also proving one thing and using a different thing in the experiments (the Gaussian approximation is not included in your theory). Also, you proclaim that your approach preserves privacy but you don't prove it (because there can still be information leakages)\n\nOne final point about the theory: there is domain adaptation that has similar goals to the generalization part in your paper (training on one dataset and testing on another), so I think it will be useful for you to take a look at it. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not very clear, the notations make it hard to follow, and some statements are not logically correct (see above).\n\nThe proposed training algorithm is to the best of my knowledge new.\n\nThe code is available and all the details (to the best of checking abilities) are included.\n",
            "summary_of_the_review": "The paper has some new and interesting ideas, but the way it was written makes it hard to follow, also the theory needs some adjustments.\n In general, the idea is interesting and I would encourage the authors to polish the paper, make it more clear, and resubmit. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4317/Reviewer_3s3d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4317/Reviewer_3s3d"
        ]
    },
    {
        "id": "AtZNUQOFhRb",
        "original": null,
        "number": 4,
        "cdate": 1667532424991,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667532424991,
        "tmdate": 1667532424991,
        "tddate": null,
        "forum": "bp6Lr0TmmUS",
        "replyto": "bp6Lr0TmmUS",
        "invitation": "ICLR.cc/2023/Conference/Paper4317/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "One of the key challenges in learning from private data sources in a collaborative manner as in federated learning is the data heterogeneity (non-IIDness) among data shards. This causes drift among local models which slows down the convergences or entails a poor generalization on local distributions. This issue is further exacerbated in the presence of infrequent synchronization which is widely used in resource-limited scenarios such as low communication bandwidth to reduce the number of communication rounds. This led to an enormous amount of efforts such as clustered learning, personalization, or explicit drift reduction using control variates or regularization (often dynamic) techniques. \n\nThis paper focuses on improving the generalization by proposing an effective gradient correction method. To do so, the authors view local training as a generalization contribution problem, how local training of a specific client contributes to the generalization performance of the global model with respect to distribution of other clients. This can provide a mechanism to understand the contribution of each client at each communication round.  The authors quantify this by the conditional Wasserstein distance between clients\u2019 distributions. The authors propose a decoupling idea to jointly learn  a low-level model (a feature extractor network) and a high-level one (head or classifier)to construct a shared identical distribution in the feature space. From a theoretical standpoint, the authors show that the effect of non-IID ness (gradient dissimilarity) on generalization and convergence can be decoupled based on two distributions: features and labels. Then it is shown that estimating and sharing feature distributions improves the generalization by  reducing gradient dissimilarity.\n",
            "strength_and_weaknesses": "To understand the contribution of each client at each communication round, the basic idea is to  generalize the margin based discrepancy to incorporate the gradient (pseudo gradient between two consecutive communication rounds for each client to be precise) as defined in Eq. 2. Lower bounding this error can guarantee the update will benefit the other clients if the drift is properly mitigated. The analysis leads to the algorithmic contribution of the paper where authors propose to learn/iteratively update a common feature distribution (Guassian in their case) among all clients.\n\nThe paper studies with an interesting question, and proposes an interesting in-training metric that can help to quantify and guide the algorithm to mitigate data heterogeneity issues. However, in my opinion the proposed algorithm and execution of ideas suffers from serious issues that prevents me from giving it a high score:\n\n1. Data heterogeneity and client drift might exist due to numerous issues: covariate shift,  model shift, or both. It seems learning a shared model for features can handle covariate shift, but it might fail in the presence of model shift. For example, consider the case that data at each client comes from a Gaussian distribution with mean 0 and covariance \\Sigma (shared among all clients), but a different regressor \\theta_i. In this case I am not sure how drift among high level parts can be mitigated. Note that the original metric considers discrepancy in both covariates and labels. I might be mistaken mostly due to the fact that authors use the same notation \\mathcal{D} when referring to both data distribution (feature, label) pair and solely feature distribution and further clarification would be very helpful. \n\n2. It is not clear how the proposed algorithm can improve the generalization theoretically.  While the decoupling idea is inspired from definition of conditional Wasserstein and decomposition of inter-client gradient dissimilarity into low and high level parts, but it would be nice to see how the iterative adaptation of these parameters and heuristics used to make the algorithm practical (such as Guassian) can indeed entail a good generalization. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-written and discusses the main idea in the context of existing works, but the presentation of the theoretical part can be improved by using better notation and adding more clarification. For example, in the statement of Thm 4, I originally thought \\Delta is missing but when I checked the proof I realized the authors abbreviated it. Also, it would be much nicer to be specific when referring to different distributions (features, or pair of features  and label).\n",
            "summary_of_the_review": "Overall, the paper studies an interesting question, proposes a novel metric to quantify in-training generalization, but IMHO is poorly executed and  the quality of exposition seems to be low which makes me lean toward rejection.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4317/Reviewer_eaTq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4317/Reviewer_eaTq"
        ]
    }
]