[
    {
        "id": "3CCVq1Y4G6n",
        "original": null,
        "number": 1,
        "cdate": 1665689829976,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665689829976,
        "tmdate": 1665689829976,
        "tddate": null,
        "forum": "uH_RlkvQMUs",
        "replyto": "uH_RlkvQMUs",
        "invitation": "ICLR.cc/2023/Conference/Paper3583/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new dataset and visual question answering task to find differences between chest xray images of the same patient taken at two different points in time. \n\nFurthermore, the authors develop a graph representation learning model that uses domain knowledge to solve this task, and provide a baseline for future extensions. This model extracts anatomical/disease features from both images, and passes them as input to a multi-relationship graph module that extracts a final image representation. The features for both images can then be subtracted and used in a language model that generates the final answer. \n\n ",
            "strength_and_weaknesses": " \n\n**STRENGTHS** \n\n1. The authors prepared a large scale dataset that can be used to advance multi-modal models working on medical text/images \n\n2. Most ML applications for chest xray analysis only consider a single chest xray image, which is limiting as lots of medically relevant information can be obtained comparing studies taken at different point in times. This model on the other hand is able to mimic what radiologists do when comparing studies to assess the progression of a disease.  \n\n3. How to embed medical domain knowledge in ML models is still a very open research questions. The knowledge-aware graph representation used in this paper is a novel interesting idea.\n\n \n\n**WEAKNESSES** \n1. Given the heterogeneity of radiological reports, they are quite hard to process with rule-based methods such as the ones used by the authors. I would have like to see a larger validation set than the 100 examples used in the paper. \n2. The model relies on anatomical parts segmentation, which is quite challenging in chest x-rays (with large opacities/effusions anatomical parts might not even be visible). What is the classification performance of your anatomical parts classifier? How does it perform on diffuse diseases? \n3. Have you tried to use classifiers instead of a language model for some of the tasks 1-6? It would be interesting for example to see a classification baseline for the \"abnormality\" tasks, that would give an idea of how good your model is.\n4. What are the training times of the model? \n ",
            "clarity,_quality,_novelty_and_reproducibility": " \nThe introduced dataset is the first of its kind, and the ideas introduced in the model are novel to the best of my knowledge. \n\nThe paper is quite clear to read, although I could spot several typos. The citation style used in the paper is often confusing when citations are in the middle of a sentence, as the author's name seems being part of the sentence. For example in page 5 you say: \"Each word is tokenized and embedded with Glove Pennington et al. (2014) embeddings.\". It would be better to use \\citep in latex to get \"Each word is tokenized and embedded with Glove (Pennington et al, 2014) embeddings.\" ",
            "summary_of_the_review": "I believe this paper could be relevant for the ICLR community:\n1. The paper introduces a interesting large-scale dataset for vision/language medical tasks.\n2. The model developed by the authors is novel.\n\nI therefore argue for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3583/Reviewer_NUDS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3583/Reviewer_NUDS"
        ]
    },
    {
        "id": "XrjMHkmPA1p",
        "original": null,
        "number": 2,
        "cdate": 1666809058486,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666809058486,
        "tmdate": 1670110812695,
        "tddate": null,
        "forum": "uH_RlkvQMUs",
        "replyto": "uH_RlkvQMUs",
        "invitation": "ICLR.cc/2023/Conference/Paper3583/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work proposes a medical image difference VQA problem and constructs a MIMIC-Diff-VQA dataset from the existing MIMIC-CXR dataset. To perform the task, this work proposes a knowledge-aware graph and a multi-relationship graph, where the former takes each anatomical structure as a node in the graph and compares the image differences in each anatomical structure, and the latter adopts the spatial relationship, semantic relationship, and implicit relationship to compute the image-difference graph representations. The experiments on the constructed MIMIC-Diff-VQA dataset show that the proposed approach ",
            "strength_and_weaknesses": "Strengths:\n1. The proposed task/problem is new. The constructed dataset seems to be useful for the community.\nsound. \n2. The proposed approach can outperform several constructed baseline models.\n\nWeaknesses:\n1. The usefulness of the proposed task should be clarified.\n- What benefits can be brought by this task in clinical? And why?\n- Otherwise, I may think that this work just applies the techniques/tasks in computer vision (i.e., NS-VQA and image difference captioning) to medical images without understanding the clinical significance of the problem being addressed. \n- More importantly, the proposed task is very similar to the existing NS-VQA task [1], but the authors neither cite nor discuss this paper.\n- Meanwhile, no mention is made of the type of abnormalities for which this approach/task is suitable. In fact, since a chest X-ray may show many pre-existing conditions which may be anatomical abnormalities (e.g. scoliosis) but not the target of the chief complaint for which the report was ordered, it is important to quantify the type of abnormalities.\n\n2. The novelty of this work is limited.\n- First, as mentioned in Weakness 1, the proposed medical image difference VQA task is very similar to the existing NS-VQA task [1], but the authors neither cite nor discuss this paper. Therefore, it's important to clarify what benefits can be brought by this task in clinical. \n- The proposed approach and the arguments, i.e., \"This is consistent with the radiologist\u2019s diagnosis practice that compares the current image with the reference before concluding the report\" in the Abstract and \"When radiologists make a diagnosis, they usually compare the main one with a reference image to find their differences\" in the Introduction, are very similar to the existing work [2].\n- Figure 7(b) is directly borrowed from [3]. The authors should note it.\n\n3. The motivation is unclear.\n- The motivation for introducing the reference image is unclear.\n- Does there are any relationships between the designed questions and the reference image? \n- What are the contributions of the incorporated reference image?\n- What are the reasons to adopt a reference image to answer the questions?\n- Regarding the proposed approach, why does this work adopt the language model? What is the motivation?\n\n4. The experiments should be improved.\n- This paper only includes one baseline model for comparison. I strongly recommend the authors attempt to re-implement existing state-of-the-art methods in NS-VQA on the constructed dataset to show the effectiveness of the proposed approach.\n- Could you provide the results of the approach on different types of question types/abnormalities?\n-  I am also interested in knowing if the approach brings errors. And what type of errors does it bring? And why?\n\n5. Some details of the pre-processing of the constructed dataset are missing.\n- As the main contribution of this paper, it's necessary to provide more details about the dataset.\n- Rule-based methods: What is the motivation for adopting rule-based methods? How to adopt the rule-based methods to construct the dataset?\n- The MIMIC-CXR dataset includes two views of chest x-rays, how do you process them?\n- Does the proposed approach and task could be applied to the lateral view image? \n- Could you give some examples of the lateral view image?\n\n6. (Minor) This paper seems to be written in a bit of a hurry, and there is a lot of scope for improving the presentation of the paper. I strongly recommend the author check and revise the text carefully. \n\n\nReferences:\n\n[1] Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding. In NeurIPS, 2018.\n\n[2] Contrastive Attention for Automatic Chest X-ray Report Generation. In ACL, 2021.\n\n[3] When Radiology Report Generation Meets Knowledge Graph. In AAAI, 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity should be improved - there are several typos in the paper. I recommend the author check and revise the text carefully. The novelty is limited: 1) The usefulness of the proposed task should be clarified; 2) An existing work Contrastive Attention in the medical image field and NS-VQA task should be discussed. The reproducibility of this paper may be good, the key resources (i.e., code and data) will be available upon publication as promised in the Abstract. However, the experiments should be improved.",
            "summary_of_the_review": "Overall, the paper has provided a new task and a new large-scale dataset, but the usefulness of the proposed task should be clarified. Meanwhile, the novelty of this paper should be discussed. Before the rebuttal, I tend to reject this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3583/Reviewer_FdAD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3583/Reviewer_FdAD"
        ]
    },
    {
        "id": "egO7Djc7_zA",
        "original": null,
        "number": 3,
        "cdate": 1667876299410,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667876299410,
        "tmdate": 1667927488006,
        "tddate": null,
        "forum": "uH_RlkvQMUs",
        "replyto": "uH_RlkvQMUs",
        "invitation": "ICLR.cc/2023/Conference/Paper3583/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "First, the paper introduces a dataset for \"difference-aware medical visual question answering\". It has chest x-ray pairs from the same patient, with associated questions and answer labels for a VQA task. It is scraped from the existing MIMIC dataset. Though this is similar to \"image captioning\" task, the authors argue that medical imaging comparisons have distinct challenges, so this should be considered a distinct task. \n\nSecond, the paper proposes a model for solving this proposed task. This model uses a detector to identify anatomical regions used for feature extraction, which are then used in a knowledge graph model. \n",
            "strength_and_weaknesses": "Strengths:\n- The dataset is useful and well-motivated. The authors argue that radiologists really do evaluate images by considering differences in pairs of images from the same patient. \n- Furthermore, the dataset is real with clear future applications. Hopefully it can be used as a challenging benchmark beyond existing toy examples in similar tasks like 'image captioning'.\n- The method far outperforms the baseline. \n\nWeaknesses: \n- Concern about dataset correctness: Fig.1 and introduction paragraph 1 argue that prior VQA datasets in medical images have significant errors in their labels due to issues with their text mining strategies (for example, issues with rule-based systems). But paragraph 3 claims that similar approaches are used to generate the new dataset. It is good that some human verification was done (as explained in the appendix), but only 300 randomly chosen pairs were checked out of from 700,821. Also, the dataset is not yet released, so reviewers cannot check. This contribution would be stronger by a more thorough verification.\n- For the method, the ablations in Table 2 suggest marginal gains for the spatial and semantic graphs, so the model could be simplified. \n- MCCFormer (Qiu 21) is the only image difference captioning baseline, and it does badly on the proposed task. The authors argued this is because they work patch-wise and so cannot handle mis-aligned pairs (which isn't a problem for datasets like CLEVER-change). But the related work discuss \"birds-to-words\" dataset which has misaligned pairs, and it mention several works that perform well on it: (Yao 22) and (Yan 21). Therefore these works may be appropriate baselines to include. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: \n- Some errors in grammar (e.g. sentences beginning \"while the current state-of-the-art ....\" is an unfinished sentence). \n- Future versions should place citations inside brackets. Currently it's a little tough to read. \n- I found the model architecture and decsription difficult to follow. \n- In the section `Multi-Relationship Graph Module`, a number of symbols in the graph definition are left undefined, e.g. $\\varepsilon_{sp}$, and the symbols in the definition for vertices. (They can be inferred from context, but they should still be defined in the text). \n- CIDEr metric not explained.\n- The difference between contributions 2 and 3 is a bit unclear. \n- [minor] \"MCCFormer\" sometimes written as \"MMCFormer\"\n- [minor] opening quote brackets (\") are the wrong way around\n- [minor] spelling errors: \"relaiton\", \"perfomrd\",\n- [minor] should bold \"ours\" results in Table 3.\n\nNovelty: \n- The dataset novelty is good. \n- Unsure about novelty in the knowledge-graph model component\n\nReproducibility: \n- Unfortunately I cannot yet access the code or dataset, so I cannot rate reproducibility as a strength.",
            "summary_of_the_review": "The dataset is a useful contribution, and I hope it is used more widely, especially by general VQA researchers. This contribution would be stronger if it is verified more thoroughly (as discussed above). I have a concerns regarding baselines and clarity (see above). \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The dataset is built on top of existing public datasets, so this should not raise new issues.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3583/Reviewer_eNEk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3583/Reviewer_eNEk"
        ]
    }
]