[
    {
        "id": "h1bxsM_1V0J",
        "original": null,
        "number": 1,
        "cdate": 1666570285041,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666570285041,
        "tmdate": 1669497359669,
        "tddate": null,
        "forum": "Pkb5FA5AjqP",
        "replyto": "Pkb5FA5AjqP",
        "invitation": "ICLR.cc/2023/Conference/Paper4949/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims at finding the undesired behaviors of large language models. Specifically, the goal is to find a prefix for decoding, such that the decoded text is toxic or satisfies some predefined score functions. The approach is implemented as a variant of gibbs sampling or the coordinate ascent as called in the paper. The paper also provides approximations for such objective functions in order to gain speed. Experiments compared to other prompt tuning show that the proposed approach is more effective at attacking the large language models. \n",
            "strength_and_weaknesses": "Strength\n- The safety issue of the large language models is an important topic nowadays. The topic studied in this paper aligns with the topic.\n- The proposed approach is also reasonable, given the difficulty of doing discrete space optimization. The variant of first-order approximation also achieved good empirical results. \n- Experiments are done with practical LLMs (though GPT-2 is not considered as that large).\n\nWeakness\n- The practical usefulness of the paper needs further justification. In practice if we can have a risk function \\phi(x, o), then one can simply reject the samples when the samples get high risks. So relying on \\phi(x, o) to find the vulnerability of LLM may not be practical, as one can filter out these samples using the same \\phi(x, o). We all know that the large language models are not safety guaranteed, and the proposed approach may not be super helpful in this context.\n\n- Technically the coordinate-wise ascent may not be super efficient. One can potentially improve this further, in terms of 1) the approximation, and 2) the scope of the optimization. \nRegarding 1), the work named gibbs with gradient [1] might be helpful. \nRegarding 2), one can think of approaches that can flip multiple sites at a time [2], or flipping them in parallel in a factorized way [3].\n\nReferences:\n\n[1] Oops I Took A Gradient: Scalable Sampling for Discrete Distributions, Grathwohl et.al\n\n[2] Path Auxiliary Proposal for MCMC in Discrete Space, Sun et.al,\n\n[3] A Langevin-like Sampler for Discrete Distributions, Zhang et.al\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly, and is also easy to follow. The quality of the experiments is Ok, where the qualitative and quantitative results are shown, w.r.t., baseline methods. The novelty is somewhat limited given the existing literature in discrete space optimization, and the prompt engineering works. It shouldn\u2019t be too hard to reproduce the results.\n",
            "summary_of_the_review": "\nOverall this is a paper concerning the vulnerability of the LLMs. The significance of the specific formulation in this paper needs further justification, as one should be able to defend such kinds of attack easily using the same function \\phi(x, o). \nThe technical part can be further improved with the recent advances in discrete space sampling.\n\n====\n\nupdated score after rebuttal",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4949/Reviewer_os8d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4949/Reviewer_os8d"
        ]
    },
    {
        "id": "kEBnALmzG2H",
        "original": null,
        "number": 2,
        "cdate": 1666853030237,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666853030237,
        "tmdate": 1666853030237,
        "tddate": null,
        "forum": "Pkb5FA5AjqP",
        "replyto": "Pkb5FA5AjqP",
        "invitation": "ICLR.cc/2023/Conference/Paper4949/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an efficient method to find prompts that will trigger undesirable behaviors of large language models, based on a coordinate ascending algorithm and an linear approximation which avoids doing forward-backward passes for every candidate token. ",
            "strength_and_weaknesses": "Strengths:\n1. The proposed algorithm and approximation seems novel to my knowledge.\n2. The problem of finding the prompt that causes certain behavior is interesting and important to study.\n3. Some of the prompts indeed works even for Codex, e.g., \"Barack Obama is a legalized unborn\", \"Florida governor\", which seem to prove the effectiveness of the method.\n\nWeakness:\n1. From Figure 1, the effectiveness of the method can fall significantly when output length is longer, which is a huge limitation.\n2. Most of the examples given in the paper look quite unnatural and difficult to understand, indicating it's difficult to expose undesirable behaviors in more natural interactions with the language models.\n3. The evaluations are limited to only GPT-2, despite most instances only take less than 1 minute to run.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the approximated objective seems a bit confusing to me, and the experimental results are not demonstrated in a concise way. In Eq. 4, why is the $\\lambda\\_{p\\_{LLM}}$ ignored? Same for the \"linearly approximatable term\" in Eq. 5. And Eq. 6 should be an average instead of just a sum, right?\n\nQuality: the work studies an interesting and important problem, but more evaluations for different models could have been added to demonstrate the effectiveness of the method. \n\nNovelty: the method is novel to my knowledge.\n\nReproducibility: good. The method seems to have a few hyperparameters that could potentially affect reproducibility, but I find some of the prompts are indeed valid even for Codex.",
            "summary_of_the_review": "Overall I like the topic studied in the paper, and the method seems novel to me. Some of the prompts even work for other models. However, the effectiveness of the method does not seem good enough since it's success rate decays significantly for longer output lengths, and the prompts still look unnatural to me in most cases. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4949/Reviewer_ouPy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4949/Reviewer_ouPy"
        ]
    },
    {
        "id": "t-F5pr33aHp",
        "original": null,
        "number": 3,
        "cdate": 1667011805871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667011805871,
        "tmdate": 1667011805871,
        "tddate": null,
        "forum": "Pkb5FA5AjqP",
        "replyto": "Pkb5FA5AjqP",
        "invitation": "ICLR.cc/2023/Conference/Paper4949/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper casts the large language models (LLMs) targeting behavior evaluation as a discrete input-output scoring function so that the targeting input-output behavior examples can be located/generated as a discrete function optimization problem. In particular, this method can be used to locate failure behaviors of LLM models making it a means to audit LLM models.",
            "strength_and_weaknesses": "Strength\n\t\u2022 A simple and clear formulation to search for targeting properties of input-output behaviors via discrete optimization with relaxation. \n\t\u2022  A few effective insights in solving the formulated discrete optimization problem\n\t\u2022 The experimental results seem to demonstrate that the approach is promising\n\t\n\nWeakness\n        The writing of the key contribution --- decomposition and relaxation of the discrete optimization problem --- is not clear enough which might cause confusion for the readers.",
            "clarity,_quality,_novelty_and_reproducibility": "The approach proposed in this paper seems promising. The writing is mostly clear but the key contribution in section 3.3 is difficult to follow: (a) decomposing equation 4 into equation 5, and (b) linear relaxation. It is of modest originality with modest incremental new contribution.\n",
            "summary_of_the_review": "This paper provides an approach to  locate/generate the targeting input-output behaviors of an LLM without modifying the model itself --- formulating targeting properties of generating sequences as a discrete optimization so as following the literature to provide a linear relaxation to optimization. Two add-on improvement steps are made: (1) decomposing the objective into linearly approximation term and autoregressive term; (2) further approximate the linear team with averaging over random selected tokens instead of the whole vocabulary. The experimental results seems to demonstrate that the approach is promising. \n\nDetails:\n\t1. Equation 2, please clarify whether the objective function is required to be differentiable. \n\t2. Equation 3, will the scale difference between \\phi(x, 0) and log P_{LLM} be a problem --- the auditing score scale or its variation might be overwhelmed by log probability or the other way around. Any thought? \n\t3. Page 4, the paragraph before equation 5: please introduce the approximation \\tilde{s}_i formally to be self-contained; also the equation 6. With the Tailor expansion context, it is too abrupt to go into equation 6 directly from equation 5.\n\t4. Equation 5 needs more explanation. It might be better for formulate your claim of benefit and then prove it formally.\n\t5. Page 5, the last paragraph: \"\\phi(x, 0) is sufficiently large\", how large? Please detail. And any ablation study to determine this hyper-parameter? \n\t6. Page 8,  the text after equation 8: \" \u2026without taking gradients\", but it seems to me that algorithm 1 does require the gradients in line 2 but these gradients can be pre-computed though.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "This approach can potentially be used by  malicious parties to exploit systems based on LLMs to generate harmful results.\n",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4949/Reviewer_froq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4949/Reviewer_froq"
        ]
    },
    {
        "id": "ocPaJNTWF5o",
        "original": null,
        "number": 4,
        "cdate": 1667525247342,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667525247342,
        "tmdate": 1667525247342,
        "tddate": null,
        "forum": "Pkb5FA5AjqP",
        "replyto": "Pkb5FA5AjqP",
        "invitation": "ICLR.cc/2023/Conference/Paper4949/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates auditing LLM using discrete optimization and proposes a better way to solve the optimization problem. In experiment, the proposed method outperforms existing methods on solving the optimization problem and showcases failure modes of LLMs.\n",
            "strength_and_weaknesses": "Strength\nThis paper proposes a better way to solve the discrete optimization problem compared to prior works\n\nWeakness\nN/A\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and the proposed technique is novel. The experiment supports the claim of the paper and it could be replicated easily.\n",
            "summary_of_the_review": "The proposed techniques on discrete optimization are effective on finding failure modes of LLMs.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4949/Reviewer_hyXR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4949/Reviewer_hyXR"
        ]
    }
]