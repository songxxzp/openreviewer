[
    {
        "id": "WqWI1gIl2zT",
        "original": null,
        "number": 1,
        "cdate": 1666555680873,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555680873,
        "tmdate": 1668339078738,
        "tddate": null,
        "forum": "b3k_8yKKdag",
        "replyto": "b3k_8yKKdag",
        "invitation": "ICLR.cc/2023/Conference/Paper2858/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of observationally robust reinforcement learning.  In this setting, an agent learns via interacting with an MDP, but is evaluated in a version of this MDP where noise is added to the observed states by applying an (unknown) \u201cnoise kernel\u201d $T: X \\rightarrow \\Delta(X)$; this evaluation environment is called a observationally-disturbed MDP (DOMDP), and is a form of POMDP.\n\n\nThe main idea is to apply lexical reinforcement learning (LRL) to this problem.  Lexical RL algorithms optimize over multiple (e.g. 2) objectives, say $R_1$ and $R_2$, to find the best policy according to $R_2$ among those that are (approximately) optimal according to $R_1$ (i.e. prioritizing $R_1$ over $R_2$).  In this case, performance (i.e. value of the policy) on the MDP is $R_1$, and the drop in performance resulting from some proxy noise kernel $\\tilde{T}$ is $R_2$.\n\nThe main benefit claimed is the guarantee that LRL will yield an approximately optimal policy, while previous approaches to observationally robust reinforcement learning may sacrifice arbitrary amounts of performance in the MDP in order to achieve robustness.  ",
            "strength_and_weaknesses": "\nStrengths:\n- Quality and Clarity (see next section)\n\n\nWeaknesses:\n- Novelty (see next section)\n- The motivation for applying LRL to observationally robust reinforcement learning is unclear.  The abstract and introduction mention \u201cexplainability\u201d, but this is not elaborated on.  Regarding the argument that this work provides a guarantee: this is a guarantee of eps-optimal performance in the MDP, but it is not clear why we care about performance in the MDP (vs. the DOMDP) in this setting.  This could be motivated by the unknown nature of $T$: if nothing is known about $T$, then it seems sensible to aim to optimize performance in the MDP, and hope $T$ only introduces mild noise.  On the other hand, in the worst case, $T$ can render observations useless at test time, in which case we might be much better off using a different approach (e.g. one that aims to optimize the performance of a constant policy).  No clear guidance is given for when the approach taken in this work is preferable, but \u201cguarantees\u201d are advertised without appropriate caveats.\n- Somewhat related to the previous point: I'm not convinced by the choice of $\\tilde{T}$ as \u201cany stochastic kernel whose set of fixed points coincides with the collection of constant policies\u201d.  This choice is only discussed minimally, and seems weakly motivated via allowing Theorem 2, and generic maximum entropy considerations.  The experiments are too limited to be informative about whether this is a good choice in practice, and if we knew more about $T$, the entire approach suggested in this work might be suboptimal. \n- Overall, more discussion is needed to justify the significance of this work, and to elaborate what value (practical or otherwise) it is expected to provide.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe clarity is moderate-to-high.  The writing and technical exposition are clear, and the paper is well organized.  I note three issues of clarity, however:\n1) The introduction and exposition do not make it easy for the reader to understand the problem setting.  Definition 1 doesn\u2019t say how $T$ is used, the beginning of the following paragraph is confusing, and it is not until half-way through that it is stated that \u201cwe do not have information about the noise kernel $T$ or a way to estimate it\u201d.  Given this information, I immediately wondered how it is possible to know how robust a policy is to $T$, something that seems necessary in order to devise sensible algorithms.  But it is not until section 4 that $\\tilde{T}$ is introduced, and it is still not clear how important it is for $\\tilde{T}$ to resemble $T$. \n2) The motivation is unclear (see previous section).\n3) The novelty with respect to LRL and previous work on observationally robust reinforcement learning is not clearly stated.\nTo address these 3 issues, I recommend expanding on the introduction to 1) informally describe the specific problem statement, 2) explain why LRL is appropriate or useful in this context, and 3) explain the novelty of this work / the technical challenges of applying LRL in this context.  \n\nQuality:\nThe quality is high.  I did not note any issues other than clarity, novelty, and significance.\n\nNovelty:\nNovelty is low-to-moderate: the work is a fairly straightforward application of an existing method to an existing problem.  The main novelties I note are the characterization of robust policies in Section 3 (which I rate as a somewhat minor conceptual contribution), and the development of LRPG, which involves proving Lemma 1 (I did not evaluate the significance of the proof).\n\nReproducibility:\nAt a glance, reproducibility appears high.  The supplementary material appears to include code, and the Appendix clear experimental details.\n",
            "summary_of_the_review": "While this work is well executed, I'm not convinced by the technical contributions, motivation, or significance.\nBut the author's response could potentially convince me on these points, and I believe the issues of clarity could be addressed in the revision.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_BQBq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_BQBq"
        ]
    },
    {
        "id": "gpC7sDSaBX",
        "original": null,
        "number": 2,
        "cdate": 1666669911969,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669911969,
        "tmdate": 1666669911969,
        "tddate": null,
        "forum": "b3k_8yKKdag",
        "replyto": "b3k_8yKKdag",
        "invitation": "ICLR.cc/2023/Conference/Paper2858/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the robustness of reinforcement learning with respect to observational disturbances. They proposed to use a recently proposed lexicographic optimization framework. Experiments on 3 grid worlds type environment and compare against an adversarial training RL baseline and one naive baseline without considering environmental uncertainty.",
            "strength_and_weaknesses": "Strength: The problem studied in the paper - RL with respect to observation perturbation is an interesting and important problem.\n\nWeaknesses:\n\n* Section 3 and 4 are written in a convoluted way and is very hard to follow. \n\n* In (5), how to compute <\\pi_{\\theta}, \\tilde{T}>(x), for arbitrary policy \\pi_\\theta? Why optimizing (5) (rather than optimizing the expected J(\\theta)) is a good or even meaningful objective function?\n\n* Algorithm 1 states MDP and \\tilde{T} as input. Does it mean the proposed method requires model knowledge of the MDP? If so, that seems very unrealistic since one of the main motivations of using RL for real-world control is that the environment model is unknown. Also, how is the \\tilde{T} computed?\n\n* In (6), which is formulated as a lexicographic optimization problem. How does K_2(\\theta) is computed? Also, how is K_1^* computed without knowing the ground-truth MDP?\n\n* It is not clear what is the main advantage that lexicographic optimization provides compared to tons of stochastic and robust RL frameworks. The authors argue that a min-max framework might be too conservative - but there are tons of other works that consider different models of environmental noise and takes a weighted combination of the average or worst-case cost.\n\n* The experimental section is rather short. More importantly, the choice of the environment (all discrete state action grid world MDP) and the choice of baselines are insufficient. Experiments on Mujoco and comparison to recent robust RL works are needed (many related works are mentioned in Section 1).",
            "clarity,_quality,_novelty_and_reproducibility": "See above. ",
            "summary_of_the_review": "This paper considers policy robustness in RL with respect to observation disturbance. They used a lexicographic optimization framework that can be plugged into different policy optimization algorithms. The method sections (Sections 3 and 4) need to be significantly improved for readers to follow and provide the rationale why the proposed objective function (e.g., eq 5-6) are reasonable objectives, and how to compute them efficiently when the environmental model is unknown. The choice of simulation environment (all discrete state action grid world MDPs) and the comparison to existing algorithms are insufficient. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_rSvU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_rSvU"
        ]
    },
    {
        "id": "dE7wRqd6Ju",
        "original": null,
        "number": 3,
        "cdate": 1667130129693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667130129693,
        "tmdate": 1667130129693,
        "tddate": null,
        "forum": "b3k_8yKKdag",
        "replyto": "b3k_8yKKdag",
        "invitation": "ICLR.cc/2023/Conference/Paper2858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies robustness to observations perturbations which are generated according to a stochastic kernel. It first characterizes robust policies by operator invariant sets, then formulates robustness criteria via a lexicographic objective. The proposed scheme preserves the convergence properties of vanilla policy-gradient algorithms and helps achieve robust policies. ",
            "strength_and_weaknesses": "The paper is well-written and easy to follow. The objectives are clearly formulated, which makes the reading enjoyable. This work is also well-motivated: how to ensure robustness to observation perturbations without altering the theoretical convergence of existing policy-gradient algorithms. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The structural view on policy sets is interesting, as it introduces a preference order between optimal policies. I am not familiar with the POMDP literature, but employing such structural analysis for the sake of robustness seems to be novel. \n\nI appreciate the awareness of the authors on some results being similar to previous work in footnote 1 and the possible shortcomings of the conducted experiments in the discussion section. ",
            "summary_of_the_review": "For the reasons above, I recommend acceptance of this paper. \n\nQuestion to the authors: \nThe way this study tackles uncertainty is specific to observation perturbation. Yet, it may be equivalently formulated to tackle transition perturbation or, perhaps also, action perturbation. Have you investigated such directions? For example, under which conditions does maximal robustness hold robustness guarantees to model (reward, transition) perturbations? ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_9ZEw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_9ZEw"
        ]
    },
    {
        "id": "fKGPDe9SjP",
        "original": null,
        "number": 4,
        "cdate": 1667169874635,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667169874635,
        "tmdate": 1667169874635,
        "tddate": null,
        "forum": "b3k_8yKKdag",
        "replyto": "b3k_8yKKdag",
        "invitation": "ICLR.cc/2023/Conference/Paper2858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers reinforcement learning in an observationally-disturbed MDP (DOMDP), where the learning agent is able to access the full state, but the state is disturbed by some unknown random signal when deploying actions. The paper provides theoretical characterization of the robust policy set, which constructs sufficient conditions to learn a robust policy. Inspired by the theoretical findings, the paper develops an algorithm based multi-objective optimization to learn a policy that is robust to the disturbing while still preserving optimality. Finally, the paper conducts empirical studies to show the effectiveness of proposed method. ",
            "strength_and_weaknesses": "Strength \n\nThe paper considers a problem setting where the observation for a learning agent can be perturbed by some random noise.  A multi-objective learning algorithm is proposed to solve this problem. Both theoretical and empirical studies are provided. \n\nWeakness\n\nI have some questions and concerns about the problem setting and practical implementation of the algorithm. Please correct me if I missed anything important. I am happy to adjust my score based on how well the authors answer the questions in the rebuttal. \n\n1. I am a little bit confused by the problem definition. Is there any application that motivates this setting? According to Definition 1, in a DOMDP, the transition of the underlying MDP is not effected by the random disturbing kernel. It is also suggested that the learning agent can access the full state, just that the policy will be disturbed by a certain mapping. This definition is very confused to me given the \u201crobot navigation in a dangerous environment\u201d example in the appendix, where it seems that the transition kernel should also be effected by the disturbing mapping? \n\n2. I think the robustness of a policy should be related to the maximum possible perturbation. That is, one cannot expect a certain degree of robustness given an extremely adversarial disturbing signal. Maybe only when $T$ is not that \u201cadversarial\u201d (e.g. using SLAM in navigation where the computed position is near the true position) some good theoretical properties can be guaranteed. Which result discusses such connection? \n\n3. It seems that the authors consider an extreme case, where the learning agent does not have any information about $T$, and cannot even estimate the random noise from data. Is there any motivation to consider such problem? In the worst case, I don\u2019t think one can expect to find any good algorithm. \n\n4. Should it be $K_{\\tilde{T}}$ in equation 5? The paper argues that a reasonable choice of $\\tilde{T}$ is the uniform distribution. I agree with this, as in the problem setting no prior knowledge of $T$ is given. But it seems that a uniform $\\tilde{T}$ (or even gaussian) is not available in most RL problems? \n\n5. Since $\\tilde{T}$ and $T$ could be arbitrarily different. How does the difference (measured by any probabilistic divergence measurement) affect the robustness of learned policy? \n\n6. I am wondering how does the MiniGrid environment is modified in the experiments. It seems to me that at a given state, an action is taken by sampling from a distribution computed by Equation 1, but the transition and reward are not affected?  Is it correct? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the detailed comments in the previous section. ",
            "summary_of_the_review": "I recommend rejecting this paper as the problem setup and some implementation details are not clear to me. Please correct me if I missed anything important. I am happy to adjust my score based on how well the authors answer the questions in the rebuttal. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_WdQf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_WdQf"
        ]
    },
    {
        "id": "QTv-gfX3S0",
        "original": null,
        "number": 5,
        "cdate": 1667204878369,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667204878369,
        "tmdate": 1667204878369,
        "tddate": null,
        "forum": "b3k_8yKKdag",
        "replyto": "b3k_8yKKdag",
        "invitation": "ICLR.cc/2023/Conference/Paper2858/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the robustness problem in partially observable MDPs. They characterize the set of robust policies for any MDP in the form of operator-invariant sets and analyze how the structure of these sets depends on the MDP and noise kernel. Then they provide an inclusion relation that motivates how to search for robust policies more effectively. Finally, they cast robustness as a lexicographic objective to compute the robust policy while maintaining the optimality of the policy.",
            "strength_and_weaknesses": "Strengths:\nThis paper provides sufficient theoretical analysis to support their idea. \nThe problem solved in this paper is interesting and the idea seems novelty.\nWeaknesses:\nIn the experiments, they test the policy against a noiseless environment and two pre-defined kernels. Here I have one question. If the environment is disturbed adversarially, how will the computed robust strategy perform? ",
            "clarity,_quality,_novelty_and_reproducibility": "The overall paper is well organized. However, the paper involves many equations and symbols. It seems a little messy when reading the paper. It would be nice if the authors could provide a table of these symbols. Since I don\u2019t family with lexicographic reinforcement learning, the idea of introducing lexicographic objectives to solve the robustness problem seems novelty to me. ",
            "summary_of_the_review": "Overall, the paper proposes a new method to solve the robustness problem. Given the concern about the weaknesses, I recommend marginally above the acceptance threshold. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_qpNg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_qpNg"
        ]
    },
    {
        "id": "FhPMMXJDAE",
        "original": null,
        "number": 6,
        "cdate": 1667278051685,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667278051685,
        "tmdate": 1667278051685,
        "tddate": null,
        "forum": "b3k_8yKKdag",
        "replyto": "b3k_8yKKdag",
        "invitation": "ICLR.cc/2023/Conference/Paper2858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides ideas for observational robustness when learning control policies via Reinforcement learning. The setting of the paper is an observationally distributed MDP (DOMDP) -- one in which the agent can access the true state during learning but stochastic noise is introduced at deployment. i.e. when acting according to the learnt policy. This work studies a notion of robustness by analyzing how policies are altered by a noise inducing stochastic kernel -- to construct robust policy sets -- policy classes which minimize the difference between the expected long run value-to-go under the unaltered and altered policies. Authors propose to leverage ideas from lexicographic optimization to trade-off between robustness and optimality of the learnt policies. ",
            "strength_and_weaknesses": "I found the paper quite interesting. The setting of DOMDP that the authors consider is quite well motivated from the point of view of practical applications - very often control policies are learnt in simulation so the true state is observable but the state observations are corrupted by noise at deployment time. I also found the idea of adding robustness objectives in lexicographic RL quite interesting - it seems to be a principled way of trading-off between optimality and robustness. The paper is also well organized - introducing the setting (DOMDP and robustness regret) in Section 2 -> characterization of maximally robust policy sets in Section 3 -> lexicographic objectives in Section 4. Some detailed comments and questions: \n\n1) On Inclusion Theorem: Given the definition of robustness regret and policy robustness along with the arguments in section 3.2, I am not sure about the gap between $\\Pi_D$ and $\\Pi_0$. I think if $\\Pi_D$ is defined as $[\\pi \\in \\Pi : \\rho^{\\pi}_{\\mu_0}(x) D^{\\pi}(x,T) = 0 \\, \\forall \\, x \\in X ]$, then it should characterize $\\Pi_0$ completely? Here $\\rho^{\\pi}$ refers to the steady state distribution under start state distribution $\\mu_0$ and policy $\\pi$.\n\n\n\n2) On Corollary 1: I am not sure I quite understand/am able to appreciate the usefulness of this. Also, the proof of statement (ii) seems to have some typos.\n\n3)  On Lemma 1 in Appendix (Convergence proof of 5): I seem to be missing something here. For a fully parameterized policy, $\\pi_{\\theta}(x, u) = \\theta_{x,u}$, we need to analyze a constrained optimization problem? Basically, one needs to restrict the parameter $\\theta$ to the probability simplex (I am thinking a projection step might be needed). The arguments from Theorem 3 might have to be modified.\n\nSome other minor comments:\n\n1) Please do correct typos in the presentation, for example indiuced -> induced etc. \n2) Do the learning rates in (3) change over time? That is, do you want to say $\\beta^1_t$ or $\\beta^1$? Both are written. On a related note, it might be a good idea to summarize convergence results and details of PB-LRL in the Appendix. A self contained presentation is often much more accessible. \n3) In (5), $K(\\theta) \\rightarrow K_{\\tilde{T}}(\\theta)$? Also, $p^{\\pi_{\\theta}}(x)$ seems to be formally undefined as well.\n4) Since a reference is made to policy gradient algorithms which converge to optimal policies to guarantee LRPG converges to an approximately optimal policy that maximizes robustness, may I suggest citing some recent work on different problem settings under which convergence of policy gradient methods to optimal policies actually holds? See for example:\n- \"On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift\"\n- \"Global Optimality Guarantees For Policy Gradient Methods\"\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think this paper is clearly written and well organized. The characterization of robust policy sets and their use to find robust policies via lexicographic optimization seems quite novel.",
            "summary_of_the_review": "Overall, I am leaning towards accepting this paper. I think characterization of maximally robust policy sets is a useful first step but to me the potential of disadvantage based policy sets seems yet to be explored which is a bit of a let down. See for example Remark 2 which only applies to robust policies invariant to the kernel $T$ and that too under a fully parameterized setting. While constant policies is one way to minimize robustness regret, is it really a useful notion? Do these not correspond to smoothness constraints on the policy class. Similarly, imposing invariance to $T$ for all states, as done in defining the set $\\Pi_{T}$, seems too strong. The notion of using disadvantage function seems the most useful - robustness corresponds to minimizing the long run disadvantage of the perturbed policy from the most useful states. \n\nPlease clarify if I am unable to appreciate the usefulness of the contributions here but I feel like the most useful ideas from this setting are missing in the current presentation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_LTJa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2858/Reviewer_LTJa"
        ]
    }
]