[
    {
        "id": "D5BondMcK9S",
        "original": null,
        "number": 1,
        "cdate": 1666495445207,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666495445207,
        "tmdate": 1666495445207,
        "tddate": null,
        "forum": "DpE5UYUQzZH",
        "replyto": "DpE5UYUQzZH",
        "invitation": "ICLR.cc/2023/Conference/Paper3050/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an algorithm, magnetic mirror descent (MMD), for RL problem and 2p0s games. Sound theoretical results are obtained for the algorithm. Empirically, MMD exhibits desirable properties as a tabular equilibrium solver, as a single-agent deep RL algorithm, and as a multi-agent deep RL algorithm.",
            "strength_and_weaknesses": "Strength: \n1.\tThis paper provides a unified perspective to view the RL problem and 2p0s games and proposes an algorithm to solve it. \n2.\tThe theoretical results are strong and some results are attained for the first time as the authors claim.\nWeaknesses:\n1.\tThe algorithm seems not to be of novelty. Maybe the contributions mainly lie in the analysis. \n2.\tThe paper is not well organized, and the problem is not highly motivated. The comparisons with existing and the idea behind the algorithm are not clearly presented. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the algorithm and the analysis method need to be highlighted.",
            "summary_of_the_review": "The idea to deal with RL and games in a unified way is good, and the algorithm proposed enjoys linear convergence. However, it seems that the improved convergence rate compared with existing approach results from stronger assumption. The authors are expected to provide more insights of the algorithm and the novelty of the algorithm and its analysis scheme.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3050/Reviewer_8q9G"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3050/Reviewer_8q9G"
        ]
    },
    {
        "id": "Ot9fye8AcQg",
        "original": null,
        "number": 2,
        "cdate": 1666583088318,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583088318,
        "tmdate": 1666583088318,
        "tddate": null,
        "forum": "DpE5UYUQzZH",
        "replyto": "DpE5UYUQzZH",
        "invitation": "ICLR.cc/2023/Conference/Paper3050/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces an iterative optimization algorithm based on mirror descent that demonstrates competitive performance in reinforcement-learning tasks while provably converging to (logit) quantal response equilibrium in both normal-form and extensive-form two-player zero-sum games. The key idea of this algorithm - called Magnetic Mirror Descent (MMD) - appears to be to model these scenarios as variational inequality problems with monotone G operators that admit efficient optimization. The authors generalize a non-Euclidean proximal gradient method for this purpose. After formally proving the convergence in Theorem 3.4, the authors present a series of experimental results to substantiate their claims. First, they evaluate the convergence to quantal response equilibrium against Predictive Update and Optimistic Multiplicative Weight Update methods. In the second experiment they leverage the logit correspondence to reach Nash equilibrium and compare the speed of convergence over iterations to CFR. The last two experiments consider reinforcement-learning, either in single-agent setting, against PPO,  or in multi-agent setting, against NFSP.\n",
            "strength_and_weaknesses": "This is a highly technical work with an extensive appendix. The text reads relatively well, given the degree of technicality, and the authors seem to exert great effort to position their work in other literature on the topic. Still, I can't shake the feeling this paper would benefit from more carefully developed exposition, with more examples in the main text, and more detailed discussions on the introduced concepts. I understand this is not possible given the space limits. \n\nIn the short timeframe given by ICLR for reviewing I attempted to go through some of the theoretical results and I did not spot any obvious errors. I find Theorem 3.4 to be a nice result. Reiterating my previous concern regarding limited space dedicated to discussions though, I wonder how strong this theorem\u2019s assumptions are in practice, especially in the context of reinforcement-learning problems. From the game-theoretic perspective, I also fail to find any other solution concept that would admit a VI formulation besides (logit) QREs. Are there some, for example, refinements of Nash equilibrium the authors know of that would be computable via MMD?\n\nIn addition, I remain confused about the convergence rates. The text mentions at multiple points the exponential convergence to QRE, yet the abstract and other parts claim linear convergence? What am I missing?\n\nFor completeness, I believe it would be also fair to discuss which Nash equilibria are actually reachable as limits of logit quantal response equilibria, i.e., to mention the convergence is pointwise to the best response if it is single valued, otherwise all actions in the best response are played with equal probabilities. \n\nFinally, I also wonder why the authors did not compare their approach to other QRE-based algorithms like ADIDAS in normal-form games, since it is readily available in OpenSpiel? Why there is no comparison to other QRE algorithms in extensive-form games like the algorithms of Farina et al. or Ling et al. which this work refers to? Why is the comparison in the second experiment done with respect to CFR and not to (at least) CFR+? Also, how time consuming is one iteration of the MMD in contrast to other iterative algorithms mentioned in this work, i.e., how would the reported results look like with a temporal x-axis instead of plotting over iterations?\n\n\nA few nitpicks:\nThe graphs in Fig 2 are missing y-axis labels.\nWhich quantity exactly is reported in Table 1? Are these utilities?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper seems clear enough, the notation is extensive but this is to be expected. The results appear correct to me, I would prefer a comparison to more baselines in the experimental section though. The work seems to make a non-trivial contribution. The experiments are done on standard domains and the algorithm seems well described, so I believe the results to be replicable. ",
            "summary_of_the_review": "This appears to be a theoretically well grounded paper that presents a new algorithm for solving variational inequality problems. It reads well and the theoretical results seem correct to me. I would appreciate if the empirical section considered more baselines to better evaluate the algorithm's performance though. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3050/Reviewer_UyUG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3050/Reviewer_UyUG"
        ]
    },
    {
        "id": "KOpRoMvowcV",
        "original": null,
        "number": 3,
        "cdate": 1666671085004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671085004,
        "tmdate": 1666671085004,
        "tddate": null,
        "forum": "DpE5UYUQzZH",
        "replyto": "DpE5UYUQzZH",
        "invitation": "ICLR.cc/2023/Conference/Paper3050/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method, Magnetic Mirror Decent, which can get comparable performance to state of the art in both single-agent and multi-agent RL settings.  To the best of my knowledge it is the first algorithm known to have such strong empirical performance in both settings, and to have strong theoretical convergence guarantees in both settings.",
            "strength_and_weaknesses": "Strengths:\n* Provides the first algorithm to get empirically strong performance in both single-agent and multi-agent RL settings\n* The theoretical convergence guarantees of magnetic mirror decent is the first time such guarantees have been shown in 2 player zero sum games with a first order solver.\n\nWeaknesses:\n* The biggest weakness of this paper is the clarity.  Given the subject matter, this is not the fault of the authors, but the authors themselves seem to be aiming for a broader audience than this paper is accessible to:\n ``\n We hope that, due to its simplicity, MMD will help open the door to 2p0s games research for RL researchers without game theoretic backgrounds.\n '' \nI believe, how the paper is written now, such researchers would have find it difficult to understand the core method or contribution.  I think this is a challenging goal to set, but I think the authors are right to aim for it.  Given that this paper has the potential to be seminal, it could be the starting point for many RL researchers aiming to understand this subfield, it would be best if the paper offered enough direction for those researchers to be successful in understanding it.\n\nI appreciate that the authors have already put some effort into doing this, it is claimed that Section 2.2 - Section 3.1 can be skipped.  However, there are a few points this misses:\n* variables used in 3.2 are not defined if Section 2.2-3.1 are skipped (eta, zeta, psi)\n* The term \"magnet policy\" is never explained even informally\n* The core reason why the intuition as to why this approach works where other approaches have failed is not clear\n\nThere are also bits of prior knowledge that are assumed from readers: \n* the term ``behavioral strategies'' is used but not explained\n* It's not initially clear from context that the Gambit engine on page 6 is only used to measure performance of the algorithm, rather than as a part of the algorithm itself.\n* The term \"regularized equilibria\" is used without explanation \n\nFinally, within Section 2.2-3.1, it would be good to be accommodating to those readers as well, at least in having a short summary that outlines the main sections of the argument.  Ideally this would highlight what the key change is that other methods missed.  For instance, just flagging that you will cast it to a variational inference problem, show a dynamical system which converges to the solution of that variational inference problem, and then noticing that that dynamical system factors into separate updates.",
            "clarity,_quality,_novelty_and_reproducibility": "To the best of my knowledge the approach is novel, and appears to be reproducible.",
            "summary_of_the_review": "This paper has the potential to be very impactful to the field, as such I am recommending acceptance.  However, it is quite dense, to some extent unavoidably.  I believe this can be mitigated to some extent, to allow the central result to be accessible to more readers. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3050/Reviewer_Mj5Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3050/Reviewer_Mj5Y"
        ]
    },
    {
        "id": "2QUC0mcTTEw",
        "original": null,
        "number": 4,
        "cdate": 1667026156217,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667026156217,
        "tmdate": 1667062693041,
        "tddate": null,
        "forum": "DpE5UYUQzZH",
        "replyto": "DpE5UYUQzZH",
        "invitation": "ICLR.cc/2023/Conference/Paper3050/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed MMD, a variant of MD with an extra \"behavior cloning\" term, to unify single-agent RL and Two-Player Zero-Sum Games. The method is shown to converge linear in a functional sense. Some numerical experiments are provided to justify the algorithm. ",
            "strength_and_weaknesses": "Strength: \n - The paper is generally well written, with complete theory and numerical experiment details.\n\nWeaknesses: \n - My biggest concern is about the motivation of designing a single RL algorithm for both single-agent RL and 2p0s. Extending single-agent methods to 2p0s seems reasonable for me: many 2p0s methods are at least inspired by single-agent methods. But the other way does not look reasonable since if we know we are solving single-agent RL, why bother using 2p0s methods? I think the authors need more elaboration on this. \n - Another concern is the novelty of the proposed method. MMD in (10) seems to add a \u201cBC\u201d term $KL(\\pi, \\rho)$ beyond MD, which from my understanding helps stabilize the algorithm in the single-agent setting (most works from offline paper adopted a similar idea: TD3-BC, BCQ, SCORE, etc). In 2p0s, this term seems to drive the policies toward an equilibrium in a uniform sense. In terms of the proof, it follows a standard MD proof on the functional level (no parameterization) with some extra work on the \u201cBC\u201d term. \n - I also have concerns about the experiments. The experiments seem to be too simple. For example, in single-agent RL, they only consider very limited atari and mujoco games. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Fair. Some terminology lacks a formal definition. \n\nQuality: Good. \n\nNovelty: Somewhat.\n\nReproducibility: I believe it should be fine. \n",
            "summary_of_the_review": "Overall I think the current revision lacks a strong motivation. Given other concerns mentioned before, I recommend reject. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3050/Reviewer_BKWB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3050/Reviewer_BKWB"
        ]
    }
]