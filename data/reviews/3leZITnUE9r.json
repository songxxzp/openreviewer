[
    {
        "id": "ezt1Rk7vY7Z",
        "original": null,
        "number": 1,
        "cdate": 1666581739713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581739713,
        "tmdate": 1666582369128,
        "tddate": null,
        "forum": "3leZITnUE9r",
        "replyto": "3leZITnUE9r",
        "invitation": "ICLR.cc/2023/Conference/Paper245/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new metric to measure the fairness of language models on a toxicity labeled dataset. Authors exploit the fact that a fair model should yield higher perplexity scores for toxic sentences. The proposed metric is bounded between [0, 1] and utilizes Mann-Whitney U statistical test to quantify the tendency of a pre-trained model towards toxic generations for a protected group. Authors use an open-source toxicity dataset to evaluate their proposed metric for a wide range of publicly available language models. ",
            "strength_and_weaknesses": "Strengths: \n- Proposed fairness metric is intuitive and is easy to compute. \n- Authors study a wide range of pre-trained models. \n\nWeaknesses: \n- Authors have not conducted any human evaluation to verify the correctness of the proposed metric. So it\u2019s not clear how much the proposed metric aligns with the human notion of fairness.\n- Some of the design choices made in the paper are not clear. In particular\n  - Why do authors divide peplexity by corresponding toxicity score?  What benefits (if any) does it provide over simply using perplexity scores? Authors have not provided any empirical or theoretical justification for it. \n  - Authors use a subsample of the ToxiGen dataset which is generated using the GPT-3 model. We know that the GPT-3 model has its own set of biases towards different protected attributes. It\u2019s not clear how such biases impact the findings in this study.  \n \n- Paper does not dive deep into some of the surprising findings and makes shallow arguments. In particular:  \n  - GPT-2 large has better safety score than GPT2-medium. It's not clear why ? \n   - Roberta seems to have a very high safety score and authors hypothesize that it's because the pre-training corpus for RoBERTa contains stories, and news. We know that news coverage of certain protected attribute is often negative. For example, many news article associated with muslim community are not positive. So it's not clear why stories and news will improve RoBERTa's safety score.   \n- Some of the claims might be too specific to the dataset studied in the paper. For example, authors state that language models in general are less likely to embed harmful content for Asian, African-Americal, Chinese and Jewish people compared to other demographics. This finding might not be correct because the presented results are heavily dependent on a single dataset which is generated by the GPT-3 model. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality:\nPaper cites the relevant literature. However, as pointed out in the weaknesses section, some of the design choices made in the paper are not clear. Further, authors use vague language in the paper. For example, authors say \u201cour metric is quantifying a different notion of fairness issues compare to the existing metrics\u201d. However, they don\u2019t define the \u201cfairness notion\u201d that they are trying to measure. \n\nNovelty:\nThe proposed metric to study fairness is novel. \n\nReproducibility:\nAuthors have used a subset of an open source dataset and have provided exact details to select the subset used in the paper. All of the models studied in the paper are open sourced so one should be able to easily reproduce the experiments presented in the paper. \n",
            "summary_of_the_review": "While the proposed fairness metric is intuitive, as pointed in the weakness section, it's not clear how the proposed metric align with the human notion of fairness. Further, some of the analysis presented in the paper is too shallow as authors have not conducted ablation studies. Overall, I think that this paper requires further experimentation.   ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper245/Reviewer_hcJz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper245/Reviewer_hcJz"
        ]
    },
    {
        "id": "oMD0_QfJ4C1",
        "original": null,
        "number": 2,
        "cdate": 1666660308020,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660308020,
        "tmdate": 1666660308020,
        "tddate": null,
        "forum": "3leZITnUE9r",
        "replyto": "3leZITnUE9r",
        "invitation": "ICLR.cc/2023/Conference/Paper245/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new metric for measuring fairness\tof language\nmodels using text toxicity level and perplexity. It shows that the new\nmetric correlates well with other gender-specific metrics in the\nliterature. Using this new metric, a comprehensive study of 24 models\nis performed, along an analysis how the depth/width of the models\ninfluences the representational harms.",
            "strength_and_weaknesses": "Strengths:\n- a comprehensive study of the fairness\tof several model architectures (e.g., BERT, AlBERT, ELECTRA) of different sizes (e.g., base, large)\n- a study of how the depth/width of the network influences their fairness\n\nWeaknesses:\n- the need for a new metric is not well motivated\n- the advantages/disadvantages of the new metric are not discussed\n- intrinsic metrics for bias have been shown to be problematic; I'm not sure how to interpret the correlation study with one such metric",
            "clarity,_quality,_novelty_and_reproducibility": "I think\tthe paper would\timprove\tif the new metric was better\nmotivated. What gaps is it trying to fill in? What are the\nadvantages/disadvantages? If I understand correctly, I think of this\nmetric as an intrinsic metric. Several works are showing that\nintrinsic metrics (see below) are not well correlated with extrinsic\nmetrics for bias/fairness. For the metric introduced, since you\nrequire knowing the toxicity for the text, why not look at extrinsic\nmetrics (see Fairness definitions explained for lots of metrics or the\nanalysis using equalized odds in Your Fairness may vary).\n\nSome papers that I think could be cited (and why they are relevant in this context):\n* Fairness Definitions Explained - series of fairness metrics\n* Your Fairness May Vary: Pretrained Language Model Fairness in Toxic Text Classification - analysis of many LMs wrt fairness and model size/training size/random seed (in the context of toxic text prediction) using\tequalized odds as fairness metric, which\tis an extrinsic\tmetric\n* Intrinsic Bias Metrics Do Not Correlate with Application Bias - study of correlation of intrinsic vs extrinsic metrics\n\nSome typos: e.g., \"large-sacle\"\n",
            "summary_of_the_review": "The paper introduces a new (intrinsic) metric for assessing fairness\nof language models using text toxicity level and perplexity. The\nmetric is not well motivated and it is not clear why extrinsic metrics\ndo not suffice.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper245/Reviewer_82Na"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper245/Reviewer_82Na"
        ]
    },
    {
        "id": "Qnb37EM9RdU",
        "original": null,
        "number": 3,
        "cdate": 1666760239505,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666760239505,
        "tmdate": 1666761935340,
        "tddate": null,
        "forum": "3leZITnUE9r",
        "replyto": "3leZITnUE9r",
        "invitation": "ICLR.cc/2023/Conference/Paper245/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the problem of representational harms in pretrained models. The paper proposes a metric, safety score, to measure the harms. Then, the paper shows a study of this metric on 13 marginalized demographics using 24 pretrained models, and discuss the findings. ",
            "strength_and_weaknesses": "Strengths\n- The paper is easy to follow\n- Well motivated study - the overall topic\n- Studying this topic (representational harms in pretrained LMs) is important in the field.\n- The proposed metric seems reasonable\n- The further analyses and findings are interesting. \n\nWeaknesses\n- not clear why a new metric is needed\n- I didn't understand \"prioritizing depth over width\" when reading it for the first time. \n- the citation formats should be revised for readability.\n- typos and grammatical errors.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed metric seems novel, and it was clearly explained. ",
            "summary_of_the_review": "This paper tackles the problem of representational bias in pretrained models.  The proposed metric seems reasonable to me, and the study findings are interesting. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper245/Reviewer_UeDp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper245/Reviewer_UeDp"
        ]
    },
    {
        "id": "8Zl-l8VrIaE",
        "original": null,
        "number": 4,
        "cdate": 1667158812626,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667158812626,
        "tmdate": 1667158812626,
        "tddate": null,
        "forum": "3leZITnUE9r",
        "replyto": "3leZITnUE9r",
        "invitation": "ICLR.cc/2023/Conference/Paper245/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides an empirical study of representational harms in pre-trained language models.\nThe authors consider safety scores derived from a Mann-Whitney U-test, and compute such safety scores across a range of different models and marginalized demographics, finding that PTLMs have a tendency to show representational harms towards some marginalized demographics more than others, with some of these groups having not been studied extensively before.",
            "strength_and_weaknesses": "* This paper tackles the important issue of understanding biases and representational harms of language models, which are prevalent and permeate society through their varied applications.\n* The draft has several typos and missing punctuation which should be fixed. Several sentences throughout the text are also poorly phrased/grammatically incorrect and hard to understand.\n* On p.3, the authors state that issues have been found in the datasets used in recent related work, could the authors expand on what those issues are. The footnote included links to another footnote (2) which does not appear in the article.",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is not always very clear, there are several typos, the term \"safety scores\" seems to be used interchangeably with \"fairness score\" with no explicit definition of fairness or safety.\n* This work is a purely empirical study of existing language models, and introduces little novelty in terms of definitions or measures.",
            "summary_of_the_review": "While this study tackles an important problem for applications of language models, and such work should be published in impactful venues such as ICLR, I do not think the scope and novelty of this study meets the criterion for publication at this conference as it stands now.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper245/Reviewer_K8hw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper245/Reviewer_K8hw"
        ]
    }
]