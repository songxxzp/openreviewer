[
    {
        "id": "C4jFlEKMN0",
        "original": null,
        "number": 1,
        "cdate": 1666234622543,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666234622543,
        "tmdate": 1666234622543,
        "tddate": null,
        "forum": "pkgVPeL9gpX",
        "replyto": "pkgVPeL9gpX",
        "invitation": "ICLR.cc/2023/Conference/Paper527/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a rescaling-based postprocessing method that is applied to the output of any machine learning model with the goal of making the predictions more robust. They conduct experiments comparing the proposed method used to postprocess the outputs of several existing robust learning techniques, along with an ERM baseline, on two standard robustness datasets (CelebA, Waterbirds).",
            "strength_and_weaknesses": "\n## Major comments\n\n* The paper lacks any motivation for the proposed method. While of course robustness is a worthy goal, why *rescaling* would be a useful or appropriate technique to advance the community toward this goal is not obvious -- and there is no discussion in the paper for why either.\n\n* The method is not well-described. The equation (3) does not fully define the problem being solved. For example, what is really meany be a \"scaling coefficient vector\" -- does it need to have unit norm? Why do we take arg max in (3)? What are the practical implications of using this method? There is no discussion of this in the paper; lacking any theoretical results, there should be strong empirical or intuitive support for why this is a reasonable postprocessing choice.\n\n* The implementations of \"baseline\" or prior works are not faithful, and report much lower numbers than those works on the same datasets. For example, JTT reports much higher worst-group and average accuracies on Waterbirds (Table9 listd their best as 88.6% and 93.1% in Table 9, see also Table 7,8; these are lower than in the current paper Table 2) and Group DRO reports mich higher Waterbirds worst-group and average accuracies in Table 3. I am concerned that this is due to poor hyperparameter tuning or model architecture selection; the approach to tuning is not described at all in the paper so it is difficult to tell (if no tuning were performed, this would also be a concern).\n\n* The empirical results are weak. For example, only 4/18 comparisons in Table 2 show an improvement greater than two standard deviations, and only one of the eight comparisons on Table 3 (for the blue \"target metric that robust scaling aims to maximize\" columns) is greater than two standard deviations.\n\n* Small contribution - beyond the rescaling method, the proposed metric is not a standalone contribution. I think the metric is lacking a thorough discussion or motivation.\n\n# Minor comments\n\n* Figure 1 needs more detail. What dataset/task is this? Are the different points different initializations, hyperparameter configurations? There is very weak evidence of a tradeoff here, but the details are far too murky to draw much of a conclusion; if there were not lines drawn on the graphs I would not conclude there was any relationship.\n\n* Figure 2 also doesn't describe (a) the dataset/task, or (b) what models are used for each method, how they were selected, or give any notion of variability (i.e. Clopper-Pearson confidence intervals). Again, it is hard to learn much without this information.\n\n* It seems Figure 3 is missing any reference to a baseline; can this comparison be made e.g. to ERM? Also, how are we supposed to conclude that any other scaling (s \\neq 1) is an improvement based on this plot?\n\n* Other hyperparameters also seem to be selected in an ad hoc manner. Why is k=20 chosen? How does varying the number of clusters impact the experimental results?\n\n# Typos etc.\n\nPage 2: Footnote 1 should probably be in main text.\n\nPage 4: \"given n training examples without group annotations\" --> without *attribute* annotations (since group is attribute + label)",
            "clarity,_quality,_novelty_and_reproducibility": "See above. Reproducibility concerns are particularly around how hyperparameter searches/tuning were conducted, but the final values are reported.",
            "summary_of_the_review": "I have several concerns with the paper. In addition to the proposed method being neither well-motivated nor well-described, the experimental results also show only very small improvements (in most cases, within the realm of statistical variation) and over baselines that appear not to be tuned at all and perform far below the original published results of the baseline works. I think it is not ready for publication in its current form. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper527/Reviewer_TDJz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper527/Reviewer_TDJz"
        ]
    },
    {
        "id": "k-nw0Dutha",
        "original": null,
        "number": 2,
        "cdate": 1666427237617,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666427237617,
        "tmdate": 1669925458472,
        "tddate": null,
        "forum": "pkgVPeL9gpX",
        "replyto": "pkgVPeL9gpX",
        "invitation": "ICLR.cc/2023/Conference/Paper527/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a test-time rescaling strategy to address the trade-off between worse-case and average accuracy. The rescaling coefficient is optimized on a validation set to maximize a certain objective function, e.g., the worse-group accuracy. With different rescaling coefficients, the method is claimed to find a Pareto frontier of the two objectives (worse-case and average accuracy). The proposed method is compared with different baselines in the experiment and is shown to be effective on two datasets.",
            "strength_and_weaknesses": "Strength:\nThe paper is clearly written and the proposed method is straightforward. The experiment looks extensive. \n\n\nWeaknesses:\n\nThe reported baselines in this paper are not consistent with their official results. For example, GroupDRO has 93.5% average accuracy and 91.4% worse-case accuracy with ResNet50, but this paper's Table 2 reports a much lower accuracy for GroupDRO with ResNet18. The result of JTT has the same concern. So I cannot say the experiment is convincing to me.  Please show the experiment using the same backbone with baselines. \n\nOnly two binary classification datasets are used in the main experiment. It will be more convincing if the method works in more challenging tasks like CivilComments, WildCam or FMoW in WILDS benchmark, which contain more classes and groups. The FairFace with 9 classes are used to show the scalability of robust rescaling, but not compared with any baselines. I would like to see a fair comparison with existing baselines on more challenging and real-world datasets.\n\nThe title has a very strong statement that class-specific scaling is all you need, which is not supported by the experiment as least for me.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good. The overall quality is satisfactory but the experiment is not rigorous thus not convincing. The novelty is just at the borderline, since the test-time rescaling is not new [1]. \n\n[1] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022.\n\n",
            "summary_of_the_review": "Given the weaknesses, I would like to give a conservative score and encourage the author to do a more rigourous experiment in the next version to show the real effect of robust scaling. Otherwise, I cannot agree with the argument that class-specific scaling is ALL you need. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper527/Reviewer_2hCQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper527/Reviewer_2hCQ"
        ]
    },
    {
        "id": "qz9w9SBRfy",
        "original": null,
        "number": 3,
        "cdate": 1666472498984,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666472498984,
        "tmdate": 1666472498984,
        "tddate": null,
        "forum": "pkgVPeL9gpX",
        "replyto": "pkgVPeL9gpX",
        "invitation": "ICLR.cc/2023/Conference/Paper527/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper suggests rescaling the output probability to achieve group robustness. The idea is to find the scale factor using the unbiased validation set to minimize the balanced group accuracy on the validation set. Once the scale factor is found, it is applied during the test as a post-process module to the test prediction. ",
            "strength_and_weaknesses": "Strength:\n- The proposed method is simple yet very effective and can be easily added to previous methods. \n- The paper proposes a new metric to both evaluate average accuracy and worst-group accuracy. \n\nWeakness:\n- It is clear to me that the proposed method is able to maximize the robust accuracy, it remains unclear to me why it can achieve better balance between robust accuracy and worst-group accuracy than the other methods. \n- The method may heavily rely on the validation set to work and requires access to the group information of the validation set which sometimes is not necessarily available. \n- The method might also need a large validation set to work as the validation set is used to estimate the scaling factor and the centroid. It would be nice if an ablation study can be performed for different sizes of the validation set. \n\n\nQuestion:\n- Wondering instead of searching the scaling factor, can it be learned?\n- does adding a bias factor be more effective than scaling factor along? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe paper is well-written and the proposed method is simple yet novel in the spurious correlation literature.",
            "summary_of_the_review": "The paper proposes a simple yet novel technique to post-process the network prediction to achieve group robustness. The method is well-motivated and experiments are conducted to validate its effectiveness.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper527/Reviewer_yYTs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper527/Reviewer_yYTs"
        ]
    },
    {
        "id": "y6hubZfHVKQ",
        "original": null,
        "number": 4,
        "cdate": 1666901025640,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666901025640,
        "tmdate": 1670555535404,
        "tddate": null,
        "forum": "pkgVPeL9gpX",
        "replyto": "pkgVPeL9gpX",
        "invitation": "ICLR.cc/2023/Conference/Paper527/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work presents a simple post-training strategy to balance the worst-group and average-case accuracies of a classifier. In addition, it proposes a new metric to summarize this tradeoff. An extension on the simple method is also proposed to achieve better results. The proposed method outperforms several existing baselines and helps to put prior work on group robustness in context.",
            "strength_and_weaknesses": "Strengths:\n- the proposed method (RS) and its extension (IRS) are both simple and effective\n- the robust coverage method is a nice way to summarize the worst-avg accuracy tradeoff and is a strong contribution\n- the related works section is written very well.\n\nWeaknesses:\n- Evaluation on more datasets would help to strengthen the analysis. In particular, the BalancingGroups codebase (https://github.com/facebookresearch/BalancingGroups) would be a great point for this to add 2 text classification tasks.\n- Following the above work, this work should also compare against subsampling baselines (SUBG/SUBY).\n- The results are often not presented in the clearest way (elaborated in more detail below).\n- In general, it is unclear to me whether this paper thinks RS or IRS should be the main method (there is also a discussion of ARS in the appendix, which seems to have very strong results but confusingly was not even discussed in the main text). It seems that IRS does have strong results on Waterbirds and should be highlighted more - in fact, I would suggest adding IRS to figure 2 and presenting all results (tables 1 & 2) with both RS and IRS. (As a side note, it is also unclear why IRS was not evaluated on GDRO/JTT etc in table 3). This point may seem small, but it has a significant impact on how the results are interpreted overall.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is easy to read, but plots could be significantly better and the results better presented:\n- I did not understand what the takeaway from figure 1 was. I would suggest having figure 2 as the main figure 1.\n- Which dataset is presented in Figure 1?\n- Figure 4 can safely be moved to the appendix - added value is not that much.\n- When reading, it was often unclear whether the results with RS referred to IRS or to RS. This is since IRS is presented before any experiments, so in the first part of the experiments it was unclear whether the results were for RS or IRS. To remedy this, one suggestion would be to introduce the IRS method *after* the first set of RS experiments.\n- Why are table 4 experiments performed on a completely random dataset used nowhere else in the paper?\n- Figure 5 bottom can be safely removed - it does not add any additional info to Figure 5 top. I would suggest using the space to draw the scaling curves for GDRO, JTT, and other important baselines instead.\n- There is no discussion of Table 2 in section 4.2.",
            "summary_of_the_review": "This paper presents some important insights regarding the tradeoff between worst and average accuracy. It presents a simple method to allow trading off the two. At the same time, this work lacks some experimental rigor (only 2 datasets and missing subsampling baseline) and clarity in exposition (the RS & IRS results are difficult to compare to each other). If these last two points were done well, there would be a nice takeaway along two directions: 1) worst and avg accuracy can be traded off for all models, and 2) RS & IRS are two simple methods that do this with strong empirical performance. Due to the limitations outlined, however, I can only have confidence and clarity in takeaway #1. This is the reason for my rating.\n\n------------------------ UPDATE AFTER REBUTTAL --------------------\n\nMy main criticisms with this work are around clarity and the small number of datasets. The authors have addressed clarity somewhat, and importantly have added results on two more datasets - CivilComments and FMOW. The results on CivilComments are just as impressive/notable as those on CelebA. The results on FMOW are a bit muddied, which is due to ERM performing worse than GDRO, so the story here is a bit nuanced but I am glad the authors included the results nonetheless. Given that evaluation has been much improved, I have updated my rating of this paper to accept. That being said, I would like to see a couple things in the camera ready:\n- Full evaluation results of all the baselines on the new datasets.\n- A figure or table within the main text or appendix that averages the numbers across all datasets (to provide a wholistic view of the method).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper527/Reviewer_3mfo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper527/Reviewer_3mfo"
        ]
    }
]