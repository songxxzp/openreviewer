[
    {
        "id": "9_KRhnB1ye",
        "original": null,
        "number": 1,
        "cdate": 1666674490324,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674490324,
        "tmdate": 1666674490324,
        "tddate": null,
        "forum": "9OoFFWDPDQ",
        "replyto": "9OoFFWDPDQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2623/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The goal of this study is to measure the \u201coppeness\u201d of CLIP-based models. The authors are adding class names that are not part of the target dataset domain and evaluating the change in accuracy from that. They then show how poorly models do and propose a new approach to this via two metrics: inter-modal alignment and intra-modal uniformity.The authors also perform prompt-engineering by generating a better search algorithm from the original dataset.",
            "strength_and_weaknesses": "**Strengths**\n\n- The authors provide in-depth empirical study to analyze the performance of CLIP on novel concepts.\n\n- The authors are studying an interesting area that is relevant and useful to the community.\n\n\n**Weaknesses**\n\n- The main weakness of that the paper is that it is very confusing and does not flow correctly. Unfortunately, I think it is because there is too much packed in. The first half of the paper addresses the problem the authors are trying to solve in great detail but was confusing because some terms were undefined and the purpose of this analysis was not necessarily clear. How does it differ from text-to-image retrieval? Is this all zero-shot, in which case, why are we interested if the model performs less accurately on a fixed, target dataset if we add some classes not relevant to the target dataset? How was the superclass hierarchy used? This is all a lot of experimentation but not clearly laid out. \n\n- Then the last \u00bc of the paper is a proposed approach and it felt very quick and rushed. It was very difficult to get the full grasp of the change or the approach that was used.\n\n**More details:**\n\n  - Terms are not properly defined, like \u201cvocabulary extension\u201d. This could mean many things, for example, the actual text-encoder has an embedding space with a fixed vocabulary size, is this what the authors are extending? If so, should models using a BERT-like text encoder not be compared since it uses sub-words during tokenization?\n\n  - It is confusing how this task varies from text-to-image/image-to-text retrieval when you have an image and several text pairs and you are measuring the accuracy of that retrieval. Is this task not just taking a classification problem and making it more like a retrieval problem because the classes are no longer fixed? If the initial analysis is zero-shot, than this difference should probably be discussed as it would also evaluate the \u201copenness\u201d without claiming that the model performs poorly on this fixed classification datasets. Furthermore, given this is zero-shot learning, how is that not evaluating \u201coppenness\u201d if it has not seen these images and text pairs during training? These concepts should be clarified and explained.\n  \n  - Using some kind of hierarchical superclass organization sounds very interesting but is not clearly explained. \n\n  - The REPE approach definition mentions KNN and FAISS in one sentence, but does not provide enough information so it is confusing how this actually works.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The explanation of the REPE approach is not focused on enough and is unclear.",
            "summary_of_the_review": "Overall, it felt like there is too much focus on the underlying problem and not enough on their proposed approaches REPE. The reader is left with only questions.\n\nIf the paper is re-focused with more emphasis on the why and how this differs from other prompting approaches, and certain concepts clarified, this paper would have more clarity.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2623/Reviewer_hc5X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2623/Reviewer_hc5X"
        ]
    },
    {
        "id": "Espc_criLe",
        "original": null,
        "number": 2,
        "cdate": 1666697170308,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697170308,
        "tmdate": 1666697517315,
        "tddate": null,
        "forum": "9OoFFWDPDQ",
        "replyto": "9OoFFWDPDQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2623/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduced a new concept extensibility, to explore the openness of CLIP on visual recognition task. Two new evaluation metrics Acc-E and Acc-S were proposed to quantify CLIP's extensibility and stability. The paper argued the confusion between text embeddings of different classes made the CLIP unstable in extensibility, and finally a new prompt engineering module, REPE was proposed to handle this issue.",
            "strength_and_weaknesses": "The paper is easy to understand and the topic of evaluating the openness of CLIP in visual task is good and important. However there are some concerns for me to accept the paper.\n- For Sec 2.2, the author proposed ACC-E to validate the extensibility of the CLIP by evaluate on different permutation of the vocabulary set.  I do not fully understand the principle and advances of the proposed evaluation mechanism. A more straightforward way is to apply the CLIP directly in different datasets with different vocabulary sizes and report the average score, especially for some uncommon classes which may not been covered by the pertaining stage of CLIP, e.g., the local food dishes in different countries.  \n- For Sec 2.2 and Table 1, the openness issue is reported in the small dataset CIFAR100. With larger dataset imageNet and stronger CLIP backbone, the gap between Acc-E/Acc-S and Acc-C reduces significantly (~13% -> ~3%). In real world, the class number is far more than ImageNet and the gap may potentially become even more marginal.\n- For Sec 2.3, it's quite reasonable in visual recognition task that involving more classes, the overall performance will drop due to the class confusion.\n- For Table 2, the results fail to validate the effectiveness of REPE to address the openness issue. The gap between Acc-E/Acc-S and Acc-C remains unchanged for both vanilla prompt design and REPE, except REPE reports higher results. REPE is a prompt engineering module and thus it's reasonable the results can improve. However, the main topic of the paper is not the prompt engineering but the openness issue of CLIP, and this issue still remains. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the code has been released so there is no issue to reproduce the paper. There are lots of experiments in the paper, yet the experiment design is not reasonable to validate the assumption.",
            "summary_of_the_review": "The motivation of the paper is important, to evaluate the openness of CLIP in visual task. However, the experiment design is not reasonable and the proposed prompt engineering cannot address reported issue in the paper either. Overall speaking, the paper is below the bar of acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2623/Reviewer_c4Um"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2623/Reviewer_c4Um"
        ]
    },
    {
        "id": "HEk4F4TnFB6",
        "original": null,
        "number": 3,
        "cdate": 1666745480364,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666745480364,
        "tmdate": 1666745480364,
        "tddate": null,
        "forum": "9OoFFWDPDQ",
        "replyto": "9OoFFWDPDQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2623/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors perform a detailed study of the \"openness\" of the CLIP image-text model and variants. Specifically, they study the ability of CLIP to generalize to new categories of objects in the zero-shot setting. For this, they propose an evaluation protocol that analyzes CLIP's performance along two axes - extensibility and stability. Extensibility refers to the ability of the model to deal with new object classes when the vocabulary of object classes is expanded. This means, having a set of target classes which is progressively grown in size by adding new categories. The measure of stability is interesting as it refers to keeping the set of target classes fixed, but adding a number of distractor classes that the model could potentially classify, but for which no images are actually present. Models which are stable are able to continue performing well on the set of target classes despite the distractors. The authors argue that stability may more realistically account for the model's performance in the real-world zero-shot setting.\nThe authors results show that CLIP models suffer from poor extensibility and stability in the real world. They show significant performance drops using three benchmarks (CIFAR100, ImageNet (Entity13) and ImageNet (Living17).\nTo explain the performance drops, the authors study the margin between positive and negative classes and observe this is the direct contributing factor to the model's poor stability. The authors discover classes which, when added to the set of options, cause significant misclassifications based on the similarity of the features. \nThe authors also study two other properties of the feature space. Specifically, the inter-modal alignment and intra-modal uniformity.\nInter-modal alignment is essentially just the average distance between the positive image and text pairs of embeddings.\nThe intra-modal uniformity basically samples pairs of samples from the same modality and takes their distance.\nThey show that existing models perform poorly on these measures.\nFinally, authors propose a new method for addressing these issues called REPE. The basic idea is to not use a standard text prompt like \"a photo of a [class]\" but to instead learn a class-specific text-embedding. To do this, given a prompt of the class name, the authors look in the original dataset used to train CLIP to find similar images. Then, they find the captions that correspond to those images. They encode the captions and then essentially just average them with some weighting.\nThe authors show that this approach improves the extensibility and stability of CLIP by 1.2% compared to not using it.",
            "strength_and_weaknesses": "[Strengths]\nThe authors do a deep-dive on an important aspect of these large-scale vision-language models, i.e. how well do they actually generalize to their use-case. To do this, they examined two measures to account for this performance drop in the real-world - i.e. extensibility and stability. They also do a deep-drive into the feature spaces of these various methods using a number of metrics they develop and demonstrate quantitatively the problems with the feature spaces that their method tries to address.\n\nBeyond just evaluating the method and exposing the problem on CLIP's feature space, the authors also propose a method to improve CLIP's extensibility and stability. The proposed method is quite straightforward but generates consistent small gains over not using it.\n\n[Weaknesses]\nAs the authors cite in their paper, much past work has explored prompt tuning for CLIP. For example, in Table 1, we see CoOP (a prompt tuning method) achieves 76.7% while authors method, shown only with vanilla CLIP in Table 2 achieves 72.6%, for example. In other words, the CoOp approach works better than authors' method. Of course, CoOp requires access to the downstream target dataset. Yet, is this really a fair comparison, since in your approach we *do* have access to a dataset used to train CLIP. Why not use your idea to retrieve relevant images / text on the CLIP training dataset, if that's fair game?\nIn sum, the authors' approach is assuming that the large-scale CLIP pre-training dataset is available for querying. But if it is, perhaps some of these other methods could work better as well. In that scenario, it is unfair to compare just to vanilla CLIP.\n\nThe authors' method is also of low technical novelty and consists mainly of finding other captions in the train dataset and taking an average. Similarly, the proposed \"measures\" like the average distance between positive classes and the uniformity are quite simple. It is also unclear to me whether the intra-modal uniformity is a good measure. For example, it adds together the average distance between image pairs and text pairs. But these are two embedding spaces learned by two different models. Shouldn't these be normalized in some way so the two distributions (image and text embedding space) is directly comparable? For example, right now one modality might dominate. Thus, there is very little technical novelty to authors' approach.\n\nSimilarly, while authors have pointed out important problems, it isn't clear to me that the proposed approach is well-motivated to solving them. For example, why does taking the average caption solve these problems? Perhaps a better training paradigm, different representation space (e.g. hyperbolic), etc. is the correct way to address the problems the authors have pointed out, rather than just taking the average caption. Thus, it seems that the authors point out a clear problem, but that the proposed method seems ad-hoc and unrelated to the action problem. Perhaps a better way to handle it would be to learn some class-agnostic projection that solves these problems or something.\n\nI'll also point out that the author's method requires this additional step of keeping the CLIP training dataset, indexing it, and doing this class-averaging step. Thus, there is this unnatural overhead to using their method for only around a 1.2% gain in performance. It's not clear that the overhead justifies its use.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality of the paper is clear and the paper is quite easy to follow.\nThe authors include detailed explanations that are intuitive and the motivation of the paper is very clearly defined and the paper is well-argued. The presentation is of high-quality.\nThe technical novelty, as mentioned in the weaknesses above, is quite low. The significance of the method or contribution is not clear. In my view, it is of incremental novelty over existing prompt leaerning approaches.\nThe paper is also reproducible - sufficient details are present to clearly implement the approach.",
            "summary_of_the_review": "The authors' paper makes a compelling case by pointing out key weaknesses of CLIP. They demonstrate this both quantitatively and qualitatively through illustrations in their paper. They include many supplemental results as well. However, the proposed method seems ad-hoc and doesn't necessarily directly address the problems pointed out. It also is of low novelty and results in only minor performance gains with considerable technical overhead. Thus, in my view the paper should not be accepted at this time.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2623/Reviewer_8DmZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2623/Reviewer_8DmZ"
        ]
    },
    {
        "id": "MrfoZLHZvU",
        "original": null,
        "number": 4,
        "cdate": 1666901634255,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666901634255,
        "tmdate": 1669064490187,
        "tddate": null,
        "forum": "9OoFFWDPDQ",
        "replyto": "9OoFFWDPDQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2623/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles three major issues with CLIP-like models: openness, extensibility and stability. The paper argues that CLIP-like models are difficult to evaluate since they are unconstrained in terms of the vocabulary use. Therefore the paper proposes to use an incremental evaluation perspective, called extensibility, to test the the models\u2019 ability to learn new concepts. \u00a0\n\nThe paper also suggests that the ambiguity in openness is not due to the failure to capture image-text similarity, but rather confusion among similar textual features. For this, the paper proposes to use a retrieval-augmented relevant texts to impose distinction between competing texts which the paper shows improves the extensibility and stability of CLIP without fine-tuning.",
            "strength_and_weaknesses": "**Strength**\n\n1. The paper is one of the firsts to investigate the openness of CLIP-like models, with 2 evaluation measures: _extensibility_ and _stability_. And the paper proposes a method called REPE (Retrieval Enhanced Prompt Engineering) to tackle the two issues. \u00a0\n    \n2. The experiments demonstrate that the CLIP-like models are indeed susceptible to fail in visual recognition, even with marginal difference in text features of different classes. \u00a0 \u00a0\n    \n3. The experiments highlight that such models, which are decent zero-shot learners, do not scale well for larger vocabulary size.\n    \n\n**Weakness**\n\n1. What is the significance of using REPE, when the incremental advantages do not seem visually (Fig.7) or numerically (Tab. 2) considerable?\n    \n2. In addition to retrieving the K nearest image captions and filtering out the class name to add visual semantics in REPE, is it possible to use the bottom-K captions and recompute the accuracies (C, E, S)? \u00a0The motivation of the paper is quite clear, however I wonder how much would the accuracies suffer if given the worst captions. The increase in accuracies being marginal, this thought occurred to me.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The paper is easy to follow, with proper references to additional material in the appendices.\n\n**Quality**: The work is strongly assertive of the issues with using CLIP-like models for open set recognition. Additionally, the experiments are extensively done to expose and formalize the openness-issue with extensibility and stability concepts. Additionally, the use of REPE kind of suggests the direction to improve for future models.\n\n**Novelty**: It is an issue I am very confused with the given work. Novel in what sense? The paper does provide good insights by using their extensibility and stability analysis. This can be considered as novel direction to evaluate for making the CLIP-like models more \u2018evaluable\u2019.  ",
            "summary_of_the_review": "The paper does good job on highlighting and quantifying issue of open world recognition with CLIP-like models. However, there are other methods poised to be developed since it seems to be a known issue. Therefore, REPE giving marginal gains does not seem a strong base for future improvement. The experiments are sufficient and extensive for the given proposal. \n\nGiven many insights into the analysis with extensibility and stability, I believe that the paper has good potential and value to add to the community. \n\n--------------------------------------------------------------------------------------\nAfter reviewing the responses, I concur with other reviewers that the paper has low technical novelty. REPE is not very superior to other methods.\n\nHowever, I still believe that the paper is good work and could serve as a nice direction for future work. I will keep my rating to above acceptance threshold. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2623/Reviewer_No3A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2623/Reviewer_No3A"
        ]
    }
]