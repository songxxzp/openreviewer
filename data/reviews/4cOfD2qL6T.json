[
    {
        "id": "h-rItKtD31z",
        "original": null,
        "number": 1,
        "cdate": 1666641225208,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641225208,
        "tmdate": 1669487092813,
        "tddate": null,
        "forum": "4cOfD2qL6T",
        "replyto": "4cOfD2qL6T",
        "invitation": "ICLR.cc/2023/Conference/Paper5276/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present an analysis of deep learning models using the change in curvature across frames of videos from pixel space to the representation space. The authors find that training deep networks on classification with adversarial images leads to straighter (like human visual representations, Henaff et al 2019) representations. They also analyze biologically plausible networks to show that there is some (in the early layers) increase in straightness but not consistent across the complete hierarchy. I find the presented analyses to be interesting, but I don't think the authors have clearly conveyed how their contributions impact either machine learning or vision science. ",
            "strength_and_weaknesses": "Strengths:\n1) Clarity of writing and figures. The paper is written with great clarity. Figures such as Fig 1 and Fig 2 are elegant and clear to convey what the authors are studying in this work. \n2) Diverse evaluation. The authors have evaluated the representational straightness in a wide range of diverse deep neural network architectures from the robust and non-robust classes for image recognition and segmentation. They also evaluate multiple biologically plausible models that are of great interest to the visual neuroscience and vision science communities. \n3) Fig 5, $\\epsilon$ vs curvature change. I found this result to be quite interesting that increasing adversarial attack strength also increases straightness. It would be nice to see how much each of these attack strengths contributes to adversarial robustness as well.\n\nWeaknesses:\n1) Significance of the contribution to representation learning. The authors have mentioned in several places how straighter representations bear more advantages than test accuracy, such as robustness, alignment to human perception, and representational stability. However, I don't find any experiment/analysis that quantifies these advantages or even demonstrates them. Is representational straightness predictive of improved robustness to adversarial attacks and OOD shift? While it is obvious that adversarial training improves robustness to adversarial attacks, I would like to see the relationship between adversarial robustness and representational straightness. Similarly one could also correlate straightness with model generalization to OOD shift on benchmarks such as ImageNet-c. These evaluations would add more information about how significant the straightness changes are and how they relate to the representation quality.\n\n2) Non-matching models in robust and non-robust class. Since one of the main takeaways of the paper is that adversarial training improves straightness, I expect to see clearly how the same architecture (and same rnd seed) -- when fixing other factors of variability such as dataset, training set size, training hyperparameters -- varies in their representation straightness. There are some matching models in the robust and non-robust classes in Fig 3. If the above conditions of varying adversarial training in an isolated manner was what was performed for this analysis, I would suggest the authors please make this clear in writing. \n\n3) Hard to understand model specifications from legends. I believe in some figures (e.g Fig 4, 5, 6) it is difficult to know the combination of architecture + training dataset. I would suggest the authors to use a common template to define models e.g. Architecture_NumberOfLayers_Dataset_Task and it would make reading the plots much easier. \n\n4) Discussion section. Currently the discussion section presents a summary of the results in the paper (how curvature changes as a function of adversarial training, optimization task etc.) but does not discuss in detail why these observations are relevant to designing future deep learning models or to human visual representations. This section could be greatly improved to highlight why, in the authors' perspective, the results are of importance to representation learning and human vision.",
            "clarity,_quality,_novelty_and_reproducibility": "- The presented work is written with great clarity. Figures that explain the authors methods such as Figs 1, 2 were very useful to understand the paper's main theme. Some of the result figure legends however lacked clarity as highlighted in my reviews. \n- This work is a novel extension of prior work analysing representational straightness in the context of adversarial training. I don't find any big reproducibility issues per se, however, the code and weights for segmentation models isn't available in the Network Comparison Spreadsheet in the supplemental data.",
            "summary_of_the_review": "I think the paper presents an interesting set of results linking representational straightness to adversarial training. However, without any evidence on a) how straightness better predicts human visual behavior/neural data or b) how straightness leads to more adversarially or OOD robust deep learning models, I am presented with a partial picture of the significance of the contributions. At this stage, I find the paper to be borderline and would be comfortable recommending acceptance if the authors make the suggested changes mentioned in my review above. Mainly, addressing the following points (quoting from my review) and how the results turn out to be will influence a change in my score:\n\n\"Is representational straightness predictive of improved robustness to adversarial attacks and OOD shift? While it is obvious that adversarial training improves robustness to adversarial attacks, I would like to see the relationship between adversarial robustness and representational straightness. Similarly one could also correlate straightness with model generalization to OOD shift on benchmarks such as ImageNet-c. These evaluations would add more information about how significant the straightness changes are and how they relate to the representation quality.\"",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5276/Reviewer_eEJj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5276/Reviewer_eEJj"
        ]
    },
    {
        "id": "uG_a3WpQklp",
        "original": null,
        "number": 2,
        "cdate": 1666665067999,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665067999,
        "tmdate": 1669229414210,
        "tddate": null,
        "forum": "4cOfD2qL6T",
        "replyto": "4cOfD2qL6T",
        "invitation": "ICLR.cc/2023/Conference/Paper5276/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates the perceptual straightness of a large set of modern vision models. Authors start from the observation that representation straightness is a known property of biological vision. They then define a metric to capture the average output curvature on a video and evaluate a wide range of models (resnet/vit trained for classification, segmentation models and biologically inspired model).\n\nThe main conclusions of the study are:\n1)\tNon-adversely trained models have the highest output curvature\n2)\tAdversarial training can reduce the model output curvature, when trained with classification tasks but not for all the tasks, i.e. it is not case for segmentation.\n3)\tBiologically inspired mechanism can also reduce the model output curvatures.\n",
            "strength_and_weaknesses": "Strength:\n-\tThe proposed study appears novel to me and is well motivated.\n-\t Authors perform an extensive evaluation on impressive number of models.\n\nWeaknesses:\n-\tWhile the study is motivated by finding in biological system, it\u2019s unclear why perceptual straightness is a desirable property from a machine learning perspective. It would be nice to identify a set of tasks where improved straightness has a benefit, beyond adversarial robustness.\n-\tWhat is the impact of the dataset distribution on which we compute the output curvature? The evaluation protocol only uses 12 videos to compute this metric. Would the finding remain stable if we were to use different videos?\n-\tVideos are most likely out-of-distribution with respect to the training dataset. Would the finding be similar if we were to reduce the distribution shift between training and evaluation?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, and the empirical evaluation appears and novel to me.",
            "summary_of_the_review": "The paper proposes an interesting empirical study. The significance of the paper could be improved by better demonstrating the advantage of representation with a higher level of straightness and showing that the conclusions are robust to the choice of the video dataset used in evaluation.\n\n\n=== After reading rebuttal.\n\nThank you for your responses. The rebuttal addressed most of my original concerns, I updated my score accordingly.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5276/Reviewer_zqwQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5276/Reviewer_zqwQ"
        ]
    },
    {
        "id": "6gn8aWn1K2M",
        "original": null,
        "number": 3,
        "cdate": 1666776088909,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666776088909,
        "tmdate": 1669711327081,
        "tddate": null,
        "forum": "4cOfD2qL6T",
        "replyto": "4cOfD2qL6T",
        "invitation": "ICLR.cc/2023/Conference/Paper5276/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to leverage a recent finding about human visual perception of movies to evaluate a variety of neural network architectures trained on different tasks. The visual feature is that humans tend to perceive movies with relatively less curvature from frame to frame compared to the actual curvature measured on pixels. We say that visual perception is straightening movies. The proposed work has three contributions :\n(i) adversarial training often leads to straightening in CNN and Vision Transformers,\n(ii) straightening depends on the training task (eg no straightening for segmentation),\n(iii) biologically-inspired piece of architecture always leads to straightening when going deeper and deeper. ",
            "strength_and_weaknesses": "Strength :\n- straightening in neural network hasn't been previously evaluated,\n- the work provides an overview of straightening in many architectures trained on many tasks,\n- the work establishes a link between straightening and robustness,\n- single focus paper with a clear message.\n\nWeaknesses : \n- the claims about robustness are weak because only adversarial training is used while there are other ways to enforce robustness,\n- the authors failed to explain why \"curvature is a useful way of evaluating neural networks representations\". ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is clearly written.\nSome citations are not in parentheses while they should be.\nThe text fontsize in the figures should be almost as large as the main text fontsize.\nThe figures in the supplementary are not referred to in the main text. As such they are useless. Other supplementary sections should also be referred to in the main text.\n\n\nQuality:\nMy opinion is that it is great to have neural network architectures that are aligned with visual perception and neurosciences. Yet, the interest of the paper might be limited to an already acquired audience if the authors do not provide more explanations about why straightening is useful or interesting to have in NNs. \nThe performed work is of good quality. Yet, as it is a single focus paper I would expect to find a broader evaluation of robustness and not only adversarial training.\n\nNovelty:\nEvaluation of straightening in neural network representation is novel but this is the single focus of this paper.\n\n\nReproducibility:\nSeems reproducible if the code is made available online. ",
            "summary_of_the_review": "For now, I slightly tend to reject the paper. Yet, I will reconsider after reading authors responses and others discussions among reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5276/Reviewer_EBgB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5276/Reviewer_EBgB"
        ]
    }
]