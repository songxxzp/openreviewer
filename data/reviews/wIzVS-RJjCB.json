[
    {
        "id": "msJiHAFiuK4",
        "original": null,
        "number": 1,
        "cdate": 1666159423328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666159423328,
        "tmdate": 1666159500035,
        "tddate": null,
        "forum": "wIzVS-RJjCB",
        "replyto": "wIzVS-RJjCB",
        "invitation": "ICLR.cc/2023/Conference/Paper5928/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a model to turn one or more entities into a natural language sentence that describes their relationship (or define the entity when there's just one). The model is a modified BART model trained with entity->sentence samples from Wikipedia. By doing a single seq2seq training, the model is able to perform definition and relation description tasks with one set of parameters.\n\nEvaluation was done on open datasets of definition (DC), relation modeling (ORM) and hyper-relation modeling (CommonGen). The results were compared with vanilla BART model on an array of commonly used metrics. It showed that the proposed model (VER) outperforms BART in low-resource settings. When the training data is sufficient, their performance is on-par.\n\nThe paper argued that the model can be used as a tool for human users to explore new concept, as well as a base model for fine-tuning towards related tasks. The generated NL descriptions may serve as an interpretable  knowledge base. ",
            "strength_and_weaknesses": "Strength:\n\n1) This paper is really well written. It's very easy to follow the author's thoughts, and all details (e.g training set up) have been explained clearly. The Reviewer didn't spot any obvious errors in writing.\n\nWeakness:\n\n1) The baseline is weak (BART). Given that VER is a model fine-tuned on BART for this task, it's almost guaranteed to be better performing. The Reviewer would like to see comparisons with STOA algorithms, even if VER is not surpassing them.\n2) Unclear about generalization ability. The data for training (Wikipedia) is already a pretty comprehensive set of descriptive relationships of entities. It's not clear how the model could generalize, vs just remembering what appeared in Wikipedia text. It's not clear to the Reviewer about how much overlap do Wikipedia and the evaluation data have.\n3) No discussion on hallucination. In order to make the results useful, e.g. helping human users or serve as a knowledge base, we need to understand how correct is VER's output. If for new (or even existing) entities, the model simply hallucinates to produce seemingly correct sentences, it would do more harm than help.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is top-notch. The Reviewer has no complaints about it. A minor suggestion is to shorten Eq.1 to Eq.3 as they basically talk about the same thing.\n\nQuality weakness is mostly in the evaluation, that a weak baseline was used, and the risk of hallucination was not covered. \n\nNovelty is weak. The methodology is a small adaption of BART to a new seq2seq task. The setup is quite routine. \n\nReproducibility is good. The authors shared part of the model and promised to share the bigger one when published.\n\nMinor suggestion: the authors might want to add a \"pronounced as xxx\" to the emoji-embedded name of the model.",
            "summary_of_the_review": "The Reviewer thinks the paper is a well written one, very easy to follow. However, the result was not thoroughly tested (weak baseline) and discussed (hallucination risk). The methodology itself is straightforward and training setup is quite routine, with less discussion on design challenges. \n\nThe Reviewer would suggest adding stronger, STOA baselines and check the model's generalization abilities beyond Wikipedia, as well as assess the hallucination risk.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5928/Reviewer_Fgdc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5928/Reviewer_Fgdc"
        ]
    },
    {
        "id": "1MMM48Bw3PQ",
        "original": null,
        "number": 2,
        "cdate": 1666820997482,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666820997482,
        "tmdate": 1666877348927,
        "tddate": null,
        "forum": "wIzVS-RJjCB",
        "replyto": "wIzVS-RJjCB",
        "invitation": "ICLR.cc/2023/Conference/Paper5928/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The work is about verbalizing entities and the relationships between them. According to the authors, existing \nworks deal with entity and relation verbalization separately. This work aims to solve entity and relation verbalization \nin a unified form by adaptive generative language models to the task of entity and relation understanding.\n\nSpecifically, definition modeling and relation modeling are formulated uniformly as follows:\ngiven a set of entities (or a single entity) in string form, a sentence is generated that describes the entities and their relationships. \nWhen a single entity is given, it is equivalent to definition modeling. When the size of the set is 2, it is equivalent to relation modeling.\n\nThe proposed approach is called VER (a Unified Model for Verbalizing Entities and Relations):\n- first, a large dataset is constrcuted from Wikipedia. The first (5) sentences from each page are included in the dataset, as those sentences \nare often definitions of the entity or its relationships. Entity mentions are identified using Wikipedia hyperlinks. \nThe models are pre-trained using `self-supervised text reconstruction': given an entity or a set of entities, they are matched with the relevant sentences\nin the training corpus. \n\nThe model of choice is BART (a transformer-based encoder-decoder architecture), using both BART-base and BART-large.\nThe following settings are evaluated: using full training data, \nEvaluate three settings: fine-tuning on full training data, partial data (10%; a low-resource setup) and with no fine-tuning.\n\nThe experiments mainly consider datasets in scientific domains (CS, math, physics).\nAlso, the models are tested on CommonGen, where given multiple entities, the goal is to generate a coherent\nsentence describing an everyday scenario using these concepts. The reference sentences of\nthe official test set of CommonGen are not released, and results are computed via a leaderboard. \nThe authors acknowledge however that state-of-the-art performance is achieved by leveraging external knowledge sources. \n\nThe results demonstrate that after continually pre-training the model with the\nentity(s)-to-sentence reconstruction task, the model acquires a better ability to verbalize entities.\nIn the low-resource setup, VER achieves a more significant improvement compared with BART. \n\nTo validate whether the joint training of relation modeling will benefit or harm the performance of\ndefinition modeling, a version of VER is trained only with data examples of a single entity. The results show that relation modeling \nand hyper-relation modeling do not harm definition modeling.",
            "strength_and_weaknesses": "Pro:\n- An interesting task\n- A creative solution\n\nCons:\n- The experimental setup is not clear\n- The experiments are not comprehensive",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not very clear technically, and is not described in enough detail to be reproducible. \nMore detailed comments:\n- If BART was pretrained on Wikipedia, it merely learns to extract sentences that are similar to the first sentence(s) on a Wikipedia page\n(either by memorization, or based on style cues?). More discussion and error analysis might shed light on what the model is intended to learn, and what is learns in practice. \n- \"For each page, we extract the plain text by WikiExtractor3. And we use the neuralcoref (Clark & Manning, 2016) coreference resolution \ntool in spaCy4 to preprocess the documents.\" - I can guess why, however the motivation to do so should be stated.\n- Crucially, it is not clear what the difference is exactly between BART and VER (Table 1). Is BART also fine-tuned using\nthe same data, but with consecutive sentences?\n- The paper should include some explanation about the datasets to make the paper self-contained.\n- In Table 2, the full BART gives better sentence than VER. Can you refer to that?\n- It would be interesting to compare the generated definition sentences with formal glosses.\n- The results should compare with some other existing methods (which are mentioned in the related work section) on definition generation.",
            "summary_of_the_review": "The paper is not clear technically, and is not described in enough detail to be reproducible. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5928/Reviewer_m8ZD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5928/Reviewer_m8ZD"
        ]
    },
    {
        "id": "6f3EcJW5wQn",
        "original": null,
        "number": 3,
        "cdate": 1666987496804,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666987496804,
        "tmdate": 1666987496804,
        "tddate": null,
        "forum": "wIzVS-RJjCB",
        "replyto": "wIzVS-RJjCB",
        "invitation": "ICLR.cc/2023/Conference/Paper5928/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method for combining definition modeling, relation modeling and hyper-relation modeling towards the purpose of generating natural language utterances for verbalizing entities and relations. The contributions of the paper are in combining multiple techniques of entity and relation modeling and devising a new pre-training task for \u201centity(s)-->sentence\u201d reconstruction. ",
            "strength_and_weaknesses": "Strengths:\u00a0\n- The results are impressively better than the baseline (BART)\n- The dataset generation is interesting and quite interesting and relevant to the community \u2013 would this dataset be released to the public?\u00a0\n\nWeaknesses:\u00a0\n- I was not convinced with the motivation behind verbalizing entities and relations. The paper focuses the story in the beginning on how humans learn entities and relations, and then goes on to saying they have developed a method that can verbalize entities and relations the way humans do. However, humans do this to create a better model of the world for themselves. How is this verbalization useful to machines, if we continue with the analogy to humans? Furthermore, real applications to this method are not mentioned until the end of the introduction section. I would expect these to be mentioned much sooner in the paper.\u00a0\n- Results compare different generation metrics. However, we know how contentious these are in the literature right now. Could there be an explanation provided on how significant the performance improvements over the baseline BART are?\u00a0\n- I was expecting a comparison to Lin et al 2020 for the hyper relation modeling since that was mentioned at the end of the introduction section. However, the results once again only compare BART as a baseline for the hyper relation modeling (Table 3). You may correct me if I am looking at the wrong table for those results.\u00a0\n- For the dataset evaluation, can there be a discussion provided on the train vs test splits \u2013\u00a0 mainly to answer the question that if you are evaluating the model on generation for entities and relations, do you provide different sets of them for train vs test set ? Does the model run into memorization issues, similar to how many generation models do, and if so, how much? Does the model generate different utterances semantically for the same entities and relations, across different runs? ",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clearly written, while holding slight novelty. ",
            "summary_of_the_review": "I found the paper to be an interesting read, but was left with many unanswered questions as listed in the weaknesses section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5928/Reviewer_yzmq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5928/Reviewer_yzmq"
        ]
    },
    {
        "id": "pDOym-h4L-",
        "original": null,
        "number": 4,
        "cdate": 1667117062047,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667117062047,
        "tmdate": 1667117062047,
        "tddate": null,
        "forum": "wIzVS-RJjCB",
        "replyto": "wIzVS-RJjCB",
        "invitation": "ICLR.cc/2023/Conference/Paper5928/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper connects definition modeling, relation modeling, and hyper-relation modeling in a unified form. The paper pre-trains VER (A\nUnified Model for Verbalizing Entities and Relations) on a large training data by forming the \u201centity(s) \u2192 sentence\u201d reconstruction task, which makes VER a useful tool for learning natural language representations of entities and relations.",
            "strength_and_weaknesses": "Strength:\n1) The paper proposes a fundamental and unified way to pre-train the models for learning natural language representations of entities and relations.\n2) Extensive experiments on three tasks and six datasets demonstrate the superiority of the proposed method, especially in low-resource settings.\n\nWeakness:\n1) There are no comparisons between the proposed method and SOTA results in each task. It is understandable that the main contribution of the paper is about learning natural language representations of entities and relations in a unified way. However, adding comparisons between the proposed method and SOTA in an aligned way (e.g., without using external knowledge) can help the readers understand how the proposed method advances the SOTA performance in these tasks (or what is the performance gap between the proposed method and SOTA).\n2) Since the contribution of the paper is to propose a unified way of learning natural language representations of entities and relations, it is interesting (and has more impact) if the paper can demonstrates improvements in more than one based models (e.g., T5).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper proposes a unified way of learning natural language representations of entities and relations. Extensive experiments on three tasks and six datasets demonstrate the superiority of the proposed method, especially in low-resource settings. The paper is clearly presented and easy to read. The idea of a unified way of learning natural language representations of entities and relations is novel.",
            "summary_of_the_review": "The paper proposes a unified way of learning natural language representations of entities and relations. Extensive experiments on three tasks and six datasets demonstrate the superiority of the proposed method, especially in low-resource settings. There are no comparisons between the proposed method and SOTA results in each task. It is understandable that the main contribution of the paper is about learning natural language representations of entities and relations in a unified way. However, adding comparisons between the proposed method and SOTA in an aligned way (e.g., without using external knowledge) can help the readers understand how the proposed method advances the SOTA performance in these tasks (or what is the performance gap between the proposed method and SOTA). Besides, it is interesting (and has more impact) if the paper can demonstrates improvements in more than one based models (e.g., T5).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5928/Reviewer_WEUx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5928/Reviewer_WEUx"
        ]
    }
]