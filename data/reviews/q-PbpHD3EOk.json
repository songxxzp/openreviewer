[
    {
        "id": "BkS2DmswzD-",
        "original": null,
        "number": 1,
        "cdate": 1666661356892,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661356892,
        "tmdate": 1666661356892,
        "tddate": null,
        "forum": "q-PbpHD3EOk",
        "replyto": "q-PbpHD3EOk",
        "invitation": "ICLR.cc/2023/Conference/Paper4220/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper approaches online time series from the angle of task-free continual learning. This was achieved with a fast adaptation mechanism utilizing a moving average of the model gradient, and an associative memory module to recall the adaptation coefficients of reoccurring events. Experiments were performed on both benchmark data and synthesized datasets designed with explicit task changes, in comparison to a good number of baseline models. \n",
            "strength_and_weaknesses": "Strengths:\n\n- The proposed mechanisms of fast adaptation and recalling recurring patterns are intuitive and interesting. \n\n- Discarding the need of task boundaries is interesting and useful, although there are some technical questions regarding how the model may handle lookback window/forecasting horizons that contain \"unknown\" tasks transitions.\n\n- The experiments considered a variety of benchmark data as well as synthetic data with controlled transitions of tasks. The comparisons considered a good number of representative baselines demonstrating margins of improvement. \n\n\n\nWeakness:\n\n- A key question is regarding the size of the memory needed in the different datasets/experiments. How is the necessary size determined, and does the memory expand? In synthetic settings where ground truth of the tasks are known, it also be good to know what were learned in M, whether it corresponds to the four true processes, and whether the correct adaptation coefficients were retrieved when the old pattern reappeared. \n\n\n- How much the key hyperprameters, such as tao that triggers the member association in Eq (3), depends on the underlying datasets and what are their effects\n\n\n- From Fig 2, it was not clear if the presented method indeed achieved \"faster\" adaptation. It seemed that the error trend is more or less the same across methods as learning continues. The presented method was overall better, but when there was an increase of error, it was not clear that the presented method had a \"faster\" drop of errors -- the trend is similar (how fast the error drops); it's just that the presented method had overall lower errors. Please clarify if my understanding was not correct.\n\n- It was not clear how the model may work when the look-back window has a mix of tasks (in between transitions). The S-G setup somewhat reflects this, but even in S-A, such scenario exists (when the look-back window or forecasting horizon contains the task transition boundary), correct? How would the presented method accommodate this? By learning and memorizing many different adaptation coefficients reflecting the transition in between tasks?\n\n- In both Fig 2 and Fig 3, for the data where tasks transition are known (eg. the two synthetic sets), it'd be good to label the boundary in the figure (even though one can track it from the texts, i.e., every 1000 steps, it'd be good to make it more intuitive on the figures).\n\n-The separation of mean and standard deviation in the main text and appendix makes it quite hard to follow and assess whether the obtained margins of improvements were statistically significant. This is an important aspect of model performance, and it'd be good to try to move the std in the main text as well.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. Technical details are clear and solid. The presented adaptation and memory components, while intuitive, are novel. Public code will increase reproducibility of the work.\n",
            "summary_of_the_review": "This paper presents a continual learning approach to online time-series modeling and forecasting, with two interesting components that are intuitive and well described. There were some lingering technical questions to be clarified, but the overall paper seems to be interesting with thorough experimental evaluations",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4220/Reviewer_MVEA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4220/Reviewer_MVEA"
        ]
    },
    {
        "id": "-g9kImwLSZv",
        "original": null,
        "number": 2,
        "cdate": 1666774200179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666774200179,
        "tmdate": 1669144568494,
        "tddate": null,
        "forum": "q-PbpHD3EOk",
        "replyto": "q-PbpHD3EOk",
        "invitation": "ICLR.cc/2023/Conference/Paper4220/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this manuscript, time-series forecasting has been studied in an online scenario. For online training of deep neural predictors one has to take into account both the new coming knowledge as well as retaining the learned patterns in the past (the so called stability-plasticity dilemma). The authors reportedly are inspired from the Complementary Learning Systems (CLS) theory,  a fast-and-slow neuroscience learning framework in human. Namely, for information retention, they have used an associative memory and for fast learning  they utilized an adapter per layer, in a Temporal Convolutional Network (TCN). \n",
            "strength_and_weaknesses": "The authors studied an interesting topic which is indeed critical in many practical cases. \n\nThe adaptation process that you explained in page 4 is somewhat very abstract and confusing (considering as your main contribution) and not even mentioned what for instance \\phi_l or h_l refer to (one could find them by continue reading the paper, but could have been mentioned in their place).\nAs in the online case you want to have fast inference, I think it is necessary to include the exact inference time, for each method. Also a detailed analysis of the overhead memory for each method can be informative.\n\nIn my opinion, the novelty of this study is very limited, as the main aspect of this work has been already studied in the past  (Pham et al., 2021a; (Pham et al., 2021b, Arani et al., 2021). Basically, a continual learning scenario has been translated to an online learning scenario, where as in most cases actually there are a lot in common in these two cases.   ",
            "clarity,_quality,_novelty_and_reproducibility": "This study is clearly explained and the work is novel in the investigated application. ",
            "summary_of_the_review": "Although the studied topic is interesting, the current version needs more work to get into the shape that it could be publish in this conference. \n\nUpdate:  in my opinion, after authors modifications in this phase the manuscript is improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4220/Reviewer_r44V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4220/Reviewer_r44V"
        ]
    },
    {
        "id": "LNrZEKc24E",
        "original": null,
        "number": 3,
        "cdate": 1667508382343,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667508382343,
        "tmdate": 1667508382343,
        "tddate": null,
        "forum": "q-PbpHD3EOk",
        "replyto": "q-PbpHD3EOk",
        "invitation": "ICLR.cc/2023/Conference/Paper4220/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper proposes an efficient learning method (slow and fast) for forecasting the time series. The task formulated as a task free online continual problem. To do so, they introduce the FSNET network where each layer equipped with a memory and an adaptor to adapt with the recent chsnges. ",
            "strength_and_weaknesses": "The idea is interesting and novel in forecasting the time series \n\nExperimental shows the good performance of proposed method \n\nAuthors formulated the problem as a continual task where can be considered a novel approch.",
            "clarity,_quality,_novelty_and_reproducibility": "However the paper is almost well written, but it seems that understanding the paper is difficult for the reader. Probably by adding more explanation in the introduction and revising the essay, this problem can be solved.\n\nFor understanding the video, there are a lot of papers that try to learn from the stream data, such as videos. For example, the slow-fast method. Can we use such methods for time series? If so, why did you not compare your method against those methods \n\n",
            "summary_of_the_review": "The novelty of paper is marginal, and also the paper should be revised to be easy to read. Also, comparing with previous method is required.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4220/Reviewer_dWpT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4220/Reviewer_dWpT"
        ]
    }
]