[
    {
        "id": "2Kaxp1dMCb0",
        "original": null,
        "number": 1,
        "cdate": 1666614746935,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614746935,
        "tmdate": 1666614746935,
        "tddate": null,
        "forum": "v3y68gz-WEz",
        "replyto": "v3y68gz-WEz",
        "invitation": "ICLR.cc/2023/Conference/Paper951/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method for learning a Riemannian metric for trajectory data over $\\mathbb{R}^D$ using optimal transport. The key technical novelty is the development of a minimax objective that avoids computing geodesics during training, thereby both improving training speed and stability.",
            "strength_and_weaknesses": "I found the paper difficult to read and have low confidence on the strengths and weaknesses listed below.\n\n## Strengths\n* The key technical innovation appears to be a minimax objective (5) for Riemannian metric learning that avoids the need for computing geodesics under the current estimate of the metric. This is potentially a big deal as computing geodesics is both expensive and unstable. I did however not understand the derivation of this objective, so I may be missing something here.\n\n## Weaknesses\n* I found the paper difficult to read as I found it non-trivial how to get from one equation to another. I get that sometimes these steps are indeed non-trivial, but a bit of hand-holding would be nice. Specifically, I did not understand how to go from Eq 3 to Eq 4 and from there to Eq 5.\n* As far as I could tell, the regularizer in Eq. 7 is going to be expensive to evaluate as I suppose you have to numerically invert the metric (cubic complexity). Is this a correct reading?\n* I missed citations to some of the many papers on learning Riemannian metrics over $\\mathbb{R}^D$. Even if the applied strategies are different than the current paper, I still think previous work should be acknowledged. See for example \"Learning Riemannian Metrics\", Lebanon (UAI 2003) or \"A Locally Adaptive Normal Distribution\", Arvanitidis et al., NeurIPS 2016.\n\n## Minor comments\n* I would suggest moving the related work section to a later part of the paper as I found that it took too long before I got to the actual contribution of the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper difficult to read as I missed explanations on how to go from one equation to another. Perhaps readers with a stronger background in optimal transport will not have such issues, but I had trouble grasping the foundation of the work.\n\nThe result, however, seems really neat and quite novel.",
            "summary_of_the_review": "A potentially really neat result, but I found the paper too difficult to read for me to provide a useful verdict. I do not recommend acceptance in its current form due to the difficulties I had reading the paper, but I also claim low confidence.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper951/Reviewer_QfVV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper951/Reviewer_QfVV"
        ]
    },
    {
        "id": "PY0eJwlaMh",
        "original": null,
        "number": 2,
        "cdate": 1666714929112,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714929112,
        "tmdate": 1666714929112,
        "tddate": null,
        "forum": "v3y68gz-WEz",
        "replyto": "v3y68gz-WEz",
        "invitation": "ICLR.cc/2023/Conference/Paper951/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This is a relatively difficult paper to follow. Despite several attempts I have more questions about the paper than answers.  My review is based on my limited understanding of the ideas presented here. The goal here seems to be to analyze data (samples) observed from temporal evolution of a probability distribution. The assumption is that individual samples evolve in time along geodesic paths (with respect to certain nonstandard metric). Thus, the paper seeks to estimate this Riemannian metric from these temporal evolutions. However, one does not have the temporal registration between samples across times, i.e., one does not know which sample at time t matches with which sample at time t+1. Thus, the paper casts it as problem of comparing distributions (approximated by empirical distributions) and hence the use of Monge\u2019s transport problem.  There is some related discussion in the paper on spatial regularization of the metric tensor field. The actual optimization over the metric tensor field is performed using neural network. The paper is motivated by applications in molecular biology, esp. in analyzing scRNA data where there is past literature on using optimal transport for estimating trajectories. ",
            "strength_and_weaknesses": "Strengths: \n\nLearning Riemannian metrics from data is an important and commonly studied problem. The use of a time series data for metric learning makes this paper more interesting. \n\nWeakness:\n\nThe paper makes for a confusing read. Perhaps it is intended for an audience where the basic setup is already well known. \n\n(1)\tWhat is the justification behind assuming that the samples follow geodesic paths under some Riemannian metric? \n(2)\tWhy solve for the parallel transport problem rather than any other metric (e.g. Fisher-Rao metric) on the space of distribution space? \n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is difficult to follow. I have tried to formulate the basic optimization problem addressed here through multiple readings and I am still not sure if I understood the idea. There has to be simpler, more direct way of explaining the problem to the audience before getting into other details. ",
            "summary_of_the_review": "\nThis is a paper about learning Riemannian metric using time series data. The paper is difficult to read and understand. Given my limited understanding, it is using the Monge transport problem for solving for the unknown metric tensor field. The paper presentation and justification of the approach needs improvement. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper951/Reviewer_m2sP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper951/Reviewer_m2sP"
        ]
    },
    {
        "id": "wQ9hYSlRiaz",
        "original": null,
        "number": 3,
        "cdate": 1666965754358,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666965754358,
        "tmdate": 1666965754358,
        "tddate": null,
        "forum": "v3y68gz-WEz",
        "replyto": "v3y68gz-WEz",
        "invitation": "ICLR.cc/2023/Conference/Paper951/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to solve a dynamical optimal transport problem where: given a sequence of probability measures sampled in time, the goal is to recover an accurate trajectory of the measures for all intermediate times. Such a scenario is highlighted over two practical applications - single-cell RNA sequencing and discovering migratory paths of birds based on sparse information about their intermediate geographic locations.\n\nThe paper proposes to model these measures over a general Riemannian manifold instead of a Euclidean space where each point is now endowed with a metric tensor. The eventual goal is then to *learn* this metric from samples and demonstrate improved trajectory prediction. The authors then formulate the trajectory inference problem as an optimal transport problem over the manifold and make use of the dual formulation to find the most convenient way to learn the metric tensor field. Additionally, they also propose a regularizer that is claimed to aid the learning process. \n\nExperiments are shown for 1 synthetic and 2 actual datasets - scRNA and bird migration. The results show improvement over simple baselines like trajectory inference using the Euclidean metric or using no regularization \n",
            "strength_and_weaknesses": "Strengths \n\n- I find the problem setup and the solution formulation to be very interesting and refreshing. Changing the metric and having to learn that from sparse trajectory data is innovative!\n- The results do showcase a proof-of-concept, as most claims made about improvements with a learned metric instead of a trivial one are indeed so in the challenging problems of the experiments (especially Figures 1,2 and 3)\n\nWeaknesses\n\n- Overall the paper lacks global quality in terms of clarity of discussion. The parts corresponding to the scRNA problem are poorly introduced and could do with a better background. \n- Occasionally the discussion feels very hurried in the paper (especially in section 5.9). Also, how do Schiebinger et al. (2019) and Tong et al. (2020) compare with the result in Figures 2 and 3?\n- In addition, the choice of regularizer and the need to parameterize the metric and function \\phi using neural networks feels a bit arbitrary to me. What is the motivation here? in addition to easy use of gradient-based optimization? \n- The result in Figure 4 is not convincing. Ideally, I suppose the predicted trajectories must resemble the ground truth very closely.  but they clearly are a bit too smooth and don't have the high frequencies like the ground truth. Could there be a baseline comparison to show the proposed improvements? \n- Some minor issues - all figures could do with bigger sizes (especially Figure 2) and with much bigger font size for the titles. ",
            "clarity,_quality,_novelty_and_reproducibility": "Reasonable quality, poor clarity very good originality",
            "summary_of_the_review": "Overall, I think this paper is below the threshold for acceptance. Despite rating highly on problem formulation and some clever tricks on using dual problems for convenient estimation of the metric - (1.) There is an occasional lack of background and the discussion lacks structure (2.) More importantly, the results are not super convincing beyond a simple proof of concept. Like specifically - the mass splitting example in Figure 1, and results for bird migration in Figure 4, the lack of baselines in Figure 2, etc. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper951/Reviewer_v7rg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper951/Reviewer_v7rg"
        ]
    },
    {
        "id": "MoXBsqK__3",
        "original": null,
        "number": 4,
        "cdate": 1667445469669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667445469669,
        "tmdate": 1667445469669,
        "tddate": null,
        "forum": "v3y68gz-WEz",
        "replyto": "v3y68gz-WEz",
        "invitation": "ICLR.cc/2023/Conference/Paper951/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper deals with identifying a Riemannian metric for the cross-sectional samples of evolving probability measures on a common manifold. In particular, it makes use of  optimal transport theory to compute the Riemannian metric (that minimizes the 1-Wasserstein distance). The usefulness of the learned metric is shown in different applications. \n",
            "strength_and_weaknesses": "I believe the paper proposes an interesting use-case of learning the metric. As such it is theoretically sound. I would like to mention that it is not in my expertise to dissect and digest the results completely. The optimization problem is still abstract. It would be great to discuss it a few more details about the precise optimization steps and updates strategies. \n",
            "clarity,_quality,_novelty_and_reproducibility": "It is clearly written but it may be a bit abstract sometimes for readers not very familiar with the literature. I also believe the work is novel and reproducible.",
            "summary_of_the_review": "I wish to point out that the paper is not in my domain of expertise. I, however, believe that the work is a novel contribution to and should be interesting to the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper951/Reviewer_M27J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper951/Reviewer_M27J"
        ]
    }
]