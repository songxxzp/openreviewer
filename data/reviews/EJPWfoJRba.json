[
    {
        "id": "81S4hhQh-Kb",
        "original": null,
        "number": 1,
        "cdate": 1666536022545,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666536022545,
        "tmdate": 1666536022545,
        "tddate": null,
        "forum": "EJPWfoJRba",
        "replyto": "EJPWfoJRba",
        "invitation": "ICLR.cc/2023/Conference/Paper2362/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work aims to draw attention to the issue that there may be multiple \u201cmodalities\u201d to the behavior data collected for offline RL and it is hypothesized that it is harmful to force a single policy to learn from the data from all those different behavior policies. \n\nThe paper proposed to use V2AE (a VAE inspired method) to separate out the different modalities in the behavior data and use one policy for each of those modalities. They propose a gating policy that selects the latent modality z with the maximum Q value, which in practice is approximated with a softmax distribution inthe variational lower bound. In addition, the authors found that by adding a MI regularization term that encourages diversity of the latent modality per (s,a) pair, they got improved performance. \n\nIn terms of the experiment results, the authors found that their proposal does well on a toy problem where there are two potential goals that the agents can head to. Existing methods exhibits hesitation around forks in the path towards those two goals, whereas the V2AE method solves it perfectly. More encouragingly, such gain is still exists for AntMaze, where the problem setup can be seen as a more complicated version of the toy problem. \n",
            "strength_and_weaknesses": "\nThe paper pointed out an often ignored factor of offline RL, which is that the dataset may come from multiple behavior policies and forcing one policy to adapt to all of them at once may make learning harder. The proposed solution is mathematically grounded and the motivation is clearly shown in the toy problem. The authors also illustrated the connection between policy structure and the critic loss. All of those points above make this paper a strong candidate. \n\nThere are also some weaknesses of the paper. One is that it is unclear what exactly caused fitting the multimodal distribution hard and whether it is a fundamental problem or something that will automatically go away with more advanced network architecture with stronger regularization, without explicitly modeling the latent z. Another area that can be improved is the clarity. (see the next section)\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe individual techniques used in this paper are not novel (e.g. VAE, modeling clusters of data, MI, softmax distribution given by the Q values) but seeing them combined together in the context of offline RL is.  This aspect makes this work original. \n\nThe paper would benefit from some further clarifications. For example, some details in the equation 7 is not immediately obvious (e.g. why is the expected value in the second term over p(z) not p(z|s_i)?)  and would merit a derivation in the appendix.  Another example is eq 10. I understand how the equation follows from Barber & Agakov (2003) and for the need of an auxiliary distribution. But how is g(z|s,a) connected with q(z|s,a)? Similarly in Algorithm 1, is the actor and the posterior updated in the same gradient step? \n\nAnother clarification question I have is whether for each value of z, you have a different actor network, or do they all share the same network, and the value of z is just a part of the input to the same actor network? Afaiu, it is the latter. Let me know if I am wrong.\n\n",
            "summary_of_the_review": "I recommend a score of 6 because this paper pointed out a unique perspective to the failure modes of offline RL methods and proposed a preliminary approach that combines various other areas of ML to solve it. It is valuable due to the novel perspective and to the sound motivation as demonstrated in the toy problem. Whether there are easier solutions to the current approach is still yet to be seen, and the clarity can be improved. That is why I did not give a higher score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2362/Reviewer_chjv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2362/Reviewer_chjv"
        ]
    },
    {
        "id": "_g_6wAyUtW",
        "original": null,
        "number": 2,
        "cdate": 1666578247585,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578247585,
        "tmdate": 1666578247585,
        "tddate": null,
        "forum": "EJPWfoJRba",
        "replyto": "EJPWfoJRba",
        "invitation": "ICLR.cc/2023/Conference/Paper2362/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a more expressive policy for offline reinforcement learning. In particular, the paper proposes to use a mixture of deterministic policies.",
            "strength_and_weaknesses": "# Strength\n* More expressive policies are an interesting direction of offline reinforcement learning research.\n\n# Weaknesses\n* The approach lacks novelty. A VAE-style policy learning scheme has already been explored in [1]. While [2] proposes more expressive autoregressive policies. \n* The paper lacks comparisons to the current SOTA results. In particular, [1] and [3] achieve significantly stronger results on the antmaze dataset, which are considered to be more challenging for offline reinforcement learning [4].\n* The advantage scaling term (15) relies on the minimum and maximum over minibatches that might be sensitive to outliers.\n\n[1] Latent-Variable Advantage-Weighted Policy Optimization for Offline RL\nXi Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin Liang, Chelsea Finn, Chongjie Zhang\n\n[2] EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL\nSeyed Kamyar Seyed Ghasemipour, Dale Schuurmans, Shixiang Shane Gu\n\n[3] Offline Reinforcement Learning with Implicit Q-Learning\nIlya Kostrikov, Ashvin Nair, Sergey Levine\n\n[4] RvS: What is Essential for Offline RL via Supervised Learning?\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, Sergey Levine",
            "clarity,_quality,_novelty_and_reproducibility": "In general the paper is easy to follow. However, it lacks novelty.",
            "summary_of_the_review": "The paper lacks novelty and omits comparisons to the current SOTA. For these reasons, I believe that they paper falls below the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2362/Reviewer_NNyS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2362/Reviewer_NNyS"
        ]
    },
    {
        "id": "rejtuqcbPdQ",
        "original": null,
        "number": 3,
        "cdate": 1666628625118,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628625118,
        "tmdate": 1668610025081,
        "tddate": null,
        "forum": "EJPWfoJRba",
        "replyto": "EJPWfoJRba",
        "invitation": "ICLR.cc/2023/Conference/Paper2362/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel algorithm for offline RL that works by dividing the state-action space into discrete latent regions, learning a deterministic policy for each, and acting with a mixture of these policies. The authors demonstrate the effectiveness of their method on a series of experiments from the D4RL benchmarking datsets and argue that it helps avoid issues with out-of-distribution actions.",
            "strength_and_weaknesses": "# Strengths\nThe paper is well-written and well motivated, and the algorithm is novel (as far as I can tell).\n\nI also wanted to highlight the comparison between the original results of AWAC and the authors in Appendix E (although more to say about this point below).\n\n# Weaknesses\nThe main weaknesses of this paper are in evaluation.\n\n## Choice of $|\\mathcal{Z}|$\nThe first point is in the choice of the cardinality of $|\\mathcal{Z}|$, which is crucial to the algorithm. I feel that the importance of this choice is not highlighted enough (the first time I saw it discussed was in section 7.2 in page 7). In Algorithm 1, for instance, it is never mentioned that $|\\mathcal{Z}|$ needs to be selected.\n\nI appreciate that the effect of the dimensionality choice was evaluated to some extent in section 7.2, but I think this is an analysis that needs to be performed more extensively, as it is one of the core components of the proposed method. Some questions that were left unanswered for me:\n* How does one pick $|\\mathcal{Z}|$?\n* How does one know if it'd be better to pick cV2AE instead (where $|\\mathcal{Z}|$ doesn't have to be picked)?\n* How sensitive is the algorithm to $|\\mathcal{Z}|$ in general?\n\n## Implementation details\nIn section 7.2 the authors say \"For TD3+BC and CQL, we used the author-provided implementations.\" This makes for a comparison I do not trust. Implementation matters and there are often subtle design choices that have a big impact on performance. For proper \"apples-to-apples\" comparison, you should really be using the same codebase, otherwise the results are suspect. Furthermore, it seems you used your own implementation of TD3+BC for the toy experiments... Why did you not use the same one for the D4RL benchmark tasks?\n\nIndeed, right below table 4 the authors say: \"Remarkably, our implementation of AWAC showed significantly better performance than that reported by Nair et al. (2020).\" This is precisely why it's not good to compare across implementations.\n\n## Statistical significance\nIn the tables presented there are bolded numbers even when improvements are not statistically significant: you should only be bolding numbers when there are no overlapping CIs, which is not the case for most of e.g. Table 3 and Table 5.\n\n\n# Questions / suggestions\n1. In equation (2), it seems $\\mathcal{Z}$ has to have a pre-specified size. How is that chosen?\n1. In equation (5), to the right of the inequality I think it should be $d^{\\beta}(\\mathbf{s})$\n1. In equations (7) and (10) the $\\mathbf{z}$ is not qualified. Are you missing a sum or an integral over $\\mathcal{Z}$?\n1. At the end of equation (10) shouldn't it be $\\mathbf{s_i,a_i}$?\n1. In Algorithm 1 you should have $\\lbrace\\ldots \\rbrace$ surrounding $(s, a, s', r)$ to indicate its a set of transitions. You should also have a for loop or something like that saying \"For each element $(s, a, s', r)$ in the mini-batch...\n1. In the second expectation in equation (12), shouldn't it be $a\\sim\\pi_{z'}$?\n1. In the paragraph above equation (10) it should say \"In addition, we also propose a regularization\" (not an)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and clear. As far as I can tell, the idea is novel and (mostly) reproducible, modulo the implementation details mentioned above.",
            "summary_of_the_review": "This paper presents an interesting and novel algorithm for dealing with out-of-distribution actions. The writing, motivation, and algorithmic description are clear.\n\nMy main concern with this paper is in the evaulation of the method, sensitivity analysis of one of its core components, and comparison to baselines (see above for details).\n\nFor these reasons I am leaning towards an accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2362/Reviewer_7tms"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2362/Reviewer_7tms"
        ]
    },
    {
        "id": "BviCT76gDo",
        "original": null,
        "number": 4,
        "cdate": 1666741875356,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666741875356,
        "tmdate": 1669230169038,
        "tddate": null,
        "forum": "EJPWfoJRba",
        "replyto": "EJPWfoJRba",
        "invitation": "ICLR.cc/2023/Conference/Paper2362/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies an offline RL algorithm by utilizing the special policy structure of the behavior policy (which generates the offline data). The paper proposes to define the policy being learned by using a mixture of sub-policies, which is in the form of pi(s, a) = sum_z p(z|s) pi_sub (a | s, z), where z is the introduced latent variables. The sub policy pi_sub is deterministic and can be written as mu(s, z).  The basic idea behind this form is that the mixture policy is less likely to force the policy to cover all modes in the offline data; covering all modes using a single-mode policy is potentially harmful as it may provide support to many OOD actions, which results in overestimation. The paper then proposes to optimize the policy pi by maximizing the variational lower bound. Experiments are presented on commonly seen testing domains in offline RL literature. ",
            "strength_and_weaknesses": "Strength: \n\n1. It seems novel to utilize policy structure to improve offline RL algorithms. \n\n2. The paper presents its idea reasonably clearly. \n\nWeaknesses. \n\nI will keep my comments short. I have several critical concerns below. \n\nMotivation. The motivation for using mixture distribution is not persuasive. The paper currently says the offline data may be multimodal, and a unimodal distribution policy can result in OOD actions, as shown in Fig 1. But the mixture distribution could also assign higher density to somewhere between two modes (i.e., Fig 1(c), somewhere between two regions could also have high density). I don\u2019t see a clear benefit of using a mixture distribution.  \n\nLack of insight into the proposed approach. The proposed method has two key components: one is the use of a mixture distribution policy; another one is to use variational lower bound to learn the policy parameters. I expect strong empirical results to show the necessity and effectiveness of the two components. Particularly to show at least the following: \n\n1. The benefit of the proposed algorithm is not due to a more complicated NN. There is a reason to believe that a very large/complicated NN could be beneficial: it is less likely to touch another action\u2019s density when updating one. So probably, the OOD action\u2019s density can be kept low in the learning process. Why not use a mixture of Gaussian as a policy parameterization (with a relatively large NN) and plug it into an offline RL algorithm as a baseline? \n\n2. Why use variational lower bound?  \n\nTechnical clarification. Eq (3), why such a definition of z? Does it mean pi_gate is also deterministic? \n\nEmpirical results: \nWhy is there no implicit Q learning? IQL empirical works really well and should be included. ",
            "clarity,_quality,_novelty_and_reproducibility": "See above. ",
            "summary_of_the_review": "Due to the concerns about the motivation, insight of the proposed method, and empirical results, I tend to reject the paper for now. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2362/Reviewer_RQRe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2362/Reviewer_RQRe"
        ]
    }
]