[
    {
        "id": "lbfxrh_CI8t",
        "original": null,
        "number": 1,
        "cdate": 1666439389079,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666439389079,
        "tmdate": 1666439517667,
        "tddate": null,
        "forum": "3l9mLzLa0BA",
        "replyto": "3l9mLzLa0BA",
        "invitation": "ICLR.cc/2023/Conference/Paper2761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates a common structural characteristic between winning tickets. The authors argue that the found winning tickets indeed share a similar structure, the sign of the connections. To analyze this feature quantitatively, they devise a new metric (sign-aware structural distance) that measures the degree of the sign and structural similarity between winning tickets better (than solely comparing the network structure). Based on their analyses and experiments on different image datasets, sign-aware structural similarities between winning tickets seem to be a dataset-independent characteristic. ",
            "strength_and_weaknesses": "Pros\n\nThe main argument of the paper is that the winning tickets share similar network structures and the sign of the connections is an important characteristic. It is interesting to see the characteristics and it is good that the authors provided a simple measure that can evaluate this feature quantitatively.\n\nCons\n\nHowever, the paper ends there. It only contains observations and there are few points that one can make out of the results. How can we benefit from knowing this? Can we somehow use this fact to find the winning ticket directly? Any insights? Any applications? Anything other than just observations? \nIn addition, the authors investigate the co-occurrence of signed weights in some specific convolution filters in the winning tickets and report that they seem to have some patterns. However, this is presented in a very abstract and qualitative way, which makes it hard to count on the results. \nOverall, the main argument needs more rigorous backup experiments. For example, would the results still hold for different methods, like edge-popup? Different initialization distributions (Kaiming, signed, Gaussian, etc)? Different architectures? More results for different datasets? In my humble opinion, it is difficult to convince the reader in its current state.\n\nThe presentation can be improved. For example, it is very hard to see the differences in figure 1, which is one of the main results of the paper. Please make the colors stand out by using more vivid ones and consider using different markers or patterns than only using colors. The size of the fonts and tick values should be enlarged. (This also applies to other figures)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is okay and has every component to reproduce the work. However, the main argument needs to be supported with more experiments in various perspectives in a more aggressive way.",
            "summary_of_the_review": "Overall, the idea of the paper is interesting, but further research is needed beyond simple observation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2761/Reviewer_nFr4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2761/Reviewer_nFr4"
        ]
    },
    {
        "id": "PaW_QxjLVQ",
        "original": null,
        "number": 2,
        "cdate": 1666652529009,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652529009,
        "tmdate": 1666652529009,
        "tddate": null,
        "forum": "3l9mLzLa0BA",
        "replyto": "3l9mLzLa0BA",
        "invitation": "ICLR.cc/2023/Conference/Paper2761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a sign-based metric for comparing the structure of two sparse subnetworks, in order to understand the importance of signs in Winning Tickets (Lottery Tickets). Using two different metrics, one that is \"sign-aware\", they compare the distances between Winning Tickets and \"random tickets\", which are in this particular work defined as random masks/random weights. The authors further attempt to analyze differences between signed patterns in sparse subnetworks found for convolutional neural networks.",
            "strength_and_weaknesses": "Strengths\n* There is less analysis about the structure (i.e. mask) of winning tickets than the weights and this paper, at least at a high level, proposes better understanding the importance of structure of winning ticket subnetworks.\n* The results in Table 1 showing differences between the metric distance between WTs and random tickets within the $NNSTD^{\\pm}$ are potentially interesting.\n\nWeaknesses\n* It appears the paper compares Winning Tickets (i.e. subnetworks with masks found from pruning a dense neural network (i.e. the pruning mask), and using the (original) Lottery Ticket Hypothesis, initialized with the original pruned dense initialization, with sparse subnetworks - with *random masks* (i.e. as described in Section 4, \"random pruning\"). While they appear to still use the original pruned dense initialization, this doesn't make much sense, as we don't expect completely random connectivity to perform as well as that found via training in winning tickets! Rather (as the authors point out themselves in section 3) the main question in this space is why we can't train from *random initialization* the *same* subnet, i.e. *same pruning mask*, to as good a generalization as in a winning ticket.\n* The paper only looks at winning tickets in the context of the original lottery ticket hypothesis - i.e. using dense initialization and tiny models. We now know in practice this work doesn't extend to larger models, and instead we must use weight-rewinding in practice. Similarly it's not clear that any analysis the authors make on these tiny models/original winning tickets would extend to real-world models.\n* The proposed \"metric\" itself ($NNSTD^{\\pm}$) is not directly comparable to vanilla NNSTD (expanded upon in the next weakness), as the authors themselves point out in the footnote on page 5, these are fundamentally different metrics. However, the author's claim that the \"relative distances between the different groups in each plot\" is comparable is also not necessarily true! These relative distances are still distances in different metric spaces. This is fine when the authors compare the separation of WT v.s. random within the same metric as done with Table 1.\n* It's not clear that the proposed (different) $NNSTD^{\\pm}$ metric for convolutional NNs is comparable to the one for fully-connected neural networks given the modifications suggested.",
            "clarity,_quality,_novelty_and_reproducibility": "While this paper is, I believe, fundamentally flawed in its analysis/method, the writing itself is good. There are some relevant references (Zhou et al. in particular), but the background doesn't cite any of the papers that have improved understanding of Lottery Tickets since Zhou, and so is lacking substantially. \n\n* The statement in the Abstract that \"We propose that the signed of the connections in winning tickets play a crucial role\" is confusing at best and misleading at worst in that the authors are not the first to propose this, indeed in the body of the paper it is motivated by the previous work of Zhou et al.\n",
            "summary_of_the_review": "While the direction of understanding the importance of the mask itself in sparse neural networks is interesting, the comparison of winning tickets with masks found from training to models with random masks is not well motivated. Furthermore, its not clear that the analysis presented in this paper is significant, and there are several reasons to question the comparison of models even within the same metric, as it differs for convolutional and fully-connected models.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2761/Reviewer_ghvk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2761/Reviewer_ghvk"
        ]
    },
    {
        "id": "rmKAeoU-sYc",
        "original": null,
        "number": 3,
        "cdate": 1667133296515,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667133296515,
        "tmdate": 1667133296515,
        "tddate": null,
        "forum": "3l9mLzLa0BA",
        "replyto": "3l9mLzLa0BA",
        "invitation": "ICLR.cc/2023/Conference/Paper2761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses the problem of understanding the differences between various sparsely connected neural networks models. Consequently, it proposes an improvement over the current existing method (metric) which measures the distance between two sparse neural network models, i.e. Neural Network Sparse Topology  Distance (NNSTD), by considering also the sign of the weight values. The new proposed method is named NNSTD\u00b1 and it is evaluated mainly on convolutional neural networks.",
            "strength_and_weaknesses": "Strength:\n* The topic addressed is well understudied even if a small progress on it could help in understanding better the performance and the behavior of sparse neural networks\n* The proposed methodology has a fair level of novelty, in my opinion\n* The empirical validation shows that the proposed method, NNSTD\u00b1, can achieve a better granularity and clarity than the baseline method (e.g., better clustering of the output like in Fig 1) in illustrating the differences between sparse neural networks models.\n\nWeaknesses:\n* The proposed method, NNSTD\u00b1, could benefit in clarity by a more thorough mathematical description and qualitative discussions in Section 3. For instance, a graphical representation of the method to improve its readability, adding more mathematical details on how the method actually works, adding an algorithm, the range of the metric output, etc.\n* The empirical validation, even if it seems to be well executed, it is incomplete as it addresses just convolutional layers. A systematic study on other type of layers (e.g., fully-connected) could make stronger the results and bring more confidence in the metric. \n* The related work is very poor, and this leads to several statements in the paper which are not fully accurate. While, up to my best knowledge, there is very little work in the literature addressing this particular problem (i.e., the distance between two sparse neural networks), there is a considerable amount of work on dense-to-sparse and sparse-to-sparse training. With respect to the statements, here are some examples: \u201c\u2026such sparse networks usually resist training when their weights are initialized randomly\u2026\u201d; \u201c\u2026However, the known pruning approaches are quite laborious, often requiring more resources than simply training the original dense network\u2026\u201d; \u201c\u2026Since their discovery by Frankle & Carbin (2019), winning tickets have attracted a lot of attention\u2026\u201d. It was actually shown from 2017 [1], that a sparse neural networks trained from scratch can easily outperform its equivalent dense counterparts (where the latter has exactly the same amount of neurons) by using the sparse evolutionary training algorithm which put the basis of what is called today dynamic sparsity, prune and grow strategies, or dynamic sparse training [2]. Non-exhaustively, starting from these two missing references a proper related work discussion can be made.\n* Accordingly, the writing style can be improved to be more rigorous and more academic. All concepts have to be properly defined. E.g., the \u201cwinning ticket\u201d term needs a proper definition somewhere in the introduction or in a background section.\n\nReferences:\n\n[1] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Madeleine Gibescu, Antonio Liotta, Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science, Nature Communications 2018, https://arxiv.org/abs/1707.04780 \n\n[2] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, Alexandra Peste, Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks, JMLR 2021, https://www.jmlr.org/papers/volume22/21-0366/21-0366.pdf \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: While the paper is relatively clear and easy to follow, it would benefit by a careful proofreading and editing to follow a more rigorous and academic writing style as discussed above. \n\nQuality: The paper seems to have a fair quality level, but some aspects of it need improvement (see above).\n\nOriginality: The paper has a fair level of novelty. The related work could be extended to cover better the topic of sparse neural networks. \n\nReproducibility: In my opinion, it is possible but not very easy to reproduce the work with the current level of details. A better methodology description together with, perhaps, the release of open-source code can increase the reproducibility level.\n\n\n",
            "summary_of_the_review": "The paper addresses a timely and relevant topic with a fair level of novelty. Unfortunately, given the above discussed limitations, I believe that the paper is not ready yet for publication in its current form.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2761/Reviewer_UJuy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2761/Reviewer_UJuy"
        ]
    },
    {
        "id": "48Te-4WlR3p",
        "original": null,
        "number": 4,
        "cdate": 1667164109315,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667164109315,
        "tmdate": 1667164109315,
        "tddate": null,
        "forum": "3l9mLzLa0BA",
        "replyto": "3l9mLzLa0BA",
        "invitation": "ICLR.cc/2023/Conference/Paper2761/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigate the structrual similarity between winning tickets. In particular, they argue that the signs in weight connection play a critical role.",
            "strength_and_weaknesses": "Strength:\n\n* The topic of whether lottery tickets (LTs) is unique or share some similarity is of great interests to the communicty.\n\nWeakness:\n\n* The paper does not organized very well.\n* What is the conclusion of this paper? The sign is only thing matters or is one part of it. LTs may have lots of similarity or dissimilarity. For example, [1] aruges that the ratio of remaining neurons in each layer matter the most. How is this paper different from that one?\n\n[1] Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?",
            "clarity,_quality,_novelty_and_reproducibility": "Not very clear. Need further improvements.\n\nThe method section is less than one page.\n\nAlso, the page seems not ready for review yet.",
            "summary_of_the_review": "I have to reject this paper in the current shape. I can see the point in the results but overall, the writing need huge improvements.\n\nThe authors are welcomed to further revise this paper to be ready for review.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2761/Reviewer_MFkC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2761/Reviewer_MFkC"
        ]
    }
]