[
    {
        "id": "doXuXrMliX",
        "original": null,
        "number": 1,
        "cdate": 1666095581736,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666095581736,
        "tmdate": 1666095581736,
        "tddate": null,
        "forum": "UYcIheNY9Pf",
        "replyto": "UYcIheNY9Pf",
        "invitation": "ICLR.cc/2023/Conference/Paper4140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to group states in continuous space into clusters, according to a learned reachability metric. Such an operation allows turning the independent transition trajectories into a graph, allowing running value iteration for robust credit assignment. With shortest-path planning and an action translator, the agent can act to maximize value in a test environment while preventing OOD actions.\nThe empirical evaluations in the d4rl setting show the proposed VMG method surpasses some offline RL methods that applies conventional DQN-style bootstrapping.",
            "strength_and_weaknesses": "Strength\n* VMG provides a rather new approach to address OOD action problems in offline RL, which may have further impacts.\n* The proposed clustering method allows graph-based credit assignment to continuous control.\n\nWeakness\n* False claim of SOTA in the abstract. There are lots of offline RL methods that perform better than CQL in d4rl. For model-free methods, there are ATAC[1] and SAC-N[2], etc. And for model-based, there is a Trajectory Transformer[3] and COMBO[4].\n* Credit assignment in the transition data graph has been investigated in discrete state/action space in the online RL setting [5,6]. While being able to work in a continuous control is impressive, this work is not the first one that tried to leverage the graph-structured replay data.\n\nMinor problems and questions:\n* Why gym-locomotion control results are not presented? I agree the tasks themselves are less interesting but they are most of the offline RL methods are developed and tested on. The empirical results of this work are done on goal-reaching tasks (although some of them, like adroit, have shaped rewards), which might be helpful for the VMG to gain advantages. I would encourage authors to show gym-locomotion results, say, in the appendix if they do have the results. If the VMG indeed struggle with gym-locomotion, it will be better to state the scope of the current state of method clearly.\n* I'm not sure if it's appropriate to call VMG a world model. Although the term world model itself is very vague, it usually means a dynamics model plus a reward model. A dynamics model should be a parameterised model that can predict the future state given the current state and action. On the other hand, VMG only memorises and organises existing data and uses it for value estimation and building a policy at test time. To me, this is more like a model-free method with data set organised as a graph.\n* How's the training and inference cost of VMG?\n\nLinks:\n- [1] ATAC ([https://arxiv.org/abs/2202.02446](https://arxiv.org/abs/2202.02446))\n- [2] SAC-N ([https://arxiv.org/pdf/2110.01548.pdf](https://arxiv.org/pdf/2110.01548.pdf))\n- [3] Trajectory Transformer (https://arxiv.org/abs/2106.02039)\n- [4] COMBO [https://arxiv.org/pdf/2102.08363.pdf](https://arxiv.org/pdf/2102.08363.pdf)\n- [5] Topological Replay buffer [https://openreview.net/forum?id=OXRZeMmOI7a]\n- [6] Graph Backup [https://openreview.net/forum?id=0UQqmPGuL4n]",
            "clarity,_quality,_novelty_and_reproducibility": "* The writing is clear, and the visualization is of high quality.\n* The method is novel, particularly, the approach that organises continuous control data into a graph and the way to get a policy out of the graph where similar states are grouped into a single node.\n* For reproducibility, the code and instructions are provided. It seems like one can easily try to reproduce but I didn't try by myself.",
            "summary_of_the_review": "I think this is a good paper with significant contributions on the methodology level and the empirical performance of the method is also nice. \nWhereas it also has some problems like a false claim of SOTA and missed related work (thus overclaiming contributions). \nAlso, I have some questions mentioned in Strength And Weaknesses, which might be turned into weaknesses after the author's clarification.\n\nIn general, the current version of the paper is not perfect but I'm leaning to accept the paper because the problems should be easily fixed and other potential weaknesses won't surpass the main contributions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4140/Reviewer_b4KN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4140/Reviewer_b4KN"
        ]
    },
    {
        "id": "1aKFpKR_VDR",
        "original": null,
        "number": 2,
        "cdate": 1666269595733,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666269595733,
        "tmdate": 1666269595733,
        "tddate": null,
        "forum": "UYcIheNY9Pf",
        "replyto": "UYcIheNY9Pf",
        "invitation": "ICLR.cc/2023/Conference/Paper4140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces Value Memory Graphs (VMG), a learned latent MDP structured as a graph. Since the VMG is relatively small, we can apply value iteration in it. The obtained value function is then used in a planner (Dijkstra\u2019s), and the resulting plan in abstract space is converted back to the original space using a learned action decoder. Experiments in several tasks show improved performance over baselines, and investigate the importance of each component through ablation studies. ",
            "strength_and_weaknesses": "Strength:\n* The paper studies a relevant topic (structured abstraction & offline RL). \n* The paper is very well written and clear. It does not make things unnecessarily complex, while all necessary details are provided. \n* Performance of the methods clearly outperforms the baselines in the tested tasks. \n* Good ablation experiments. \n* Related work is well covered. \n* Good visualizations, such as Figure 4, which really show what the method does. \n\nWeaknesses: \n* Sec 3.2: Since each vertex is essentially a D-dimensional ball with radius gamma. These balls can not cover the full space without overlap. What happens if a certain point can be assigned to multiple vertices?\n* Sec 3.3: I understand why you construct the combined reward function from internal and transition rewards (with weight \u00bd on each internal half), but there is a strong assumption here, being that there are as many true steps internal of a vertex as there are between vertices. This will probably not be true in practice: especially with larger values of gamma, the number of steps within a vertex will be much bigger than the number of steps between them. I guess one could argue that you need extra weight on the internal components in the reward function.  \n* Sec. 3.4: I was highly surprised you used Dijkstra\u2019s, after first having solved the problem with VI. I had written a comment about this, only to find out why you make this choice in the \u201cMulti-step Search\u201d section of the ablation (p 9). Try to explain why you make certain choices directly when you introduce them. \n* You only test on three tasks of the D4RL. This is fine, but some variation in environment type would have made the results a bit stronger. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. \nQuality: Good. \nNovelty: Good. \nReproducibility: The authors promise to release code. Otherwise, the methodology is clearly explained and should be reproducible.  ",
            "summary_of_the_review": "I recommend this paper for acceptance. It is well-written, covers a relevant topic, has good results, and extensive insight/ablation studies. There are some things to improve, as listed above, but overall I think this would be a valuable addition to the ICLR conference programme. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4140/Reviewer_UqHS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4140/Reviewer_UqHS"
        ]
    },
    {
        "id": "3j24NEI6FzG",
        "original": null,
        "number": 3,
        "cdate": 1666614056426,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614056426,
        "tmdate": 1666614056426,
        "tddate": null,
        "forum": "UYcIheNY9Pf",
        "replyto": "UYcIheNY9Pf",
        "invitation": "ICLR.cc/2023/Conference/Paper4140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The main focus of the paper is to tackle some critical challenges in continuous control RL tasks including sparse rewards and long-horizon planning. The idea is to build a simple and discrete world model leveraging an offline dataset (and the paper focuses on the offline RL setting) to learn an abstraction of the original environment. The authors propose Value Memory Graph that represents the MDP as a directed graph with \"abstract\" state and actions. Then, value iteration can be applied given the small state and action spaces. The authors also learn an action translator to convert abstract actions to real actions later. The authors conduct experiments on some of the tasks from D4RL to evaluate its effectiveness.",
            "strength_and_weaknesses": "### Strengths\n- Originality: The authors propose an interesting method to tackle sparse rewards and long-horizon planning, which is different from existing methods (such as improving exploration in sparse reward tasks). Specifically, the paper follows the \"abstraction\" perspective, which builds an abstract graph of the underlying MDP, which leads to a small state and action spaces that allow for tractable value iteration. To achieve this goal, the authors propose a contrastive learning method for state merging, and also propose an action translator to further translate the abstract action to a real action.\n\n- Clarity: The paper is clearly written and easy to follow. I think the paper motivates the problem well, and describes the procedure clearly including how to build a metric space, how to construct the graph in value memory graph, and how to use such value memory graph.\n\n### Weaknesses\n- Quality and significance: My main concern for the paper is the applicability of the method and the significance of the experiments. The paper can be improved by studying the method in the online RL setting (I understand that it is more difficult and challenging to apply the method in online RL settings, but I think that could greatly improve the paper.) In addition, I wonder how the method compare against baselines in commonly-used MuJoCo tasks from D4RL like hopper, walker2d, and halfcheetah. These are tasks with dense rewards, but they also have continuous state and action spaces, and it is worth evaluating VMG in these tasks (since this is also one of the challenges highlighted in the paper). I also have a question in Eq. (2). Why does it need to consider all the other next states? What if sj' and si' are very similar? I will be happy to increase the score if additional experiments on MuJoCo are included and analyzed.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. I think the authors have done a good job in motivating the problem and introducing the method. The paper is also novel. I think the main limitation is that it is designed for the offline RL setting for now, and is not ready to tackle online RL problems. The authors also try to understand the value memory graph via a reasonable visualization and analysis.",
            "summary_of_the_review": "The authors propose a novel method to tackle some of the important challenges in DRL. I think the experimental section can be improved by evaluating VMG on commonly-used MuJoCo tasks from D4RL. It can also be improved to extend VMG to online RL problems.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4140/Reviewer_Wbjx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4140/Reviewer_Wbjx"
        ]
    },
    {
        "id": "3xkHAqWsgM1",
        "original": null,
        "number": 4,
        "cdate": 1666675587694,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675587694,
        "tmdate": 1669223973347,
        "tddate": null,
        "forum": "UYcIheNY9Pf",
        "replyto": "UYcIheNY9Pf",
        "invitation": "ICLR.cc/2023/Conference/Paper4140/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes learning a graph structured abstract world model for offline reinforcement learning. Here the learned abstract world model is built on top of a learned representation and used in conjunction with Value Iteration solver, Dijkstra\u2019s search, and temporal inverse dynamics model. In general learned abstract models are powerful when combined with optimal planning and look-ahead policies. Here the authors use a clustering based approach to build the world model and show empirical evaluation of such an approach on a subset of D4RL tasks. Here the empirical evaluation is outperforms the current SOTA methods for the selective domains shown in the paper.",
            "strength_and_weaknesses": "While the empirical results in isolation is very convincing, the paper lacks rigor in different design choices of the algorithm. The paper will definitely provide more value to the reader with better explanation on the design choices along with more discussion on the implementation details as outlined below. \u00a0would be a good contribution \u00a0VI and leveraging the graph for planning in continuous domain.\n\nStrengths:\n\n- The paper definitely boasts some empirical novelty as optimal planning on abstract graphs have not been explored well for d4rl tasks. The approach discussed performs well for the \u00a0shown subset of these d4rl tasks.\n    \n- The ablation studies presented does hint towards the contribution of different loss functions used towards representation learning / metric learning which is employed by the graph building algorithm.\n    \n- The notion of building a abstract graph for exact planning and combining it with lookahead policy (employing goal conditioned inverse dynamics) is powerful and broad in its scope\n    \n\nWeakness:\n\n- The design choices made throughout the approach lacks some rigorous justification as no theoretical evidence has been provided for these choices.\n    \n    - [C1] \u00a0(eq 1)Why was m set to 1 ? this value will definitely have bunch implications depending on the dimensionality of the latent space and the complexity of the environment dynamics.\n        \n    - [C2] (eq 2) why would we take max of the l2 norm and M. could we not have replaced this with max(l2(norm)-m, 0). In the current formulation this component of the loss will never go to zero if I understand it correctly.\n        \n    - [C3] (eq 4) We are basically ignoring any \u201cself loops\u201d that arise with this kind of clustering and only considering dynamics that take us from one state to another. Why is this? How often do these self loops occur. Moreover, the reward design from vj1 to vj2 seems arbitrary and not backed by any theory. Here we are basically assuming uniform weighting over all states in a cluster. I understand that adding half of the rewards from both self loops in source and target state, allows the trajectory reward to sum up to the true trajectory reward, but the reward definition here does seem arbitrary, which is hard to justify with empirical evaluations alone.\n        \n    - [C4] I did not quite get the notion of using Dijkstra\u2019s search to find the best path across the world model, using one more weighting mechanism that is not theoretically justified. Would the Value iteration already not provide a relatively optimal path from any state to the best state reachable ?\n        \n    - [C5] The choice of k = 10 (for inverse dynamics) seems arbitrary as we are only using 1 or 2 step lookahead goals for the models. It seems that we are baking more into the prior as 1 or 2 step lookahead in the abstract space can technically be much more than 1 or 2 as we are aggregating states to a single cluster. i.e. a transition in abstract space may amount to more than one step in the real world, how much of this is actually happening in empirical evaluations is unclear.\n        \n    - [C6] The choices of penalty beta (from table 6 in appendix) seems low and arbitrary. generally we set this to 0.99. It does make sense that we would like to have smaller discount factors in abstract models as well as constrain it further to reduce cumulative errors, however the choices made here even without a hyper-parameter search is not rigorous by any means.\n        \n- Lacks some key implementation details.\n    \n    - What was the dimension of the latent space used for the latent dynamics model.\n        \n    - No training curves has been presented.\n        \n    - When we mention \u201c\u201cbest one from the checkpoints saved from the 500th to the 800th epochs.\u201d ([pdf](zotero://open-pdf/library/items/NHHMIX9V?page=15)) does it mean best in terms of the training loss / validation losses?\n        \n    - The values used for hyper-parameter search are not listed down, only the best performing ones. Moreover the number of runs used for hyper-parameter search must be mentioned in the main paper for clarity, as it is one of the crucial yet generally overseen.\n        \n    - Again the choices made for these hyper-parameter sweeps does not seem to be well justified.\n        \n- The paper is missing some key connections in the related work section.\n    \n    - Bisimulation metrics has been well explored for learning abstract models for a long time now. [4, 5, 6]. They provide a rigorous frame \u00a0and other abstraction literature is not cited well.\n        \n    - Secondly optimal planning on graph structured world models has also been explored in works like [1,2,3]. [1] Here is especially relevant as the world model is constructed using metric learned in abstract space. Careful comparison / discussion of these related works would greatly bolster the context of the paper.\n        \n\n[1] Shrestha, A., Lee, S., Tadepalli, P., & Fern, A. (2021). DeepAveragers: Offline Reinforcement Learning by Solving Derived Non-Parametric MDPs.\u00a0*ArXiv, abs/2010.08891*.\n\n[2] Char, I., Mehta, V., Villaflor, A., Dolan, J.M., & Schneider, J.G. (2022). BATS: Best Action Trajectory Stitching.\u00a0*ArXiv, abs/2204.12026*.\n\n[3] Marklund, H., Nair, S., & Finn, C. (2020). Exact (Then Approximate) Dynamic Programming for Deep Reinforcement Learning.\n\n[4] Ferns, N., & Precup, D. (2014). Bisimulation Metrics are Optimal Value Functions.\u00a0*UAI*.\n\n[5] Ferns, N., Panangaden, P., & Precup, D. (2011). Bisimulation Metrics for Continuous Markov Decision Processes.\u00a0*SIAM J. Comput., 40*, 1662-1714.\n\n[6] Bertsekas, D.P. (2009). Neuro-Dynamic Programming.\u00a0*Encyclopedia of Optimization*.",
            "clarity,_quality,_novelty_and_reproducibility": "-",
            "summary_of_the_review": "In the current state of the paper, I will favor rejection. however I look forward to the authors rebuttal on my questions / concerns and am open to updating the score afterwards. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4140/Reviewer_ZzyC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4140/Reviewer_ZzyC"
        ]
    }
]