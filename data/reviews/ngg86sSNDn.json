[
    {
        "id": "udzbb0J2eL",
        "original": null,
        "number": 1,
        "cdate": 1666539833908,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666539833908,
        "tmdate": 1666539833908,
        "tddate": null,
        "forum": "ngg86sSNDn",
        "replyto": "ngg86sSNDn",
        "invitation": "ICLR.cc/2023/Conference/Paper1795/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to learn explanations for the predictions from a neural network by employing a multilevel explanation approach. More specifically, the authors propose to leverage attributes for specific images and map the coarse class labels to the fine-grained object attributes during training. They show that this leads to human-like explanations while achieving nearly similar prediction accuracy. The perform experiments on benchmark datasets and show improved explanation ability of the model compared to state of the art.",
            "strength_and_weaknesses": "The paper looks at the important problem of explanation in deep neural networks which are of inevitable importance. Their motivation of defining what is learned by a neural network by breaking down the learning process from coarse categories to fine grained attributes is interesting, intuitive and well supported experimentally.\n\nDespite the interesting approach, there is a limitation as the results and the method are only applied for easy datasets / classes and will be very hard to scale for imagenet classes for instance. What will be the approach to scale for instance the attributes and how would the method perform for thousands of dog breeds in imagenet for example ? \n\nWhat is the loss function that is used to match the feature and attributes obtained from zero shot literature ? \n\nIt is not very clear what the difference is between the single level (Explainable MLP) versus the multilevel Explainable CNN. How\u2019s explaining using attributes (whether with or without saliency maps) a multilevel approach ? \n\nIs there any way to measure the attribute prediction accuracy ? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, original and can be reproduced by the details mentioned in the paper.\n",
            "summary_of_the_review": "The paper is well motivated but the contributions for explainability are marginal and does not bring any significant improvements to the already existing research or outcomes in the field of explainability. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1795/Reviewer_4txy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1795/Reviewer_4txy"
        ]
    },
    {
        "id": "s6i1nK7Tgv",
        "original": null,
        "number": 2,
        "cdate": 1666664507006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664507006,
        "tmdate": 1666664507006,
        "tddate": null,
        "forum": "ngg86sSNDn",
        "replyto": "ngg86sSNDn",
        "invitation": "ICLR.cc/2023/Conference/Paper1795/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new explainable neural network architecture called Multilevel XAI architecture. The key contribution of the work is to use both visual and language-guided interpretability, where a list of attributes pertaining to a class prediction (e.g., Dog --is furry, ...). Experiments show the model's ability to ground salient attributes in images in addition to providing region-based saliency maps. ",
            "strength_and_weaknesses": "**Strengths:**\n\n*[S1:]* Interesting results: The results obtained are certainly interesting; The per-attribute visualization can indeed shed greater light on the decisions of the neural networks compared to \"class-level\" heat maps. \n\n\n**Weaknesses:**\n\n*[W1]:* Inadequate baselines, comparisons, and discussion: The proposed visualizations are interesting, but a huge swath of work in vision and language that already demonstrate very similar ability, or can be trivially extended to produce similar visualization and/or linguistic explainability (E.g., concept bottleneck models https://proceedings.mlr.press/v119/koh20a.html). This is woefully missing in the paper and it is very hard to contextualize why the proposed method is more explainable than any number of other works. I will list a few ways this could be done, but the main issue is that it already feels trivial to do what the paper does with existing tools. What new benefits does the proposed work bring?\n\nW1.1. Visualization of existing V&L models: Almost all of the recently introduced VL pretraining methods can ground image regions for a given word in a caption. (E>g., https://github.com/salesforce/ALBEF). Some methods even work for new objects as long as they are adequately described with attributes (A llama, which is a white tall animal in the picture (e.g., https://github.com/microsoft/GLIP)). Going back, even more, most of the object-based representations are also trained to predict object attributes (https://github.com/peteanderson80/bottom-up-attention). The paper makes a distinction between \"class-based attributes\" and \"image-based attributes\" but I see no distinction when the attributes are describing various objects; For a \"dog\" isn't the \"class-based attributes\" simply a union of all attributes contained for \"dog\"? (E.g., https://vawdataset.com/). Almost all methods that work with objects and attributes can visualize their affinity in various image regions to some degree. How does this compare with the proposed method?\n\nW1.2 Trivial modification of existing work (E.g., Grad-CAM) to use additional information used by the paper: The paper makes use of additional attributes to obtain the given visualizations; However, if any network was modified to predict these attributes in addition to the class label, Grad-CAM visualization for making the prediction \"has beak\", could be used in exactly the same way the paper is using.\n\nWithout a comparison, discussion, or adequate comparative analysis with these, the impacts of the paper are hard to justify.\n\n*[W2]:* The effect of components used is not explained: Firstly, the method requires additional annotations. While the paper does mention that may be easy to obtain (e.g., via large language models such as GPT-3), there are no experiments showing whether a lower-quality (noisy) attribute annotations would still work. Similarly, the training of MLP_L is not described well. How sensitive is it? What happens when only a small portion of the attributes are present? What happens when attributes (y_hat) is not predictive of class y? Since having high-quality attribute data seem to be the central requirement \n\n\n*[W3]:* Lack of Clarity: The paper feels obfuscated and very hard to follow. However, upon closer inspection, the core concepts are fairly simple. I would urge the authors to streamline the presentation and explain in terms of their function rather than hiding details unnecessarily under hard-to-follow conventions. For example, in Section 2.2 paragraph 2, the first sentence is literally just describing the process to generate predictions from a neural network. It is entirely unnecessary to write that as:\n\n> 8Xj 2 X used for test, let yXj = \u2020(\u02dc\u21e1( (Xj ))), i.e., the predicted class label of the test image Xj. (Not rendered properly here -- please see pdf.)\n\nThe second part of the sentence beginning with i.e. describes the same thing as the stuff before that. This is simply an example in what is an unnecessarily dense symbol used to describe fairly simple underlying processes in Section 2 (Methodology).  \n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** To me, the clarity is poor and feels deliberately obfuscated, especially in the methods section.\n\n**Quality:** See strengths and weaknesses above.\n\n**Novelty:** There is a limited novelty in both the techniques used and the empirical evidence (and discussion).\n\n**Reproducibility:** The method is built on existing models and a code is also released in an anonymized link, I think reproducibility is excellent. ",
            "summary_of_the_review": "I am leaning toward recommending the rejection of this paper. While there certainly are interesting tidbits, the biggest gripe is that the proposed model requires both class label and attribute annotation per class; If we allow other methods to have access to the same (or similar) type of data, they could (and do) produce similar \"interpretable\" visualizations as shown in the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1795/Reviewer_iU9V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1795/Reviewer_iU9V"
        ]
    },
    {
        "id": "KXwmbFhowk",
        "original": null,
        "number": 3,
        "cdate": 1666848369295,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666848369295,
        "tmdate": 1666848369295,
        "tddate": null,
        "forum": "ngg86sSNDn",
        "replyto": "ngg86sSNDn",
        "invitation": "ICLR.cc/2023/Conference/Paper1795/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a visual and linguistic bounded explanation method to make part of NN models explainable by adding both attribute-wise and language-wise explanations. Specifically, they add a trainable part between the feature extractor (e.g., pre-trained Resnet) and label embedding, and the added part can be explained. The paper is easy to follow and clear.",
            "strength_and_weaknesses": "**Strength**\n\n1 Using multilevel (visual and linguistic) to conduct more intuitive explanations is an interesting and promising idea.\n\n2 The author shows an easy-to-follow method and results.\n\n**Weaknesses** \n\n1 Generalization. The proposed method could only be used to explain a specific kind of NN, which learn a mapping starting from an extracted feature to an attribute-related label embedding. It is hard to explain general pre-trained neural networks (e.g., ResNet, GNN, Transformers). \n\n2 Partial explainable. The whole model has three parts, feature extractor, added learnable mapping, and attribute to label mapping; only the middle part can be explained, and the best part still performs as a black box.\n\n3 Fidelity issue. The method treats not the true label Y but an attribute set as a target during training. How to guarantee the GPT-3 described attributes are useful features for prediction? If you directly train a model using true label Y, they use different features during the decision. So the explained attributes do friendly to human understanding but may not follow the original model's logic.\n\n4 Need to improve accuracy results. The final accuracy is lower and has a relatively large gap compared with the original model, even though the proposed method uses more learnable parameters. These results also aligned with point 3 above. The newly learned explainable mapping does not use the same logic as the original model. \n\n5 Accumulated bias. There may be some bias in the feature extractor model (Resnet) and GPT-3. The new trained mapping can not remove those biases if they are input and target.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is interesting. Writing is easy to follow. Reproducibility is good due to the provided code.",
            "summary_of_the_review": "Overall, using a multilevel explanation is interesting, while the reviewer thinks the weakness outweighs the strength. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1795/Reviewer_Snsk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1795/Reviewer_Snsk"
        ]
    }
]