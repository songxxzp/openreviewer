[
    {
        "id": "6A6O_fUTQup",
        "original": null,
        "number": 1,
        "cdate": 1666627076174,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627076174,
        "tmdate": 1666627076174,
        "tddate": null,
        "forum": "F0UQv_MNWCt",
        "replyto": "F0UQv_MNWCt",
        "invitation": "ICLR.cc/2023/Conference/Paper5631/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors try to locates a very small subset of pretraining data that directly supports the model\u2019s predictions in a given task. The main idea is to use cosine similarity between gradients of the downstream samples and pre-training corpus.  They designed experiments based on 0.5% of the Wikipedia and BookCorpus over IMDB and MNLI.",
            "strength_and_weaknesses": "Strength:\n1. The authors provide a novel metric to locate pretraining data that supports the model\u2019s predictions in a given task.\n2. Table 1 shows that using retrieved corpora can effectively improve the effectiveness of LMs in downstream tasks.\n\nWeaknesses:\n1. the problem is not well defined. A corpus that improves the effectiveness of a downstream task cannot necessarily be used to explain the predictions of the original language model. While many of the claims in this paper are about the interpretability of pretraining by locating training corpora, the experiments are mostly about performance improvement.\n2. If the goal is to improve model effects by some continued pretraining, the related work and experiments in this paper lack comparison with state-of-the-art methods, e.g. dense retriever methods like REALM.\n3. The method has some assumptions. However, they are not validated. For example, whether continuing training on the task data directly will improve the original model? Why the gradient of the sample to be found is similar to the gradient of the target task samples?\n4. I think the method in this paper cannot be scaled to large LMs. In LMs, there are at least millions of gradients for each sample.\n5. Some experiments are confusing. For example, I cannot understand why words like good/bad/yes/no happen to be masked out in Table 3. This is not consistent with the random mask strategy.\n6. Table 3 is inconsistent with the intuition of this paper. According to the examples in Table 3, it looks like the model is looking for verbalizer words, instead of the training corpora that explain the model behavior in the downstream task.\n7. I suggest that the authors use more downstream tasks for their experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to the Strength And Weaknesses.",
            "summary_of_the_review": "Overall, I think the problem is not clearly defined. This causes the inconsistencie in the method and experimental descriptions. In addition, there is an issue with the choice of baselines.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5631/Reviewer_st4M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5631/Reviewer_st4M"
        ]
    },
    {
        "id": "e-39YsP1PG",
        "original": null,
        "number": 2,
        "cdate": 1666631738011,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631738011,
        "tmdate": 1666631738011,
        "tddate": null,
        "forum": "F0UQv_MNWCt",
        "replyto": "F0UQv_MNWCt",
        "invitation": "ICLR.cc/2023/Conference/Paper5631/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for finding the subset of a large language model (LLM) training corpus that is most responsible for the 0-shot performance of the LLM on a given task. The method involves finding examples whose gradient is similar to the gradient of task-specific supervised data. This is done in an iterative process in which the original LLM is fine-tuned on the examples found during the previous step, in order to account for interactions among examples. Performance is evaluated by fine-tuning the LLM on the identified subset, then measuring task performance. \n\nOn IMDB and MNLI tasks with BERT, the proposed method is found to outperform random-sampling and kNN baselines. The most striking finding in the resulting analysis is that most of the valuable examples come from the Books portion of BERT\u2019s training corpus.\n",
            "strength_and_weaknesses": "Strengths:\n\n1. The problem is important, as insights can inform selection of training data for future LLMs. The proposed technique thus has the potential to be useful in many different contexts.\n\n2. The idea is novel. Although gradient-based similarity is widely used, this specific application is new, as far as I know.\n\n3. The paper is very well written: the technique is described clearly and precisely, and the experiments should be easy to reproduce.\n\nWeaknesses:\n\n1. Although the technique is intuitive and easy to understand, it\u2019s not clear to what extent it\u2019s a good approximation of the true quantity of interest, namely performance if the LLM were retrained from scratch on the whole corpus with the identified subset removed. Influence functions obviously get at that more directly, and I think the paper should have included an influence-function baseline. This needn\u2019t be an iterative procedure (which might be difficult to formulate), just the set of individual examples with the greatest influence.\n\n2. The experiments are somewhat limited: only on BERT, and only on two tasks. They are also limited in exploring only tiny subsets. It\u2019s interesting to know that fine-tuning on such a set can provide a good performance boost (at least in the case IMDB), but it would be better to have a sense of where this flattens out as the subset is expanded.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See strengths and weaknesses.",
            "summary_of_the_review": "The paper is a bit thin on substance, both in terms of theoretical justification and breadth of experimental results. However, it attacks a very central problem, is clearly written and proposes a simple, potentially very useful technique, so I am leaning in favour of acceptance.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5631/Reviewer_uxbN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5631/Reviewer_uxbN"
        ]
    },
    {
        "id": "CrzOAdtHaE",
        "original": null,
        "number": 3,
        "cdate": 1666747765398,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666747765398,
        "tmdate": 1666747765398,
        "tddate": null,
        "forum": "F0UQv_MNWCt",
        "replyto": "F0UQv_MNWCt",
        "invitation": "ICLR.cc/2023/Conference/Paper5631/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors want to explore a very interesting question why the prompting learning of large pretrained language models leads to strong performance in a variety of downstream tasks, especially in zero-shot setups? This paper proposed a novel method to identify the evidence of the model\u2019s task-specific competence in prompt-based learning based on the general large pre-trained language model. It uses the gradient information related to the downstream task, IORCA can locate a small subset of pretraining data that is similar to the downstream tasks. This work is an very interesting exploration but not very novelty.",
            "strength_and_weaknesses": "Strengths:\nThis paper proposed a gradient-based method to identify the evidence of the model\u2019s task-specific competence in prompt-based learning based on the general large pre-trained language model. It is interesting.\n\n\nWeaknesses:\nNovelty of the proposed method is somewhat limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The Clarity and Quality is Good!",
            "summary_of_the_review": "Same with Summary Part.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5631/Reviewer_sFZJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5631/Reviewer_sFZJ"
        ]
    },
    {
        "id": "OIFGULYRyy",
        "original": null,
        "number": 4,
        "cdate": 1666784524661,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666784524661,
        "tmdate": 1666784524661,
        "tddate": null,
        "forum": "F0UQv_MNWCt",
        "replyto": "F0UQv_MNWCt",
        "invitation": "ICLR.cc/2023/Conference/Paper5631/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new approach to find a subset of the pretraining corpus that supports BERT\u2019s zero-shot predictions in a given task. This is done by iteratively finding pretraining examples whose gradient is the most similar to that of the downstream task examples. The technique is used to analyze the performance of BERT on IMDB and MNLI, finding that (i) BookCorpus is more relevant than Wikipedia despite the latter being larger, (ii) pretraining examples masking the downstream verbalizers are the most relevant, and (iii) the most relevant pretraining examples are not particularly similar to the downstream task examples on the surface, showing that the model is not relying on pure memorization.",
            "strength_and_weaknesses": "STRENGTHS\n- The paper tackles an important problem (understanding how pretrained models are able to perform downstream tasks in a zero-shot fashion), and the approach followed is original and technically sound.\n- The finding that the context of the supporting data evidence is not particularly similar to the task input data is interesting and not obvious.\n\nWEAKNESSES\n- While I really like the general idea of the paper, the analysis itself is rather shallow. The main findings are (i) BookCorpus is more relevant than Wikipedia despite the latter being larger, (ii) pretraining examples masking the downstream verbalizers are the most relevant, and (iii) the most relevant pretraining examples are not particularly similar to the downstream task examples. While (iii) is interesting, (ii) is not surprising at all, and more analysis is necessary to interpret (i). For instance, it could be that the verbalizers (e.g. good and bad) occur more frequently in BookCorpus, in which case (i) would be implied by (ii) and not surprising at all.\n- I am not satisfied by the evaluation of the proposed approach in Section 4. ORCA outperforms all baselines in Table 1, but that is not surprising at all, as it is the only approach using labeled downstream data, and it works by picking examples from the pretraining corpus whose gradient is similar, therefore approximating gradient descent in downstream data. Some basic baselines are missing (e.g. only picking examples that mask the downstream verbalizers), and it would be useful to have a variant finetuning on the downstream task as an upper-bound to put the numbers in perspective. It would also be useful to finetune another pretrained model (e.g., RoBERTa) in the same set of examples (selected based on BERT): if these examples are truly supporting the task in question, I would expect them to be helpful for all models. In addition, the results in Table 2 are negative: even if the authors claim that they get an improvement on IMDB, this is likely not significant, as this applies to only one variant of the proposed method and even in that case the improvement is only 0.27 points while the standard deviation is 0.65. This is puzzling and deserves some more discussion: does this imply that all pretraining data is equally useful when using prompt-tuning?\n- The manual analysis of the examples is dissapointing. We are only shown a single example for each task and verbalizer. As the authors say, it is not clear at all how the examples mined for MNLI can be useful to learn entailment. The authors say that the examples mined for IMDB do express sentiment, but I think that even that is questionnable (the second one does, but the first one is talking about the genetics of bacteria). I would have liked to see more examples in the appendix and a more systematic analysis of them, with the aim to understand how interpretable the examples are.\n- The evaluation is limited to two tasks and a single model, using a very small fraction of the pretraining corpus. While I understand that the authors were constrained by compute, this cannot be use as a cheap excuse for everything. I would have liked to see more extensive experiments and, if truly not feasible due to compute constraints, the authors should at least report the compute required for each experiment to justify this (e.g. if each experiment takes a month on their hardware I would understand not having more, but if it takes a few hours this wouldn\u2019t be a valid excuse).",
            "clarity,_quality,_novelty_and_reproducibility": "I think that the paper has enough novelty. The writing is ok although not particularly brilliant, and the mathematical formalization is understandable but could be made more clear. The paper does a good job in terms of reproducibility: I did not miss any critical experimental detail, and the authors promise that the code and data will be released.",
            "summary_of_the_review": "I think that this is a borderline paper, perhaps leaning a bit more on the negative said. It tackles an important problem from an original angle and is technically sound, but the analysis presented is rather shallow.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5631/Reviewer_NC89"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5631/Reviewer_NC89"
        ]
    }
]