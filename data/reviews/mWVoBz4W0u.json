[
    {
        "id": "BzlPJCsIpC",
        "original": null,
        "number": 1,
        "cdate": 1665890328802,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665890328802,
        "tmdate": 1669078251666,
        "tddate": null,
        "forum": "mWVoBz4W0u",
        "replyto": "mWVoBz4W0u",
        "invitation": "ICLR.cc/2023/Conference/Paper3201/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This is a technical paper exploring a couple of new directions toward a state-of-the-art *enormous*-scale multilingual vision-and-language model. There are two novel and interesting observations: 1) joint scaling of the vision and language components and 2) taking advantage of pre-training with large multilingual datasets for multilingual downstream tasks.",
            "strength_and_weaknesses": "### Strength\n\nThis paper consistently shows state-of-the-art performance across multilingual image captioning, visual question answering, and zero-shot image classification tasks. Especially, the VQA performance in an open-vocabulary generation setting is inspiring. \n\n\n### Weakness\n\n**W1. Weak argument on the joint scaling of the vision and language components.** The third paragraph in Introduction argues that the visual component gives a better return on investment (RoI). Figure 2 seems to be the evidence for this claim; however, we cannot remove the possibility of other explanations since there is only one condition for each variable, the language side is from 1B to 13B (too wide interval) compared with that the visual side is from 2B to 4B. The return on investment can be non-linear. One suggestion is that we need the report of \"PaLI-5B (L, ViT-e)\" to see the RoI where the visual component cannot exploit the large language component. *Emphasize that, according to this result, one of two major claims can be rejected. The rating might be lowered if there is no reasonable explanation or experimental support.*\n\n**W2. Weak support on multilingual pre-training.** Since the proposed models are pre-trained with multilingual datasets, we expect them reasonably works on multilingual downstream tasks. Table 2 and Table 4 Right (MaXM) only show the multilingual downstream performances, while Table 4 Left shows the cross-lingual for English-answers only (xGQA). However, this xGQA experimental setup does not firmly support the need for multilingual pre-training in cross-lingual tasks since the model architecture is not controlled (MPT may underperform because of model architecture or pre-trained datasets; we cannot discern by this experiment). Here, one suggestion is that, if PaLI-17B is pre-trained with the subset of the English-only WebLI, this model will significantly underperform the (original) multilingual PaLI-17B for the cross-lingual xGQA benchmark (even exploiting translations for multilingual tasks)?\n\n**W3. Exclusiveness in the study.** The authors do not intend to release the dataset and pre-trained models considering the risk of exposure to \"unknown biases or stereotypes, or propagate inaccurate or otherwise distorted information.\" In this vein, how can the community reproduce the results? The current *Reproducibility Statements* do not discuss this issue and the impact on research communities studying this topic, seriously enough.\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\n**The visual component, ViT-e.** It is unclear how it seamlessly changes the input resolution from 224x224 to others. Which part of representations of ViT-x is used for the input tokens to the Transformer encoder? Section 3.1 or Appendix A.1 should include this key detail for readers unfamiliar with this experimental setup.\n\n**The name of PaLI.** This paper does not mention why they call it PaLI, which the readers only can assume -- It is for *Language and Image*, while reminiscent of the PaLM in the second paragraph of the Introduction.\n\n### Novelty \n\nThe authors provide two major arguments on the joint scaling of the vision and language components and taking advantage of pre-training with large multilingual datasets as novel technical observations. However, for the reasons of W1 and W2 in the weakness section, their arguments are grounded on weak evidence to support their claims. Overall, the authors could be more carefully discussing the contributions of their model architecture and newly collected datasets by controlling other factors. The authors should resolve these issues in the rebuttal period for the recommendation of acceptance.\n\n**N1. Resemblance to the Unified-IO.** The proposed VQA-like generalized task and the modular architecture with Transformer encoder/decoder are substantially similar to Unified-IO [1], but the authors fail to relate to, and properly compare with that. This work was initially published on June 17, 2022 (3-month before the ICLR deadline).\n\n[1] Lu, J., Clark, C., Zellers, R., Mottaghi, R., & Kembhavi, A. (2022). Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks. http://arxiv.org/abs/2206.08916\n\n### Reproducibility\n\nPlease see the weakness W3 about the exclusiveness in this study.",
            "summary_of_the_review": "The proposed method shows strong performances across multilingual multimodal and unimodal downstream tasks. As a technical paper with empirical explorations, the authors tried to show two major observations; however, the two claims are weakly supported by experiments. In the weakness section, the suggestions to resolve these issues can be considered to improve the quality of the manuscript.\n\n---\n\nAfter reading the author's feedback, the raised issues seem to be sufficiently resolved. \nI decide to raise the score recommending to accept. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3201/Reviewer_6grV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3201/Reviewer_6grV"
        ]
    },
    {
        "id": "_T0MEx1WJ-",
        "original": null,
        "number": 2,
        "cdate": 1666640909336,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640909336,
        "tmdate": 1666640909336,
        "tddate": null,
        "forum": "mWVoBz4W0u",
        "replyto": "mWVoBz4W0u",
        "invitation": "ICLR.cc/2023/Conference/Paper3201/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes and effective strategy to jointly scale vision and language models that works on vision, language and vision & language multimodal tasks. The model is trained on a mixture of several pretraining tasks on a new dataset containing 1.6B samples in over 100 different languages. The model is simple and modular and achieves good performance on several multimodal tasks.",
            "strength_and_weaknesses": "### Strengths\n- The paper proposes an effective scaling strategy that significantly improves downstream performance on wide array of multimodal(vision+language) tasks\n- The VIT-e model trained for this work seems to be really helping multimodal tasks and will be a very useful contribution to the community if released\n- Experiments are quite extensive and thorough. Paper is well written and detailed.\n- The paper does a thorough ablation on the different pre-training mixtures and usefulness of the different objectives on downstream tasks, which is valuable.\n\n\n### Questions\n- I am slightly concerned with the technical novelty of the work as it is widely known that most of the multimodal benchmarks continue to perform well when the vision backbone/component of the model is better. \n- Language only tasks seem to have lower performance compared to T5XXL base model even with PaLI-17B. How does this change with smaller PaLI models where there is a smaller vision model? \n- It is unclear what is the impact of multilingual data and would have been great to see ablation with and without using multilingual data. \n- The performance on some of the datasets like VQA are way above human performance of ~81, which is a bit surprising. On tasks like VQA it has been evident from some time that better visual models have room for improvement, however to improve further, larger models must also get ambiguous answers correct. Although we are deduplicating and removing any overlap between pretraining and text data, do you think it might be possible there is some leakage, not in a full sample form, but either only on text side or image side? Can you provide more details about the deduplication?",
            "clarity,_quality,_novelty_and_reproducibility": "The above work is very clear and of high quality demonstrating importance of scaling vision and language components for significant improvement in performance on existing benchmarks. However there is limited novelty as the work shows results of scaling on a new dataset\n\nIn terms of reproducibility it is unclear how to reproduce the WebLI dataset which is important for pretraining PALI model. Similarly, the best performing model, VIT-e is trained on JFT-3B dataset which is also difficult to reproduce. Authors also has not mentioned about releasing code for this work.",
            "summary_of_the_review": "This work clearly shows the benefit of scaling both language and vision specific parameters in vision-language models. The work supports the findings through detailed experimentation and outperforms existing state-of-the-art methods by a significant margin. There is however not significant novelty in the methods but the findings are important for the community and hence my current rating. Looking forward to the author discussion phase.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3201/Reviewer_h6TT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3201/Reviewer_h6TT"
        ]
    },
    {
        "id": "7wVhXsXEW1",
        "original": null,
        "number": 3,
        "cdate": 1666680669561,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680669561,
        "tmdate": 1666680669561,
        "tddate": null,
        "forum": "mWVoBz4W0u",
        "replyto": "mWVoBz4W0u",
        "invitation": "ICLR.cc/2023/Conference/Paper3201/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new, joint encoder-decoder model for language and vision that leverages existing pre-trained encoder-decoder language models and vision transformers. The authors train the model on a large multilingual dataset containing 10B images and texts from 100 languages and investigate the joint scaling of the pre-trained components. Experiments show state-of-the-art results on several language and vision tasks, and the importance of scaling the vision component beyond the previous-largest.",
            "strength_and_weaknesses": "**Strengths**\n\nThe proposed encoder-decoder model design for language and vision tasks is simple and leverages existing pre-trained models to reduce training costs.\n\nFinds that scaling the vision component leads to better accuracy per parameter/FLOP compared to language components. \n\nPaLI outperforms the state-of-the-art on a large array of language and vision tasks such as image captioning, visual QA, language understanding, and zero-shot image classification. Notably, it even outperforms fixed-vocabulary approaches using an open-vocabulary generative setting.\n\n**Weaknesses** \n\nAnalysis of the computational cost & time required for training and comparison with other models such as Flamingo are missing. \n\nThe paper shares model and dataset cards, however, it is not clear if the actual artifacts will be shared publicly which can hinder reproducibility and lead to wasted time for other researchers. \n\nOther: \n-  When comparing to other models, it would be useful for the reader to show the size of the baselines. E.g. Tables 1 and 2. \n\n- What are the 1-shot and 5-shot performances of PaLi on the INet dataset? It reaches a very close performance to Flamingo even with a 0-shot but it would be interesting to also show if it can outperform the 5-shot version with a few examples. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the proposed ideas are clearly described.  Its narrative was easy to follow and the main claims were supported by extensive empirical results. \n\nIn terms of model novelty, the technical contribution is rather limited as it re-uses components that are already available. The novel contributions of this work include the scaling of language and vision components, reaching new state-of-the-art in several tasks, and coming up with an effective multilingual pre-training task for training such models. ",
            "summary_of_the_review": "Overall, this paper proposes a joint model for language and vision based on pre-trained components that reaches state-of-the-art in several downstream tasks. Key to its success is appropriate scaling and multilingual pre-training on a large number of tasks. Even though the artifacts are not shared, it provides models and data cards that can help interested researchers to reproduce the results. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3201/Reviewer_4Maa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3201/Reviewer_4Maa"
        ]
    },
    {
        "id": "Jyn_2t5-bxE",
        "original": null,
        "number": 4,
        "cdate": 1667414658706,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667414658706,
        "tmdate": 1667414658706,
        "tddate": null,
        "forum": "mWVoBz4W0u",
        "replyto": "mWVoBz4W0u",
        "invitation": "ICLR.cc/2023/Conference/Paper3201/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work introduces PaLI, a new large-scale vision-language pretraining model with multilingual enhancement. The architecture follows the previous scheme that leverages language pretraining as the main component which takes in vision and language feature tokens. The main contributions of this work are three folds: A new large-scale dataset on multilingual image-language pretraining, in-depth analysis of multilingual vision-language pretraining, pretrained models of different scales with SOTA performance on downstream tasks. ",
            "strength_and_weaknesses": "Strengths:\n- This work demonstrates a new multilingual pretrained vision-language model with the following distinguished properties: SOTA performance on multimodal tasks without catastrophic forgetting language-only understanding capabilities; multilingual pretraining while maintaining SOTA performance on English-only tasks. \n- An interesting empirical insight that may benefit future VL pretraining: large-scale vision pretraining may not benefit vision-only tasks, but could improve VL pretraining tasks by a large margin.\n- Sufficient in-depth analysis and ablation studies are performed for better model understanding.\n\nWeaknesses:\n1. Similar architecture for vision-language pretraining has early been proposed in various works, for example in [a]. To the best of my knowledge, the major difference is to use encoder-decoder architecture (mT5) for downstream VL pretraining. \n\n[a] MERLOT: Multimodal Neural Script Knowledge Models. NeurIPS 2021.\n\n2. Intuitively, the pre-trained model can be used for multimodal machine translation task. However, most tasks are still traditional VL tasks, such as VQA.\n\n3. Both image captioning Sec. 4.1 and VQA Sec 4.2 require extra finetuning on downstream datasets. What are the results of zero-shot performance? Is it possible to generate target answers with few-shot demonstrations by prompting?\n\n4. Can you show training and inference time duration comparisons with current VL pre-trained models across different scales of the proposed model?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is written in high-quality with clear demonstrations and discussions. The work is hard to be reproduced because of its high computational cost and massive data requirements. Nevertheless, this work provides detailed training and testing settings.",
            "summary_of_the_review": "This work provides a new multilingual multimodal pre-trained language model. The learning framework is not entirely new, yet some empirical insights are provided with sufficient experiments. This work may benefit future VL Pretraining or multilingual pretraining.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3201/Reviewer_ErFD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3201/Reviewer_ErFD"
        ]
    }
]