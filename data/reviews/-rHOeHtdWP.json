[
    {
        "id": "wHNFL6eN37L",
        "original": null,
        "number": 1,
        "cdate": 1666687256955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687256955,
        "tmdate": 1666687256955,
        "tddate": null,
        "forum": "-rHOeHtdWP",
        "replyto": "-rHOeHtdWP",
        "invitation": "ICLR.cc/2023/Conference/Paper4598/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes modifying the standard transformer recipe by using more attention heads instead of using layers. Their model has less parameters overall since they keep the feed forward network constant. Despite that, their model demonstrates better performance on a wide range of classification tasks compared to deeper counterparts.",
            "strength_and_weaknesses": "The paper convincingly lays outs the advantages of their approach. Better classification performance, better latency, and interpretability. The tasks they used are standard benchmarks. Mixed attention is also interesting.\n\nWeaknesses:\n- They have restricted themselves to classification tasks. What about language modeling or translation for example?\n- They haven't convincingly explained away why models like PaLM would choose to scale to 118 layers and GPT-3 has 96 layers.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and the experiments appear well done.\n\nIt should easily be reproducible based on description alone.\n\nThe main thrust of the paper (using more heads) is not novel, but using mixed attention is an interesting idea.",
            "summary_of_the_review": "The paper presents an intriguing idea that is worth exploring further, but ultimately I do not feel the that it is novel enough without the further exploration.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4598/Reviewer_Kk5h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4598/Reviewer_Kk5h"
        ]
    },
    {
        "id": "1SgUOYkKiR",
        "original": null,
        "number": 2,
        "cdate": 1666724175323,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724175323,
        "tmdate": 1666724175323,
        "tddate": null,
        "forum": "-rHOeHtdWP",
        "replyto": "-rHOeHtdWP",
        "invitation": "ICLR.cc/2023/Conference/Paper4598/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper argues that for transformers going deeper isn\u2019t always better, and they show this by proving that wide single layer transformers sometimes outperform deeper transformers in 4 NLP classification tasks, with no pre-training.\n\nThe paper reports results on an interesting dimension: for different numbers of attention heads per layer, keeping the total number of heads and other hyper-parameters constant. They run 9 different transformer variations (e.g. BigBird, Longformer, ...), and use different input lengths between 500 and 4000.\n\nThe result is that one-layer transformers seem to perform the best on avg by about 0.3%. One-layer transformers are also smaller in terms of number of parameters 1.4x and faster to run (on CPU 3.1x times faster, on GPU 1,9x).\n\nThe authors also claim that one-layer transformers are also more interpretable showing some examples, but this is a difficult claim to make objectively.\n",
            "strength_and_weaknesses": "Strengths:\n  * The contains a pretty thorough and principled comparison between deep and wide transformers.\n  * The paper does suggest that shallow transformers might be useful as distilled production models, for example if run on CPU. However they don't have experiments in this direction.\n\nWeakness:\n  * The study only considers the case of no pre-training. This makes their exploration much easier and less expensive, but their conclusions might not hold at the scales that are currently widely used. In fact the paper cites Xue et al (2022), saying that they come to the conclusion that deep transformers are better than wide in pre-training, although this is for images.\n  * The datasets used are only for classification. It would be very nice to see some generation or translation results as well.\n  * The interpretability claim is anecdotal and it is not compared against strong baselines for model interpretability in deep models.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and seems mostly reproducible. The direction of exploring shallow transformers is somewhat novel, but it doesn't seem that the authors have been able to show their usefulness for realistic applications, for example with pre-training or for distillation.",
            "summary_of_the_review": "The lack of results for pretraining really weakens the paper, since virtually all current interesting results in NLP are based on massive pretraining.\n\nGiven that the authors claim improvements in speed and footprint, they could try making this contribution stronger by comparing against prior work optimizing transformers for efficiency.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4598/Reviewer_RFvg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4598/Reviewer_RFvg"
        ]
    },
    {
        "id": "C7c2VVmGu_",
        "original": null,
        "number": 3,
        "cdate": 1666736460924,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666736460924,
        "tmdate": 1666736460924,
        "tddate": null,
        "forum": "-rHOeHtdWP",
        "replyto": "-rHOeHtdWP",
        "invitation": "ICLR.cc/2023/Conference/Paper4598/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates deep vs. wide (in terms of the number of attention heads) models with a variety of recent transformer variants. It argues that, compared to their deep and thin counterparts, shallow and wide architectures (1) achieve the same or better performance, (2) has fewer parameters, and (3) run faster. The experiments on four text classification datasets provide evidence supporting the claim, while those on image classification yield negative results.\n\n\n",
            "strength_and_weaknesses": "Strength:\n- Straightforward idea.\n- Writing is reasonably clear.\n- It\u2019s great that the paper considers CPUs in addition to GPUs in efficiency comparisons.\n\nWeaknesses:\n- The paper overclaims: it opens with a general claim that going wide is better than going deep on a \u201cvariety of NLP tasks,\u201d while the experiment is on three toy text encoding datasets. \n- The paper fails to acknowledge existing findings. The wider vs. deeper debate has been there for perhaps two decades, and recent results suggest that going deeper leads to better generalization. The paper argues for the opposite without discussing any existing work.\n\nFurther details and comments:\n- The wider vs. deeper debate has been there for a while. To convince the community that going wider is better, the paper needs to do a much better job. I suggest starting by covering a diverse set of tasks, e.g., language modeling, text generation, QA; and then considering larger-scale LM/MLM pretraining. Having a more comprehensive evaluation of the transformer model is more relevant than covering all the X-formers on toy datasets.\n- Given the existing work, my prior is that it is challenging (if at all possible) to match deep transformers\u2019 performance with wide and shallow models on most tasks. If this is the case, I\u2019d encourage the authors to explore the efficiency direction: can wider models achieve a better tradeoff between accuracy and efficiency? \n- I don\u2019t get the 5.3 arguments on interpretability. Is there anything you can do with a wide transformer that cannot be done with a deep one?\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is easy to follow\n- Technical novelty is not its strength. There are flaws in its claims.\n- I am not able to determine its reproducibility based on the experimental detail the paper provides.",
            "summary_of_the_review": "The paper has serious flaws in its claims and is not ready for publication. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4598/Reviewer_xaMf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4598/Reviewer_xaMf"
        ]
    },
    {
        "id": "lMk8lg6Kky",
        "original": null,
        "number": 4,
        "cdate": 1666815532345,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666815532345,
        "tmdate": 1666880138146,
        "tddate": null,
        "forum": "-rHOeHtdWP",
        "replyto": "-rHOeHtdWP",
        "invitation": "ICLR.cc/2023/Conference/Paper4598/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work empirically studies decreasing the number of layers in transformers while increasing the number of attention heads. On four classification problems and across many attention variants, this work found that a transformer with a single layer but more attention heads (such that the total number of attention heads remains constant) has on par or better accuracy than multi-layer transformers. This paper also argues that the attention matrices in a single-layer transformer are easier to interpret. Besides, results on image classification show that deep transformers work better there.",
            "strength_and_weaknesses": "Strengths:\n1. Surprising results that a single-layer transformer works as good as multi-layer transformers on sequence classification problems including listops which was constructed to test reasoning and presumably requires deep models.\n\nWeaknesses:\n1. I think the results do not support the title that wide attention is the way forward for transformers, since experiments are run on classification type problems, and it's not clear whether a single-layer transformer works on other types of problems such as text generation.\n2. While having fewer parameters is considered a plus, it is also possible that single-layer transformers overfit less due to having fewer parameters. What if you use really large datasets and more complex tasks such that model capacity becomes crucial? To convince readers that it's not the case, an experiment where the number of parameters of deep transformers is reduced to the same as single-layer transformers would be nice.\n3. In my opinion a fundamental limitation of single-layer models is that there is a single feedforward layer, which might lack the capacity to learn more complex tasks. A counter argument might be that we can increase the hidden dimension of the feedforward layer to increase capacity, but that comes back to general neural architecture search, and a really convincing experiment would be to run these experiments on more complex types of problems such as sequence generation.\n4. Regarding the interpretability argument, I'm not fully convinced it's more interpretable looking at Figure 3. It seems that we can pick one or two attention heads out of 48 heads that are interpretable, but I think we can do something similar in deep transformers.\n\nTypos:\n1. page 6: eg. the original ",
            "clarity,_quality,_novelty_and_reproducibility": "This work is clear, well-written, and original to my knowledge.",
            "summary_of_the_review": "I'm not recommending the acceptance of this paper because to me this work only shows that single-layer transformers work well for simple classification problems. I would be convinced the other way if the authors can show similar trends on more complex tasks such as sequence generation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4598/Reviewer_MwhQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4598/Reviewer_MwhQ"
        ]
    }
]