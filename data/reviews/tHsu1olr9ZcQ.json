[
    {
        "id": "fou44K6gcGG",
        "original": null,
        "number": 1,
        "cdate": 1666385890707,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666385890707,
        "tmdate": 1666737546145,
        "tddate": null,
        "forum": "tHsu1olr9ZcQ",
        "replyto": "tHsu1olr9ZcQ",
        "invitation": "ICLR.cc/2023/Conference/Paper486/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies reinforcement learning (RL) in high-dimensional continuous action spaces. It introduces Variational Reparametrized Policy (VRP), which formulates the policy as a generative model for optimal trajectories, optimized using a variational approach. The paper claims that this allows modeling complex and multi-model trajectory distributions. It evaluates VRP on two differentiable environments (a bandit and a grasping task) and compares against standard REINFORCE and Adam.\n",
            "strength_and_weaknesses": "Strengths: the basic idea of generatively modeling multimodal trajectory distributions is interesting.\n\nWeaknesses:\n\n- The structure and writing of the paper is poor, which makes it hard to follow. \n- The paper only discusses the very limited setting of differentiable environments.\n- The empirical evaluation is of low quality.\n\nPlease find detailed comments below.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity/Novelty**: One of my main concerns is the submission's clarity. The proposed method is not properly motivated, the writing style and the paper's structure is poor and the paper's contributions are not delineated clearly enough from prior work. This makes it hard for the reader to grasp the central ideas and to judge the approaches validity. I encourage the authors to restructure their exposition and better guide the reader by better motivating the central theoretical insights.\n\n**Quality**: As stated above, it is hard to judge the theoretical validity of the proposed approach due to the poor presentation. Furthermore, the paper focuses on the limited scope of differentiable environments and does not discuss nor empirically validate the applicability of VRP in the more general, non-differentiable RL setting. In addition, the empirical evaluation of VRP on differentiable environments is of low quality. I.p., the paper only contains results on a simple bandit problem and a grasping task and compares only against two very basic methods (REINFORCE, standard ADAM), completely ignoring any state of the art RL approaches (e.g., PPO or SAC). I encourage the authors to add more powerful baselines, extend their method to the non-differentiable setting and present quantitative evaluations on further problems.\n\n**Reproducibility**: While the paper contains the hyperparameter settings used in the experiment, there is  no source code available (although the authors promise to provide it in the future). As the experimental settings are only vaguely described, it will be hard to reproduce the experiments from the paper alone.\n",
            "summary_of_the_review": "While generatively modeling multimodal trajectory distributions is an interesting idea, the submission lacks clarity and provides only very basic empirical evaluations with limited scope. I encourage the authors to improve the exposition of their work and extend the experimental evaluation. In its current form, I recommend to reject this submission.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper486/Reviewer_5kNp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper486/Reviewer_5kNp"
        ]
    },
    {
        "id": "LrrmtbLRihk",
        "original": null,
        "number": 2,
        "cdate": 1666618661001,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666618661001,
        "tmdate": 1666618661001,
        "tddate": null,
        "forum": "tHsu1olr9ZcQ",
        "replyto": "tHsu1olr9ZcQ",
        "invitation": "ICLR.cc/2023/Conference/Paper486/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The work proposes a method for exploration in reinforcement learning.\nThe idea combines the concepts of variational inference and control as inference. In the method,\na variational autoencoder is used to learn a latent representation of the optimal trajectory distributions, where\nthe probability of optimality of a trajectory is proportional to $\\exp(R(\\tau))$ as in the typical control as inference\nframework. The \"decoder\" in this framework includes the policy that takes the latent variable $z$ as an input and tries to maximize the rewards. The latent variable $z$ can either be sampled at the beginning of an episode and kept as a fixed input \nto the policy, or it can also be periodically resampled using a state-dependent sampler.\nIt seems the main benefit is that $z$ leads to temporally correlated exploration.\n\nOne of the most closely related works is SeCTAR (Co-Reyes et al., ICML 2018) which uses a similar trajectory modeling framework, but\nthey use planning instead of policy search for action selection.\nOther related works include skill or options learning frameworks that also use a latent variable input to the policy.\nAnother difference to prior works is that the current work uses differentiable simulation to aid in the optimization.\n\nExperiments were performed in small bandit and other tasks, as well as in manipulation tasks with differentiable physics.",
            "strength_and_weaknesses": "**Strengths**\n1. The method seemed like a technically sound way to learn a latent space for exploration.\n2. The idea is widely applicable in reinforcement learning.\n\n**Weaknesses**\n1. The experimental comparisons all appeared new without any comparison on previous benchmarks or against previous implementations. Including at least one comparison with previous work might be good.\n2. The theorems (primarily the gradient estimator and the discussion around discontinuities) directly follow from previous work, and I don't think there was anything to really derive. The proofs and regularity conditions appeared redundant to me as the continuity requirements are well-known.\n3. I think the work lacked focus. For example, I think the main points of interest are whether the encoder is useful, and whether the ELBO objective is a useful theory. I didn't see any ablation study removing the encoder, or experiments where one samples a random $z$ giving this as an input to the policy and optimizing the rewards as usual. I would have liked to see stronger justifications regarding why the proposed method is useful in exactly the current embodiment compared to slight modifications of it.\n4. The experiments appeared contrived to me. For example on the bandit 1 and 2 tasks, results for only one set of hyperparameters were shown. If the initial variance of the Gaussian distribution is set sufficiently high, I believe all of the methods should be able to solve these tasks. It would be good to test more hyperparameters and show the sensitivity of the methods.",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality**\nWhile the theoretical derivations seemed fine, I was not convinced by the experimental work. \nI think the work would have benefited from including some standard benchmarks, and showing results for a greater range of hyperparameters. Also ablation studies on the different components of the algorithm would have been useful.\n\nAlso, the literature discussion could have included works on variational skill discovery and option discovery.\n\n**Clarity**\nI think there was a lot of room for improving the writing. \nThe introduction could be more focused, and mention closely related works such as SeCTAR (Co-Reyes et al., ICML 2018).\nIt would also be good to clarify what exactly the contribution is.\n\nI guess your method requires access to either a differentiable simulator or a model. These points could be made more explicit.\n\nThe discussion around discontinuities seems out of place. Also these issues were known much earlier than the cited works, e.g. see https://www.iro.umontreal.ca/~lecuyer/myftp/papers/intercha.pdf from 1993.\n\nLevine's review was also not the first the mention control as inference, and it may have been good to cite some earlier works.\n\n**Novelty**\nThere are several closely works, but I believe there are important differences to these related works.\n\n**Reproducibility**\nI believe sufficient details were given to reproduce the work.",
            "summary_of_the_review": "I recommend rejecting the paper.\n\nWhile the idea was theoretically founded, I believe the writing lacked focus making the paper not that clear. Moreover, the experimental work did not appear thorough, and I believe it requires more investigation. Including comparisons with existing benchmarks or implementations would be good. Ablation studies on the different components of the algorithm would also aid in convincing me.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper486/Reviewer_CRPH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper486/Reviewer_CRPH"
        ]
    },
    {
        "id": "DNs20FOOc51",
        "original": null,
        "number": 3,
        "cdate": 1666684504574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684504574,
        "tmdate": 1666684504574,
        "tddate": null,
        "forum": "tHsu1olr9ZcQ",
        "replyto": "tHsu1olr9ZcQ",
        "invitation": "ICLR.cc/2023/Conference/Paper486/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a new RL algorithm for continuous control. A key idea is to represent the policy as a distribution of trajectories in the problem space to capture the multi-modality nature of the problem. A variational lower-bound is derived to create a practical optimization objective and with some assumptions on the smoothness of the problem a gradient calculation formulation is derived that can leverage differentiable physics simulation. The resulting method is applied to a few robotic control problems in simulation to show the effectiveness of the method.",
            "strength_and_weaknesses": "Strengths:\n1) The proposed method is solid \n2) The ability to leverage differentiable physics and mitigate local minima issue is desirable\n3) Experiments shows the effectiveness of the algorithm\n\nWeaknesses:\n1) It\u2019s not clear how well the method can work if a differentiable physics is not available.\n2) It is mentioned that the method assumes lipschitz continuity in the problem while some of the presented tasks doesn\u2019t seem to be so all the time (e.g. when the object make contact with the rope). How does the method handle this?\n3) Furthermore, I'm wondering how well would the model be able to handle non-stable dynamics such as balancing a pole, or bipedal locomotion, where a small perturbation would lead to drastically different states later on. \n4) The method seems to be also related to methods that uses mutual-information for encouraging diverse behaviors (e.g. Mutual Information State Intrinsic Control, Discovering Diverse Solutions in Deep Reinforcement Learning by Maximizing State-Action-Based Mutual Information, etc). Some discussions with these methods would be useful.\n",
            "clarity,_quality,_novelty_and_reproducibility": "good",
            "summary_of_the_review": "The presented method is interesting and novel as far as I can tell. \nAs mentioned in the sections above, it'll be helpful if more discussions and potentially comparisons can be provided.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper486/Reviewer_LHSH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper486/Reviewer_LHSH"
        ]
    }
]