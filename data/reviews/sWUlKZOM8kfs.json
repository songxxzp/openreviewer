[
    {
        "id": "78-JhGsSzOd",
        "original": null,
        "number": 1,
        "cdate": 1666485716921,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666485716921,
        "tmdate": 1666485716921,
        "tddate": null,
        "forum": "sWUlKZOM8kfs",
        "replyto": "sWUlKZOM8kfs",
        "invitation": "ICLR.cc/2023/Conference/Paper4668/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a shuffled transformer structure for privacy preserving. The networks are split into two parts, where the first part encodes the input data and the the input to the second part network is shuffled features without knowing the original data. The experiments have shown superior performance against diverse adversarial attacks. ",
            "strength_and_weaknesses": "Pros:\n+) The paper proposes an interesting topic of performance privacy protection on shuffling input features in the Transformer structure;\n+) The paper provides theoretical deduction on shuffled transformer;\n\nCons:\n-) The motivation is not clearly elaborated. Examples of privacy leakage of transformers should be provided to validate the importance of designing a shuffled transformer;\n-) After shuffling, is the positional encoding not working on such Transformer structure?\n-) I'm curious about the performance degration with the proposed method compared with naive transformer structure. More analysis should be given.\n-) The paper is very hard to follow. Authors should provide a more clear elaboration of the proposed method. \n-) Writing could be improved. ",
            "clarity,_quality,_novelty_and_reproducibility": "-) Quality of the paper is difficult to evaluate based on the limited clarity of the method.\n+) The paper is original of its own. ",
            "summary_of_the_review": "The paper proposes a shuffled transformer for privacy protection. The idea is interesting and original, and the authors have provided a detailed theoretical deduction of the inference stage. However, the motivation seems vague to me and it is very hard to follow. I have doubts of whether such a shuffling on the feature space will affect the position encoding, and thus I'm not convinced by the qualitative and quantitative results. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4668/Reviewer_N7YY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4668/Reviewer_N7YY"
        ]
    },
    {
        "id": "NV_P0JFHGW",
        "original": null,
        "number": 2,
        "cdate": 1666734860430,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666734860430,
        "tmdate": 1666734860430,
        "tddate": null,
        "forum": "sWUlKZOM8kfs",
        "replyto": "sWUlKZOM8kfs",
        "invitation": "ICLR.cc/2023/Conference/Paper4668/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces blind training to preserve the data and model privacy via shuffled Transformers. An intriguing finding is proposed that, inputs and the model weights of the Transformer encoder blocks, the backbone of Transformer, can be shuffled without degrading the model performance.",
            "strength_and_weaknesses": "strength:\n\nThis work focuses on a valuable problem \u2013 data and model privacy preservation. The authors give a clear problem statement about the concept of blind training. Moreover, experiment results prove the effectiveness of the proposed method.\n\nweakness:\n\n1. The authors state that \u201cP_R can be chosen randomly for each Z, while P_C is randomly chosen once for the model\u201d. How the row and column shuffling strategy is determined? \n\n2. What are the differences between row shuffling in the paper with the original training data shuffling operations?\n\n3. Since only one type of column shuffling is adopted for each model, will the performance be affected by the highly correlated features? If two dimensions have large information overlap, swapping them seems have a small impact on the performance. Did the authors consider such a situation? Such a situation is beneficial or harmful for the proposed method? \n\n4. What does \u22a5 in Equ. (7) refer to? Perpendicular?\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper is understandable. \nThis is no code attached for reproducibility.",
            "summary_of_the_review": "This paper proposed a blind training method to realize privacy-preserving split learning, where the cloud trains over unknown data and model for the edge. Theoretical proofs, property verification, and real-world performance-resisting attacks are provided. The method successfully\ndefends black-box, and white-box attacks without degrading accuracy and efficiency. However, there are some concerns/ weaknesses need to be clarified.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4668/Reviewer_pVBj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4668/Reviewer_pVBj"
        ]
    },
    {
        "id": "3s1gSqFMQUs",
        "original": null,
        "number": 3,
        "cdate": 1666797150418,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666797150418,
        "tmdate": 1666797150418,
        "tddate": null,
        "forum": "sWUlKZOM8kfs",
        "replyto": "sWUlKZOM8kfs",
        "invitation": "ICLR.cc/2023/Conference/Paper4668/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes *blind learning*, a novel split learning framework for training transformers. In this framework, the patch embedding, MLP, and loss layers reside at the client, and the transformer blocks reside at the server. During training and inference, the client obfuscates its data and model weights by shuffling the patch embeddings before sending them to the server, and then unshuffles the outputs of the transformer blocks sent back by the server. Crucially, since transformer blocks are invariant to random shuffling of the input data, the proposed shuffling by the client does not degrade model training or inference. Theoretical analysis and experimental evaluations demonstrating the transformer invariance to shuffling are provided. A privacy analysis of the proposed framework is also provided. A novel privacy definition for shuffling mechanisms is also provided. Lastly, the proposed framework is compared against various baselines with respect to classification accuracy for a target task and susceptibility to black-box and white-box reconstruction attacks.",
            "strength_and_weaknesses": "# Strengths\n\n* Novel algorithm. The main idea of using the shuffle invariance property of transformers to obfuscate the client data and model weights during split learning is ingenious, novel, and well motivated.\n\n* Novel theoretical results. Analysis of the shuffling invariance property of transformers is novel and provides a strong theoretical foundation for the proposed framework. Privacy definition for shuffling mechanisms is also interesting and novel, albeit a bit tangental.\n\n* Good results. The proposed method achieves a favorable privacy-utility trade-off compared to relevant baselines. This is demonstrated clearly with experiments on the CelebA and CIFAR10 datasets. \n\n# Weaknesses \n\n* Poor writing. The paper would benefit greatly from a review by a native english editor.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity.** Fair: The writing, figures and tables are clear, but the english is poor.\n\n**Quality.** Good: The paper appears to be technically sound. The proofs, appear to be correct, but I have not carefully checked the details. The experimental evaluation, is adequate, and the results convincingly support the main claims.\n\n**Novelty.** Good: The paper makes non-trivial advances over the current state-of-the-art.\n\n**Reproducibility.** Excellent: key resources (e.g., proofs, code, data) are available and key details (e.g., proof sketches, experimental setup) are comprehensively described such that competent researchers will be able to easily reproduce the main results.",
            "summary_of_the_review": "Novel framework and theoretical analysis. Strong results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4668/Reviewer_kXfb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4668/Reviewer_kXfb"
        ]
    },
    {
        "id": "HIK1ojUw3oO",
        "original": null,
        "number": 4,
        "cdate": 1667204907716,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667204907716,
        "tmdate": 1667204907716,
        "tddate": null,
        "forum": "sWUlKZOM8kfs",
        "replyto": "sWUlKZOM8kfs",
        "invitation": "ICLR.cc/2023/Conference/Paper4668/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a shuffling based method to improve the privacy and enable a split learning paradigm, where the cloud server only requires shuffled data from the edge. ",
            "strength_and_weaknesses": "Strength: The proposed approach can improve the privacy of edge users.\n\nWeaknesses: \n\n1. The experiments are limited to vision Transformers, but the title seems to over claim it to any Transformer. For languages, it should be hard to train without position embeddings, as the orders of the tokens are important. I would suggest the authors to either modify the title or show some empirical evidence that the method also works for language models.\n\n2. I am quite confused with the notion of row and column shuffling. How are the matrix multiplications defined (Wx or xW)? At the beginning of Section 3, each X is just a vector. Do you shuffle on the patch level or pixel level? If you only change the order of the patches, isn't it trivial that the output won't change when position embedding is not used?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity can be improved by giving the definition of the \"rows\" and \"columns\" in a more obvious place. ",
            "summary_of_the_review": "I feel the paper lacks clarity in both the scope and the technical details. The proposed method might just be a trivial shuffling of the image patches without position encodings, and therefore lack technical novelty. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4668/Reviewer_jfZZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4668/Reviewer_jfZZ"
        ]
    }
]