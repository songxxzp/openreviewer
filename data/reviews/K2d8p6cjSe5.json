[
    {
        "id": "v8wH9dpcS6",
        "original": null,
        "number": 1,
        "cdate": 1666185782347,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666185782347,
        "tmdate": 1668702544655,
        "tddate": null,
        "forum": "K2d8p6cjSe5",
        "replyto": "K2d8p6cjSe5",
        "invitation": "ICLR.cc/2023/Conference/Paper2789/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the learning problem in three aspects. 1. From the perspective of information theory, when the loss is bounded and the entropy of the data distribution is small, one can learn only in the typical set; 2. Modeling RNN propagation as the iterative method that solves sparse sensing and RNN training is equivalent to dictionary learning; 3. Proposed the RNN algorithm with weighted image. Finally there are experiments verifying the points.",
            "strength_and_weaknesses": "This paper is broad that touches three aspects of the learning problem, and the insight from info theory provides a different perspective to ML community. I believe the math proof is correct and there are illustrative experiments as well. Below are detailed questions.\n\n- The introduction is not clear. I would suggest moving Figure 1 to the front. Now when I read it, seems the beginning part starts from introducing the last two blocks of the figure, not mentioning the left part. I\u2019m confused about the setup yet, is it like, there is a system that encodes x as y, and we want to find the decoder/inverse map from y to x? How is it relevant to RNN? \n- I think the logic is not quite clear to readers. The connection among three parts seems to be weak. For example, the first part is relevant to sample complexity, but the second part is analysis of RNN forward algorithm. How is it relevant to typical set stuff?\nAlthough there are a few theorems in the first part, I think most of them are textbook knowledge. The entropy is directly relevant to how hard the learning is and the typical set represents the most likely appearing instances following the distribution. I don\u2019t find much novelty here.\n- The justification of the Algo 1, especially the novelty, is very short and I cannot understand it. \n- The paper has the title \u201cfew-shot\u201d, but I cannot see how this paper contributes to it, i.e. how to learn from \u201cfew\u201d data. The typical set stuff seems to be relevant, but perhaps I did not catch its relation to the other sections. \n- Section 3,4 constructs the equivalence of iterative lasso solver and RNN, but I cannot see why this connection is important and correct. For example, the whole sequence of RNN is important, but are the iterations in the middle of lasso solver important, what do they mean? I feel in lasso, the iterations would be more and more sparse, is it an expected and important property of RNN sequence? Slight differences between lasso and RNN exist, like the bias $b$, and the thresholding parameter $\\lambda$, etc. \n- Critical: Comparing (14) and (15), is $W_{zz}$ forced to be a PD matrix as assumed in lasso solver, is it enforced that $I -c W_{zy}W_{zy}^T = W_{zz}$? If $W_{zy}$ and $W_{zz}$ represent $D^T/c$ and $I - D^TD/c$ then they have to follow that equality. One needs to justify and properly use the similarity between lasso solver and RNN, otherwise it just looks like a coincidence.\n- Page 5, $cI-D^TD \\succ 0$.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is original, but a clearer connection among sections and better explanation would be helpful.",
            "summary_of_the_review": "Given my confusions and questions above, I would like to see more authors' response and clarification, and would definitely be willing to increase the score if it's convincing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2789/Reviewer_WHvp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2789/Reviewer_WHvp"
        ]
    },
    {
        "id": "Uj4yWqlVX6Z",
        "original": null,
        "number": 2,
        "cdate": 1666580280359,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580280359,
        "tmdate": 1666580280359,
        "tddate": null,
        "forum": "K2d8p6cjSe5",
        "replyto": "K2d8p6cjSe5",
        "invitation": "ICLR.cc/2023/Conference/Paper2789/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper targets the understanding of few-shot learning under the image reconstruction task. Specifically, it focuses on OCT speckle suppression and image deblurring, which use local information to clean the speckle/noise from the full image. The performance on the two tasks has successfully demonstrated the effectiveness of the approach though the tasks are kind of simple and may be of limited impact. Regarding the approach side, it first provides a theoretical analysis of few-shot learning from the perspective of asymptotic equipartition property and shows that there is a relatively small set (i.e., typical set) whose sample entropy is close to the true entropy and can represent the whole data distribution. Then, it proposes to connect RNN with sparse coding and treat the RNN as an unfolding of one iteration of the sparse coder. Finally, the RNN network is used in few-shot learning for related tasks. ",
            "strength_and_weaknesses": "Strength:\n1.\tIt is interesting to find the connection between RNN with a sparse coder, which is then used for few-shot learning in speckle suppression and image deblurring tasks.\n2.\tIt is surprising to obtain reasonably good performance by training the model on a single training image set.\n3.\tClear explanation about the newly proposed perspective of data generation w.r.t. AEP.\n\nWeakness:\n1.\tAt Page 4, right after Eq.3, it is claimed that the sample size should be necessarily large. However, I fail to see clear connection between the sample size and the proposed RNN-based approach. \n2.\tThough RNN can be used as an unfolding of an iterative algorithm, it still requires the assumption that different columns in single patches are formulated as a sequence with clear order. However, this may not be true. At least a bi-directional RNN considering the order of columns along two complementary directions should be considered.\n3.\tThe experiments are not comprehensive. In Table 1, the performance difference between RNN & RNN-RFN is not significant. A more detailed explanation should be provided. Also, the PSNR and SSIM are not clearly labeled (I assume only the bottom approaches have SSIM scores).\n4.\tWhat are the compared approaches? The author should at least give a reference for approaches listed in Table 1 and explain why the performance underperforms the compared approaches by a large margin.\n5.\tAs mentioned above, the tasks used for evaluation are too simple. In addition to the simple cases, evaluation on a more challenging task (e.g., a more challenging noise situation added in image deblurring) should be performed.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: \nThe concept is clear.\n\nQaulity: \nThe idea is realtively novel but the task is simple.\n\nReproducibility:\nNeed some more details",
            "summary_of_the_review": "Though detailed analyses are provided, the experiment is not quite promising, and it is hard to measure the impact of this approach. In addition, as mentioned above, further discussion is needed to determine whether the image patches can be formulated as sequences.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2789/Reviewer_GAhp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2789/Reviewer_GAhp"
        ]
    },
    {
        "id": "ay7bvMDixP",
        "original": null,
        "number": 3,
        "cdate": 1666691529307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691529307,
        "tmdate": 1666691529307,
        "tddate": null,
        "forum": "K2d8p6cjSe5",
        "replyto": "K2d8p6cjSe5",
        "invitation": "ICLR.cc/2023/Conference/Paper2789/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors provide some theoretical statements showing that a relatively small set that can empirically represent the input-output relationship for learning.\nThey then pointed out that sparse coding with iterative solutions can be modeled by RNN computation.\nThey applied the proposed method to few-shot learning.\nExperimental results on image deblurring and optical coherence tomography (OCT) speckle suppression demonstrate that the proposed method performs well.",
            "strength_and_weaknesses": "*Strength\n- Experimental results of the proposed method showed promising performance.\n\n*Weaknesses\n- The relationship between the theoretical part (Section 2) and the actual implementation part (Sections 3-5) is unclear. The theory of sparse coding itself might work for this instead of the discussion of AEP.\n- Clarity is low.",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity\n- In p.3, there are no definitions for g(x) and e(x).\n- In p.3, n has multiple meanings for the superscript n for x^n and the number of dimensions of x, n, which is confusing.\n- In Eq.1 and 2, x and y are not bold. Is it a typo? Or are they different from other bold x and y?\n- In Eq.3, there is no definition for Delta.\n- The character lambda is duplicately used in different meanings as the parameter in Eq.7 and eigenvalue.\n- In Eq.15, there is no bias term compared to Eq.5. Are there any analyses or discussions with that?\n- In Eq. 11 and 15, what can we interpret W and S as (like D is a dictionary)?\n- How do the authors optimize W and S? What are training samples for z?\n- Definition 1 is hard to follow. Providing some illustrations can help.\n\n*Quality\n- Please see the above comments.\n\n*Novelty\n- The proposed method might be novel.\n\n*Reproducibility\n- Code is unavailable.",
            "summary_of_the_review": "The relationship between the theoretical part and the actual implementation part is unclear. Clarity is low.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2789/Reviewer_uFRZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2789/Reviewer_uFRZ"
        ]
    },
    {
        "id": "ypWV3msJICc",
        "original": null,
        "number": 4,
        "cdate": 1666700508012,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666700508012,
        "tmdate": 1666700508012,
        "tddate": null,
        "forum": "K2d8p6cjSe5",
        "replyto": "K2d8p6cjSe5",
        "invitation": "ICLR.cc/2023/Conference/Paper2789/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "First, the authors make some proofs (whose correctness I haven't fully verified) based on  the asymptotic equipartition property that show there exists a small set that can represent the whole data distribution. Then, the authors make some connections between RNNs and sparse coding. Finally, authors test RNNs and RNNs with receptive field normalization in some ad-hoc tasks without implementing proper baselines.",
            "strength_and_weaknesses": "The empirical evaluation of the paper is very very limited. If the authors want to make a convincing argument they will have to use more standard datasets and make comparisons to better approaches. The RNN model performs very close to the RNN-RFN according to Table 1. I wouldn't be surprised if dropout or l2 regularization would push the performance of a basic RNN beyond the other approaches.\n\nI haven't fully understood the mathematical results, but they seem overly complicated without adding much to the proposed approach (I spent quite some time going through them).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is unnecessarily complicated to read and doesn't focus on it's topic. Sections 3 and 4 give a lot of details about sparse coding that do not add much to the proposed approach.",
            "summary_of_the_review": "The paper needs to have a clear focus and structure. The theoretical results don't seem insightful. On top of that, the empirical evaluation of the results is far from enough to make any serious claims about the approach.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2789/Reviewer_WSnj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2789/Reviewer_WSnj"
        ]
    }
]