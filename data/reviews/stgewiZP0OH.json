[
    {
        "id": "9OguoBqXoak",
        "original": null,
        "number": 1,
        "cdate": 1666567043984,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666567043984,
        "tmdate": 1666567043984,
        "tddate": null,
        "forum": "stgewiZP0OH",
        "replyto": "stgewiZP0OH",
        "invitation": "ICLR.cc/2023/Conference/Paper4518/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes RTC, an imitation learning approach for stochastic environments that aims to learn policies that avoid mode collapse and cover a wide range of behaviors shown in demonstrations. The paper argues that current methods relying on inferring agent types to explain multi-modal behaviors in stochastic environments suffer from Conditional Type Shift: the agent latents are encoded using a full trajectory and therefore do not account for environment uncertainty, while at test time, agents do not have access to the future, creating a shift between how the agent was encoded at training time and how it is deployed now. To address this, RTC uses proposes to learn a policy by using both the prior and posterior over agent types during training, making sure that trajectories under both distributions are likely according to a discriminator and that the likelihood of both distirbution is high.  \n",
            "strength_and_weaknesses": "**Strengths:**\n\n- Motivation\n\t- This is an important and well motivated problem, there are many scenarios in which having agents that cover multiple modes is important, and most environments are stochastic or become stochastic as multiple agents are added.\n- Clarity\n\t- Section 3 gives a clear and intuitive overview of the conditional type shift problem that this work is addressing. I wish some of it was more present in the introduction. \n\t- Very clear overview of RTC in Figure 3 and generally over setions 3 and 4. I value that authors provide explanations of the effects that every loss is trying to mitigate, and propose strategies for different kinds of environment conditions (such as having vs no having access to environment gradients).\n\t- Very clear analysis of results, for both benchmarks.\n- Novetly\n\t- The work proposes a new approach to encode variations in policies learned via imitation learning. The core idea builds on top of GAIL and other works encoding agent variability using latent codes, but introduce novelty to separate agent behavior variability vs environment variability.\n- Experiments:\n\t- Approach is tested against strong baselines and sound ablations, with a simple environment that is easily interpretable and a more challenging real-world environment.\n\t- SOTA results in the Waymo Open Motion Dataset, compared to strong baselines, in terms of performance and mode coverage. The results, particularly the comparison of Hierarchy and RTC shows that the proposed method allows to both cover a high number of modes, but also deal with covariance shift that arises from using a posteriro during training.\n- Limitations well addressed in conclusion.\n\n**Weaknesses:**\n- More details on claims\n\t- It would be valuable to provide a reference or more details on some of the claims made in the paper, such as the failure to cover modes of adversarial approaches (in intro).\n\t- I am not sure I see why encoding encoding all the stochasticity into the agent is a bad approach. In the example in Figure 1, a model-based approach would need to account for stochasticity of the other agents, but the behavior of the agent we are imitating, which only has information of the current and previous states could be controlled by their internal factors, such as risk tolerance to cross or not. A risk tolerant driver will cross when a car is approching, but will stop if the car has already started crossing, avoiding problems. A non-risk tolerant driver will stop directly. I am not sure why we need to explicitly encode the env stochasticity. \n\t- Isn't it a high assumption that there is a differentiable environment? How well does this method work if having to use reinforcement learning?\n- Experiments:\n\t- For the double-goal environmnet, it would have been interesting to test the effect of environment stochasticity, given that this is one of the claims of the paper. How do the baselines change where there is more/less stochasticity.\n\n**Details**:\n- I see that the Lprior, maximizes the likelihood of \\hat{g}_{e} and \\hat{g}_{p}. Would it make sense to minimize the KL divergence between the 2? Is there a degenerate case where both prior and posterior have high likelihood according to p_theta, but non over-lapping support? What would prevent that?\n\n- Where is the value function in the method proposed here, compared to Symphony? Does it come from the discriminator? Why does it matter if the policy is optimized end-to-end here, instead of using policy gradient?\n\n\n- Caption in Figure 4 could explain more what the curves in the bottom row mean.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\nVery clear explanation of the method, motivating examples and limitations/comparisons with other methods, both through text and in the figures.\n\n**Reproducibiliy:**\nMethod is explaining with high enough detail, but there is no information about hyperparams, resources, training time.\n\n**Novelty:**\nThe work proposes a new approach to encode variations in policies learned via imitation learning. The core idea builds on top of GAIL and other works encoding agent variability using latent codes, but introduce novelty to separate agent behavior variability vs environment variability.",
            "summary_of_the_review": "The paper addresses an important problem, which is well motivated, and described why it arises. While there are some details (see weakness/details) that I would really like clarified, the approach for RTC addresses some of the limitations of existing IL approaches with latent codes for agent type. The results, both the simple environment to provide more intuition about the method and the Waymo Open Motion show the value of this method. Hence, I think this paper should be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4518/Reviewer_iTsU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4518/Reviewer_iTsU"
        ]
    },
    {
        "id": "MDrZA0NbHa",
        "original": null,
        "number": 2,
        "cdate": 1666878334105,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666878334105,
        "tmdate": 1666878334105,
        "tddate": null,
        "forum": "stgewiZP0OH",
        "replyto": "stgewiZP0OH",
        "invitation": "ICLR.cc/2023/Conference/Paper4518/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Mode collapse is a problem in imitating multi-modal trajectories. Hierarchical method use latent code to represent the agent type or context variable which is inferred during the training and later used at test time, and the policy should be conditioned on the latent code. Context variable include internal (intrinsic setting of agents) and external factors (coming from environment)\n\nBeyond Covariate shift, the author considers marginal type shift where the prior distribution of agent type changes, where conditional type shift is the external context variable is dependent on the internal context variable. Adversarial imitation learning optimize the policy via a discriminator to distinguish between true trajectory and learned trajectory in a min max fashion where the policy try to imitate the observed trajectory and the discriminator try to distinguish between them. By dividing the data input to be the policy sampled trajectory and the real observed trajectory, the discriminator classification loss could be used as reward for a reinforcement learning agent. \n\nThe author proposed Robust Type Conditioning by incorporating prior during training, which is summarized into a 4-term loss function evaluated on prior policy and encoded policy.\n\n\nResults are tested on Waymo Open Motion dataset. \n\n",
            "strength_and_weaknesses": "Strength:\n1. The author considered a very comprehensive treatment of distribution shift, while most previous work focuses on covariate shift only. \n\nWeaknesses:\n2. I think the method section (section 4: Robust Type Conditioning) is too short, the different components of the loss are not fully explained, which only last for a bit more than 1 page.  For instance\n\n2.1 Why the reconstruction loss prevents mode collapse by penalizing the agent for being unable to mimic \\pi?\n2.2 Why the adversarial loss could eliminate marginal and conditional shift?\n2.3 Could you explain more about the information bandwidth of \\hat{g}_e?  Why the prior on agent type could filter out the environment (external) noise? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Fair\nQuality: Good\nNovelty: Good\nReproducibility: Not sure",
            "summary_of_the_review": "Adding latent variables to imitation learning is crucial for mimicking multimodal behaviors, as well as being able to generalize better in terms of distribution shift. \n\nThe author offers a very comprehensive view about tackling the problem with fair amounts of experimental efforts, however, I think it is not very well explained, how each of the individual aspects were tackled, for instance, if one remove one of the loss terms, how much the performance will deteriorate? \nWhat is the theoretical contribution of this paper? Is that learning the prior of agent type? ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4518/Reviewer_JuGQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4518/Reviewer_JuGQ"
        ]
    },
    {
        "id": "yeYX5_K9fc",
        "original": null,
        "number": 3,
        "cdate": 1667337221683,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667337221683,
        "tmdate": 1667392995867,
        "tddate": null,
        "forum": "stgewiZP0OH",
        "replyto": "stgewiZP0OH",
        "invitation": "ICLR.cc/2023/Conference/Paper4518/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper focuses on the problem of imitation learning in stochastic environments. The stochasticity in the environment is characterized by the parameter  $\\xi$, that is independent of the expert/agent.\nThe goal of this work is to build imitation agent that achieve distributional realism \u2014 match the expert distribution covering all modes of demonstrations while achieving good task performance.\nAs a solution approach, the authors attempt to build on and improve previous works on designing hierarchical policies - conditioning the imitation policy on type of expert demonstration- to achieve the stated goal.\nThe hypothesis is that the demonstrations exhibit multiple modes due to difference in types of expert (where type models expert-specific features e.g. persona, goal, strategy or belief) and such types may be sampled from demonstration of available as labels in the data. The authors argue that existing works in type-conditioned imitation learning (especially when such types are latent and sampled from data) either fail to achieve satisfactory mode coverage or compromise task performance. The authors identify two types of distribution shift in the inferred types - marginal (mismatch between posterior and prior type samples)  and conditional (mismatch arising due to the inability of existing type encoders  to disentangle environment noise and agent specific features) - as the reason for inducing causal confusion in the agent, thereby degrading distribution matching and task performance. The authors propose new objective that aims to mitigates these shifts by allowing to train with types sampled from both posterior and prior (unlike previous methods which only consider posterior types in training) and augmenting the imitation loss with information bottleneck term to address the conditional shift stated above. Empirical investigation on a toy double goal problem and more complex Waymo Open Motion dataset is presented to discern various components of the proposed objective and provide comparisons with representing adversarial IL baselines in terms of task performance and distribution matching.",
            "strength_and_weaknesses": "Strengths:\n-----------\n+ Imitation learning in stochastic environments is an important problem as most real-world environments have stochastic elements and an approach that can disentangle stochasticity in environment from stochasticity in agent behavior is very useful direction. \n+ The proposed method show consistent improvement in performance across different metrics over considered baselines.\n+ Effective evaluation of imitation learning method is an open issue and the considered metrics, especially Average displacement error and JSD based metrics are interesting and insightful to use for evaluation.\n\nWeaknesses:\n---------------\n- The proposed approach heavily relies on the insight that existing type conditioned approach leads to causally confused agents as the design of such methods require the agents to infer the stochasticity of environment from the latent type encodings whose primary purpose is to provide agent-specific information. However, this entire premise is not given a rigorous treatment and presented very loosely with anecdotal examples without any theoretical or empirical  grounding. This makes the identification of conditional distribution shift as a reinterpretation of previous works in light of the proposed approach rather than a concrete motivation. For instance, the example of pass/yield is used to claim that the policy stops using current observation to infer stochasticity and only relies on latent type. Without establishing theoretically or empirically, that existing strong baselines indeed end up doing this, such a claim is not substantiated.\n- The main contribution of this work is the objective function Eq 2. that is proposed to mitigate the marginal and the conditional distribution shifts. The reconstruction and adversarial loss have been extensively used by previous imitation learning works [1] even for hierarchical type-conditioned policies [2]. Effectiveness of Information bottleneck term has also been studied previously in the context of imitation learning [3]. Further, the explanation of why this term, while helpful in gaining most of empirical performance (see later), contributes to the disentanglement of $\\xi$ and type is unclear and adhoc. The use of sampled prior for adversarial term is novel but it is highly incremental contribution given the existence of previous works on type-conditioned imitation learning. Again, it is not clear why introducing this term in effect, mitigates the marginal shift. \n- There are several strong works that are either missing citations [4.5,6] or discussed in a very hand-wavy manner without providing comparisons with them [2,6]. Further, a great weakness of the paper stems from the unsubstantiated claims by the authors claim that hierarchical methods such as [2,6,7] give rise to conditional and marginal shifts . It is imperative that such statements be backed up with rigorous analysis of those methods to effectively showcase the existence of identified issues.\n- Due to the above limitations, the burden of the work lies on empirical support, however, the paper has several shortcomings in this area:\n    - Lack of comparisons with strong baselines [2,4,5,6,7] is a big miss. It is imperative to show the efficacy of approach as compared to these works as they can be considered as different variants of this approach. \n    - The authors mention that an ablation Hierarchy-NoPT which considers no prior training is similar to existing hierarchical methods but it does not cover the methods cited here. Further, if one considers Hierarchy-NoPT as representative of existing methods, then the improvement with prior training is marginal and hence full approach has very limited contribution. \n    - Minor follow-up: Given the high performance on  minADE for Hierarchy-NoPT in WOMD dataset, the high collision rate seems like a bug and the reasoning for this is not adequate and seems adhoc. \n    - On the double -goal problem, the hyper-parameter for IB is shown to be very sensitive which is a limiting factor as it may not be easy to tune this hyper-parameter. Further, given that this term affects the performance more than any other objective and it is very sensitive, comparisons of the form baselines + IB becomes very useful. For instance, will [2] + IB provide superior performance than proposed approach?\n    - Ablation is only provided on toy task however, there is a wide gap in the characteristics of double goal problem and WOMD and hence an ablation on WOMD is also required. Plus, further granular ablation is needed for ex. does reconstruction + IB provide as good performance?\n    - It is mentioned that learning rate was only tuned on MGAIL to 0.0001 and then used for all methods. This is a strange design choice and tuning should be done specific to each method.  \n    - While there is performance improvement across different metrics, it is very difficult to establish that this gain has anything to do with the mitigation of conditional and marginal distribution shifts induced by other methods, which fails to support the main claims of the paper.\n\n[1] Imitation Learning: Progress, Taxonomies and Opportunities, Zheng et. al. 2021.\n\n[2] Robust Imitation of Diverse Behaviors,  Wang et. al. Neurips 2017.\n\n[3] Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow, Peng et. al. ICLR 2019.\n\n[4] Imitation Learning from Visual Data with multiple intentions, Tamar et. al. ICLR 2018\n\n[5] Triple-GAIL A multi modal imitation learning framework with Generative Adversarial Nets, Fei et. al. 2020\n\n[6] Multi-modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets, Hausman et. al. Neurips 2017\n\n[7] Learning a Multi-modal policy via Imitating Demonstrations with Mixed Behaviors, Hsiao et. al. Neurips Workshop 2018",
            "clarity,_quality,_novelty_and_reproducibility": "- The quality of the paper is fair. The paper lacks rigorous treatment of the previous works and makes unsubstantiated claims about the problem in those works without providing adequate evidence. It then uses these problems as motivation to propose the new approach which appears more adhoc than inspired from these problems. The lack of adequate empirical support for the claims of  existence of conditional and marginal shifts and lack of comparisons with strong baselines further decreases the quality of this work. \n- The paper is easy to follow for most parts but there are several statements or lack thereof that decreases the clarity of paper. \n    - For instance, in the introduction, It is not clear what is meant by inferring types from future trajectories. Does it signify the use of entire trajectory for inferring the latent vector (so the future mean all transitions of trajectory known in advance)? \n    - Next, access to a differentiable environment is strong requirement and it is not clear if it only applies to the compared MGAIL baseline or also used for the proposed approach.\n    - In eq 2 why does $\\mathcal{L}_{prior}(\\tau)$ appear twice?\n    - Presentation of plots is very complicated e.g. frequency lower goal in Figure 4 also has box plots belonging to different methods which are hard to read.\n    - Why is it required to have last action as part of state for double goal problem but not WOMD?\n    - Why is adversarial training of only sampled prior (i.e. no training based on posterior sample) not enough?\n- Given that most terms (Except the prior based adversarial training) have been explored and studied previously, the overall originality of the work is very limited. This limitation is further exacerbated by the lack of support for the claims of existence of conditional and marginal shifts.  ",
            "summary_of_the_review": "While the paper deals with an important problem of imitation learning in stochastic environments, the proposed approach is highly incremental and adhoc. The identification of marginal and conditional distributional shifts appears as a reinterpretation rather than motivations grounded in theoretical or empirical evidence. This coupled with multiple major shortcomings in the empirical exercise makes this contribution below par in its current form and informs my current assessment. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4518/Reviewer_MSid"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4518/Reviewer_MSid"
        ]
    },
    {
        "id": "y-d5pAKGrx5",
        "original": null,
        "number": 4,
        "cdate": 1667513798870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667513798870,
        "tmdate": 1667513798870,
        "tddate": null,
        "forum": "stgewiZP0OH",
        "replyto": "stgewiZP0OH",
        "invitation": "ICLR.cc/2023/Conference/Paper4518/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to address the issue of replicating an expert's entire distribution of behaviors in imitation learning. In stochastic environments, an agent's behavior distribution can be caused due to its internal strategies or environment stochasticity. The paper claims that prior approaches suffer due to either not being distributional or because they do not disentangle environment stochasticity. This paper proposes existing tricks, such as using variational models, sampling from a prior distribution while training, and adding an information bottleneck to solve these issues.",
            "strength_and_weaknesses": "## Strengths\n- The overarching goal of replicating the entire distribution of an expert's behaviors in stochastic environments is well known to be important.\n- Experiments are reported on a real-world dataset with good practical impact.\n\n## Weaknesses\n- **Writing and lack of clarity**: The paper is written in a way that is very hard to follow and understand the primary contributions of this work.\n    + Many uncommon terms are used for existing alternate terminology, e.g., agent type (for trajectory encoding), hierarchical architecture (for variational methods), and distributional realism (which is already a goal of variational methods).\n    + Lack of structure and flow across paragraphs makes it hard to follow.\n        * The introduction lists many aspects and solutions but does not clearly list what the exact problem being solved in this paper is and how their contributions solve them.\n        * Likewise, Section 3 is quite hard to follow. Is the paper solving two problems, marginal and conditional type shifts, or causal confusion as well?\n    + The assumption of a known differentiable environment suddenly appears on Page 3 without any prior mention or justification.\n    + What are the contributions of this paper's Robust Type Conditioning (RTC) method? From the writing, it seems that all 4 proposed losses are novel contributions. But that is undoubtedly not the case, as all of these losses are well-known and commonly used losses. The writing should be clear enough to distinguish what exists in the prior work and what is contributed by this method. This is also important for knowing what ablations should be performed. More on this is below.\n- **Problem Formulation (Section 3)**:\n    + A lot of issues are there with clarity.\n        * If the majority of the problems in the paper come because of stochasticity from the environment in the input trajectory, then why is Figure 2 explained using a trajectory that has just one (s, a) pair? It makes it very hard to understand conditional-type shifts.\n        * If marginal type shift is an existing problem in VAEs, called the prior hole problem, how is it solved in the prior work, and what is this paper doing differently?\n- **Approach (Section 4)**:\n    + Unclear why the problems mentioned in the paper arise when using standard approaches. For instance, training a standard state-conditioned (not trajectory-conditioned), variational GAIL policy should be able to resolve the problem of mode collapse, which is claimed to be one of the critical problems in the paper.\n    + The paper says that the reconstruction loss prevents mode collapse. But, the paper also argues that the hierarchical (or variational) model is what prevents the mode collapse. The way it is written, it seems the reconstruction loss is a novelty. However, it is unclear to me how that is.\n    + By minimizing the $L_2$ distance between agent true and predicted states at time $t$, how does the policy learn to imitate actions?\n    + Why isn't there any KL term as is common in variational models? Would the $L_{prior}$ be needed if we have the standard KL loss?\n    + The paper claims that $L_{adv}$ eliminates the marginal and conditional type shifts but does not explain how. Two paragraphs later, the paper claims that the conditional shift is not resolved, and thus information bottleneck is needed. Which one is right?\n    + The claimed information bottleneck is not a novel solution and is well-known in the literature.\n- **Experiments (Section 6)**:\n    + Overall, because the paper's contributions are unclear, it is hard to understand what the ablations should be. In the current way of writing, I would imagine that all 4 losses should be ablated, but only \"No Prior Training\" is ablated.\n    + While I appreciate the existing baselines and environments, it's hard to comment on the correctness without understanding the exact contributions of the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality and clarity are subpar; thus, it is hard to judge the originality of the work. In its current form, any proposed metrics or methods do not seem original. Even if it is a valid combination of existing approaches, that would be acceptable. But it is not explained in that way currently. It is still unclear why simple approaches would fail on their proposed setup.",
            "summary_of_the_review": "To the best of my understanding, lack of clarity in writing, unjustified claims, and supposed lack of novelty are my reasons for recommending rejection.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4518/Reviewer_4XDc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4518/Reviewer_4XDc"
        ]
    }
]