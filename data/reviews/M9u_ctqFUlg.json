[
    {
        "id": "jqkjD-h6EVk",
        "original": null,
        "number": 1,
        "cdate": 1666369442246,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666369442246,
        "tmdate": 1666370463941,
        "tddate": null,
        "forum": "M9u_ctqFUlg",
        "replyto": "M9u_ctqFUlg",
        "invitation": "ICLR.cc/2023/Conference/Paper1549/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "**Potential double-blindness violation**\n\nThe source codes, scripts, and license files in the supplementary materials have the copyright notice of NVIDIA. It is not unnatural if only some of them have it since they may be third-party codes with the third-party being NVIDIA, but in this case, as far as I checked, all the files have it. I don't know if this would be regarded as a violation of double-blindness but report here just in case.\n\nBy the way, posting something independently from \"Official Review\" is not enabled at this stage, which I found a bit inconvenient for reporting this kind of thing.\n\n---\n\n**Summary of the paper**\n\nThe authors propose a regularization of learning generative models supposing the presence of an energy function that (hopefully) captures the true distribution's nature to some extent. Their theoretical discussion is twofold. One is about the fact that the regularization does not hurt the estimation as long as the hyperparameter is appropriately chosen and is shown for a prototypical Gaussian fitting example. Another is about the uniqueness of global optimum and probabilistic convergence of SGD to it. The authors show the benefits of the proposed regularization on different image datasets.",
            "strength_and_weaknesses": "### Strengths\n\n- The paper is nicely written and easy to follow.\n- The experimental results are clearly reported and support the claim well.\n- The idea of using a non-generative model's features for defining an energy function is very interesting.\n\n### Weaknesses\n\nThe analyses in Section 3 are not wrong, but the current way of presentation looks a little awkward. What's described here is that the facts that hold without the regularizer also hold with the regularizer under some conditions. In this sense, discussing those conditions in the main text, instead of deferring them to the appendix, would be more natural.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written, and the discussions and the experiments are nicely constructed to support the main claim of the paper. Also, the source code in the supplementary materials seems to be well documented.",
            "summary_of_the_review": "This is a solid paper. While the presentation around the analyses could possibly be improved, otherwise it is nicely written, and the experimental results well support the claim of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1549/Reviewer_5xKN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1549/Reviewer_5xKN"
        ]
    },
    {
        "id": "4rYlxCDpaJ_",
        "original": null,
        "number": 2,
        "cdate": 1666605572141,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605572141,
        "tmdate": 1666605572141,
        "tddate": null,
        "forum": "M9u_ctqFUlg",
        "replyto": "M9u_ctqFUlg",
        "invitation": "ICLR.cc/2023/Conference/Paper1549/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a regularization strategy for training deep generative models such as GANs on limited data. The strategy involves adding an additional term to the objective function for the generator that penalizes the expected mean square error between the features of a generated sample and a real sample, where the features are extracted from a pre-trained model. The authors present some theoretical analysis to motivate their approach and provide comprehensive empirical results to demonstrate its effectiveness.",
            "strength_and_weaknesses": "Strengths: \n\n1) The paper is very well-written and easy to understand.\n2) The authors present theoretical analysis to motivate/justify their approach.\n2) The authors provide comprehensive experiences to demonstrate the effectiveness of their approach. The proposed regularization is applied to three models (StyleGAN, ADA, APA) trained for generation on three datasets (FFHQ, LSUN CAT, CIFAR-10), and provides a modest boost to each of the models. The authors' best model outperforms all the baselines in the main table.\n4) The authors provide ablation studies and connect their proposed approach to other regularization strategies such as entropy minimization and feature matching.\n\nWeaknesses:\n1) There is a gap between the theoretical analysis and the actual method that is tested empirically. The bound of Section 2.1 is limited to the simple Gaussian example. The theoretical results do not provide intuition/justification for using a pretrained feature extractor. \n2) The comparison between the proposed approach and existing regularization strategies such as entropy minimization and feature matching should be emphasized, both in discussion and in results. At the moment, the results for the other two regularization strategies are in the Appendix and the presentation of the results does not easily allow the reader to compare them to the proposed approach. For example, it seems that feature matching achieves better FID than the proposed method, but this is not clear from the presentation.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - very good. Paper is well-written.\nQuality - good. The experimental results seem sound. The theoretical analysis is a nice plus to help motivate the approach.\nNovelty - ok. Feature-wise losses are used in many related tasks, but the authors try to motivate/justify their use here with some theoretical analysis. At first glance, the method seems similar to feature matching, which is why the discussion of the differences is very useful.\nReproducibility - seems straightforward to reproduce.",
            "summary_of_the_review": "The method of regularization (expected feature-wise MSE) is not a novel concept, but the authors motivate their use here with some theoretical analysis and show promising experimental results. I would suggest more clearly discussing the differences between this work and other regularization strategies such as feature matching and entropy minimization, as well as showing the quantitative comparison in the main paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1549/Reviewer_rg8D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1549/Reviewer_rg8D"
        ]
    },
    {
        "id": "PfQZ_WbRK7",
        "original": null,
        "number": 3,
        "cdate": 1666614689750,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614689750,
        "tmdate": 1670762509850,
        "tddate": null,
        "forum": "M9u_ctqFUlg",
        "replyto": "M9u_ctqFUlg",
        "invitation": "ICLR.cc/2023/Conference/Paper1549/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "To address the limited data in generative modeling, this paper proposes a regularized deep generative model (Reg-DGM) to reduce the variance and prevent over-fitting. There are two main contributions of this paper: 1) proposed a regularized deep generative model based on pre-trained model with an energy function. 2) Experiments on three main datasets and serval base model demonstrate that RegDGM improves the generation performance and outperform the SOTA methods.",
            "strength_and_weaknesses": "Strength:\nThe paper contributes some new ideas. It provides an alternative way compared to fine-tuning-based methods to help generative modeling in limited data. Experiments with Reg-StyleGAN2, Reg-ADA, and Reg-ADA-APA show the proposed method outperforms the corresponding base models. \n\nWeaknesses:\nIt is not clear why incorporating pre-trained models as regularization can improve generative modeling. It is said that this can reduce variance. However, it will also introduce bias. Does any pretrained models always help?\n\nFurthermore, it is claimed in the paper that the paper focuses on the limited data setting. However, I don't see any data settings described in the experiments. In the appendix, it has some descriptions on how to split the training data, but it is too rough to know whether they use the original whole dataset? If so, I am not sure if it is on the setting of \"limited data\". In my opinion, limited data means few-shots. So it should at least have some studies on the performance w.r.t. the number of training data.\n\nIn addition, to show the effectiveness of the proposed method, existing methods to deal with limited data should be compared, e.g., the methods discussed in the Related Work. The current results do not have these comparisons, thus it is difficult to claim the proposed method is a better choice.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The presentation is generally clear. \nQuality: The quality of this paper is fair. \nNovelty: The idea is simple and intuitive, but has limited technical novelty. \nReproducibility: Code for some experiments is attached.",
            "summary_of_the_review": "This paper proposes a simple regularized deep generative model by leveraging pretrained models to deal with limited training data. The experiments demonstrate the effectiveness of the method on some datasets compared with the unregularized versions. However, the motivation is not well demonstrated, and not enough evidence is shown to demonstrated the effectiveness of the proposed method.\n\nPost rebuttal:\nThanks for the clarifications. Most of my concerns are addressed, and I am happy to raise my score to 6. The reason of not raising to higher scores is that I think the proposed method is somewhat restricted. I think it improves the generative model only if the distribution of the pretrained models has a distribution similar enough to the training data, which is hard to guarantee in practice. In addition, the paper only shows successful cases, it would be great to see some failure cases to see how much worse a bad-pretrained model can bring to the generative model.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1549/Reviewer_ZXuR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1549/Reviewer_ZXuR"
        ]
    },
    {
        "id": "eJyn0A4vgY",
        "original": null,
        "number": 4,
        "cdate": 1667106825987,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667106825987,
        "tmdate": 1667107999284,
        "tddate": null,
        "forum": "M9u_ctqFUlg",
        "replyto": "M9u_ctqFUlg",
        "invitation": "ICLR.cc/2023/Conference/Paper1549/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies generative modeling under a limited data regime and proposes a regularized generative model by utilizing a pre-trained model from an external dataset. The paper presented some theoretical justification and analysis of the proposed approach from the bias-variance trade-off perspective. Experiments on image generation are conducted in comparison to several baseline methods, using GANs as the backbone generative model, showing that the proposed model can achieve better performance with the help of the proposed regularization approach.",
            "strength_and_weaknesses": "- This paper is generally well-written and the overall structure is clear. The motivation is clearly stated.\n- The authors suggest looking at the problem and proposed model from the bias-variance trade-off perspective. However, the presented theoretical analysis does not seem to be very intuitive in explaining why and how this perspective would help. To put it in another way, bringing another pre-trained model from a larger-scale dataset into the training of a generative model under a limited data regime is not a very surprising idea in the first place, which makes the synthetic study and the theoretical analysis a bit obsolete.\n- Why does the author emphasize the role of an energy-based formulation of $f$ rather than a regularization term, which is pretty much a perceptual loss in previous works? What does the distribution induced by the EBM imply in the context of the proposed model and its training process?\n- The exact description of how the limited data is prepared is not very clear, as well as how the metrics are computed. For example, are the FID scores computed against the original whole training set or in the limited dataset?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see above comments.",
            "summary_of_the_review": "This work studies the generative models under a limited data regime which is interesting, some parts including theoretical analysis and evaluation can be improved to make the contributions clearer in terms of soundness and novelty.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1549/Reviewer_rAFU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1549/Reviewer_rAFU"
        ]
    }
]