[
    {
        "id": "0ojrtdrjyN",
        "original": null,
        "number": 1,
        "cdate": 1666603229629,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603229629,
        "tmdate": 1666603229629,
        "tddate": null,
        "forum": "bHW9njOSON",
        "replyto": "bHW9njOSON",
        "invitation": "ICLR.cc/2023/Conference/Paper646/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new method to improve calibration of multi-class classifiers. It is similar to the existing methods of MMCE and SB-ECE in the sense that it adds a trainable auxiliary loss function (Expected Squared Difference, ESD) to the cross-entropy loss (NLL) during training of the classifier. Similarly to SB-ECE, the paper uses interleaved training where the new auxiliary loss and NLL are trained on separate parts of the training set. ESD is parameter-free and its estimator is unbiased. The experiments on 5 datasets demonstrate that compared to MMCE, SB-ECE and the baseline NLL it results in better-calibrated predictions (lower ECE) both before and after post-hoc calibration with temperature scaling or vector scaling. About 1% of accuracy is sacrificed compared to the baseline NLL.",
            "strength_and_weaknesses": "Strengths:\n* The paper is well-written and clear.\n* The proposed method ESD has been justified theoretically and the ESD estimator has been proved to be unbiased and consistent.\n* The experiments on 5 datasets demonstrate that ESD results in better calibrated probabilities than earlier methods, while not sacrificing more accuracy.\n\n\nWeaknesses:\n* The intuition behind the proposed method could have been explained more, perhaps on a toy example.\n\n* More information about the experimental results could have been reported, for example as an appendix. It would be good to know whether the range of values for the hyperparameter lambda is sufficient and which values were eventually chosen by the method on different datasets. Also, it would have been good to report NLL as an evaluation measure, in addition to accuracy and ECE. E.g., Table 1 could have had extra rows for NLL, NLL after TS, and NLL after VS, or alternatively, this information could have been provided in the appendix. The paper does not mention why accuracy after vector scaling is so low for CIFAR100 and ANLI. Can there really be so much overfitting even though the number of classes is quite high?\n\n* The conclusion fails to mention that some accuracy has been sacrificed to achieve better calibration. On the positive side, this fact has been mentioned in Section 6 about experimental results.\n\n* Notation $k$ is overloaded in Section 3 - first it is the total number of classes, then just any class in Eq.(1), and then the class with maximum output probability in Eq.(2). This can create confusion, in particular because Eq.(1) is about being classwise-calibrated whereas later the paper only studies confidence calibration.\n\nMinor weaknesses:\n* Table 3 and Section 7.3 fails to mention which dataset it is about. I assume CIFAR100?\n\n* The last sentence of page 7 (first paragraph of Section 6) states the decrease of accuracy by more than 2% - is it not about 1% in this case (47.08 vs 48.0) because the comparison is all between ESD and ESD+VS?",
            "clarity,_quality,_novelty_and_reproducibility": "All is novel and reproducible and clear, except for the weaknesses highlighted above.",
            "summary_of_the_review": "The paper proposes a parameterless method and improves the state-of-the-art. The proposed method ESD is simpler and better than the existing methods MMCE and SB-ECE. There are some weaknesses highlighted above but these can be fixed for the camera-ready v\nersion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper646/Reviewer_ywx4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper646/Reviewer_ywx4"
        ]
    },
    {
        "id": "QuMmJbNreY",
        "original": null,
        "number": 2,
        "cdate": 1666625237751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625237751,
        "tmdate": 1670287262819,
        "tddate": null,
        "forum": "bHW9njOSON",
        "replyto": "bHW9njOSON",
        "invitation": "ICLR.cc/2023/Conference/Paper646/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper present Expected Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable calibration objective loss, which views the calibration error fromthe perspective of the squared difference between two expectations.",
            "strength_and_weaknesses": "Strengths:\n1. Build a well-established  trainable calibration objective loss ESD, which is binning-free and does not need hyperparameters. \n2. Extensive experiments to prove that the esd produces more accurate ECE without tuning the hyperparameters.\nWeaknesses:\n1. This calibration method itself does not need any hyper-parameter but still incorporates some other hyperparameter, such as the regularizer hyperparameter \u03bb, monte carlo sample number N, etc. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity&Quality: this paper is well-written.\nNovelty: notable novelty\nreproducibility: Settings and parameters are listed and the authors committed to release the code publicly",
            "summary_of_the_review": "This paper proposes a hyperparameter-free calibration method, ESD and produces accurate predictions and calibration results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper646/Reviewer_smEk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper646/Reviewer_smEk"
        ]
    },
    {
        "id": "8YONtaf8pGm",
        "original": null,
        "number": 3,
        "cdate": 1666664406217,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664406217,
        "tmdate": 1666664406217,
        "tddate": null,
        "forum": "bHW9njOSON",
        "replyto": "bHW9njOSON",
        "invitation": "ICLR.cc/2023/Conference/Paper646/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a hyperparameter free training based calibration method using proposed calibration metric, namely expected squared difference (ESD). The proposed calibration metric is inspired from the recently introduced KS metric that compares cumulative distribution.",
            "strength_and_weaknesses": "Strengths:\n- The paper is easy to follow and well-written.\n- There has been a lot of research focused on post-hoc calibration methods in the literature but this paper looks upon the solving the mis-calibration at the training time without significant additional computational costs. This problem is particularly challenging as the overfitting is a significant challenge to solve this task. \n- The paper achieves impressive results on multiple benchmarks of calibration while proposing a theoretically concrete approach to calibration.\n\nWeaknesses:\n- Since this is a training based calibration method, I would suggest the authors add calibration results on large scale datasets such as ImageNet. \n- An important addition to the paper, should be along the lines of calibration performance under distribution shifts [a] as this has been known to be difficult for post-hoc calibration methods but I would assume the proposed method should be able to perform better under such a scenario.\n\nReferences:\n- Ovadia et al. Can You Trust Your Model\u2019s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift. NeurIPS 2019.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well written.\nQuality: Empirical and theoretically solid.\nNovelty: The proposed method of calibration is novel especially in terms of its application at train time.\nReproducibility: The authors have promised to release code publicly.",
            "summary_of_the_review": "I am in favor of acceptance of the paper as its a novel way of calibration and proposed method looks clean. I suggest some few additions to the paper which will make it stronger. If my concerns are addressed, I will increase my rating.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper646/Reviewer_XMyj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper646/Reviewer_XMyj"
        ]
    },
    {
        "id": "b1motHOB5tB",
        "original": null,
        "number": 4,
        "cdate": 1666676103974,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676103974,
        "tmdate": 1670546352389,
        "tddate": null,
        "forum": "bHW9njOSON",
        "replyto": "bHW9njOSON",
        "invitation": "ICLR.cc/2023/Conference/Paper646/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a trainable objective less ESD as the extra calibration loss jointly optimized with the NLL loss during training. It is claimed as a binning-free objective lease without need to tune any additional parameters. The experiments were extensively conducted across three architectures (MLPs, CNNs, and Transformers) and both vision and NLP datasets. The experimental results seem to strongly validate the efficacy of the proposed method.",
            "strength_and_weaknesses": "#### Strengths\n- Overall the paper is well written and easy to follow.\n- The idea of ESD is very interesting and should be able to bring a positive impact to the AI fields not limited to both vision and NLP domain.\n- Strong theoretic explanation with solid mathematical functions. \n\n#### Weaknesses\n- The accuracy has no obvious advantages when compared to other competing baselines, according to Table 1.\n- As the paper proposes a general calibration loss, I would like to suggest the authors to conduct the experiments on the more general large-scale vision datasets (e.g., MS-COCO and ImageNet).",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: overall this article is well-organized and easy to follow. \n- Quality and novelty: the paper is solid and the novelty is intermediate.\n- Reproducibility: The authors promise to release  the code.",
            "summary_of_the_review": "Based on the above statements, I would like to weakly accept the paper at the initial stage. After carefully reading the other reviewers' comments and the authors' response, I would like to keep the initial rating.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper646/Reviewer_md7e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper646/Reviewer_md7e"
        ]
    }
]