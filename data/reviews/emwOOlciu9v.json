[
    {
        "id": "uvm7TUeEr4",
        "original": null,
        "number": 1,
        "cdate": 1666155318653,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666155318653,
        "tmdate": 1666155318653,
        "tddate": null,
        "forum": "emwOOlciu9v",
        "replyto": "emwOOlciu9v",
        "invitation": "ICLR.cc/2023/Conference/Paper3737/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the role of data augmentation in contrastive learning is studied, especially in terms of whether label-preserving augmentation is useful or not. First the authors investigated the empirical evidence using a Viewmaker network (Tamkin et al., 2021b), which automatically finds effective augmentation. The results show that the augmentation that changes label information makes NNs more robust and generalizable to many downstream tasks. To explain this, a theoretical analysis is presented. ",
            "strength_and_weaknesses": "Strengths\n1. Overall the paper is well written.\n1. Motivation is clear and the problem this paper addresses is important.\n\nWeaknesses\n1. The novelty is somehow limited. I recognized that the main contributions are two-fold: (a) Empirically finding that label-disturbing augmentation is useful, contrary to common opinion. (b) Theoretical analysis of that. Although both are nice, this paper doesn't provide a concrete outcome (new method, new algorithm, etc). \n1. I have several concerns for the theoretical part. \n- The problem considered in 4.1, to estimate the correlation between u and v can be analytically solved by the eigenvalue decomposition of the covariance $cov[u, v]=E[uv^\\top]$. For simplicity, let K=1. The empirical covariance $\\Sigma = 1/n \\sum_i u_i v_i^\\top $ is then converges to $\\alpha \\mu\\mu^\\top$ for large n, and $\\mu$ is given as the largest eigenvector of $\\Sigma$ where $\\alpha$ is the eigenvalue. Similarly, the correlation btw u and $\\tilde{v}$ is given as the eigenvector of $cov[u, \\tilde{v}]$. In this case, the eigenvector is the same $\\mu$, but the eigenvalue is decreased from $\\alpha$ to $\\beta\\alpha$ ($\\beta \\le 1$). The scale of eigenvalue determines the strength of true signal (i.e. correlation) and larger eigenvalue makes the estimation problem easier. So the noisy problem ($cov[u, \\tilde{v}]$) is more difficult to obtain an accurate solution. This observation contradicts what Theorem 4.1 suggests. Why does this happen? Did I overlook something?\n  - Note that for $K>1$ we can have the same idea by thinking $K$ eigenvalue decomposition problems.\n- As mentioned above, the noisy problem should be vulnerable to data noise. However, the speech results (table 2) show the radical augmentation makes NNs robust to noise. Why this is possible?\n- Although the theoretical part is intriguing, it is written a bit dense. Also, any intuitions behind Theorem 4.1 are not explained. Since the formal proof is over 10 pages, I really want some proof sketch in the main paper. \n- It's better to contain experiments that verify the theorem. ",
            "clarity,_quality,_novelty_and_reproducibility": "As I wrote above, clarity is high except the theory part (Section 4).\n\nI suspect the quality of Theorem 1. The empirical part looks fine.\n\nI feel the novelty is limited (see Weakness 1).\n\nWhen the code becomes publicly available, the reproducibility should be OK. ",
            "summary_of_the_review": "While the motivation is interesting, the novelty is not significant enough. Also, the theoretical analysis is counterintuitive and needs more careful assessment. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3737/Reviewer_AVvr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3737/Reviewer_AVvr"
        ]
    },
    {
        "id": "52Sr8q091a",
        "original": null,
        "number": 2,
        "cdate": 1666544924225,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666544924225,
        "tmdate": 1666544924225,
        "tddate": null,
        "forum": "emwOOlciu9v",
        "replyto": "emwOOlciu9v",
        "invitation": "ICLR.cc/2023/Conference/Paper3737/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper questions the widespread argument/intuition for the success of contrastive learning that a good augmentation must be label preserving. The paper argues that label-destroying augmentations may be important for contrastive learning to learn diverse general purpose representations. It further hypothesizes that label destroying augmentations serve as some kind of \u201cfeature dropout\u201d since they can prevent one \u201cshortcut feature\u201d from suppressing other features. The following arguments are provided to support the hypothesis:\n- Discussion (in Section 2) about why some standard augmentations are not label-preserving and why invariance to those augmentations is undesirable. E.g. while random gray scaling is an augmentation, we often do not want invariance w.r.t. it since color information can be useful for downstream tasks.\n- Experiments with ViewMaker networks (a recent method to automatically learn augmentations) on hybrid datasets (e.g. CIFAR with superimposed digits) where the learned augmentations are generally better than standard augmentations. The paper observes that despite performing better, ViewMaker has more label-destroying tendencies, as verified with some visualizations of the augmentations and other statistical evidence (questions about this later)\n- A theoretical result with a linear model that tries to capture that idea that adding noise to some features (to simulate label destruction) can speed up the learning of other features (for one gradient step).",
            "strength_and_weaknesses": "**Strengths**\n\n- This paper raises interesting points about how label-destroying augmentations may not be harmful for contrastive learning, questioning the conventional wisdom about contrastive learning and the role of augmentations.\n\n- The experimental findings on the hybrid datasets that ViewMaker networks learn more label-destroying augmentations than \u201cstandard\u201d augmentations but perform slightly (for CIFAR) or much better (for audio), are also very interesting.\n\n- The paper is clearly written and easy to follow for most part\n\n**Weaknesses**\n\nMy main concern is the claim from the abstract that \u201clabel-destroying augmentations are often crucial in the foundation model setting\u201d is not adequately justified. While the paper sufficiently argues why label-destroying augmentations *may not be harmful*, the results in the paper fail to convince me that they are *crucial*.\n\n(W1) The main evidence provided is the ViewMaker experiments, but in my view the findings are not sufficient to conclude that label-destroying augmentations from ViewMaker were \u201ccrucial\u201d for its superior performance.\nThere could be many other reasons why those are better augmentations and it is also possible (in theory) that the label destroying augmentations are somehow even holding it back. I believe that some more experiments, where the only thing that changes is the level/amount of label-destroying augmentations, would be required to make such a conclusion.\n\nOne idea that comes to mind is the following: Run contrastive learning with only those ViewMaker augmentations that do not destroy labels a lot (e.g. pick augmentations that have P(correct) greater than some threshold in Figure 3). If filtering out such augmentations leads to a decline in performance, that would provides evidence that label destroying augmentations were indeed crucial.\nSome such experiment that isolates the effect of the presence or absence of label destroying augmentations, without other potential confounding factors, seems important for the conclusions in the paper.\n\n(W2) Similarly the theoretical result from Section 4 does not characterize the necessity or benefit of label destroying augmentations well enough.\nThe result shows that if noise is added to some feature (to \u201cdestroy\u201d it), then one step of gradient descent will lead to better learning of other features.\nThis result fails to paint a more global picture of what would happen if contrastive learning is run with augmentations that do and do not destroy augmentations/features every now and then.\nWhile interesting, the one step gradient result that is presented is quite intuitive, since at the extreme if one feature is completely destroyed, then other features start becoming more important for the contrastive loss.\nHowever it is not clear to me that the result provides a \u201csimple linear model that captures the essence of how label-destroying augmentations can improve downstream accuracy \u201c, as claimed on page 6.\nSome more comments:\n- Is this result saying that with label-destroying augmentations, the useful features will be learned faster? Because it seems to me (although I do not have a proof) that with or without label-destroying augmentations, all the features should be learned eventually. I'm not sure if any suppression of features will happen without the occasional destruction of features.\n- Even if an end-to-end result cannot be shown, even some simulation experiments with contrastive learning on this example for two augmentation schemes, one that occasionally destroys features while another that doesn\u2019t destroy, might help shed some more light and strengthen the claim that label destroying augmentations can help.\n- Will help to include a proof sketch or at least the key steps in the proof, even if for a special case like \\beta=0, for the reader to get some intuition.\n\n\nOther comments\n\n- In Section 2, I\u2019m not sure if an augmentation \u201capplied repeatedly\u201d is the right way to think of this. A function $f$ might just be invariant on the support of the input distribution and one augmentation applied to those images, and not necessarily on augmentations of augmentations. This will not lead to the \u201ccatastrophic augmentation collisions\u201d that is shown in Appendix B.\n\n- The bimodal distribution in probabilities in Figure 2 is quite interesting. However there might be some confounding factors since the histograms for ViewMaker and Expert augmentations use different network (based on the description in Appendix C).\nTo get rid of confounding factors it would help to plot histogram from both using both networks (so 4 plots instead of 2). Was this image randomly selected? Is the finding true for other images as well?",
            "clarity,_quality,_novelty_and_reproducibility": "The message and experimental findings about ViewMaker networks are original, to the best of my knowledge. The paper is clearly written and easy to follow for most part.",
            "summary_of_the_review": "The message and experimental findings about ViewMaker networks are original, to the best of my knowledge, and overall I find them quite interesting. However I believe that the paper is lacking in execution of the ideas, more precisely in its justification of the *necessity* of label-destroying augmentations. I am thus inclined to assign a score of weak reject, but I would be happy to change the score if there is some more justification for the point about necessity, through experiments or theory.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3737/Reviewer_WzK9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3737/Reviewer_WzK9"
        ]
    },
    {
        "id": "RuFMaHQBDGj",
        "original": null,
        "number": 3,
        "cdate": 1666580254672,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580254672,
        "tmdate": 1666670612778,
        "tddate": null,
        "forum": "emwOOlciu9v",
        "replyto": "emwOOlciu9v",
        "invitation": "ICLR.cc/2023/Conference/Paper3737/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper challenges the common belief in literature in self supervision that augmentations need to be label preserving, in order to perform well in downstream tasks. The authors first show that when using a Viewmaker network to automatically derive good views, the augmentations derived by the model may drop significant information about the label of the downstream task. Using this observation, the authors then demonstrate theoretically that in a simple problem, adding noise to the features of the inputs helps the optimization process to find a better representation.",
            "strength_and_weaknesses": "Strengths:\n- This is an interesting paper, in that it questions a commonly accepted paradigm for self supervised learning, in that the augmentations applied on the inputs need to be carefully selected, so as not to affect the downstream task. Since this field is overall novel, I believe that challenging commonly accepted views is important for the field as a whole.\n\n- The authors also make a convincing argument regarding the use of feature destroying augmentations, in the setting of multitask learning. According to the authors, while previous works only consider the setting of a single downstream task, they argue that in order for a model to perform well in a multitude of different downstream tasks, no single feature should be prioritized over the others. This is an interesting argument for the case of the use of a variety of augmentations, whether they are label preserving or not.\n\n- The experiments provided by the authors demonstrate that the use of Viewmaker networks to automatically extract (possibly label destroying) views show benefits over the use of expert augmentations, which is an argument for their case that invariance is not necessarily required.\n\nWeaknesses:\n- I believe that the motivating examples behind the arguments made by the authors in Section 2 can be improved. In particular, the authors argue that the brightness augmentation used in practice is not label preserving, while this may only be true in extreme cases. Furthermore, I also believe that their motivating argument would be cleaner if they described hue shifts and greyscaling the same way they describe brightness augmentations, in that they may not be label preserving despite their widespread use. I also think that the fact that an augmentation may be invariant for a given task and not for a different one should be stressed here.\n\n- The authors use Viewmaker networks to automatically extract augmentations, and make their central observation that augmentations need not be label preserving based on this. While this is a good argument, I am not sure if it is adequately supported. From the qualitative side, Figure 1 claims that Viewmaker views tend to destroy labels for one of the tasks. A similar argument can be made for the expert views in some cases (particularly in row 4, where the expert views corrupt the input greatly). From the quantitative side, the authors analyze how the predictive capabilities of a classifier are affected by the Viewmaker and the expert augmentations. However, this is done using a single sample of the data, and deriving several views from it. I believe that this should be done with a multitude of the samples of the dataset, in order to avoid any possible bias arising from this sample.\n\n- The authors provide a good theoretical argument on the use of noise on a given feature, and how doing so actually improves the learning of other features. However, in the setting that the authors describe, I find it slightly weird that neither of the two views of the samples that we consider contain by themselves any information about the directions $\\mu_k$ that we want to learn. This information is rather incorporated into their joint distribution, and thus it seems more natural to me to consider them both as a single sample, instead of two views of the same object. This makes the theoretical example somewhat disjoint from the rest of the work, if I understand correctly. Moreover, the theoretical result states that if a feature is corrupted, then we can learn the rest of the features well. I think it is also useful to know in the main theorem how much we need to corrupt that feature to achieve this (thus, what is the total effect on the alignment of our learned vectors with the ground truth).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is clear and easy to follow. I have the following suggestions to make about the clarity of the paper:\n\n- I believe that expanding upon the motivating example in Section 2, as stated above, is useful in order to improve the reader\u2019s understanding of the motivation behind the paper.\n- Regarding the theoretical result, I believe the explanation on the addition of the noise before Theorem 4.1 should be included in the Theorem itself, in order for the expression of the corrupted feature to be clearer to the reader.\n\nWith respect to the novelty, as stated above I believe that the paper provides a novel interpretation on the necessity of invariance across augmentations. I consider this interpretation to be original.\n\nThe authors provide details in order to reproduce their experiments, and plan to release their codebase.",
            "summary_of_the_review": "Overall, I think this is an interesting paper, due to the fact that it presents a novel interpretation on the necessity of invariance in the augmentations. I lean towards acceptance due to the interesting aspects of the above, however I also believe that there are a few issues in the paper that need to be addressed, with respect to how the quantitative evaluation of invariance is performed and how the theoretical result is linked to the rest of the paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3737/Reviewer_mMWc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3737/Reviewer_mMWc"
        ]
    },
    {
        "id": "JrfNjzCJvp",
        "original": null,
        "number": 4,
        "cdate": 1666643010560,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643010560,
        "tmdate": 1666643010560,
        "tddate": null,
        "forum": "emwOOlciu9v",
        "replyto": "emwOOlciu9v",
        "invitation": "ICLR.cc/2023/Conference/Paper3737/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper addresses the question, what role does data augmentation play in (contrastive) self-supervised learning? As the authors write they complicate the simplistic view that data augmentation implements invariances: \"We complicate this picture by showing that label-destroying augmentations are often crucial in the foundation model setting, where the goal is to learn diverse, general-purpose representations for multiple downstream tasks.\"\n\nRather the authors show that data augmentation can lead to a more balanced representation of features (essentially by feature level dropout), so that a broader set of down stream tasks is served.\n\nThe evidence presented is experimental, indicating that a combined (ensemble) approach of augmentation interventions lead to better general down stream generalizability. A theoretical analysis of SGD with data augmentation (added noise) complements the experimental evidence.\n",
            "strength_and_weaknesses": "Strengths:\nRather convincing experiments are presented supporting the hypothesis that data augmentation leads to more balanced feature weights.\n\nWeaknesses: While carefully supported by creative designed experiments the idea is not entirely new. In fact work by Ericsson et al. [1] has pointed to a quite similar conclusion - this work is not cited in the present contribution.\n\n[1] Ericsson, L., Gouk, H. and Hospedales, T.M., 2021. Why do self-supervised models transfer? investigating the impact of invariance on downstream tasks. arXiv preprint arXiv:2111.11398.\n\nThe paper's theorem is relatively weak evidence (many conditions, weak conclusion)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, with careful experiments and a relevant theorem (with a very detailed proof in supplement).\n\n",
            "summary_of_the_review": "Clear hypothesis with strong experimental evidence, supported by somewhat weaker theory.\n\nIn rebuttal please related the present hypothesis to the prior work of Ericsson et al.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3737/Reviewer_Xou4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3737/Reviewer_Xou4"
        ]
    }
]