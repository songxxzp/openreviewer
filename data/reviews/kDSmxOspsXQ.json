[
    {
        "id": "AhWvKP2-2sY",
        "original": null,
        "number": 1,
        "cdate": 1666348715703,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666348715703,
        "tmdate": 1666688249694,
        "tddate": null,
        "forum": "kDSmxOspsXQ",
        "replyto": "kDSmxOspsXQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4240/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Message-passing neural networks (MPNNs) and their extensions (Subgraph MPNNs) have inspired state-of-the-art models for graph data; however, their counting power is still limited and it is known that Subgraph GNNs cannot count cycles with more than $4$ nodes.\n\nTo boost the counting power, this paper extends the idea of having a single root node identifier in Subgraph GNNs to two identifiers for a (root node, neighbour) pair resulting in the proposed I$^2$-GNN model.\n\nI$^2$-GNN has the following properties:\n1. It can count all cycles of length at most $6$,\n2. It has linear space and time complexity provided the node degree is bounded,\n3. It achieves competitive results on open benchmarks compared to existing models.\n\n___",
            "strength_and_weaknesses": "\\\n**Strengths**\n\n\\+ The paper is well-organised, well-written, and the problem of counting power is well-motivated.\n\n\\+ The proposed method is potentially scalable to large-scale real-world data and potentially useful for real-world chemical data involving cycles of length at most $6$ (e.g., benzene rings).\n\n\\+ Under certain conditions (e.g., bounded node degree), the proposed model is the first linear-time GNN model to count $6$-cycles.\n\n\n**Weakness**  \n\nThe only major weakness of the paper is the lack of positioning/comparison of the idea of using multiple identifiers for node pairs (of edges) with labelling tricks [1] (used in the context of link prediction)\n\n[1] Labelling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning, In NeurIPS'21\n\n___",
            "clarity,_quality,_novelty_and_reproducibility": "\\\n**Clarity**\n\nThe paper is generally clearly written.\n\nExplaining what the accuracies mean in Table 1 and their extreme differences ($0\\%$ vs. $100\\%$) in Table 1 would improve the clarity (especially for non-expert readers).\n\n\\\n**Quality**\n\nThe paper is technically sound and both the theory and the experiments support the claims.\n\nSince the complexity depends on the node degrees, an inference time and number of parameter comparison on real data (e.g., datasets of tables 4 and 5) would strengthen the quality of the paper.\n\nSince benzene rings were used as an example real-world scenario of counting power, performing experiments (or highlighting if already performed) on real data with benzene rings would also strengthen the soundness of the claims.\n\n\n\\\n**Novelty**\n\nThe proposed model is the first linear-time GNN model to count $6$-cycles.\n\nThe novelty of the work can be further strengthened by positioning the contributions with respect to GNNs for link prediction which also exploit subgraphs around each edge.\n\nA discussion of the differences between the expressivity of labelling trick vs. I$^2$-GNNs would further strengthen the contributions.\n\n1. Labelling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning, In NeurIPS'21\n2. Link Prediction Based on Graph Neural Networks, In NeurIPS'18\n\n\n\n\\\n**Reproducibility**\n\nThe main part and the supplementary part include enough material, e.g., proofs of theorems, dataset details, baselines with references, hyperparameters, ablation study, for an expert to replicate the results of the paper.\n\n___",
            "summary_of_the_review": "While the paper is well-motivated and the claims are well-supported with theory and experiments, positioning with respect to relevant prior work can further strengthen the contributions.\n___",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_Yf1m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_Yf1m"
        ]
    },
    {
        "id": "wpaHfhnh1T",
        "original": null,
        "number": 2,
        "cdate": 1666642372388,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642372388,
        "tmdate": 1666642372388,
        "tddate": null,
        "forum": "kDSmxOspsXQ",
        "replyto": "kDSmxOspsXQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4240/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors of this paper propose a new Subgraph MPNN architecture, called $I^2$-GNNs, that can count up to 6-cycles in contrast to other Subgraph MPNNs which cannot count such long cycles. For each rooted subgraph centered at node $i$, they construct d(i) subgraphs, where d(i) is the degree of the center node, defining a mapping between the generated subgraphs and the neighbors of the root node. Then, they assign a unique identifier to the center node $i$, and another unique identifier to one of the neighbors of the center node, for each generated subgraph. They use MPNNs in each subgraph to construct the graph's final representation, followed by a readout layer. They evaluate their model in cycle counting tasks as well as in molecular property prediction benchmarks, achieving competitive performance.\n",
            "strength_and_weaknesses": "Strengths: \n- The authors of the paper present some new theoretical results regarding Subgraph MPNNs. Specifically, they prove that the proposed models can count 3-cycles and 4-cycles, but cannot count 5-cycles or any longer cycles.\n\n- The authors also present a new Subgraph MPNN architecture that is more powerful than existing Subgraph MPNNs. They also prove that it can count all cycles with length less than 7 which is important for real-world applications (e.g., identifying benzene rings).\n\n- The proposed models achieve competitive performance in graph classification and regressions tasks (QM9, ZINC and ogbg-molhiv) and demonstrate increased expressiveness in discriminating non-isomorphic graphs and in substructure counting.\n\nWeaknesses: \n- My main concern with this paper is with regards to the novelty of the work, as in essence, it proposes to add an extra identifier for each pair of nodes. There are many works that add unique node identifiers in GNNs to increase the expressive power of the models [1,2] and in the current paper, a proper discussion of these approaches is missing. Also, a comparison with [3,4] would be interesting, as they also build more expressive GNNs with extra node features.\n\n- The proposed model generates a new subgraph for each edge in the original graph, and therefore the computational complexity is higher than that of other subgraph MPNNs. As the authors mentioned, the proposed model is evaluated only on graphs with a small average node degree. I would suggest the authors discuss how the proposed approach can be applied to graphs with high average node degree and also evaluate their model on such graphs, for example, in the TUD benchmark [5]. \n\n[1] Andreas Loukas. How hard is to distinguish graphs with graph neural networks? In Advances in Neural Information and Processing Systems (NeurIPS), 2020.\\\n[2] Andreas Loukas. What graph neural networks cannot learn: depth vs width. In ICLR, 2020.\\\n[3] Dwivedi, Vijay Prakash, et al. \"Graph neural networks with learnable structural and positional representations.\" arXiv preprint arXiv:2110.07875 (2021).\\\n[4] Wijesinghe, Asiri, and Qing Wang. \"A New Perspective on\" How Graph Neural Networks Go Beyond Weisfeiler-Lehman?\".\" International Conference on Learning Representations. 2021.\\\n[5] Morris, Christopher, et al. \"Tudataset: A collection of benchmark datasets for learning with graphs.\" arXiv preprint arXiv:2007.08663 (2020).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well written, and the idea is clear and well presented.\\\nQuality: The quality of the paper is high.\\\nNovelty: The paper has some novelty, as it enhances Subgraph MPNNs with a new strategy of node pair identifiers, but the general concept of node identifiers in GNNs has been studied in previous works. Also, a more detailed related work, discussing these approaches is missing.\\\nReproducibility: The experimental setup is well described and also the source code is provided by the authors. \n",
            "summary_of_the_review": "The proposed method is technically sound, well written, and leads to performance gains on some molecular property prediction datasets. However, I think that the main idea of incorporating identity node features has been studied in the past thoroughly and the paper does not discuss these methods in details. Specifically, I would suggest the authors do the following:\n- Discuss more and extend the related work with other similar approaches that incorporate extra node features/identifiers and explain why the current approach is more suitable than the others.\n- Compare with more baselines and evaluate the model in more benchmarks (see above).\n- Explain how the current approach can be applied to dense graphs, where the average node degree is high, probably by employing some sampling strategies.\n\nI would be happy to increase my score if my concerns are well addressed. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_Mx4a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_Mx4a"
        ]
    },
    {
        "id": "Sr9PkPjeou",
        "original": null,
        "number": 3,
        "cdate": 1666730752718,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666730752718,
        "tmdate": 1666730752718,
        "tddate": null,
        "forum": "kDSmxOspsXQ",
        "replyto": "kDSmxOspsXQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4240/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new Graph Neural Network architecture enhancing the standard message passing architectures (MPNN). The main idea is to run various MPNNs on rooted subgraphs of the original graph. The authors show that their architecture I2-GNN is more powerful than previous MPNN. They show theoretical results where I2-GNN is able to count longer cycles (3,4, 5 or 6-cycles) and paths (3 or 4-paths). They also show some theoretical limits of I2-GNN. In their last section, they conduct experiments to substantiate their claims.",
            "strength_and_weaknesses": "The theoretical analysis made in this paper is rather convincing.\nThe main weakness is the special focus on counting (short) cycles and paths. The expressive power of MPNN can be improved by adding extra-features like it is done in this paper but it is not clear that focusing on cycles and paths is actually the best use of resources. Indeed, the experimental results on real datasets are quite disappointing in this respect as CIN seems to outperform the I2-GNN on ZINC and ogbg-molhiv.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. If possible, I would recommend to move the discussion about the complexities of the various architecture in the main part of the paper.\nThe code is provided but I did not run it.",
            "summary_of_the_review": "The paper presents interesting theoretical results but experimental results are weak.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_GzCp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_GzCp"
        ]
    },
    {
        "id": "-hvSqGrUVy",
        "original": null,
        "number": 4,
        "cdate": 1666967684494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666967684494,
        "tmdate": 1666967684494,
        "tddate": null,
        "forum": "kDSmxOspsXQ",
        "replyto": "kDSmxOspsXQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4240/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Message Passing Neural Network (MPNN) is a simple yet efficient class of Graph Neural Networks (GNNs), which have been widely adopted in many previous works. However, it is also known that MPNN has limited representational power due to its simple structure. This paper, in particular, considers subgraph methods. The authors first show that Subgraph MPNNs can count 3-cycles and 4-cycles, but cannot count 5-cycle or any longer cycles at the node level. The cycle counting is important to understand structure since cycles are the basic elements constructing some important substructures. Then, they propose I^2-GNN that can count cycles with lengths less than 7. Based on this, I^2-GNN has a stronger discriminative power than subgraph MPNNs. I^2-GNN outperforms baselines in molecular prediction benchmarks.",
            "strength_and_weaknesses": "Strengths\n\n- The proposed algorithm $I^2$-GNN theoretically guarantees the power of counting 6-cycles. This is the first algorithm that can guarantee such power. \n\n- The algorithm's empirical performances support the theoretical results.\n\n\nWeakness\n\n- The computational complexity of $I^2$-GNN is proportional to the degree of the graph. So, when the degree follows a power law (a heavy tail distribution), the complexity could be much higher than before.\n\n- The empirical study considers QM9 and ZINC. This could be a limited set. It would nice to consider node OGB data set and classification tasks with cora, citeceer, pubmed. ",
            "clarity,_quality,_novelty_and_reproducibility": "The algorithm is novel, and the proof looks correct, although I couldn't check all the details.",
            "summary_of_the_review": "This paper has a novel method and its theoretical motivation. However, the experiment section is weak.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_YNBY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_YNBY"
        ]
    },
    {
        "id": "loGbWpN_jFO",
        "original": null,
        "number": 5,
        "cdate": 1667163253830,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667163253830,
        "tmdate": 1667163253830,
        "tddate": null,
        "forum": "kDSmxOspsXQ",
        "replyto": "kDSmxOspsXQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4240/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies the counting ability of subgraph MPNN models, and present I^2-GNN to increase the counting ability of subgraph MPNNs. The proposed I^2-GNN is able to discriminate 6-cycles in theory and achieves good performance in molecular property prediction.",
            "strength_and_weaknesses": "Strengths:  \n(+) This work makes very solid and novel contributions to developing more expressive graph neural network models. The idea of improving the subgraph counting ability by assigning different identifiers to the root node is interesting and elegant.  \n(+) Comprehensive theoretical analysis and empirical studies are presented to show the effectiveness of the proposed I^2-GNN model.  \n(+) The writing of this paper is very clear and well-organized.  \n\nWeaknesses:  \n(-) Some experimental settings are unclear to me in graph substructure counting. For experiments in section 6.2, the prediction targets are cycle or graphlet counts, which are discrete numbers. Do GNN models output continuous numbers to approximate discrete targets and the MAE is evaluated between them?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper clearly clarifyies the novelty of the proposed method. The quality of this work is solid and strong in the aspects of both theory and experiments. Source codes and detailed experiment settings in the appendix are provided to ensure good reproducibility.",
            "summary_of_the_review": "Overall, this work proposes a novel method in enhancing the representation ability and counting power and of graph neural networks. Both the theorical and experimental contributions of this work are strong. I vote for accepting this work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_FzhU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4240/Reviewer_FzhU"
        ]
    }
]