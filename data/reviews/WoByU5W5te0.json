[
    {
        "id": "ydVhsCIx-Wy",
        "original": null,
        "number": 1,
        "cdate": 1665667412202,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665667412202,
        "tmdate": 1668665290420,
        "tddate": null,
        "forum": "WoByU5W5te0",
        "replyto": "WoByU5W5te0",
        "invitation": "ICLR.cc/2023/Conference/Paper765/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focus on the novel view synthesis with only sparse inputs. The authors propose a method called GeCo-NeRF that enforces geometric consistency to regularize the training of NeRF. Specifically, input images are warped to unseen viewpoints for supervision at feature level. They also filter out erroneous warped patches and engineer several training strategies to improve training. Experiments on both synthetic and real data demonstrated the effectiveness of GeCo-NeRF.",
            "strength_and_weaknesses": "Strength:\n- The results are quite promising and achieve the state-of-the-art in the few-shot NeRF setting\n- The authors put a lot of engineering efforts to make the warping technique work\n\nWeaknesses:\n- The method is not novel enough for me. Many methods propose to supervise at unseen viewpoints, e.g. DietNeRF.  Also, in [1], input image color was used to supervise unseen viewpoints at pixel level.\n- The authors doesn't discuss the computational cost of the proposed method. In the paper, they report that 8 hours is needed to train 120k iterations with 2 RTX 3090Ti. As far as I know, training vanilla NeRF for 200k iterations only need 8 hours on a single 1080Ti.\n- Unless I am missing something, ablation is not provided for the training strategy in sec 4.5, which I think is part of the core contribution of this paper\n- Some references can be incorporated and discussed: concurrent work [2], [3] also uses estimated depth to warp input images to other camera poses to regularize NeRF (This point doesn't influence my rating, but including them would make this paper more complete)\n- It seems that I cannot find the mentioned supplementary material\n\nMinor: \n- some references are not compiled successfully, as can be seen in sec 5.1\n- In \"Plugging in NeRF'' in sec 5.3, table 3 should be table 1\n\n[1] Ray Priors through Reprojection: Improving Neural Radiance Fields for Novel View Extrapolation\n\n[2] Data augmentation for NeRF: a geometric consistent solution based on view morphing\n\n[3] StructNeRF: Neural Radiance Fields for Indoor Scenes with Structural Hints",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clearly demonstrated its contribution. The results presented both qualitatively and quantitatively outperforms baselines. \n\nThe paper should be reproduced with some efforts according to the details provided. Due to the several training strategies proposed, it might be hard to achieve the same results as in the paper. Therefore, it would be great if the code can be open-sourced after publication.",
            "summary_of_the_review": "Overall, I think this paper falls into the borderline. The techniques are not novel enough and some details are missing, but I appreciate the engineering effort and the results are quite promising. Therefore, I lean towards acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper765/Reviewer_Copi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper765/Reviewer_Copi"
        ]
    },
    {
        "id": "gQvhrVJWHtx",
        "original": null,
        "number": 2,
        "cdate": 1666486822317,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666486822317,
        "tmdate": 1669452847133,
        "tddate": null,
        "forum": "WoByU5W5te0",
        "replyto": "WoByU5W5te0",
        "invitation": "ICLR.cc/2023/Conference/Paper765/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method that regularize nerf in a few-shot setting. The method utilize NeRF rendered depth as pseudo ground truth to warp rgb value to other near view. The error between source view and target view are taken as an additional loss to train NeRF in a few shot setting.",
            "strength_and_weaknesses": "Strengths:\n\n1. A novel way to generate pseudo ground truth by warpping multiple input views to the novel view location. The local features are aligned using NeRF generated depth map by an occlusion-aware mask map.\n\n2. Instead of vanilla NeRF only using pixel-level reconstruction loss, the paper proposes to add multi-level perceptual loss between wrapped pseudo ground truth image and generated novel view image. By adding such constrain at the feature level, the framework improves quality of generated novel views by a large margin in comparison to the baselines.\n\nWeaknesses:\n\n1. The novelty is not significant. The main contribution of this paper is the depth rewarping loss. However, using depth to for few-shot nerf is not new [1][2]. Also, warping input to unseen views is widely used in self-supervised MVS [3][4].\n\n2. It is not convincing that depth estimation (Eq. 4) is accurate enough with few viewpoints. Also, the resolution could be limited given the fact that the depth is generated with grid sampling. \n\n3. It is weird to mention Monocular depth estimation in related work, which may be self-supervised photometric consistency.\n\n4. There are a lot of missing citations. \n\n[1] Depth-supervised NeRF: Fewer views and faster training for free\n\n[2] Dense depth priors for neural radiance fields from sparse input views. \n\n[a] M3vsnet: Unsupervised multi-metric multi-view stereo network \n\n[b] Learning Unsupervised Multi-View Stereopsis via Robust Photometric Consistency",
            "clarity,_quality,_novelty_and_reproducibility": "In terms of writing, the paper is organized, and the overall flow is clear. However, there are a few typos in the paper. For example, in Figure 1 illustration, it should be NeRF instead of NerF. Some citations with question marks (?) in the paper. \n\nSome idea needs more clarification, for example, in section 2, \"... the density\u2019s entropy in each ray and ensures consistency across rays in the neighborhood. However, the performance of these methods was still limited\". It should be discussed in-depth that why their performance is limited, as these methods are direct competitors to the proposed approach. In section 4.4, quote \u201cAs explained above, we do not \u2026 generating Ml\u201d. I think the authors try to explain that they don\u2019t apply masks directly to the image pairs, but to the feature map level. However, the sentence needs to be reorganized. \n",
            "summary_of_the_review": "Based on the weakness discussed above, I think the paper has limited novelty, and there is also some defacts in terms of technical and writing issues. Therefore, I vote for rejection at this stage. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper765/Reviewer_8ZEs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper765/Reviewer_8ZEs"
        ]
    },
    {
        "id": "RUYWTUMKnw",
        "original": null,
        "number": 3,
        "cdate": 1666685741677,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685741677,
        "tmdate": 1666685741677,
        "tddate": null,
        "forum": "WoByU5W5te0",
        "replyto": "WoByU5W5te0",
        "invitation": "ICLR.cc/2023/Conference/Paper765/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces a geometric regularization technique for nerf training under a sparse view setting. The regularization is based on cross-view warping between seen/unseen views using rendered depth, where the warped image is utilized for supervision in feature space.\n\n",
            "strength_and_weaknesses": "Pros:\n\n1. The overall idea is tidy and easy to follow\n\n2. Experiment results seem promising\n\nCons:\n\n1. Some important references which address the multi-view photo-consistency are missing, e.g., MVSDF, NeuralWarp, and GeuNeuS. Specifically, MVSDF also applies feature consistency to constrain the neural rendering optimization.\n\n2. Following Cons 1, I would expect a comparison between the proposed method with the explicit multi-view consistency formulation in MVSDF/NeuralWarp/GeuNeuS. (maybe an ablation study plus some explanations). From my perspective, they are all some kind of similar photoconsistency but implemented in different ways.\n\n3. There are several wrong citations in the [Datasets and metrics] section.",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "See above. I would like to hear the author's feedback on the missing references and I would raise my rating if my concerns could be well-addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper765/Reviewer_m8Jd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper765/Reviewer_m8Jd"
        ]
    },
    {
        "id": "M6gtsr8eeWz",
        "original": null,
        "number": 4,
        "cdate": 1667127222956,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667127222956,
        "tmdate": 1667127222956,
        "tddate": null,
        "forum": "WoByU5W5te0",
        "replyto": "WoByU5W5te0",
        "invitation": "ICLR.cc/2023/Conference/Paper765/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method to enable only sparse image set as an input to a NeRF network and enable rendering of artifact free novel views which are typically not possible using current existing methods. Their main contribution is to warp given input images to novel camera locations given the current estimate of the depth at novel view and minimize the error between the warped image and the NeRF rendered image at that novel viewpoint. The warped image is assumed to be a ground truth at the novel view-point. Since warping across translated views can lead to occlusion effects, a best estimate of occlusion mask is also calculated and used to mask out the cost error regions. The loss function is designed to be minimizing feature maps at multiple scales instead of direct pixel intensity difference because the pixel intensities can be different in reality if the scene contains non-Lambertian surfaces. The results are show to be better than standard and existing approaches.",
            "strength_and_weaknesses": "Strength:\n* The paper addresses a very relevant area of NeRF application to real world where capturing a scene with many images maybe impossible due to scene motion or having a 100-camera multi-camera system to capture a scene is economically and practically prohibitive.\n* The paper is well written and all aspects are well discussed.\n* The results look good and show improvement over existing works.\n\nWeakness:\n* No major weakness except a few \"?\" in the paper which can be resolved.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and addresses a relevant area of NeRF application.",
            "summary_of_the_review": "Please see Strengths above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper765/Reviewer_w2pC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper765/Reviewer_w2pC"
        ]
    }
]