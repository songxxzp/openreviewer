[
    {
        "id": "idJQ7xrjjlr",
        "original": null,
        "number": 1,
        "cdate": 1666279465616,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666279465616,
        "tmdate": 1666279465616,
        "tddate": null,
        "forum": "aCGIa6GfbmN",
        "replyto": "aCGIa6GfbmN",
        "invitation": "ICLR.cc/2023/Conference/Paper1171/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates GNNs' expressive power. That is, building on the work of Xu et al. (2019) and Morris et al. (2019) it proposes injective and continuous aggregating functions used within GNNs. Specifically, the paper argues that both injectiveness, as well as continuity, is needed to compute meaningful graph and node representations. To that, the paper proposes a simple injective and continuous function for multisets by building on results by Zaheer et al. (2017). Moreover, they derive sufficient conditions for injectivity. \n\nThe theoretical results are complemented with an empirical study, showcasing the benefits of the new aggregation function on a number of standard benchmark datasets. ",
            "strength_and_weaknesses": "**Strength**\n- Paper is well presented and easy-to-read\n- Good empirical results, good experimental study \n- Theory seems, to some extent, align with experimental results  \n\n**Weaknesses**\n- Not clear enough, if existing works already solve the problem of infectivity and continuity; see below \n- Some weaknesses in the experimental protocol; see below \n\n**Questions**\n- Why is the function considered by Xu et al. (2019) not continuous? To the best of my knowledge, it is a composition of continuous functions. \n\n\n**Suggestions/Remarks**:\n- The simple layer proposed by Morris et al. (2019) is already injective *and* continuous.\n- You assume a finite subset of $\\mathbb{R}^d$. Hence, it is countable and can be handled by the approaches of Morris et al. (2019) or Xu et al. (2019) \n- Make more clear that you deal with multiset functions and not set functions\n- In the abstract, you claim \"to improve the expressive power of GNN\". However, existing works, e.g., Xu et al. (2019), are already maximal expressive.\n- Page 2: Corso et al. (2020) does not increase the expressivity of GNNs\n- In the experiments, it would beneficial to also include larger datasets from OGB or TUDataset with continuous node features. The used datasets are rather small \n- The choice of reporting validations set performance is questionable even if other papers have done so in the past.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the source code is provided in the appendix. Moreover, all datasets are publicly available, ensuring reproducibility. \n\nI have some doubts about the novelty; see above. For example, the simple layer proposed by Morris et al. (2019) is already injective *and* continuous. The proof of Theorem 2 is rather straightforwardly based on known results. ",
            "summary_of_the_review": "This is a well-written paper with somewhat interesting theoretical results and a decent experimental study. I have some doubts about the novelty of the theoretical results. That is, prior works already seem to solve the problem of infectivity and continuity.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1171/Reviewer_EYLv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1171/Reviewer_EYLv"
        ]
    },
    {
        "id": "5HEL11WR30x",
        "original": null,
        "number": 2,
        "cdate": 1666476683512,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666476683512,
        "tmdate": 1666476683512,
        "tddate": null,
        "forum": "aCGIa6GfbmN",
        "replyto": "aCGIa6GfbmN",
        "invitation": "ICLR.cc/2023/Conference/Paper1171/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an aggregation framework (set representation) which can lead to both injective and continuous GNN node embedding even if raw node features are from conintuous space rather than countable space.",
            "strength_and_weaknesses": "## Strength\n\n- This paper shows that there is no way to continuously embed a $M$-size set whose elements are from $R^d$ into less than $Md$ dimension with injectivity. I think formally claiming this is useful.\n\n## Weakness\n\nThe significance of this paper is limited.\n\n- This paper proposes an injective and continuous set representation in Eq (5) for $M$-size set with $R^d$ elements, and the final representation is $Md$ dimension. However, if we simply sort all elements in the set, and concatenate sorted elements together as a $Md$ vector, I believe this is also an injective and continuous set representation. Then, is there any significant benefit to use Eq (5)?\n- From the experiment results, it seems that iceGNN-fixed is not feasible or worse than iceGNN-MLP in half of the cases, and improves slightly than iceGNN-MLP in the remaining half. It seems that injectivity is not very important in practice, but it is a key factor used to blame other models, e.g., GIN.\n- Your iceGNN-fixed looks like a slight variant of EdgeConv which you can easily find in PyTorch Geometric library, thus I think the novelty of this is limited. Besides, it will be better to have GAT results.\n- It will be a lot better if you can provide theory to study how dimension reduction hurts the injectivity and continuity since feature reduction is very common in real world applications.\n\nSome statements are not precise.\n\n- In the theorem 2, the definition of capital $D$ is missing. It is neither provided in appendix. Indeed, in the proof, the value of $D$ is different for different cases.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and clarity is mostly good.\nHowever, the novelty and originality is limited.",
            "summary_of_the_review": "This paper focues on both injectivity of continuity of GNN (set) representations on continuous space which is lack of formal studies.\nAlthough this paper provides an interesting finding of requirement for such representation, the other parts including proposed methods and experinmental results does not give enough contribution.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1171/Reviewer_84dJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1171/Reviewer_84dJ"
        ]
    },
    {
        "id": "gQeyuTLEvy7",
        "original": null,
        "number": 3,
        "cdate": 1666512321799,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666512321799,
        "tmdate": 1666512321799,
        "tddate": null,
        "forum": "aCGIa6GfbmN",
        "replyto": "aCGIa6GfbmN",
        "invitation": "ICLR.cc/2023/Conference/Paper1171/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper identifies the problem of non-injective functions when the dimensions of intermedia representation vectors are not enough. It proposes a new expressive GNN model with an emphasis on the injectivity and continuity of set functions. ",
            "strength_and_weaknesses": "Strengths:\n1. The theoretical analysis of the GNN and the injective functions are built on solid foundations,\n2. This paper gives a practical method to construct injective aggregation functions.\n3. The presentation logic is clear.\n\nWeaknesses:\n1. The experiments on node classification are not comprehensive enough. The three citation network datasets are rather toys, and the baselines are limited. More experiments on more datasets and baselines are expected. (Minor: Typo in Table4, ICGNN->iceGNN?)\n2. My main concern is about the novelty. Considering the learnable variant of the aggregate function designed in this paper, it seems very similar to the original solution in [1], which resorts to an MLP (the universal approximation theorem) to model $f \\circ \\phi$ to get injective $\\phi$.\n\n[1] How Powerful are Graph Neural Networks?, Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, ICLR 2019",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is presented clearly, providing a practical method to construct injective aggregation functions along with its theoretical analysis. \nAs I mentioned in the above weaknesses, I have a bit of concern about its novelty. The presented learnable variant of the aggregation function is similar to the design in [1].\n",
            "summary_of_the_review": " This paper provides a practical method to construct injective aggregation functions and solid theoretical analysis. However, I have concerns about the novelty of the proposed solution. I may raise my rating if my concern is resolved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1171/Reviewer_eUaW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1171/Reviewer_eUaW"
        ]
    },
    {
        "id": "2Z-pw7qsvg",
        "original": null,
        "number": 4,
        "cdate": 1666560507769,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560507769,
        "tmdate": 1666560507769,
        "tddate": null,
        "forum": "aCGIa6GfbmN",
        "replyto": "aCGIa6GfbmN",
        "invitation": "ICLR.cc/2023/Conference/Paper1171/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work tackles the problem of using a injective and continuous set function as a neighborhood aggregation scheme in GNNs. The authors prove that such a construction exists (by extending some results from DeepSets) and how it would look like in GNNs. Finally, they present the practical benefits of such design in a set of benchmark tasks for graph learning.",
            "strength_and_weaknesses": "---Strength---\na) The problem tackled is relevant and few previous works have tried to address it.\nb) As far as I was able to follow and check, all proofs and theoretical contributions hold.\nc) The paper is well written and clarity is a strength.\n\n\n---Weaknesses---\na) I'm not sure to what extent the changes in the functions proposed by the authors impact their own results, i.e. the functions in 4.2 vs the ones from the theorems. If it doesn't impact, which I'm pretty confident it's not true, then the authors should present the theory matching them. If it doesn't at least we would need to evaluate what's then the actual benefit of using their architecture.\nb) The datasets used to evaluate the method are not sufficient to understand whether it outperforms existing solutions or not. We would need to see the method compared against more baselines, e.g. recent higher-order GNNs, in larger datasets, e.g. open graph benchmark (OGB).",
            "clarity,_quality,_novelty_and_reproducibility": "I really appreciate the authors' intellectual honesty in pointing the limitations of their results throughout the text ---which really helps with clarity. Overall, the main text is well written and easy to understand.  Code is available, which makes it as reproducible as we can ask from the authors.\n\nSome minor comments:\n\na) Please, use proper punctuation when writing equations, e.g. periods or commas.\nb) Please, add hyperparam search and choices for each architecture in the appendix.\nc) Please, avoid inline text and sentences such as \"it's easy to see from Lemma X [...]\" in the proofs. It gets harder to read and some of the comments tend to not be mathematically precise. The more self-contained your work is, the better.",
            "summary_of_the_review": "At this point in the graph learning literature it's hard to accept a paper evaluated only on the TUDataset, Planetoid and ZINC benchmkarks. These were used five years ago and the field has progressed since then. I recommend that the authors take a look at OGB and compare against the baselines that are highly ranked there. Further, I recommend the authors elaborate a result for the gap between their theory and their architecture. My recommendation is thus a result of these weaknesses outweighing the paper's strength.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1171/Reviewer_Bsrk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1171/Reviewer_Bsrk"
        ]
    }
]