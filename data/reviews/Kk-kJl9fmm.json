[
    {
        "id": "oXweZYI5W6V",
        "original": null,
        "number": 1,
        "cdate": 1666588362373,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588362373,
        "tmdate": 1668762369371,
        "tddate": null,
        "forum": "Kk-kJl9fmm",
        "replyto": "Kk-kJl9fmm",
        "invitation": "ICLR.cc/2023/Conference/Paper5177/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work aims to provide a unified Thompson-sampling based algorithm for linear bandits. It provides nearly-matching upper and lower bounds, and also discusses the results in some specific cases: misspecified linear bandits, non-stationary linear bandits with bounded switches, non-stationary linear bandits with bounded path length, and lifelong linear bandits with shared representation.",
            "strength_and_weaknesses": "Strength:\n1. The paper is generally easy to follow and well-organized.\n2. The literature review is detailed.\n\nWeaknesses:\n1. In section 4.3, the comparison with FGTS mentions that \"the original FGTS samples over models (or equivalently, reward functions) rather than policies\". However, a more vital question is: what is the corresponding analytical challenge?\n2. Since Algorithm 1 is not computationally efficient and even hard to implement, what are the possible solutions? Any thoughts?\n3. In section 5, when discussing the results in the four variations of linear bandits, the contribution of this work would be clearer if the author(s) could provide a comparison between the existing bounds and those provided by this work.\n4. Small typo: page 6 --- before Definition 4.2: should be \"we define the approach of $\\epsilon$-net and covering number.\"",
            "clarity,_quality,_novelty_and_reproducibility": "This work does not include experiments, and I think the quality would be improved if the weaknesses mentioned above could be fixed.",
            "summary_of_the_review": "Generally speaking, this work tries to deepen the study of feel-good Thompson-sampling algorithm proposed in Zhang (2021) and it does make some contribution to the field. I would appreciate that if the weaknesses above could be solved.\n\n\n=========\n\nAfter rebuttal:\nThanks for your efforts. I increased the score to 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5177/Reviewer_iGav"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5177/Reviewer_iGav"
        ]
    },
    {
        "id": "ypnAXr2Z1J3",
        "original": null,
        "number": 2,
        "cdate": 1666655579278,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655579278,
        "tmdate": 1666655579278,
        "tddate": null,
        "forum": "Kk-kJl9fmm",
        "replyto": "Kk-kJl9fmm",
        "invitation": "ICLR.cc/2023/Conference/Paper5177/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors use Feel Good Thompson sampling to derive general bounds for three different linear contextual bandits 1) non-stationary bandits with S switches, 2) non-stationary bandits with path-length P and 3) lifelong bandits over M tasks. In each case they show that their general bound is close to optimal of the lower bound (nearly minimax for 1 and 2 but 2 has a bigger gap between upper and lower bounds). ",
            "strength_and_weaknesses": "The algorithm proposes a general framework that then is able to be close to optimal for the three cases that are shown. They introduce an algorithm that is close to optimal for the lifelong case.\n\nThe main drawback is that it would be been good to see some simulations results of the methods to see if even in toy examples, these bounds hold in simulation and that also gives a good visual of the constants involved in the algorithm.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear, the have made references to related work and the overall presentation of the work is good.",
            "summary_of_the_review": "Overall it is a good paper that gives a near-optimal algorithm for the lifelong learning setting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5177/Reviewer_oQHX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5177/Reviewer_oQHX"
        ]
    },
    {
        "id": "-4WqzVBoSD",
        "original": null,
        "number": 3,
        "cdate": 1666761803146,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666761803146,
        "tmdate": 1666762590688,
        "tddate": null,
        "forum": "Kk-kJl9fmm",
        "replyto": "Kk-kJl9fmm",
        "invitation": "ICLR.cc/2023/Conference/Paper5177/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a framework that covers several variants of linear contextual bandits, and proposes a variant of feel-good Thompson sampling introduced in Zhang (2021). The theoretical results show that the proposed algorithm has (near-)optimal regret bounds for various settings of linear contextual bandits.",
            "strength_and_weaknesses": "The main strength of the paper is that the proposed algorithm has (near-)optimal regret bounds for several variants of linear contextual bandits.\n\nThe main weakness is that the proposed algorithm is (almost) unimplementable. I could not see a practical way to implement this algorithm.  Besides this , I have several questions and comments.\n1. The paper claims resolving the respective open problem in each setting of linear contextual bandits. I think it is better to clearly describe these open problems.\n2. Why $\\pi$ is called (meta-)policy? To me, it is the whole model of $T$ periods.\n3. It seems that the proposed algorithm does not need to know the parameters including the misspecification level, the bound of number of switches denoted by $S$ and the bound of the path length denoted by $P$, right? If my understanding is correct, could the authors elaborate more on how the analysis of the algorithm deals with these unknown parameters?\n4. In terms of the algorithm design, why at each period, the algorithm samples the whole model of $T$ periods? I mean at each period $t$, why we need to obtain samples of $\\theta$'s before $t$? I think this is the main step that makes the algorithm not implementable.\n5. In terms of the regret bound in Theorem 5.1, how sensitive is this bound to the choices of $\\lambda$ and $\\beta$? The conditions of $\\lambda$ and $\\beta$ are in the form of $\\Theta$. What are the potential ranges of the hidden constants?\n6. The analysis follows and extends that in Zhang (2021) based on the notion of Decoupling Coefficient (DC). After Zhang (2021), Dylan et al. (2022) introduces a very similar notion called Decision-Estimation Coefficient (DEC) and proposes a general algorithm based on DEC. However, their algorithm's regret bound even for linear bandits is suboptimal in $d$. Could the authors elaborate why the analysis based on DC gives the optimal dependence in $d$, while that based on DEC does not?\n\nDylan J. Foster, Sham M. Kakade, Jian Qian, and Alexander Rakhlin, The Statistical Complexity of Interactive Decision Making, 2022\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is a follow-up work to Zhang (2021). It is not hard to read the paper. Since the proposed algorithm is (almost) unimplementable, there is no any empirical result.",
            "summary_of_the_review": "My main concern is about the practicality of the proposed algorithm. For example, how to implement even an approximation of the proposed algorithm.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5177/Reviewer_vpXj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5177/Reviewer_vpXj"
        ]
    }
]