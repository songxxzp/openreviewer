[
    {
        "id": "rLwKqq7Zpn",
        "original": null,
        "number": 1,
        "cdate": 1666393072796,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666393072796,
        "tmdate": 1666393072796,
        "tddate": null,
        "forum": "bjCAHZLoKy",
        "replyto": "bjCAHZLoKy",
        "invitation": "ICLR.cc/2023/Conference/Paper4641/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to solve the model editing problem via training prefix prompts and using them together with a frozen language model for inference. The proposed approach, SEPROG, is less computationally heady and requires less memory than gradient-based approaches. SEPROG outperforms state-of-the-art methods by 20% on entailed edit benchmarks and provides up to 30% better performance over gradient-based methods on non-entailed benchmarks.",
            "strength_and_weaknesses": "### Strength\n- The approach is simple and easy to follow. The method visualization is clear. \n- I like that the authors did a thorough analysis to understand the training and inference efficiency of the baselines and the proposed approach. This is extremely useful for understanding the practicability of the approaches.\n\n### Weaknesses\n- The writing is not very clear, and I have a bunch of questions throughout reading the paper.\n    - The concept of entailed predictions is explained in the experiment section, but the concept starts to be used in the introduction. It causes a bit of misunderstanding. \n    - The authors should provide better justifications for the selection of the base models. Is it because different tasks require different base encoders? Or simply for a fair comparison to previous works? And why can we not simply use one structure for the framework? The necessity of using different kinds of encoders for different tasks hurts the generalization ability of the approach. \n    - Why is performance so sensitive to batch size? Does one batch always contain examples of one edit description or could be any in-scope and out-of-scope examples? Why are the proposed approach and baselines behave differently for Copy-edit datasets and Entail-edit datasets in terms of a change of batch size?\n    - Table 2 caption is confusing. Is the average performance averaged over both datasets and batch size, or averaged over datasets on one batch size?\n- The results in Table 2 and Table 3 show that the approach is comparable to SERAC in terms of performance and training/inference cost but not significantly better. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper could benefit a lot from more editing. Details are missing or confusing to readers (see weaknesses section). \n- The idea is novel and easy to follow, it should not be hard to reproduce the results from my experience. ",
            "summary_of_the_review": "The proposed idea to address the model editing problem is easy and simple to follow. The experimental results are not compelling enough compared to existing approaches. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4641/Reviewer_k5Q7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4641/Reviewer_k5Q7"
        ]
    },
    {
        "id": "MXvkkBJWdCQ",
        "original": null,
        "number": 2,
        "cdate": 1666717180424,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717180424,
        "tmdate": 1666717180424,
        "tddate": null,
        "forum": "bjCAHZLoKy",
        "replyto": "bjCAHZLoKy",
        "invitation": "ICLR.cc/2023/Conference/Paper4641/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a method for model editing, how to locally update the output of the model to exhibit desirable properties.\nIt builds on SERAC, which consists of three parts:  cache edits, an edit scope classifier, and a counterfactual model that overrides the base language model. A scope classifier determines when the output of the base language model will be in scope, and a counterfactual model determines what should be edited.\u00a0\nThis paper argues that having separate counterfactual and base models is inefficient, since we want to share knowledge between them.\u00a0\nTo address this problem, this paper proposes to use the prompted base model as the \u201ccounterfactual model\u201d.\u00a0\nThe main claim of the paper is that doing so will achieve better generalization on hard edits.\nUnfortunately, the experiments do not support the claim.\u00a0",
            "strength_and_weaknesses": "Strength\n- Model editing is an important and high-impact problem, which is the focus of this paper.\u00a0\n\n- The use of prompted language models as counterfactual models is an interesting research direction. \u00a0\n\nWeakness\n\n\n- This is a minor extension or a special case of SERAC and there\u2019s no overwhelming evidence that this is better than the original model. (See table 2 & 3)\n\n\n- The writing can be improved. The paper assumes the reader has detailed knowledge of SERAC and the entire literature on model editing. In order to understand what this paper is about, I have read the SERAC paper.\nIn order to reach a broader audience, the authors should motivate the model editing problem in the introduction and then clearly explain SERAC and its limitations in the second part.\n\n\nSome other specific examples related clarity.\n\n- The sentence\u201c Good parts of the above strategies while minimizing its limitations\u201d is vague: what\u00a0 are the good parts, and what are the limitations? How is this paper addressing the limitations?\n\n- How is this a \u201csemi-parametric\u2019 model? This was never defined?\n\n- Figure 2 and 3 are near impossible to read\n\n- Table 3 might be interpreted too generously. I would not say that SEPROG is twice as fast as SERAC looking at it. The table also only reports the results for batch size 64, and not the others.\u00a0",
            "clarity,_quality,_novelty_and_reproducibility": "see above",
            "summary_of_the_review": "This paper presents an extension to SERAC, a model editing algorithm.  Unfortunately, the extension is neither theoretically justified nor empirically significant.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4641/Reviewer_8Z3Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4641/Reviewer_8Z3Z"
        ]
    },
    {
        "id": "TpKSiiaHL7",
        "original": null,
        "number": 3,
        "cdate": 1667026710927,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667026710927,
        "tmdate": 1667026710927,
        "tddate": null,
        "forum": "bjCAHZLoKy",
        "replyto": "bjCAHZLoKy",
        "invitation": "ICLR.cc/2023/Conference/Paper4641/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a semi-parametric approach which trains neural network to recognize in-scope and out-scope data, and generating prompts that enables editing the response of the base model. The result is on par with state-of-the-art in most cases, while the training time and inference memory seem better.",
            "strength_and_weaknesses": "The proposed method is intuitive and utilizes the power of neural networks to generate good prompts. However, the performance is often not better than SERAC, as shown in Table 2. The success of the end-to-end system depends on the correct prediction from the edit classifier module, as well as the effectiveness of the prompt output decoder, which seems quite challenging to me. I expect that to be effective, the SEPROG modules require additional training every time when we want to edit a new set of facts, which makes it less flexible. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-written and easy to follow. The method lacks novelty, as it simply uses neural networks to predict in-scope and out-scope data and generating the prompts, and I feel difficult to draw insights from the results. ",
            "summary_of_the_review": "I feel model editing is an interesting problem to study, but the proposed method lack novelty, and the result did not show significant improvements over existing approaches. I look forward to the improvements in the next version.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4641/Reviewer_EG39"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4641/Reviewer_EG39"
        ]
    }
]