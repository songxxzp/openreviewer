[
    {
        "id": "bOhDGnO5Mr",
        "original": null,
        "number": 1,
        "cdate": 1666477227091,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666477227091,
        "tmdate": 1666639540609,
        "tddate": null,
        "forum": "Nn-7OXvqmSW",
        "replyto": "Nn-7OXvqmSW",
        "invitation": "ICLR.cc/2023/Conference/Paper2596/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies a proposed VCReg loss and its relation to pairwise independence of features.\n\nTheoretically, the paper equates the covariance term in VCReg loss to HSIC, for both arbitrary nonlinear point-wise function and sufficiently wide random linear transformation.\n\nEmpirically, the paper studies how properties of projection head affect pairwise independence. The results suggest that shallower and wider projectors should be used, and that a random projector suffices if we are only concerned about pairwise independence. Moreover, since VCReg enforces pairwise independence which is sufficient for linear ICA, the paper finds that VCReg is competitive with specialized algorithms such as Fast ICA.\n\nA major concern though is that it is unclear why pairwise independence is an interesting property to study, since the paper finds that it does not correlates well with downstream performance.",
            "strength_and_weaknesses": "The paper is centered around pairwise independence of the features. I find the paper clearly structured overall, and there is sufficient discussion on related works.\n\nHowever, I'm unconvinced why this is a property of interests or desirable. My current impression is that fully collapsing is certainly not good, but neither is full independence:\n- Empirical findings show that more independence does not always mean better accuracy.\n       - For example, random projectors usually get lower HSIC than learned projectors, but learned projectors get better accuracy.\n- dHSIC plateaus during training, which suggests that there may be a mismatch between the SSL objective and pairwise independence.\n\nThat said, pairwise independence may be useful for model selection, if there is a clear correlation between HSIC and test accuracy.\n- The empirical evidence is Figure 2 (top right), which I currently find questionable: how many replicates/runs are in each dot of the plot? If there's only a single run, then the trends may be due to variances in random seeds.\n\nA clarification question about Lem 1: There seems to be a discrepancy of taking $L=1$ in the proof, and taking $L$ to be large when approximating the random kernel. How does the proof handle $L \\neq 1$? I also find the notations confusing: what does $1-I_D$ mean (i.e. subtracting a matrix from a number)? Is $\\otimes$ here denoting the tensor product?\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Novelty**: it has been realized by previous papers that objectives such as Barlow Twins, W-MSE, VICReg all enforces pairwise independence. The findings in this paper is therefore unsurprising, though this paper is the first to formally characterize the precise relation between the training objective (i.e. the covariance term) and the independence measure (i.e. HSIC). However, I do not consider this as sufficient novelty.\n\n**Reproducibility**: setup details are provided in Appendix E.\n\n**Clarity**: The paper is clearly structured overall, though many writing details could be improved.\n- below eq (4): what is \"null dHSIC\"?\n- below eq (5): the first term in eq (5) does not \"regularize the variance to be unit\", but \"to be at least unit\".\n- Lem 2: from the lemma statement itself, it's unclear what's the relationship between $g$ and $W$.\n- Figure 2: please consider changing to colors with more contrast; for example, currently it's hard to distinguish \"Learned MLP (l=3)\" vs \"Learned MLP (l=3) + no BN\".\n- Sec 5.3: $M, Y$ have dimension mismatch.\n- Please consider some notation changes.\n    - dHSIC: $X_D$ vs $\\mathbf{X}_D$: one is a random var (a vector), the other is a sample matrix?\n    - $K$ is used to denote both a kernel (e.g. in Lem 1) and a dimension (e.g. in Lem 2's proof).\n- Please proofread for typos and grammar mistakes. \n\n",
            "summary_of_the_review": "This paper is centered around pairwise independence, however, currently it's unclear to me why this is an interesting property to study for representation learning, especially since the empirical results already show that there is no clear correlation between HSIC and downstream accuracy.\n\nThe paper is clearly structured overall, but many details could be improved for better readability.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_EpBg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_EpBg"
        ]
    },
    {
        "id": "WG2bkx4dBgU",
        "original": null,
        "number": 2,
        "cdate": 1666558901750,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558901750,
        "tmdate": 1666558901750,
        "tddate": null,
        "forum": "Nn-7OXvqmSW",
        "replyto": "Nn-7OXvqmSW",
        "invitation": "ICLR.cc/2023/Conference/Paper2596/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "\nSummary:\nThis paper provides some explanations for regularization in self-supervised learning algorithms and, to some extent, other phenomena beyond self-supervised learning. Its analysis is reasonable and comprehensive. However, this analysis does not yield very useful insights into the improvement of self-supervised learning or supervised learning. It can neither fundamentally explain why existing self-supervised learning works nor provide effective guidance on how to improve the efficiency of self-supervised learning in the future. Overall, this is an article on the borderline, leaning toward being accepted.\n",
            "strength_and_weaknesses": "\n\n(Positive) The success or failure of self-supervised learning is still a mystery. This article attempts to provide some explanation for the regularization involved, which is highly encouraged.\n\n\n(Positive) The paper demonstrates that regularizing the projector's output helps to encourage independence between the projector's input features. This is very interesting.\n\n\n(Positive) The article also analyzes that the depth and width of the projector also affect the independence of its input features. This is also included as part of the theoretical framework presented in this paper.\n\n(Positive) The method in this paper is very simple and clean.\n\n(Negative) The theory in this paper (including the central limit law) relies on many random projections. But this does not fit perfectly with both self-supervised learning and supervised learning, whose parameters are to be learned. This creates a huge gap between its theoretical and practical needs.\n\n\n(Negative) The most worrying thing is that low HSIC does not bring high test accuracy. For example, it is demonstrated in the paper that both random projection and resampling reduce HSIC (Figure 2). But what's the use of this? We cannot improve test accuracy with these strategies. Although the authors say, \"the correlation is true within the same method, see Figure 2 (top right)\", this is misleading. The x-axis is obtained by increasing the width. It is already a consensus that the test accuracy can be improved by increasing the width, which was not first discovered by this article. So, we did not learn much useful knowledge from this paper to help self-supervised learning.\n\n(Negative) The analysis in this paper does not yield very useful insights into the improvement of self-supervised learning or supervised learning. It can neither fundamentally explain why existing self-supervised learning works nor provide effective guidance on how to improve the efficiency of self-supervised learning in the future. \n\n(Negative) From an HSIC perspective, the results in Figure 1 are good. But this is only for HSIC.\n\n(Negative) Learning the projector is not necessary to obtain pairwise independence and vice versa.\n\n(Positive) I am happy to see that modifying Batch Normalization to get closer to a universal kernel in lemma one yields significant test accuracy gains for VICReg over Bardes et al. (2022).\n\n\n(Negative)  The authors said that projector capacity should rather be increased via width than via depth: adding layers can be detrimental to HSIC as seen above but also to test accuracy. Where is the evidence of \"detrimental to the test accuracy\"?\n\n\n(Positive) I am happy to see that \"Learning the projector does not enforce pairwise independence.\"\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe quality and clarity are median. The originality is good.\n",
            "summary_of_the_review": "\nSee \"Summary Of The Paper.\" This article is an above-average article and deserves a weak acceptance.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "\nThere is no ethics concern.\n",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_NhDr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_NhDr"
        ]
    },
    {
        "id": "i7Dyvd8d9m",
        "original": null,
        "number": 3,
        "cdate": 1666638235607,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638235607,
        "tmdate": 1666638235607,
        "tddate": null,
        "forum": "Nn-7OXvqmSW",
        "replyto": "Nn-7OXvqmSW",
        "invitation": "ICLR.cc/2023/Conference/Paper2596/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies VCREG, a particular regularization function used for contrastive representation learning. The purpose of this term is to prevent embedding collapse, where all inputs map to the same embedding (a trivial way to satisfy that embeddings corresponding to different views of the same object should map to the same embedding). In this paper, they model the embedding procedure as operating in two stages: first, an encoder produces a representation X for each input, then X is passed to a neural network g to produce a higher dimensional embedding Z=g(X). The VCREG term aims to enforce 0 covariance between the elements of Z. The main observation of the paper is that 0 covariance between elements of Z can be viewed (approximately) as enforcing independence between the elements of X. ",
            "strength_and_weaknesses": "Strengths: the basic observation is fairly clean and, I believe, correct (though I have not checked the proofs).\n\nWeaknesses:\n\nThe main weakness I see is a lack of motivation. It seems pretty clear a priori that the regularization term is pushing towards a relaxed notion of independence between elements of X. The main contribution of the paper is then an argument that, loosely, this relaxation is not too severe and we can view the regularizer as targeting independence between elements of the representation.\n\nWhat's not clear to me is: why is this independence desirable? What might go wrong with a weaker notion of independence? Does this somehow help us understand when VCREG should or should not be preferred to other techniques for preventing embedding collapse? Without some answer to these questions, it's not clear to me why the technical results here are insightful.\n\nThese questions seem particularly salient given that 1. the experiments show that the learned representations don't actually have elementwise independence (e.g., in figure 1 measured HSIC never gets close to 0), and 2. the existence of arguments that elementwise independence of representations is anyways undesirable---see e.g., https://arxiv.org/abs/2109.03795\n\nClosely related: I was generally unclear about what questions the experiments were meant to answer.\n\nMore minor: the results only hold for a variant of the encoder learning where the function g taking X to Z is random (i.e., not learned). This is clear in the relevant section, but not appropriately flagged in the introduction. Indeed, the authors present empirical results showing that training this map does indeed cause the independence to fail.\n\nI also have some relatively minor problems with the writing, which is not yet fully baked.",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the work is likely technically correct. Modulo some trouble parsing notation, it is clear what the main result is and why it's true.  As far as I can tell, this submission does have supplementary material available, so I can't comment on reproducibility.\n\nWith respect to clarity, I think the paper could use some polish. Illustrative examples:\n\n1. The word \"variable\" is used throughout to mean \"element of the representation X\", but as far as I can tell this isn't made explicit. I was confused for quite a while about what exactly independence was being enforced between.\n\n2. Section 2.2 and section 3 have conflicting definitions for *Z*. The definition of VCREG in section 2.2 actually only makes sense with the section 3 definition of Z. \n\n3. The statement of Theorem 3 omits some apparently key conditions---e.g., presumably the result is only true in the limit of infinite layer width. ",
            "summary_of_the_review": "This paper is not yet ready for publication. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_MKGJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_MKGJ"
        ]
    },
    {
        "id": "naj1OwIeZ0",
        "original": null,
        "number": 4,
        "cdate": 1666696817998,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696817998,
        "tmdate": 1666699702999,
        "tddate": null,
        "forum": "Nn-7OXvqmSW",
        "replyto": "Nn-7OXvqmSW",
        "invitation": "ICLR.cc/2023/Conference/Paper2596/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the VCRed regularization in self-supervised learning algorithms. The authors show that VCReg enforces pairwise independence between the features of the learned representation. Detailed analysis is provided. ",
            "strength_and_weaknesses": "Strengths:\n\n1: This paper studies the VCRed regularization in self-supervised learning algorithms. This problem is interesting and could have a high impact on the community.\n\n2: This paper is well-written and well-organized. The analysis is comprehensive.\n\n\nWeaknesses:\n1. The motivation is unclear to me. Can the study on independence explain why self-supervised learning works or help improve the existing SSL methods?\n\n2. The contributions are limited. The results in this paper do not provide enough useful intuition or knowledge for improving SSL.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: This study is somewhat novel.\n\nQuality: The analysis is comprehensive.\n\nClarity: This paper is well-written and well-organized.\n\nReproducibility: The authors provide the necessary details for reproducibility. The code is not provided.",
            "summary_of_the_review": "This paper studies the VCRed regularization in self-supervised learning algorithms. However, the motivation is not clear and the results are not interesting.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_UspR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_UspR"
        ]
    },
    {
        "id": "oDFJgNN8JM",
        "original": null,
        "number": 5,
        "cdate": 1666819190743,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666819190743,
        "tmdate": 1666819190743,
        "tddate": null,
        "forum": "Nn-7OXvqmSW",
        "replyto": "Nn-7OXvqmSW",
        "invitation": "ICLR.cc/2023/Conference/Paper2596/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to connect an effective regularization technique used in deep learning for visions with a special kind of measuring independence technique through kernel method. This connection would show that the regularization's goal is to suppress correlation of learned representations. Then the authors proceed to validate their claim via experiments. \n\nThe central idea is quite interesting (though I doubt whether that's very new): a random projection would intuitively give you a set of good basis and if the features are not correlated along these basis, then they should not be correlated. I am aware that Andoni, Panigraphy, Valiant, and Zhang's work utilizes a similar idea (and I believe this is well known enough so that probably there are other works doing similar things). The authors are aware of some weakness associated with this approach, e.g., it needs to assume the projection is random and not learned, and it appears that there does not exist any technique to circumvent the problem. \n\nOverall, it feels like a paper that can appear in top venues but it is at the borderline side. ",
            "strength_and_weaknesses": "Strength: The direct connection the authors made between kernel-based independence test and the regularization seems to be quite interesting. Non-trivial amount of analysis is needed to make the connection rigorous. \n\nWeakness: \nIt seems that the analysis relies on the projection is random and there was a remark on that. \n\nAlso, there was a discussion on the fully independence (Eq. 4): I guess I am a bit lost: do we have fully independence if we have pairwise independence under the specialized test (usually the answer is no? but I feel less sure because I dont have too much intuition on the kernel-based test). If we need fully independence, does that mean we need exp(d) samples, which is impossible in practice? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. It seems the authors expect readers have knowledge in both vision dnn and kernel independence test. I know the latter a bit more and wish the authors could have explained the vision part in more details --- the paper is comprehensible. \n\n",
            "summary_of_the_review": "1. it builds a quite interesting connection between deep learning and independence test. \n2. it is around the borderline papers accepted to neurips/icml/iclr (calibration). \n3. while the specific connection is new, the idea of taking advantages of random projections seems to be around for a while. \n4. the projection has to be random to make the analysis work. The authors seem to be aware of the weakness. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_PbKY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2596/Reviewer_PbKY"
        ]
    }
]