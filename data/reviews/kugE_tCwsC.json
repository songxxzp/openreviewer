[
    {
        "id": "XA7wyDhOmq",
        "original": null,
        "number": 1,
        "cdate": 1666451701120,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666451701120,
        "tmdate": 1666474916693,
        "tddate": null,
        "forum": "kugE_tCwsC",
        "replyto": "kugE_tCwsC",
        "invitation": "ICLR.cc/2023/Conference/Paper3668/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper mainly studies a method to evaluate the robustness of c-MARL agents. The main contributions of this paper are using model-based appoach rather than the model-free approach. The most critical benefit of this approach is that the learned dynamic model can be used to generate the next failure state. In addition, the authors also formulate a mixed integer problem to locate the vulnerable agent (which is named as victim-agent in the paper) to inject stronger attacks to damage the whole multi-agent system. ",
            "strength_and_weaknesses": "## Strength\n1. The description of the proposed method is concrete and comparison to the related work is clear.\n2. The experimental design is comparatively fair to demonstrate the effectiveness of the proposed method.\n\n\n## Weaknesses\n1. There are multiple editing errors which should be addressed in the modified version.\n2. The authors claim that in the setting of Dec-POMDP each agent receive an individual reward. However, as far as I know there should be only one team reward. It should be a factual error.\n3. The author also claim that \"The requirement on training an adversarial policy is impractical and expensive compared to learning the dynamics model.\", which has not been clearly verified. For example, how is the sample efficiency between the off-policy methods and the model-based method?\n4. The authors claim that \"In our experiments, we notice that the quality of the dynamics model does not significantly affect the result as seen in Figure 11.\". To me, it is so wierd. I wonder whether learning a dynamic model is a key factor in this method. For example, I wish the authors can show whether a linear model with randomized parameters that characterizes the relationship between state-action pair and next state can still achieve a comparatively good result.\n5. About Eq. (5), if I understand it correctly, the $W_{i}$ obtained is only for ranking the victim-agents and the state perturbations are not penalised with $W_{i}$. In other words, the coefficient prior to each perturbation is either 0 or 1. If it is correct, this is inconsisitent with the formulation in Eq. (4). I wish the authors can give more explanation for it.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of this paper is generally good, but the writing of this paper should be improved. For example, it is better to give a very brief outline before moving to the details in Section 3.\n\nThe originality (or novelty) of this work is comparatively minor, since it just combines the techniques in adversarial attacks and model-based RL together. I agree that this is an improvement but not a major breakthrough work, since everything seems unsurprising.\n\nThe reproducibility of this work is good, since the authors provide detailed experimental setups and pseudo codes are clear.\n\nOverall, the quality of this work is not good enough.",
            "summary_of_the_review": "This paper describes a methodology to attack c-MARL with model-based approach, but there are still multiple concerns about the proposed approach which need to be addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety",
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "This paper studies the methodology of attacking the cooperative multi-agent reinforcement learning algorithms, which could lead to some potentially harmful insights and security issues though they claim that this is for evaluating the robustness of algorithms.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_SrTs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_SrTs"
        ]
    },
    {
        "id": "LMtrZpjJYv5",
        "original": null,
        "number": 2,
        "cdate": 1666672312721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672312721,
        "tmdate": 1666672312721,
        "tddate": null,
        "forum": "kugE_tCwsC",
        "replyto": "kugE_tCwsC",
        "invitation": "ICLR.cc/2023/Conference/Paper3668/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method that attack cooperative multi-agent reinforcement learning. The major features are\n- Model-based: predict future states and find those with low rewards.\n- Continuous action space: distinguish the paper from those on discrete action space.",
            "strength_and_weaknesses": "The main concern of the reviewer is about the novelty and applicability of the proposed method.\n\n- The proposed attacking method depends on a pre-trained dynamics model. Training such a model may be costly in multi-agent settings.\n\n- The most frustrating point about the proposed method is that the target state needs to be collected, which largely limits the applicability of the proposed method to sparse reward settings and tasks with large action-oberversation space.\n\n- The proposed method has access to the policy of all agents. Such a white-box attacking method is less applicable.",
            "clarity,_quality,_novelty_and_reproducibility": "__Novelty__\n\nThere are many other papers that study attacking (cooperative) multi-agent reinforcement learning. The authors may want to at least discuss these papers in the related work section.\n\n- Guo et al., Adversarial policy learning in two-player competitive games\n- Wang et al., Backdoorl: Backdoor attack against competitive reinforcement learning\n- Pham et al., Evaluating robustness of cooperative MARL\n- Nisioti et al., Robust multi-agent q-learning in cooperative games with adversaries. ",
            "summary_of_the_review": "The research direction is interesting and promising, but the proposed method relies on less realistic assumptions and models.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_oW7x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_oW7x"
        ]
    },
    {
        "id": "u8T9SxDVTR",
        "original": null,
        "number": 3,
        "cdate": 1667185354698,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667185354698,
        "tmdate": 1667185354698,
        "tddate": null,
        "forum": "kugE_tCwsC",
        "replyto": "kugE_tCwsC",
        "invitation": "ICLR.cc/2023/Conference/Paper3668/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the author propose a novel model-based method to attack the team performance in cooperative Multi-agent reinforcement learning. By changing the input state of the victim agent, the goal is to reduce the reward function of the whole team. In addition, the authors also propose a  victim-agent selection strategy to choose the most vulnerable agent to attack. The evaluation results shows that the proposed method can consistently outperforms other baselines in all tested environments.",
            "strength_and_weaknesses": "Strong point:\n+ This paper tries to design a model-based attacking algorithm for MARL, this problem has not been explored too much before.\n+ A novel victim selection algorithm is proposed, which is also novel.\n+ Evaluation results shows that the performance is great.\n\nWeak point:\n- It seems that MARL is very easy to attack, according to Figure 5 and 6. This is also mentioned in the early work (Lin et al. 2020). The author should justify if their approach is still necessary given its relevant high implementation cost.\n- The other problem is training this attack method requires the attacker to have access to the RL environment and model parameters, which may seriously limited the application of this approach. \n- Compared with the early work (Lin et al. 2020) The novelty is not that obvious.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the paper is clear. The authors also provide the code. The novelty is ok, although this problem has been solved by other paper before, this paper introduce a novel method on victim agent selection, which has never been explored before, to the best of my knowledge.",
            "summary_of_the_review": "Overall, I think secure MARL is an interesting and important topic. The paper did bring some novelty to the field -- proposing an algorithm to select the victim agent which is the most vulnerable. However, the author should justify the applicability of their approaches, as mentioned in the Strength And Weaknesses section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_giiS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_giiS"
        ]
    },
    {
        "id": "cRIMsKHKxA",
        "original": null,
        "number": 4,
        "cdate": 1667586654470,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667586654470,
        "tmdate": 1667586654470,
        "tddate": null,
        "forum": "kugE_tCwsC",
        "replyto": "kugE_tCwsC",
        "invitation": "ICLR.cc/2023/Conference/Paper3668/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a model-based adversarial attack framework (c-MBA) on continuous action spaces of cooperative MARL. The core idea is to find the best perturbations to the states of the agents' policy networks using a transition model such that the state after the transition will be the closest to the failure state. The idea of finding the most vulnerable victim agents is interesting.",
            "strength_and_weaknesses": "**Strength**\n  * The research topic is interesting and the paper is well-motivated and clearly written.\n\n**Weaknesses**\n  * The definition of the failure states in Section 3.2 is too simple. Since the target of RL is to maximize the expected cumulative reward, simply minimizing the immediate reward the agent can obtain is not a good practice and may get in suboptimal solutions. Instead, we should find a target state which could minimize the Q-value, i.e., the expected cumulative reward, of the RL agent.\n  * I think there exist more simple but effective baselines. Since the authors perform adversarial attacks on agents trained using MADDPG. The most straightforward way to apply adversarial attacks to MADDPG is:\n    * $\\min _{\\Delta s=\\left(\\Delta s^1, \\cdots, \\Delta s^n\\right)} Q(\\mathbf{s}, \\mathbf{\\hat{a}})$, s.t., $\\mathbf{s}=\\text{concat}(s_1, \\ldots, s_N)$ and $\\mathbf{a}=\\text{concat}(\\hat{a_1}, \\ldots, \\hat{a_N})$, where $\\hat{a_i}=\\pi^i\\left(s_i+\\Delta s^i\\right), \\forall i \\in \\mathcal{N}$ and $Q(\\mathbf{s}, \\mathbf{\\hat{a}})$ is the centralized critic Q-function of MADDPG.\n    * In other words, the idea is to modify the input state to the policy network of the agents such that the $Q(\\mathbf{s}, \\mathbf{\\hat{a}})$ value of MADDPG can be minimized. This optimization problem can be implemented via SGD methods. As this method is very simple to implement, I'd like to see how well does this simple algorithm perform compared with the proposed (a little more complicated) one.\n  * What's the performance of the proposed method on some more complicated MARL benchmarks, e.g., SMAC?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well motivated and clearly written.\n\nThe research topic of studying the vulnerability of multi-agent cooperative RL methods is interesting. The idea of applying a environmental transition model to find the best state perturbation is novel.\n\nThe code is also attached in the Appendix.",
            "summary_of_the_review": "This paper proposes a model-based adversarial attack framework method to evaluate the robustness of multi-agent cooperative RL methods. The idea is novel and interesting. But some parts of the method designs can be further improved. The reviewer also thinks that there exist more simple but effective baselines. So, the reviewer currently recommends a weak reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_Rtpm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_Rtpm"
        ]
    },
    {
        "id": "024kOHKzj1",
        "original": null,
        "number": 5,
        "cdate": 1667596120537,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667596120537,
        "tmdate": 1667596120537,
        "tddate": null,
        "forum": "kugE_tCwsC",
        "replyto": "kugE_tCwsC",
        "invitation": "ICLR.cc/2023/Conference/Paper3668/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new attack method, called c-MBA, for evaluating the robustness of cooperative multi-agent reinforcement learning (c-MARL) environment. c-MBA algorithm is a model-based attack to craft adversarial observation perturbations. The authors empirically demonstrate c-MBA\u2019s superior performances against model-free baselines in two domains: multi-agent MuJoCo and particle benchmarks.",
            "strength_and_weaknesses": "## Strengths\n\n- I think the problem and approach are interesting and novel. A model-based approach for adversarial attacks on c-MARL settings seems promising. Furthermore, leveraging the unique setting in MARL to select the most vulnerable agent to attack is novel and intuitive.\n- The authors present numerous experiments to demonstrate the superior performance of their method against simple and model-free baselines for adversarial attacks.\n\n## Weaknesses\n\n- I understand the method is designed for continuous action tasks. I believe it should, with a straightforward change, also work on discrete action settings. If so, it would be beneficial to apply it to SMAC and compare it directly with Lin et al. (2020). I think this would strengthen the arguments made in the paper.\n- Applying it to problems would a larger number of agents would also strengthen the paper.\n\nNote: My score is not final and can change (in either direction) based on the authors' responses or the feedback of other reviewers.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The manuscript is generally clear and understandable.\n\nQuality: High.\n\nNovelty: Medium.\n\nReproducibility: The code is not included as part of the submission.",
            "summary_of_the_review": "This is an interesting paper where the authors propose model-based approach for adversarial attacks on c-MARL settings with continuous actions (for the first time). It includes extensive empirical analysis and ablation studies. I think the paper could also use discrete action environments and compare c-MBA directly with other baselines (designed specifically for those settings).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_Hx5n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3668/Reviewer_Hx5n"
        ]
    }
]