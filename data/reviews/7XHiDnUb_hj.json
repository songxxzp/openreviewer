[
    {
        "id": "SEVJoCfZ0zL",
        "original": null,
        "number": 1,
        "cdate": 1666912451358,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666912451358,
        "tmdate": 1666912451358,
        "tddate": null,
        "forum": "7XHiDnUb_hj",
        "replyto": "7XHiDnUb_hj",
        "invitation": "ICLR.cc/2023/Conference/Paper4973/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors presented a neural search method NSUBS to solve the subgraph search problem. The key idea is to pass information between the query and a local candidates pool at every search step to optimize the search. ",
            "strength_and_weaknesses": "The paper is well-executed and easy to follow. All parts are clearly presented and convincing. Experiences are thoroughly done. \nThe key contribution also involves the dynamic message passing + a look-ahead loss to guide the RL to find the optimal order of search. \n\n1. It is great that the authors provided time complexity and empirically validated it via experiments. But It is unclear the training complexity. Especially with the look-ahead loss, do we have to add up many future steps to compute the loss at every step?\n2. It would be helpful in the experiments the authors could provide a total time for training the learnable models, including this model. For models that often get stuck in a local minimum and often do not improve further after the first 20-60 seconds (fig 2), would it be fair to restart them with some different random seeds and repeat them until 5min? \n3. I was a bit confused: does NSUBS also follow a general DFS + backtracking approach? If so, wouldn't it suffer from the local minimum problem as well? \n4. The authors claimed NSUBS works better at large query graphs. But the results section doesn't provide further performance stratifications on queries w. different sizes. It would strengthen the paper if the readers can see some breakdown numbers to better understand this claim. \n5. Related to 4, it would be helpful if the authors could have some effort into the model interpretation. For example, show an actual search trajectory and show how the NSUBS modifies the local candidate list to speed up and optimize the search. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The quality is good except I would love to see a few more experiments. Novelty is fair. ",
            "summary_of_the_review": "I am willing to increase my score if the authors could address my questions and provides some more fair comparison and insights/limitations of the model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4973/Reviewer_HPUs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4973/Reviewer_HPUs"
        ]
    },
    {
        "id": "DxdSy0Uq2l",
        "original": null,
        "number": 2,
        "cdate": 1666929426250,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666929426250,
        "tmdate": 1670742102985,
        "tddate": null,
        "forum": "7XHiDnUb_hj",
        "replyto": "7XHiDnUb_hj",
        "invitation": "ICLR.cc/2023/Conference/Paper4973/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a learning based method for subgraph matching, using a graph neural network-based encoder-decoder architecture, and a reinforcement learning strategy. The GNN consists of two main modules: intra-graph message passing for modeling graph structures, and inter-graph message passing for query-graph matching. Furthermore, a look-ahead loss is formulated to supplement training signals.",
            "strength_and_weaknesses": "Strengths:\n1. The idea of inter-graph message passing for matching is novel to me. Existing GNN architectures only focus on the intra-graph counterpart.\n\n2. The use of reinforcement learning can reduce or eliminate labeling effort. \n\n\n\nWeaknesses:\n1. There is a large body of literature on the related subgraph isomorphism counting problem. In many applications, the profile of subgraph counts can sufficiently characterize a graph and it is not necessary to find all or most instances of the query.  Discussing some case studies and example applications could be useful to motivate why it is necessary beyond counting. \n\n2. The proposed model has high initial overhead. The methods only show significant advantage after a few mins. However, in many cases it might be important to get a few matching instances fast rather than getting many instances later on. \n\n3. Some parts are not well explained in detail.\n- 3.1: \"We define our reward as R(+) = 1 if the subgraph is fully matched and R(\u2212) = 0 otherwise.\" Do you actually mean the \"current\" subgraph is fully matched?\n- 3.1: \"Therefore, after the search, we collect the positive training signals at each state as all the node-node pairs (u, v) that lead to a solution\" I'm not clearly how this is exactly done. A step-by-step explanation might be useful.\n- 3.2.1: How are the mapping M, M' are constructed? Again, a step-by-step instruction might be clearer.\n\nMinor issues:\nThe ablation study is important and should be moved to the main paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presents some novel ideas. Certain parts need to be better explained. Practical impact (motivation/applications/results) may not be as significant as one would have hoped.",
            "summary_of_the_review": "Overall I think the weaknesses slightly overweigh the strengths.\n\n---\n\nUpdate: I'm largely satisfied with the responses, except for the initial overhead. The cost of executing NNs is part of the cost, which cannot be simply discounted. Overall, i will upgrade my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "nil",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4973/Reviewer_AyD2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4973/Reviewer_AyD2"
        ]
    },
    {
        "id": "VzxV5fDhh_s",
        "original": null,
        "number": 3,
        "cdate": 1667359165915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667359165915,
        "tmdate": 1667359165915,
        "tddate": null,
        "forum": "7XHiDnUb_hj",
        "replyto": "7XHiDnUb_hj",
        "invitation": "ICLR.cc/2023/Conference/Paper4973/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the subgraph matching problem to perform a subgraph isomorphism check between a query graph and a large target graph. The authors attempt to solve the solution and reward sparsity issues by designing an encoder-decoder neural network and look-ahead loss function based on reinforcement learning. Experiments conducted on six datasets show the effectiveness.",
            "strength_and_weaknesses": "Strengths:\n1. Detailed preliminary introduction to help the readers to understand the pipeline.\n2. The problem is not easy and deserves more attention.\n3. The proposed method is technically sound. \n\nWeaknesses:\n1. As far as I know, the related work for learning-based methods for subgraph matching is incomplete.\n2. The paper contains many notations and is hard to follow. Some preliminary knowledge could be moved to the Appendix because the main focus of this paper is on neural design.\n3. The methodology contribution is limited. Several works have adopted the idea of designing the encoder to encode two graphs and the design of the decoder is not novel.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The authors claim that heuristics-based solvers often fail to find any solution. Is the proposed method still based on heuristic-based ordering to select Ut?\n\n2. The related work is incomplete. Several related works should be discussed and probably need to be compared.\n\na. Consistent\u00a0Subgraph\u00a0Matching\u00a0over Large Graphs.\u00a0ICDE\u00a02022\n\nb. Reinforcement Learning Based Query Vertex Ordering Model for\u00a0Subgraph\u00a0Matching\n\nc. Interpretable Neural\u00a0Subgraph\u00a0Matching\u00a0for Graph Retrieval.\u00a0AAAI\u00a02022:\n\nd. Graph Convolutional Networks with Dual Message Passing for\u00a0Subgraph\u00a0Isomorphism Counting and\u00a0Matching.\u00a0AAAI\u00a02022\n\n3. The authors claim that aggregating nodes from Vq to obtain the state representation can address the challenge (2). Does it mean the NP-hard task can be solved?\n\n4. The state includes two graphs; its representation is only learned from the query graph. The motivation is not clear. How does the proposed method ensure the information on the target graph can be completely preserved?",
            "summary_of_the_review": "The paper proposes a reinforcement learning framework to solve the subgraph matching problem and designs an encoder and a decoder. The paper is technically sound but missed several related works and is not easy to follow. Also from my perspective, the methodology contribution is limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4973/Reviewer_181W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4973/Reviewer_181W"
        ]
    },
    {
        "id": "Vbmn9fHcUm",
        "original": null,
        "number": 4,
        "cdate": 1667553878709,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667553878709,
        "tmdate": 1667553878709,
        "tddate": null,
        "forum": "7XHiDnUb_hj",
        "replyto": "7XHiDnUb_hj",
        "invitation": "ICLR.cc/2023/Conference/Paper4973/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper provides a reinforcement learning method for subgraph matching problem. Given a query graph, they attempt to find the subgraph from a large graph that matches a query graph. Then the paper compare the underlying method against several baselines on real world datasets.",
            "strength_and_weaknesses": "Strengths:\n+ Important problem\n+ Intuitive approach\n\nWeakness:\n- Lack of well known baselines\n- Lack of novelty",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly. However, the quality of the experiments are poor. \nThe code is not given with the paper. Also, novelty of the paper is limited as it follows a very close approach with https://arxiv.org/abs/2002.03129.",
            "summary_of_the_review": "I have two key concerns with the paper. \nFirst, lack of powerful baselines. The paper acknowledges that most baselines are supervised learning methods. Thus, they adapted it into the RL method, which we appreciate. In this context, there are two powerful baselines which are never used:\n(1) https://arxiv.org/abs/1904.12787 and\n(2) https://ojs.aaai.org/index.php/AAAI/article/view/20784\n\nSecond, I really did not find any significant difference (of course there is some difference but they are not significant) from the approach adapted in https://arxiv.org/abs/2002.03129.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4973/Reviewer_vmC2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4973/Reviewer_vmC2"
        ]
    }
]