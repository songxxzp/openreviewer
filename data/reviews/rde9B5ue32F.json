[
    {
        "id": "p_FsPy5vgT",
        "original": null,
        "number": 1,
        "cdate": 1666585065256,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585065256,
        "tmdate": 1666585065256,
        "tddate": null,
        "forum": "rde9B5ue32F",
        "replyto": "rde9B5ue32F",
        "invitation": "ICLR.cc/2023/Conference/Paper5990/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims at extracting low-dimensional structure for dynamic data, especially for time series data. The proposed method, Compressed Predictive Information Coding (CPIC), both minimizes compression complexity and maximizes the predictive information\nin the latent space. This work extends the prior works dynamical components analysis and deep autoencoding predictive components. This work includes the analysis for both linear and non-linear encoding. The core idea is to estimate the latent predictive information without heavy matrix computation, but with cheap approximations. The resulting model achieves good performance on synthetic lorenz attractor, two neuroscience datasets and other real-world scenarios.",
            "strength_and_weaknesses": "Pros:\n1. The variational bound alleviates the cost of predictive information. It can potentially be used in more scenarios as a plug-in module.\n2. Operating in the latent space makes more sense than input space and employs the power of non-linear layers.\n3. Various experients and real-world datasets are shown.\n\nCons:\n1. It seems that DAPC has a very similar framework, also with a deterministic and probabilistic encoders (also a VAE?). Could you further elaborate on the differences?\n2. Could you further justify the stationarity assumption? Is it commonly seens in time series analysis?\n3. When you demonstrate the qualitative results (e.g. synthetic data), maybe you can also show some results from other methods.\n4. You can also show some time cost numbers to demonstrate the advantage of your method speed-wise.",
            "clarity,_quality,_novelty_and_reproducibility": "In general, this method extends the prior works and propose a variational bound to reduce the computation cost. The paper has both theoretical and empirical results. Most related methods are included for comparison. The code is not provided. The method is not straightforward to re-implement.",
            "summary_of_the_review": "This paper improves over previous method, and potentially reduces the time cost. Operating in the latent space also takes advantage of the large capacity brought by deep models. It would be better if the method is also tested on larger and more mainstream time series datasets.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5990/Reviewer_jyD8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5990/Reviewer_jyD8"
        ]
    },
    {
        "id": "dcKoq14WuPP",
        "original": null,
        "number": 2,
        "cdate": 1666633647402,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633647402,
        "tmdate": 1670469410260,
        "tddate": null,
        "forum": "rde9B5ue32F",
        "replyto": "rde9B5ue32F",
        "invitation": "ICLR.cc/2023/Conference/Paper5990/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a way to optimize time series forecasting model through adapted information bottleneck loss. In particular, it is proposed to learn low dimensional representations of the input time series (using variational encoders) via maximizing the mutual information between representations of the input and target and minimizing mutual information between input and representation. The approach is termed compressed predictive information coding (CPIC) and the authors propose two different computable bounds, based on the existing estimators of mutual information, that can be used to train the models. Further, authors explore the empirical performance of the proposed bounds, first on the artificially generated data with clear structure, to check the ability of the representations learned with CPIC to reconstruct data. The second experimental evaluation is done on the real data where forecasting is made using linear regression on the learned representations.",
            "strength_and_weaknesses": "The paper is proposing an information bottleneck type of the loss for a different from usual setup. The time series forecasting is a complicated task that is an active research area and novel approaches are always interesting.\n\nThe paper uses existing estimators and bounds on mutual information, thus the main novelty of the paper is the formulation of the information bottleneck loss and finding methods that allow to compute it. The evaluation shows that the proposed method performs better than two existing approaches, though still with not very high performance.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper has some drawbacks that make it harder to follow up the ideas. Some examples are in the following:\n1) use \\citep when citations are inside of the sentence\n2) sequential data is not always dynamic and not always timeseries (using i.e. in this case is not justified)\n3) some terms are used without introduction, for example \"low-level information\" and \"compression complexity\" in section3\n4) it would be nice to specify what activation functions are used in the encoders\n5) notation on page4, in the end, is not very nice: usually p(X(T), Y(T)) will denote a probability of having this tuple, not the probability distribution, which is meant there\n6) typo in paragraph after Theorem3: q(y(T)|y(-T)), not q(y|x)\n7) in section4.3 it is an upper bound on CPIC, not lower bound\n8) in section4.3 it was unclear why there is a one-sample bound and multiple-samples bound. It should be explained more precisely.\n9) in section5.2 it is mentioned that \"predictions are conducted by linear regression to emphasize the structure learned by the unsupervised methods\". It is a confusing formulation and it should be described in more details what is the reason to use linear regression.\n\nThe novelty of the paper is about formulating information bottleneck loss for the time series forecasting task.\n\nThe code for the experiments is not provided, reproducibility might be hard.",
            "summary_of_the_review": "The paper formulates an information bottleneck loss for timeseries forecasting problem, using representations learned with variational encoders. The existing estimators combined to produce a computable bound for the proposed loss and further this bound evaluated for the task of reconstruction and forecasting.\nOverall, my main concern is about the reported evaluation results, which seem to be not very high. Since the core of the paper is formulation of the loss, the empirical evaluation should be very convincing.\nMoreover, why the reported results for existing methods (CPC and DCA) are so much worse, nearly twice on average or even more? Are these methods so bad for the problems selected for experiments? Or only with the SNR that is very low? The reported results are also very low for the real world data. I wonder what is the state-of-the-art scores on the problems considered.\n\n----\nI thank the authors for the clarifications. I will stick with my score after the internal discussion.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5990/Reviewer_aQPA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5990/Reviewer_aQPA"
        ]
    },
    {
        "id": "3uuFwUol-A",
        "original": null,
        "number": 3,
        "cdate": 1666655936837,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655936837,
        "tmdate": 1666655936837,
        "tddate": null,
        "forum": "rde9B5ue32F",
        "replyto": "rde9B5ue32F",
        "invitation": "ICLR.cc/2023/Conference/Paper5990/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a method for self-supervised time-series prediction called Compressed Predictive Information Coding (CPIC). The core idea is that a sequence of data points are mapped to a Gaussian latent space via some encoder function. Then, these latent representations can be used for downstream tasks, e.g. regression. The authors propose minimizing the difference between two computationally intractable mutual information terms to train the model. To make their approach practically applicable, the authors propose to minimize a variational upper bound to the original objective. The authors test their approach on synthetic and some real-world regression problems.",
            "strength_and_weaknesses": "## Strengths\nThe proposed approach is well-motivated and an interesting idea. It is quite simple; hence, I wonder if it has already been proposed elsewhere. However, I am not familiar enough with the sequential prediction literature to be able to comment on the originality of the approach though. \n\nSome of the empirical results also seem promising.\n\n## Weaknesses\nIn addition to the list below, the paper suffers from severe stylistic and clarity issues; see the section below.\n - In Sections 4.1 & 4.2, the authors state four \"theorems\" that give certain variational bounds on the intractable terms in their proposed loss function in Eq 4. However, it is not entirely clear what the authors are claiming here as their original contribution. The statements in Thm 1 and 3 follow trivially using elementary information-theoretic arguments in two lines (Eqs 9 and 11 in the appendix). The statements of Thm 2 and 4, and 5 are essentially taken from other works with trivial substitutions. Could the authors please comment on what exactly they are claiming as their contribution here?\n - The authors never formally state the exact models used for their experiments, making their results impossible to interpret.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing generally needs improvement. For example, there are countless typos, imprecise technical language and strange grammatical structures. However, there are also more major stylistic issues:\n - Much of the Introduction is dedicated to discussing related works in a way that doesn't directly pertain to the authors' proposed method. These parts should be moved into the related works section. The clearest example is the second paragraph, which should be (perhaps even verbatim) moved to Section 2.\n - Similarly, most of the first paragraph of Section 3 should also be moved to the Related Works section.\n - The authors' work seems to be very closely related to variational recurrent auto-encoders and variational state-space models, yet they are not discussed in the related works. Could the authors please comment on this?\n - In Section 3, the authors write: \"A nonlinear CPIC refers to a stochastic nonlinear encoder including a nonlinear mean encoder and a linear variance encoder, while a linear CPIC refers to a stochastic linear encoder in which it replaces the linear mean encoder by a nonlinear mean encoder.\" - I highlight this particular sentence because it is supposed to describe the variants of CPIC that are later presented, but is very challenging to parse and probably contains errors.\n - In Thm 4, the definitions of \"critic function\" and \"baseline function\" are missing.\n - The authors only present results for the multi-sample upper bounds in the main text. Hence, I don't think presenting the univariate bounds is useful, and their discussion should be moved to the appendix.\n - Probably every section title in Section 5 should be reworded. In particular, \"Numerical demonstration of the superiority of CPIC\" should be renamed to \"Results\" or \"Experiments\".\n - \"The motivation for using linear forecasting models is that good representations contribute to disentangling complex data in a linearly accessible way\" - what does \"linearly accessible way\" mean?\n - Just below Eq 3: \"$U \\in \\mathcal{R}^{N \\times D}$\" - what is $U$?\n - What is $f_1$ and $\\gamma$ in Eq 8? In the sentence below, what is $\\beta$?\n - Label font sizes in Figure 2 and the legend font sizes in Figure 3 should be increased as they are currently hard to read.\n - Instead of showing mean and best performance, Figure 3 should show median performance with the inter-quartile range as the error bars to give a better idea of performance.\n - \"Finally, as the SNR gets lower (SNR = 0.001, bottom-right) all methods perform poorly, but we note that, numerically, considering predictive information in latent space is much better than that between latent and observation space.\" - why is stochasticity better numerically? There is no justification given for this claim.\n",
            "summary_of_the_review": "The authors present a nice, well-motivated idea. However, it is unclear what theoretical contributions they claim, the description of their empirical methodology is incomplete, and the presentation quality is generally low.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5990/Reviewer_3inW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5990/Reviewer_3inW"
        ]
    },
    {
        "id": "Omt83sE__lT",
        "original": null,
        "number": 4,
        "cdate": 1667018671976,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667018671976,
        "tmdate": 1667018671976,
        "tddate": null,
        "forum": "rde9B5ue32F",
        "replyto": "rde9B5ue32F",
        "invitation": "ICLR.cc/2023/Conference/Paper5990/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a learning criterion and numerous bounds for learning representations of dynamic systems that are at once maximally compressed (minimal mutual information with their original representation, i.e., minimal rate) while being maximally informative a future time point (or, due to symmetry, a previous timepoint). This leads to learned encodings which are, up to the SNR and trade-off parameter $\\beta$,  reflective of the dynamical system.",
            "strength_and_weaknesses": "Strengths:\n* The method is grounded in established literature (information bottleneck and variational information bottleneck, and more generally Rate-Distortion theory), yet contributes a novel criterion. Secondly, the recognition that the stationarity assumption reduces the learning criterion to a two term loss function, which is, in structure, very similar to many other information trade-offs in the literature.  \n* Multiple bounds are explored for the mutual information minimization/maximization.  \n* Experimental results are generally well done, modulo exact generation details.  \n\nWeaknesses:\n* Allowing time-windows T, for periodic or pseudoperiodic phenomena with period T' we might observe aliasing artifacts. While this remains stationary in a global sense, will such artifacts impede dynamics. Moreover, will the interpretation or downstream use of those dynamics be impeded by this induced false beat frequency at (T-T')/2?  \n* The variational lower bound (VLB) of Theorem 3 does not appear to require a decoder, contrary to the comment in the sentence immediately following the statement of the theorem; it seems as though it instead requires a good estimate of the conditional likelihood (a prediction from either T to T' or vice versa).  \n",
            "clarity,_quality,_novelty_and_reproducibility": "It might be more clear to note the differing time-blocks at $T_0$ and $T_1$, since there is no reliance on any symmetry pattern (i.e., there is nothing special about -T versus T as far as I understand.\n\nIt might be also helpful to be more clear about the lifting process/simulation for the attractor datasets. While the general idea is conveyed, the exact details it seems could be easily shared (or code given to reproduce the test cases). Overall however this paper seems reproducible.\n\nOverall the bounds are restatements of other results; this is somewhat clear in the paper, but it should still be noted re:Novelty for the purposes of review. There are several other differentiable mutual information estimators, including some that avoid the Gaussian encoder function. Though the paper is already quite thorough, it may be helpful to also test these functions, e.g. the correction of MINE in Song et al 2019 (called SMILE), and the Echo noise encoders in Brekelmans et al 2018.",
            "summary_of_the_review": "The authors present an elegant embedding method for dynamical systems, and the estimation machinery to fit the embedding efficiently. The method is novel, but also fits well into existing literature, adding new and interesting directions to Rate-Distortion based encodings. I think the ICLR community at large would be interested in such a manuscript.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5990/Reviewer_U5Xb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5990/Reviewer_U5Xb"
        ]
    }
]