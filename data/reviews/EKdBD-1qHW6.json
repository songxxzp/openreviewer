[
    {
        "id": "8YZY2ix2gOM",
        "original": null,
        "number": 1,
        "cdate": 1666687751687,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687751687,
        "tmdate": 1666687751687,
        "tddate": null,
        "forum": "EKdBD-1qHW6",
        "replyto": "EKdBD-1qHW6",
        "invitation": "ICLR.cc/2023/Conference/Paper3383/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a novel Spectral Neural Network for matrix learning problems. Some theoretical results are proven to show the effectiveness of SNN in matrix sensing. Several numerical experiments show the convergence of the matrix singular values.",
            "strength_and_weaknesses": "Strength:\n1. The target problem is interesting. \n2. This paper proposed a novel SNN architecture.\n3. Both theoretical and numerical results are given to demonstrate the effectiveness of the proposed methods.\n\nWeakness:\n1. It would be better to present more details of the target problem and proposed architecture in the introduction and discuss a few more related works.\n2. There are many parameters in SNN. It would be better to present more numerical experiments to study the sensitivity of SNN.\n3. It would be interesting to perform SNN on some classical problems, such as matrix completion.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has some merit in terms of the novel proposed architecture. But the presentation needs to be improved.",
            "summary_of_the_review": "This paper proposed a novel SNN architecture for matrix learning problems. More numerical experiments are required to show the effectiveness of SNN.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3383/Reviewer_Ajzu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3383/Reviewer_Ajzu"
        ]
    },
    {
        "id": "khACaBV17F",
        "original": null,
        "number": 2,
        "cdate": 1666692679056,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692679056,
        "tmdate": 1666692679056,
        "tddate": null,
        "forum": "EKdBD-1qHW6",
        "replyto": "EKdBD-1qHW6",
        "invitation": "ICLR.cc/2023/Conference/Paper3383/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes spectral neural networks (SNNs), an architecture that applies the nonlinear activation function to the singular values of a matrix, to demonstrate the role of implicit regularization in deep learning. Section 3 introduces the SNN architecture, and then Section 4 presents Theorem 3 suggesting that under the assumptions made the SNN training will converge to the nuclear-norm minimzing solution, indicating the implicit regularization of the gradient flow. A few numerical results are discussed in Section 5 to support SNNs and the implicit regularization in training them via gradient descent.",
            "strength_and_weaknesses": "Strengths:\n\n1) The paper proposes a novel neural network architecture (Spectral neural networks) for matrix learning problems.\n\n2) The paper is well-written, and the theoretical discussion clearly states the assumptions and theorems. \n\nWeaknesses:\n\n1) The assumptions in section 4 look too strong to me. Especially the assumptions that the neural network and measurement matrices share the same left and right singular vectors seem restrictive to me. I recommend the authors include more explanation on the role of these assumptions in the theoretical analysis.\n\n2) While I understand this is a theoretical work, the numerical results look preliminary. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and I think the authors have done a good job in writing this work. Also, the spectral neural network architecture and the theoretical analysis all seem novel to me.",
            "summary_of_the_review": "This paper proposes a novel neural network architecture for matrix learning problems. The paper contributes several nice ideas; however, it can still be improved by including additional numerical results and some explanation of the role and necessity of the assumptions in section 4.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3383/Reviewer_xZxU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3383/Reviewer_xZxU"
        ]
    },
    {
        "id": "g4JMLAv3cGJ",
        "original": null,
        "number": 3,
        "cdate": 1667317669972,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667317669972,
        "tmdate": 1667319696521,
        "tddate": null,
        "forum": "EKdBD-1qHW6",
        "replyto": "EKdBD-1qHW6",
        "invitation": "ICLR.cc/2023/Conference/Paper3383/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a nonlinear neural network (SNN) to solve the matrix sensing problem, and show that the gradient flow on SNN leads to a minimum nuclear norm solution to the matrix sensing problem.",
            "strength_and_weaknesses": "Strength: this paper tries to use a nonlinear neural network to solve the matrix sensing problem.\n\nWeakness: There are several major concerns and technical drawbacks of this paper.\n\n1. Unclear motivation and contribution: previous works [Gunasekar et al. (2017) and Arora et al. (2019)] already showed that gradient flow by linear neural network achieves the minimum nuclear norm solution, this paper proposes a computationally expensive nonlinear neural network which leads to the same minimum nuclear norm solution (however, under very unrealistic assumption, see below). It is not clear what is the theoretical contribution or insight provided by this paper. \n\nThis paper presented a very vague description regarding the contribution, \"Despite the large body of works studying implicit regularization, most of them consider the linear setting. It remains an open question to understand the behavior of gradient descent in the presence of\nnon-linearities...\"\n\nAs stated above, one studies nonlinear neural network in the hope of brining theoretical benefits to the matrix sensing problem (or its implicit regularization property, that is, the minimum nuclear norm solution). However, this paper fails to provide any such benefit.\n\n2. Unrealistic assumption: the main result of this paper relies on a key Assumption 2: note that one needs to access the information of the oracle (in terms of $\\sigma^*$ obtained from $X^*$) in condition (c) of assumption 2, which is very unrealistic both theoretically and empirically. \n\n3. Expensive computation: the feed-forward of the proposed network requires SVD on the input (see eq. (5)), and condition (a) in Assumption (2) is achieved by performing SVD of the sensing matrices, which are all computationally expensive, and in strong intransitive to existing works [Gunasekar et al. (2017), Arora et al. (2019), Li et al. (2018)].",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this paper should be significantly improved. There are numerous inconsistencies, such as reused notations and lacking parenthesis for equation numbers. Sometimes equation is referred before its presence, such as Eq. 74. The code of SNN is provided for reproducibility.",
            "summary_of_the_review": "While this paper proposes a nonlinear neural network to study the implicit regularization in matrix sensing problem, it suffers from multiple major technical drawbacks and unrealistic assumption. Due to the above major concerns, I recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3383/Reviewer_p8b1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3383/Reviewer_p8b1"
        ]
    },
    {
        "id": "hH_Ai_oyTUJ",
        "original": null,
        "number": 4,
        "cdate": 1667544731250,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667544731250,
        "tmdate": 1667544731250,
        "tddate": null,
        "forum": "EKdBD-1qHW6",
        "replyto": "EKdBD-1qHW6",
        "invitation": "ICLR.cc/2023/Conference/Paper3383/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the implicit regularization phenomenon in neural network with gradient descent. Different from the previous work, the authors works on the more general neural nets with non-linear activation functions. In particular, the authors demonstrated the implicit regularization phenomenon with non-linear activation function for matrix sensing problem. A new neural network structure called spectral neural network is proposed for solving the matrix learning problem. Both theoretical analysis and empirical evaluations are reported to support the proposed solution.",
            "strength_and_weaknesses": "This paper focuses on an interesting topic of the implicit regularization for neural network with non-linear activation functions. It should attract a lot of attention from the community. The paper is well-written and well-organized, every concept is well explained and easy to follow. I feel really pleasure reading the paper. To analyze the implication regularization for matrix sensing problem, the authors proposed the spectral neural network architecture. Such structure incorporate the singular values and singular vectors of the matrix, instead of the entries, which is quite interesting. Besides, the following theoretical analysis and empirical evaluation support the claims and shows the effectiveness of the proposed solution.\n\nOverall, this paper is good. I have some minor suggestions and questions. \nFirst, for the matrix sensing problem this paper focuses on, is there any assumption on the matrix X*, e.g., low rank matrix? I didn't see any claims in the problem setting section (correct me if I am wrong).\nThe illustration of the SNN structure is a little bit confusing. For example, is every blue rectangular in the SNN corresponding to the block, or it is just one row of the block? I would suggest the authors to provide more details of the figure to help readers better understand the structure. Besides, what is the relation between the K in the block and the L in the SNN structure? How to choose the value of K and L in experiments, are they related to the property of the matrix?\nMinor: the size of the font could be enlarged in the result figures, e.g. ,Figure 3",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - The paper is well-written and clear. Quality - This paper has good quality of both the writing and the solution. Originality - Good.",
            "summary_of_the_review": "This paper focuses on a really interesting topic and proposed the first analysis of the implicit regularization for neural network with non-linear activation function (for matrix sensing problem). The proposed SNN structure is well suited for the matrix sensing problem and the corresponding theoretical analysis supports the claims. Empirical evaluations also demonstrates the effectiveness of the proposed solution. Overall, this paper has high quality and I would recommend accept the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3383/Reviewer_yAZt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3383/Reviewer_yAZt"
        ]
    }
]