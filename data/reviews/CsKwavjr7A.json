[
    {
        "id": "5UiGlxiaMYh",
        "original": null,
        "number": 1,
        "cdate": 1666109473974,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666109473974,
        "tmdate": 1670232068214,
        "tddate": null,
        "forum": "CsKwavjr7A",
        "replyto": "CsKwavjr7A",
        "invitation": "ICLR.cc/2023/Conference/Paper606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes deep nearest centroids, a new non-parametric classification layer. For each class, a number of sub-centroids are learned during the course of training. At test-time, the prediction is the class corresponding to the closest sub-centroid. The sub-centroids are learned in an iterative fashion: 1) Network parameters are updated to minimize the distance between each sample and the closest sub centroid of the ground truth class 2)  The subcentroids are updated using \u201cmomentum updates\u201d using the centroid assignments of the current minibatch + a memory bank.\n\nImprovements are shown on CIFAR-10 (ResNet backbones) and ImageNet (ResNet + Swin backbones). Results on semantic segmentation are even stronger on 3 algo + backbone combinations. If the sub-centroids are restricted to training images, the algorithm shows out-of-the-box interpretability, in the sense that these sub-centroids can be leveraged to explain classification decisions by the model.\n",
            "strength_and_weaknesses": "**Strengths**\n\n* The paper shows strong results on both classification and segmentation benchmarks. Ablations are shown to evaluate the impact of #subcentroids, showing that this design choice is important, i.e with no sub-centroid clustering, the algorithm performs worse than the classification layer.\n* While a number of architectural changes have been proposed to neural networks, this paper proposes to modify just the last layer with performance improvements, which is of interest to the ICLR audience.\n* The model provides out-of-the-box interpretability under some constraints. A few epochs before convergence, the authors propose to anchor each sub-centroid to the closest training image which can provide explainable predictions.\n\n**Weaknesses**\n\nSee below for detailed feedback.",
            "clarity,_quality,_novelty_and_reproducibility": "**Novelty**\n\nThe proposed deep centroids layer can be considered as the application of the same algorithm for cluster assignments done in unsupervised learning [1, 2] to do within class cluster assignments. The authors should be upfront about this. Note that this does not mean, that the current work is not novel, but the writing in the introduction/related work should reflect this and the work should be placed in this context.\n\n**Reproducibility**\n\n* It would be nice to have a small Algorithm section showing the pseudcode of the algorithm. For example, the algorithm can show in what sequence the cluster assignments, (parameter, memory bank and cluster) updates are done. For reference, look at Algorithm 1 of the MoCo paper (https://arxiv.org/abs/1911.05371)\n* The deep centroids layer has 2 additional hyperparameters (K and $\\mu$). How was hyperparameter tuning done? Was there a separate validation set split from the train set? Please provide these details.\n\n**Clarity and Quality**\n\nI would appreciate it if the authors can clarify these details.\n\n* **Section 3, transferrability**: The authors claim that for after pretraining, the softmax layer is thrown away, leading to \"wastage\" of parameters. This is not 100% convincing since the equivalence to the deep nearest centroids is the \"sub-centroids\" which are K times more than the paramters in the softmax layer which also have to be thrown away during finetuneing.\n* How important is the Sinkhorn-Knapp algorithm to determine sub-centroid assignments? Can one just set the cluster assignment to be the nearest sub-centroid as done in K-Means?\n* **Section 4.3**: For the \"anchoring of sub-centroids\", to obtain the images used to anchor the sub-centroids, do the authors find the closest image over the entire training set or is just the memory bank sufficient?\n* **Parametric Softmax Classifier**: \"and cannot directly supervise on the representation $x$\". It is unclear what this means as $s$ is a a function of $x$\n* **Memory Bank Size**: It might be more intutive to show this hyperparameter as a function of #examples rather than #batches.\n* W and b are learnt without considering any underlying data distribution. It is unclear what this means as W and b are learnt by modeling the  conditional P(y|x) which is the data distribution.\n\n\nThe paper introduces some terms which in my opinion are a bit colloquial and diffficult to understand. The colloquail terms are bolded below\n\n* Abstract: Case-based reasoning, it is unclear what \"case-based\" refers to\n* Intro, para 1: \"detached from the physical nature of the problem\"\n* Intro, para 2: it is fully aware of the limitations of parametric counterparts\n* nonparametric manner and understandable from user\u2019s view, in contrast to the parametric classifier that learns **non-transparent parameters** for each class. \n* Section DNC Classifier: finding **classification evidence** for a previously unseen sample by retrieving the most similar exemplar.\n* Section Training of DNC: DNC directly optimizes the representation by **adjusting the arrangement** between sub-centroids and data samples.\n* Section Versatility: **Reforms the training regime**\n* Section Ad-hoc Explainability: explanations are **loyal to the internal decision mode**\n* **stronger adhoc explainibility** - I think this sentence should be restructured.\n\n\n\n**Typos**\n\nThis did not affect my rating,\n\n* proximity of test data and the class sub-centroids in the feature space -> proximity of test data to the class sub-centroids in the feature space\n* Parametrization nature -> parametric nature\n* ImageNet [9] trained image classifiers initialize segmentation networks -> ImageNet-trained classifiers to initialize\n* memory for online estimating the sub-centroids -> for estimating the sub-centroids online\n* investigate post-hoc explains -> investigate classifier posthocs.\n* even coming with better performance -> even with better performance\n* The final output dimensionality is constrained as many as the classes. Constrained as -> constrained to be the number\n* if using multiple sub-centroids (local means) per class instead of just on -> if multiple sub-centroids per class are used.\n* coming with minimal architecture change -> with\n* one single class weight/center is far enough -> Did you mean not enough, in the context of the results\n\n[1] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, NeuRIPS 2020\n[2] Self-labelling via simultaneous clustering and representation learning, ICLR 2020",
            "summary_of_the_review": "I initally rate this paper below the border, even though the results are strong. See above for a list of my concrens.\nI am happy to adjust my rating if the authors can address my concerns with respect to the novelty and clarity of the paper and update the draft as a result.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper606/Reviewer_k9FP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper606/Reviewer_k9FP"
        ]
    },
    {
        "id": "qEfew5QisY",
        "original": null,
        "number": 2,
        "cdate": 1666295422721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666295422721,
        "tmdate": 1669728842075,
        "tddate": null,
        "forum": "CsKwavjr7A",
        "replyto": "CsKwavjr7A",
        "invitation": "ICLR.cc/2023/Conference/Paper606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the use of nearest centroid classifiers in the context of deep neural networks, and in particular a setting where multiple centroids are used per class, obtained through clustering the feature vectors for each class.\nThe proposed method consists in concurrently learning the feature backbone network, and a per-class online clustering. For the clustering an optimal-transport based method is used to approximately equalise the amount of data assigned to each cluster.\nExperiments on image classification (ImageNet, CIFAR-10) and semantic segmentation (ADE20K, Cityscapes) are conducted using different network architectures.\nResults indicate improvements in prediction accuracy when comparing parametric linear classifier heads and nearest centroid classification heads. Moreover, networks pre-trained with the nearest centroid classifier, also transfer better to semantic segmentation tasks. \nIn addition, when using data points close to the cluster centres for classification, the method allows for interpretable results via visualisation of the exemplar used for classification. \n",
            "strength_and_weaknesses": "\nStrengths:\n\n+ Two different tasks are considered to evaluate the effectiveness of deep nearest centroids classifiers: image classification and semantic segmentation (pixel classification). \n\n+ For each task two datasets are considered: ImageNet, CIFAR-10 for classification, and ADE20K, Cityscapes for semantic segmentation.\n\n+ Multiple network architectures are considered for each experiment.\n\n+ Exploration of the use of training samples rather than cluster centres as anchors for classification is interesting from an interpretability perspective.\n\n+ The use of nearest centroid classification is shown the be beneficial for semantic segmentation: (i) using it to fine-tune the segmentation model, and (ii) using it to pre-train the feature backbone network. Using it for both yields best performance. \n\n \nWeaknesses:\n\n- The claimed improved transferability is shown when pre-training on ImageNet classification, and then fine-tuning on ADE20K/Cityscapes semantic segmentation. There is no experimental evidence that transferability is improved towards other image classification tasks. Such results would be relatively easy to add, and would strengthen the support of the claim of improved transfer of features learned using the centroid-based classifier. \n\n- Experimental results do not include analysis of the stability of the results w.r.t. various sources of randomness in the experimental setup (initialisation of parameters, sampling of training batches). In particular for tables 1 and 2 this would help, as differences in accuracy are relatively small.  For ImageNet classification adding evaluation on the ImageNetv2 test set (https://github.com/modestyachts/ImageNetV2) would also strengthen the experimental results.\n\n- It is not clear where the normalised exponential for of the solution presented in Eq 6 comes from. In my understanding it is a consequence of the KL-regularization used in [25], but which is not adopted here in Eq 5. I guess it is an omission.  \n\n- The effect of the optimal-transport / Sinkhorn clustering approach as compared to a naive k-means approach is not ablated in the paper, while this is the main novelty of the paper over [52] besides using multiple centroids per class.\n\n- The setting of the temperature parameter epsilon in Eq 6 is not discussed, nor is its impact experimentally analysed. \n\n- Relations of the current work to [23,52] are not discussed in sufficient detail, it should be clear from the paper how this work is different from these earlier works. For example, the related work section states that [52] showed weak results even on small datasets, yet it is not clear why the current paper would work better for those cases: the ablations show only limited improvements over the case with K=1 cluster center per class as used in [52]. It is not clear from this why for the current paper experimental comparison to [52] is not included. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\nThe paper is clearly written and easy to follow, but the quality of writing can surely be improved. There are numerous typos, sentences that are unclear or poorly formed. \nExamples include\n- page 1: \"greatly boots pixel recognition\"\n- page 1: \"... and ever popularity [20-23], Nearest Centroids, and particularly...\"\n- page 4: \"... cannot providing...\"\n- Eq 5: the clustering objective that is stated suggests using dot-product as a score to determine cluster assignments and probably L2 normalised feature vectors. This is not explicitly stated, however, at this point in the paper. \n\n# Quality\nSome claims in the paper seem too strong, and the contributions are not clearly identified.\nFor example, the introduction claims that \n-linear classifiers assume unimodality of the data \"...bearing no intra-class variation.\", which is not substantiated. \n- \"linear classifiers are trained purely for classification accuracy\":  this also holds for centroid based classifiers, their difference lies in the pararmeterization of the classifier.\n- On page 2 there is a list of 5 attractive properties of centroid-based classification: these seem generic and not associate with the contributions made by the authors in the current paper. The list of experimental findings in the last paragraph of page 2 (running over to page 3) seem to be the claimed contributions of the paper.\t\n\n# Novelty\nThe paper evaluates nearest centroid classifiers in combination with deep feature learning networks. \nThe nearest centroid classifier, as well as the optimal-transport based clustering approach are known in the literature. Their combination, however, appears novel to me.\nI haven't seen nearest centroid classifiers for semantic segmentation, but I'm not actively working on this  application nor on centroid-based classifiers, so I could have missed something. \nIn particular the transfer ability of the imageNet pertained feature backbone to the segmentation task is an interesting experiment in the paper, that contributes to the literature.\n\n# Reproducibility\nThe work seems fairly easy to reproduce. The method is clearly described. Code is promised, but not provided. Experiments were conducted across an 8 V100 GPU hardware setup. \nIt wasn't clear to me in Section 4.4 why using K>4 centroids per class was not possible for the imageNet classification experiments, but it was possible to go up to K=20 for the ADE20K semantic segmentation experiment. It would be useful to add a paragraph analysing the memory cost of the model wrt the number of centroids K and the \"external memory\" size used to store additional batches for clustering. \n",
            "summary_of_the_review": "\nThis paper considers nearest centroid classifiers in combination with deep feature learning networks. The authors train the feature network, and cluster the data of each class in parallel using an optimal-transport based technique: a novel combination of existing techniques. \nExperiments on image classification and semantic segmentation compare the centroid-based classifier with a linear classification head, and conclude improved accuracy using centroid-based classifiers.\nExperiments also indicate that centroid-based pre-training for image classification yields better results when fine-tuning for semantic segmentation. \nReplacing centroids with the nearest datapoints (images) induces a small loss in predictive accuracy, but yields an interpretable exemplar-based classifier.\nMy ratings are based on a small perceived technical novelty, combined with some interesting experimental findings. The quality of the presentation can be substantially improved.\n\n### Post-rebuttal comments ###\nAfter reading author rebuttal and other reviews, I maintain my original recommendation of this paper as weak accept. \nThe authors have addressed most of the comments in my review: a number of clarifications have been made, and additional results are added, in particular evaluation on the ImageNetv2 evaluation sets, which confirm the results obtained earlier on the imageNet validation set.  I do not see any major reasons to reject the paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper606/Reviewer_GtUv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper606/Reviewer_GtUv"
        ]
    },
    {
        "id": "4pYnJQpOqD",
        "original": null,
        "number": 3,
        "cdate": 1666522035360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666522035360,
        "tmdate": 1666522035360,
        "tddate": null,
        "forum": "CsKwavjr7A",
        "replyto": "CsKwavjr7A",
        "invitation": "ICLR.cc/2023/Conference/Paper606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a deep nearest centroids (DNC) network for large-scale visual recognition. In contrast to the commonly employed parametric classifier (e.g., a parametric Softmax Classifier), the proposed DNC is claimed to be able to capture the underlying data distributions and is more interpretable. Experiments on a few benchmark visual recognition and semantic segmentation datasets demonstrate the superiority of DNC.\n",
            "strength_and_weaknesses": "Strength:\n++ Nearest centroids based classifiers have been used in many classification problems and exhibit strong performance compared with its parametric counterparts. This work applies the non-parametric classifier in the basic image classification task and achieves competitive performance by utilising sub-centroids of training samples to describe class distributions.\n++ Thorough experiments have been conducted to support the claims in the manuscript.\n\nWeaknesses:\n-- The number of centroids for each class K seems to be an important hyper-parameter. In the paper, K is set as a unique value for all classes; however, this may not be optimal since the intra-class variability can be very different across different classes. Is it possible to automatically determine the K value for different classes?\n-- The nearest centroids classifier only considers the 1st-order statistics. Would the 2nd-order statistics of clusters contribute to the classification performance?\n-- It would be interesting to compare the errors made by DNC and a parametric softmax classifier. Do they learn complementary knowledge?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, easy to follow. Although the nearest centroid classifier itself is not a novel idea, the successful application of it to large-scale visual recognition is non-trivial. The method is clearly described and should be reproducible.",
            "summary_of_the_review": "To summarize, the paper investigates a classific non-parametric classifier and successfully integrates it into deep learning frameworks. Competitive performance have been achieved. The work can provide useful insights into representation learning and visual recognition for the community.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper606/Reviewer_3KaQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper606/Reviewer_3KaQ"
        ]
    },
    {
        "id": "wnxd2eDo-QK",
        "original": null,
        "number": 4,
        "cdate": 1666874670958,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666874670958,
        "tmdate": 1666874670958,
        "tddate": null,
        "forum": "CsKwavjr7A",
        "replyto": "CsKwavjr7A",
        "invitation": "ICLR.cc/2023/Conference/Paper606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method that learns deep prototype representations, by revisiting the nearest centroids clustering in the setting of deep network training. The authors construct a framework to combine the gradient descent kind of learning of neural networks with the distance-based learning of cluster centroids. In this paper, the clustering is done class-wise, defining a number of centroids K per class.",
            "strength_and_weaknesses": "__Strenghts__\n- the idea of learning deep centroids is relevant\n- interpretability of the classification with prototypes or a real sample closest to the prototype\n- experiments with different backbones\n\n\n__Weaknesses__\n- _statements about limitations of existing methods are too stretched_, e.g. \n\t1. lack of simplicity or explainability mentioned as weaknesses of existing approaches is not addressed clearly in the paper: the mention of lack of simplicity of existing methods makes one assume that the authors would propose a simpler method. Anyway, it is still absed on gradient descent optimization, with the addition of extra steps to ensure clustering after every update of the weights. Furthermore, the memory and algorithmic complexity of the proposed solution increased, making me wonder where the lack of simplicity is addressed. \n\t2. learning without considering underlying data structures (i.e. by similarity) and on fixed dimensional spaces: the authors miss the relation of this paper with a body of work on similarity-based and distance learning, e.g. contrastive learning or arcloss, is not covered. In these works, learning and optimization of parameters is done on the basis of similarity/metric learning, and subsequently decisions are taken by looking at the closest samples in the latent space. Other methods based on prototype learning are also ignored, e.g. [1][2]\n- _experiments should be extended_: methods based on latent similarity metric learning should be compared to, as the principle of classification/prediction for those methods is the same of closest-prototype picking.\n- _result improvements are marginal, with no statistical analysis of differences or details about replication of experiments_: the results are only very marginally above the baselines, and statistical margins are not reported. Are the results from the best runs of training or e.g. median over 5 runs, average over N runs, etc.? It is difficult to evaluate the contribution of the clutering approach.\n- _explainability/interpretability study needs extension and comparative analysis_: other approaches are used to do predictions based on distance/similarity in the latent space (e.g. supervised contrastive learning, circle loss, arcloss) that are not considered in the comparative analysis, also in terms of interpretability of the classifications. \n\n[1] nauta et al. Neural Prototype Trees for Interpretable Fine-grained Image Recognition\n[2] Chen et al. This looks like that: Deep learning for interpretable image recognition",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and the method design is well-constructed with proper motivations of the several choices. Novelty is related to the formalization of the method and is, in my opinion, acceptable for ICLR, but the placement of the work within the state of the art on prototype- and distance-based learning is falling short (see statements above). Some statements, especially in the introduction, are not backed with thorough experimental results or not addressed explicitly in the remainder of the paper. \n\nI trust that reproducibility can be ensured as long as the author would publish their code, as promised.",
            "summary_of_the_review": "The paper presents an interesting idea, well-constructed, and with a clear presentation of the devised methodology. The main concerns with the paper regard placement within the current literature, clarification of several statements made in the introduction and experimental analysis. \n\nThis work seems to relate with metric learning, e.g. contrastive learning, approaches but the authors do not cover this part. Indeed, equation 7 reminds very clearly the 'contrastive loss function' (with the difference that only the min distance is considered in (7)), see [3] equation 1.\n\nThe reported results are only marginally 'better' than baseline networks, but the authors do not compare with other related approaches (mentioned few above), making the evaluation of the contribution of deep prototype learning difficult to appreciate. Also in the discussio nabout explainability, methods based on classification by nearest sample (arcloss/arcface-like methods, contrastive approaches) should have been considered. \n\n[3] https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Understanding_the_Behaviour_of_Contrastive_Loss_CVPR_2021_paper.pdf",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper606/Reviewer_1ipB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper606/Reviewer_1ipB"
        ]
    }
]