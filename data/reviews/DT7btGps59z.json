[
    {
        "id": "z2bFyPl8o7E",
        "original": null,
        "number": 1,
        "cdate": 1666587057434,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666587057434,
        "tmdate": 1669335643427,
        "tddate": null,
        "forum": "DT7btGps59z",
        "replyto": "DT7btGps59z",
        "invitation": "ICLR.cc/2023/Conference/Paper956/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of improving the expressiveness of graph neural networks. In particular, the paper proposes NC-GNN, a GNN training method that incorporates edges among neighbors as additional information for improving the accuracy of GNN. The work also provides some theoretical justification on why the proposed method is more expressive than existing work such as the 1-dimensional WL algorithm, in terms of identifying non-isomorphic graphs. Evaluations of several data sets show that the proposed method can achieve higher accuracy than existing methods such as GCN, GraphSAGE, and GIN.",
            "strength_and_weaknesses": "Strengths:\n- It is an interesting idea to explore how edges among neighbor nodes would affect the effectiveness of GNN. \n- The proposed method is guided by theoretical analysis and the end results demonstrate promising accuracy improvements over existing methods. \n\nWeaknesses:\n- While getting promising accuracy results, the proposed method appears to have a higher memory complexity than existing ones. Therefore, it is less clear why the proposed method is more scalable than existing methods.\n - While getting higher accuracy under a similar amount of trainable parameters, the training time cost of NC-GNN is significantly higher than baseline methods such as GIN. For example, on Pattern and Cluster, NC-GNN incurs 4-6x longer training time, raising the question of whether the improved accuracy is at a cost of a much higher training budget. Also, notably, when the training budget is about the same (e.g., IMDB-B), the accuracy difference is minimal (with no statistical significance). This raises questions on whether the improvement can be achieved by simply training with a larger training budget and some hyperparameter tuning (e.g., learning rate schedules). Without this question answered, one cannot definitely say that considering neighbor node connections is the mere factor for the observed accuracy. Unfortunately, the paper makes no attempt to demonstrate whether that's the case. To be more convincing, it would be better for the authors to compare the proposed method with existing methods under the same training budget (not just the same amount of parameters but also a similar training time) with detailed hyperparameter setups. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n- The problem setup and motivation are clear. The paper also provides details about its methodology. \n\nQuality\n- The writing quality of this paper overall is pretty decent. \n\nNovelty\n- The paper has good discussions in the introduction and related work about its novelty.  \n\nReproducibility\n- The paper does not provide limited details about its implementation and hyperparameter settings. The reviewer is not very confident of reproducing the results with the description. \n",
            "summary_of_the_review": "Despite there being some unanswered questions such as whether the improvement is at the cost of an increased training budget, the reviewer is overall positive about the work given the theoretical justification of its method and the promising accuracy results.\n\n\n==============================\nPost-rebuttal comments:\n\nThe reviewer has read the other reviewers' comments and the authors' responses. The reviewer appreciates the authors for adding additional evaluation results to show the benefits of reduced training time and memory usage than existing work such as GIN-AK+. From that result, the improvement indeed looks quite promising and large. On the other hand, the concerns raised by the reviewer Agxt and other reviewers, such as limited theoretical contribution and the evaluation methodology (e.g., metrics and missing datasets), also seem to be valid. That said, the reviewer finds this work exposes opportunities for further scaling GNN training, so I will keep my score positive. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper956/Reviewer_xnLi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper956/Reviewer_xnLi"
        ]
    },
    {
        "id": "ZtxW3bg9pq",
        "original": null,
        "number": 2,
        "cdate": 1666695144986,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695144986,
        "tmdate": 1666695144986,
        "tddate": null,
        "forum": "DT7btGps59z",
        "replyto": "DT7btGps59z",
        "invitation": "ICLR.cc/2023/Conference/Paper956/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, based on message-passing frameworks, the authors propose a new graph neural network with expressiveness upper-bounded by 3-WL theoretically. Instead of aggregating the 1-hop neighbours, NC-GNN considers the edge between 1-hop neighbours. More importantly, the implementation is very efficient and thus is able to run in large-scale datasets. Experiments demonstrate that the proposed NC-GNN achieves good performance on various benchmarks.",
            "strength_and_weaknesses": "Strength:\n1. This method is simple and efficient.  The superiority of our NC-1-WL lies in improving the expressiveness over 1-WL, while being\nefficient as 1-WL.\n\n2. NC-GNN is provably as powerful as NC-1-WL in distinguishing non-isomorphic graphs.\n\nWeakness:\nAlthough this paper mainly focuses on a GNN for the large-scale dataset. I think a comprehensive comparison is necessary even if the NC-GNN\u2018s performance is not as good as others. GraphSNN is only mentioned in this paper but do not compare in the experiment.  Comparison of real training time also needs to add other methods. In general, I felt that the experimentation was rather inadequate and seemed to deliberately avoid certain methods. This, therefore, reduces the convincing of the experiment.\n\nIn fact and I'm curious as to why NC-GNN didn't try it on the node classification task. In my opinion, ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the quality of the article is good, although the comparison in the experiments is not sufficient. Clarity is fine. I cannot evaluate the originality of the work because I am only knowledgeable on the k-WL problem. ",
            "summary_of_the_review": "As I'm not an expert in the field, I can't give an opinion at this stage as I don't see any hard points. I therefore prefer to refer to other reviewers' comments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper956/Reviewer_zieR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper956/Reviewer_zieR"
        ]
    },
    {
        "id": "W7Hl3FL0XR",
        "original": null,
        "number": 3,
        "cdate": 1666980995258,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666980995258,
        "tmdate": 1669793042820,
        "tddate": null,
        "forum": "DT7btGps59z",
        "replyto": "DT7btGps59z",
        "invitation": "ICLR.cc/2023/Conference/Paper956/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles the challenging goal: developing a general GNN framework with provably expressive power while maintaining the scalability of the message-passing scheme. The authors first model the edges among neighbors as a multiset of multisets and formulate the NC-1-WL graph isomorphism. They show that the expressiveness of NC-1-WL in distinguishing non-isomorphic graphs is strictly above 1-WL and below 3-WL. But based on NC-1-WL, they can propose a differentiable version called NC-GNN, which is intrinsically more scalable than the algorithms that mimic the WL graph isomorphism test. Experimental results on not only graph classification but also node classification tasks show that NC-GNN can consistently outperform GIN by significant margins on various tasks and achieve competitive performance as existing expressive GNNs while being much more efficient.",
            "strength_and_weaknesses": "#### Strengths\n1. To the best knowledge, I think the construction of this modified graph isomorphism test, NC-1-WL is novel. And I like the idea of further modeling the \"edge coloring\" as multisets of multisets, which seems naturally extend the WL-test algorithm. It is not hard to understand its superiority to the standard 1-WL test, while it still only relies on only local operations, in contrast to 3-WL.\n2. The experimental results compared with GIN demonstrate that this added embedding from pairs of neighbors is effective for real-world datasets and not only graph classification but also node classification tasks.\n3. The paper writing is very polished and clear.\n#### Weaknesses\n1. I think the experimental comparison to subgraph-based GNNs is very insufficient in the current manuscript. The authors only show the performance comparison to subgraph-based GNNs on the TUDatasets, which are mostly small graphs and are gradually replaced by OGB (Hu et al., 2020) and GNN Benchmark (Dwivedi et al., 2020). Thus it would be interesting and important to see more comparisons to subgraph-based baselines on those datasets. More importantly, one major advantage of the proposed NC-GNN is the scalability/efficiency. So the authors should expand Table 5. to compare with subgraph-based GNNs in terms of the epoch-time, memory usage, convergence speed, etc.\n2. I think the related work section needs some expansion to discuss more prior papers (apart from subgraph-based GNNs) which (1) propose more powerful GNNs, maybe not in the sense that strictly better than 1-WL, while maintaining the scalability (e.g., [1]) or (2) follow the WL-hierarchy and are probably better than 1-WL, while being much more efficient than mimicking the k-WL test algorithm (e.g. [2]). Recently there have been many papers trying to tackle these two directions, and I hope the authors could do a more careful literature review of those papers.\n\n#### Minor Issues\n1. I suggest the authors color the reference links.\n\n##### References\n- [1] Papp, P. A., Martinkus, K., Faber, L., & Wattenhofer, R. (2021). Dropgnn: random dropouts increase the expressiveness of graph neural networks. Advances in Neural Information Processing Systems, 34, 21997-22009.\n- [2] Puny, O., Ben-Hamu, H., & Lipman, Y. (2020). Global attention improves graph networks generalization. arXiv preprint arXiv:2006.07846.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is well-written, and I believe in the correctness of the theoretical results (although I did not check the proofs in detail).\n2. To the best knowledge, I think the proposed algorithm is novel. However, the empirical novelty and significance of the contribution are limited due to the insufficient comparison to existing baselines, including subgraph-based GNNs. \n3. Since the authors did not share the code anonymously so far, I cannot verify the reproducibility. I hope the authors can share the code later if accepted.",
            "summary_of_the_review": "Overall for the current manuscript, I recommend weak rejection. I like the idea of the proposed NC-1-WL test and the corresponding NC-GNN algorithm. However, since recently there have been many papers on provably powerful while scalable GNNs, the current insufficient experimental comparison to those work, including subgraph-based GNNs, on larger graphs, and in terms of efficiency, cannot support the claimed contributions of NC-GNN well.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper956/Reviewer_nYWZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper956/Reviewer_nYWZ"
        ]
    },
    {
        "id": "nrcINoNv5G2",
        "original": null,
        "number": 4,
        "cdate": 1666998104363,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666998104363,
        "tmdate": 1667485356698,
        "tddate": null,
        "forum": "DT7btGps59z",
        "replyto": "DT7btGps59z",
        "invitation": "ICLR.cc/2023/Conference/Paper956/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a simple extension to current message passing based GNN, inspired from recent subgraph GNNs. Although being less powerful than subgraph GNNs, the proposed method is efficient with limited computation overhead comparing with message passing GNNs. The author demonstrates its improvements over many real-world datasets.",
            "strength_and_weaknesses": "**Strength**:\n1. Simple, and fast. \n2. Effective in several real-world datasets. \n\n**Weakness**:\n1. Not novel. The design is too incremental comparing with these Subgraph GNNs. Although being simple and fast. \n2. It is just a special case of recently neurips 2022 paper [How Powerful are K-hop Message Passing Graph\nNeural Networks], with K=1. The NeurIPS paper is general and with extensive theoretical analysis. In this paper I can only find the simplicity as a contribution. The incremental design doesn't help the research area. \n3. All theorems are mentioned in existing papers. The 3-WL bound is inside [Understanding and extending subgraph gnns by rethinking their symmetries]. \n4. Experimental evaluation misses too many baselines. First, we all know that TUDataset is not good for evaluating expressivity. Second, widely used ZINC is missing. Third, performance of NestedGNN, GNN-AK, ESAN are not inside the paper. Of course the performance shouldn't be higher than them, but the author should include and also include run-time comparison. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Presentation is clear. ",
            "summary_of_the_review": "The author proposes a simple add-on to the current message passing based GNN with the same intuition mentioned in [Zhao et al. 2021] but more much efficient implementation (with being less powerful), however this efficient implementation is also covered by a recent paper [Feng et al. 2022] . Given all these existing works, I can hardly find enough contribution in this paper. (If this paper is designed 1-year eariler it's possible to be novel, however currently I really cannot find novelty.)\n\n\n\n\n[Zhao et al. 2021] From stars to subgraphs: Uplifting any GNN with local structure awareness.   \n[Feng et al. 2022] How Powerful are K-hop Message Passing Graph Neural Networks\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper956/Reviewer_Agxt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper956/Reviewer_Agxt"
        ]
    }
]