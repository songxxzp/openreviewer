[
    {
        "id": "3FtxHGTRZx",
        "original": null,
        "number": 1,
        "cdate": 1666239721455,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666239721455,
        "tmdate": 1666239721455,
        "tddate": null,
        "forum": "djfoLX57p9L",
        "replyto": "djfoLX57p9L",
        "invitation": "ICLR.cc/2023/Conference/Paper4864/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors construct a dataset of sentences paired with a \u201cvisualness\u201d label, indicating if the sentence evokes visual imagery. One part of this dataset is automatically synthesized by processing PDFs and computing CLIP scores between sentences and images in the PDFs. Another part of the dataset consists of sentences (also sourced from PDFs), that are then labeled by AMT workers for \u201cvisualness.\u201d \n\nThe authors then use this dataset to further finetune a CLIP model to match \u201cnon-visual\u201d sentences with a \u201cNULL\u201d image. This model is compared to various strong baselines, and shown to be the best of all considered methods. Various ablations and analysis are also conducted, such as measuring the correlation of BERT and CLIP models trained on the dataset to MRC scores, and ablating out automatic or human-labeled portions of the dataset for finetuning. Qualitative analyses of the attention of TIP-CLIP are also presented, as well as DALLE generations conditioned on \u201cvisual\u201d and \u201cnon-visual\u201d text.",
            "strength_and_weaknesses": "Strengths:\n\nComprehensive analysis of TIP-CLIP, with good ablations of training methods, good comparison to existing MRC scores, and interesting visualizations. Appendix section contains even more analyses of the dataset, the ablations, the NULL image, and the TIP-CLIP model.\n\nThe baselines seem quite strong, and TIP-CLIP is able to outperform all of them. \n\nA dataset of sentences with \"visualness\" labels is novel (to the best of my knowledge). \n\nWeaknesses:\n\nIt's not clear to me that the data collection procedure should produce sentences that are really \"non-visual\" if the criteria for \"non-visualness\" is \"CLIP similarity with all images on page less than T_{neg}.\" For example, it seems possible to me that I can have a very visual sentence that just happens to not match any image on a page in a pdf.\n\nThe novelty and usefulness of the TIP-CLIP model is a tad unclear. It's just CLIP trained with an additional NULL image tag, and as mentioned in the paper one could easily just train a binary classifier. The justification for using TIP-CLIP rather than a binary classifier is that TIP-CLIP gives us representations that can be used for image retrieval, but TIP-CLIP actually underperforms CLIP in this task (MRR of 0.937 against 0.989). Relatedly, it seems that a binary classifier was trained and the F_1 score is presented, but not any other results. Could this be added to table 2?\n\nThe baselines are all very reasonable, but I'm surprised that the pre-trained CLIP model performs so well. My understanding is that it's a vanilla CLIP model and has therefore never seen the NULL image, which is why I find it interesting that it should perform so well. Is my understanding correct? And if so how come this model does so well?\n\nThe analysis of the TIP-CLIP model is well done, but I have a few questions about the qualitative results. In figure 2 the image embeddings and the text embeddings in the T-SNE plot seem disjoint, whereas I was under the impression that the contrastive loss used to train the CLIP model would bring these two clusters together. Is this simply how CLIP representations look? Or does this indicate a problem with the dataset? Or perhaps this is a problem with T-SNE hyperparameters?\n\nAdditionally, how was the attention calculated for figure 3 and 4? Anecdotally, I think attention in language models are all over the place, so I don't know how much weight should be put into these visualizations.\n\nVery small nitpick: I^e_m in equation 1 is defined self-referentially",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: very clear, well written and well motivated paper.\n\nNovelty: I think the dataset and the idea of sentence \"visual-ness\" is novel. I think the TIP-CLIP model has more marginal novelty.\n\nReproducibility: I did not find any code, but I believe the details in the paper are sufficient to reproduce the TIP-CLIP result. The details of the construction of the dataset are also well documented in the paper.",
            "summary_of_the_review": "This paper poses an interesting problem: measuring the \"visualness\" of a sentence. The dataset presented is novel, but I have some slight concerns on the TIP-CLIP model and the dataset construction (see weaknesses section) that I would like the authors to address.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4864/Reviewer_yCLp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4864/Reviewer_yCLp"
        ]
    },
    {
        "id": "Meg5HMwX-Z",
        "original": null,
        "number": 2,
        "cdate": 1666646233876,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646233876,
        "tmdate": 1666646233876,
        "tddate": null,
        "forum": "djfoLX57p9L",
        "replyto": "djfoLX57p9L",
        "invitation": "ICLR.cc/2023/Conference/Paper4864/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is very interesting. It studies the visualness of text based on a simple example. Some words can incur our imagination for the corresponding image, while the others might not. With this motivation, the author proposed a new dataset called TimeD with both manually labeled data and the automatically model labeled data. The author also proposed a fine-tuning strategy that tunes the CLIP model to predict the visualness given a text. The whole paper is very fun to read.",
            "strength_and_weaknesses": "The idea is very interesting and novel to me. I like the motivation example and the dataset collection part. Especially using the CLIP to automatically labels the relatedness between image and text and the human annotated data to further enhance the labeling quality is pretty clever. \n\nBut I have concerns with the fine-tuning stage.\n1. The negative text is aligned with a NULL image. Given a batch of positive and negative text, and a set of positive images and NULL images. I wonder are the NULL images identical to each other?\n2. If no, during different training iterations, suppose we sample a text which has been sampled before, would the NULL images looks different comparing to the previous iteration?\n3. If all the negative images looks the same in the training batch, would the contrastive learning loss work? Basically the negative text is asked to aligned with a NULL image and not aligned with the same NULL image in this batch.\n4. If all the negative images are different in the training batch, I doubt the contrastive learning would work. Basically, the logic follows. The negative text is asked to be close to one of the NULL image, but be far away from the other NULL images.\n5. As the TIP-CLIP is always fine-tuned on the pre-trained CLIP model. I wonder how much would the pre-trained CLIP contribute? Did the author try to train from scratch on the proposed dataset? Due to lack of the data, this might be done with a smaller model.\n6. For implementation detail, the model is fine-tuned on a batch size of 32 with one GPU. As the CLIP would achieve quite low performance on smaller batch size. I wonder if this is a typo or this is designed on purpose?\n\nFor Fig 1.a, the figure is rendered incorrectly on my PDF reader (I tried multiple ones) and my web browser. Only part of the image has been rendered. I wonder is this a PDF issue or the figure was drew on that way on purpose.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is easy and interesting to read.\nThe proposed dataset is pretty novel to me. Although I have concerns with the proposed fine-tuning stage approach.\nWith the publicity of the dataset, I think this paper is pretty straightforward to implement.",
            "summary_of_the_review": "I think the paper is interesting and the idea is novel. However I have concerns with the proposed fine-tuning stage approach. I am happy to raise my rating if my concern is resolved during rebuttal. For this stage, I will give the rating of 5.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4864/Reviewer_gb6o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4864/Reviewer_gb6o"
        ]
    },
    {
        "id": "Fli4Tp9Te9f",
        "original": null,
        "number": 3,
        "cdate": 1666832672519,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666832672519,
        "tmdate": 1666832672519,
        "tddate": null,
        "forum": "djfoLX57p9L",
        "replyto": "djfoLX57p9L",
        "invitation": "ICLR.cc/2023/Conference/Paper4864/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper aims at learning whether a certain text is visualizable or not as generative models such as DALLE-2 are becoming very popular. To this effort, the authors curate a dataset called Text Imageability Dataset (TIMED) which contains 3620 sentences with label for whether it is a visual or non-visual text. The authors also curate an automatic dataset with similar labels and show that when a customized CLIP model is trained on a combination of these, the model is able to perform better than a lot of other baselines. Overall, this paper focuses on a niche area of research of visualness of text and provides a solution for it but there a lot of caveats as discussed in rest of the review.",
            "strength_and_weaknesses": "### Strengths\n- The paper provides a new dataset for evaluation and an automatic dataset for training a model for the task of testing the visualness.\n- The paper also provides a method to train CLIP model to detect whether a text is visual or not. \n- Provides ablations on the choice of the training objective for the purpose of testing visualness.\n- Clearly written and easy to read.\n- Ablations on MRC-I score and efforts to improve them using word embeddings.\n- Disentangles the effect of multi-stage training and shows that both datasets are needed to reach optimum performance.\n\n### Weaknesses\n- The motivation of problem statement is weak. The paper doesn't focus on why we need to solve the problem but mostly focuses on whats and the hows. Is it even needed to solve this problem? There is no discussion on how this approach can actually improve the current generative models by incorporating the visualness into the pipeline. The data stable diffusion is trained on (LAION) is already filtered out using CLIP scores which makes this process redundant. The question is whether we want to do automatic filtering on the fly during the training or beforehand as done in stable diffusion. I also don't understand why a human will ask the model to generate a non-visual text. Overall, I am not sure why this problem is important to solve.\n- I expected more analysis on the different backbones. For example, DeCLIP [1] performs better overall, FLAVA [2] and Unified-IO [3] have a better text encoder. It is hard to take away any conclusions from a single backbone test.\n- BERT (full) performs better than TIP-CLIP model when only trained on auto-labeled data. It is unclear whether the difference is due to pretraining objective? Maybe using VisualBERT-style [4] model would have disentangled that?\n- The downstream impact of the pretraining CLIP as TIP-CLIP-style objective is unclear as the only evaluation that has been conducted is on text-to-image retrieval and that too again on a subset that has been already filtered out by CLIP. There is a compounding effect of problems here. A clear comparison would be on rather MS-COCO or Flickr30k retrieval along with other tasks used in CLIP paper. The claim that the model has almost same performance on retrieval would strengthen if we have those results.\n- No other stronger text backbones have been used except BERT. Comparisons with RoBERTa/T5 would be useful to take a call.\n- Another compounding effect in the dataset is the fact that CLIP was used to filter out the matching images with custom threshold. \n- It is unclear how scene text fits into this picture. If the models get good at generating scene text, they can directly replicate non-visual text as scene text on a pdf page or a bland background. Overall, visualness is a subjective term changing from person to person and formulation of the problem.\n\n[1] Li, Yangguang, et al. \"Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.\" arXiv preprint arXiv:2110.05208 (2021).\n\n[2] Singh, Amanpreet, et al. \"Flava: A foundational language and vision alignment model.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[3] Lu, Jiasen, et al. \"Unified-io: A unified model for vision, language, and multi-modal tasks.\" arXiv preprint arXiv:2206.08916 (2022).\n\n[4] Li, Liunian Harold, et al. \"Visualbert: A simple and performant baseline for vision and language.\" arXiv preprint arXiv:1908.03557 (2019).",
            "clarity,_quality,_novelty_and_reproducibility": "- How were the documents filtered out to only include brochures, flyers, and inforgraphic and academic figures?\n- Clustering of all of the non-visual text into a single NULL cluster may have downstream impact on text encoder. Does image-to-text retrieval performance decrease on other tests? FLAVA-style backbone where text encoder is stronger would be a better test.\n- Selection of pdfs vs webpages is ambiguous. Could it be done with web pages as well with dom diff 1? This might cause the results to be limited to a specific domain?\n- Was there a hyperparameter search conducted on BERT? If not, it is hard to conclude anything from one particular set of hyperparameter when number are very close. Was there a statistical significance test run?\n- Why are the numbers for BERT in Table 4 row 3 different from BERT in Table 2.\n",
            "summary_of_the_review": "The paper focuses on a niche problem but doesn't discuss its significance and how it can help us build better model instead focuses on how to solve the problem. There is a lack of ablation study around backbones, objectives and text encoders which makes it hard to take away any significant lessons. The limitations imposed by the compounding effect of filtering using CLIP are prevalent and need to be disentangled. Given these concerns, I suggest a rating of ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4864/Reviewer_oEJV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4864/Reviewer_oEJV"
        ]
    }
]