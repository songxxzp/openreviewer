[
    {
        "id": "dkI4nGcKi7",
        "original": null,
        "number": 1,
        "cdate": 1666320254805,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666320254805,
        "tmdate": 1666320254805,
        "tddate": null,
        "forum": "njAes-sX0m",
        "replyto": "njAes-sX0m",
        "invitation": "ICLR.cc/2023/Conference/Paper955/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "PATCorrect is a non-autoregressive (NAR) error correction module.  PATCorrect is distinguished by using aligned phoneme and word inputs for making correction decisions.  Evaluation across a number of input ASR models demonstrates that PATCorrect outperforms FastCorrect, a similar NAR correction model which does not use phone information.",
            "strength_and_weaknesses": "Strength\n* The use of phonetic information in error correction is well motivated.\n* The architecture is well described and well motivated.\n\nWeakness\n* The improvements offered by PATCorrect are somewhat modest.  There is a small, but consistent improvement over FastCorrect.  But the performance is still substantially worse than AR Transformer. \n* The reporting of performance is, at some points, strained.  1) The decision to use F_0.25 instead of a more common detection metric of F_1 is surprising, but the authors gesture toward the bias toward Precision.  However, this decision is less convincing when observing that FactCorrect has a better F_1 score than PATCorrect, though PATCorrect has a higher F_0.25 score.   2) The reported improvement of 20% in the abstract and introduction is a relative improvement to the relative improvement to WER (the primary metric for ASR).  This is not clear in the abstract.  The absolute WER difference between FastCorrect and PATCorrect is between 1 and 2%. WERR (relative improvement to WER) may be a more relevant measure for error correction, however, since this is already a relative measure, absolute difference to WERR are a more interpretable description of performance differences.  The underlying work is strong, but these presentation decisions leave the impression that the work is presented in an overly optimistic manner.",
            "clarity,_quality,_novelty_and_reproducibility": "* Mostly quite clear.  One clarification question: Where does the phone sequence used for correction come from?  Is it generated from the ASR system directly, or is it a post-processing of the ASR hypothesis?  Also, Is there any benefit from consuming an N-best list of hypotheses from the ASR models?  While not all ASR models generate phone sequences, some do (or can be trained to simultaneously with words).  All decoders can produce N-best lists.\n\nSince the models explored are full-context models (i.e. not streaming), it's not clear that latency is critical to their performance.  Low latency error correction would be more compelling on a streaming model.\n\nQuality: Strong\nNovelty: Sufficient Novelty\nReproducibility: Could reproduce.",
            "summary_of_the_review": "The central contribution of the work is strong, and clearly described.  The evaluations themselves are good.  However, the presentation could be more tempered to more accurately demonstrate the strengths and weaknesses of the approach.  A stronger motivation for the use of NAR error correction will also provide additional context for the contribution of the work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper955/Reviewer_XMid"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper955/Reviewer_XMid"
        ]
    },
    {
        "id": "SJ1PbQlzX9",
        "original": null,
        "number": 2,
        "cdate": 1666531818391,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666531818391,
        "tmdate": 1666531914211,
        "tddate": null,
        "forum": "njAes-sX0m",
        "replyto": "njAes-sX0m",
        "invitation": "ICLR.cc/2023/Conference/Paper955/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The task is automatic speech recognition (ASR). The paper proposed PATCorrect, a new correction model which operates on the output of another speech recognition model and tries to improve the output by correcting errors.\n\nThe proposed model works in a non-autoregressive way and thus can run in parallel, efficiently on hardware like GPUs.\n\nIt uses three different pretrained models to test the correction model on. The best model is a Conformer, which gets 10.03% WER on CommonVoice Test without correction, and the best PATCorrect correction gets down to 9.78% WER. The other models are very weak in comparison, but then the relative improvements are larger.",
            "strength_and_weaknesses": "Strengths:\n\n- An interesting novel model for error correction.\n- I think the use of phoneme inputs is novel.\n\nWeaknesses:\n\n- No comparison to a standard language model fusion? See below.\n- The code is not published?\n- The analysis and ablations study is too short.\n- Ablation studies only with weak model.\n- Only tested on speech recognition.\n- Many aspects are unclear. See below.\n- Studies are on Common Voice, but LibriSpeech would be better, as this is what most people know much better.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Where does the phoneme sequence come from?\u00a0It seems just from the recognized word sequence. So no pronunciation variants? Also, why this way, why not use the ASR encoder output or so? Also, how relevant are the phonemes anyway? There is a bit of ablation study on this, but this comes very short, and was only done for some of the very weak models. It should be done for Conformer.\n\nRelated work: It only addresses error correction. But just shallow fusion / log-linear combination with a standard language model (LM), how is that really different? As far as I know, not much really can be gained from such error correction models over standard LM fusion with a good LM.\n\nIn general, this is really an important comparison, to make use of an external LM, in comparison to such error correction, or in combination, or other variations. Further, it makes sense to test different model architectures, such as CTC, Transducer or Attention-based Encoder-Decoder.\n\nIt seems the error correction model is just trained on transcriptions, not more. So this is much weaker than a LM trained on text-only data, which is usually available in much larger quantities. But the error correction model could maybe also trained on text-only data? This should be addressed.\n\nIt's even not clear, those pretrained models, are those Transducer, CTC, att-based enc-dec?\nHow is decoding done? Unclear.\n\nPretrained models, but not explained how pretrained or which pretrained models exactly? Are they public?\n\nSpeed comparison: I don't really know: Does this only measure PATCorrect itself, or the whole decoding? What is actually the underlying model and decoding procedure?\n\nLatex wrong math usage, use \\operatorname for all whole-word-functions like softmax, Addition, Max, etc.\nLatex wrong math usage, use \\textrm or so for text subscripts like PATCorrect etc.\n",
            "summary_of_the_review": "There are too many weaknesses, as explained above. This needs more work.\n\nIn general, this paper seems more appropriate for a speech conference (Interspeech etc)? While in principle it might be applicable to other tasks, I think this should be tested, and I think this is necessary for publication on ICLR. The novelty and scope is otherwise clearly not high enough. Otherwise, even for speech conferences, I think the quality is not high enough yet, and this needs to address the weaknesses I explained before.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Ok.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper955/Reviewer_9fDp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper955/Reviewer_9fDp"
        ]
    },
    {
        "id": "gBLeWr2nJZ",
        "original": null,
        "number": 3,
        "cdate": 1666573069288,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573069288,
        "tmdate": 1666639808923,
        "tddate": null,
        "forum": "njAes-sX0m",
        "replyto": "njAes-sX0m",
        "invitation": "ICLR.cc/2023/Conference/Paper955/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a new non-autoregressive model that incorporates the pronunciation information with text information for the task of ASR error correction. The proposed model uses a transformer architecture for two encoders and a decoder, it adopts three different fusion approaches to combine the representations from phoneme encoder and text encoder for the target length prediction. The decoder computes the cross attention with both encoders sequentially and predicts the target words. ",
            "strength_and_weaknesses": "Strength:\n\n1.The idea of phoneme fusion and cross attention strategy is simple yet effective. The model outperforms FastCorrect on English datasets, and achieves SOTA results on English corpus.\n\n2. The paper compares three feature fusion approaches, and shows that cross attention is effective for text and phoneme features.\n\n3. Experimental results are based on comparisons on 3 English ASR systems to show the robustness of the proposed approaches.\n\nWeakness\uff1a\n1. A Phoneme MHA module is added behind the original Text MHA module in the Transformer decoder block, but the ablation study shows that without this phoneme attention, the model performance roughly remains the same as adding it. So with more calculation on the phoneme attention, the model barely gets any improvement from it.\n2. The improvements on some datasets are minor. All of the fusion approaches of combining the phoneme embedding and text embedding don\u2019t consider the relationship between each word and its corresponding pronunciation, which may be important for the words error correction.\n3. The novelty of proposed framework is not significant. The architecture does not seem that there is too much difference with FastCorrect except the phoneme embedding, and two extra cross attention. Besides that, model architecture and train process are almost the same.\n4. Phoneme feature is commonly used in error correction [1, 2], and from the ablation experiments we see a lot of contributions are from phoneme feature. To my knowledge cross-attention feature fusion can be seen in Image field previously [3].\n\n[1] Wang et al., ASR Error Correction with Augmented Transformer for Entity Retrieval. Interspeech 2020.\n\n[2] Fang et al., Non-Autoregressive Chinese ASR Error Correction with Phonological Training. NAACL 2022.\n\n[3] Bai et al., TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers. CVPR 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the proposed multi-modal fusion method is easy to follow. But compared with FastCorrect, the novelty is quite limited. \n",
            "summary_of_the_review": "This paper proposes a network of NAR Transformer with a phoneme encoder. Compared with FastCorrect, the improvement is limited, but it requires more latency than FastCorrect. Besides, the idea of incorporating phoneme information to gain better ability of error correction is useful but not very creative, since many text correction approaches have been proposed in recent years with similar ideas.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper955/Reviewer_RfZS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper955/Reviewer_RfZS"
        ]
    }
]