[
    {
        "id": "dZoUW1efPYn",
        "original": null,
        "number": 1,
        "cdate": 1666640641596,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640641596,
        "tmdate": 1666640641596,
        "tddate": null,
        "forum": "uATOkwOZaI",
        "replyto": "uATOkwOZaI",
        "invitation": "ICLR.cc/2023/Conference/Paper1155/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of entropy-regularized optimal transport (EOT). The authors advocate the usage of L-BFGS algorithm to solve the EOT problem instead of the popular Sinkhorn algorithm. They analyze the convergence of L-BFGS on EOT and demonstrate its better stability compared to the Sinkhorn algorithm. They also derive a closed form of the gradient of the Sinkhorn loss w.r.t the cost matrix, leading to efficient computation of the backward pass. Finally, we illustrate their results with numerical examples.",
            "strength_and_weaknesses": "**Strength:**\n1. The problem studied is a highly relevant problem.\n1. They provided a convergence analysis of the L-BFGS method on the EOT problem.\n1. The advocated L-BFGS + analytic gradient is computationally more efficient than other methods with which they compare.\n1. The paper is well-written and easy to follow.\n\n**Major comments:**\n1. The novelty of the paper is not super clear to me.\n    1. As mentioned by the authors, using L-BFGS to solve the EOT problem is a known practice. One selling point of this paper is the convergence and stability analysis of this algorithm (Theorems 2 - 4). However, this seems to be a practice problem given that L-BFGS has been extensively studied in other (potentially broader) settings. Is the proof technique new? Moreover, why can you conclude $T^{(k)}$ does not underflow from (e)? Don't you need to prove a lower bound for $min T^{(k)}$?\n    1. Cuturi et al. (2019) and Cuturi et al. (2020) consider gradients w.r.t data points. Since the cost matrix is constructed from $c(X, g_\\theta(Z))$ in the generative modeling application, knowing the gradients w.r.t data points is enough to get the gradients w.r.t $\\theta$. Can you also compare your approach with theirs?\n    1. You missed an important reference on this topic [1].\n1. The numerical experiments are not convincing. In particular,\n    1. Fig. 2 does not provide a fair comparison. As mentioned by the authors themselves, one iteration of the L-BFGS algorithm is more time-consuming than one iteration of the Sinkhorn algorithm. Why not use a similar plot as in Fig. 3?\n    1. In Table 2, what are the stopping criteria for each of the methods?\n    1. In Sections 7.2 and 7.3, while the goal is to compute the Sinkhorn loss, the performance is measured in Wasserstein distance. This mismatch makes it difficult to evaluate the improvement. I would use the Sinkhorn divergence (Feydy et al. '19) instead since this value should decrease as well.\n1. I would like to see the full algorithm of the advocated forward and backward passes with complexity analysis.\n\n[1] Tianyi Lin, Nhat Ho, Michael I. Jordan. On the Efficiency of Entropic Regularized Algorithms for Optimal Transport. JMLR (2022).\n\n**Minor comments**\n1. The notation $\\mu^{-}$ is a bit confusing. I would use $\\mu^{-1}$ instead.\n1. It would be better to define $s_v$ directly rather than $\\tilde s_v$.\n1. For matrices $M$ and $T^\\star$, their tilde versions are not defined.\n1. It would be interesting to know how the marginal constraints of $T^{(k)}$ change with the number of iterations in Theorem 3.\n1. In Theorem 4, how large is $\\sigma$ typically?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow, and the authors provided code to reproduce their results. However, I have concerns about the quality and novelty of the work as detailed in my major comments in the \"Strength and Weakness\" section.",
            "summary_of_the_review": "The paper studies a relevant problem and provides some new insights. However, I have some concerns about the quality and novelty of the work as detailed in my major comments in the \"Strength and Weakness\" section. I thus recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1155/Reviewer_eynF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1155/Reviewer_eynF"
        ]
    },
    {
        "id": "1MNttDvJ-J",
        "original": null,
        "number": 2,
        "cdate": 1666683870373,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683870373,
        "tmdate": 1666683903052,
        "tddate": null,
        "forum": "uATOkwOZaI",
        "replyto": "uATOkwOZaI",
        "invitation": "ICLR.cc/2023/Conference/Paper1155/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to use L-BFGS algorithm to solve the OT problem by exploiting a loose condition in the Sinkhorn algorithm. They derive a formula to compute the derivative with respect to cost matrix of the transportation cost. Then they prove some results of the convergence of their proposed algorithm.\n",
            "strength_and_weaknesses": "Strength:\n\n1. The paper presents some useful results (Theorem 1) of computing derivatives of the OT cost between two distribution with respect to the cost matrix. The results could be used in some important settings, i.e generative modeling.\n\n2. The author also presents some theoretical results to support their arguments. More particular, the convergence rate of the objective function, bound of the derivative etc. Those results are presented in Theorem 2,3 and 4.\n\n3. Emperical results show that the method work well and faster than other methods. \n\nWeaknesses:\n\n1. Theorem 2,3 and 4 are helpful but not useful. In particular, index $I$ in Theorem 2 depends on the solution. The rate $r$ in Theorem 3 is unknown, it is supposed to be dependent on $\\lambda$.  The same situation in Theorem 4, where $\\sigma$, $C_S, C_v$ and $C_u$ are difficult to quantified as a function of the original parameters: the marginal distribution  $a$, $b$ and the cost matrix $M$. That makes it very difficult to compare the order of the complexities of this method and the Sinkhorn algorithm in order to answer the question if this method is faster because of its initialization or it has a larger step to the optimal value etc. Note that there are already some works [1] to show the linear convergence of the objective function of the Sinkhorn algorithm, but it does not improve the upper bound of its complexity [2]. \n\n2. Empirical results are limited to some toy examples and small data set (MINIST).\n\nReferences\n\n [1] Vladimir Kostic, Saverio Salzo and Massimiliano Pontil. Batch Greenkhorn Algo- rithm for Entropic Regularized Multimarginal Optimal Transport: Linear Rate of Con- vergence and Iteration Complexity. Proceedings of the 39th International Conference on Machine Learning, PMLR 162:11529-11558, 2022.\n\n[2] Dvurechensky, P., Gasnikov, A., and Kroshnin, A. Com- putational optimal trans- port: Complexity by accelerated gradient descent is better than by Sinkhorn\u2019s algorithm. In International conference on machine learning, pp. 1367\u2013 1376, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "It is well-written paper, easy to follow.",
            "summary_of_the_review": "Results of Theorem 1 is useful in application such as generative modeling, but other results are not. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1155/Reviewer_7wxm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1155/Reviewer_7wxm"
        ]
    },
    {
        "id": "OvMIDZ1uK8_",
        "original": null,
        "number": 3,
        "cdate": 1666700788745,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666700788745,
        "tmdate": 1666700788745,
        "tddate": null,
        "forum": "uATOkwOZaI",
        "replyto": "uATOkwOZaI",
        "invitation": "ICLR.cc/2023/Conference/Paper1155/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides an analytic overview of the ``sharp Sinkhorn loss'' and its derivatives with respect both the input weights and the cost matrix (that typically depends on the input locations). Formally, given two discretes probability measures $\\mu = \\sum_i a_i \\delta_{x_i}$ and $\\nu = \\sum_j b_j \\delta_{y_j}$, and a cost $M = (|x_i - y_j|^2)_{ij}$ (or actually any other expression), the considered loss reads\n\n$$ S(M,a,b) = \\braket{T^\\star, M} $$\n\nwhere\n\n$$T^\\star \\in \\mathrm{argmin} \\braket{T, M} - \\eta h(T), $$\n\nwhere $\\eta > 0$ is a regularization parameter, $h$ denotes the entropy function, and the minimization is performed over the transportation polytope between $\\mu$ and $\\nu$ (that, in this discrete setting, only depends on $a = (a_1,\\dots, a_n)$ and $b = (b_1,\\dots, b_n)$). A well-known fact is that $T^\\star$ can be computed (or at least, estimated) by running the Sinkhorn algorithm, an iterative fixed-point procedure. \n\nIn applications (especially in generative models), it is often needed to compute gradients of $S$ with respect to $a,b$ and/or $M$. However, the latter is typically obtained via ``unrolling'' the iterations of Sinkhorn (via automatic differentiation). In contrast, this work computes $T^\\star$ relying on a L-BFGS approach instead of the usual iterative loop. \n\nWhile using L-BFGS to get $T^\\star$ is not entirely new, this paper has the following main contributions : \n- They study precisely the analytical behavior of the L-BFGS algorithm. The main takeaway are that convergence rates are (asymptotically) the same (i.e. geometric convergence), but the L-BFGS approach can be provably stable. This is an advantage over the standard Sinkhorn loop, and while unstability of the Sinkhorn loop is typically handled by performing iteration in log-domain, this comes at the price of computational efficiency. Here, the proposed approach gets the best of both world, being stable while remaining efficient. \n- An analytic expression of $\\nabla_M S(M, a,b)$ has been derived, allowing \"direct\" differentiation rather than unrolling the iterations when---for instance---training a generative model.\n- Showcase their approach through some numerical experiments. ",
            "strength_and_weaknesses": "## Strengths\n\nThis is a solid work overall. I particularly appreciate that algorithmic aspects (convergence, stability) are supported by precise theoretical results (and not just empirical ones / informal intuition). \nI also think that the main idea conveyed by the paper may be impactful when it comes to use OT in ML tasks. \n\n## Weaknesses\n\n- My main concern is that, in some sense, focusing on the loss $\\braket{T^\\star, M}$ feels somewhat ``outdated''. While it is the seminal loss proposed by Cuturi in 2013, many variants/improvements have been considered since. In particular, I would think that the so-called _Sinkhorn divergence_ [1,2,3] have taken the lead when it comes to introduce OT-based losses in ML. While I guess that a vast majority of this work holds/adapts to that setting, I think it would have been arguably better to focus on it. If there is a precise reason to focus on $\\braket{T^\\star, M}$ rather than the Sinkhorn divergence, this should be discussed in the paper. \n- One may argue that, although fairly convincing, the experimental section is not ``groundbreaking'' \n\n## Complementary comments/suggestions/questions\n\n- I think that few statements in the introduction should be made more precise. For instance, \"the computational cost of OT quickly becomes formidable\" --> give the precise complexity; similarly, it would be nice to recall what is the best theoretical complexity known for the Sinkhorn algorithm. \n- It is slightly inaccurate to talk about \"**The** optimal solution to (1)\" as it may not be unique (in contrast to those of (2), as highlighted after in the paper). \n- [minor suggestion] Of course, that is a matter of taste, but I think that using $\\eta = \\lambda^{-1}$ would alleviate notations and has become more standard in computational OT literature. That would disambiguate claims like \"Sinkhorn-type algorithms may be slow to converge, especially for small regularization parameter\" (here, one may think that \"small regularization parameter\" refers to \"small $\\lambda$\", while if my understanding is correct, this should be understood as \"small $\\lambda^{-1}$\"). \n- Formally speaking, I think that (5) is a _concave maximization problem_ (but of course this is equivalent to a convex minimization problem). \n- In Theorem 1, $s_v$ has not been defined if I am correct. \n- \"exponentiation operation are more expensive than matrix multiplications\", is that correct? I would expect term-wise exponentiation to be faster as it is $O(n^2)$ vs $O(n^3)$ overall (unless \"matrix multiplications\" refers to \"matrix-vector multiplications\", but even there I would expect the complexity to be at least similar). I also think that both operations can be efficiently performed on a GPU. \n- [question] In Theorem 1 and 4, what guarantee that $D$ is invertible (and that $\\lambda_{\\min}(D) \\neq 0$)? I tried to quickly check the appendix but did not find any discussion on that topic (I may have missed it though). \n- [suggestion/question] In Theorem 4, it may be more \"reassuring\" to state/recall that the condition $\\epsilon^{k_0} < ... $ is always met since $\\epsilon^{(k)} \\to 0$ (and is decreasing); assuming the rhs term is not $0$. Related to the above question, do we have any control on how long it takes for this condition to be satisfied? \n- In numerical experiments, it is observed that when the regularization parameter $\\lambda^{-1}$ is too large, $S$ is a poor approximation of $W$ in sense that it does not go to zero in that setting. In my opinion, this is an additional argument in favor of using the Sinkhorn divergence $\\mathrm{SkDiv}$: while for large regularization parameters these divergences will naturally differ from exact OT in general, one still has $\\mathrm{SkDiv}(\\mu,\\nu) = 0 \\Leftrightarrow \\mu = \\nu$ (and more precisely, they metricize the same topology as $W$ for any $\\lambda$), so they may be more suited to study the expressiveness/convergence of generative models. \n- [typo] In the appendix, both section B.4 and B.5 are called \"proof of Theorem 3\". \n\n## References \n\n- [1] _On wasserstein two-sample testing and related families of nonparametric tests_, Ramdas et al.\n- [2] _Learning generative models with sinkhorn divergences_, Genevay et al.\n- [3] _Interpolating between optimal transport and mmd using sinkhorn divergences_, Feydy et al.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nThe paper is overall clear. Theoretical statement are precise and numerical experiments are well described overall. \n\n## Quality\n\nI think this is a solid work overall. Few points remain to be discussed (see Weaknesses and Questions above). \n\n## Novelty\n\nThough tackling an existing problem, the proposed analysis is new to the best of my knowledge. My main concern about novelty would rather be that the work may not be placed in the most \"state-of-the-art\" setting of computational OT. \n\n## Reproducibility\n\nFrom a theoretical perspective, the claims are supported by detailed proofs in the appendix. While I could not proofread all of them, I checked the proof of Theorem 1 and did not identify major flaws. The following proofs looked convincing at first glance, though I obviously encourage the authors to carefully proofread the paper as the proofs are quite technical. \n\nOn the experimental side, code to reproduce experiments has been anonymously provided and though not tested, I am fairly confident that one could reproduce the numerical results of the paper. Also, the fact that implementation is available in both PyTorch and Jax is a nice bonus. ",
            "summary_of_the_review": "I think this is a solid work overall, with potential impact for computational OT practitioners. Nonetheless, few details remain to be clarified/discussed in my opinion before fully supporting it. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1155/Reviewer_toCu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1155/Reviewer_toCu"
        ]
    },
    {
        "id": "faBiyJkO7j9",
        "original": null,
        "number": 4,
        "cdate": 1666719919446,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666719919446,
        "tmdate": 1666780627809,
        "tddate": null,
        "forum": "uATOkwOZaI",
        "replyto": "uATOkwOZaI",
        "invitation": "ICLR.cc/2023/Conference/Paper1155/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on the so-called 'sharp Sinkhorn distance', which can be read as between the Wasserstein distance (without regularization) and the vanilla Sinkhorn divergence. The main contributions are two-fold: (i) the authors propose an L-BFGS algorithm to optimize the dual objective function of the Sinkhorn divergence leading to a stable transport plan (ii) an analytic form of the gradient of the shape Sinkhorn with respect to the input matrix is given. Some numerical experiments to corroborate the theoretical finds are illustrated on synthetic and real datasets.",
            "strength_and_weaknesses": "**Strength**\n- LBFGS algorithm to optimize the dual objective function of the Sinkhorn divergence.\n- An analytic form of the gradient of the sharp Sinkhorn with respect to the cost function.\n\n**Weakness**\n- The authors argue that the transport plan resulting from the L-BFGS algorithm is stable compared to ones from vanilla /stabilized Sinkhorn iterations. For me, this comparison is not fair, since it is only visually; it is mandatory to establish a theoretical result or an extensive empirical study showing this gain of stability. I mean one can show that the Frobenius norm (or any matrix norm) between the LBFGS plan and the unregularized plan is less than the vanilla Sinkhorn plan and the unregularized plan. Alternatively, one can investigate the difference between the objective dual function.\n- The motivation behind calculating the gradient cost should be highlighted. \n- The numerical experiments on real data are limited.",
            "clarity,_quality,_novelty_and_reproducibility": "- I thank the authors for joining the code of the numerical experiments. Reproducibility is guaranteed. \n- The paper is clear and easy to follow. ",
            "summary_of_the_review": "Overall, the paper lacks some theoretical results or an extensive numerical study to prove the stability of the proposed LBFGS algorithm. \n\n**Superlative phrases**\n\nIn the core of the paper, I noticed that there are lots of superlative phrases. I am wondering if this fact is scientifically acceptable. \n- Abstract: \"superiror performance\"\n- Page 8: \"enjoys superior computational efficiency\"\n- Page 8: \"huge advantage of a closed-form\"\n- Page 8: \"proposed algorithm is the most efficient one\"\n- Page 9: \"highlights the value of the proposed method\"\n\n**Typos**\n- Page 2: \"The optimal solution to (1)\" --> \"An optimal solution to (1)\". Problem (1) could have many solutions.\n- Page 2: \"satisfies some suitable conditions\" --> Add a reference.\n- Page 3: \"diagonal matix\" --> \"diagonal matrix\".\n- Page 5: in Theorem 2: the norm $\\||{\\gamma^*}\\||$ is not specified; is it the $\\ell_2$-norm? or $\\ell_\\infty$-norm? etc.\n- Page 6: in Theorem 4: the matrix $D$ is not defined.\n- Page 9: \"methods achives\" --> \"methods achieves\"\n- References: \"wasserstein\" --> \"Wasserstein\"\n- References: \"sinkhorn\" --> \"Sinkhorn\"",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1155/Reviewer_sDPq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1155/Reviewer_sDPq"
        ]
    }
]