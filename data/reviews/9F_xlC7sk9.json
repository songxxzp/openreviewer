[
    {
        "id": "zpsrqA2qJW",
        "original": null,
        "number": 1,
        "cdate": 1666191552348,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666191552348,
        "tmdate": 1666368095366,
        "tddate": null,
        "forum": "9F_xlC7sk9",
        "replyto": "9F_xlC7sk9",
        "invitation": "ICLR.cc/2023/Conference/Paper3024/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies model-based networks in the context of sparse coding.\nA model-based network is constructed by unfolding an iterative algorithm, i.e. mapping each iteration of the original algorithm to a layer of the resulting neural network.\nThis allows to have a trainable neural network that can adapt to specific problems, while encoding some stuctural information of the original iterative algorithm.\n\nThe main goal of the paper is to provide rigorous bounds on the generalisation capabilities of two model-based networks inspired by two specific sparse coding algorithms, namely \"iterative shrinkage and thresholding algorithm\" (ISTA) and  \"alternating direction method of multipliers\" (ADMM).\nMoreover, the authors aim to compare model-based neural networks to generic fully-connected ReLU networks in the context of these bounds.\n\nThe authors prove two sets of bounds for fully-connected ReLU networks and ISTA/ADMM-based neural networks:\n- they bound the generalisation error, intended as the gap between empirical and population loss of the worst-performing function in the model class, averaged over training datasets. They find that the bound for model-based networks is better than that of ReLU networks, and this is crucially linked to the effect that component-wise soft-thresholding has on the Rademacher complexity of a given model class.\n- they bound the estimation error, intended as the gap between the population loss of the ERM minimiser and the population loss of the best performing function in the model class. Again, model-based networks enjoy a better bound on estimation error than ReLU networks, and again this is linked to the effect of soft-thresholding on the local Rademacher complexity of a given model class.\n\nThe authors then study numerically the estimation error of ISTA and ReLU networks, finding that ISTA networks perform better. They also show that the dependence of the estimation error on the scale parameter of the soft-thresholding non-linearity is the same as that predicted by the bounds.\n\nQuestions (see also the weak points section):\n- how does the specific model considered (eq 1) affect the results? \n- are all numerical results robust to a change in sparsity, noise level and batch size? Moreover, what is the batch size used in the numerical experiments?\n- the numerical experiments mention a \"learned bias\" setting that is not accurately described in the main text or in the supplementary material. What is that?\n\nMinor points:\n- the authors could add direct links to the proofs of their lemmas and theorems after stating them.\n- the statement of assumption 1 is not completely clear to me. Is there some notation change or notation missing?\n- if at all relevant, the authors could add the performance of the original ISTA algorithm to their numerical experiments as a baseline performance. If not relevant, why is it not relevant?\n- the operative definition of EE (eq 7) is not completely clear. h_hat is the empirical risk minimiser, so it depends on a training set, is this correct? If so, is there some averaging missing?\n\nCuriosities (not relevant for the review process as far as I am concerned):\n- is there a natural interpretation to why soft-thresholding lowers the Rademacher complexity of model classes?\n- the bound presented in theorem 5 is the same for ReLU, ISTA and ADMM based networks. Can one give a \"physical\" interpetation to r*? In some sense, r* encodes some information about the network architecture that could determine some other property and not only the EE bound.\n- is there an explanation to the fact that learning biases seems to improve the EE of ReLU networks while degrading the EE of ISTA networks?",
            "strength_and_weaknesses": "Strong points:\n- the bounds that the authors provide seem to be non-vacuous, which I find non-trivial. Indeed, the bounding terms suggests, for example, that the estimation error depends on the soft-thresholding scale parameter, and this is confirmed numerically.\n- I could not check the correctness of the proofs given the short time-frame of this review process. Nonetheless, the relevant appendices are very detailed and I feel that the interested reader could check the proof details easily if needed. Also, the results as well as the overall proof techinque seem reasonable. \n- The paper is extremely well written and clear. I appreciated particularly that formal mathematical concepts were always explained and justified in the discussion text surrounding them, allowing a much broader audience to understand the results.\n\nWeak points:\n- it is not completely clear what role the specific model that the authors consider (eq. 1) plays in the various theorems. This doesn't allow the reader to fully gauge the applicability of the results to different data models.\n- (minor weak point) the numerical experiments have been performed fixing the sparsity level of the signal to be retrieved, the noise level and, I suppose, the batch size of SGD. It is not clear whether the numerical results are robust across the choice of these parameters.\n- (minor weak point) the numerical experiments mention a \"learned bias\" setting that is not accurately described in the main text or in the supplementary material.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and besides some minor weak points it seems to respect the common standards of scientific quality, included reproducibility of proofs and numerical experiments.\nUp to my limited knowledge of the relevant literature, the paper seems original.",
            "summary_of_the_review": "My initial recommendation is: accept.\nThe paper is very clearly written, the results are nice, in particular the fact that the bounds obtained seem not vacuous, and, while I did not check the details of the proofs, the theorems seem sound.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": " ",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3024/Reviewer_zELY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3024/Reviewer_zELY"
        ]
    },
    {
        "id": "g-C3m6bLAB",
        "original": null,
        "number": 2,
        "cdate": 1666361818024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666361818024,
        "tmdate": 1668864316732,
        "tddate": null,
        "forum": "9F_xlC7sk9",
        "replyto": "9F_xlC7sk9",
        "invitation": "ICLR.cc/2023/Conference/Paper3024/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies generalization and estimation error bounds for model-based neural networks that employ soft-thresholding activation functions. By re-examining the proof of the contraction lemma (of Rademacher complexity) for soft-thresholding functions, the paper shows that applying soft-thresholding potentially decreases complexity (although the Lipschitz constant of soft-thresholding is 1).\n\nUsing this result, the paper establishes norm-based bounds on generalization error (GE) and estimation error (EE) of these networks for the problem of sparse vector recovery. The manuscript claims that the GE and EE bounds of the soft-thresholding network are better than those of ReLU. These findings are then validated through experiments.",
            "strength_and_weaknesses": "### Strengths\n\n- The paper provides theoretical insights into the simple (but important) intuition that model-based methods should perform better than model-free approaches. The main ideas and techniques of the work were developed based on well-founded rationales and in general, the paper is well-written with a sufficient literature review.\n- The mathematical analyses of the work are rigorous and seem correct. The results of the work, which relies on directly bounding the Rademacher complexity, are significantly stronger and more accessible (readable) than previous works (which focused on estimating Rademacher complexity through covering numbers).\n- The bound on Rademacher complexity of soft-thresholding is novel and meaningful and could potentially be of interest to a broader audience in the field beyond the context of model-based networks.\n- If the comment below (in the Weakness section) is well addressed, the bound of the Theorem shows that a nonincreasing GE as a function of the network\u2019s depth is achievable, by limiting the weights\u2019 norm in the network. This is an strong improvement over existing bounds, which exhibit a logarithmic increase of the GE with depth.\n\n### Weaknesses\n\n- In my opinion, the paper, in its current state, overstates the significance of its results. I don\u2019t think the results support the claim that their bounds on GE and EE are better than those of ReLU.\n    \n    While Lemma 1 establishes that applying soft-thresholding potentially decreases complexity, it doesn\u2019t come with any estimation of the improvement (T) and it is not even clear that T is non-zero. In fact, the statements of the theorems all specify that T\u2019s could be zero. The only place where T is estimated is in the discussion after the proof of Lemma 1 in the Appendix, which states that in certain scenarios, it is possible to bound T from below by an explicit constant depending on m. However, the result is written in a general context and it is unclear how this result applies to the studied networks.\n    \n    I strongly suggest that these lower bounds on T are rigorously stated and proved to support the main results of the work. Discussions about the dependence of the improvements on lambda, are not correct if we are not certain that T is non-zero. An explicit bound, even if further assumptions are made, would provide more insights about the order of the improvements, which would be of broad interest to the readers.\n    \n- Similarly, in its current state, Lemma 1 is trivial and doesn\u2019t need its complicated proof. To prove its current statement (existence of T in [0, \u2026] such that\u2026), we just need to establish the inequality for T = 0, which can be directly deduced from the contraction lemma (soft-thresholding has Lipschitz constant 1).\n\n### Questions\n\n- In my understanding, the main idea of the proof of Lemma 1 is that although the Lipschitz constant of soft-thresholding is one (it is a translation in certain regions), there are sub-regions of RxR where |S(x) - S(y)| < |x - y| - 2\\lambda, and the \u201cimprovement\u201d depends on the measure of such sub-regions, specifically, on\n    \n    \\{ h*(y) > \\lambda) \\text{and} h*\u2019(y) < -lambda\\}\n    \n    My question is, the ReLU function also \u2018contract\u2019 on this region, and the contraction is at least lambda, i.e., |S(x) - S(y)| < |x - y| - \\lambda if x>\\lambda and y<-\\lambda. Does that also mean that we can obtain a similar result for ReLU using the same argument?\n    \n- (Optional. This is not a part of my decision assessment) In the classical proof of the contraction lemma (e.g., Lemma 26.9 of Understanding Machine Learning), the absolute value in the bound can be removed and added when convenient because of the symmetry of h and h\u2019. In the proof of Lemma 1, cases (a, b) is treated differently from (b, a). Is there any particular reason for that?\n\n### Additional feedbacks\n\n- Typos: Section 1.1: \u201clogarithnmic\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "(see Strengths and Weaknesses)\n\n",
            "summary_of_the_review": "The paper addresses a meaningful question and the approach is novel and of broad interest. However, the paper, in its current state, overstates the significance of its results. I believe some significant restructuring and additional estimations need to be done to support the claims.\n\nAfter revision: My main concern about the manuscript have been addressed and I think this is a good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3024/Reviewer_4ojw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3024/Reviewer_4ojw"
        ]
    },
    {
        "id": "nRAq3wXDbDw",
        "original": null,
        "number": 3,
        "cdate": 1666627117951,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627117951,
        "tmdate": 1666627117951,
        "tddate": null,
        "forum": "9F_xlC7sk9",
        "replyto": "9F_xlC7sk9",
        "invitation": "ICLR.cc/2023/Conference/Paper3024/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the theoretical properties of model-based neural networks which are usually interpretable and inherit the prior structure of the problems, such as ISTA and ADMM networks associated with soft thresholding operators. The bounds for both generalization and estimation errors of these networks are derived via the lens of Rademacher complexity and local Rademacher complexity, which are shown to be lower than that of the common ReLU neural networks under mild conditions. Numerical experiments are also provided to verify the theoretical results.",
            "strength_and_weaknesses": "$\\textbf{Strength}$\n\n1. It is a novel contribution that broadens the understanding of model-based neural networks and their superiority compared to the standard ReLU networks.\n\n2. The Rademacher complexity of function classes after applying the soft-thresholding operation is derived, which further leads to the generalization and estimation errors of model-based neural networks, such as ISTA and ADMM networks.\n\n3. Codes are provided for the numerical experiments.\n\n$\\textbf{Concerns}$\n\n1. The difference between ISTA networks and ReLU networks is that (2) uses a soft-thresholding operator $S_\\lambda$ and (10) uses ReLU. Can we think of $S_\\lambda$ as a special type of activation function? I was wondering if the performance of ISTA networks is also better than that of leaky ReLU networks or ELU networks?\n\n2. In Figure 3, estimation error decreases as $\\lambda$ increases while $L_1$ loss increases as $\\lambda$ increases, especially for large sample sizes. In practice, how to choose the best $\\lambda$?\n\n3. In Figure 2, it is a little bit confusing to me that the estimation error of ISTA with learned bias is larger than that with constant bias, any explanation?\n\n4.  Is it possible to extend the results to convolutional neural networks?\n\nMinor typos:\n\n1. Is the last term in (13) $\\frac{\\lambda T^{(l)}}{m}$\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-organized and clearly written. The results are technically sound and novel.",
            "summary_of_the_review": "The paper derives the generalization and estimation error bounds of model-based neural networks and provides insights into the superiority of ISTA and ADMM compared to ReLU networks, which should be of interest to the ICLR community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3024/Reviewer_CF5w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3024/Reviewer_CF5w"
        ]
    },
    {
        "id": "iEROEDTeyL",
        "original": null,
        "number": 4,
        "cdate": 1666690968351,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690968351,
        "tmdate": 1666690968351,
        "tddate": null,
        "forum": "9F_xlC7sk9",
        "replyto": "9F_xlC7sk9",
        "invitation": "ICLR.cc/2023/Conference/Paper3024/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies the generalisation properties of model-based neural networks which can achieve unparalleled performance on sparse coding and compressed sensing problems. In particular, the authors leverage complexity measures including the global and local Rademacher complexities to provide upper bounds on the generalisation and estimation errors of model-based networks. They show that the generalisation abilities of model-based networks for sparse recovery outperform those of regular ReLU networks. ",
            "strength_and_weaknesses": "**Strength**\n\n* I am not an expert on model-based neural network, but from my perspective, I think this work adopts a suitable theoretical tools to solve an important problem.\n\n* This writing is quite good. It is eazy to follow the main idea of this work.\n\n**Weakness**\n\n* Though the authors provide simulations to verify the proposed theory, it is still not clear how realistic the theory is and what is the gap between the considered scenario and the realistic settings.",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is very clear. Very organized and enjoyable to read.\n\n* The paper is of high quality.\n\n* The paper is quite novel to me.",
            "summary_of_the_review": "I understand the context of this work and the results it is trying to convey. If the results of this work are correct, I think this work deserves to be accepted by ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3024/Reviewer_FqeZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3024/Reviewer_FqeZ"
        ]
    }
]