[
    {
        "id": "UisXoZFdAJC",
        "original": null,
        "number": 1,
        "cdate": 1666481417931,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666481417931,
        "tmdate": 1669221944630,
        "tddate": null,
        "forum": "DJEEqoAq7to",
        "replyto": "DJEEqoAq7to",
        "invitation": "ICLR.cc/2023/Conference/Paper2913/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents the first work to train a sparse DRL agent from scratch. The major contributions are that the robust value estimation and effecient topology evolution are important to train a sparse DRL agent, and this paper proposes to leverage the gradient for network topology evolution and multi-step TD-target with dynamic buffer for robust value estimation. Experimental results are presented using two off-policy TD learning DRL models, and show the best performance over existing approaches.",
            "strength_and_weaknesses": "Strengths:\n- Training ultra-sparse DRL model from scratch is important while underexplored topic in practice.\n- The contributions of this paper are clearly summarized, and the motivations are grounded in empirical observations.\n- Experimental results show significantly better performance than the listed baselines.\n \nWeaknesses:\n- Since the one-step and multi-step TD-targets are used in a hybrid way in training, it's not a proper claim that a multi-step TD target could overcome the issue of inaccurate value estimation from a sparse network. \n- Also, from the Eq. (4), it's unclear why the multi-step TD-targets could overcome the unreliable and inaccurate value estimations.\n- The dynamic size of replay buffer is controlled by a pre-defined threshold on the policy distance, which is kind of heuristic in practice. \n- Moreover, transitions are actually **randomly** selected from the replay buffer (to keep samples iid) rather than using the oldest top-K transitions. Therefore, the policy distance measure (Eq. (6)) may not effectively evaluate the inconsistency between behavior policy and current target policy.\n- From Table 3, it's curious to me that the performance degrades when more than 3 steps are used. On the Hal. and Ant. benchmarks, the best number of steps is 2. Since it's a consistent observation that the performance would degrade with too many steps, this paper should present more in-depth discussion to explain this phenomenon.\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "- The organization is clear and the technical approaches are well motivated.\n- A little bit overclaim. The scope of the method in this paper should be in the off-policy TD learning area rather than the entire DRL community. For other DRL methods, such as the on-policy methods, it's unclear if the method is applicable or not. \n- The novelty of method may be limited. Though I acknowledge the interesting empirical observations, the proposed technical solutions (gradient-based topology evolution, multi-step TD-targets, and thresholding the policy distance) are kind of trivial in practice.",
            "summary_of_the_review": "Beyond the technical side, this paper is of good readability (well organized, empirically grounded motivations, etc). However, in terms of the technical novelty, the proposed solutions are a little bit trivial in sense. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2913/Reviewer_i62R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2913/Reviewer_i62R"
        ]
    },
    {
        "id": "-Yg3J9WeY5R",
        "original": null,
        "number": 2,
        "cdate": 1666698885867,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698885867,
        "tmdate": 1669800374660,
        "tddate": null,
        "forum": "DJEEqoAq7to",
        "replyto": "DJEEqoAq7to",
        "invitation": "ICLR.cc/2023/Conference/Paper2913/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to learn an efficient sparse network for reinforcement learning (RL). It learns the sparse network from scratch instead of relying on knowledge distillation or pruning. The proposed method is designed based on the gradient-based topology evolution criteria. Experiments show that the proposed method can learn very sparse networks with satisfying performance. ",
            "strength_and_weaknesses": "Strength\n- The proposed method is carefully designed and well implemented. It can maintain good performance with very high-level sparsity on different datasets. The proposed method integrates the topology evolution method into the RL task very well. \n- The details of the algorithms and experiments are discussed and analyzed carefully, making the work easy to reproduce. \n- The paper is easy to follow. \n\nWeakness\n- The proposed method is an integration of the RigL method into the RL task and framework. The major contribution is on how to integrate it into the RL framework. This limits the significance of the proposed method, especially the lottery ticket based method for sparse network learning has been studied wildly.\n- Apart from RigL, the authors may consider studying how different sparse network learning methods can help RL in what kind of way. This may generalize the proposed techniques to a more general sparse network based RL framework. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The whole paper is written clearly. The work is with good reproducibility with a detailed introduction of the details. Considering that the core method is RigL, the novelty is limited at some level. But the studies on the RL are in detail. \n",
            "summary_of_the_review": "The major contribution is about how to integrate the RigL based sparse network learning approach into the RL framework, which limits the novelty. But the work conducts very detailed design and studies and analyses of the sparse network learning for RL. Many learning strategies are investigated.  \n\nThe rebuttal addressed my concerns. I am happy for this paper to be accepted. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2913/Reviewer_7aAy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2913/Reviewer_7aAy"
        ]
    },
    {
        "id": "uzAeGXkam4",
        "original": null,
        "number": 3,
        "cdate": 1666738968738,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666738968738,
        "tmdate": 1666738968738,
        "tddate": null,
        "forum": "DJEEqoAq7to",
        "replyto": "DJEEqoAq7to",
        "invitation": "ICLR.cc/2023/Conference/Paper2913/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Sparse neural networks that have an order of magnitude fewer parameters than a dense network without a significant drop in performance can greatly reduce computational requirements of reinforcement learning (RL) agents, allowing their deployment on resource-limited devices. Different sparse network topologies can have drastic differences in performance. A method for the training of sparse networks from scratch, known as RigL, learns the topology throughout training by dropping low-magnitude weights and growing connections that have large gradients. This paper extends RigL to the deep RL domain by adding two techniques for making the value function estimation more robust. One technique is using multi-step temporal difference targets instead of one-step ones. The other is dynamically changing the size of the replay buffer if its stored actions get too far from the current policy. The paper demonstrates with examples that robust value learning is essential for improving the performance of RigL since, in part, the value function affects the gradient-based topology learning of RigL. Experiments with RLx2 using SAC and TD3 algorithms on four mujoco environments show significant model compression with minimal loss in performance.",
            "strength_and_weaknesses": "The RigL method learns a sparse topology from scratch. Unlike other model compression methods, there is no need for pre-training a dense network, which markedly reduces the computational requirements of both training and inference. RLx2 could therefore greatly benefit online learning problems on resource-constrained devices. I appreciate that the author has validated the importance of robust value estimation and its effect on the topology learning of RigL.\n\nThere are two main weaknesses in the paper. The first is the lack of proper sensitivity analysis for some of the new algorithm hyper-parameters. Although the hyper-parameters are the same for all four environments, it is unclear how sensitive the performance is to each hyper-parameter and how the sensitivity differs between environments. These hyper-parameters include the initial mask update fraction, mask update interval, buffer adjustment interval, buffer policy distance threshold, and multi-step delay.\n\nThe second weakness relates to the computational complexity of the algorithm. The topology learning of RigL consumes O(N logN) time with N as the number of weights in the corresponding dense network. Although the topology is updated every 10000 steps and the cost may be negligible compared to the main RL algorithm in these experiments, 10000 is still a constant. With the exponentially increasing number of parameters, in the future the computational requirement of the topology learning step might begin to dominate and grow faster than the main algorithm, making it less scalable. It would be great if the dropping and growing of connections could be done locally for each connection instead of requiring a global sort. This issue concerns the RigL method adopted by this paper and not the techniques introduced in this paper for making the value estimate robust.\n\nHere are some more questions and comments:\n* What do the shaded areas in the plots represent?\n* Did you experiment with using multi-step TD targets from the beginning? What did you observe?\n* What happens if you do not anneal the update fraction, but keep it constant at a smaller value?\n* Should the two robustification techniques be applied to the original algorithms even when not using RigL?\n* \u201cWe also show in Appendix C.2 that the performance of RLx2 is insensitive to the policy threshold.\u201d I do not see any sensitivity plots in C.2.\n* \u201cUpon any update to \\phi and appending new transitions to\u2026\u201d Is this sentence correct? Was the buffer adjustment not done every \\Delta_b steps?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, well-written, and makes novel scientific contributions. Pseudocodes of algorithms and their hyper-parameters are provided in the appendix.",
            "summary_of_the_review": "The paper establishes that having a robust value estimator is important when learning the sparse network topology using RigL in Deep RL. The method trains a sparse network from scratch and achieves significant model compression with minimal loss in performance. Claims are backed up with experiments. The two introduced techniques are simple yet effective for applying RigL to Deep RL. The paper can be improved by adding sensitivity analyses for the new hyper-parameters as discussed previously.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2913/Reviewer_Qrdc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2913/Reviewer_Qrdc"
        ]
    },
    {
        "id": "1QpvdM95Ab",
        "original": null,
        "number": 4,
        "cdate": 1667142981505,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667142981505,
        "tmdate": 1667142981505,
        "tddate": null,
        "forum": "DJEEqoAq7to",
        "replyto": "DJEEqoAq7to",
        "invitation": "ICLR.cc/2023/Conference/Paper2913/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies sparse training with dynamic sparsity in the context of extremely sparse neural network models, trained from scratch, as function approximators for deep reinforcement learning. Consequently, the paper proposes a new sparse training method for deep reinforcement learning, named the Rigged Reinforcement Learning Lottery (RLx2). According with the experimental evaluation on four continuous control task environments, RLx2 seems to perform considerably better than the sparse baseline methods considered. Moreover, it can also outperform in a handful number of cases the dense equivalent.",
            "strength_and_weaknesses": "Strength:\n* Timely, relevant, and under-studied topic, particularly for deep reinforcement learning. If enough efforts are put into it, and if it will become more mature, it has the potential of seriously decreasing the costs associated with deep reinforcement learning. Counterintuitively, as shown in the paper, it also has the potential of improving the state-of-the-art performance in deep reinforcement learning.\n* The proposed method has a good level of novelty.\n* The empirical validation is well-designed. The results show that the proposed method can outperform the baselines.\n* Well written paper\n\nWeaknesses \u2013 in my opinion, the paper doesn\u2019t seem to have major flaws. Further, I would raise some points for discussion which are intriguing me:\n* Why the Humanoid environment hasn\u2019t been considered in the experiments? I assume that at this very high sparsity levels, given its large state space, the performance may be far from optimal. Can you consider adding an extra small experiment on Humanoid where you can study two network architectures (one like in the paper and one with more hidden neurons to compensate for the high sparsity regime) for RLx2 and the typical baselines used in the paper?\n* From Table 2, I see that the proposed method (RLx2) consistently improves the dense SAC baseline in terms of performance even when over 90% sparsity is considered. At the same time, on TD3 the gain induced by RLx2 is considerably smaller. Do you have any idea why RLx2 impacts SAC in a different manner than TD3?\n\nOther comments:\n* In table 1, I believe that it is worth also adding the work of Graesser et al., 2022 for a more complete overview.\n* page 4, I believe that \u201c\u2026links with the smallest weights.\u201d is not accurate for negative weights. According with Alg 1, line 9, it is rather about \u2026 links with the smallest absolute value of the weights\u2026 \n* page 5, \u201c\u2026Here we adopt the methods in (Frankle & Carbin, 2019) for\u2026\u201d. The following procedure is not exactly the one from the mentioned paper. I believe that it would be informative to clarify exactly what was taken from that paper and what was added by you.\n* page 6, Dynamic capacity buffer \u2013 According with the description, the buffer size (capacity) is always increasing? Is my understanding correct?\n* page 7, \u201c\u2026SET (Bellec et al., 2017),\u2026\u201d is not correct. SET was proposed by Mocanu et al., July 2017, if referring to the first arxiv version, or 2018 if referring to the year of formal publication.\n* page 8, \u201c\u2026which replace the topology evolution scheme in RLx2 with Tiny, SS and SET, while keeping\u2026\u201d may be ok, but it is not sufficiently clear to me what are the changes as some of those methods are static and some are dynamic (evolves). Perhaps you can clarify it better in the paper.\n* page 9 \u2013 Conclusion \u2013 a fair paper, with a good level of self-criticism, discusses also a bit of the limitations of the proposed methodology, while possibly indicating future work directions. These are missing in this paper.\n * page 25 \u2013 Appendix C.5 \u2013 Figures 9 ad 10 \u2013 it may be useful for the reader to see also somehow the sparsity levels in those plots without the need of scrolling back to Section 5.1\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clear and reads well, in my opinion.\n\nQuality: The paper has a good quality level, and it seems to be well executed.\n\nOriginality: Good level of novelty as reflected also by the proper related work discussion.\n\nReproducibility: I believe that it may be possible (but not straightforward) for a reader with a good level of experience on the topic to reproduce the proposed methodology given just the paper details, if enough time is put into it. Anyway, the authors promise the release of open-source code if accepted (but it is not presented in the supplementary material) and this shall increase seriously the reproducibility levels. \n",
            "summary_of_the_review": "Overall, I believe that the paper has a good level of novelty, with good results, and a good presentation. I am curious to see the authors\u2019 opinion about the comments raised above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2913/Reviewer_B2aD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2913/Reviewer_B2aD"
        ]
    }
]