[
    {
        "id": "M0OGX6G_XsX",
        "original": null,
        "number": 1,
        "cdate": 1666713745903,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713745903,
        "tmdate": 1666713745903,
        "tddate": null,
        "forum": "WhoOFXdnys6",
        "replyto": "WhoOFXdnys6",
        "invitation": "ICLR.cc/2023/Conference/Paper5100/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a simple, distributed approach to robust RL coined MAD. The main components of MAD are:\n1) conditional mean and variance normalization of the task reward based on a population of conditional samples, sampled from the current (local) policy over a range of temperatures. This promotes balance in terms of ensuring that there is positive and negative feedback for each set of conditional samples, and regularity in the gradients across sets of conditional samples.\n2) An importance weight that depends on the mean absolute deviation (MAD) of the probability of conditional samples from the median probability in a sample population. This is meant to promote more conservative updating of the policy, and mitigate against policy collapse, acting as an entropy regularizer.\n\nMachine translation results on NIST, IWSLT\u201914, WMT\u201914, and WMT\u201920 seem to indicate that the method outperforms several baselines, including MRT (see S&W section for caveats).",
            "strength_and_weaknesses": "Strengths:\n\n- Well written in general. Ablations over using conditional reward normalization (fig. 5), MAD IWs (fig. 5,6), and different reward functions (Table 2) are included.\n\n- Performance comparisons with several important baseline methods, including minimum risk training (MRT) (with serious caveats, see limitations below for details).\n\nLimitations:\n\n- Conditional reward normalization (CRN) is identified as the key factor to MAD performance. However, based on the current manuscript (e.g. section 2.2 and discussions elsewhere), it seems that the baseline approaches being compared to such as REINFORCE and MRT, which trivially have conditional variants of their baselines, are NOT conditionally normalized. The importance of CRN is well known, and the comparisons must be \"apples to apples\".\n\n- Techniques such as MRT and MAD rely on sample population statistics for reward normalization, whereas other techniques estimate a conditional baseline and are much more sample-efficient to train, including cited methods in the paper such as in MIXER (learned baseline), self-critical training (MAP sequence estimate defines baseline), and actor-critic methods for sequence prediction (word slot-dependent rewards). Since conditional reward normalization seems to be the dominating factor in performance, it seems essential to also situate the results relative to one or more of these techniques (the former 2 are very easy to try out).\n\n- Importance weighting based on how far from the median probability each sequence is in probability seems excessively conservative for noisy models such as text sequence generators, and will certainly foster the stated goal of \"exploration-heavy\" training, but it will also prevent the model from converging. Since this a heurstic, I feel that some other competing heursitics should be compared to and reported on... For example, perhaps the distance from expected probability is better, it is less \"exploration heavy\", but will allow the policy to converge (for better or for worse).\n\n- Related, it seems that MAD reweighting should be compared with standard regularization baselines, like an entropy term!\n\n- The main components of the approach 1) mean and variance normalization of conditional rewards, and 2) importance weights based on the distance from the median probability can be considered standard and ad hoc, respectively. From this perspective, the paper is lower in novelty for a method-focused ML conference.",
            "clarity,_quality,_novelty_and_reproducibility": "See S&W section.",
            "summary_of_the_review": "Based on the current manuscript, it appears that the proposed method is utilizing conditional baselines, while competing methods which support conditional baselines are not, which is not appropriate, and will necessitate a major revision of the paper before it can be considered for acceptance. In addition, further investigations around their importance weighting heuristic and comparisons with techniques that directly estimate their baselines and so do not need a sample population to train (and so are much more efficient), should also really be included in their experimental investigation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5100/Reviewer_VmFP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5100/Reviewer_VmFP"
        ]
    },
    {
        "id": "ia5lRtwJnHP",
        "original": null,
        "number": 2,
        "cdate": 1666794955291,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794955291,
        "tmdate": 1666794955291,
        "tddate": null,
        "forum": "WhoOFXdnys6",
        "replyto": "WhoOFXdnys6",
        "invitation": "ICLR.cc/2023/Conference/Paper5100/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper propose an easy-to-implement fine-tuning for machine translation via reinforcement learning. \nThe major issue for reinforcement fine-tuning is described as variance reducing, both in rewards and updates.\nThe main contributions are listed as the following:\n- A conditional reward normalization to reduce variance;\n- A novel robust importance weighing scheme similar to trust-region update, striking the balance between exploration and exploitation.\nThe translations are first sampled for tuning rewards via a variable temperature. Then, the rewards are standardized to update the central policy by rescaled weights.\nThe methods improves the specific translation metric and certain robustness.",
            "strength_and_weaknesses": "# reasons to accept:\n-  This paper is well organized and easy to understand. \n- The MAD importance weight is a sophisticated trust region scheme that achieves robust translation against decoder\u2019s beam-size and length normalization. Experiments show that MAD also achieves stable convergence compared to traditional PPO. The scheme also strikes a balance between exploration and optimization, which is worth noting. \n- The proposed methods greatly boost stability for RL tuning. The improvement is satisfying compared to the baseline. \n- Authors conduct thorough experiments across different languages and different reward types to validate the proposed methods. The results are promising.\n\n# reasons to reject:\n- The proposed reward normalization is a standardization, which can hardly be considered as \u2018source conditioned\u2019. The authors do not explain how the conditional reward standardization relates to the difficulty of source sentences for its designed purpose. \n- According to the ablation study, both MRT and PPO fail to converge on the long run. But SOTA RL tuning involves certain implementation to achieve tuning by MRT and PPO, which should be clarified in baseline settings. \n- The paper is somewhat misleading. According to their ablation study, conditional reward normalization is the most important one among the three components. The MAD method, as stated in the title, does not contribute much to the performance gain, and it even fails the training process.\n- Reinforcement learning can be tricky and empirical. The tradeoff to implement such methods for tuning rather than data augmentation is worth discussing. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear in describing the techniques, but could be improved in discussion and analysis. Technically, I believe this is a novel interesting contribution.",
            "summary_of_the_review": "The paper presents a interesting framework for fine-tuning MT system. But the analysis and discussion of the method is problematic. It seems to me that the interpretation from the authors are not well-matched with the experiment results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5100/Reviewer_qbvg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5100/Reviewer_qbvg"
        ]
    },
    {
        "id": "-TvCBV_I6rj",
        "original": null,
        "number": 3,
        "cdate": 1666875447210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666875447210,
        "tmdate": 1666875447210,
        "tddate": null,
        "forum": "WhoOFXdnys6",
        "replyto": "WhoOFXdnys6",
        "invitation": "ICLR.cc/2023/Conference/Paper5100/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper improves the policy gradient algorithm with three methods: (1)multi-temperature sampling, generate multiple translation samples of the same source sentence using different temperatures; (2)conditional reward normalization, standardize the rewards of the same source sentence by removing the mean and dividing by the standard deviation; (3)MAD importance weights, decide which sampled trajectories are most valuable for updating the policy and which others should be downweighted. Experiments on multiple machine translation datasets show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "The strengths of this paper include:\n1.\tThe proposed method shows good performance, especially on greedy decoding.\n2.\tVery extensive experiments on multiple datasets and multiple settings.\n3.\tThis paper brings new insights for improving RL-Based NMT.\n\nThe weaknesses of this paper include:\n1.\tFigure 5 shows that the major improvements come from conditional reward normalization, which is more of a trick and not novel to me. The improvement of MAD itself seems to be marginal.\n2.\tTable 2 lacks the results of the cross-entropy baseline. It is important to know whether the proposed method only improves BLEU and lower other metrics.\n\nA few questions:\n1.\tWill you release the source code?\n2.\tIn REINFORCE and PPO, did you sample multiple translations and calculate their average reward as the reward baseline?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This is an original work with good clarity. In my opinion, its quality is around the acceptance threshold of the conference.",
            "summary_of_the_review": "This paper improves the policy gradient algorithm with three methods. The performance looks good, but the major improvements come from conditional reward normalization, which is more of a trick and not novel. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5100/Reviewer_Gvqo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5100/Reviewer_Gvqo"
        ]
    }
]