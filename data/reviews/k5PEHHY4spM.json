[
    {
        "id": "d8Ro5UXtWK",
        "original": null,
        "number": 1,
        "cdate": 1666609878325,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609878325,
        "tmdate": 1666609878325,
        "tddate": null,
        "forum": "k5PEHHY4spM",
        "replyto": "k5PEHHY4spM",
        "invitation": "ICLR.cc/2023/Conference/Paper3751/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the issue of generic response generation by first using adapters for efficient multi-decoder frameworks, and next using a balanced EM algorithm to train the EM of the multi-decoders. The main contribution is to train the balanced EM by the classic Hungarian algorithm. Experiments compare the proposed model with other methods that address the same problem.\n",
            "strength_and_weaknesses": "Strengths: The idea is clear and the implementation of the idea should be easy.\n\nWeakness:\nThe authors argue that the E-step in the proposed model contributes to less than 1% of the total training time. I want to know if the full experimental results can be provided, including the total training time, the number of EM steps, and the exact time of the E-step and M-step in the training time. Also, the time efficiency should be compared for all methods in your experiments.\n\nThe experimental results are not convincing.  First, only results on BLEU-2 of BLEU-n are given. How about BLEU-1? Also, human evaluation results of Table1 should be given.  Also, I think a pretrained T5 should not be proper to compare here, as it is not for dialog generation. Perhaps DialogGPT is better if the authors want to compare a pre-trained model. However, I found most of the compared methods except the proposed one perform even worse than T5, which is hard to believe.",
            "clarity,_quality,_novelty_and_reproducibility": "The authors should publicize their code, and testing results, which are not stated in the paper.",
            "summary_of_the_review": "Overall, I think the idea is reasonable but the experiments are a bit weak.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3751/Reviewer_3nYS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3751/Reviewer_3nYS"
        ]
    },
    {
        "id": "Oad8QIGg_1F",
        "original": null,
        "number": 2,
        "cdate": 1666613382223,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613382223,
        "tmdate": 1666613382223,
        "tddate": null,
        "forum": "k5PEHHY4spM",
        "replyto": "k5PEHHY4spM",
        "invitation": "ICLR.cc/2023/Conference/Paper3751/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores mixture models for diverse dialogue generation, following the same pipeline proposed by Shen et al., (2019). This method adopts a multi-decoder model to address the one-to-many mapping phenomenon, where a multi-adapter architecture is used to form the multi-decoder model. Based on that, the EM algorithm (soft or hard) is leveraged to learn the latent variable (i.e.,  the choice of the decoder). The authors further handle the non-training collapse of the Hard-EM algorithm by introducing equal-assignment constraints, which ensure that all decoders are adequately trained. Automatic evaluations on Weibo and OpenSubtitles datasets prove the effectiveness of introduced equal-assignment constraints, compared with the original EM algorithm.",
            "strength_and_weaknesses": "Strengths:\n- This paper explores mixture models for dialogue generation tasks. \n- This paper proposes EqHard-EM to handle the non-training collapse of the Hard-EM algorithm.\n\nWeaknesses:\n- It is an incremental work of Shen et al., (2019) and the novelty of the proposed method is limited. \n- It is fine to apply and extend the method of Shen et al., (2019) in dialogue generation tasks, but the experiments are not good enough, including implementing all model variants in Shen et al., (2019) and full human evaluation. \n- The main contribution of this paper is to introduce equal-assignment constraints to handle the non-training collapse of the Hard-EM algorithm. However, the E-step of Hard-EM in this paper is different from Shen et al., (2019), which leverages learned prior (lp) or uniform prior (up) to ease this issue. It is difficult to judge that this method could lead to further improvements compared with Shen et al., (2019)'s version of Hard-EM.\n\n\nDetailed Comments:\n\nOverall, this paper is easy to follow and this idea is straightforward. I like this idea of introducing mixture models for diverse dialogue generation. But the experiments are not convincing enough to me. The E-step of Hard-EM in this paper is different from Shen et al., (2019), which leverages learned prior (lp) or uniform prior (up) to ease the non-training collapse of the Hard-EM algorithm. It is not sure that this method could lead to further improvements when combined with the version of the Hard-EM algorithm of Shen et al., (2019). It is necessary to reproduce the version of Shen et al., (2019) in the dialogue generation task.\n\nWhile Shen et al., (2019) have explored four model variants and some crucial tricks (e.g., the effect of dropout) for diverse machine translation, it is better to revisit these models and details in dialogue generation. In addition, this paper lacks full human evaluations across two datasets and different methods, and automatic evaluations are not convincing enough in dialogue generation tasks. The authors only perform human evaluations on beam search and the proposed method, which should be improved.\n\n\nQuestions for the Author(s):\n- As shown in Table 5, the example of the Hard-EM algorithm tends to repeat. Since we directly optimize the conditional language model in M-step, these examples are weird. What is your thought about it?\n- Why is the E-step of the Hard-EM algorithm in this paper different from Shen et al., (2019)? ( i.e.,  argmax_{z} p (z| x,y; \\theta) v.s.  argmax_{z} p (y| z,y; \\theta)  )\n- Have you evaluated the effect of the dropout during the EM algorithm? \n- How about training different decoders instead of introducing adapter layers?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to Detailed Comments.\n",
            "summary_of_the_review": "This paper investigates introducing mixture models for diverse dialogue generation and proposes EqHard-EM to handle the non-training collapse of the Hard-EM algorithm. But the experiments are not convincing enough to me, including lacking more details of all model variants in Shen et al., (2019) and full human evaluation. The E-step of Hard-EM in this paper is also different from Shen et al., (2019), and it is uncertain whether this method will be further improved when combined with the version of Shen et al., (2019).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3751/Reviewer_ov7h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3751/Reviewer_ov7h"
        ]
    },
    {
        "id": "VGABuRC_C7",
        "original": null,
        "number": 3,
        "cdate": 1666669474003,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669474003,
        "tmdate": 1666669474003,
        "tddate": null,
        "forum": "k5PEHHY4spM",
        "replyto": "k5PEHHY4spM",
        "invitation": "ICLR.cc/2023/Conference/Paper3751/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In order to solve the issue of generic responses generated by the neural dialogue systems because of one-to-many mapping phenomenon, the authors of the paper studied the state-of-the-art methods including the methods for training time and reference time, the application of the standard EM, Soft-EM, Hard EM and etc. In the paper, the analysis of the disadvantages of these methods was given and a new method EqHard-EM algorithm was proposed. \nIn EqHard-EM, this paper proposed a multi-adaptor architecture with multi-decoder and a shared encoder, adopted hard assignments from Hard-EM but impose equal-assignment constraints to ensure all decoders are adequately trained. To evaluate the new method, the paper included the experiments on Weibo and Open Subtitles. The results and the comparison with the state-of-the-are methods and EM variants have been given in the paper. In the results, the EqHard-EM demonstrated its advantages again EM variants and a few state-of-the-art methods. ",
            "strength_and_weaknesses": "Strength: \nThe study of the literature for the solutions to justify the  one-to-many mapping phenomenon was complete. The paper also analyzed the issues coming from standard EM, Soft-EM and Hard-EM. \nIn the proposed new method, new neural architecture based on multi-adaptor architecture was created on top of and beyond a few state-of-the-art methods. Training methods and the theoretical analysis was given. \nWeakness or questions: \nIt was not that clear how the Theorem 1 came up. Can the author elaborate more details of it? \nThe organization of the paper can be improved. It maybe better to have related work section (Section 4) come earlier because now there are duplications among Section 1 (introduction), Section 2.2 and Section 4. ",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method in the paper is novel and the ideas came from a completed analysis of related methods and the innovations beyond them. \nAs I said in the above weakness, the organization of the paper can be improved.  ",
            "summary_of_the_review": "The method in the paper is new and developed based on a good study of related method and innovations/extensions beyond them. It justifies the one-to-many mapping phenomenon and the results of the two experiments demonstrates advantages comparing to others. \nThis paper will benefit the readers with novel approaches. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3751/Reviewer_wvrp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3751/Reviewer_wvrp"
        ]
    },
    {
        "id": "LH84s3zANF",
        "original": null,
        "number": 4,
        "cdate": 1667699438237,
        "mdate": 1667699438237,
        "ddate": null,
        "tcdate": 1667699438237,
        "tmdate": 1667699438237,
        "tddate": null,
        "forum": "k5PEHHY4spM",
        "replyto": "k5PEHHY4spM",
        "invitation": "ICLR.cc/2023/Conference/Paper3751/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed the use of Equal size hard EM algorithm to address the problem of diverse response generation in open domain dialog modeling. The paper modified the EM algorithm to overcome mode collapse issues with hard EM and synchronous training collapse issues with soft EM. Experimental results show improvement in diverse response generation on Weibo and Open subtitle datasets. Ablation studies also show that Equal size hard EM performs better than beam search, soft EM and pure hard EM. Qualitative analysis also shows that some of the modes learned unique attributes of the dialog data such as response length, question mark, and like/love. The paper also shows that the performance improves with increasing number of decoders, although the incremental improvement reduces. Overall, the paper is clearly written and well motivated.",
            "strength_and_weaknesses": "Strength:\nThe paper is clearly written and easy to follow. There are couple of minor typos to be fixed. The proposed method is novel and experimental results is thorough. The ablation study is also informative. \n\nWeakness:\nThe use of mixture of experts for dialog modeling is not new, but the implementation using adaptor is new. The evaluation did not include existing baseline that have used GAN or reinforcement learning to accomplish diverse response generation. Also, it is not clear if encode-decoder architecture is suitable for dialog response generation as the conditional probability modeling P(y/x) exacerbate the one-to-many problem. Would be useful if authors could compare the proposed method with finetuning vanilla GPT-2 modeling P(x,y) with the evaluated dataset as explored in [1].\n\n[1] https://arxiv.org/pdf/1908.01841.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. The use of EM method for response generation which does not suffer from model collapse is novel. However, it's not clear how the approach compares to existing work that have attempted to solve similar problem of diverse response generation using GAN, reinforcement learning, or joint distribution modeling. ",
            "summary_of_the_review": "The proposed method of Equal-size hard EM is novel and well motivated and supported by experimental results. However, the paper needs to add other baselines to really evaluate the diverse response generation capability of the proposed approach.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3751/Reviewer_EYAz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3751/Reviewer_EYAz"
        ]
    }
]