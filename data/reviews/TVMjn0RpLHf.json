[
    {
        "id": "cI7CyaGob6",
        "original": null,
        "number": 1,
        "cdate": 1666588370257,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588370257,
        "tmdate": 1669385662549,
        "tddate": null,
        "forum": "TVMjn0RpLHf",
        "replyto": "TVMjn0RpLHf",
        "invitation": "ICLR.cc/2023/Conference/Paper6014/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a way to solve the problem that GNNs tend to increase Bias. First, the authors give a mathematical interpretation of why GNNs increase Bias. Based on the causes obtained from this interpretation, we propose a method to suppress the increase in bias that occurs during message passing. Experimental results showing the effectiveness of the method are also presented.",
            "strength_and_weaknesses": "Weakness\n- The biggest problem is that the content is not complete, especially after Section.4. This makes the evaluation of the proposed methodology difficult. If we try to interpret and understand what is written in a favorable manner, we can assume that the claims are probably correct, but we cannot accept them as being of publishable quality. I think the assignment settings and methods are interesting and should be evaluated carefully after they are completed.\n- Specifically, the major problems are as follows\n\t- I'm not sure what equation (2) means, but since it seems to be an optimization problem, perhaps\n$$\\min L s.t.\\tilde{A}={ij}\\in\\{0,1\\}$$\nwhere $L=H1+\\alpha H2+\\beta H3$?\n\t- Regarding Table 1, it says \"To validate the effect of bias enhancement of GNNs,\" but it is not a comparison with the proposed method. The intent may be to confirm the current status, but in that case the text is confusing, and furthermore, there should be a separate comparison with the proposed method.\n\t- Figure. 6 is reffered, but it is not appropriate to refer the figure in the suppliment. Figure.3 may be correct instead of Figure.6. It seems that a comparison with the proposed method may be more appropriate for the content in one piece. Therefore, it may be appropriate and intended to publish Fig. 6 in the main body. However, even in that Fig. 6, two of the three are compared to the proposed method, but the quality of the remaining one is poor, with no comparison.\n\t- It may not be required, but it is not reader friendly to have RELATED WORK at the end.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is simple, but the fact that it uses an analysis of why Bias increases as a basis for its proposal is an originality novelty. However, the quality is poor and unclear at the stage of showing the effect of the proposed method.",
            "summary_of_the_review": "Issues related to Bias are important issues in AI operations, and the fact that GNNs tend to increase Bias is problematic, and it is a very important task to analyze these factors and then present improvement methods. The analysis of the causes of GNNs increasing Bias seems justified, and the improvement methods are highly convincing.\n\nOn the other hand, since the quality of the structure since the proposal of the methodology is in a low state and the information to evaluate the methodology is unclear, the text should first be refined and the evaluation of the methodology should be clarified. We believe this will result in something that can be made public.\n\n**Conclusion following discussion with the authors**\n\nThe problems with the sentence structure that were hindering my understanding have been largely remedied. As noted before, the value of the methodology itself is positive. For this reason, I am raising the score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6014/Reviewer_bTV8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6014/Reviewer_bTV8"
        ]
    },
    {
        "id": "Wb0YDNJVe4",
        "original": null,
        "number": 2,
        "cdate": 1666636390836,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636390836,
        "tmdate": 1666636390836,
        "tddate": null,
        "forum": "TVMjn0RpLHf",
        "replyto": "TVMjn0RpLHf",
        "invitation": "ICLR.cc/2023/Conference/Paper6014/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies prediction bias amplification of graph neural networks using contextualized stochastic block model. Specifically, their analysis utilizes a distribution distance as bias measurement and discusses when $\\Delta$ bias is enhanced regarding homophily edge ratio, graph size and density. Based on it, the author proposes a regularization framework FairGR to rewire the graph structure and reduce the representation bias. Experiment shows FairGR achieves both good accuracy and low bias. ",
            "strength_and_weaknesses": "Strength:\n1. The analysis is theoretically sound with appropriate discussion and remarks.\n2. The proposed FairGR coupled with proposed representation bias closely.\n\nWeakness:\n1. The motivation of FairGR is unclear. From Table 1, we observe GNN is just on par or slight better than MLP while the fairness measure is a lot worse. Does that mean Fair learning on GNN is not necessary on these datasets ? Furthermore, whether the proposed FairGR can reduce $\\Delta_\\text{DP}$ and $\\Delta_\\text{EO}$ to the level of MLP in Table 1.\n2. A lot of notations in the paper are not clear or assuming readers with relevant knowledge. For example, in definition 2, \"random graph sampled from ()\", after reading the whole paper, I realize the generalization process is exact the contextual stochastic block model (CSBM), it is very confusing in this definition about how A and X are generated. \n3. The novelty is limited. Based on my understanding, if we treat sensitive feature as the only node feature, the paper's contribution is not that different from [1][2], which both talks about GNN's concentration property. \n[1] IS HOMOPHILY A NECESSITY FOR GRAPH NEURAL NETWORKS? ICLR 22'\n[2] Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization ICML 21'",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper can be greatly improved on clarity. The main analysis is also similar with existing work.",
            "summary_of_the_review": "Overall, the idea of the paper is clear and proposed method is straightforward. My main concern is around the novelty and experiments. First of all, the representation bias is not that different with other homophily understanding on GNNs. Second, the selected datasets cannot convince that proposed method is significant for fairness learning: (1) GNN is not better than MLP on accuracy (2) I suspect the bias of FairGR is still larger than MLP.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6014/Reviewer_K1rB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6014/Reviewer_K1rB"
        ]
    },
    {
        "id": "dphyyu9vW3A",
        "original": null,
        "number": 3,
        "cdate": 1666644579342,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644579342,
        "tmdate": 1666646176745,
        "tddate": null,
        "forum": "TVMjn0RpLHf",
        "replyto": "TVMjn0RpLHf",
        "invitation": "ICLR.cc/2023/Conference/Paper6014/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this article, the authors propose to study and improve fairness in graph neural networks. More precisely, they provide conditions under which the graph aggregation steps increase prediction biases, and they propose an optimization problem that allows to find an optimal graph topology that reduces biases while preserving topology information. They finally demonstrate the efficiency of their approach on several experiments.",
            "strength_and_weaknesses": "Strengths:\n---The paper is overall well written but a bit hard to follow\n---To my knowledge, it addresses an interesting problem about graph neural nets\n---The experiments are convincing\n\nWeaknesses:\n---Some parts of the method have a little bit of a heuristic feeling; for instance, I am wondering if other models could be used (instead of gaussian mixtures) for random graphs? Is there a reason (other than mathematical computations) for using such random models?\n---Similarly, there is no discussion about the optimization properties of (2). Would it be possible to show some kind of convergence results, or to prove anything concerning the limit point?\n---Graph topology is often characterized with persistent homology. Are there links between the proposed optimization scheme and recent papers about topology optimization based on persistence diagrams? (see for instance http://proceedings.mlr.press/v139/carriere21a.html) It would be nice to add some discussion on this point.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I am not very knowledgeable in graph neural networks, but the proposed approach looks quite original and easy to reproduce to me.",
            "summary_of_the_review": "Overall, the paper looks good to me, even though I am no expert in graph neural networks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6014/Reviewer_qbDf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6014/Reviewer_qbDf"
        ]
    },
    {
        "id": "US9iQcKl4Tw",
        "original": null,
        "number": 4,
        "cdate": 1666679406837,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679406837,
        "tmdate": 1666679525146,
        "tddate": null,
        "forum": "TVMjn0RpLHf",
        "replyto": "TVMjn0RpLHf",
        "invitation": "ICLR.cc/2023/Conference/Paper6014/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of fairness-aware learning on graphs from the perspective of graph topology. According to the authors, this is the first work that theoretically understands the different roles graph statistical information played in the node representation bias amplification during message passing. Then a graph refinement method FairGR is proposed based on the theoretical results to reduce the sensitive homophily accordingly. \nExperiments on both synthetic and real-world datasets show the effectiveness of FairGR by achieving better tradeoff performance over three representative baselines.\n",
            "strength_and_weaknesses": "Strengths:\n* The investigation of fairness-aware message-passing from the topology view is interesting.\n* The theoretical foundations on node representation bias amplification are concrete with insights. Experiments are carefully designed and are conducted on both synthetic and real-world graph datasets.\n* The paper is generally well-written (though with typos and grammar errors) and almost clear everywhere. \n\nWeaknesses:\n* Some typos appear as I walk through the manuscript. For example, by that fact -> by the fact, the last section on page 1; many enhance -> may enhance, the last section on page 5. I suggest the authors do thorough proofreading during the rebuttal.\n\n* The scalability of the proposed FairGR is not demonstrated and the optimization strategy is briefly described without computational analysis, which is important to employ in practical scenarios for graphs with massive nodes. Meanwhile, the sensitivity analysis of hyperparameters $\\alpha$ and $\\beta$ is suggested to be included for better demonstrating the contributions of label homophily coefficient and graph topology perturbation during learning.\n\n* Synthetic experiments from Figure 2 are somehow vague and confusing. Since it shows the DP difference during message passing with respect to the sensitive homophily coefficient, what is the instantiation of GNNs used here for the message-passing process? Does MLP also show similar behavior regarding DP difference and sensitivity homophily here?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Most of the paper is clear except for some experimental setups that are a little vague.\n\nQuality: The theoretical analysis is concrete but the writing can be further improved.\n\nNovelty: The idea of theoretically analyzing the fairness-aware graph learning approaches has novelty and merits.\n\nReproducibility: The reproducibility is relatively weak with no codes provided.\n",
            "summary_of_the_review": "In summary, my concerns are mainly from two aspects:\n* The scalability of FairGR could be further demonstrated through complexity analysis. The sensitivity of hyperparameters should be addressed.\n* The details of experimental setups could be further explained for more convincing empirical support to the theoretical foundations.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6014/Reviewer_SmbS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6014/Reviewer_SmbS"
        ]
    }
]