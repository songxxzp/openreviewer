[
    {
        "id": "Ixr4annrjRZ",
        "original": null,
        "number": 1,
        "cdate": 1665870962446,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665870962446,
        "tmdate": 1669828393453,
        "tddate": null,
        "forum": "PbfgkZ2HdbE",
        "replyto": "PbfgkZ2HdbE",
        "invitation": "ICLR.cc/2023/Conference/Paper3242/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper learns a policy to automatically re-mesh the current mesh used in physics simulation, so that delicate regions where rich dynamics happens can leverage more computational power. It uses reinforcement learning to train the policy end-to-end, with the reward being a combination of simulation accuracy and speed. The resulting method, called LAMP, is evaluated on simple 1D PDE and 2D mesh-based paper simulation. ",
            "strength_and_weaknesses": "Strength\n1. The paper is well-written on the high-level and easy to follow. \n2. The idea is straightforward and clearly presented. \n\nWeakness\n1. The paper missed important literatures to compare against. \n2. To further verify the approach, more thorough experiments need to be performed.  \n3. Some important details are missing, which could prevent audience unfamiliar with the literature from fully understanding this paper. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general the paper is quite easy to follow. On the other hand, it has a few important limitations: \n\nA. Literature. \n\nThe paper did not reference and/or compare with recent paper \"Reinforcement Learning for Adaptive Mesh Refinement\" (https://arxiv.org/abs/2103.01342), which focus on the same applications (DRL for AMR). I would strongly encourage the authors to make a formal comparison with the work.  \n\nB. To further verify the approach, more thorough experiments need to be performed.  \n\n1. From the paper, it is clear that re-meshing is important but I am not sure whether RL is necessary. Did the author compare LAMP with simple heuristics (e.g., if the velocity in a local neighborhood is high, then it is obvious that the neighborhood requires some refinement)?  How does LAMP deal with PDE with large stiffness, e.g., the dynamics of a piece of cloth contacting with an immovable support. Will LAMP discover the stiffness in time and refine the mesh, and does it do better than existing re-meshing heuristics used in adaptive solvers (e.g., ZZ policy mentioned in https://arxiv.org/abs/2103.01342)? Given the current evaluation, there is no convincing evidence that RL is necessary. \n\n2. What's the gap between LAMP and the ground-truth simulation using very fine-grained mesh? Table 1 shows that the number of vertices is up to 100. How does this approach scale to simulation of larger scale?\n\nC. Some important details are missing, which could prevent audience unfamiliar with the literature from fully understanding this paper. For example:   \n\n1. The definition of action space is quite vague. While I see the definition of refinement/coarsening action in the appendix for 2D simulation, many details still remain unclear. Are split+flip together counted as \"refinement\"? Why \"two edges on the same face of the mesh cannot be refined at the same time, nor can they be both coarsened\"? What's the intuition? Can an example be given? In the equation of Sec 3.1 (under action representation), how the conditional probability $p(K^{re})$ and $p(K^{co})$ are evaluated, are you using softmax? How does summation over random variables $p(K^{re})$ and $p(K^{co})$ work? It is not clear at all from the text. Would be good to put a figure to explain. Also, how these actions defined in 1D still remain unclear.  \n\n2. In Sec. 3.2, the paper mentioned that the ground truth is computed with ground-truth solver \"with very fine-grained mesh\". Does the ground-truth simulation start from initial time step t = 0? What if the mesh predicted by the current policy is very far from the ground truth? If the simulation starts from t (and execute for S steps), then will such on-policy ground truth evaluation lead to very slow training process? Unfortunately, I didn't see any evaluation regarding to the time cost. \n\n3. It is not clear how the training and evaluation PDE distribution are picked. Is the performance reported as in-distribution or out-of-distribution? \n\nDue to clarity issues in the paper, it may not be able to be reproduced easily.",
            "summary_of_the_review": "Overall due to these important limitations, I cannot recommend acceptance.  \n\n======\n\nUPDATE: the authors have made substantial efforts and addressed most of my concerns. I really appreciate. Thanks! \n\nAs a result, I raised the score to 6 and will not object acceptance of this paper in ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3242/Reviewer_tR71"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3242/Reviewer_tR71"
        ]
    },
    {
        "id": "diMRLEixBi",
        "original": null,
        "number": 2,
        "cdate": 1666559746033,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559746033,
        "tmdate": 1666559746033,
        "tddate": null,
        "forum": "PbfgkZ2HdbE",
        "replyto": "PbfgkZ2HdbE",
        "invitation": "ICLR.cc/2023/Conference/Paper3242/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to jointly learn the dynamics as well as a remeshing scheme for simulation based on discretized (triangular mesh) space. To consider both the simulation accuracy and the computational cost, this problem is formulated as a multi-objective optimization. Two sets of graph neural networks are used for learning the forward evolution and the remeshing policy, respectively. Experiments show that the relative importance of simulation error vs. computational cost can be adjusted by choosing different weight values during the training.",
            "strength_and_weaknesses": "Strength\nThis method models the adaptive mesh simulation as a multi-objective optimize and provides users with a convenient way to modify the relative importance of computational cost and simulation fidelity.\n\nMoreover, the way they handle remeshing and treat the computational cost as a objective function is interesting. It could probably be generalized to other relative tasks other than simulation, e.g. inverse rendering with adaptive remsheing.\n\nWeakness\nFirst, the keyword `multi-scale` in the title seems not very appropriate to me. Even though this method can adaptively change the resolution in some local areas, all the dynamics are governed by the same evolution network. There are no multiscale or hierarchical structures as I can see. Moreover, according to the experiment results, the number of vertices does not change a lot (33->37.6, 50->53.2, 100->100.0, and 81.0->97.9). It is rather refinements than multi-scale physics.\n\nThe biggest concern for me is the experiment part. MeshGraphNet is its backbone and it also should be an important baseline, since MeshGraphNet itself can perform remeshing. However, it is not compared against MeshGraphNet. \n\nMoreover, the paper also mentioned that it wants to verify \u2018Can LAMP improve the Pareto frontier of Error vs. Computation, compared to state-of-the-art deep learning surrogate models\u2019. But I did not find this experiment. Figure 3 is not referred to in the main text. I did not understand what does this figure mean. If possible, I\u2019d like to see whether the Pareto frontier of this method can cover the performance of MeshGraphNet.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\nThis paper is a little be hard to follow for me. There are so many different symbols in Section 3. I\u2019m wondering whether a high-level diagram could improve its presentation. Also, Figure 3 should be important, but I did not find where it is referred in the main paper.\n\nQuality\nThe experiments lack an important comparison with MeshGraphNet, which is the backbone of this work, and it can also learns to remesh. \n\nMoreover, only two experiment cases are shown here (1D PDE and 2D square cloth). The authors could also use examples demonstrated in MeshGraphNet to make their evaluation more solid and comprehensive.\n\nFurthermore, will the scale of the experiments be too small? In Section 3.1, the authors use Nedge = 1000 as an example. In contrast, the experiments only have ~100 edges.\n\nNovelty\nThe concept of adaptive and adjustable remeshing from learning looks exciting to me. The novelty seems okay to me, as long as the paper could really demonstrate the benefit and effectiveness of its proposed method.\n\nReproducibility\nThere are many components (networks, RL, remeshing modules, etc) and hyperparameters in this pipeline, which are described as abstract symbols. I am afraid it might be hard to reproduce unless the authors can release their code.\n",
            "summary_of_the_review": "In summary, I think this paper is tackling an important problem in an interesting way. However, the experiments are not sufficient to really validate the advantage of their method. I hope the users can add more test cases and compare their method with MeshGraphNet. The composition of this paper can also be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3242/Reviewer_WPia"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3242/Reviewer_WPia"
        ]
    },
    {
        "id": "9WjB62yDBK",
        "original": null,
        "number": 3,
        "cdate": 1666589715085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589715085,
        "tmdate": 1670832896073,
        "tddate": null,
        "forum": "PbfgkZ2HdbE",
        "replyto": "PbfgkZ2HdbE",
        "invitation": "ICLR.cc/2023/Conference/Paper3242/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors propose a novel methodology based on Graph Neural Networks (and reinforcement learning) to predict the evolution of a process of interest. The primary context in which the proposed `LAMP`  (Learning Controllable Adaptive simulation for Multi-scale Physics) model is applied is to forecast the evolution of systems governed by PDEs and on a real-world task of predicting the evolution of deformation of paper with forces acting on it.  Overall, the main novelty of the proposed model is that the model is able to not only forecast the evolution of states of the process of interest, but is also able to adaptively yield fine-grained / coarse-grained predictions as needed based on the inferences made by a learnable adaptive-mesh-refinement policy network (also part of the LAMP pipeline).  The results demonstrate that LAMP yields results superior to powerful state-of-the-art PDE forecasting models. \n\nAfter the authors' responses especially about the ablation study, I am relatively more convinced about the effectiveness about the proposed method and that the proposed method is indeed able to address an important problem of developing ML based surrogates of scientific simulations while also being able to perform adaptive meshing on the domain.",
            "strength_and_weaknesses": "## Strengths: \n\nThe paper proposes a novel (LAMP) model which jointly learns to predict the evolution of a process of interest (using graph-neural network based architecture) as well as learns to adaptively refine / coarsen various parts of the network topology (I.e., mesh) of the domain being forecast. The proposed loss functions combining  RL policy learning task and domain evolution learning task are novel and indicated to be effective from the presented results. \n \n\nThe results demonstrate the power of the LAMP model over other state-of-the-art models for forecasting PDE trajectories including the benefit of mesh-refinement which is one of the novel proposals made by the current work. Further, the mesh refinement (measured in Table 1, as Avg. # vertices) does not seem to significantly increase the number of nodes (I.e., the refinement policy seems to be applied systematically without leading to an explosion of the problem complexity). \n \n\nThe problem formulation, especially as a balance between the number of FLOPs (computational cost of refinement / coarsening resulting form the RL policy actions) and long-term prediction error is novel and effective. \n \n\nThe paper is (mostly) clearly written and well organized but for a clear description of the experimental setup. \n\n## Weaknesses: \n\nMy major problem with the paper is the lack of a cohesive / complete discussion of the experimental setup. The proposed modeling pipeline (RL + forecasting using graph neural networks where each of the forecasting and RL based losses further have multiple tasks optimized therein) has a non-trivial organization and many hyperparamteres / steps that are not described in detail. E.g., Eq. 4 the $g^{\\mathrm{interp}}(\\cdot)$ function does not find detailed mention in the discussion. \n \n\nThe paper does not compare with MeshGraphNets [1], which is the base model the paper employs to design its framework. Without this, it is hard to contextualize the degree of improvement of the prediction performance of the proposed framework. Although the current paper makes other contributions (I.e., adaptive-mesh-refinement based on learned policy) it is still important to understand how the evolution performance in the LAMP model fares w.r.t MeshGraphNets which is a closely related model. \n \n\nFurther, without access to the source code, it is non-trivial to fully comprehend the effectiveness of the proposed pipeline with so many moving parts.   \n \n\n## Questions for Authors: \n\n1. How are $\\alpha_s^{\\mathrm{policy}}$, $\\alpha_s^{\\mathbb{I}}$ parameters (Eq. 10) set / learned? \n \n\n2. How sensitive is the proposed model to values of $\\beta$, $\\alpha_s^{\\mathrm{policy}}$, $\\alpha_s^{\\mathbb{I}}$? \n \n\n3. What type of interpolation function is used for $g^{\\mathrm{interp}}(\\cdot)$ ? How many types of interpolations were tried, and which was the most effective? Also, how does the computational cost of $g^{\\mathrm{interp}}(\\cdot)$ figure into the cost of the overall learning model? \n \n\n4. What is the reasoning behind the selection of an alternating strategy to train the evolution and policy losses? \n \n\n5. According to Eq. 9, the $L^{\\mathrm{evo}}_S$ term is comprised of a fine-grained mesh based forecasting loss (second term) and a forecasting loss based on the mesh generated by the RL policy. What would be the effect of decaying the second term I.e., fine-grained mesh based forecasting over the course of model training as this might have significant effects on decrease in computation time of training for large differences between fine-grained / coarse-grained meshes? \n \n \n## References\n1. Pfaff T, Fortunato M, Sanchez-Gonzalez A, Battaglia PW. Learning mesh-based simulation with graph networks. arXiv preprint arXiv:2010.03409. 2020 Oct 7.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper quality and organization as well as the novelty of the proposed method is good. However, reproducibility of the method is hard as the source-code and data were inaccessible at the time of review.",
            "summary_of_the_review": "The proposed LAMP model is novel as it is able to effectively develop a graph neural network based model for forecasting evolution of non-linear PDE based domains along with adaptive-mesh-refinement as proposed by a learned RL policy (jointly learned with the evolution learning component of the LAMP pipeline).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3242/Reviewer_mWzR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3242/Reviewer_mWzR"
        ]
    },
    {
        "id": "zAoBgYWXYc",
        "original": null,
        "number": 4,
        "cdate": 1666793642124,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666793642124,
        "tmdate": 1668772385570,
        "tddate": null,
        "forum": "PbfgkZ2HdbE",
        "replyto": "PbfgkZ2HdbE",
        "invitation": "ICLR.cc/2023/Conference/Paper3242/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes uses an actor-critic for adapting the simulation mesh, specifically to add or collapse edges. The approach explicitly makes a trade-off between error and computational cost, allowing to tune it at test time.",
            "strength_and_weaknesses": "\n**Strengths**\n\nThe main advantage of the proposed approach is being to trained the re-meshing procedure in an unsupervised way, while the previous approach requires explicit supervision on the sizing field provided by the simulator or estimated (assuming that the GT simulations include re-meshing). The unsupervised approach can be particularly advantageous on the real data, where the information about ground-truth re-meshing is not available.\n\n**Weaknesses**\n\n1. Choice of K^max and evaluation on larger experiments. \u2028If my understanding is correct, the model performs up to K^max actions of refinement and coarsening at each step of the simulation. Intuitively, K^max should be higher on the more fine mesh, as we might need to modify more edges. How was K^max chosen?\n\n    The current paper evaluated the simulations only up to 100 nodes, while the previous MeshGraphNet paper demonstrated the results on up to ~5000 nodes. Do the authors think that the approach can scale to this number of nodes. How do you expect the choice of K^max to change, if we increase the number of nodes to 5000? Given that the action space is N_edge^K, do the authors think that it would be feasible to train the method with such a large action space?\n\n\n2. The choice of baselines and comparison to the MeshGraphNet+remeshing\nI am surprised with the choice of the baselines. The cloth mesh can be naturally represented as a graph, but the paper provides only one \none graph-net baseline (LAMP with not re-meshing). Using the CNN as a baseline enforces that the initial mesh should be represented as a grid, which is quite limiting.\n\n    Also, the paper does not provide the comparison to the previous MeshGraphNet method *with re-meshing*, as it is the only other method that uses re-meshing. It is not an entirely fair comparison, as MeshGraphNets use supervision on re-meshing, but it would be useful to compare which parts of the mesh are modified, and how many edges are added/collapsed between the two approaches.  \n\n**A few questions**\n\n1. It seems that the re-meshing process only adds 3-4 vertices on average (Table 1). I am surprised that it results in such a big change in MSE for 30 and 50 initial edges. Can the authors explain why this might be the case? Also, do you have a sense of how many edges were actually changed through re-meshing? For example, it is possible that some edges were added/collapsed, but the average number of vertices stayed the same.\n\n2. Figure 4: LAMP seems to add new edges to a different regions than the ground-truth. Specifically, GT adds more edges on the cloth bending with the highest curvature, while LAMP adds the edges closer to the centre of the cloth. Can the authors provide the intuition why this might be the case and, perhaps, how to remedy this?\n\n3. The RL-based approaches tend to require a lot of training episodes, especially if several edges are modified at each time step. How many episodes are roughly needed to train the policy and the value function. Also, is the model trained together with f_evo? Do you have a sense how the required number of episodes changes with the choice of K^max?\n",
            "clarity,_quality,_novelty_and_reproducibility": "As far as the method, the paper is easy to follow and the approach is well motivated. The approach is clearly described, but might have missed some of the details on training the model. Specifically: How the episodes for the RL training are set up? Is f_evo pre-trained or trained jointly with the re-meshing? How to determine the number of action steps K? What K did you use?  During the training, do you compute the reward on the rollouts or after every time step *t*?",
            "summary_of_the_review": "Overall, learning how to modify the mesh in an unsupervised way using RL makes a lot of sense. It allows to 1) increase the simulation accuracy by adding vertices to the part of the cloth with high curvature 2) save computation time by removing vertices from the mesh with low curvature 3) unlike the previous MeshGraphNet approach, LAMP does not require to have the re-meshing sizing field from the ground-truth simulator, which is hard to obtain. \n\nHowever, it will be helpful to have more insight in the choice of K^max and how the required number of coarsening/refinement steps is needed for different mesh sizes. It is also concerning that the approach adds only 3-4 vertices on average and does not add new vertices to the mesh with 100 nodes. Also, Figure 4 shows that the approach does not add the edges to the parts of the cloth with the highest curvature. It would be great to get author\u2019s insights on why the might be the case. ~I am giving borderline accept, but willing to increase to accept if the authors demonstrate the ability to scale to meshes with >100 nodes and clarify why so few vertices need  to be added to the mesh.~\n\nEDIT: the authors addressed all my concerns about scalability, provided more examples and explanation how the edges are refined/coarsened and added a relevant baseline with the MeshGraphNet. The updated version of the paper is convincing and provides value to the learning simulation community. I have increased the score to 8 (accept).\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3242/Reviewer_vMEa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3242/Reviewer_vMEa"
        ]
    }
]