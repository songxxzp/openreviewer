[
    {
        "id": "ZUrMBW8Kij",
        "original": null,
        "number": 1,
        "cdate": 1666532453467,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666532453467,
        "tmdate": 1669899447649,
        "tddate": null,
        "forum": "k2CRIF8tJ7Y",
        "replyto": "k2CRIF8tJ7Y",
        "invitation": "ICLR.cc/2023/Conference/Paper4165/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes using a reweighted kernel by the target distribution for Stein variational gradient descent (SVGD) and provides its convergence analysis. Specifically, the authors show the convergence of the proposed method under the standard log-Sobolev inequality (LSI) with additional assumptions on the trajectory which are empirically verified by the synthetic experiment.",
            "strength_and_weaknesses": "**Strengths**:\n\nThe convergence analysis of SVGD is challenging because of the vanishing gradient phenomenon caused by the kernel smoothing of the gradient. Previously, Stein LSI was used for the convergence analysis in the related studies, but examples which satisfy this condition are very limited. Instead, this work managed to show the convergence by exploiting the standard LSI  like Langevin dynamics. Moreover, it is also clarified that a reweighted kernel basically provides a sufficient condition for Stein LSI. Thus, this work well contributes to this context.\n\n**Weaknesses**:\n\n- There are several limitations in the main result (Theorem 3.1) compared to Langevin dynamics. For instance, the theory requires a warm start condition $D_X(p_0,p/*)\\leq 1/4$ and bounded and smoothness conditions on the trajectory of $\\{p_t\\}$ in Assumptions (A2) and (A3).  Although the latter conditions are verified empirically in a synthetic experiment, it would be nice if the authors could provide a specific example. Such an example will make the paper stronger.\n\n- A reweighted kernel involves the value of the target distribution $p_*$. Thus, the computation of the normalization term is needed. This computation is not trivial and approximation error to the normalization usually remains. How does the error affect the convergence rate? It would be better to clarify this.\n\n- SVGD with a reweighted kernel using a small $\\sigma$ will finally get close to the Wasserstein gradient flow of KL-divergence when $p_t$ is close to $p_*$. Thus, I\u2019m wondering what the advantage is over Langevin dynamics. The empirical or theoretical comparison with Langevin dynamics will be helpful if possible.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and easy to read. The novelty and significance are sufficient as commented above.\n\nMinor comments:\n\n- Typos in Eq. (10): $\\nabla \\phi_t (x)$ \u2192 $\\nabla \\phi(x)$ and $\\leq S(p_t,q)$ \u2192 $\\leq 1$ (?)\n- The definition of notations $p_{k,z}$ and $p_{*,z}$ in Eq. (16) are missing in the main text.\n- What is the constant $C$ in Eq. (21)?",
            "summary_of_the_review": "The paper certainly contributes to the context by giving a specific example for Stein LSI. But there are still limitations compared to Langevin dynamics. Examples that satisfy additional assumptions (A2) and (A3) would increase the significance of the paper.\n\n-- After reading the author's response --\nMy concerns have been well addressed. Indeed, I agree with reviewer 8ZrX that assumptions are rather strong. However, I think this paper suggests a new strategy for the convergence analysis of SVRG under Stein LSI, which is a quite challenging problem. Thus, I would like to keep the score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4165/Reviewer_ecWW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4165/Reviewer_ecWW"
        ]
    },
    {
        "id": "22_zopAG5J8",
        "original": null,
        "number": 2,
        "cdate": 1666579079609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579079609,
        "tmdate": 1666579079609,
        "tddate": null,
        "forum": "k2CRIF8tJ7Y",
        "replyto": "k2CRIF8tJ7Y",
        "invitation": "ICLR.cc/2023/Conference/Paper4165/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the convergence rate of SVGD with a reweighted kernel. The main theoretical result is the local linear convergence rate to the target in the sense of KL divergence using the reweighted kernel. ",
            "strength_and_weaknesses": "**Strength**\n- The idea of introducing a reweighted kernel is well-motivated. \n- The paper proves the convergence rate of KL divergence in a neighborhood of the target.\n\n**Weakness**\n- The reweighting requires the knowledge of $p_*$ or $p_t$. They might not be tractable, e.g. the normalizing constant is unknown. The idea doesn't lead to a practical algorithm.\n- The experiments only have toy examples (2d Gaussian with diagonal covariance). SVGD is expected to sample from much more challenging distributions. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper doesn't provide enough details for reproducing the experiments.\n\nThe writing is clear.  The proof of the local linear convergence of KL divergence is novel.",
            "summary_of_the_review": "This paper proves the convergence rate of SVGD with a reweighted kernel. The paper is clearly written. But the empirical results are weak.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4165/Reviewer_sU2Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4165/Reviewer_sU2Z"
        ]
    },
    {
        "id": "4wWPdXQpTr",
        "original": null,
        "number": 3,
        "cdate": 1666657413967,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657413967,
        "tmdate": 1666676925013,
        "tddate": null,
        "forum": "k2CRIF8tJ7Y",
        "replyto": "k2CRIF8tJ7Y",
        "invitation": "ICLR.cc/2023/Conference/Paper4165/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of sampling wrt to a distribution p* proportional to exp(-V) given access to the gradient of V. The authors consider the mean field limit of Stein Variational Gradient Descent (SVGD). More precisely, SVGD algorithm is an algorithm that relies on updating sequentially the location of a finite number of particles in order to their empirical distribution to approximate the target distribution p*. To run SVGD algorithm, one has to select a RKHS and use its kernel in the update. In continuous time, and with an infinite number of particles, SVGD algorithm converges to a PDE called the SVGD PDE, or in this paper, just SVGD. This PDE rules the evolution of the distribution p_t of the particles. \n\nSVGD can be seen as a dynamics to minimize the KL divergence w.r.t. p*. One issue with this perspective is that the time derivative of KL along SVGD is equal to the opposite of the squared kernelized gradient of the KL. One would like to obtain the squared gradient of KL in order to use Polyak Lojasiewicz inequality (called Log Sobolev Inequality in this setting). This paper develops a technique to do that.\n\nThe main result is that, under LSI and using the reweighted kernel, SVGD PDE converges linearly (in terms of KL) up to a neighborhood.",
            "strength_and_weaknesses": "Strength:\n\n- I have been wondering for a long time how to use a target distribution specific kernel for SVGD, and the reweighting technique sheds some light.\n\n-Section 1 and 2 are well written and I appreciate the overview of Section 2.\n\nWeaknesses:\n\n- This paper considers the infinite particle regime and the continuous time. Although it sheds light on SVGD algorithm, it is still far from the practice of SVGD (I know that the finite particle regime is an open problem). Besides there is some confusion in the paper about continuous and discrete time. In continuous time, the current analyses of SVGD are rather easy and they do not require the kernel to be bounded. Boundedness of the kernel is used in discrete time to establish the descent lemma (Liu'17, Korba et al' 20 and Salim et al 21). So, in my opinion there is no contradiction between Eq 18 and Eq 19 in continuous time.\n\n- In particular, I am worried about how all these results carry in discrete time, since this is where the kernel usually needs to be smooth.\n\n-Assumptions A2 and A3 are conditions on the trajectory of the dynamics, I don't think that they are more reasonable than a bounded kernel. They would need to be proven. This problem is acknowledged and studied empirically though. But A2 and A3 seem cooked up in order to obtain the final result.\n\n- I would have liked to see a comparison between KSD convergence and KL convergence, and why KSD convergence is weaker (even if I understand the intuition)\n\n-I expect the paper to be difficult to read for non-experts (see below).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality:\n\n- I am not convinced by Section 3.1. First, I don't think it is necessary to explain the background from Bayesian ML in this paper. I also disagree with the last sentence because one could have taken the kernel k without reweighting by sigma^d.\n\n- Section 4 is confusing. Here, one would need a more focus writing, with perhaps one idea per subsection. In the current form I don't understand the story line. The relationship between Eq 29, 30, 31 is unclear.\n\n- Around Eq 24: The gradient vanishing problem is explained in a confusing way. Putting the proof of Prop 4.1 in the main text would help the reader, but I am not convinced that this is the bottleneck to KL convergence.\n\nCheck the sentences:\n\"In the population limit, optimal smoothing kernel should be Dirac delta function.\"\n- Why?\n\n\"After that, we explain Stein log-Sobolev Inequality (SLSI) the necessary condition for analyzing KL convergence\nof SVGD can hardly be verified.\"\n- I don't think that SLSI is necessary\n\n\"we provide the convergence analysis for SVGD algorithm in KL divergence.\"\n- Overclaim\n\nNovelty:\n\nThe approach of this paper relying on reweighting the kernel for KL convergence is new to my knowledge. The conclusion of the main theorem (Linear KL convergence up to a neighborhood) is good, even if I have concerns about the assumptions and the fact that it would not carry to discrete time.\n\n\nNo reproducibility issue.\n\n\nMINOR: \n\nSec 2.2 PL is milder than Strong convexity\n\nRemark after Prop 2.2. Technically SVGD algorithm is not a Monte Carlo estimation of Eq 11. It is an exact implementation of Eq 11, because if p_t is an empirical measure, then an integral wrt to p_t is a finite sum.\n\nEq 23. Could one directly use a kernel k_t which approximates the reweighted Dirac in order to make the kernel approx error small?\n\n\n\n\n\n\n\n",
            "summary_of_the_review": "The paper uses interesting techniques but the exposition is below the standard (especially from Section 3). Besides, the assumptions are too strong, and I wonder if the main message of the paper (linear convergence in KL up to a neighborhood) is actually true.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4165/Reviewer_8ZrX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4165/Reviewer_8ZrX"
        ]
    }
]