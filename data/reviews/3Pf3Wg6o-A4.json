[
    {
        "id": "fh2Qm9pfrw_",
        "original": null,
        "number": 1,
        "cdate": 1666043840936,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666043840936,
        "tmdate": 1666709902567,
        "tddate": null,
        "forum": "3Pf3Wg6o-A4",
        "replyto": "3Pf3Wg6o-A4",
        "invitation": "ICLR.cc/2023/Conference/Paper4350/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work presents a new way to perform multi-step logical reasoning tasks with frozen pre-trained large language models. In particular they first select a few (most often two?) pieces of information (Selection) by scoring each fact in the context with the model\u2019s log-likelihood. Then the retrieved facts are combined and the model generates a conclusion based only on those facts (Inference).\n\nResults on ProofWriter and Baby tasks show that the Selection-Inference process is better than other traditional prompting methods (CoT). The work also shows improvement over models with more parameters.\n",
            "strength_and_weaknesses": "**Strengths** : This is a good paper with good motivation. The work can have a positive impact on the field of multi-step reasoning. However some aspects are not entirely clear (more details in the next section). Some improvement suggestions are listed below:\n\n**Weakness** : The Selection algorithm relies on the manual selection of K: number of facts to include in the prompt. This makes the framework hard to use in practice as manually selecting the number of required facts for each Selection step can be very tedious. Did the authors explore setting this parameter to something large (say 4 for all steps, for all tasks)? \n\nSimilarly and most importantly for the entire process of Selection & Inference the steps are repeated until a predefined number of steps. Hence the model is not responsible for identifying when it has answered the question and when it needs to stop this process.\nDo the authors have some thoughts about how to resolve this issue in future work? It would be nice to discuss this in the final section of the paper.\n\n**Question** : Do the authors have some intuition why in Figure 6b the baseline model performs better as the depth increases? The opposite would be a more natural behavior (like in the case of the SI model).\n\n**Recommendation** : Although to a smaller scale, and with models trained from scratch, the idea of using language models to generate step by step reasoning chains was first explored in \u201cMeasuring systematic generalization in neural proof generation with transformers\u201d (Gontier et. al, 2020). The paper should compare their approach to this one in the Related Work section.\nSimilarly, the authors could consider testing their Selection & Inference framework on the CLUTRR benchmark (Sinha et. al, 2019) in addition to Baby & ProofWriter.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper somewhat novel, well written and easy to read, however some things could be more clear, see below for clarification suggestions:\n\n**Clarity (c1)** : The paper never mentions what type of model is used. The authors claim it is to preserve anonymity, but once a model is public, anyone can use it given the available compute resources. Saying that a 280B model was used is also having an impact on the anonymity of the work but it is still valuable information, just like the model type. At minimum the authors should discuss if the model is an encoder-decoder or decoder-only architecture.\n\n**Clarity (c2)** The abstract of the paper initially claims to evaluate models on a suite of 46 tasks, however the SI framework in the main result sections is evaluated on Baby (Sec.5) and ProofWriter (Sec.5 & Sec.6) only. This makes the paper misleading. It may be better to announce ProofWriter and Baby in the abstract as this seems to be the main focus.\nIf that is not the main focus, then why are the 46 tasks in the Appendix (Figure A5)? It is not clear if the SI framework was used in these appendix tasks or some other type of prompting? More details on the division of tasks and what prompting mechanism has been used would be beneficial.\n\n**Clarity (c3)** : The paper would be even more clear if it explicitly explained what is meant by \u201ctesting in 5-shot setting\u201d. For instance does it ever happen that the 5 examples in the prompt, when tokenized, are longer than the input size supported by the model? What happens in this case?\n\nSimilarly, the paper should explain how the prompt examples were picked?\n\nIn the case of ProofWriter D0-D5, were the 5 examples in the model\u2019s prompt from the same depth as the one being tested? In the ProofWriter paper, the authors had experiments where they trained on D3 and tested on D5 for instance, does this work explore compositional generalization like this as well with the prompts? If not, I think it would be an interesting result to test: prompting with D3 examples and testing on D5 examples.\n",
            "summary_of_the_review": "Overall, this is a good paper with impactful research, however it would benefit some clarifications and a discussion on how to resolve the weakness mentioned above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_BSuH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_BSuH"
        ]
    },
    {
        "id": "dxI8dbF4jn",
        "original": null,
        "number": 2,
        "cdate": 1666315639837,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666315639837,
        "tmdate": 1666315639837,
        "tddate": null,
        "forum": "3Pf3Wg6o-A4",
        "replyto": "3Pf3Wg6o-A4",
        "invitation": "ICLR.cc/2023/Conference/Paper4350/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper explores the reasoning capabilities of large language models (LLMs). They carry out a comprehensive evaluation of 46 reasoning tasks to show that language models perform fairly well on single step reasoning problems, but suffer at multi-step reasoning problems. To that end, the paper proposes a new algorithm called selection-inference where LLMs solve multi-step reasoning problems by iterating over selection and inference modules to generate a series of interpretable causal reasoning steps. Through this method, they show that a smaller (in terms of parameters) LLM can outperform a baseline larger LLM. The paper also presents several interesting ablations with scale, fine-tuning, and performance on various levels of multi-step reasoning problems.  ",
            "strength_and_weaknesses": "\nStrengths: \n1.  The paper presents a comprehensive study of 46 reasoning tasks and gives an understanding of how good are the current LLMs in single step and multi-step reasoning problems. \n2. The proposed selection-inference algorithm is a very interesting and novel idea. It makes the reasoning trace more interpretable.\n3. The paper demonstrates that this approach will enable small language models to outperform baseline large language models that doesn\u2019t use SI algorithm\n\nWeaknesses: \n1. It would have been a more interesting comparison to see how this method performs in comparison to other recent works such as self-consistency, verifiers, etc. The proposed method has its own merits, but comparing with more methods would have been more interesting.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. It seems like the proposed method is computationally more heavy than CoT. Please have a discussion on that in the paper. \n2. Does the paper see the chain-of-thought approach as not interpretable? \n3. Can you clarify on whether selection and inference use fixed prompts? If they are fixed, do you see any benefit in making them dynamic based on which iteration step of the reasoning the model is at? \n",
            "summary_of_the_review": "The paper provides some very interesting understanding of large language model capabilities for single vs. multi-step reasoning. The proposed SI algorithm is novel and very useful. All the ablations are very useful. Overall, I recommend this paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_g5nq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_g5nq"
        ]
    },
    {
        "id": "9_vPxDL4W5",
        "original": null,
        "number": 3,
        "cdate": 1666402654092,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666402654092,
        "tmdate": 1666402654092,
        "tddate": null,
        "forum": "3Pf3Wg6o-A4",
        "replyto": "3Pf3Wg6o-A4",
        "invitation": "ICLR.cc/2023/Conference/Paper4350/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors focus on leveraging pre-trained LLMs to solve language-based reasoning tasks, via in-context learning. \n\nFirst, on a wide range of tasks, the authors investigate how well existing LLMs at various model sizes tackle logical reasoning tasks in a standard 5-shot prompting setting. Results suggest that they perform better on simple single step logical inference, but suffer from more complex tasks such as multi-step reasoning. \n\nMotivated by the above observation, the authors propose a novel Selection-Inference (SI) framework, which is an iterative prompting method designed for solving logical reasoning tasks. At every iteration step, SI first ask the LLM to select a subset of most relevant supporting evidence from a larger set of input facts, conditioned on the question; then in an inference step, LLM is asked to infer a new fact from the selected evidence. The inferred new fact is either as the final answer, or added into the input fact set to the next iteration. \n\nThe authors show that equipped with this SI method, a LLM with 7B parameters can outperform a set of baselines, including a much larger 280B LLM, equipped with Chain of Thought (COT), the current state-of-the-art prompting method. \n\nThe authors also show that with some minor fine-tuning, a 7B LLM with SI can further a significant improvement. \n\nThe author also discuss and demonstrate how the SI framework can facilitate model interpretability, transparency, and humans-in-the-loop study. ",
            "strength_and_weaknesses": "### Strength\n\n* **Neat idea**: The idea of the two step selection-inference framework can be seen as a way to leveraging human prior knowledge in prompt engineering. As the authors discussed in the motivation section, this to some extent bridges neurosymbolic AI with recent large-scale deep learning approaches. There are a few neat ideas I like in this paper:\n\n  1. Ranking negative log-likelihood for selection instead of directly prompting LLM for a list of outputs. This could effectively avoid the potential trouble of LLM generating something uncontrollable or hard to parse. Although I have to say this requires the LLM output probabilities to be available, which is not always the case (for practitioners who has less ML knowledge, or using online LLM APIs).\n  2. \"Occlude\" question from the inference module. This is in some sense adding an information bottleneck. \n\n* **Empirical value**: I'd love to see research along this line, which can potentially have great empirical value. Better prompting strategies can be useful in general and this is not limited to the ML community. \n* **Significance test**: I appreciate the authors include significance test in their experiment and result section. \n* **Interpretability, transparency, and humans-in-the-loop**: Compared to vanilla in-context learning and prior methods such as COT, the proposed framework can have a better interpretability and help researchers to better understand how an LLM makes certain decision. I also like the opportunity of humans-in-the-loop that the SI framework brings (e.g., users help to verify/filter selected evidence).\n* **Explicit knowledge representation / memory**: In some sense, the iteratively growing set of evidence can be seen as a working memory, or a representation of past experience that one can easily retrieve from. Thinking broadly, from a sequential decision making or robotics perspective, instead of multi-step reasoning, I can imagine something like the SI framework being used to facilitate multi-step planning. \n\n\n### Weaknesses\n\n1. Please see my question 3 below regarding the $H$ and $K$ values.\n2. The paper suffers from a few accessibility issues. Please see the minor issue and typos section below. ",
            "clarity,_quality,_novelty_and_reproducibility": "### Questions and concerns:\n1. The authors mention several times that the SI framework `is unlikely to make up information to answer the question`, I fail to understand why is that. It would be helpful to show some examples where COT or other systems make up information but SI does not, and discuss the reason. \n2. It's not fully clear to me why in the selection step you have `Therefore, ` in your prompt. It seems to be the trigger of inference?\n3. The authors very briefly mention in the conclusion section about `the halting issue`. I agree that this is one of the major limitations of this work. As the authors mention, this is the case for both 1) the fixed $H$ in Algorithm 1 and 2) the fixed $K$ in Algorithm 2. I wonder in the current version, are there any heuristics to prevent the LLM from receiving distracting information? This would happen when either $H$ or $K$ is greater than sufficient amount of evidence. Please elaborate a bit more on this, including how $H$ and $K$ are determined in the current version, and how the authors plan to make these numbers adaptable. \n4. In Figure 4 (b), is there any specific reason that the 280B vanilla generative model performs so poorly on Babi tasks?\n\n\n### Minor issues and typos:\n\n1. Appendix A appears to be empty. I know it includes a bunch of figures, but it's a bit weird to be named `APPENDIX` and without any text content there. \n2. I appreciate the authors include many examples and detailed prompt design in the appendix, however, the current version is not easy to read. A table of content would greatly increase the accessibility. \n3. There are a few `see Appendix for ...` in the main body without specific appendix label, please add pointers. \n4. Also accessibility issue, please increase font size for all text in Figure 3 and Figure 4. They are almost unreadable in a printed version.\n5. Figure 1 caption: `Chain-of-Though` --> `Chain-of-Thought`.\n6. The paragraph above Section 7: `... our approach is able to generalised beyond ...` --> `... our approach is able to generalise beyond ...`.\n",
            "summary_of_the_review": "Overall, I like this paper, it's clearly written, the proposed method makes sense to me and results suggest that the SI framework indeed improves the LLM's performance on a range of reasoning tasks. Unless I had some major misunderstandings, I recommend to accept this paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_wTCG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_wTCG"
        ]
    },
    {
        "id": "fW5Iots2udF",
        "original": null,
        "number": 4,
        "cdate": 1667195660655,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667195660655,
        "tmdate": 1667196050861,
        "tddate": null,
        "forum": "3Pf3Wg6o-A4",
        "replyto": "3Pf3Wg6o-A4",
        "invitation": "ICLR.cc/2023/Conference/Paper4350/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel framework called Selection-Inference(SI), which exploits pre-trained large language models as general processing modules to solve logical reasoning problems. Using this framework, LLMs will continuously alternates between selection step and inference step to generate a sequence of casual reasoning steps. So not only the answers can be concluded from these reasoning steps, but also these causal natural-language-based reasoning trace helps us to learn how the model reached the answer and opens the system\u2019s decisions to human critique. The results are very impressive. The paper shows that a 7B SI model outperforms the 280B model. ",
            "strength_and_weaknesses": "trengths\n\n- This framework of SI is different from all previous frameworks, as illustrated in Figure 1. SI use LLMs to produce each reasoning step one at a time, which is similar with ProofWriter. While ProofWriter can only answer \u201cProve this statement to be True/False\u201d style question, SI is able to solve reasoning problems.\n\n- The designs of splitting of each step of reasoning into selection and interference makes the model more unlikely to make up information to answer and it cannot ignore previous reasoning when getting the answer.\n\n- The authors conduct extensive experiment and provide a comprehensive evaluation of LLMs on a set of 46 tasks. They show that LLMs are good at simpler single step logical inference in 5-shot generalisation settings, but struggle with harder problems.\n\n- Using SI framework, the 7B model shows impressive results and even outperform 280B LLM baseline using COT framework. Also, the SI framework has ability to produce a causal reasoning trace and is able to recover from errors.\n\nWeaknesses\n\n- Some important details are not presented. The algorithm now just simply halts after some times of steps. So will the result differs from different numbers of steps, and how. \n\n- The results are not convincing enough. In the paper, the authors just show the results of 7B SI. I wonder know that if the 280B version using SI will outperform all previous models.\n\n- There are many case studies, more quantitative metrics to evaluate the effect is better.\n\n- There are some typos in this paper, which causes additional burden for understanding. For example:\n\n1) typo in page 2, \u201cFigure 1\u201d paragraph, \u201cindicate\u201d -> \u201cindicates\u201d\n\n2) typo in page 5, \u201cWe use prompt...\u201d paragraph, \u201cthe the following form\u201d -> \u201cthe following form\u201d\n\n3) typo in page 26, \u201cWe have seen...\u201d paragraph, \u201cwhich we we now\u201d -> \u201cwhich we now\u201d\n\n4) typo in page 26, \u201cWhen observing...\u201d paragraph, \u201cwe us\u201d -> \u201cwe use\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good, but novelty is somewhat limited.",
            "summary_of_the_review": "This paper proposes SI framework, which improves the ability of LLMs to solve reasoning problems and outperforms other framework including COT. Even though this framework still has many limitations, it does develop a new approach and achieves state-of-the-art results. And this work can be extended to many further works, which are inspirational for other works in the field.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_TQ9M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_TQ9M"
        ]
    },
    {
        "id": "UDH0sOEL9Y",
        "original": null,
        "number": 5,
        "cdate": 1667197319314,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667197319314,
        "tmdate": 1667197319314,
        "tddate": null,
        "forum": "3Pf3Wg6o-A4",
        "replyto": "3Pf3Wg6o-A4",
        "invitation": "ICLR.cc/2023/Conference/Paper4350/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new framework for multi-step reasoning by splitting reasoning into two main stages: selection and inference. Selection involves choosing relevant information from the question to the inference module. The inference module uses the information from the selection module to reason and produce an answer. The authors test the framework on 10 logical reasoning tasks and show that they can get a 7B parameter lm to outperform 280B parameter lm.",
            "strength_and_weaknesses": "## Strengths\n- ****************Clarity:**************** The paper is well-written and easy to follow. The problem, and motivation are clear and the method directly follows from them.\n- **************Novelty:************** Splitting reasoning into modules has been explored but to my knowledge, using something like the proposed SI method has not been seen.\n- The SI framework significantly improves performance on the reasoning tasks over vanilla CoT/ scratchpad approaches.\n- Selecting a subset of information ensures that the language model is restricted by the information it uses for reasoning.\n## Weaknesses\n- One missing baseline for the framework is where the CoT prompts are framed as SI prompts without splitting SI into two separate modules.\n- SI using larger language models: I expected to see how models a magnitude larger than 7B would perform with the SI framework. This would help us understand how important the specific components of the framework are.\n- For example, a larger model might not make facts up in the selection step when using greedy decoding (with temperature = 0).\n- Or, one might not need two separate calls to the LM for S and I, simply using a CoT script with steps for selection and inference is all that one might need.\n\n### Other\n\n- **********************************Reproducibility:********************************** The method is straightforward to implement and test. The authors provide the prompts but access to the larger 280B language model is restricted and the baseline results would be impossible to replicate.",
            "clarity,_quality,_novelty_and_reproducibility": "Detailed in the main review.",
            "summary_of_the_review": "The approach is empirically quite strong! The paper would be better with some additional baselines and justifications for design choices.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_QvM7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4350/Reviewer_QvM7"
        ]
    }
]