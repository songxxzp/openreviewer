[
    {
        "id": "CRceaSbW3N",
        "original": null,
        "number": 1,
        "cdate": 1666695241804,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695241804,
        "tmdate": 1666695241804,
        "tddate": null,
        "forum": "R1U5G2spbLd",
        "replyto": "R1U5G2spbLd",
        "invitation": "ICLR.cc/2023/Conference/Paper3444/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles developing neural machine translation models in the federated learning scenario.\nThere have been several methods that attempt to solve the identical scenario.\nThe authors point out that the conventional methods require vast communication overheads and heavy synchronization that makes the methods impractical when we consider the actual use-case for training machine translation models, including the way to use training data with privacy.\nThis paper proposed a method that can be built only with low overhead while preserving privacy to mitigate inefficiency.\nThe key idea is to leverage the technique of nearest-neighbor neural machine translation (kNN-MT).\nThe experimental results on two datasets in the federated learning scenario show significantly better performance over several conventional methods, with fewer communication costs.",
            "strength_and_weaknesses": "Strength:\n\n* This paper is well-organized and easy to read.\n* Figure 1 is neat and concise, and can help a deeper understanding of the proposed method.\n* The claims and the proposed method seem reasonable.\n* The experiments show that the proposed method can perform better than a few conventional methods in practical federated learning scenarios.\n\nWeakness:\n\n* The primary idea of the proposed method is just borrowing the previous method of kNN-MT. From this perspective, the proposed method can be considered just applying the existing method (with slight modifications) in a different learning configuration.\n* most of the sub-modules that consist of the proposed method are also a combination of the existing methods. Therefore, the method is not innovative, so this is somewhat of an incremental study.",
            "clarity,_quality,_novelty_and_reproducibility": "These points are all written in the above Strength and Weakness text box.\nPlease refer it.\n",
            "summary_of_the_review": "As I pointed out above, the method itself is not super novel, but this paper carefully considers and tackles a practical scenario in actual use.\nTherefore, this paper has sufficient contributions to be accepted to the conference.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I found no ethical concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3444/Reviewer_61jW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3444/Reviewer_61jW"
        ]
    },
    {
        "id": "yl38r2D8Kde",
        "original": null,
        "number": 2,
        "cdate": 1666735587820,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735587820,
        "tmdate": 1666735587820,
        "tddate": null,
        "forum": "R1U5G2spbLd",
        "replyto": "R1U5G2spbLd",
        "invitation": "ICLR.cc/2023/Conference/Paper3444/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes the federated nearest neighbor (FedNN) machine translation framework, which is built on top of the nearest neighbor machine translation method (kNN-MT). FedNN extends kNN-MT in a federated learning setting, where a server provides a global model and clients keep private data to improve the model. The paper proposes that to avoid privacy leakage, the clients send out an encrypted datastore which is built on a private dataset. By doing this, the system avoids multi-round model-based interactions, so that the communication overhead is made less. The paper conducts experiments on WMT14 and multi-domain en-de dataset. The experiments show that FedNN outperforms several baselines while keeping the communication and computational overhead low.\n\nThe main contribution of this paper is to apply kNN-MT to a federated learning setup, which is a reasonable application given kNN-MT is a cheap and effective method for domain adaptation. \n",
            "strength_and_weaknesses": "Strengths:\n- The paper studies an intersection of two important topics -- machine translation and federated learning. I believe this topic can be potentially impactful.\n- The paper is well-motivated and the general idea of building domain-specific datastores without updating the model in the federated learning setting makes sense to me.\n- The paper is well-written and easy to follow.\n\nWeaknesses:\n- The main concern of this paper is that it may not make enough technical contributions. The FedNN framework is an extension of the kNN-MT in the federated learning setup, but only this contribution doesn\u2019t guarantee the paper to be published in ICLR in my opinion. The two-phase datastore encryption is interesting, however it is not studied extensively in the paper but built on off-the-shelf tools. The paper can be made stronger by considering different encryption methods.\n- The paper compares the performance and communication/computational overhead of FedNN and existing methods. However, the paper does not discuss the privacy preserving ability of different approaches. For example, how do multi-round model-based interaction methods preserve the privacy of clients compared to FedNN?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity & Quality: the paper is well-written and easy to follow. There are a few places that I am a bit confused about and I would suggest the authors make them more clearly or include more discussions.\n- In Table 1, there is too much content. It is better to highlight some numbers.\n- In Table 1, why is the communication/computation of FedAvg_1 more than FedAvg_inf?\n- In Table 1, the centralized model shows slightly worse performance compared to FedNN. This is surprising to me, as the centralized model is trained with client data. Could you provide more discussion about this?\n\nNovelty: See my evaluation in the previous section.\n\nReproducibility: I believe the results can be reproduced easily. I encourage authors to include more implementation details.\n",
            "summary_of_the_review": "In summary, I think the paper studies an important problem and the proposed solution makes sense to me. However, I also think the paper can still be improved in some places.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3444/Reviewer_acCF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3444/Reviewer_acCF"
        ]
    },
    {
        "id": "UyG-HyuRhaH",
        "original": null,
        "number": 3,
        "cdate": 1666767932303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666767932303,
        "tmdate": 1666767932303,
        "tddate": null,
        "forum": "R1U5G2spbLd",
        "replyto": "R1U5G2spbLd",
        "invitation": "ICLR.cc/2023/Conference/Paper3444/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a federated nearest neighbor (FedNN) method for machine translation. Instead of multi-round model-based interactions, this mothed leverages one-round memorization-based interaction to share knowledge across different clients to build low-overhead privacy-preserving systems.",
            "strength_and_weaknesses": "Strength\n\nIn general, this paper is well organized. The method is reasonable and technically sound, the experiments are well designed and basically sufficient to support the conclusions.\n\nWeakness\n\n1. The innovation in the approach proposed by the authors is incremental. It is a combination of machine translation methods (e.g., kNN-MT) and federated learning methods.\n\n2. There is too little related works about federated learning.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: clear\n\nQuality\uff1ahigh\n\nNovelty: incremental\n\nReproducibility: low\n",
            "summary_of_the_review": "Authors propose a federated nearest neighbor (FedNN) method for machine translation. The proposed method combines federated learning and neural machine translation to achieve SOTA results. However, the innovation is limited and do not give much context to federated learning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3444/Reviewer_Ljh8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3444/Reviewer_Ljh8"
        ]
    },
    {
        "id": "iZ87INkpAkP",
        "original": null,
        "number": 4,
        "cdate": 1667249563682,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667249563682,
        "tmdate": 1667249563682,
        "tddate": null,
        "forum": "R1U5G2spbLd",
        "replyto": "R1U5G2spbLd",
        "invitation": "ICLR.cc/2023/Conference/Paper3444/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present an adaptation of the kNN MT model into the federated learning framework. The motivation is to achieve client-side machine translation with minimal communication costs with the central server NMT model and while maintaining the privacy of client data. The authors propose the kNN federated learning approach paired with (K,V) encryption of the data-stores as an alternative to federated learning approaches that use multi-round model updates to integrate client-side data to the central model. The authors compare their model to other baseline approaches regarding BLEU score as well as communication, computational and inference costs and they experiment with IID and non-IID setups, while they also perform ablation tests regarding the number of clients and the heterogeneity degree of data.",
            "strength_and_weaknesses": "The paper is well motivated and the authors perform comparisons across different aspects. They draw useful insights on the impact of the data distribution, the number of clients etc. They also do show that their approach outperforms the other models, especially for the non-IID setup. Overall, it is interesting to see the integration of the kNN MT approach in the federated learning workflow and the authors do a good job at addressing privacy and computational/communication concerns.\n\nHowever, I have a few concerns regarding the experimental setup and decisions. The authors seem to compare against the FedAvg method which is an implementation of the McMahan et al. (2017) method, and they point out the vast communication cost. However, there have been more recent papers (that are cited by the authors as related work: Roosta et al., 2021; Passban et al., 2022) that show improvements in terms of communication. It would make more sense to compare to these methods instead, or at least make some attempt to calculate an optimal update interval for the FedAvg method instead of considering only the FedAvg_1 and FedAvg_infinity. Similarly it is unclear how the authors aggregate the prediction outputs for the FT-Ensemble method: in this case too, the aggregation approach (e.g. is it a weighted fusion?) and potential optimisation of the parameters could impact the performance of the FT-Ensemble.\n\nMoreover, it is not clear why the authors did not use the WMT14 De-En language pair (but use the WMT14 En-De instead) that was used by the previous works and would allow for direct comparisons with reported results in other work (especially since the authors seem to have not optimised the methods they compare against). \n\nAdditionally, the authors mention significantly outperforming other methods (e.g.: \"FedNN has a significant performance improvement of 4.41/1.99 BLEU scores...\") but it is unclear what is the statistical significance test that was performed (if any). \n\nWhile my main concerns are listed above, I would like to add some further suggestions:\n- While I appreciate the already performed ablation tests, I am missing some ablation regarding the impact of the size of the data on the client and the server side. It is hinted in the paper that the size of the client data impacts the performance of the FedAvg methods but there is no specific experiment about it. Still it seems to be a crucial parameter that could impact the performance and cost of most of the compared methods and it would be very interesting to see a more detailed analysis about this.\n- I would propose that the authors consider comparing the performance of different systems using some more recent, neural metric (e.g. BLEURT, COMET) in addition to BLEU as it would allow to draw better comparisons.\n\nSome formatting/grammar/typo comments:\n- Some fonts of figure 1 are way too small\n- In the mention: \"the server builds (K, V)-encryption model\" I think it should be:  \"the server builds a (K, V)-encryption model \" . Additionally, you mention (K,V) a couple of times before explaining what (K,V) stands for. I would propose to add the definition/explanation in the first mention.\n- \"vis kNN retrieval based on the context representation\" \u2014> via instead of vis?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written and clear with the exception of some typos etc. It is easy to follow but I would perhaps clarify the privacy/encryption related information sooner.\n\nThere is no novel method proposed per-se, but the novelty lies in integrating the kNN MT method with federated learning and addressing the privacy and computational concerns. It is a solid contribution with nice insights, but some issues that could be improved in the experimental setup and the implementation of the methods comparing against.\n\nIn terms of reproducibility, the method proposed by the authors could probably be reproduced with some effort and using the information in the appendix, but some details for the methods compared against are less clear (see also my comments in the previous section).",
            "summary_of_the_review": "Well-motivated approach with interesting insights, but some questionable choices in the experimental setup that could be improved or further clarified/justified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3444/Reviewer_1QjM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3444/Reviewer_1QjM"
        ]
    }
]