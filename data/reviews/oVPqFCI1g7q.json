[
    {
        "id": "DqJ9-fdOpT-",
        "original": null,
        "number": 1,
        "cdate": 1666650345243,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650345243,
        "tmdate": 1666886600752,
        "tddate": null,
        "forum": "oVPqFCI1g7q",
        "replyto": "oVPqFCI1g7q",
        "invitation": "ICLR.cc/2023/Conference/Paper5547/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a new communication compression method called $\\beta$-stochastic sign SGD, which is Byzantine-resilient and differentially private. The authors provide theoretical results about differential privacy and convergence. Besides, the proposed method is empirically compared with SignSGD and FedAvg.",
            "strength_and_weaknesses": "**Strength**: The main idea of randomly flipping the sign is easy to understand and makes sense. The proposed method is not hard to implement.\n\n---\n**Weaknesses**: \n1. The novelty of this work is limited since the proposed method seems to be a combination of stochastic sign SGD and random flipping.\n2. The $L_1$ norm of gradients can be directly bounded by $\\sum_{i\\in[d]}B_i$ according to Assumption 3. It seems that the constant terms in the right-hand side of (5) and (6) are very likely to be not smaller than the trivial upper bound $\\sum_{i\\in[d]}B_i$. Could the authors compare the results in Theorem 5 with this? \n3. The empirical results are not solid enough. The proposed method is only compared with SignSGD and FedAvg. Comparing $\\beta$-stochastic sign SGD with more Byzantine-resilient methods is required.\n4. This paper is not well-written.\n  - $\\beta$ is frequently used before being formally defined in the abstract and the introduction. Besides, $d$ appears in the statement of contributions (page 2) without any definition.\n  - It is ambiguous what the notation $\\nabla F_i(\\cdot)$ means. Does it mean the partial derivative w.r.t. the $i$-th variable (i.e., the $i$-coordinate in the gradient)? If so, it is highly suggested to use $\\nabla_i F(\\cdot)$ or $(\\nabla F(\\cdot))_i$ since the original notation is very likely to be confused with $\\nabla f_m(\\cdot)$, which denotes the gradient of function $f_m(\\cdot)$.\n  - It is highly suggested to explicitly clarify which norm the notation $||\\cdot||$ in Assumption 2 denotes.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper is not well-written. \n- Several claims are not well-supported. \n- The novelty of this paper is limited. \n- There are almost no concerns about reproducibility.",
            "summary_of_the_review": "Although the proposed method is easy to implement, given the concerns about novelty, quality, and readability, this work is currently below the bar of ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5547/Reviewer_nUrq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5547/Reviewer_nUrq"
        ]
    },
    {
        "id": "DpyVIOfAgBn",
        "original": null,
        "number": 2,
        "cdate": 1666677768396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677768396,
        "tmdate": 1669747114434,
        "tddate": null,
        "forum": "oVPqFCI1g7q",
        "replyto": "oVPqFCI1g7q",
        "invitation": "ICLR.cc/2023/Conference/Paper5547/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyzes an optimization algorithm $\\beta$-stochastic sign SGD. The authors proves that $\\beta$-stochastic sign SGD is differentially private in the case of $\\beta > 0$. The authors also show that $\\beta$-stochastic sign SGD achieves Byzantine resilience under some problem assumptions.",
            "strength_and_weaknesses": "Strength: \n1. The paper shows that $\\beta$-stochastic sign SGD achieves communication efficiency, differential privacy, and Byzantine resilience simultaneously.\n2. The proposed algorithm is very practical, easy to implement, and have good performance in MNIST and CIFAR-10 datasets in experiments. \n\nWeakness:\n1. The technical contribution in proving differential privacy of $\\beta$-stochastic sign SGD is not that significant. \n2. The assumptions for bounded gradients and sub-Gaussian element-wise gradient noise are kind of strong for federated learning or requires more justification, and these two assumptions again make the technical contributions less significant. \n3. More discussions with existing works such as [1] are needed for comparing the final error bounds for Byzantine resilience.\n4. Basically, the reviewer gets the idea that $\\beta$-stochastic sign SGD achieves two-fold benefits, but wants to see some comparisons with existing works in each dimension, so that the paper is more persuasive in that $\\beta$-stochastic sign SGD achieves performances matching the state of the art of at least comparable, and the technical challenges therein is non-trivial given that the algorithm itself is not that novel. \n5. In the experiments, it seems that the authors tune the algorithm to find good parameters $B$, which makes the algorithm less practical since it means that this algorithm needs to be run several times to achieve good performance (this will contradict to the target benefits of the algorithm), so a good estimate of a favorable $B$ would be helpful. \n6. Can the authors add some experiments to compare the performance of $\\beta$-stochastic sign SGD with other existing algorithms either designed for DP or Byzantine resilience?\n\n\n[1] Karimireddy, S. P., He, L., & Jaggi, M. (2021, September). Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing. In International Conference on Learning Representations.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The reviewer does not quite follow why Theorem 1 is a standalone result, it seems to me it is an intermediate lemma for proving convergence, can the authors provide more explanations?\n2. The definition of sub-Gaussian has a typo that one side should be $-t$.",
            "summary_of_the_review": "This paper has a very interesting finding that $\\beta$-stochastic sign SGD achieves Byzantine resilience and differential privacy simultaneously. It will help the reviewer to understand the contributions of this paper better if there were more performance comparisons of $\\beta$-stochastic sign SGD with existing methods designed for DP or Byzantine resilience only, whether they are comparable theoretically or in experiments in each dimension, and maybe some more explanations on the significance of the presented results. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5547/Reviewer_W1pz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5547/Reviewer_W1pz"
        ]
    },
    {
        "id": "oXhFH8-2mPK",
        "original": null,
        "number": 3,
        "cdate": 1666850299745,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666850299745,
        "tmdate": 1666850814220,
        "tddate": null,
        "forum": "oVPqFCI1g7q",
        "replyto": "oVPqFCI1g7q",
        "invitation": "ICLR.cc/2023/Conference/Paper5547/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides the first algorithm based on sign SGD, which achieves Byzantine resilience and differential privacy.",
            "strength_and_weaknesses": "1)Author motivates the subject from a theoretical and practical point of view. \n2)In the introduction, the author reviewed the previous works carefully. \n3)the author successfully addressed the issue and recommended the algorithms.\n4)The authors successfully provide simulation results for their algorithm.\n5)The proof is mathematically correct.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n1)Author motivates the subject from a theoretical and practical point of view. \n2)In the introduction, the author reviewed the previous works carefully. \n3)the author successfully addressed the issue and recommended the algorithms.\n4)The authors successfully provide simulation results for their algorithm.\n5)The proof is mathematically correct.\n",
            "summary_of_the_review": "I think this is a good paper and should get accepted. But, the writing should improve.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5547/Reviewer_9qJV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5547/Reviewer_9qJV"
        ]
    },
    {
        "id": "2C8rQ-w0ax",
        "original": null,
        "number": 4,
        "cdate": 1667348181072,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667348181072,
        "tmdate": 1667348181072,
        "tddate": null,
        "forum": "oVPqFCI1g7q",
        "replyto": "oVPqFCI1g7q",
        "invitation": "ICLR.cc/2023/Conference/Paper5547/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers gradient compression in federated/distributed learning with two additional requirements: robustness against Byzantine clients and differential privacy (DP). The paper proposes a sign-based gradient compression method, called $\\beta$-stochastic signSGD, which applies clipping and a stochastic sign operator to gradients (Definition 3) for compression. SignSGD, which simply applies a sign operator to gradients was proposed in [Bernstein et al., 2019], and its stochastic version was proposed in [Jin et al., 2020]. The method proposed in the paper reduces to that in [Jin et al., 2020] for $\\beta = 0$. The paper presents convergence result and DP guarantees, along with experimental results. ",
            "strength_and_weaknesses": "**Strengths:** \n\n1. The family of sign-based compressors is practically appealing, and the paper proposes an interesting variant of stochastic signSGD. \n\n**Weaknesses:**\n\n1. The paper does not cover all details and the technical presentation leaves open several questions. For instance, the paper does not discuss any upper bound on the number of Byzantine clients $\\tau(t)$ in an FL iteration. It is well-known in Byzantine robustness literature that, when the fraction of Byzantine clients is greater than 1/2, then it is not feasible to achieve robustness. Remark 1 discusses the case of sufficiently large $\\tau(t)$, which is quite confusing. (Other comments and suggestions are mentioned later). \n\n2. The paper does not give details on whether DP guarantees are for client-level or item-level privacy. There are also no empirical comparisons with DP-SGD and/or local-DP algorithms with the proposed method. \n\n3. The $\\epsilon$ of DP for the proposed method is $O(d)$, where $d$ is the number of parameters of ML model (i.e., the length of gradients). This is going to be significantly large for several ML models such as deep neural networks. Would such a high epsilon be practical in terms of DP?\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality:** The presentation can be significantly improved. There are several questions that need more details:\n\n1. What is an upper bound on the number of Byzantine clients that the method can tolerate? The second part of Theorem 5 only considers a lower bound on $\\tau(t)$. What is the upper bound on $\\tau(t)$ up to which equation (6) holds? As mentioned under \u2018Weaknesses\u2019, when the fraction of Byzantine clients is greater than 1/2, then it is not feasible to achieve robustness. Without such an upper bound on $\\tau(t)$, it is not possible to assess the correctness of the result, and it is critical to add details.\n\n2. Is there any upper bound on $\\beta$? The $\\epsilon$ value of the DP guarantee is $d \\log\\left(\\frac{2B + \\beta}{\\beta}\\right)$, and larger values of $\\beta$ may be able to lower $\\epsilon$. However, the paper does not discuss details about how large can $\\beta$ be.\n\n**Novelty:** Considering [Jin et al., 2020], the novelty of the method is fairly limited. The main novelty is in terms of analysis. I did not go through the detailed proofs.  \n\n1. The result of Theorem 1 is quite straightforward.\n\n2. For adaptive adversaries, the method can support the following upper bound on the number of Byzantine clients: $\\tau(t) \\leq \\frac{2}{p^2} \\log\\frac{6}{c}$, where $p$ is a probability with which a client participates in a FL round and $c$ is a positive constant. When $p = O(1)$, then the $\\tau(t)$ seems to be a constant, and it would not increase with the number of clients. In contrast, several prior works on Byzantine robust FL can tolerate a constant fraction of Byzantine clients (as opposed to a constant number of Byzantine clients). \n\n3. Empirical results only consider a multi-layer perceptron (MLP) model. It would be helpful to understand how the proposed method performs on more sophisticated models such as CNNs and deeper networks such as ResNet.",
            "summary_of_the_review": "The proposed method of $\\beta$-stochastic signSGD and the analysis are interesting. However, there are several questions that need to be addressed before the paper becomes ready for publication. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5547/Reviewer_YSJ7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5547/Reviewer_YSJ7"
        ]
    }
]