[
    {
        "id": "wWeewi6Nff2",
        "original": null,
        "number": 1,
        "cdate": 1666180612555,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666180612555,
        "tmdate": 1669198526906,
        "tddate": null,
        "forum": "7GEvPKxjtt",
        "replyto": "7GEvPKxjtt",
        "invitation": "ICLR.cc/2023/Conference/Paper1550/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper looks at a critical but rarely studied problem: the certification of robustness to Universal Perturbations (UPs). In particular, UPs (e.g., universal adversarial noise, backdoor, or neural trojan attacks) have become alarming lines of threats that make it hard to use Machine Learning as a service safely and reliably.\n\nThe authors formulated and solved the robustness certification for UPs utilizing a method based on linear relaxation and Mixed Integer Linear Programming. An intriguing discussion on the difference between existing sample-wise certification and the certification against UPs was provided. The authors also theoretically analyzed the relationship between the certification result based on observed samples and the actual robustness of neural networks to UPs w.r.t. the underlying distribution. Multi-perspective evaluations of the tightness (compared with existing certification), soundness of the bound (real UAP and backdoor attacks), and potential implications (to existing attacks and defenses) are offered.",
            "strength_and_weaknesses": "Strength:\n\n1.\tThe paper is generally well-written and easy to follow, with a well-designed structure;\n\n2.\tThe problem of robustness certification to UPs is of great importance, and detailed implications to model/defense comparison and backdoor detection are discussed with empirical results;\n\n3.\tThe proposed method with MILP and linear relaxation accounting for establishing the information sharing between samples is intuitive but compelling; the tightness of the certification results is shown with a comprehensive study with existing certification techniques and actual attacks;\n\n4.\tTo the best of my knowledge, UP\u2019s generalization analysis in this paper is the first of its kind effort, which also leads to an intriguing analysis of the error of certification results using the proposed robustness certification method.\n\nWeakness:\n\n1.\tSome settings of the experiment are unclear. For example, I think the settings for the Implication to backdoor defenses (i.e., the backdoored class identification) can be further improved with more details on the threat model and how to use the proposed tool for detection regarding existing backdoor attacks.\n\n2.\tI can understand the difficulties of the robustness certification to large neural networks or large datasets, especially the problem of certifying against UPs requiring computing w.r.t. multiple samples (it seems it is required to compute a much larger computational graph when the network size and input size grow). I highly recommend the authors add a section discussing these limitations.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The theoretical contribution in this paper is solid and original, and it should be of interest to researchers in both the universal adversarial perturbation, backdoor domains, and robustness certifications. And empirical evaluations took different perspectives into account and lead to some intriguing findings and implications of the robustness certification against UPs. ",
            "summary_of_the_review": "In summary, the authors study a promising robustness certification method against UPs, along with theoretical analysis and a thorough discussion of the implications to model structure, UAP attacks, and backdoor attacks. Some minor issues with the clarity of the settings and the discussion of the limitations should be further improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1550/Reviewer_GEmp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1550/Reviewer_GEmp"
        ]
    },
    {
        "id": "uKiAsZTZyr8",
        "original": null,
        "number": 2,
        "cdate": 1666409767923,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666409767923,
        "tmdate": 1666409767923,
        "tddate": null,
        "forum": "7GEvPKxjtt",
        "replyto": "7GEvPKxjtt",
        "invitation": "ICLR.cc/2023/Conference/Paper1550/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper focused on exploring and extending linear-relaxation-based sample-wise robustness certification to evaluate the robustness of trained neural networks against Universal Perturbations (UPs). The authors pointed out an interesting intersection between universal adversarial perturbations (UAPs) and backdoor triggers. They showcased how the robustness certification to UPs of neural networks can impact these two lines of effort. The theoretical contribution evaluates the generalizability of the effects of the UPs computed using observed samples to unseen data populations. Detailed empirical evaluation regarding multiple perspectives of the proposed method is presented on two standard computer vision benchmarks (MNIST and CIFAR-10) regarding five different model structures (Conv-small, Conv-4-layer, Conv-big, ResNet-2B, and ResNet-4B)",
            "strength_and_weaknesses": "Strength:\n\n1. This paper focused on a critical problem, certifying the robustness of neural networks to UPs;\n2. The paper is well-written. I enjoyed reading this work;\n3. A clear structure for recognizing the road map of existing work in UAPs, backdoors, and robustness certification, which facilitates readers to position this work in an interesting intersection between UAP attacks and backdoor attacks;\n4. Interesting and solid technical efforts. Given the lack of theoretical analysis in existing UAP and backdoor attacks, the proposed theoretical analyzing framework in this paper gives an interesting first attempt toward a better understanding of neural networks' robustness against these two types of threats;\n5. Interesting experimental design. In particular, the author thoroughly evaluated the proposed UP certification and compared it with existing sample-specific certifications, and validated the results with three actual UAP attacks and two types of backdoor attacks. They also devised the theoretical efforts in practice and studied three implications of UP-robustness certification;\n\nWeaknesses:\n\n1. I find the intersection/similarity/difference between UAP attacks and backdoor triggers extended in this work quite interesting. Apart from the current related work section, I highly recommend the authors add a discussion to talk about the similarity/difference between these two lines of threats. Especially some of the recent work in the backdoor literature utilize this interesting intersection for stronger attack efficacy and better stealthiness [1,2] or effective defenses [3].\n2. I find most of the experimental settings well-detailed, except for the backdoor target-class identification experiment (Section 5.3). Please add more details to the threat model and the availability of the defender's knowledge, e.g., data accessibility.\n3. There is a limitation when using the proposed method to compare different UAP defenses regarding large norms (Table 7, Appendix C.1), i.e., all the results except the result on the IBP-trained model are trivial. I encourage the authors to add discussions on this limitation.\n\nReferences\n\n[1] Zhang, Quan, et al. \"AdvDoor: adversarial backdoor attack of deep learning system.\" Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis. 2021.\n\n[2] Zeng, Yi, et al. \"NARCISSUS: A Practical Clean-Label Backdoor Attack with Limited Information.\" arXiv preprint arXiv:2204.05255 (2022).\n\n[3] Kolouri, Soheil, et al. \"Universal litmus patterns: Revealing backdoor attacks in cnns.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The structure of the paper is clear and easy to read. This work looks into a vital problem but not addressed before in the literature. This study makes some novel and exciting technical contributions to the understanding of UP's generalizability, leading to a solid evaluation of the error bound of the proposed UP-robustness certification. Section 5.3's additional empirical discussion on instantiating UP-robustness certifications to compare model structures, UAP defenses, and to use them as a practical backdoor defense is inspiring and engaging.",
            "summary_of_the_review": "This work resides at the interesting crossroads of UAP and backdoor attacks. The authors presented a novel certification methodology for UP-robustness together with a theoretical study of the result's generalizability. With empirical findings, the implications of this work are discussed in detail. This work has a number of intriguing technical/empirical findings that could lead to future work on backdoor defenses, UAP defenses, and more advanced robustness certification for these threats.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1550/Reviewer_e7K7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1550/Reviewer_e7K7"
        ]
    },
    {
        "id": "ILcr4-NeI3g",
        "original": null,
        "number": 3,
        "cdate": 1666656471324,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656471324,
        "tmdate": 1666656471324,
        "tddate": null,
        "forum": "7GEvPKxjtt",
        "replyto": "7GEvPKxjtt",
        "invitation": "ICLR.cc/2023/Conference/Paper1550/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a certification defense against universal adversarial\nexamples and backdoor attacks. The proposed method is based on the combination\nof linear relaxation-based perturbation analysis and Mixed Integer Linear\nProgramming. It also provides a theoretical framework for analyzing the\ngeneralizability of the certification results. Experiments demonstrate that it\nis better than existing sample-wise certification methods when defending\nagainst UAPs.",
            "strength_and_weaknesses": "Strengths\n\n\u00b7 Interesting topic and good motivation.\n\u00b7 The proposed method has better results than sample-Wise certification.\n\u00b7 The writing is good.\n\nWeaknesses\n\n\u00b7 The novelty might be limited.\n\u00b7 It's unclear if the proposed method can be applied to certify the robustness\nagainst backdoor attacks.\n\u00b7 Comparisons with some related works are missing.\n\u00b7 Generalization of the proposed method is unclear.\n\u00b7 Experiments on adversarial patch attacks are missing.",
            "clarity,_quality,_novelty_and_reproducibility": "\u00b7 The contributions and the novelty are limited. This paper proposes a\ncertification defense against universal adversarial examples by modifying\nexisting certification methods for adversarial examples, i.e., \"auto LiRPA\".\nThe only difference is that the proposed method adds a constraint to ensure\nthe perturbation is global-wise. The technical challenges of the modification\n(adding the global-wise constraint) are unclear, and it does not seem very\nchallenging. Thus, the novelty of the proposed method might be limited.\n\n\u00b7 This paper claims it can be applied to backdoor attacks. The certification\nmethod uses $L_\\infty$ norm to bound the perturbations. However, most backdoor\nattacks are not constrained by $L_\\infty$ bound. For example, the patch\ntrigger in BadNets can have a large $L_\\infty$ norm. Thus, it is unclear if\nthe proposed method is able to certify the robustness against backdoor\nattacks. I think only a limited number of backdoors can be certified.\n\n\u00b7 Only empirical defenses for backdoor attacks are discussed. The discussion\nabout existing certification methods against backdoor attacks (Wang et al.\n[1]) is missing. This paper also lacks empirical comparisons between the\nproposed method and the existing certification method against backdoor\nattacks. \n\n\u00b7 Generalization to different datasets and models is unclear. Existing\ncertification work Xu et al. [2] demonstrated it can generalize to different\ndatasets, including ImageNet and various models (e.g., DenseNet, Transformer\nand LSTM). However, in this paper, all experiments are conducted on two\nsmall-scaled datasets (MNIST and CIFAR-10), and it only uses self-defined small\nCNNs and ResNet.\n\n\u00b7 While this paper claims the proposed method is general for different UPs, it\nlacks the discussion and the experiments on an important type of attack, i.e.,\npatch-based adversarial examples [3]. Thus, it is unclear if the proposed\nmethod is general.\n\n[1] Wang et al., On Certifying Robustness against Backdoor Attacks via Randomized Smoothing. AML@CVPR 2020.\n[2] Xu et al., Automatic Perturbation Analysis for Scalable Certified\nRobustness and Beyond. NeurIPS 2020.\n[3] Tom et al., Adversarial Patch. arXiv 2017.",
            "summary_of_the_review": "My main concern is the novelty might be limited, and the claim that the\nproposed method can be applied to certify the robustness against backdoor\nattacks might not be well supported.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1550/Reviewer_wZWn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1550/Reviewer_wZWn"
        ]
    },
    {
        "id": "B3eBmV0t7l",
        "original": null,
        "number": 4,
        "cdate": 1666735131925,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735131925,
        "tmdate": 1669229525004,
        "tddate": null,
        "forum": "7GEvPKxjtt",
        "replyto": "7GEvPKxjtt",
        "invitation": "ICLR.cc/2023/Conference/Paper1550/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the verification of neural networks against universal\nadversarial perturbations, i.e., perturbations that are universally applied on\na set of images. It solves the verification problem by adapting bound\npropagation- and MILP-based verification methods to  universal perturbations.",
            "strength_and_weaknesses": "+ The paper is the first to the best of my knowledge to discuss the formal\nverification of universal perturbations.\n\n+ The paper includes an analysis of the generalisation of robustness results\nfrom samples to the entire data distribution.\n\n- The verification method is a straightforward adaptation of existing methods. \n\n- Unsound comparison with bound propagation-based methods: whereas the\n  certified robust rate for the bound propagation methods is taken as the rate\n  of images for which a model is robust, said rate for the current method is\n  being minimised in an MILP formulation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and presented. It is highly incremental to previous\nwork as existing methods on bound propagation and MILP formulations are almost\nentirely used as appeared in previous works.  I think that the novelty can be\nimproved by exploiting the restricted nature of the problem studied (which\ncouples the perturbation radius to all inputs) to improve the scalability of\nverification.",
            "summary_of_the_review": "Whilst the paper is the first to study the verification problem for neural\nnetworks and  universal adversarial perturbations, the resulting method is\nhighly incremental to previous work and the soundness of the comparisons with\nrelated work is not convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1550/Reviewer_Kpjw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1550/Reviewer_Kpjw"
        ]
    }
]