[
    {
        "id": "0AO-62KUMG",
        "original": null,
        "number": 1,
        "cdate": 1666029918242,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666029918242,
        "tmdate": 1666029918242,
        "tddate": null,
        "forum": "ybFkELZjuc",
        "replyto": "ybFkELZjuc",
        "invitation": "ICLR.cc/2023/Conference/Paper1440/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops an interpretable anomaly detection framework, DIAD,  by combining the NodeGAM structure with the PID objective. The NodeGAM provides an interpretable mechanism, and PID can be trained for anomaly detection. Overall, the framework makes sense to me. The paper includes detailed experimental evaluation by comparing with several state-of-the-art baselines on multiple datasets.",
            "strength_and_weaknesses": "Strength\n1. The paper targets an important research problem, interpretable anomaly detection.\n2. The proposed framework leverages the existing techniques and makes sense.\n3. Detailed evaluation is conducted to show the performance of anomaly detection.\n\n\nWeakness:\n1. In the unsupervised setting, the proposed DIAD can achieve better or comparable performance on large datasets, but may not be as good as baselines when the datasets only have small amounts of samples. Do the authors have any insight into such observations? \n\n2. My major concern is in the interpretation part. How to achieve interpretable anomaly detection based on DIAD is not very clear to me. For example, given an anomaly, do we explain the result based on the ranking of features evaluated by the sparsity values or based on the feature-wise function? Meanwhile, as Mammo. only consists of 6 features, I am also curious about the interpretability of the proposed model on high-dimensional data. I believe for low-dimensional data, it is not very challenging for domain experts to exam the anomalies. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is not very hard to follow as long as the readers have knowledge about NodeGAM and PID. By combining these two approaches, DIAD can achieve interpretable anomaly detection on tabular data, which is somewhat novel. The authors promise to provide code upon paper acceptance.",
            "summary_of_the_review": "I feel that the proposed approach has some merits, but I am also not very sure whether the combination of NodeGAM and PID is trivial or challenging. Meanwhile, the term \"interpretable anomaly detection\" in the title really draws my eye, but the evaluation in this part is weak in my view. I understand currently there is no golden metric to evaluate the interpretability, but maybe showing more cases could be more convincing, just like the papers in the CV area. \n\nI also have one more question, which may be outside the scope of this paper: If the purpose is to achieve sample-wise interpretation (explaining the anomalies), could we apply other post-hoc interpretation techniques to interpret PIDForest? ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1440/Reviewer_235S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1440/Reviewer_235S"
        ]
    },
    {
        "id": "n7Un4AKwt4L",
        "original": null,
        "number": 2,
        "cdate": 1666188968931,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666188968931,
        "tmdate": 1666188968931,
        "tddate": null,
        "forum": "ybFkELZjuc",
        "replyto": "ybFkELZjuc",
        "invitation": "ICLR.cc/2023/Conference/Paper1440/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new interpretable method for unsupervised or semi-supervised anomaly detection on tabular data, in the presence of noisy or unlabeled data. ",
            "strength_and_weaknesses": "Strengths:\n- Innovative approach based on generative additive models.\n- The model is explainable.\n- There is a little technical contribution (Prop 1) creating some insight into the method's inner workings.\n\nWeaknesses:\n- Section 6.1. on unsupervised AD experiments has some problems in terms of the chosen datasets and baselines: [1] determined [2] as the overall best performing deep AD method on tabular data. However, the method [2] is missing in the comparison here. On the 14 datasets from Gopalan et al., the proposed approach shows inferior performance, except for SatImage, where a tiny improvement of 0.4% is achieved. Out of the 9 datasets from Pang et al., only 6 have been selected. To avoid dataset selection bias, all 9 should be analyzed. Besides NeuTraL, also DSVDD - a common baseline - is missing in the comparison. \n- In the noise experiments, only two datasets have been analyzed, which is insufficient to draw conclusions.\n- Section 6.2 on semi-supervised AD: These experiments look better. The approach achieves higher AUROC than the Devnet baseline. However, the original Devnet paper reported on AUPRC, which is why I cannot confirm that the present results are consistent with their analysis. The semi-supervised DSVDD baseline is missing.\n\n\n\n[1] Maxime Alvarez, Jean-Charles Verdier, D'Jeff K. Nkashama, Marc Frappier, Pierre-Martin Tardif, Froduald Kabanza: A Revealing Large-Scale Evaluation of Unsupervised Anomaly Detection Algorithms. https://arxiv.org/abs/2204.09825\n[2] Chen Qiu, Timo Pfrommer, Marius Kloft, Stephan Mandt, Maja Rudolph Proceedings of the 38th International Conference on Machine Learning, PMLR 139:8703-8714, 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to read and follow. The approach is novel. The reproducibility is rather high, but could be improved at places (see above).",
            "summary_of_the_review": "In summary, the authors propose an innovative approach to AD on tabular data. The experimental evaluation shows good promise, but is not entirely convincing. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1440/Reviewer_qGLb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1440/Reviewer_qGLb"
        ]
    },
    {
        "id": "O-6hnlSJ5t3",
        "original": null,
        "number": 3,
        "cdate": 1666671297318,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671297318,
        "tmdate": 1666671383864,
        "tddate": null,
        "forum": "ybFkELZjuc",
        "replyto": "ybFkELZjuc",
        "invitation": "ICLR.cc/2023/Conference/Paper1440/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper introduces a new anomaly detection method, namely DIAD, that uses Partial Identification (PID) as an objective to perform anomaly detection optimization with a tree structure of an existing generalized additive model. It is also flexible to use an additional loss function, such as AUC optimization or BCE loss, to utilize auxiliary labeled anomaly data in the semi-supervised setting. Due to the use of GAM model structures, it also offers a two-way feature interaction based explanation of detected anomalies. The effectiveness of the method is evaluated on 18 tabular datasets.",
            "strength_and_weaknesses": "The work aims to tackle both the interpretability and the capability of utilizing a few labeled anomaly examples in anomaly detection. It has the following strengths:\n- Both of the interpretability and the sample-efficient capability are important to different ML tasks, including anomaly detection. The paper addresses both of these two important aspects.\n- The presented method can work in both unsupervised and semi-supervised settings. Limited AD methods have such a property.\n- Experiments on 18 datasets show the effectiveness of the method against popular unsupervised/semi supervised methods\n\nSome issues that may require further investigation include:\n- The technical novelty of the method is weak. The methods like PID were used in anomaly detection, and the AUC loss is a widely used loss function and it does not bring much improvement over BCE. The full method seems to be a combination of these widely used components.\n- The method is weak in the unsupervised anomaly detection setting, which does not show clear improvement over the closely related competing methods like PIDForest and IF. There are also other more effective improved IF methods that tackle the studied issues in the paper, such as [a-c] and references therein. It would be more convincing to review these studies and possibly include them in the comparison.\n- In terms of the semi-supervised setting, the proposed method involves two stages of training, unsupervised training and fine-tuning. I wonder whether the competing methods like CST and DevNet are trained from scratch, or only fine-tuned in a similar way as the proposed method. Questions here include how much the contribution is it from the unsupervised training in the semi-supervised setting? Does such a training strategy also help improve other existing semi-supervised methods?\n- The work argues a two-way feature interaction-based anomaly explanation, but the presented result in Figure 4 is only the explanation of individual features. Figure 4(d) is a two-way interaction example, but the results indicate only individual features, e.g., the Area feature, are sufficient for the detection and interpretation.\n- The interpretation results are limited to qualitative ones, and do not involve comparison to other methods, and thus, they are not convincing enough.\n- There exist a large number of gradient backpropagation-based methods for anomaly explanation, especially on image data. This type of methods is applicable to tabular data, too. The authors are suggested to elaborate more based on more related work in the discussion in Related Work. \n\n**References**\n- [a] \"Sparse modeling-based sequential ensemble learning for effective outlier detection in high-dimensional numeric data.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1. 2018.\n- [b] \"Extended isolation forest.\" IEEE Transactions on Knowledge and Data Engineering 33, no. 4 (2019): 1479-1489.\n- [c] \"Generalized isolation forest for anomaly detection.\" Pattern Recognition Letters 149 (2021): 109-119.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper clarity and technical novelty is weak. The reproducibility is fair.",
            "summary_of_the_review": "Considering both the pros and cons above, the paper is at the borderline and towards the reject side.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1440/Reviewer_1y6T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1440/Reviewer_1y6T"
        ]
    },
    {
        "id": "z_uwjyHUuK",
        "original": null,
        "number": 4,
        "cdate": 1666683535570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683535570,
        "tmdate": 1669190852173,
        "tddate": null,
        "forum": "ybFkELZjuc",
        "replyto": "ybFkELZjuc",
        "invitation": "ICLR.cc/2023/Conference/Paper1440/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents an approach for tabular anomaly detection which is based on Partial Identification (PID) and on Generalized Additive Models (GAM) and extensions. The method works also in semisupervised settings and compares well with alternatives, as shown in a quite extensive experimental evaluation.",
            "strength_and_weaknesses": "Positive points\n- The paper is well written and easy to read\n- The topic is definitely interesting\n- The structure of the manuscript is clear\n- Experiments are extensive\n\n\nNegative points\n- Significance of the proposed method. \nMy main concern is about significance of the proposed method. It seems that authors start from ideas presented in PID and PIDforest, for anomaly detection, and optimized such framework and created an effective pipeline by adding several carefully chosen ingredients. The resulting framework is definitely well performing, as shown in the experiments, but I\u2019m wondering how large is its methodological contribution from the Anomaly Detection perspective (but of course this is my personal opinion). As for the theoretical contribution, I think it is not so relevant, being reduced to the three lines of page 5 (by the way, the proof of the proposition, found in Appendix, is not clear to me, especially the sentence which concludes the proof)\n\n- Conclusions from the experiments.\nExperiments are very extensive, based on 20 tabular datasets and involving many different competitors. One comment which applies to the whole analysis. The tables show bold values, which are meant to highlight the best results; however these do not derive from a rigorous statistical analysis, but simply by the rule \u201cMetrics with standard error overlapped with the best number are bolded\u201d (from the paper, caption table 2). Without a rigorous statistical evaluation conclusive observations can not be derived. I suggest authors to use the Friedman test followed by a post-hoc Nemenyi test, and to show results via critical diagram \u2013 see for example\n\nDem\u0161ar, Janez. \"Statistical comparisons of classifiers over multiple data sets.\" The Journal of Machine learning research 7 (2006): 1-30.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, only two comments on presentation:\n- Section 6.3 of the results (part \u201cExplaining anomalous data): this section is not completely convincing. Are you extracting something that other methods can not extract? \nMoreover, plots in Fig 4 are too small, it is very difficult to get colors, lines and in general the content\n\n- References: in general there are many citations to arxiv papers, also for old papers. Please try to avoid this whenever it is possible. There are also some duplicated entries:\n- Pang and Aggarwal 21\n- Pang, Shen, van den Hengel 19\n- Yoon et al 2020\n\nFor what concerns novelty: \nEstimation of the PID in section 5. Maybe authors can consider also the work by Goix and colleagues, which extends Isolation Forests by optimizing a criterion based on volume ratios (formulated however in a different shape, with respect to what authors did):\n\nGoix, N., Drougard, N., Brault, R., Chiapino, M.: One class splitting criteria for random forests. In: ACML, pp. 1\u201316 (2017)\n\n",
            "summary_of_the_review": "Paper which proposes a carefully tailored pipeline for anomaly detection of tabular data. Some doubts on its significance. Large experimental parts which should be completed with a rigorous statistical analysis.\n\nUPDATE AFTER THE REBUTTAL. \nI carefully read the responses, the clarifications, and the additional material, and I thank the authors for the significant efforts made in clarifying my doubts. Even if I consider that the paper has potential, I\u2019ll maintain my score unchanged, since I still have few doubts, especially on the significance and on the analysis of the results (even if I acknowledge the efforts made by the authors in improving these aspects in the rebuttal)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1440/Reviewer_XPd4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1440/Reviewer_XPd4"
        ]
    }
]