[
    {
        "id": "nhd1ymtNW-",
        "original": null,
        "number": 1,
        "cdate": 1666615910546,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666615910546,
        "tmdate": 1669923336632,
        "tddate": null,
        "forum": "D7srTrGhAs",
        "replyto": "D7srTrGhAs",
        "invitation": "ICLR.cc/2023/Conference/Paper5200/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a task-agnostic knowledge distillation method for NLP models based on iterative pruning combined with distillation. A novel distillation loss is proposed, that performs differentiable matching of the latent representations. The proposed distillation method is compared to a selection of baselines, and is shown to perform well.",
            "strength_and_weaknesses": "Strengths:\n* An efficient distillation method is proposed that leverages both KD and pruning\n* Experiments are realistic, and show the superiority of the proposed method compared to the SOTA baselines\n* Latent space mapping/alignment is done elegantly via a learned weight matrix\n\nWeaknesses:\n* Reported medians over 5 runs, no standard deviations: how robust is the method?\n* Table 2 could be presented visually",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well-written, with minor grammatical issues. The experiments are run five times, and medians are reported: I would have appreciated also seeing means and standard deviations. The method itself combines existing technique in a new setting, thus there is novelty, albeit somewhat limited. \n\nWill the code be made available should the paper be accepted? This would obviously help with reproducibility.",
            "summary_of_the_review": "Overall, I think this is a useful, practical paper, showing how existing methods and ideas can be combined to achieve higher quality solutions. Compressing huge models is an important task, since many resource-constrained settings exist that currently can\u2019t leverage the modern NLP models. The authors should clarify if the latent representation alignment via a differentiable weight matrix is their idea: this is obviously an excellent way of discovering the alignment, I have not encountered it in the literature before. However, if the idea is borrowed, the sources must be stated clearly.\n\nSee below for additional suggested corrections:\n\nPage 4: \u201cthe loss for continual pre-training the student model on the open-domain data\u201d -> the loss for continual pre-training of the student model on the open-domain data\nPage 4: \u201cconsists both the distillation\u2026\u201d -> consists of both\u2026\n\nMedians of 5 runs are reported - what about the corresponding standard deviations? Perhaps Table 2 can be represented visually (e.g. box-and-whisker plots)?\n\nRegarding the matching rule: you propose a differentiable weighted matching scheme, i.e. the matching is determined via training. Is my understanding correct? Has similar matching been applied in the NLP domain before? Please indicate the sources.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5200/Reviewer_1n6p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5200/Reviewer_1n6p"
        ]
    },
    {
        "id": "TPuaa__W3bl",
        "original": null,
        "number": 2,
        "cdate": 1666669351912,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669351912,
        "tmdate": 1669493062401,
        "tddate": null,
        "forum": "D7srTrGhAs",
        "replyto": "D7srTrGhAs",
        "invitation": "ICLR.cc/2023/Conference/Paper5200/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper propose HomoDistil, a method which combines knowledge distillation and pruning to obtain efficient networks.",
            "strength_and_weaknesses": "The proposed method is simple yet effective, good performance are obtained according to experiments;\n\nThe paper is easy to follow;\n\nHowever, the proposed method has limited novelty, it is based on some existing methods;\n\nIt would be nice if the authors can provide a comprehensive comparison of computation cost/time between the proposed method and previous methods;\n\nWhy results of model with 66M parameters is not reported in Table 3?\n\nPlease adjust the position of Table 2, Table 3 and the paragraph between the two tables.\n\nSome related works are suggested to be compared.\n\n[1]. Multi-Granularity Structural Knowledge Distillation for Language Model Compression. Chang Liu, Chongyang Tao, Jiazhan Feng, Dongyan Zhao\n\n[2]. BERT Learns to Teach: Knowledge Distillation with Meta Learning. Wangchunshu Zhou, Canwen Xu, Julian McAuley.\n\n[3]. LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding. Hao Fu, Shaojun Zhou, Qihong Yang, Junjie Tang, Guiquan Liu, Kaikui Liu, Xiaolong Li.\n\n[4]. MixKD: Towards Efficient Distillation of Large-scale Language Models. Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, Lawrence Carin.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow, the novelty is limited. No code is provided, but the experiment seems to be reproducible.",
            "summary_of_the_review": "Although the proposed method is simple and seems effective, I lean towards rejection because of the limited novelty.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5200/Reviewer_Y2q6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5200/Reviewer_Y2q6"
        ]
    },
    {
        "id": "9amM0PJPZIs",
        "original": null,
        "number": 3,
        "cdate": 1666697331105,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697331105,
        "tmdate": 1666697331105,
        "tddate": null,
        "forum": "D7srTrGhAs",
        "replyto": "D7srTrGhAs",
        "invitation": "ICLR.cc/2023/Conference/Paper5200/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles with a problem that when distilling a small student model using a large pre-trained transformer, the \u201ccapacity gap\u201d between the student and the teacher often hinders the teacher from transferring good performance to the student. The HomoDistill method proposed by this paper investigates to progressively prune the teacher model into a small student model, while in the same time distilling it using the original teacher model. Experiments show that the combination of pruning and distillation shows advantage on several NLU and QA tasks.",
            "strength_and_weaknesses": "strength:\n* HomoDistill illustrates advantageous performance over other distillation methods like TinyBERT.\n* The combination of pruning and distillation in task-agnostic setting seems novel. \n\nweakness:\n* A comparison with other prune & distill methods such as (Xu et al., 2021; Xia et al., 2022) would make this submission more complete.\n* Please also discuss the difference between HomoDistill and [1], which presents a similar method for vision tasks, where a teacher model is slimmed into a small student model while distilling it.\n\n[1] Slimmable Neueral Networks, ICLR 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "The wrting is clear, the whole paper is easy to understand. I see the experiments are all conducted on public benchmarks.",
            "summary_of_the_review": "In general, I think HomoDistill is a novel method that combines pruning with distillation in a task-agnostic setting for NLP tasks. The experiments on NLU and QA benchmarks show the advantages of HomoDistill over other distillation methods. However, the paper lacks experimental comparison with other methods that combine pruning and distillation. Appending such comparison would make this paper more solid.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5200/Reviewer_na3M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5200/Reviewer_na3M"
        ]
    }
]