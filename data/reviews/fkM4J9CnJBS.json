[
    {
        "id": "ML4UkkbWdT",
        "original": null,
        "number": 1,
        "cdate": 1666430714574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666430714574,
        "tmdate": 1666430714574,
        "tddate": null,
        "forum": "fkM4J9CnJBS",
        "replyto": "fkM4J9CnJBS",
        "invitation": "ICLR.cc/2023/Conference/Paper2799/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "(1) This paper aims to solve the long-tailed recognition task.     \n(I) This paper proposes that the class-conditional distribution shift can not be ignored in practice with empirical analysis.    \n(II) Based on this observation, this paper proposes to utilize DRO to minimize the worst risk and It can be understood as an adaptive data augmentation method.     \n(2) Experiments on CIFAR-LT and Tiny-ImageNet-LT are conducted. Some improvements are observed compared with baselines. \n",
            "strength_and_weaknesses": "Strength:    \n\n(1) The paper gives a sound theoretical analysis.   \n\n\nWeakness:    \n\n(1) Experiments are conducted only on CIFAR-LT or TinyImageNet-LT, which makes the experimental results unconvincing. In the long-tailed community, we usually validate the effectiveness of methods with ImageNet-LT, iNaturalist 2018, and Places-LT.     \n\n(2) The authors claim that \"decoupling methods avoid more severe CCD shift\". Does it mean that it is necessary to address long-tailed recognition with 2 stages? However, recent state-of-the-art methods, like PaCo [1], train representation and classifier jointly. Can the proposed method improve performance when it is applied to such a strong baseline?     \n\n[1] Parametric Contrastive Learning. ICCV 2021.      \n\n(3) How can we remove the effect of shifted CCD?      \n     As analyzed in Sec 3.2, function (5) is used to sample images. From my understanding, with function (5), all images can be viewed by the trained model, which is the same as that of training models on balanced data. The only difference is that the sample probability for tail-class images will be smaller.   \n\n(4) Analysis of the computational cost for the proposed method is missing.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "No code is provided.",
            "summary_of_the_review": "The paper aims to address the long-tailed recognition problem. Analysis of the CCD issue is interesting.\nHowever, the evaluation is unconvincing. Comparisons with state-of-the-art methods are missed. Also, only small datasets are evaluated.   \n\nIf concerns are well addressed, I'm glad to raise my score.\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2799/Reviewer_9ohR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2799/Reviewer_9ohR"
        ]
    },
    {
        "id": "BOAAevfnrtI",
        "original": null,
        "number": 2,
        "cdate": 1666558808713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558808713,
        "tmdate": 1666558808713,
        "tddate": null,
        "forum": "fkM4J9CnJBS",
        "replyto": "fkM4J9CnJBS",
        "invitation": "ICLR.cc/2023/Conference/Paper2799/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors tackle the problem of long-tailed recognition. The authors highlight the issue of Class-Conditional Distribution (CCD) shift due to scarce instances. The authors propose an adaptive data augmentation method, Distributionally Robust Augmentation (DRA) to improve performance for long-tailed recognition tasks. Experiments across several semi-synthetic datasets illustrate the efficacy of DRA. ",
            "strength_and_weaknesses": "**Strengths**: \n- The paper tackles an important problem of long-tailed recognition. \n- Empirical results with the proposed DRA technique in Table 2 are promising. \n\n\n**Weaknesses**: \n- Discussion on Proposition 1 is a bit misleading. Authors argue that ` with enough instances from each class, models working well on training set could perform well in the test'. However, it is well accepted that usually even for balanced benchmark datasets (i.e., CIFAR100 or Imagenet), with overparameterized networks, we can not approximate $p(x|y)$ or the generalization performance. In general, for high-dimensional vision datasets typically used, the sample size is small enough that we would see high drift as evaluated by Proposition 1. If this understanding is incorrect, I would encourage authors to provide examples in balanced datasets like CIFAR10, CIFAR100, or Imagenet where bounds like in Proposition 1 are tight. \n- To put it differently, the issue here is that authors are trying to argue that it is not possible to estimate $p(x|y)$ from finite samples in LT classes. While this is understandable, it is unclear if (i) any method actually explicitly computes $p(x|y)$ to handle long tail classes; and (ii) $p(x|y)$ can be estimated reasonably even for balanced benchmark datasets like CIFAR-10, CIFAR-100, etc.  \n- In light of the above comment, I guess a more appropriate argument would be to discuss the relative sample sizes in tail classes of long-tailed datasets versus sample sizes in balanced datasets like CIFAR100. However, I am not sure if such an argument could be theoretically made with bounds in Proposition 1. \n- It is unclear if equation (4) is actually used to sample datasets in prior work. To the best of my understanding equation (4) represents the empirical distribution of a dataset sampled to satisfy LT phenomena. The core issue here is if we use sampling with replacement from distribution in (4) versus distribution in equation (5), we may see a lot of repeated examples from long tail classes in case of (4) than in case of (5). While this is intuitive, it is unclear if prior work has really used sampling with replacement from a model as in (4). Can authors add references to prominent prior works which used this strategy for benchmarking?\n- The paper is hard to follow starting from Section 4. In particular, I could not find a clear description of the proposed DRA method in the main paper. In Algorithm 1, it is unclear why the loop in Line 4 will be executed with $g_i^*$ initialized the way it is in Line 3. \n- Figure 3 is also a bit unclear. It is hard to follow the description of the DRA method from this figure. For example, it is unclear what \"From the k-th,...,M-th loops\" mean? What is the difference between blue and orange arrows? Why there are two input images to the networks (one in blue and one in orange)?  ",
            "clarity,_quality,_novelty_and_reproducibility": "\n\n**Writing suggestions**: \n- As mentioned before, the exposition of Section 4 can be significantly improved. \n- WRM is discussed in the paper (specifically while introducing DRA in Sec 4.2) without really explaining what the method is. For completeness, it would be great if the authors can add a brief discussion of this method.  \n- Minor: In the conclusion paragraph, \"we\" is capitalized without any reason in the first line. \n- Statements like this \"The convergence and generalization of DRA are theoretically guaranteed.\" are misleading in the abstract. Since authors do not present an end-to-end theoretical guarantee that DRA would generalize in all settings without any assumptions, it would be great to contextual and tone down the language of the contribution. \n\n\n**Missing related work**: \n- The assumption of $p_{train}(x|y) = p_{test}(x|y)$ and $p_{train} (y) \\ne p_{test}(y)$ is well known as label shift assumption in the DA literature [1,2,3,4]. Authors should add a discussion on this line of literature with appropriate references. \n\n[1] Z. C. Lipton, Y.-X. Wang, and A. Smola. Detecting and Correcting for Label Shift with Black Box Predictors. In International Conference on Machine Learning (ICML), 2018.\n\n[2] S. Garg, Y. Wu, S. Balakrishnan, and Z. C. Lipton. A unified view of label shift estimation. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\n[3] A. Storkey. When Training and Test Sets Are Different: Characterizing Learning Transfer. Dataset Shift in Machine Learning, 2009\n\n[4] M. Saerens, P. Latinne, and C. Decaestecker. Adjusting the Outputs of a Classifier to New Priori Probabilities: A Simple Procedure. Neural Computation, 2002.\n\n ",
            "summary_of_the_review": "Overall, I think while this paper tackles an important problem of long-tailed recognition, the paper is hard to follow in its current stage and several arguments are presented without concrete theoretical and empirical evidence. In my understanding, the motivation for the CCD shift can be presented in a simpler way. A clear description of the proposed DRA algorithm is also missing in the paper. While the proposed DRA taking looks promising from empirical results, the connection with the proposed motivation is a bit thin. \n\nI encourage authors to participate in the discussion to clarify if I share any misunderstanding. I will be open to changing my score if they think I misunderstood any of the key arguments made in the paper.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2799/Reviewer_7ExN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2799/Reviewer_7ExN"
        ]
    },
    {
        "id": "W_c9IzzhYSJ",
        "original": null,
        "number": 3,
        "cdate": 1666687865150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687865150,
        "tmdate": 1667446527507,
        "tddate": null,
        "forum": "fkM4J9CnJBS",
        "replyto": "fkM4J9CnJBS",
        "invitation": "ICLR.cc/2023/Conference/Paper2799/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studied the phenomenon that inconsistent CCD causes bad performance in long-tailed recognition. It proposed a new approach DRA, for data augmentation. The theoretical properties of the proposed method were proven, and experimental results revealed the improvement of applying DRA in long-tailed problems. ",
            "strength_and_weaknesses": "Strength\n\n* Good motivation and theoretically proved algorithm;\n\nWeakness \n\n* The theory about the algorithm is trivial and does not provide much insight into the progress of the field;\n* The high-level framework of DRA is missing. The paper quickly dived into details, making it very hard to understand the big picture;\n* The connection between each part is not clear. For example, Eq.5 seems to express the paper's central idea but is never referred to/used in the next parts. The connection between Eq.5 and the theorems is also not clearly stated. \n* The performance improvement seems marginal. \n* The authors only conducted experiments on CIFAR and Imagenet-LT, which are relatively small datasets. Would the authors like to run experiments on larger datasets, such as iNaturelist, to confirm the effectiveness of the authors' data augmentation scheme?\n* The paper seems to use a sampling method to remove the CCD shift (Eq. 5), so the problem becomes a traditional label shift problem. Then it applies the DRO to solve the label shift problem. The second part is quite similar to the work \"Coping with Label Shift via Distributionally Robust Optimisation, ICLR 2021\". Could the author tell the difference between the second part of this paper and that paper?\n* The paper proposed the sampling method (Eq.5) and then quickly moved to the optimization method. I'd like to see a more thorough analysis of the sampling method, including in what circumstances the sampling method is guaranteed to remove the CCD shift. \n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: Poor\n* Quality: Fair\n* Novelty: Fair\n* Reproducibility: Fair ",
            "summary_of_the_review": "The motivation of this paper is good, and the method seems reasonable. However, the theoretical analysis does not provide much insight into the field, and the performance improvement is marginal. Furthermore, the paper's organization is not good, making the paper very hard to read. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2799/Reviewer_qfH1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2799/Reviewer_qfH1"
        ]
    },
    {
        "id": "rD4sKPAQp4",
        "original": null,
        "number": 4,
        "cdate": 1667119491852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667119491852,
        "tmdate": 1667119491852,
        "tddate": null,
        "forum": "fkM4J9CnJBS",
        "replyto": "fkM4J9CnJBS",
        "invitation": "ICLR.cc/2023/Conference/Paper2799/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper investigates the Class-Conditional Distribution (CCD) shift issue in long-tailed recognition due to scarce instances, which exhibits a significant discrepancy between the empirical CCDs for training and test data, especially for tail classes. To alleviate the issue, this paper presents a data augmentation approach to generate more virtual samples.",
            "strength_and_weaknesses": "# Strength\n1. The studied problem is interesting and not well-addressed in existing literature;\n2. The proposed method is well-motivated and equipped with theoretical analyses;\n3. Extensive experiments show the advantages of the proposed method;\n4. This writing is clear and easy to follow.\n\n# Weakness\n1. Novelty: the proposed Distributionally Robust Augmentation (DRA) approach is borrowed from previous work DRO and no significant improvement has been made. The used min-max optimization is very commonly used in related works;\n2. Empirical improvement is weak. As reported in Table 2 and 7, the proposed DRA does not consistently outperforms existing ones, such as Logit Adjustment, PC softmax, LADE;\n3. Rationale behind the approach: There is a gap between the DRA and CCD shift. Why DRA can remove CCD shift? How accurate can DRA approximate real CCD? Those questions seem not answered in the theorems.\n4. Hyperparameters: the method involves several hyperparameters, e.g., $C, S, \\beta, M, \\alpha_{inner}$ and some of them require a validation set for tuning which is not applicable in some cases. ",
            "clarity,_quality,_novelty_and_reproducibility": "For clarity, quality, and novelty, see Strength And Weaknesses;\nReproducibility should be okay.",
            "summary_of_the_review": "This paper studies a relatively new problem in long-tailed learning and a simple strategy is proposed. However, the proposed method seems not novel enough and has several hyperparameters. In addition, experimental show that the advantages of the method is not significant.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2799/Reviewer_Lnfc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2799/Reviewer_Lnfc"
        ]
    }
]