[
    {
        "id": "EmaqzywPDa",
        "original": null,
        "number": 1,
        "cdate": 1666640956206,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640956206,
        "tmdate": 1666640956206,
        "tddate": null,
        "forum": "RUzSobdYy0V",
        "replyto": "RUzSobdYy0V",
        "invitation": "ICLR.cc/2023/Conference/Paper6620/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "1. This paper studies the effect of label error on a model\u2019s group-based disparity metrics. Differences in terms of calibration error can be observed for the minority (smallest) group and the majority (largest) group.\n2. They also propose an approach (influence function) to estimate how changing a single training input\u2019s label affects a model\u2019s group disparity metric on a test set. Based on the influence function, they can identify training points that have a high effect on a model\u2019s test disparity metric.\n",
            "strength_and_weaknesses": "Strength\n\n1. They try to answer two significant questions regarding label errors and fairness:\n    What is the sensitivity of a model\u2019s disparity metric to label errors in training and test data? Does the effect of label error vary based on group size?\n    How can a practitioner identify training points whose labels have the most influence on a model\u2019s group disparity metric?\n2. Various disparity metrics, models, and datasets are considered\n    model disparity metrics: expected calibration error (ECE), the Brier Score (BS), False Positive Rate (FPR), False Negative Rate (FNR), and Error Rate (ER).\n    datasets across different modalities: 4 tabular, and a text dataset.\n    models: a logistic regression model, a Gradient-boosted Tree (GBT), and ResNet-18.\n3. The closed form influence function is potentially useful for regularizing or mitigating the label error while training.\n\nWeaknesses\n1. Although multiple datasets are used, how to make sure that the conclusions and analysis drawn is not dataset-specific is still an open question.\n2. The label flipping is indeed useful for empirical results but not interesting as a technical contribution.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. Their theoretical analysis seems to be more interesting. But is the chain rule analysis covered by other work? This is one part which I am not sure about regarding novelty.\n2. I feel like the key approach for the influence function regarding label errors is not brand new, but modified for the new target applications (regarding fairness).\n3. Although label noise and label errors are not exactly the same,  but I would appreciate it if the authors can briefly explain the difference (in terms of the key approach) between this work and the paper [Fair Classification with Group-Dependent Label Noise]. \n",
            "summary_of_the_review": "1. The overall quality of the paper is good, and the key ideas are clear enough to make reviewers easy to follow.\n2. If as they claimed, label errors\u2019 influence on group disparity is not yet covered by literature, this is a strong submission.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6620/Reviewer_DK6U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6620/Reviewer_DK6U"
        ]
    },
    {
        "id": "1UlNMuZj0fp",
        "original": null,
        "number": 2,
        "cdate": 1666652125635,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652125635,
        "tmdate": 1666652537741,
        "tddate": null,
        "forum": "RUzSobdYy0V",
        "replyto": "RUzSobdYy0V",
        "invitation": "ICLR.cc/2023/Conference/Paper6620/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers an important problem of label noise in the training data. Specifically, it studies the effect of label error on a model's group-based disparity metrics, with more focus on smaller groups in the data. Then, the authors of the paper take a step further by considering a method based on influence function to identify training samples that significantly impact the test disparity metric of interest. The authors of the paper conduct a series of experiments to answer these questions and offer valuable insight into this important problem. ",
            "strength_and_weaknesses": "Strength:\n- This paper is very well organized and written in general. Most of the claims are supported by ample experimental analysis. \n- The problem of concern has a unique fairness perspective, which has great practical significance. \n\nWeaknesses: \n- Learning with noisy labels is a widely studied topic, especially in the context of neural networks. As an empirical paper, it would be nice if the authors of the paper can conduct some additional analysis to show the effect of some of the recently proposed noise-robust algorithms on such group-based disparity metrics. \n- Similar to the above point, it would be nice if the authors of the paper benchmarked the proposed \"IF-Calib-Label\" against some other recently proposed noise-robust algorithms that can potentially identify label errors.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written in general, and ample details are provided to help reproduce the results shown in the paper. ",
            "summary_of_the_review": "Despite the interesting perspective and a well series of well-conducted experiments, I feel like the authors of the paper can provide some additional experimental insight to the paper, as suggested above. As such, I recommend a weak accept for now. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6620/Reviewer_oCK7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6620/Reviewer_oCK7"
        ]
    },
    {
        "id": "7AWRUUgNqYe",
        "original": null,
        "number": 3,
        "cdate": 1666676339265,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676339265,
        "tmdate": 1666758992420,
        "tddate": null,
        "forum": "RUzSobdYy0V",
        "replyto": "RUzSobdYy0V",
        "invitation": "ICLR.cc/2023/Conference/Paper6620/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the effect of label error on the model\u2019s disparity metrics (e.g., calibration, FPR, FNR) on both the training and test set. Empirically, the authors have found that label errors have a larger influence on minority groups than on majority groups. To mitigate the impact of label errors, The authors have proposed a method to estimate the influence of changing a single training input\u2019s label on a model\u2019s group disparity metric.",
            "strength_and_weaknesses": "Strength:\n+ The research problems are important and may have many practical applications. The real-world machine learning dataset can easily contain label errors. Improving the robustness of learning models trained on noisy data is important. Existing methods mainly focus on downstream accuracy, but group-based disparity metrics have been ignored which are also important for designing a robust algorithm. \n+ The proposed method is well-motivated. Estimating the influence of a single training input on a model\u2019s group disparity metric is important for confident example selection and dataset purification.\n\n\n\nWeakness:\n+ The technical insight may not be enough. The authors have empirically illustrated that minority groups are more sensitive to label errors than majority groups. To make the conclusion more meaningful and practical, I think it would be great to add some theoretical analysis on the influence of label errors with different minority and majority group sizes.\n\n+ The proposed method for estimating the \u2018influence\u2019 of perturbing a training point\u2019s label on a disparity metric may not practical. The computational cost of the method seems very expensive and needs a lot of retraining processes to detect the effect of all training inputs, which can be hard to apply to a dataset with high-dimensional features. In addition, to demonstrate the performance of the proposed methods, some SOTA methods should be compared (e.g., JoCoR, CVPR\u201920; DivideMix, CVPR\u201920; MEIDTM, CVPR\u201922). The benchmark datasets such as CIFAR10 and CIFAR100 with different types of synthetic noise should also be compared.\n\n\n+ The experiment setting is not clear to me. For example, it is not clear how the minority group and majority group in Fig. 1 and Fig.2 are obtained. I think the authors may also need to discuss that how to apply the convolutional network Resnet-18 to tabular and text datasets. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper generally is well-written and easy to follow, but most discussions are based on experimental results obtained from a few datasets. The experimental settings and comparison should be more detailed and comprehensive.",
            "summary_of_the_review": "For me, the motivation and research problems of this paper are strong and important. My major concerns are that the technical contribution may not that strong, and the proposed method may not practical and hard to be applied to real-world machine learning datasets. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have not found any ethics concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6620/Reviewer_Xn1n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6620/Reviewer_Xn1n"
        ]
    }
]