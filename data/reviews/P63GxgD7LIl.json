[
    {
        "id": "canPogon_3j",
        "original": null,
        "number": 1,
        "cdate": 1666577382745,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577382745,
        "tmdate": 1666577382745,
        "tddate": null,
        "forum": "P63GxgD7LIl",
        "replyto": "P63GxgD7LIl",
        "invitation": "ICLR.cc/2023/Conference/Paper6290/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a white-box adversarial attack against neural machine translation model named TransFool. TransFool uses an optimization loss function with three terms: a) adversarial loss to maximize the loss of the target NMT model; b) a similarity term to ensure that the adversarial example is similar to the original sentence; c) and loss of a language model to generate fluent and natural adversarial examples. The authors compare their TransFool method with two NMT attack methods, kNN and seq2sick, with respect to the Attack Success Rate, Relative decrease of translation quality, Semantic Similarity, Perplexity score and Token Error Rate. The proposed method can outperform baseline methods on two NMT model, Marian NMT and mBART50 MNMT model. The authors also show that the adversarial examples they found can conduct transfer attack on black box NMT models, using the adversarial examples from Marian NMT to attack mBART50 MNMT model and google translation api.",
            "strength_and_weaknesses": "Pros:\n+ This paper is good written and easy to follow.\n+ The authors valid their method not only on research models but also on real world commercial product, like google translation.\n+ The transferability analysis section is enlightening. It is not widely studied in NLP field. The transferability between different language is an important issue in multilingual NLP filed.\n\n\nCons:\n- Novelty is limited. \na) First, the authors consider not only the attack success rate but also the fluency and semantic similarity. However, these two issues are widely studied in NLP attack field. For example, [1] has systematically reveal that the fluency and semantic similarity issue in textual adversarial attack and they empirically study the threshold of filtering the adversarial examples according to the fluency and semantic similarity. \nb) Second, this paper claims that they propose a new strategy to incorporate the embedding vectors of a language model. However, utilizing language model to attack NLP model is not a novel technique. For example, [2-3] utilize BERT to conduct word-level substitution.\n\n- Lack of soundness. This is not only the weakness of this paper but also the weakness of a large portion of textual adversarial attack works.\na) The soundness of their method is weak. This paper use language model due to it provide a meaningful representation of the tokens. However, based on the finding of previous work [1], the semantic similarity cannot be guaranteed with language model. A good example is that \u201cI [like] eating apple.\u201d and \u201cI [hate] eating apple\u201d have very high representation similarity according to the language model representation. But their semantic are totally opposite. So I am worried that the adversarial attack will change the semantic meaning of source sentence, leading to the over-estimation of attack success rate.\nb) The soundness of their evaluation is weak. The authors evaluate the semantic similarity by the universal sentence encoder and BERTScore. However, as I mentioned above, the soundness of such model-based automatic metrics is limited. The high attack success rate could be partially due to the changing of meaning in source sentence. Conducting a human annotation experiment will be better than just showing an example.\n\n[1] John Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, Yanjun Qi ,Reevaluating Adversarial Examples in Natural Language, EMNLP 2020.\n[2] Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu, BERT-ATTACK: Adversarial Attack Against BERT Using BERT, EMNLP 2020.\n[3] Siddhant Garg, Goutham Ramakrishnan, BAE: BERT-based Adversarial Examples for Text Classification, EMNLP 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is well-written and easy to follow.\nQuality: The soundness of their method and evaluation is limited. Please refer to the weakness part.\nNovelty: This paper is not very novel. Please refer to the weakness part.\nReproducibility: The reproducibility will not be a issue. The authors claims that their code will be released. And they provide the link of the metric and scripts used in their paper.\n",
            "summary_of_the_review": "This paper proposes an adversarial attack method against neural machine translation model. The authors conduct experiments to show the superiority of their method. However, the novelty and the soundness of this paper is limited. So I tend to reject this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6290/Reviewer_8Cx6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6290/Reviewer_8Cx6"
        ]
    },
    {
        "id": "Bk9krBqi4fR",
        "original": null,
        "number": 2,
        "cdate": 1666607074302,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607074302,
        "tmdate": 1671176382577,
        "tddate": null,
        "forum": "P63GxgD7LIl",
        "replyto": "P63GxgD7LIl",
        "invitation": "ICLR.cc/2023/Conference/Paper6290/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes TransFool for generating non-targeted adversarial examples against neural machine translation models. One core idea is to utilize an autoregressive language model (GPT-2) to add a language model loss term, which helps generate fluent adversarial examples. They also add a similarity loss, which constrains the distances of embedding distances between the original input sentences and adversarial examples. They perform a comprehensive study to evaluate their approach on translating from English to different target languages, and show that TransFool achieves higher attack success rates compared to baselines, while the generated adversarial examples better preserve the semantic meaning and look more natural. Meanwhile, they show that the generated adversarial examples transfer to other models in the black-box setting, including Google Translate, and can also transfer to different target languages.",
            "strength_and_weaknesses": "Strengths:\n1. Each term in the loss function of TransFool is well-motivated and properly designed.\n\n2. The empirical study is pretty thorough. To me, the most interesting finding is that the generated adversarial examples can transfer to different target languages. \n\nWeaknesses:\n\n1. The study of defense is lacking. For example, it is helpful to see how the attack works with existing defenses against adversarial examples for language models. Also, it is interesting to try adversarial training with TransFool adversarial examples.\n\n2. This is more of a question rather than a weakness, but have you tried TransFool for targeted attacks, and how does that work?\n\n3. It is good to investigate more into the transferability between different target languages. For example, have you done these experiments on Google Translate?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear, and the approach is well-designed.\n\nTransFool is novel as an adversarial attack algorithm for machine translation. The individual terms are not that new as there are prior works leveraging language models to improve the fluency of adversarial examples in the NLP domain, but this work provides a more comprehensive study for machine translation problems. \n\nThe authors plan to release their code for reproducibility.",
            "summary_of_the_review": "This work proposes a well-designed adversarial attack for neural machine translation models, though it is not entirely novel. The paper presents a comprehensive study in different attack settings. In particular, the transferability of attacks between different target languages is interesting and new. Therefore, I lean towards accepting this work.\n\n-------------\nI thank the authors for explanation and adding more experiments, and I keep my original score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6290/Reviewer_ZrSf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6290/Reviewer_ZrSf"
        ]
    },
    {
        "id": "NwvnPv0fRsx",
        "original": null,
        "number": 3,
        "cdate": 1666614269024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614269024,
        "tmdate": 1666614269024,
        "tddate": null,
        "forum": "P63GxgD7LIl",
        "replyto": "P63GxgD7LIl",
        "invitation": "ICLR.cc/2023/Conference/Paper6290/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new approach for adversarial attacks on NMT models.\nThe authors design a way to propagate the gradient for embeddings and propose a specific loss that targets reasonable criteria.\nThe propagation is based on the common idea of relaxation with a new additional FC part to create embeddings for comparison.\nThe metrics are good, and overall, the results are promising.",
            "strength_and_weaknesses": "Strengths:\n- Quality metrics are better than that of competitors.\n- Experiments are interesting and numerous. They include human evaluation as well.\n- Nice adversarial example found in Table 2\n\nWeakness, methods:\n- The novelty is limited. The paper contains some engineering tricks to make everything work, but all ideas are similar to those previously discussed in the literature.\n- Not clear why we need the FC part in the approach. We can use BERT-like architectures to generate similar differentiable $v_i$-s as well. What would be the difference in this case?\n- Investigation on the distortion of the generated adversarial sequence $x'$ is limited and includes only \"Semantic Similarity\" from Yang et al. Can you include metrics like BERT score for a pair of $x$ and $x'$ as well?\n",
            "clarity,_quality,_novelty_and_reproducibility": "- New loss is proposed that consists of three terms. The ideas for these are out there for some time.\n- New architecture is proposed to find adversarial sequences. It is an interesting modification of a common one.\n\nI also imagine that the observed behaviour can be related to the requirement to include a projection layer in self-supervised learning or in other settings.",
            "summary_of_the_review": "The results seem promising, on the other side, the novelty is limited. From the paper, it is not clear, why this approach works in the NMT-attacks field, where other approaches failed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6290/Reviewer_Jg82"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6290/Reviewer_Jg82"
        ]
    },
    {
        "id": "SB5O_LGPxO",
        "original": null,
        "number": 4,
        "cdate": 1666726972902,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666726972902,
        "tmdate": 1666726972902,
        "tddate": null,
        "forum": "P63GxgD7LIl",
        "replyto": "P63GxgD7LIl",
        "invitation": "ICLR.cc/2023/Conference/Paper6290/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper defines a new optimization objective function which combines the fluency, similarity and translation error to adversarially attack machine translation models. A gradient projection algorithm is applied to solve this optimization. Experiment results show the proposed method outperforms baselines. The transferability is also examined. \n",
            "strength_and_weaknesses": "Strength\n\n- The proposed method is simple and straightforward. \n- The experiment results show superior performance in attack success rate and significant decrease in translation quality. \n- This work also demonstrates the capability and efficiency of blackbox attack.\n\n\nweaknesses\n\n- Existing adversarial attack on NMT aims at improving the robustness of these models. However, in this work, achieving a high attack success rate seems to be the goal. Therefore, I\u2019m wondering if TransFool is as effective as baselines in improving robustness. Or what is the desired use case for this method?\n- Translation models use beam search or similar mechanisms to generate high-quality output. Is the proposed attack still effective when using these mechanisms?\n- Missing human validation. Although automatic metrics show significant decrease in translation quality, I\u2019m not convinced that the algorithm triggers incorrect translation. Maybe the translation is correct but has a very low BLEU or chrF score (i.e., the attack method is attacking the automatic metrics instead of the NMT model, which could also be an interesting finding). \n",
            "clarity,_quality,_novelty_and_reproducibility": "The space after figures and tables are being squeezed too much. \n",
            "summary_of_the_review": "The novelty of the proposed method is somewhat incremental. \n\nThe experiment part is strong in terms of datasets and models. The automatic evaluation metrics also show strong performance. \n\nI think adding a human evaluation would reveal lots of insights. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6290/Reviewer_6HVS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6290/Reviewer_6HVS"
        ]
    }
]