[
    {
        "id": "6T3slFXYecs",
        "original": null,
        "number": 1,
        "cdate": 1666260920034,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666260920034,
        "tmdate": 1667817083842,
        "tddate": null,
        "forum": "yYbhKqdi7Hz",
        "replyto": "yYbhKqdi7Hz",
        "invitation": "ICLR.cc/2023/Conference/Paper2055/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper has three main contributions.\n\nFirstly, under quasar convexity (an assumption weaker than convexity that still guarantees local minima are global minima) and smoothness, authors develop an optimization algorithm using the continuized framework of Even et al 2021 and obtain optimal rates with high probability, in contrast with Hinder et al. 2020 that have an extra log factor (although their convergence was deterministic).  They note that the convexity inequality in the analysis of Even et al is only used once and can be easily replaced by the quasar convexity inequality which leads to their result. \n\nThe second contribution is extending the GLMtron paper (Kakade et al) to use this framework as well for Generalized Linear Models. Some empirical comparisons are provided for algorithms in both contributions.\n\nThirdly, contribution is showing that certain classes of functions that had been used before satisfy quasar convexity, expanding the set of applications of this framework.",
            "strength_and_weaknesses": "This is a nice paper. Realizing that the continuized framework of Even et al. does not require the progress-regret balancing step that leads to needing the binary search of Hinder et al. is a nice result. I would phrase the \"our contributions include\" and would not say that you \"improve\" over Hinder et al., since this result is in high probability (or with this E[T_k^2 * gap] result) and their result is deterministic\n\nI found really interesting that with this framework one can show that for the class of functions in Gille-Escuret et al. 2022, for which gradient descent is optimal, if you substitute the gradient Lipschitzness condition by smoothness then you can accelerate.\n\nOn the other hand, there is a small error/typo in your interpretation. You say that \"$L$-smoothness implies the $L$-Lipschitness of the gradients\". But it is the other way around. $L$-Lipschitness of the gradients implies smoothness. \nEDIT: I think I got confused and concluded that despite of this fact, what you said made sense to me. But now that I read it again it does not make sense. Since $L$-Lipschitness of the gradients implies smoothness, the class of smooth functions is larger. So it seems to me you are not getting any improvement under any additional assumptions. You are just showing \"better rates\" but because it is a different optimization metric (i.e., additive error in function value instead of multiplicative error in iterates, and also your result only holds with high probability). I would appreciate a clarification. \n\nIt would be better if you had error bars in the plots of your experiments.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, and I already commented on the quality. Of course, most of the analyses in the proofs are borrowed from Even et al. and Kakade et al. 2011 since the algorithms are modifications of those, but this is fine.\n\nSome typos:\n\nSection 3.1.3 Guassian\n\npage 7 \"c.f.\" -> \"cf.\"\n\npage 8 \"and that the derivative\" -> \"and when the derivative\" \n\npage 8 cae -> can",
            "summary_of_the_review": "The paper contains a nice idea that was well developed to show algorithms and their convergence in some quasar-convex frameworks and also adds new examples of quasar convex functions to the literature, which is another nice contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2055/Reviewer_ZBaY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2055/Reviewer_ZBaY"
        ]
    },
    {
        "id": "kD7saERU9H",
        "original": null,
        "number": 2,
        "cdate": 1666699204609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699204609,
        "tmdate": 1666700555445,
        "tddate": null,
        "forum": "yYbhKqdi7Hz",
        "replyto": "yYbhKqdi7Hz",
        "invitation": "ICLR.cc/2023/Conference/Paper2055/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studied minimizing quasar convex functions. Authors studied the relationship between quasar convexity and many common structure assumptions in nonconvex optimization, and verify that GLM satisfies quasar convexity under mild conditions. Then they proposed a new algorithm based on the continuized discretization technique in literature, the continuized Nesterov acceleration improves over existing algorithms and achieves the optimal complexity.",
            "strength_and_weaknesses": "Strength:\n1. Connect quasar convexity with many existing structural nonconvex assumptions, and real application GLM\n2. Provide a complexity result which is claimed to outperform existing works.\n\nWeakness:\n1. The comparison with existing works is a little confused to me.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is well-written and easy to follow. The paper considers both theory and real applications, the main technique, continuized discretization, is relative new in literature, which reveals novelty.",
            "summary_of_the_review": "Generally I am satisfied with this work. My main concern lies in that:\n\n1. How do you choose $T_k$ in Theorem 1, you mentioned that $T_k$ are random times. Are you referring it as a \"stochastic stepsize\", or you will predefine the $T_k$ (or the total iteration number as in many classical optimization literature) before running the algorithm?\n2. When comparing to Hinder's work, you mentioned they need an extra log complexity in the function evaluations. First I feel that such logarithmic difference is a bit incremental; second for me such binary search subroutine seems to be more toward a practical preference, if we just follow their \"General AGD Framework\" (Algorithm 1 therein), then the results in your work and Hinder's work should be basically the same.\n\nThank you for the effort.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2055/Reviewer_deQT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2055/Reviewer_deQT"
        ]
    },
    {
        "id": "KcHkY0el_4",
        "original": null,
        "number": 3,
        "cdate": 1667033713359,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667033713359,
        "tmdate": 1667035366360,
        "tddate": null,
        "forum": "yYbhKqdi7Hz",
        "replyto": "yYbhKqdi7Hz",
        "invitation": "ICLR.cc/2023/Conference/Paper2055/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a randomized acceleration method based on the continuized scheme proposed in [1]. The method targets the nonconvex optimization problems  with quasar convex structure, and obtains the optimal complexity rate in expectation. The authors also illustrate that for a class of generalized linear models the quasar property holds and provides theoretical analysis to estimate the quasar convexity constants.\n\n\n\n\n[1] Even, Mathieu, et al. \"A continuized view on Nesterov acceleration for stochastic gradient descent and randomized gossip.\" arXiv preprint arXiv:2106.07644 (2021).",
            "strength_and_weaknesses": "Strength:\nThis paper not only improves the complexity of first-order methods for quasar convex optimization, but also identifies a large class of problems satisfying quasar convexity. It shows the great potential of developing algorithms in continuous time and I believe it will have growing impact to the community\n\n\nWeakness\n\nI am a bit curious whether quasar convexity is useful beyond the setting discussed in this paper. \nGLM is a motivating application for the proposed continuized method. However, it looks like the quasar convexity assumption relies on the fact that there is a perfect $w_*$ that interpolate all the data (i.e. the overparameterized setting), this seems to suggest some limitation of in real applications. \n\nIn example 3, the relu link function is not differentiable at z=0. Can you still apply the gradient based algorithm for such setting?\n\nTypo\n\nPage 4\n\nOn the last line: there exists a $w_*\\in\\mathbb{R}^d$\n\n\nNotations are inconsistent: Both $\\|\\cdot\\|$ and $\\|\\cdot\\|_2$ stands for Euclidean norm; they should be unified.",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity:\nThe paper is well-written and is easy to follow, though I did not fully go through the technical proof. \nThe paper also uses many examples to illustrate the application of quasar convexity and the potential impact of the proposed method.\n\n\n\nNovelty\nThe result is new but the technique seems to be standard as it is based on putting the continuized technique and complexity analysis of quasar-convex optimization together, both of which have been well-established in the prior work. \n\n",
            "summary_of_the_review": "The paper is of good quality, it provided a unified and justified quasar convex framework for many important nonconvex functions, and developed a novel accelerated method with improved complexity rate. While there is some limitation in the application, the theoretical contribution of this paper is sufficient, which has great potential to the development of nonconvex optimization. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2055/Reviewer_YMJ7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2055/Reviewer_YMJ7"
        ]
    },
    {
        "id": "AL_sPbnSMzv",
        "original": null,
        "number": 4,
        "cdate": 1667183238207,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667183238207,
        "tmdate": 1667183238207,
        "tddate": null,
        "forum": "yYbhKqdi7Hz",
        "replyto": "yYbhKqdi7Hz",
        "invitation": "ICLR.cc/2023/Conference/Paper2055/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper found a simple (but effective) variant of the continuized Nesterov acceleration (Even et al., 2021) that achieves the optimal complexity (with high probability) under the quasar convex condition for the first time. (Quasar convexity is a non-convex condition that some gradient methods are known to find a global solution.) Previous results, built upon the standard (discrete) Nesterov acceleration, required multiple gradient evaluations at each iteration, and thus only had the near-optimal complexity. This paper then provides several quasar convex examples, such as generalized linear models (GLM), to support the importance of studying the quasar convexity.",
            "strength_and_weaknesses": "- This is the first first-order method that has the optimal complexity for quasar convex minimization.\n- Various examples satisfying the (strong) quasar convex condition are provided.\n---\n- One might view this as a simple modification of the continuized Nesterov acceleration.\n- An intuition behind the success of the proposed randomized method over the existing method for quasar convex minimization is not clearly given (other than the final proof), and this could be of interest to some readers.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written, and its contribution is clearly novel and interesting.",
            "summary_of_the_review": "Finding an optimal first-order method for the (strong) quasar convex minimization has been an open question in the optimization community, and this paper has resolved it. Momentum has been the main workhorse for training \"non-convex\" machine learning models, and this work provides a nice contribution on expanding our knowledge on accelerated non-convex minimization.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2055/Reviewer_8aev"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2055/Reviewer_8aev"
        ]
    }
]