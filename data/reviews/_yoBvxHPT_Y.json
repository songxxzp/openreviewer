[
    {
        "id": "5xQ7FyUsUq",
        "original": null,
        "number": 1,
        "cdate": 1666590304096,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590304096,
        "tmdate": 1666590382152,
        "tddate": null,
        "forum": "_yoBvxHPT_Y",
        "replyto": "_yoBvxHPT_Y",
        "invitation": "ICLR.cc/2023/Conference/Paper1294/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors try to unravel why domain generalization is much more difficult in the adversarial scenario. Since the generalization bound is with respect to the adversarial Rademacher complexity term, they propose to analyze this key measure and show that adversarial Rademacher complexity is always greater than the non-adversarial one. They also try to explain the impact of adversarial training over generalization by a bound with respect to clean target error and adversarial source error. The experiments partly validate the theoretical results.",
            "strength_and_weaknesses": "## Pros:\n1. The writing is clear and easy to follow. The paper is well-organized.\n2. The experiments partly validate the theoretical results.\n## Cons: \nI have two main concerns about the paper:\n1. About the definition of adversarially robust risk. In this paper, $$\\mathcal{R} _\\mathcal{D}^{adv}(h _w,h _{w'})=\\mathbb{E} _{x\\sim \\mathcal{D}}[max _\\delta \\ell(h _w(x+\\delta),h _{w'}(x+\\delta))].$$ However, the classical definition [A][B] of adversarial error is $$\\mathcal{R} \n_\\mathcal{D}^{adv}(h _w,h _{w'})=\\mathbb{E} _{x\\sim \\mathcal{D}}[max _\\delta \\ell(h _w(x+\\delta),h _{w'}(x))].$$\nThe definition of adversarial error is different with the classical one. Could the authors give some explainations about this? Furthermore, it seems that the authors use the classical definition to train their network in the experiments (see eq. (13)). Does it contradict with the theory?\n\n2. About the reason why adversarial training gets better domain generalization than standard training. The authors explain this by an error bound which shows that the target clean error can be bounded by the source adversarial error. In my opinion, the bound can indeed explain that adversarial training in source domain can robustify the model on target domain. But, **there is no direct evidence** that adversarial training entails better standard accuracy on target domain, **compared to the normally trained model.**\n\nSome typos in the paper:\n1. In Lemma 2 and Corollary 1, the hypothesis is denoted by $w$ and $h_w$, are these two notations denote the same hypothesis?\n\n[A]: Theoretically Principled Trade-off between Robustness and Accuracy. ICML 2019.\n\n[B]: Towards Deep Learning Models Resistant to Adversarial Attacks. ICLR 2018.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The motivation and writing are clear.\n\nQuality and novelty : The conclusion of this paper is somewhat intuitive. In fact, there are many related works propose the similar relationship between Rademacher complexity in standard scenario and in adversarial scenario. The authors just replace the hypothesis class here. I do think the work of this paper is meaningful, but not novel enough.",
            "summary_of_the_review": "I believe the paper is well written and I appreciated the theoretical bounds about generalization error. However there are some parts of the paper that are confusing. Therefore, I believe the paper could be significantly improved by clarifying the points above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1294/Reviewer_K4Lj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1294/Reviewer_K4Lj"
        ]
    },
    {
        "id": "BLBxx66vz9",
        "original": null,
        "number": 2,
        "cdate": 1666665366599,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665366599,
        "tmdate": 1670526562130,
        "tddate": null,
        "forum": "_yoBvxHPT_Y",
        "replyto": "_yoBvxHPT_Y",
        "invitation": "ICLR.cc/2023/Conference/Paper1294/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper attempts to provide generalization bounds for adversarially robust domain adaptation. \n\nThe seminal work of Ben-David (2010) established the H-Delta-H divergence as a complexity measure for this task in the binary classification setting. Mansour (2010) established the discrepancy based distance as a complexity measure for domain adaption for multi-class classification with general loss functions. The discrepancy-based distance is further estimated from finite samples via empirical Rademacher complexity of the disagreement hypothesis class H-Delta-H.\n\nThis paper traces the following abstract template of results similar to Yin et. al. (2018) and Awasthi. et. al. (2019) but for the task of robust domain adaptation. \n- Define an adversarial Rademacher complexity via the adversarial loss function. (Here in addition the hypothesis class is H-Delta-H and the adversarial discrepancy is defined). \n- Show that adversarial Rademacher complexity is bounded by standard-Rademacher complexity + an expression that depends on the norm of the predictors and size of corruption. \n- Show that adversarial Rademacher complexity can be lower bounded by the standard complexity with a dimensional cost. \n- Instantiate the adversarial Rademacher complexity upper bound for linear predictors and 1-hidden layer neural networks.\n\nIn addition to the above, the authors show that adversarial target risk can be linked to the adversarial source risk via the adversarial Rademacher complexity and adversarial discrepancy distance. This is in the same spirit as the result from Mansour (2009). \n\nCertainly establishing the above points require sufficient effort and the authors appear to have done so rigorously. Finally, the authors establish a link between standard target risk and adversarial source risk - a result that can potentially explain why adversarially trained classifiers have better standard domain adaptation. \n",
            "strength_and_weaknesses": "Strength\n- Extension of the same flavor of results from standard generalization -> robust generalization -> robust domain adaptation. \n- Dimension-dependent lower bounds for uniform convergence in robust domain adaptation task. \n- Link between standard target risk and adversarial source risk. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively clear and of sufficient quality. The exact story of Yin. et. al. (2018) dampens the novelty of this work however I concede that the proofs certainly require extra consideration. \n",
            "summary_of_the_review": "I recommend accept as I believe this work clears the bar of sound technical contribution. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1294/Reviewer_Du9x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1294/Reviewer_Du9x"
        ]
    },
    {
        "id": "r9906dmIp8W",
        "original": null,
        "number": 3,
        "cdate": 1666692076528,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692076528,
        "tmdate": 1670801579926,
        "tddate": null,
        "forum": "_yoBvxHPT_Y",
        "replyto": "_yoBvxHPT_Y",
        "invitation": "ICLR.cc/2023/Conference/Paper1294/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies robust learning and domain adaptation from a learning theory perspective. It does so notably by looking at the adversarial Rademacher complexity (RC) of the robust loss function between two classes. Various results are presented: adversarially robust domain adaptation (Lemma 2), an upper bound on the RC of binary classification for linear classifiers (Lemma 3), upper and lower bounds on the adversarial RC of binary classification for linear classifiers (Theorem 1), as well as similar results for regression tasks (linear classifiers) and neural networks with ReLU activations. They finally related the robust and standard risks for linear classifiers. The authors finish with an empirical study.",
            "strength_and_weaknesses": "Note: Work cited in parentheses refers to the authors\u2019 bibliography. Work cited in the form [ABC12] is for this review and the bibliography appears at the end of the review.\n\n## Strong points\n\nThe work is timely and interesting, and brings new ideas to the field of robustness and domain adaptation. The paper is clear and well-written. Lots of results are presented.\n\n## Weak points/omissions/clarifications:\n\nI think there are issues in the definitions of the problem (in the definition of robustness), and the related work needs clarifications. \n\n**Robust Risk Definitions**\n\nAdversarial risks defined on p.2 and p.3:\nThe adversarial risk defined on p.2 is not the same as the adversarial risk defined on p.3. Indeed, in the former, the label of the perturbed point is compared to the label $y_{\\mathcal{D}}(x)$ of the *unperturbed* point, while in the latter, it is compared to the another hypothesis\u2019  labelling (potentially the ground truth) *also* on the perturbed point. The risk on p.2 has been defined in the literature as corrupted instantce/constant-in-the-ball robustness, and the second as error region/exact-in-the-ball. \n\nIt would be necessary to name the robust risk(s) used in this paper, as when the two definitions don\u2019t coincide, the guarantees obtained have a different meaning.  On one hand, the error region/exact-in-the-ball risk talks about the probability measure of the expansion of the error region; we ask that the target and hypothesis *agree* in the perturbation region. On the other hand, for the more commonly-used robust risk in the literature, which is the corrupted instance/constant-in-the-ball one, a correctly classified point should not change labels if it is perturbed. \n\nIn Lemma 2 for example, a key assumption is that the loss function is symmetric and obeys the triangle inequality. This is the case for the exact-in-the-ball/error region one, but not for the constant-in-the-ball, because the first labelling function is w.r.t. a perturbed point, while the second is w.r.t. an unperturbed point. I think this poses some issues with the triangle inequality too. \n\n(Q1:) In this paper, it seems that most results use the exact-in-the-ball while Lemma 5 and Corollary 1, and the adversarial training procedure in section 6 appear to use the constant-in-the-ball. I have skimmed through the proof of Lemma 5, and it does not seem to be using prior results in the paper \u2014 can you confirm that the proof is \u201cstand alone\u201d, and for this particular notion of robustness? Otherwise, there could potentially be a mistake in there. \n\n(Q2:) Moreover, with this definition of robustness, I think that the adversarial RC used in this result is the same one as in (Yin et al 2018). If so, could you comment on the difference between your results and theirs (Thm 2 in particular), as well as other follow-up work using the same notion of robustness? \n\nWork that has compared different notions of robustness, to cite/read for reference: [DMM18], [GKKW21] and [PJ21]. All papers also look at fundamental limitations of robust learning compared to standard learning.\n\n\n**Related Work**\n\n(Diochnos et al. 2019). The exponential dependence on the budget not totally right: in the Theorem 3.4 statement, we can see that the budget (= number of bits) is defined as $b = \\rho \\cdot n$, so the exponential dependence on $\\rho$ is actually the normalized budget. E.g., the bounds are superpolynomial only when the number of bits the adversary can flip at test time is $\\omega(\\sqrt{n\\log(n)})$. The work of [GKKW22] gives an actual exponential dependence on the number of bits ($=b$) the adversary is allowed to perturb (though for a more restricted setting), so a superpolynomial lower bound happens when the budget is $\\omega(\\log(n))$ instead. In both works the dependence on the budget is for the error region/exact-in-the-ball robustness and is for specific problems: assumptions on concept classes and distribution families are necessary to obtain such guarantees \u2014 closeness and Normal L\u00e9vy families for (Diochnos et al. 2019) and monotone conjunctions (and superclasses like linear classifiers and decision lists) under the uniform distribution for [GKKW22].\n\n(PAC Learning frameworks for robustness). The works of (Cullina et al. 2018) and (Montasser et al. 2019) use the corrupted instance/constant-in-the-ball robustness, while the work of (Diochnos et al. 2019) uses the error region/exact-in-the-ball one. This means that the guarantees obtained differ in meaning and are general incomparable (e.g. the perturbation budget doesn\u2019t explicitly appear in the work of (Montasser et al. 2019)). Also follow-up works [MHS21] look at implementing this algorithm with the use of a Perfect Adversary Oracle and [MHS22] gives a characterization of robust learnability through a minimax learner. The notion of robustness used by each work should be stated explicitly. \n\n(The $\\mathcal{H}\\Delta\\mathcal{H}$ class). (Cullina et al. 2018) also had a look at the robust loss of a hypothesis class, though for the constant-in-the-ball/corrupted instance notion of robustness and through the adversarial VC dimension, defined in their work. [APU20] also looks at the margin class (robust loss function) and relate its VC dimension to the sample complexity of robust learning for this notion of robustness. Recent work [GKKW22b] looks at the adversarial perturbation region of $\\mathcal{C}\\Delta\\mathcal{H}$ (i.e. the robust loss functions w.r.t. to exact-in-the-ball/error region robustness), where $\\mathcal{C}$ is the set of possible labelling functions, though through the lens of VC dimension as well, similarly to (Cullina et al. 2018) and [APU20]. Since the VC dimension and Rademacher complexity are related, there may be a link here. \n\n## Other Questions \n\n- How do the goals of robust learning and (standard and adversarial) domain adaptation differ? It would be a good idea to devote a section in the appendix to go into more details on this.\n- Could you comment on the methodology used to get RC upper bounds in Sections 3 and 4, and how they differ in approach of the works mentioned in Appendix A?\n- On p.8, you say that \u201cIf the adversarial budget $\\epsilon$ is larger, $\\Lambda$ will be larger, which means the quantity $V^*(p\u2019,p,\\ell,\\Lambda)$ will be smaller.\" Can you explicitly state this dependence?\n\n**Typos/Comments**\n\n- p.2, second paragraph: a few typos:\n\n\"That is, the gap between robust risks on old domain and new domain can be dramatically huge...\" -> \"That is, the gap between robust risks on the old and new domains can be dramatically huge...\"; \n\n\"This observation naturally leads to the question of, *why the robust risk is harder to adapt to different domains?*, which we aim to examine in this paper.\" -> \"This observation naturally leads to the question of why the robust risk is harder to adapt to different domains, which we aim to examine in this paper.\" or \"This observation naturally leads to the question *Why is the robust risk harder to adapt to different domains?*, which we aim to examine in this paper.\"; \n\n\"...we properly extend this complexity measure to adversarial learning setting, and propose the adversarial Radmeacher complexity over H\u2206H class.\" -> \"...we properly extend this complexity measure to *the* adversarial learning setting, and propose the adversarial Radmeacher complexity over *the* H\u2206H class.\"\n- p.3, paragraph above Defn 2: \u201csource domain $\\mathcal{S}$ and target domain $\\mathcal{T}$\u201d \u2014 is there a \u201cdistribution\u201d missing after this? Also, $\\hat{\\mathcal{T}}$ and $\\hat{\\mathcal{S}}$ should be switched. \n- p.3, paragraph after Defn 2: \u201cThe another advantage\u201d -> \u201cAnother advantage\u201d\n- p.3 paragraph above Defn 3: $d_{H\\Delta H}$ -> $disc_{H\\Delta H}$? \n- p.3, last sentence: \u201cto adversarial setting\u201d -> \u201cto the adversarial setting\u201d\n- p.4, paragraph after Defn 4: I think this is misleading: Lemma 19 is only derived for linear classifiers, while the sentence implies that the result is more general. \n- p.4, Lemma 2 statement: in the first term on the RHS of the inequality (adversarial RC w.r.t. $\\mathcal{S}$), should $\\mathbf{w}$ be $h_{\\mathbf{w}}$ instead?\n\n**References**\n\n[APU20] Ashtiani, H., Pathak, V., and Urner, R. (2020)  Black-box certification and learning under adversarial perturbations. In *International Conference on Machine Learning*.\n\n[DMM18] Diochnos, D., Mahloujifar, S., and Mahmoody, M. (2018). Adversarial risk and robustness: General definitions and implications for the uniform distribution. In Advances in Neural Information Processing Systems. \n\n[GKKW21] Gourdeau, P., Kanade, V., Kwiatkowska, M., and Worrell, J. (2021). On the hardness of robust classification. In *Journal of Machine Learning Research*, 22. \n\n[GKKW22] Gourdeau, P., Kanade, V., Kwiatkowska, M., and Worrell, J. (2022). Sample complexity bounds for robustly learning decision lists against evasion attacks. In *International Joint Conference in Artificial Intelligence*. \n\n[GKKW22b] Gourdeau, P., Kanade, V., Kwiatkowska, M., and Worrell, J. (2022). When are local queries useful for robust learning?. In *Advances in Neural Information Processing Systems*. \n\n[MHS21] Montasser, O., Hanneke, S. and Srebro, N. (2021). Adversarially Robust Learning with Unknown Perturbation Sets. In *Conference on Learning Theory*.\n\n[MHS22] Montasser, O., Hanneke, S. and Srebro, N. (2022). Adversarially Robust Learning: A Generic Minimax Optimal Learner and Characterization. In *Advances in Neural Information Processing Systems*. \n\n[PJ21] Pydi, M. S. and Jog, V. (2021). The many faces of adversarial risk. In *Advances in Neural Information Processing Systems*. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and the quality of the contribution is adequate. For reasons outline above, I am not sure about certain aspects of the novelty/originality of the submission (particularly Q2 and the methodology to get bounds in Sections 3 and 4). Other parts of the paper (e.g., bringing robust learning and domain adaptation together, the particular work on the class $\\mathcal{H}\\Delta\\mathcal{H}$ through Rademacher complexity) are novel and interesting. \n",
            "summary_of_the_review": "I think this is interesting work, with novel contributions. I believe there remains work to be done in how the paper situates itself in the literature, and that certain aspects of the contribution should be explained in more details. \n\nI gave a 5/10, because of the questions above. I am open to changing my score after the response from the authors, and I am looking forward to our discussion. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1294/Reviewer_RkC2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1294/Reviewer_RkC2"
        ]
    }
]