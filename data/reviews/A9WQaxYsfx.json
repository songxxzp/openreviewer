[
    {
        "id": "2sN41zHnPG",
        "original": null,
        "number": 1,
        "cdate": 1665832971462,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665832971462,
        "tmdate": 1669260356123,
        "tddate": null,
        "forum": "A9WQaxYsfx",
        "replyto": "A9WQaxYsfx",
        "invitation": "ICLR.cc/2023/Conference/Paper5380/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of obtaining private text data from federated transformer models. This paper studies the malicious server setting, where the server can arbitrarily tamper with the model sent to clients. This paper also studies a unique attack setting --- it focuses only on several privacy sensitive key words and the tokens following the key words, such as 'card number'. The proposed method elaborately designs transformer parameters such that the gradients directly reveal the token embeddings following the key words. Experiments are performed on real-world transformer models, where the proposed method outperforms a state-of-the-art baseline, Decepticon (Fowl et al. 2022). ",
            "strength_and_weaknesses": "Strength: \n- The problem setting studied in this paper is unique and practical. This paper studies malicious servers instead of honest-but-curious servers studied in existing works. This makes sense as one can never assume that an adversary is 'honest'. Also, the paper studies how to extract tokens following specific key words. This is also practical, since most sentences typed by users are not highly related to privacy. \n\nWeaknesses: \n- This paper does not seem self-contained and hard to understand without sufficient background. This paper heavily refers to related works (especially (Fowl. et al. 2022)) in the designs and equations. However, the intuition/implications of them are not sufficiently introduced. I list several examples here. (It is suggested that the authors number the equations). \n    - Eqn. 1 is derived from Phong et al. (2017). However, although I knew the related work and understood it, it is not immediate for those who are unfamiliar with the related work. Also, it seems that $\\frac{\\partial b}{\\partial L}$ should be $\\frac{\\partial L}{\\partial b}$. \n    - The equation following Eqn. 1 (Page 4) from Fowl et al. 2022 is also not immediate and hard to understand. Also, it seems that the equation only works for one layer of transformer block (otherwise, the input should not be token embeddings $e$ but output features from the previous layer). \n    - The first equation on Page 6 from Fowl et al. 2022 is hard to understand without interpretation. The authors should interpret what the equation intuitively means. (e.g. organizing $w^Tx_j$ into bins). \n    - The equation above Eqn. 5 from Fowl et al. 2022 is also hard to understand. I know it is more or less similar to Eqn. 2, but as they serve different purposes, it is still beneficial to explain it.   \n    - Thus, the paper does not seem self-contained. This paper is hard to understand without sufficient knowledge on (Fowl et al. 2022). \n\n- It is not sure that the proposed method can be used in practice. I list several doubts on the applicability. \n    - First, it is not sure that the proposed method is compatible with model training. If the local model is training smoothly (local loss decreasing) but the global model is not training (e.g. loss significantly increases after server aggregation), clients will not have the motivation to participate. \n    - Second, the proposed method leads to non-negligible and weird parameter distribution. For example, a client can simply compute the norm of token embeddings and decide that the server model is weird and potentially malicious. By comparison, backdoor attacks on FL requires that the model outputs normally on normal data (Bhagoji et al. 2019, Bagdasaryan et al. 2019). \n    - Third, for the motivating example of credit card numbers, the sensitive information (i.e. the card number) is a string of numbers which is almost certainly out-of-vocabulary. Therefore, the card number may not even have a token embedding, making the retrieval impossible. \n    - Finally, the experiments use FedSGD (i.e. communication every iteration) as the attacked method. However, FedSGD is rarely used in practice for its high communication cost. In practice, FedAvg, which averages the gradients for multiple local steps (e.g. ~100) is used. It is not sure how this method works with FedAvg. Furthermore, if secure aggregation is used, the server only obtains aggregated gradients. Then, it is not clear how the retrieved information can be traced back to specific users. \n    - Of course, as I am not very familiar with (Fowl et al. 2022), a central related work of this paper, I may make misunderstandings. Please correct me if there are any. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The clarity of this paper is poor. It is hard to understand for readers without sufficient knowledge of (Fowl et al. 2022). \n\nNovelty: The paper is novel. It identifies a practical and novel attack setting, i.e. malicious server and tokens following specific keywords. \n\nQuality: It is not very clear how the proposed method can be put in practice. Thus, I am doubtful about the quality of this paper. \n\nOf course, as I am not very familiar with (Fowl et al. 2022), a central related work of this paper, I may make misunderstandings. Please correct me if there are any. ",
            "summary_of_the_review": "From my perspective, the main problems of this paper are i) Not sufficiently self-contained; 2) Hard to be deployed in practice. Therefore, although this paper studies a practical and unique problem setting, I cannot recommend acceptance at this stage. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_q4yB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_q4yB"
        ]
    },
    {
        "id": "gLMBdaDVpz0",
        "original": null,
        "number": 2,
        "cdate": 1667202815083,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667202815083,
        "tmdate": 1667202815083,
        "tddate": null,
        "forum": "A9WQaxYsfx",
        "replyto": "A9WQaxYsfx",
        "invitation": "ICLR.cc/2023/Conference/Paper5380/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the potential privacy leakage in federated learning settings. The paper argues that previous attacks are too ambitious to extract as much data as possible and this is at the expense of fidelity, so that previous attacks do not work under large-scale settings. The paper proposes an attack on FL which extracts input text that contains targeted privacy-critical words in language modeling training. In the proposed method, the parameters of the transformer are carefully programmed to filter relevant sequences from user data and encode the information in the gradients. The conducted experiments show that the attack works well and outperforms previous approaches in different settings; they show that the attack works even when the batch size is as large as 2000.\n",
            "strength_and_weaknesses": "Strengths:\n- The paper is well-motivated. I agree with the authors\u2019 arguments on the existing approaches and also believe that focusing on only privacy-sensitive data is a practical setting for attackers.\n- The attacking setting (e.g., malicious server scenario), background, and related work has been discussed well. Generally, I like the positioning of this paper.\n- Although the method is based on a couple of existing works, it is relatively novel. \n- The paper conducts extensive experiments to evaluate the attacks with different setups. The results really show the vulnerabilities of federated language model training in a practical large-scale setting.\n\nWeaknesses:\n- The paper can be presented better. The method is complicated and section 3.3 is not easy to follow. Since the approach is built based on existing approaches, some details are not included in this paper, which may confuse readers.\n- The paper can be written better by discussing possible defense methods and it will also benefit the community and facilitate more research in this direction.\n- The paper lacks qualitative attacking examples reconstructed by the approach. It is hard to have a sense about how strong the attack is by only looking at those metrics.\n- What is the \u201ctotal accuracy\u201d metric exactly?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper can be presented better.\n\nQuality: the paper is somehow solid.\n\nNovelty: the paper is technically novel.\n\nReproducibility: the paper provides some experiment details in the appendix. In general, the method should not be very hard to reproduce.\n",
            "summary_of_the_review": "In general, I am positive towards accepting the paper. However, I also believe the paper can be better if there is another round of review.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_dBzB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_dBzB"
        ]
    },
    {
        "id": "APxVqNhVB5L",
        "original": null,
        "number": 3,
        "cdate": 1667257572290,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667257572290,
        "tmdate": 1667257572290,
        "tddate": null,
        "forum": "A9WQaxYsfx",
        "replyto": "A9WQaxYsfx",
        "invitation": "ICLR.cc/2023/Conference/Paper5380/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel privacy attack against FL systems, where the proposed attack extracts text sequences that contain targeted, privacy critical phrases. The targeted attack mechanism is achieved by maliciously modifying the model parameters on the server. In particular, the attack consists of three steps: tagging target sequences, filtering out irrelevant tokens and post-processing. According to the experimental results, the proposed method is more powerful than the selected Deception baseline.",
            "strength_and_weaknesses": "#### **Strengths**:\n- The targeted attack setting is novel, well-motivated and interesting\n\n#### **Weaknesses**:\n- The designed three steps of the proposed attack match the intuition to perform targeted attack, but it seems to me that the proposed techniques are largely inspired by or directly adopted from Fowl et al. (2022), which makes the technical contribution of this work incremental. \n- Based on my understanding, this method requires the model gradients returned by the users. Is this the reason that the authors only performed experiments on FedSGD? What if the gradients are not available, how could the proposed attack work (for instance on FedAVG)? This raises my concern on the generality of the proposed method. Maybe the author could provide some discussions and insights on this point as in Fowl et al. (2022) and Fowl et al. (2021).\n- In my opinion, the experimental setup is not very sufficient. Firstly, as mentioned above, only FedSGD is studied. Secondly, the selected evaluation metrics seem to be too general and could not really reveal whether the attack is indeed \u2018targeted\u2019. In the worst-case, the performance gain could be simply attributed to the additional attention weights added instead of the actually proposed method. It would be more convincing if the author can show that the proposed attack could recover the sequences with **targeted** key words.\n- Finally, this might be not so important. But I am curious about whether the proposed attack could surpass very simple defenses? For instance, the users could just add some noises to the gradients when returning the gradients to the server. Will this break this attack? Not to mention more advanced defenses discussed in Fowl et al. (2022).\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Novelty: This work presents a novel idea of targeted privacy attack against FL systems. This targeted setting is novel and well-motivated.\n- Clarity: This work is well organized. The concepts are either clearly explained or highlighted with proper references.\n- Quality: This work has some minor issues. Such as my concerns about the generality of the method as well as the experimental design on FedSGD.\n- Reproducibility: I didn\u2019t find the code of this paper. But I still trust the authors regarding their results.\n",
            "summary_of_the_review": "Overall, I think this work is interesting and I rated this work as 6. I do have some concerns and I am not an 100% expert on attacking FL systems, but I am willing to increase my score if the authors could address my concerns above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_7zTS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_7zTS"
        ]
    },
    {
        "id": "XfdM_uF-u2C",
        "original": null,
        "number": 4,
        "cdate": 1667358243812,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667358243812,
        "tmdate": 1667358243812,
        "tddate": null,
        "forum": "A9WQaxYsfx",
        "replyto": "A9WQaxYsfx",
        "invitation": "ICLR.cc/2023/Conference/Paper5380/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is about attacking the privacy of users when they do federated learning in specific settings. The setting is for training (large) language models that specifically use the \u201ctransformer\u201d architecture. Moreover, the attack is done in the \u201cmalicious\u201d setting, in which the central server who is running the federated learning does *not* behave honestly (& curiously) but it rather sends fabricated models to the users with the goal of extracting personal data from them. The paper is focused on extraction of *targeted* information; e.g., a credit card number as the string that follows the text \u201ccredit card\u201d.\n\nThe paper compares its findings with recent works that also performs data extraction attacks on federated learning for the same architecture. Since the paper\u2019s focus is on extracting targeted sentences it will do better in doing the exact goal (of this paper) because the other papers aim to extract texts rather indiscriminately. ",
            "strength_and_weaknesses": "Strength:\nI like the aspect of \u201ctargeted\u201d extraction of the texts. Here the attacker plants specific traps to extract specific texts from the model. This is a plus for this paper\u2019s attack.\n\nWeakness:\nThe fact that an attack is done in a malicious setting is a downside. This means that the users can *potentially notice* that the server is *not* following the protocol honestly. The paper has (as far as I can see) zero discussions on this important aspect, and I would like to hear from the authors about this, if possible: Can you argue that the (malicious) changes to the model are indistinguishable for the users?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality is not great. The paper assumes a lot of familiarity with the transformer architecture and explains things bottom up. Indeed, the reader is facing a lot of technical terms and details without being given the big picture that is crucial.",
            "summary_of_the_review": "In summary, I find the targeted aspect of the attacks of this paper interesting and I think this will lead to more papers like this. On the other hand, the paper suffers from a presentation that is not suitable for general audience and does not investigate the limitations of their attacks; e.g., against users who inspect the models sent to them. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_8KC8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_8KC8"
        ]
    },
    {
        "id": "sOQQ-NjaMw",
        "original": null,
        "number": 5,
        "cdate": 1667371635642,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667371635642,
        "tmdate": 1667372953851,
        "tddate": null,
        "forum": "A9WQaxYsfx",
        "replyto": "A9WQaxYsfx",
        "invitation": "ICLR.cc/2023/Conference/Paper5380/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new attack on federated learning using text transformers, where the assumption is that the server is not honest-but-curious, but instead malicious and cannot be trusted. Current existing attacks in this scenario try to recover all input tokens of all users. The paper argues that this is not necessary, as an attacker is only interested in specific private information, which in reality is most likely only a small part of the whole sequence. To only reconstruct sequences containing sensitive data, the paper modifies the multi-head attention blocks as well as the linear layers of the transformer and filter for sequences containing specific keywords (e.g., like \"credit card\" or \"social security number\"). ",
            "strength_and_weaknesses": "Strengths:\n- The idea of the paper to only reconstruct parts of sequences that are relevant for an attacker is novel.\n- This is an important area of research, as the current real-world implementations are based on trustworthiness of the server. Showing that such an attack is possible is showing that changing the implementations and protocols might be necessary to ensure privacy.\n- The paper is well written and most of the time easy to follow.\n\nWeaknesses:\n- It should be clearer how many heads are needed in total for the attack, as this might be a major limitation. Sudden drastic changes in the architecture when sending the modified model to the users might allow the users to detect this kind of attack.\n- It is a bit hard to understand the reasoning behind some of the actions for the attack. For example, what is the reasoning behind shifting the entries to the first d' positions? Without deep knowledge of the related work this paper is building on, it is hard to follow the mathematical reasoning. A short sentence to explain the intuition would be nice and benefit the reader a lot.\n- Experimental evaluation is limited. The experiments were conducted only on 2 models Fowl et al. (2022) is for example using another model (Transformer-3) for evaluating their approach.\n- The metrics used for evaluating the approach are not explained in detail. This makes it hard to interpret the results.\n\nQuestions to Authors:\n- Do we need one head for each keyword and additionally one for each of the positional imprinting of each keyword?\n- If you write \"We fix the sequence length to 32\" do you mean that every sequence has the length of exactly 32 tokens or is it possible that sequences have less than 32 tokens which are then padded using padding tokens?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clearly written. However, the approach is quite complicated and some aspects of the approach could be explained in more detail without relying on related work.\n\nQuality: The paper is well written and is mostly easy to follow. Parts where the paper is building on previous work could be explained better. However, the idea is clearly explained and supported by mathematical deductions.\n\nNovelty: The idea of the paper is novel. In previous works the whole input sequence is reconstructed whereas in this paper only parts of the sequence which are relevant to the attacker are reconstructed.\n\nReproducibility: The experiments seem reasonable. However, no code was shared and a statement that the code will be made public upon acceptance was missing.",
            "summary_of_the_review": "The paper proposes a novel privacy attack on federated learning using text transformers. The problem statement is well motivated and it is shown that reconstructing only part of sequences that are relevant for the attacker is increasing the performance in contrast to previous approaches. The paper is well written and most of the time easy to follow. If the reader is not very familiar with the work that this paper is building on, it is hard to follow the reasoning.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_zieL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5380/Reviewer_zieL"
        ]
    }
]