[
    {
        "id": "wIAwdtKXKpt",
        "original": null,
        "number": 1,
        "cdate": 1666429949558,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666429949558,
        "tmdate": 1666429949558,
        "tddate": null,
        "forum": "EQiRSnqUYOh",
        "replyto": "EQiRSnqUYOh",
        "invitation": "ICLR.cc/2023/Conference/Paper377/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work theoretically proves the training dynamics of deep and wide probabilistic neural networks and generalization guarantees described by the minimum eigenvalue of the probabilistic NTK. The PAC-Bayesian framework follows the previous classical NTK based, lazy-training results on DNNs, e.g., using the squared loss and gradient descent to prove the kernel equivalence. Besides, a training-free proxy based the derived theoretical results is used for hyper-parameter selection. \n",
            "strength_and_weaknesses": "**Pros:** \n\n1. Deriving the training dynamics and kernel equivalence for deep and wide probabilistic neural networks under the NTK regime\n2. Building a PAC-Bayesian framework for generalization guarantees of probabilistic neural networks\n\n**Cons:**  \n\n1. This paper focuses on a certain style of DNNs, where the weights always follow a certain distribution. The authors follow the re-parameterization trick to ensure the weight distribution to be Gaussian during training. I think this is a significant assumption/setting by Eq. (2), which is different from classical deep learning theory literature. In this case, the fourth point in the contribution requires well-polished to avoid some over-claim.\n\n2. In theorem 4.2, the authors forgot a significant assumption: lambda_0(K) > 0. Another drawback is the exponential order the depth L due to the perturbation propagation. \n\n3. More importantly, the matrix in Definition 4.1 is a Jacobian matrix instead of NTK. The authors put the probabilistic NTK into an important position but no formulation of the final PNTK is given. Only \\Theta^{\\inf} and K^{(l)}_{\\inf} in an inner product formulation is not enough until I find it in the appendix at page 17, 18. The paper requires a better organization.\n\n4. Besides, I don\u2019t agree with the authors claiming that PAC-Bayesian bound has an improvement over the Rademacher complexity-based bound. Comparing Eq. (14) and (15), even though the first term in both equations is different, the final convergence rate is the same at the $O(1/\\sqrt{n})$ order due to the second term. In fact, Rademacher complexity bound is more general than NTK based bound. This is because, if the neural networks don\u2019t work in a NTK regime, Eq. (14) can not be obtained but Rademacher complexity bound independent of NTK still holds.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\n\nThis paper is well written and easy-to-follow. But the formulation on NTK in page 17 and 18 can be moved to the main text. Besides, the related work has no significant difference on PAC-Bayesian analysis and PAC-Bayesian learning. All of them center around PAC-Bayesian bound. \n\n**Novelty:**\n\nWhen reading this paper, I thought this paper is quite similar to previous deep learning theory work, e.g., Du et al. (2019), Arova et al. (2019a) in terms of the theoretical results and the technical proofs. Admittedly, the authors mentioned that the probabilistic neural network has another group of parameters, leading to relatively complex derivation. Nevertheless, the contribution in theory and techniques appear not enough and the obtained results incur an extra exponential order of L. In my view, if the authors get rid of the certain reparameterization trick and are able to prove that the weights during the training follow a Gaussian distribution, this will be interesting in both deep learning theory and the probabilistic neural networks community.\n\nBesides, the proposed proxy metric in Eq. (16) based on the label similarity matrix, which has already been discussed in kernel alignment literature. This is able to motivate the authors to have a better discussion. \n\n**Minor issues:**\n\nI only check the derived theoretical results in the appendix and they make sense.\nBut there are some typos in the derivation, e.g., the last line at the end of page 15; the second inequality in at the top of page 16. \n",
            "summary_of_the_review": "In summary, this paper presents the NTK-based analysis for deep and wide probabilistic neural networks in training dynamics and generalization guarantees. Nevertheless, there is not enough enthusiasm to vote for acceptance due to the not significant results and the not new technical proof.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper377/Reviewer_ag6P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper377/Reviewer_ag6P"
        ]
    },
    {
        "id": "xT76b0HE63g",
        "original": null,
        "number": 2,
        "cdate": 1666670206544,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670206544,
        "tmdate": 1666693124205,
        "tddate": null,
        "forum": "EQiRSnqUYOh",
        "replyto": "EQiRSnqUYOh",
        "invitation": "ICLR.cc/2023/Conference/Paper377/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper authors study wide stochastic networks, where the weights are sampled from a Gaussian with diagonal covariance. Authors aim to give a convergence analysis when the means and variances are obtained via gradient descent trained on the PAC-Bayesian bound objective.  \n\nAuthors obtain these results by introducing probabilistic neural tangent kernels that explain the dynamics of the training of mean weights and variances.",
            "strength_and_weaknesses": "Strengths\n\n- Results obtained are quite involved due to the nature of the convergence analysis\n\nWeaknesses\n\n- Paper lacks clarity; the results obtained are difficult to follow and authors don\u2019t give a palatable proof sketch and defer to the appendix for the detail which is not feasible to review for the conference.",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned before, the paper lacks clarity. I found it hard to follow the results in this paper and how authors have derived them. \n\nPaper makes some claims that do not make intuitive sense. Thm 4.2 claims for large m empirical risk of probabilistic network converges to zero, i.e. even with perturbation leads to no loss in training error wrt squared loss which seem to suggest the Gaussian should get perfectly concentrated around weights with zero training error. Secondly, when analysing KL divergence term, authors also claim in the theorem B.1 that variance does not change while training. It doesn\u2019t appear to be true and there likely is a mistake in the analysis somewhere.",
            "summary_of_the_review": "I recommend rejecting this paper. Main reason for my recommendation is that the results are not clearly written and likely have mistakes in them. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper377/Reviewer_Fsnz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper377/Reviewer_Fsnz"
        ]
    },
    {
        "id": "GJuXkHdQQH5",
        "original": null,
        "number": 3,
        "cdate": 1666848941557,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666848941557,
        "tmdate": 1666848941557,
        "tddate": null,
        "forum": "EQiRSnqUYOh",
        "replyto": "EQiRSnqUYOh",
        "invitation": "ICLR.cc/2023/Conference/Paper377/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the problem of learning a probabilistic neural network using the regularizer that comes from the PAC-Bayes generalization bounds. The main assumption is that the weights are sampled from Gaussian distribution with given mean and variance. The training algorithm is based on applying the gradient descent on the mean and variance of each weight. The training loss function is based on the PAC-Bayes bound which consists of an empirical risk and KL term. Then, in the limit that the width goes to infinity the authors provide a closed-form expression on the output of the neural network similar to NTK results.  The author also provides some numerical results which show the effectiveness of their approach.",
            "strength_and_weaknesses": "The problem setup in the paper is very interesting. However, I found the paper quite difficult to follow. The connection between theorems is not very clear. More importantly, the author did not discuss how their proof technique is differ from the NTK results.\n\nQuestions:\n\n1- I think equation 3 only works if the loss function is 0-1. Later in the paper the squared loss is used. Could you clarify this point?\n\n2- What does $+- \\xi$ in Eq. (13) mean?\n\n3- What is the intuition behind Equation (16)?\n\n4- In the numerical results section, why is there no plot on the actual generalization versus the theoretical bounds?\n\n5- In Thm 4.2 the KL term is dropped and the GD is only on the empirical risk. Later, in Thm 4.3 the authors also consider the KL term. I don't quite understand the intuition behind the proof technique here. ",
            "clarity,_quality,_novelty_and_reproducibility": "The problem setup is very clear. I had difficulty following the technical results. The main issue is that results do not come with intuition and the connection between the theorems is not clear.",
            "summary_of_the_review": "The paper considers an important problem: using PAC-Bayes objective to train a probabilistic neural network. The contribution of the results is not clear in terms of the differences between NTK literature. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper377/Reviewer_7Cjm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper377/Reviewer_7Cjm"
        ]
    },
    {
        "id": "-3QsZDTwGHT",
        "original": null,
        "number": 4,
        "cdate": 1666852176835,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666852176835,
        "tmdate": 1666852732621,
        "tddate": null,
        "forum": "EQiRSnqUYOh",
        "replyto": "EQiRSnqUYOh",
        "invitation": "ICLR.cc/2023/Conference/Paper377/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThis paper aims to theoretically investigate the optimization and generalization of deep PAC-Bayesian learning in the setting of wide neural networks. Technically, based on \nthe analysis ideas of the neural tangent kernel (NTK) for the deterministic models under the overparameterized settings, this paper characterizes the convergence of the probabilistic neural \nnetworks, which builds the connection with the kernel ridge regression via the probabilistic neural tangent kernel (PNTK). Then, the authors give the PAC-Bayesian generalization bound, \nwhich is an improvement over the Rademacher complexity-based bound. Further, inspired by the obtained bound, they propose a proxy measure for efficient hyperparameter selection. Finally, some experimental results are also \nprovided.",
            "strength_and_weaknesses": "\n## Strength\n1. Overall, this paper is well-organized and written clearly.\n2. The proofs of the theoretical results seem correct although I have not checked them line-by-line.\n3. The obtained theory results are valuable if it is the first time for deep probabilistic neural networks. \n4. Based on the theory results, the proposed proxy measure for efficient hyperparameter selection is effective with experimental support.\n\n\n\n## Weaknesses\n1. As highlighted for deterministic models, the analysis framework based on the neural tangent kernel has limitations, such as it cannot characterize the feature learning process in deep learning. This paper also has these limitations, which are for probabilistic models.\nThis paper does not discuss these limitations and more discussions should be added.\n2. The technical novelty may be limited since the NTK-based analyses have been widely used in deep learning theory although this paper considers the probabilistic neural networks. \n3. In my view, the comparisons between obtained the PAC-Bayesian generalization bound and the Rademacher complexity-based one might not be fair because of the following reasons. First, the definitions of the generalization errors for these two are different. Second, from these two bounds, although we \ncan find the key differences are the first term, as the authors claimed, one is for $O(\\frac{1}{n})$, and another is $0(\\frac{1}{\\sqrt{n}})$, but these bounds totally depending on $o(\\frac{1}{\\sqrt{n}})$. Further, the authors claimed that the obtained PAC-Bayesian bound is non-vacuous in the experiments.\nThus, I recommend the authors do additional experiments to compare these two bounds and related discussions should be added.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nPlease see the ''Strength And Weaknesses'' section.",
            "summary_of_the_review": "In summary, this paper theoretically characterizes the optimization and generalization of deep probabilistic neural networks via the analytical framework of the neural tangent kernel. It is valuable if it is the first time to provide these results.\nHowever, this paper needs some necessary revisions as in the 'Weaknesses' part. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper377/Reviewer_DZ2U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper377/Reviewer_DZ2U"
        ]
    }
]