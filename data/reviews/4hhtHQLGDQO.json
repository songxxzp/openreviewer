[
    {
        "id": "_SC61x5D7t",
        "original": null,
        "number": 1,
        "cdate": 1666562528403,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666562528403,
        "tmdate": 1666562528403,
        "tddate": null,
        "forum": "4hhtHQLGDQO",
        "replyto": "4hhtHQLGDQO",
        "invitation": "ICLR.cc/2023/Conference/Paper5805/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies an interesting problem on how to automatically generate augmented data for learning invairance representations. The paper proposes a MCMC based methods, and then tested the results in some datasets. The paper seems to be on a novel track of goals, but needs further development.  ",
            "strength_and_weaknesses": "- strength\n    - the study of generating augmented samples for invariance seems very novel to me. \n    - the writing of the paper is reasonably good\n- weakness\n    - the algorithm still needs further development, it's hard for me to believe a MCMC based method is the best solution in this scenario, given the rich techniques developed in deep learning regime. \n    - the empirical scope is quite limited: the deepAA method is barely compared. ",
            "clarity,_quality,_novelty_and_reproducibility": "- clarity: \n    - good\n- quality: \n    - the paper needs further development on its empirical end to be considered for publication. \n    - the MCMC based method can probably be further improved in terms of efficiency and performances with more modern techniques. \n- novelty:\n    - another highly relevant work regarding section 3\n        - Toward Learning Robust and Invariant Representations with Alignment Regularization and Data Augmentation\n    - Algorithms 1 and 2 seem fairly standard to me, not sure where the novelty is from. \n- reproducibility: \n    - seems good. ",
            "summary_of_the_review": "The paper seems still in its preliminary stage, a little bit limited in technical novelties, and more importantly empirical validation. I'm happy to give it a more thorough read if the authors can provide more convincing and more comprehensive empirical evaluations. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5805/Reviewer_159N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5805/Reviewer_159N"
        ]
    },
    {
        "id": "QLv2fWchEV",
        "original": null,
        "number": 2,
        "cdate": 1666636043016,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636043016,
        "tmdate": 1669517666819,
        "tddate": null,
        "forum": "4hhtHQLGDQO",
        "replyto": "4hhtHQLGDQO",
        "invitation": "ICLR.cc/2023/Conference/Paper5805/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to treat data augmentation in the context of a constrained optimization problem, where the constraints are defined using a set of desired invariances. The data augmentations are then chosen using an MCMC scheme based on the loss of the model (which is supposed to be invariant), while the strength of their influence is optimized using a Langrage multiplier.",
            "strength_and_weaknesses": "Strengths:\n- The problem of tuning data augmentation to different tasks is important.\n- The paper is clearly written and the theory seems correct.\n\nWeaknesses:\n- The experiments are a bit limited and inconclusive.\n\nMajor comments:\n- How does the choice of just two MC steps in the experiments affect the performance? I would imagine that the samples stay quite close to the uniform proposal then instead of actually sampling from the right distribution.\n- DeepAA seems like a quite strong baseline, it outperforms all other methods in one out of two experiments where it is included. Why is it not included in the other ones? I feel like it should be.\n- Given that TA is designed to be a ridiculously simple approach, it performs quite well compared to the proposed method. How does it compare in terms of runtime? I would imagine that TA would be much faster, is that true?\n- I appreciate the image classification experiment, but I think it would be illustrative to have an experiment with artificial invariances (similar to the ones in [1]), so one could discern to what extent the learned augmentations would correctly capture those.\n\nMinor comments:\n- The related work could mention [1]\n- In Eq. 3, what is $h$? Should that be $f_\\theta$?\n- Sec. 3.2: a intance -> an instance\n\n[1] https://arxiv.org/abs/2202.10638",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and is of high theoretical quality. The quality of the experiments is a bit lower. The method seems novel and the experiments seem reproducible.",
            "summary_of_the_review": "Overall, I think this is a really nice idea with a strong theoretical motivation. If the experiments could be improved, I would be happy to recommend acceptance; for now, I'm keeping my score slightly lower due to the inconclusive experiments.\n\nUPDATE: Based on the changes made by the authors during the rebuttal, I have updated my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5805/Reviewer_8185"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5805/Reviewer_8185"
        ]
    },
    {
        "id": "YXStEoMbe5",
        "original": null,
        "number": 3,
        "cdate": 1667508024535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667508024535,
        "tmdate": 1667508024535,
        "tddate": null,
        "forum": "4hhtHQLGDQO",
        "replyto": "4hhtHQLGDQO",
        "invitation": "ICLR.cc/2023/Conference/Paper5805/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a novel approach to learn a data augmentation method by formulating the task as invariance-constrained learning problem. They leverage Monte Carlo Markov Chain (MCMC) sampling to solve it, which achieves state-of-the-art results on CIFAR10 and CIFAR100 for specific wide neural network architectures.\n",
            "strength_and_weaknesses": "Strengths:\n+ The overall presentation is clear, the background is explained in detail, and the respective terms and their high level interpretation are discussed.\n+ The proposed data augmentation learning method seems to outperform the baselines on CIFAR10 and CIFAR100.\n+ The presented problem formulation has the advantage that if a solution to the unconstrained problem is feasible, the presence of that constraint has no effect on the statistical problem.\n-> Could this fact also be helpful to assess whether the learned transformations are consistent with the data distribution or impose an unwanted bias?\n\nWeaknesses and open questions:\n- Competing papers (e.g., M\u00fcller and Hutter, 2021) compare run times of different augmentation algorithms. Such an analysis is completely missing.\n- The authors make multiple claims regarding the advantages of their formulation as constraint optimisation problem but these are not demonstrated experimentally.\n(Can invariances be learned?)\n- In contrast to competing methods, no experiments on ImageNet are provided. This is relevant for the question whether the proposed data augmentation method scales to large, complex datasets.\n- The authors claim themselves that they fail to achieve large improvements in accuracy over baselines but this could be attributed to a stagnation in data augmentation research (M\u00fcller and Hutter, 2021) and \ncould reflect limits of the benchmarking setup. In this case, the authors could also look for other datasets and demonstrate improvements of their method over benchmarks there.\n- A general question for this line of research: Why are no synthetic experiments conducted to test the ability of automated data augmentation algorithms to recover symmetries in the data? It would be easy to generate data that is invariant to some transformations and test how many samples are required to learn the correct transformations. The claim that the relevant transformations are learned is otherwise unsupported.\n- The data augmentation proposal seems to rely on a couple of hyper parameters that need to be tuned in practice.\n- A couple of hand-crated baselines are discussed in the introduction but not compared with in the experiment section.\n\nPoints of minor critique and open questions:\n- No guarantees with respect to the convergence of the sampler to the equilibrium distribution are provided. (This would, however, be a hard problem and might be too much to ask for.)\n- The related work section is quite short and adds little value in addition the introduction.\n- Figures 1 and 2 are a bit difficult to read for yellow markers.\n- Considering that a lot space is left in the main paper, the conclusions and insights into the proposed method could be extended. In particular, moving the analysis of the sampled transformations could highlight additional features of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is very clear and didactically well designed. While the proposed approach seems to be novel for learning data augmentation strategies, its benefits could be clarified and also be subject of more extensive experiments. Currently, the practical impact of the approach seems minor. Importantly, also an analysis of the computational complexity of the proposed method is missing (and could present a considerable downside of the approach).",
            "summary_of_the_review": "While the paper is well motivated and clearly written, some relevant analysis (like the runtime analysis, potentially additional experiments that highlight the specific advantages of the proposed approach, etc.) are missing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have not ethical concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5805/Reviewer_E3e9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5805/Reviewer_E3e9"
        ]
    },
    {
        "id": "ffo_XD8cr7v",
        "original": null,
        "number": 4,
        "cdate": 1667508035438,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667508035438,
        "tmdate": 1670101550740,
        "tddate": null,
        "forum": "4hhtHQLGDQO",
        "replyto": "4hhtHQLGDQO",
        "invitation": "ICLR.cc/2023/Conference/Paper5805/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Paper proposes to automatically learn which transformations to augment during training without introducing biases, for example, when the augmentations are not appropriate for the task. Authors rewrite the problem as a semi-infinite constrained problem similar to a recent adversarial robustness framework. Proposed approach learns to sample from a distribution over the transformations using a Metropolis-Hastings approach. ",
            "strength_and_weaknesses": "Strengths:\n\n1. Automatically learning data augmentations for a particular task is a significant and important challenge which helps not just in-distribution but also for out-of-distribution robustness of models. \n2. Not requiring differentiable transformations (unlike baselines) can be helpful. \n3. Typically, the tradeoff between the original task loss and the augmentation loss is done via hyperparameter tuning, the proposed approach instead maximizes the relevant dual variable (whose advantages are discussed in the paper). \n\nWeaknesses:\n\n1. I think the paper can be improved with a more formal treatment of the problem and method. \n   1. The problem is not formally defined, for example, the paper never discusses formally the true invariances of the data distribution. Essentially, the task of an automatic data augmentation method will be to learn transformations that hold for the true (unknown) data generation process.\n   2. Degree of invariance and approximate invariance are used loosely and not well-defined. \n   3. The paper is motivated solely on improving the sample complexity but no such bounds are provided; while data augmentation itself can lower the sample complexity, it is unclear if that continues to hold when learning augmentations automatically especially with an approximation from constrained optimization (CSRM) to unconstrained optimization (D-CERM). \n\n2. Empirical evaluation does not show case the claims of advantages of the proposed method vs the baselines.\n\n   1. Experiments do not explicitly show cases when data augmentation results in biasing the model and is harmful, which is one of the main reasons for learning the appropriate distributions over the transformations. \n\n   2. Another advantage of the proposed method compared to the baselines is to allow non-differentiable transformations, but no such experiments are performed. \n   3. Overall, the empirical gain in performance is not significant compared to the baselines.\n\n\n\nOther Questions/Comments:\n\n1.  Introduction: \u201cThe use of invariance overcomes the need to search for a specific distribution over transformations\u201d. I do not understand this statement. \n2. As opposed to the existing methods, $\\gamma$ is treated as a dual variable (and optimized) which allows the tradeoff to be learnt directly. However, $\\epsilon$ is a hyperparameter which is cross-validated which also trades-off the approximate invariance, thus leading to the same problem. Further, it is not clear from the paper, what cross-validation methodology is used (i.e., what is the training and test datasets for this cross-validation)?\n3. There seems to be a single slack variable \\epsilon that controls all different types of transformations (for example, rotations, translations, shear, etc) with no additional regularization to force toward invariance unlike [1].\n\n3. I do not agree with the statement \u201csampling $\\lambda^*_c$ accurately is not a concern given that we are interested in promoting approximate invariance\u201d as we still need to characterize the kind of approximation. For instance, using a constraint level ($\\epsilon$) is one way of approximation but it is different from not able to accurately estimating $\\lambda_c^*$.\n\nMinor/Typos:\n\n1. Typos: \u201cwhithout\u201d, \u201cinfromativeness\u201d\n\n2. $\\lambda^*_c$ takes different inputs at different places in Section 4.2. \n\n   \n**References**\n[1] Benton, Gregory, et al. \"Learning invariances in neural networks from training data.\" Advances in neural information processing systems 33 (2020): 17605-17616.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Writing can be improved with a more streamlined and concise presentation. Some aspects of the framework (e.g., semi-infinite constrained learning and MCMC sampler required) exist in a previous work in adversarial robustness. ",
            "summary_of_the_review": "Paper can be improved with a more formal treatment of the problem and method (e.g., in defining the problem, discussing sample complexity of the proposed method). Empirical evaluation does not show case the claims of advantages of the proposed method vs the baselines (tasks where some augmentations are not appropriate, non-differentiable transformations). ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5805/Reviewer_wrTV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5805/Reviewer_wrTV"
        ]
    }
]