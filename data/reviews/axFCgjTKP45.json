[
    {
        "id": "7dEW_ce9ZU",
        "original": null,
        "number": 1,
        "cdate": 1666358723782,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666358723782,
        "tmdate": 1666358723782,
        "tddate": null,
        "forum": "axFCgjTKP45",
        "replyto": "axFCgjTKP45",
        "invitation": "ICLR.cc/2023/Conference/Paper4645/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The work proposes to adapt pretrained language models for CDR loop infilling. Essentially, this is a task that requires the model to predict the identity of a span of masked out tokens in a sequence of amino acid tokens. The work proposes to do the adaptation using a model reprogramming approach, where two linear layers are used to map from the (task_input, pretrained_output) domains to the (pretrained_input, task_output) domains, thereby learning a lightweight network that casts the task at hand into something that can be solved by the pretrained model.\nThis is a novel idea to practitioners in the protein deep learning field, where typically no benefit is expected from using models pretrained on human language. The paper claims that the method achieves good infilling (=reconstruction) performance while also having high sample diversity.\n",
            "strength_and_weaknesses": "+The idea is novel\n- The performance compared to the baselines is not convincing. The comparably strong performance of EnglishBert, which should be expected to have no knowledge of the problem whatsoever, is especially worrying.\n- Not clear how useful sequence diversity is as a quality metric of the model\n- It is not clear if the baseline models are fair, as they have fewer learnable parameters.\n- Discussion not addressing the implications of the results.\n\nQuestions\nHow do the authors explain the strong performance of EnglishBERT, which has no knowledge about the antibody inpainting problem?\nWhat is the meaning of ProGen perplexity for CDR loops? Do native CDRs have lower perplexity?\nWhy was EnglishBERT chosen for reprogramming, when ProtBERT is pretrained on a domain much closer to the task?\nWhy are BERT models pretrained on antibody sequences not considered?\n\nComments\nFigure 1 is confusing. Showing the 3D structure and the light chain sequence on the input side of the figure is distracting, and only upon reading it becomes clear that those are not part of the model input.",
            "clarity,_quality,_novelty_and_reproducibility": "It is a novel and original approach that has seen no prior application in biological sequence design. The paper lacks clarity as to why exactly model reprogramming should be the right choice for this biological problem.",
            "summary_of_the_review": "I am not convinced that this method works well. Essentially, the paper claims that the task of antibody sequence infilling can be solved by reprogramming (=adding two linear layers) a BERT model that is pretrained on english language and that this works better than a) a BERT model pretrained on proteins and b) antibody infilling models that additionally use structural information. This would be a very unexpected result that shows that the task of antibody sequence infilling is actually surprisingly trivial and can be solved with minimal knowledge of the problem. However, the paper fails to discuss this adequately and just presents the results as they are without further discussion of these implications.\n\nThe authors base their claim on a) good sequence recovery performance and b) highly diverse samples. While recovery is easily interpretable as it is the task that is optimized for, diversity as such has no inherent meaning, as long as it cannot be validated that the diverse samples are indeed biologically plausible. ProGen perplexity as it is presented is not conclusive of the biological plausibility, as it is not clear what perplexity behaviour can be expected for CDR loops which are highly variable and comparably unstructured.\n\nEnglishBert should be understood as an almost-random baseline, as it has no knowledge about antibody structure aside from what can be incorporated from learning new amino acid embeddings in the presented setup. Yet, on CDR3, according to the presented metrics, it performs better than Refine-GNN. Why is that?\n\nIn the reprogramming framework, two linear layers with a large number of parameters are introduced to map from the task domain to the pretrained domain. For the baseline methods, this is not the case. How is ProtBERT considered to be a fair baseline method for the task, if it was never optimized for it and has no learnable parameters?\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4645/Reviewer_qyah"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4645/Reviewer_qyah"
        ]
    },
    {
        "id": "hEZKXmi4V3g",
        "original": null,
        "number": 2,
        "cdate": 1666398047137,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666398047137,
        "tmdate": 1666398047137,
        "tddate": null,
        "forum": "axFCgjTKP45",
        "replyto": "axFCgjTKP45",
        "invitation": "ICLR.cc/2023/Conference/Paper4645/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduced \"Reprogramming for Protein Sequence Infilling\", for the antibody CDR design problem. Starting from a more general protein language model, the model is \"reprogrammed\" for the infilling task (infilling for antibody CDR loops). The method is then empirically evaluated on antibodies in the SAbDab database.",
            "strength_and_weaknesses": "Strengths:\n+ Reprogramming is an interesting idea and hasn't been previously applied to protein design.\n\nWeaknesses:\n+ Limited empirical improvements compared to ProtBert.\n+ Limited in scope. The infilling problem for protein sequences could be of more general interest on other classes of proteins, although this paper is focused on the specific subarea of antibody CDR design.\n+ Can the model capture interactions between the CDR loops?",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: Limited empirical improvements from reprogramming, when compared to ProtBert without reprogramming.\n\nClarity: Overall easy to read, with clear tables and figures. It would help to be more clear on the motivation for the problem setup. In particular, typically CDR1 and CDR2 are largely dependent on the V gene (also reflected in framework regions). What's the motivation for only designing the CDRs when the V gene is already known?\n\nOriginality: Limited technical novelty.",
            "summary_of_the_review": "Limited technical novelty and application scope. This approach is interesting and probably could be useful in practical application, but I'm not sure how useful it is to the broader community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4645/Reviewer_RNXK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4645/Reviewer_RNXK"
        ]
    },
    {
        "id": "rUGuW3IU0Dw",
        "original": null,
        "number": 3,
        "cdate": 1667053037286,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667053037286,
        "tmdate": 1667188924190,
        "tddate": null,
        "forum": "axFCgjTKP45",
        "replyto": "axFCgjTKP45",
        "invitation": "ICLR.cc/2023/Conference/Paper4645/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a framework for the antibody sequence design task. The main idea is to use the pre-trained model for sequence prediction. The difference is that they use a pre-trained English BERT model with some modifications. The results show improvements of the sequence prediction. \n\nOverall speaking, this is an easy work. \n",
            "strength_and_weaknesses": "Strength:\n1. The writing is clear and the authors give a clear motivation for the work.\n2. The results show improvements in the work. \n\nWeakness:\n1. Novelty is really limited. The authors do not give enough modifications to the model, while it is interesting to use the English BERT model, but the method seems lack enough motivation when comparing the MR method with protein pre-trained model.\n2. Similar to the above one, the results are interesting that the MR is better than protein pre-trained model, but the reason is not explained and the study is not enough. This strong lacks enough content for this paper. \n3. More studies and results are expected for the later version. \n4. Code is not released, though the results are promising.",
            "clarity,_quality,_novelty_and_reproducibility": "novelty is not enough.",
            "summary_of_the_review": "See above. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4645/Reviewer_UBBq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4645/Reviewer_UBBq"
        ]
    },
    {
        "id": "syHKGDBL7O",
        "original": null,
        "number": 4,
        "cdate": 1667440249272,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667440249272,
        "tmdate": 1667440249272,
        "tddate": null,
        "forum": "axFCgjTKP45",
        "replyto": "axFCgjTKP45",
        "invitation": "ICLR.cc/2023/Conference/Paper4645/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors reappropriate the term \u201cfine tuning\u201d and call it \u201creprogramming\u201d. The main aim of the work is to propose new CDR loops as an infilling approach Some experimental results suggest that the proposed approach outperforms the state of the art.\n",
            "strength_and_weaknesses": "To me, the main weakness of this work is the lack of reference to the \u201cfine tuning\u201d literature.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty\n\nFor me, it\u2019s difficult to gauge the novelty since the authors completely missed the \u201cfind tuning\u201d literature. From my perspective so far, there is no novelty.\n\nQuality\n\nThe technical work lacks context, as already mentioned.\n\nClarity\n\nAside from the lack of context, the work is clear.\n\nReproducibility\n\nI didn't seen any code or data, so I guess this isn't very reproducible.\n\n",
            "summary_of_the_review": "This paper misses a lot of relevant context, so it\u2019s difficult to put in context. I would recommend the authors find relevant \u201cfine tuning\u201d papers and start from there.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4645/Reviewer_9eKN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4645/Reviewer_9eKN"
        ]
    }
]