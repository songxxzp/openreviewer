[
    {
        "id": "v6N-r_7sNwr",
        "original": null,
        "number": 1,
        "cdate": 1666639675645,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639675645,
        "tmdate": 1666639675645,
        "tddate": null,
        "forum": "p8hMBcPtvju",
        "replyto": "p8hMBcPtvju",
        "invitation": "ICLR.cc/2023/Conference/Paper1754/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper describes a method for subset sampling based on conditional Poisson sampling, called neural conditional Poisson subset sampling.  This approach builds on the general conditional Poisson sampling approach, which samples each element in the subset independently, and conditions this procedure to return subsets of exactly $k$ elements.  Conditional Poisson sampling has some weaknesses, including high computational complexity, and that it is not differentiable and thus cannot be included in differentiable models.  The proposed neural conditional Poisson subset sampling approach addresses these issues, allowing for efficient sampling of large subsets, and is fully differentiable.",
            "strength_and_weaknesses": "Strengths:\n* The proposed approach is scalable, allowing for efficient sampling of large subsets.\n* The proposed approach is fully differentiable, allowing it to be incorporated into differentiable models that are trained via gradient-based learning methods.\n* The experiments in this paper show strong empirical evidence that the proposed approach outperforms competing approaches in terms of both predictive performance and runtime performance, sometimes by a significant margin.\n\nWeaknesses:\n* The proposed neural conditional Poisson subset sampling assumes that the $k$ elements for a sampled subset are drawn IID from the universe of $n$ elements.  That is, it assumes that there are no interactions (correlations) between elements within a subset.  This is a significant limitation.  Scenarios involving text or image data may often involve interactions between elements within a subset, which may not be represented well with the approach in this paper.\n* No theoretical analysis of the runtime complexity of the proposed algorithm (Algorithm 3) is provided in the paper.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is reasonably high quality, well written, and clear.  While relatively straightforward, the proposed approach appears to be original.  Enough detail is included in the paper to implement and reproduce the proposed approach, and code is provided in order to reproduce the experimental results.  ",
            "summary_of_the_review": "While the approach described in this paper is relatively simple and straightforward, it is in fact more scalable than a number of prior approaches for subset sampling, and also has the advantage of being fully differentiable.  This paper is well written, and the empirical results are convincing.  Taken together, this is a good paper and should be accepted. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1754/Reviewer_ZXrw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1754/Reviewer_ZXrw"
        ]
    },
    {
        "id": "aHCqXH97GMk",
        "original": null,
        "number": 2,
        "cdate": 1666735527358,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735527358,
        "tmdate": 1666735527358,
        "tddate": null,
        "forum": "p8hMBcPtvju",
        "replyto": "p8hMBcPtvju",
        "invitation": "ICLR.cc/2023/Conference/Paper1754/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new technique for subset sampling via conditional Poisson sampling. Given a universe of $n$ elements and inclusion probabilities $p_i$ for each element $i$, the problem is to sample a subset of size $k$ that respects the inclusion probabilities.\n\nPoisson sampling samples each element with probability $kp_i$, leading to set of size $k$ in expectation, while conditioninal Poisson sampling would repeat unconditional Poisson until a set of size $k$ is achieved. Both of these have limitations, as uncoditinoal sampling only produces a set of size $k$ in expectation ( and may have high variance) and if $k p_i \\geq 1$, element $i$ is always included; conditional sampling may not respect the inclusion probabilities.\n\nThis paper suggests techniques for handling these various issues, along with a differentiable variant that can be used in a gradient-base learning framework. Additionally, the subset size $k$ can also be made differentiable, which is useful when the exact subset size is not known and needs to be optimized, for e.g., when selecting features that are most important for downstream classification models.",
            "strength_and_weaknesses": "Strengths:\n- The algorithm is quite elegant and simple. It also remarkably modular, as it can be used for a variety of downstream tasks, such as feature selection for explainability of downstream classifiers.\n\n- The authors identify flaws with the Poisson sampling approach, and show algorithms that can overcome said challenges.\n\n- I am not familiar with the state of the art for subset selection, but it appears that the proposed algorithm performs at least as well as competing approaches, is quicker, and is more flexible with respect to choice of $k$ or optimizing $k$.\n\nWeaknesses:\n- This motivation for using Poisson sampling requires some more work. There exist exact sampling methods such as weighted reservoir sampling (https://en.wikipedia.org/wiki/Reservoir_sampling#Weighted_random_sampling) that do not suffer from the problems experienced conditional  / unconditional sampling. Since the differentiable part of the Poisson network is an application of the Gumbel-Softmax trick to replace the differentiation of Bernoulli r.v.s, why not use the same technique for reservoir sampling?\n\n- The experiments section is nice, but it seems like the three experiments are really variations of the same problem, i.e., selecting a mask that helps choose features that explain a text / image / face classifier. This surely cannot be the only application of subset selection --  what are the other standard baseline applications of subset selection? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Originality:\nI'm not familiar with the area of subset selection, but it appears that the authors have reasonably original procedures to handle the flaws of Poisson sampling (which is a standard technique with known flaws). Allowing for a differentiable subset size also seems sufficiently original. \n\nI have some concerns that the paper ends up sounding like a series of patches to fix the flaws on Poisson sampling (e.g., lack of differentiability, high variance, etc.), but I think Poisson sampling has advantages over existing approaches, and the empirical results are convincing.\n\nClarity:\nThe paper is well written. The advantages and disadvantages of prior work and this approach are well covered, and the proposed procedure is well articulated.\n\nQuality:\nIt meets the ICLR standard, and is a sufficiently good quality paper.",
            "summary_of_the_review": "My score is based primarily on the fact that the proposed approach is:\n- modular\n- quick\n- flexible in choice of subset size $k$, and also allows for optimizing $k$ in a differentiable manner\n- can be used for selecting interpretable features for downstream classification tasks\n\nThat being said, I am not an expert in subset selection and do not know much about current state of the art, and my score of weak accept is based on my current understanding of the paper. I'm happy to increase my score based on the author feedback and discussion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1754/Reviewer_H9tb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1754/Reviewer_H9tb"
        ]
    },
    {
        "id": "er_yLvCYKwF",
        "original": null,
        "number": 3,
        "cdate": 1666814974191,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666814974191,
        "tmdate": 1666814974191,
        "tddate": null,
        "forum": "p8hMBcPtvju",
        "replyto": "p8hMBcPtvju",
        "invitation": "ICLR.cc/2023/Conference/Paper1754/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work aims to propose a differentiable and scalable k-subset sampling algorithm based on conditional Poisson sampling. The scalability comes from the conditional Poisson sampling scheme where each instance is sampled independently such that the vectorized complexity of the proposed algorithm is independent of the subset size k. The differentiability is achieved by the use of a straight-through gradient estimator. It further carries out experiments on learning to explain text and image, image subsampling, and k-nearest neighbor search.",
            "strength_and_weaknesses": "I find the most compelling part of the proposed algorithm is scalability as shown in the subsampling large image experiment where some baselines fail. The technical details seem solid to me. Still, there are a few concerns:\n- My main concern is that the subset size being k is not a hard constraint. Even though Figure 1 shows that increasing the number of iterations in the conditional Poisson sampling process might mitigate this issue and result in a distribution over subset sizes being more concentrated on the desired subset size, this is at the price of high computational cost and it is not guaranteed that all the sampled subsets would have their subset size being exactly k. While this guarantee is achieved by the other existing subset sampling approaches such as RSS.\n- I wonder for the conditional Poisson sampling, what is the expected number of iterations to sample a subset with a size being exactly k? Besides, while Proposition 3.1 is about the decreasing rate of variance, I wonder if this result can be generalized to the case of arbitrary input probabilities instead of the equal input probability assumption.",
            "clarity,_quality,_novelty_and_reproducibility": "This work is overall well-written and easy to follow. It provides sufficient background for the readers to understand the proposed approach.",
            "summary_of_the_review": "The proposed approach demonstrates good scalability. My main concern is that the sampled subset size is not guaranteed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1754/Reviewer_oHmW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1754/Reviewer_oHmW"
        ]
    },
    {
        "id": "bRDAr-3b3O",
        "original": null,
        "number": 4,
        "cdate": 1667378126633,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667378126633,
        "tmdate": 1667378126633,
        "tddate": null,
        "forum": "p8hMBcPtvju",
        "replyto": "p8hMBcPtvju",
        "invitation": "ICLR.cc/2023/Conference/Paper1754/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new approach to sampling subsets from a set of $n$ elements using an iterative version of Poisson sampling. This avoids the high variance of standard Poisson sampling and the high cost of conditional Poisson sampling by iteratively adding or deleting elements a finite number of times. The lower variance and computational efficiency of the approach are also empirically validated across a range of experiments.",
            "strength_and_weaknesses": "Strengths:\n\n1. The approach is elegant, intuitive and also theoretically proved to have low (exponentially decaying) variance.\n2. There are clear empirical improvements over prior work in a wide range of applications including some applications like subsampling high resolution images which are computationally infeasible with prior work (due to high memory requirements).\n\nWeaknesses:\n\n1. The approximate (straight-through) gradients may be too far from the true gradients. While straight-through gradients appear to be doing well in the current set of experiments, will it be feasible to explore some of the other gradient estimators mentioned, even just for the same tasks, to comment more conclusively on the right estimator?\n\n2. The approach lacks a discussion of convergence rates. Specifically, it would be interesting to see some tail bounds on the deviation from the mean after T iterations but it is fine to leave that for future work.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall very clearly written and easy to follow. I only have a couple of minor comments:\n\n1. The claim that exactly $k$ samples may not always be needed seemed a bit strong when it was first mentioned near the end of Section 1. I would recommend adding an example at that point to clarify why one could do without exactly $k$ elements.\n\n2. Please add standard deviation/standard error values to the results in Table 1 and 2.",
            "summary_of_the_review": "The paper introduces a novel approach for subset sampling that has low variance and is significantly more scalable than prior approaches. The approach is theoretically principled and outperforms baselines on a range of empirical experiments. It would be interesting to see some convergence rates for the deviation of the subset size from the expected size in a future work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1754/Reviewer_iTJN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1754/Reviewer_iTJN"
        ]
    }
]