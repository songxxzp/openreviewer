[
    {
        "id": "aGKR4mN8Di",
        "original": null,
        "number": 1,
        "cdate": 1666367907573,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666367907573,
        "tmdate": 1666367907573,
        "tddate": null,
        "forum": "Kn-HA8DFik",
        "replyto": "Kn-HA8DFik",
        "invitation": "ICLR.cc/2023/Conference/Paper233/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the problem of cross-domain few-shot learning, where the base dataset and the target domain are far away from each other. It builds upon past distillation-based frameworks, where unlabeled data from the target domain is used to distill a source teacher, pretrained on the base dataset. It introduces multiple modifications to this basic approach, including using distillation of multiple layers, a way of feature selection on the target problem, and an approach for avoiding inheriting the teacher's biases.\n",
            "strength_and_weaknesses": "Strengths:\n1. The problem of cross-domain few-shot learning is a practically significant problem.\n2. The results obtained by the proposed method are much more accurate than prior state-of-the-art.\n3. The paper has thorough ablations for its many contributions.\n\nWeaknesses:\n1. The contributions seem like engineering innovations rather than new insights. However, each contribution has well thought-out motivations, so I am less worried.",
            "clarity,_quality,_novelty_and_reproducibility": "Generally I find the paper well written. The code will be released, and that should make the approach reproducible. The insights though not particularly novel are still well motivated and worth knowing for the community.",
            "summary_of_the_review": "Despite my short review, I find the contributions useful, the problem significant, and the result impressive. I am in favor of acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper233/Reviewer_yRW4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper233/Reviewer_yRW4"
        ]
    },
    {
        "id": "nZWQP-vz0k",
        "original": null,
        "number": 2,
        "cdate": 1666539280919,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666539280919,
        "tmdate": 1673293773740,
        "tddate": null,
        "forum": "Kn-HA8DFik",
        "replyto": "Kn-HA8DFik",
        "invitation": "ICLR.cc/2023/Conference/Paper233/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors propose a cross-level distillation method for cross-domain few-shot learning. The are four stages in the pipeline. A pre-trained model is trained on the base classes data, then a student network is trained with cross-level distillation by mimicking the teacher network. A feature denoising step is further added to reduce overfitting of the features. The authors conduct extensive experiments on the BSCD-FSL benchmark to show the effectiveness of the proposed approach. ",
            "strength_and_weaknesses": "Strength: \n\n1. The writing of the paper is clear and the organization is also well structured.\n2. The idea of cross-level distillation is also interesting and novel. \n3. The authors also conducted extensive ablations to show the benefits of the proposed approach. \n\nWeaknesses:\n\n1. There are too many hyperparameters in the method. It may be difficult to tune the parameters for new applications.\n2. The introduction of the old student network may introduce additional overhead.\n\nThe authors may have some discussions on how to choose the hyperparameters and compare the computational complexity of the method with the baselines. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: overall, the paper is clearly written and easy to understand. Although there should be more motivations for the design choices of cross-level distillation. \n\nQuality: the quality of the paper is good.\n\nNovelty: the idea of cross-level distillation is novel for cross-domain few-shot learning. \n\nReproducibility: the paper seems reproducible. ",
            "summary_of_the_review": "The paper proposes a novel method for cross-domain few-shot learning and extensive experiments show the effectiveness of the method. The main drawbacks of the method are the 1) introduction of excessive hyperparameters and 2) the computation complexity. \n\nPost-rebuttal:\n\nI have read the authors' rebuttal. It partially addresses my concerns. I tend to keep my rating but do not have an inclination to support the paper if other reviewers have concerns. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper233/Reviewer_8cJ8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper233/Reviewer_8cJ8"
        ]
    },
    {
        "id": "VIHbMWu922Y",
        "original": null,
        "number": 3,
        "cdate": 1667130842080,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667130842080,
        "tmdate": 1667130842080,
        "tddate": null,
        "forum": "Kn-HA8DFik",
        "replyto": "Kn-HA8DFik",
        "invitation": "ICLR.cc/2023/Conference/Paper233/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to tackle the problem of cross-domain few-shot classification, where such problem scenario especially considers the few-shot learning under the case of having (large) domain-shift between base and target domains. In particular, with being motivated to address the large domain gap, this paper follows the prior works (i.e. STARTUP@ICLR2021 and Dynamic-Distillation@NeuraIPS2021) to include a specific assumption of having access to a small portion of unlabelled images in the target domain during the training stage. \n\nMoreover, the proposed method of this paper also follows the similar train of thoughts as the prior works: 1) having a teacher model trained on base dataset; 2) having self-supervised contrastive learning on the unlabeled target images; 3) the student for the target domain tries to learn from the teacher model by using the knowledge distillation (KD) technique, while the knowledge distillation strategy gradually changes from taking \"the teacher trained on base dataset\" as the main source of knowledge (similar to STARTUP) in the early iterations to taking \"the moving average of the updated teachers on previous iterations\" as the main source of knowledge (similar to Dynamic-Distillation) in the later iterations. \n\nIn addition to three aforementioned points which are similar to prior works, the proposed method in this paper has another two key designs: 4) the knowledge distillation implementation is cross-level (i.e. encouraging the shallow layers of student to learn from the deeper layers of teacher), in which the idea is similar to BYOT@ICCV2019 (the slight difference is that BYOT operates self-distillation); 5) due to the phenomenon stemmed from the self-supervised loss and discovered by previous works where the final feature vector for classification has a small number of strongly activated elements while the others are close to zero, the proposed method introduces the \"feature denoising\" technique to truncate the elements with small magnitudes to zero during the fine-tuning stage (i.e. updating the model to the target domain by using the given target-domain few-shots with labels). \n\nThe proposed method is experimentally evaluated on the BSCD-FSL benchmark and the comparison with respect to several baselines (including ATA and CI which tackle the cross-domain few-shot classification problem without access to target domain data during training, self-supervised methods, STARTUP, and Dynamic-Distillation) is made, where the proposed method is shown to provide superior performance. ",
            "strength_and_weaknesses": "Strength:\n+ The proposed method is able to outperform different types of baselines (including ATA and CI which tackle the cross-domain few-shot classification problem without access to target domain data during training, self-supervised methods, STARTUP, and Dynamic-Distillation, where the latter two are under the same problem scenario as the proposed method) in a challenging benchmark.\n+ The detailed investigations/ablation studies are performed to validate the model designs and hyper-parameter settings in the proposed method (e.g. the model structure for cross-level knowledge distillation, the hyperparameter $\\tau$ for building the old student, ablation study for feature denoising, the hyperparameter $\\lambda$ to balance self-supervised loss and knowledge distillation loss for training student model, and the hyperparameter $h$ for determining the number of truncation in feature denoising operation).\n+ The overall organization is clear and the writing is easy to follow.  \n\nWeaknesses:\n- Though I can understand that the large domain gap between base dataset and target domain is quite challenging to be tackled, but the assumption of having access to a portion of target domain data (even it is unlabeled) seems to be quite impractical and hard to envision its usage in real-world scenarios. Moreover, even there exists two prior works (i.e. STARTUP and Dynamic-Distillation) addressing the same problem setting as the proposed method, the authors should still provide sufficient motivations as well as the description on the potential usage of such problem setting in their paper to make the whole story self-contained and persuasive. \n(Besides, in this paper they set to have 20% of target domain data available during training, in which such amount is not \"a small portion\" in my perspective).\n- In addition to the teacher model trained on base dataset and kept fixed during knowledge distillation, the authors introduce another model, named as \"old student\" (which in implementation is the student model in the previous learning iteration), in order to keep the knowledge of historical students during the learning of student model. Though such design seems to provide benefit to the overall model performance, having such additional model actually increase the computational cost for the overall training procedure. Moreover, the proposed method adopts the linear combination of such old student model and the teacher model as the actual source of knowledge to execute the knowledge distillation, in which such old student model acts similarly to the moving average of historical students (i.e. an ensemble) if we expand the overall iterative procedure (thus being closely related to Dynamic-Distillation@NeuraIPS2021), hence the linear combination of the old student and the teacher becomes an \"ensemble of ensemble\" or analogously a straightforward integration over the distillation strategies of STARTUP and Dynamic-Distillation. Overall, such the distillation strategy of the proposed method seems to be incremental from the methodology perspective. \n- The cross-level distillation mechanism is closely related to the well-known BYOT@ICCV2019 work (where the shallow layers try to learn the knowledge from the deeper layers), in which the only difference exists on that BYOT is doing self-distillation while the proposed method applies the same idea across two networks. The cross-level distillation proposed in this paper thus seems to be incremental as well. \n- Though the experimental results show that the proposed method provide superior performance, there are some concerns as detailed below: 1) we see that the proposed method using BYOL as its self-supervised loss (denoted as BYOL+CLD+FD (ours) in Table.1) does not present significant performance improvement with respect to Dynamic-Distillation (especially the loss function in Dynamic-Distillation is closely related to BYOL); 2) the baseline SimCLR (self-supervised learning applied on unlabeled target domain data) already provides strong performance (e.g. better than BYOL+CLD+FD (ours) and even competitive with SimCLR+CLD+FD (ours) which also adopts SimCLR in its loss functions). From these two observations, the contribution of the proposed method becomes skeptical (in which the proposed method adopts many different designs into its framework but in results only achieves sightly better, comparable, or even worse performance with respect to the baselines). ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the clarity of the paper is satisfactory (well organization and good paper writing) but the novelty is incremental (taking the aforementioned weaknesses into consideration, in which the proposed distillation strategy can be simply treated as a dynamic combination over STARTUP and Dynamic-Distillation and the proposed cross-level distillation mechanism is also identical to the well-known BYOT). Moreover, as the problem setting of having access to a portion of target domain data is not well motivated and seems to be impractical, together with the insignificant performance improvement compared to the baselines, the quality of the paper becomes also problematic. ",
            "summary_of_the_review": "Due to the limited/incremental novelty, potentially skeptical improvement with respect to baselines, as well as not-well-motivated and impractical problem setting (please refer to weaknesses for more details), I would suggest to reject this paper from ICLR2023 but is willing to change my rating if my concerns can be well addressed.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper233/Reviewer_am4M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper233/Reviewer_am4M"
        ]
    }
]