[
    {
        "id": "6vUWk890P71",
        "original": null,
        "number": 1,
        "cdate": 1666529770152,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666529770152,
        "tmdate": 1668914751135,
        "tddate": null,
        "forum": "04K3PMtMckp",
        "replyto": "04K3PMtMckp",
        "invitation": "ICLR.cc/2023/Conference/Paper3748/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThe paper reports the effects of a hidden prior (i.e., the uniform feature distribution assumption) used in the existing self-supervised pretraining methods for class-balanced datasets and real world class-imbalanced datasets. Moreover, a power-law distribution assumption to perfer long-tail prior is formulated for self-supervised pre-training and a method called Prior Matching for Siamse Networks (PMSN) is proposed. Extensive experiments are conducted on several benchmark datasets, showing that using the power-law distribution prior yields some improvements on class-imbalanced pretraining dataset (i.e. iNaturalist18) but leads to significant performance degeneration on class-balanced pre-training data (i.e., ImageNet). ",
            "strength_and_weaknesses": "The  strengths of the paper: \n\n+ The paper points out an interesting hidden prior used in the existing self-supervised representation learning methods and did extensive empirical evaluation to show the effects on down-stream classification task. \n\n+ MSN is extended Prior Matching for Siamense Networks (PMSN) to incorporate arbitrary prior. \n\n\nThe weaknesses of the paper: \n\n- While using a power-law distribution assumption to account for the long-tail class pre-training dataset leads to some improvements, it is somewhat marginal. However, it leads to significant performance degeneration when the pre-train dataset is class-balanced. In this sense, it potential value in application is limited. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and somewhat novel in understanding the self-supervised representation learning. The empirical evaluation is extensive and convincing. ",
            "summary_of_the_review": "The paper is clearly written and the experiments are extensive and convincing. The founding is novel and interesting. However, notice of that when using the long-tail class pre-training dataset and the power-law prior leads to marginal improvements, it potential value is limited. \n\n\n\n=====\nAfter reading the responses and the submission, the reviewer would like to increase the rating due to the contribution on implicit priors in self-supervised learning. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3748/Reviewer_vagN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3748/Reviewer_vagN"
        ]
    },
    {
        "id": "TUU6M3XILRo",
        "original": null,
        "number": 2,
        "cdate": 1666645345813,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645345813,
        "tmdate": 1666645345813,
        "tddate": null,
        "forum": "04K3PMtMckp",
        "replyto": "04K3PMtMckp",
        "invitation": "ICLR.cc/2023/Conference/Paper3748/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper points out that the overlooked uniform class prior would harm the learning of semantic representation for SSL methods, especially those with collapse prevention techniques, on class-imbalanced real-world datasets. The paper formulates and proves the objectives of the recent representative SSL methods to be essentially k-means problems with assumption of uniform prior. The paper shows the negative impact of uniform prior on class-imbalanced dataset through a comparative experiment on sampling strategies and visualizes the learned prototypes. The paper subsequently proposes a remedy by extending the objective of MSN through the recovering of the entropy term back to the relative entropy from an arbitrary prior to the average prediction. The arbitrary prior to the paper\u2019s interest is the power-law distribution which is often close to a typical real-world dataset. Alongside with a toy experiments to showcase the impact of different priors and mini-batch distributions, downstream tasks of classification and object distance detection are compared under different pretraining datasets and priors and shows the performance gain on class-imbalanced dataset with power-law prior. With different impacts of prior mismatch on uniform and non-uniform datasets, the author notes that the class-imbalanced pretraining issue is not completely solved.",
            "strength_and_weaknesses": "Strength:\n1. It is probably the first paper in SSL domain to classify and formulate the current typical SSL methods into more or less constrained k-means problems with detailed proofs. which can be referred to as a review from the theoretical perspective.\n2. The paper utilizes smartly designed experiments to demonstrate the negative impact of prior mismatch (in mini-batch) to SSL with volume maximization regularizers.\n3. Performance improvement of pretraining on class-imbalanced dataset is consistent on multiple classification downstream tasks.\n\nWeakness:\n1. Unfair comparison with different models on different pretext dataset. In Table 3. The model pretrained on iNat18 is ViT-S while the one on ImageNet-1K is ViT-B. The capacities of this two model are very different (the latter is 4 times larger than former i.t.o # parameters) compared to the size differences of the two dataset (roughly 2 times). To better demonstrate the gain of power-law prior on the class-imbalanced dateset, the model impact should be controlled.\n2. Limited gain on choosing long-tailed prior over uniform. As shown in Table 2 and Table 3, the performance improvement on classification is minor compared to the loss with power-law prior on balanced dataset. Looks like without the knowledge of class distribution, the uniform prior is a much safer option.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: 7\nQuality: 6\nOriginality: 7\nReproducibility: the source code isn't provided. The main algorithmic change is in Eq. 9, which seems straightforward to implement.",
            "summary_of_the_review": "The paper formulates current SSL as K-means problem and points out the overlooked uniform prior in those methods with volume maximization techniques and demonstrate that it can harm the model pretrained on class-imbalanced datasets, which is usually the case in real world. The proposed method is a straightforward extension of MSN. With power-law prior, it yields improved performance over uniform prior on class-imbalanced dataset. However, experiments in this paper also shows that the cost of the prior mismatch over balanced\ndataset is much more higher than over class-imbalanced dataset, which suggests the uniform prior is a safer prior when there is no access to the label. Furthermore, there is a potential of unfair comparison in the experiment reported Table 3.\n\nOn the whole, my recommendation of the paper is marginal accept",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3748/Reviewer_sHjw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3748/Reviewer_sHjw"
        ]
    },
    {
        "id": "X7tIbGd9rR",
        "original": null,
        "number": 3,
        "cdate": 1666668374910,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668374910,
        "tmdate": 1666668374910,
        "tddate": null,
        "forum": "04K3PMtMckp",
        "replyto": "04K3PMtMckp",
        "invitation": "ICLR.cc/2023/Conference/Paper3748/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper points out a phenomenon that typical self-supervised learning losses implicitly enforces a feature prior that forms clusters of roughly equal size. The authors empirically verifies that this behavior can hurt the performance if the data samples are class-imbalanced. Then the authors proposed a variant that assumes other prior class distribution (i.e. long tail distributions such as power-law), and demonstrated that this could remedy the problem.",
            "strength_and_weaknesses": "Strength:\n- Class imbalanced data is ubiquitous and deserves attention, especially for self-supervised setting where you don't know the data distribution.\n\nWeakness:\n- The only disappointment I had is that the authors only demonstrated the performance comparison for the MSN algorithm, where less deviation is needed. It would be more comprehensive if the authors can show how long tail distribution prior can be enforced in other algorithms.\n- The experiment is clear and convincing, but it seems only image modality is considered. I'm not sure how much this affects the generalizability of the problem/solution. \n- It would also be a good sanity check to visualize the features learned from class-balanced sampling vs class-imbalanced sampling.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well written with clear demonstration of the formulations, observations and solutions.\n- The quality and the novelty of this paper are good overall.\n- I'm confident to reproduce the paper's proposed algorithm.",
            "summary_of_the_review": "I like the simplicity and the insight of the paper. The only concern is that only one algorithm and its variant is demonstrated.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3748/Reviewer_1J6n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3748/Reviewer_1J6n"
        ]
    },
    {
        "id": "OumtEmOVPcw",
        "original": null,
        "number": 4,
        "cdate": 1666970990947,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666970990947,
        "tmdate": 1666970990947,
        "tddate": null,
        "forum": "04K3PMtMckp",
        "replyto": "04K3PMtMckp",
        "invitation": "ICLR.cc/2023/Conference/Paper3748/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper formulates several self supervised learning methods (VICReg, SimCLR, MSN, SwAV) with the volume maximization principle as variants of k-means, analyzes their difference on class-balanced and class-imbalanced data, and indicates that the uniform prior w.r.t. prototype vectors is the curse of performance degeneration of class-imbalanced data. For resolving this problem, this paper develops PMSN, an extension to MSN which replaces the uniform prior with a power-law distribution. Experiments verifies the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n1. The analysis of SSL methods with volume maximization principle on class-imbalanced data is novel and impressive\n2. Experiment analysis of the performance degeneration on class-imbalanced data is thorough and impressive\n3. The findings of the connection between SSL methods and k-means is insightful\n\nWeakness:\n1. The toy experiment can be further analyzed. From Figure 3, the power-law prior encourages the model to retain semantic features related to MNIST but discard semantic features related to CIFAR10 (the nearest neighbors have different CIFAR classes). It seems that the choice of prior decides which types of class conditional semantic features will be retained (uniform or power-law). Is it possible to retain semantic features for both MNIST and CIFAR (e.g., by using a mixture prior?). Hope to see more analysis.\n\n2. The power-law prior is only tested on MSN. As several volume maximization based SSL methods are discussed in the paper, a natural question is that whether introducing the power-law prior will be helpful for these method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and easy to follow. Details about experiment settings can be found in the paper.",
            "summary_of_the_review": "In general, this paper focuses on an important and interesting problem w.r.t. self supervised learning, and its findings are insightful. Experiments can be further improved to better support the claims and verifies theoretical analysis made in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3748/Reviewer_JLgU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3748/Reviewer_JLgU"
        ]
    }
]