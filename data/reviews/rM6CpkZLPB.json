[
    {
        "id": "KaKv29W2NK",
        "original": null,
        "number": 1,
        "cdate": 1666612197813,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612197813,
        "tmdate": 1666612197813,
        "tddate": null,
        "forum": "rM6CpkZLPB",
        "replyto": "rM6CpkZLPB",
        "invitation": "ICLR.cc/2023/Conference/Paper4481/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The author(s) extends the Moreu-envelop based personalized FL to the composite problems. Convergence analysis under certain assumptions are developed. Experiments on synethtic data and MNIST are conducted to evaluate the proposed algorithm.",
            "strength_and_weaknesses": "Pros:\n- The story line of the paper is clear and related works are well-addressed to my knowledge.\n\nCons:\n- The presentation of this paper should be improved: (1) the author(s) should use \\citep in many places, for example in the first paragraph. (ii) Figure 1,2,3,4,5,6 are not properly cropped. (3) Notations are kind of heavy, it is better to include a subsection for notations.\n- From the perspective of personalized FL, the paper seems to be a direct extension of [1] to composite problems. However, generalizing [1] to the composite problem does not seem to be a challenging task.\n- The author(s) spent much space for the theoretical analysis (page 5 and page 6). However, Theorem 1 does not seem to be a particularly strong results, the algorithm uses second order information and does not coverge faster.\n- Using second order information (the Hessian matrix) is expensive in practice, this issue is only briefly mentioned above Section 3. It is better to discuss more on how to efficiently leverage the second order information, i.e., how to do the Hessian-vector product efficiently and what is the time complexity of the proposed algorithm.\n- Experiments are also not very convincing, the datasets are mostly aftifically generated, the only real dataset is the MNIST dataset. It is more convincing the evaluate the proposed algorithm on more real-world datasets.\n\n\n[1] Canh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau envelopes. Advances in Neural Information Processing Systems, 33:21394\u201321405, 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The writting of the paper should be improved; novelty is maringal; reproducibility is unclear;",
            "summary_of_the_review": "The presentation of the paper should be improved. On the theory side, the contribution of the paper is kind of marginal compared with the exiting Moreu-envelop based personalized FL; on the empirical side, the experiments are mostly on synthetic and tiny datasets. The author(s) should consider improving either from the theory or the empirical aspect.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4481/Reviewer_WBRe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4481/Reviewer_WBRe"
        ]
    },
    {
        "id": "d9kAlkR7e5",
        "original": null,
        "number": 2,
        "cdate": 1666666953046,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666953046,
        "tmdate": 1670818849235,
        "tddate": null,
        "forum": "rM6CpkZLPB",
        "replyto": "rM6CpkZLPB",
        "invitation": "ICLR.cc/2023/Conference/Paper4481/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a personalization federated learning method (i.e., pFedFBE) by using forward-backward envelope as clients\u2019 loss functions. It provided the convergence analysis  of the proposed method, which shows the same convergence complexity results as FedAvg for FL with unconstrained smooth objectives. Numerical experimental results demonstrate the effectiveness of the proposed pFedFBE method.",
            "strength_and_weaknesses": "Strength:\n\nThis paper proposed a personalization federated learning method (i.e., pFedFBE) by using forward-backward envelope as clients\u2019 loss functions. It provided the convergence analysis of the proposed method, which shows the same convergence complexity results as FedAvg for FL with unconstrained smooth objectives.\n\nWeakness:\n\nThe novelty of this paper is limited. This paper basically follows the existing methods and  theoretical analysis such as in (https://arxiv.org/pdf/2011.08474.pdf).",
            "clarity,_quality,_novelty_and_reproducibility": "The writing can significantly be improved. \nThe novelty of this paper is limited.  This paper basically follows the existing methods and \ntheoretical analysis such as in (https://arxiv.org/pdf/2011.08474.pdf).",
            "summary_of_the_review": "This paper proposed a personalization federated learning method (i.e., pFedFBE)  by using forward-backward envelope as clients\u2019 loss functions. It provided the convergence analysis  of the proposed method, which shows the same convergence complexity results as FedAvg for FL with unconstrained smooth objectives. Numerical experimental results demonstrate the effectiveness of the proposed pFedFBE method.\n\nSome Comments:\n\n1. Assumption 1-(A3) is stronger than the basic assumptions in the existing FL methods such as FedAvg.\n\n2. It would be great if the authors would detail the tuning parameters in the experiments.\n\n\n------------------------------------------------------------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------------------------------------------------------------\n\nThe authors still did not solve my main concern: the limited novelty of this paper. So I support to reject this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4481/Reviewer_4Vq3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4481/Reviewer_4Vq3"
        ]
    },
    {
        "id": "O37U9sB20y",
        "original": null,
        "number": 3,
        "cdate": 1666676884640,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676884640,
        "tmdate": 1666676884640,
        "tddate": null,
        "forum": "rM6CpkZLPB",
        "replyto": "rM6CpkZLPB",
        "invitation": "ICLR.cc/2023/Conference/Paper4481/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes federated composite optimization, i.e., the nonconvex minimization of sum_i f_i + h in a federated setting. Here, f_i is smooth, h is not smooth but proximable. Note that the f_i are not assumed to be equal to each other (heterogeneous data).\n\nThe Forward Backward Envelope (FBE) of f_i + h is a function F_i s.t. one gradient step on F_i is roughly equivalent to one Forward Backward step for f_i + h (it involves the Hessian though). Similarly to the Moreau envelope, the FBE is smooth and depends on a parameter lambda. The paper proposes to minimize F = sum F_i. Tuning lambda allows for more federation or more personalization.\n\nTo minimize F, one can use basically any Federated Learning algorithm since the F_i are smooth. \nThe main theorem upper bounds the squared norm of the gradient of F evaluated at the federated iterate, along the algorithm.",
            "strength_and_weaknesses": "Weaknesses\n\n- The gradient of the FBE is actually not a FB step, because it requires the Hessian of the smooth function. So the methods based on the FBE require the Hessian.\n\n-I think that the assumptions A2 and A3 are too strong. They basically say that the first second and third derivatives are bounded. Usually, the smooth term is gradient Lipschitz and we do not assume Lipschitzness of the nonsmooth term. The third derivatives are needed because the Hessian appears in the algo.\n\n- The approach based on the FBE is not justified. Is it a good model to minimize F instead of the true loss f? Why won't we apply a federated version of the Forward Backward algorithm for example? (and then take one additional FB step for personalisation?)\n\n\nStrength\n\n- Numerical exp show the superiority of the approach over concurrent methods in several contexts.\n\n- The F_i is smooth, so the model can be solved by any FL algorithm which gives a lot of flexibility. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality: \n\nOverall the paper is well written. However, the paragraph after Eq 8 could be clarified. \n\nBesides, it is not clear that if lambda = 0 then there is more personalisation. Indeed, the objective becomes constant in this case. \n\nThe presentation of personalisation vs federation is a bit confusing and is not discusses theoretically. The main theorem deals with the federated iterate. More generally, one would need more justifications for the approach. \n\nNovelty:\n\nThe paper provides one way to deal with composite functionals in a Federated setting, a problem which seems to be new. Otherwise, the proof technique seems rather standard given the assumptions. I mean that, once the model F = sum F_i is formed, the reminder seems to follow from existing analyses in 1. the FBE 2. the FL algorithm used to minimize F. However, the numerical experiments show that at least in practice the approach performs well.\n\nMINOR\n\n\"Bregman distrance\"\n\n \"unbaised\"\n\n\"Caculate\"\n\n\" standrad\"",
            "summary_of_the_review": "This paper proposes a meta algorithm to solve composite problems in a federated setting. This meta algorithm relies on minimizing the sum of the FBEs with any FL algorithm, because the FBEs are smooth. Once the approximated problem with FBEs is formed, the paper forgets about the original problem (except in the numerical experiment section).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4481/Reviewer_3u8w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4481/Reviewer_3u8w"
        ]
    },
    {
        "id": "V-HpowXYQPX",
        "original": null,
        "number": 4,
        "cdate": 1666805167529,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666805167529,
        "tmdate": 1666805167529,
        "tddate": null,
        "forum": "rM6CpkZLPB",
        "replyto": "rM6CpkZLPB",
        "invitation": "ICLR.cc/2023/Conference/Paper4481/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a novel personalized method for FCO via the forward-backward envelope. Personalized models are firstly updated, then the local models are updated with Second-order optimization. After the local update, the global update is done via FedAVG. Convergence results of the proposed algorithm are shown under similar assumptions in pFedMe. Numerical experiments on the federated lasso, federated matrix completion, and nonsmooth deep neural network were done.",
            "strength_and_weaknesses": "Strength:\nthe topic is important for distributed learning and federated learning. This paper proposed an interesting method to obtain explicit forms of gradients. A convergence proof is performed to justify the algorithm's efficacy. The paper is not hard to follow.\nWeaknesses:\nMy major concern is the novelty. It seems that the proposed method uses the framework of pFedMe but estimates the local gradient using an explicit form, plus additional computation on the Hessian matrix. Although the experimental result of FedFBE shows faster convergence over other methods, it is not fully convincing to say that FedFBE has higher efficiency. Estimating the Hessian matrix definitely helps with convergence, but it also leads to the consumption of much more computation. pFedMe doesn't use the explicit form partly because it can save computational power by avoiding that Hessian matrix. Therefore, I am not sure that the problem will be solved without creating another bigger problem. Thus, the novelty in terms of the solution technique is thus limited.\nI wonder about the influence of hessian components of the proposed method on the final results. Some ablation studies can be done to justify the efficacy of FBE.",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is written clearly and easy to follow. The authors can highlight their contributions better by stressing the differences between FedFBE and pFedMe in the algorithm derivation and convergence proof.",
            "summary_of_the_review": "Generally speaking, the topic of the paper is of importance and interest. However, the authors can improve the work by addressing the above-mentioned concerns, especially about the computational efficiency and the ablation studies in the second-order info in the optimization.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4481/Reviewer_bfG5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4481/Reviewer_bfG5"
        ]
    }
]