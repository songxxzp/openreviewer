[
    {
        "id": "6rPa7W7S1l-",
        "original": null,
        "number": 1,
        "cdate": 1666511514779,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666511514779,
        "tmdate": 1667500192378,
        "tddate": null,
        "forum": "l0mX03b3UZv",
        "replyto": "l0mX03b3UZv",
        "invitation": "ICLR.cc/2023/Conference/Paper388/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies well-known Adam algorithm. Previous work shows that Adam can converge to a neighborhood of stationary point if the gradients are bounded (functions are smooth). In this paper, the authors show that, Adam can guarantees a even better results under a milder condition, i.e., L_0,L_1 smoothness, under which the gradients can be unbounded. Moreover, SGD may diverge under this condition. \n\n\n",
            "strength_and_weaknesses": "I have the following problems/concerns, and I hope the authors could help me understand these points:\n\n\na) For Theorem 1, I have the following problems/concerns:\n\n1.\tRandomness of the algorithm: In page 4, the authors say that the algorithm is actually a double-loop algorithm with stochastic shuffling. However, Theorem 1 holds without expectation or high probability, which is odd. Moreover, in which part of the proof the shuffling is used? The authors say \u201crandom shuffling is not a key factor in our analysis. The proof also works for other settings.\u201dCan the authors be more specific about what are the \u201cother settings\u201d? Finally, isn't shuffling also a modification of Adam, which makes the algorithm not a pure Adam algorithm?\n\n2.\tDependance on n and d: From the Appendix, it seems that the upper bound is linear wrt n and d. Can the authors be clearer about this? \n\n3.\tThe inequality in (4): It is not clear to me why we can get this inequality. Where is rootd and n? Moreover, delta is upper bounded by 1/sqrt{D_1}. Thus, when D_1=0 in Assumption 2 (which only means the sum of the norm of the gradients is upper bounded), delta will explode? \n\n\n4.     Theorem 1 does not assume beta_2<1, which seems incorrect. \n\n\nb) **I have the following major concern** about writing: After a comparison, I find that many paragraphs here are basically copied/paraphrased from a previous paper (Zhang et al., 2022). For example, one can compare the two paragraphs of this paper under Assumption 2 and those under Assumption 2.2 of Zhang et al, (2022). Although the two papers are strongly related, I do not think this way of writing is appropriate. \n\nc. Other problems: \n\n\n1. Proof of Lemma 1: second equality: I do not see how to obtain this equality. Why is v_l (second order momentum of the l-th dimension) related to the max_{[n]} of abs(gradient)? \n\n\n2. Relation between Assumptions 1 and 2: when n=1, then Assumption 2 implies \\emph{bounded gradient} when D_1\\in(0,1] (it can also extend to n with D_1\\leq 1/n). So Assumption 1 only makes sense when D_1 is large. \n\n3. In page 17, it seems that all C_i are non-decreasing wrt n_1. So in this case why don't one just set eta_1=0? Is there a term related to 1/eta_1?\n\n4. The experiments are not very sufficient. As mentioned before, the authors did make some modifications to Adam (the shuffling and double-loop), which makes it not exactly the original Adam.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I have several concerns about this paper, which are listed above. ",
            "summary_of_the_review": "I do like this paper, but I have several concerns about this paper, which are listed above. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper388/Reviewer_z7aZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper388/Reviewer_z7aZ"
        ]
    },
    {
        "id": "46tkojP2ziy",
        "original": null,
        "number": 2,
        "cdate": 1666595318229,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666595318229,
        "tmdate": 1666595464436,
        "tddate": null,
        "forum": "l0mX03b3UZv",
        "replyto": "l0mX03b3UZv",
        "invitation": "ICLR.cc/2023/Conference/Paper388/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors mainly prove the convergence of Adam under the (L0,L1)-smoothness condition which is borrowed from existing work. Moreover, they use examples to show that GD and SGD can converge much slower than Adam under this assumptions. The reason behind for this advantage of Adam contributes to its usage on the local smoothness which is called \u201cadaptivity\u201d. ",
            "strength_and_weaknesses": "Strength: \nThere are main two contributions in this work. \n1)It proves the convergence of Adam under the (L0,L1)-smoothness condition.\n\n2)Under the same assumption, it uses examples to show the advantage of Adam over GD and SGD.\n\nWeaknesses:\n1)The main assumption is borrowed from other works but is actually rarely used in the optimization field. Moreover, the benefits of this assumption is not well investigated. For example, a) why it is more reasonable than the previous one? B) why it can add  gradient norm L_1||\\nabla f(w_1)|| in Eqn (3) or why we do not add other term?  It should be mentioned that a milder condition does not mean it is better, since it may not reflect the truth. For me, problem B) is especially important in this work, since the authors do not well explain and investigate it. \n\n2)Results in Theorem 1 show that Adam actually does not converge, since this is a constant term O(D_0^{0.5}\\delta) in Eqn. (5). This is not intuitive, the authors claim it is because the learning rate may not diminish. But many previous works, e.g. [ref 1],  can prove Adam-type algorithms can converge even using a constant learning rate.  Of course, they use the standard smooth condition. But (L0,L1)-smoothness condition should not cause this kind of convergence, since for nonconvex problem, in most cases, we only need the learning rate to be small but does not care whether it diminishes to zero. \n\n[ref 1] Dongruo Zhou, Jinghui Chen, et al. On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization\n\n3)It is not clear what are the challenges when the authors analyze Adam under the (L0,L1)-smoothness condition. It seems one can directly apply standard analysis on the (L0,L1)-smoothness condition.  So it is better to explain the challenges, especially the difference between this one and Zhang et al. \n\n4)Under the same assumption, the authors use examples to show the advantage of Adam over GD and SGD. This is good. But one issue is that is the example reasonable or does it share similar properties with practical problems, especially for networks. This is important since both SGD and ADAM are widely used in the deep learning field.\n\n5)In the work, when comparing SGD and ADAM, the authors explain the advantage of adam comes from the cases when the local smoothness varies drastically across the domain. It is not very clear for me why Adam could better handle this case. Maybe one intuitive example could help. \n\n6)The most important problem is that this work does not provide new insights, since it is well known that the second order moment could help the convergence of Adam. This work does not provide any insights beyond this point and also does not give any practical solution to further improve. ",
            "clarity,_quality,_novelty_and_reproducibility": "For writing, most parts of this work are well-written and clear. But some key problems, such as the above ones,  are not well explained. \n\nFor novelty, since the key assumption comes from others, and it seems standard optimization can be directly adopted here, the novelty of this work is not high. The most severe issue is that it does not provide new insights since it is well-known that the second-order moment could help the convergence of Adam.\n",
            "summary_of_the_review": "Although this work provides some good theoretical analysis under the (L_0, L1) smoothness assumptions, it does not provide good new insights and practical improvements.  Moreover, some key points are not well explained. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper388/Reviewer_8ES2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper388/Reviewer_8ES2"
        ]
    },
    {
        "id": "ptlQczSIXLS",
        "original": null,
        "number": 3,
        "cdate": 1666662384431,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662384431,
        "tmdate": 1666662384431,
        "tddate": null,
        "forum": "l0mX03b3UZv",
        "replyto": "l0mX03b3UZv",
        "invitation": "ICLR.cc/2023/Conference/Paper388/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Adam is a popular optimizer for training neural networks in deep learning. This paper analyzes the convergence of Adam under the so-called ($L_0, L_1$)-smooth condition, and shows that Adam has a convergence guarantee without the bounded gradient assumption. They also provide counter-examples where GD/SGD can diverge under the ($L_0, L_1$)-smooth condition. A toy example is used to test the main theory. ",
            "strength_and_weaknesses": "Strengths:\n\nS1. The work establishes the convergence property of Adam under the ($L_0, L_1$)-smooth condition that is weaker than the $L$-smooth condition.\nS2.  It provides counter-examples where GD/SGD can diverge under the ($L_0, L_1$)-smooth condition, which shows the advantage of Adam over GD/SGD.\n\nWeakness:\n1. The main issue lies in the experimental part although the main point of the paper is its theoretical analysis. At the beginning of the paper, it mentions the examples of deep neural networks that satisfy ($L_0, L_1$)-smoothness requirements. But, the experiments only consider a quadratic function that is also $L$-smooth. It should add some more complex experiments that match the motivations of the paper.\n\n2. The paper should give a detailed comparison with recent related works including [1,2,3,4,5]. In [1], similar convergence results are established under the same choice of ($\\beta_1,\\beta_2$). It should clarify the technical difficulty under this new smoothness constraint. In [2], the weakness of SGD under the ($L_0, L_1$)-smooth condition has been studied. Though this work considers the diminishing stepsize, the significance should be discussed. The potential function in Section~4.2 is a widely-used trick in the analysis of the momentum-based methods, e.g., see [3,4,5]. The authors could provide some intuitive ideas behind the construction of the potential function and analyze its difference from other momentum-based methods.\n\n3. More theoretical comparisons between Adam and GD/SGD can be discussed.:1) both GD/SGD and Adam use the constant learning rate; 2) If the hyperparameters of Adam are chosen improperly, the convergence of Adam can be arbitrarily slow; 3) The convergence rate of Adam in Theorem~1 depends on the dimension $d$, while GD/SGD does not depend on it.\n\n4. As the paper contains many lemmas and theorems. It can be better organized for readers, especially for the appendix part. More details can be added for some inequalities e.g. the first inequality in the proof of Lemma 2.\nIn the second paragraph on Page 1, the statement ``larger $\\beta_1$ and $\\beta_2$ will bring more historical information into the update'' is rather vague. The authors can add explanations for the historical information.\n\n5. It could be good if the author can add some discussions on below questions:\n\n(1) The theorem suggests that $\\beta_2 = 1$ is superior for Adam since $\\delta(\\beta_2) = 0$ in this case. Does it mean $\\beta_2$ is redundant for Adam? If not, can you provide more evidence of the effect of $\\beta_2$ in theory or with experiments?\n\n(2) In practice, it is common to use a constant learning rate in a stage and it is known that GD converges with constant stepsize. So what is the convergence result of Adam when a constant learning rate is used?\n\n(3) Since gradient clipping is proven to be beneficial for GD/SGD [2], what is the advantage of Adam over the GD/SGD with gradient clipping or normalized gradient?\n\n(4) Since pretrained model is common, where the initialization is good, what is the advantage of Adam over SGD in this case? Note that in the last equality of Page 7, it may assume that $\\|\\nabla f(w_{k,0})\\|_2$ is large, so the cubic term of the gradient is dominant. \n\nBut, in practice, when the initialization is good, i.e. $\\|\\nabla f(w_{k,0})\\|_2$ is small, more discussion can be added.\n\n(5) In Page~7, the paragraph above Inequality (8): ``further notice that $u_{k,i}$ is close to $w_{k,i}$''. Can you give more explanation of this insight?\n\n\n[1] Yushun Zhang, et al. Adam can converge without any modification on update rules. Advances in Neural Information Processing Systems, 2022.\n\n[2] Jingzhao Zhang, et al. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations, 2019.\n\n[3] Euhanna Ghadimi, et al. Global convergence of the Heavy-ball method for convex optimization.  European Control Conference, 2015.\n[4] Yanli Liu et al. An Improved Analysis of Stochastic Gradient Descent with Momentum. Advances in Neural Information Processing Systems, 2020.\n\n[5] Xiangyi Chen, et al. On the convergence of a class of Adam-type algorithms for non-convex optimization. International Conference on Learning Representations, 2019.\n\n[6] Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. RMSProp converges with proper hyper-\nparameter. In International Conference on Learning Representations, 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The three paragraphs after Assumption 2 are very similar to the paragraphs after Assumption 2.2 in [1]. The authors should rephrase these parts.",
            "summary_of_the_review": "The paper works on an important problem related to the convergence of Adam and shows the potential advantages over GD/SGD. I hope the authors add more experiments and more analysis in the revision for better reading and deeper understanding.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper388/Reviewer_8djk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper388/Reviewer_8djk"
        ]
    },
    {
        "id": "zY-B92Ipmdp",
        "original": null,
        "number": 4,
        "cdate": 1667175304710,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667175304710,
        "tmdate": 1670932071836,
        "tddate": null,
        "forum": "l0mX03b3UZv",
        "replyto": "l0mX03b3UZv",
        "invitation": "ICLR.cc/2023/Conference/Paper388/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "ADAM has been observed to have better convergence properties than SGD in property, but the existing theoretical analysis cannot distinguish them, under the standard L-smooth setting. This paper considers a strictly broader class of loss functions, that is, the functions satisfy $(L_0, L_1)$-smoothness (Zhang et al., 2019), and rigorously show that ADAM can find an approximate first-order stationary point for any $(L_0, L_1)$-smooth loss function, while there is some $(L_0, L_1)$-smooth function that SGD will diverge.\n",
            "strength_and_weaknesses": "##Pros:\n\n1. This paper tackles an important problem in optimization theory, regarding the superior convergence of ADAM over SGD. Despite the empirical success of ADAM, no theoretical separation on convergence rate has been made before. This paper first made such separation rigorously, in the $(L_0,L_1)$-smooth setting, which is arguably more realistic than the standard $L$-smooth setting for deep learning loss. \n\n2. This paper is well-written and easy to follow. I really appreciate the effort made by the authors to clearly explains the motivation, setting and proof ideas. \n\n3. The paper also points out a necessary condition for ADAM to converge, that is, $\\beta_2$ has to be sufficiently close to $1$. This is reflected through both  their upper bounds for gradient norm and their simulation.\n\n4. Though the indivual insights from the proofs are not compeletely novel, like the construction of the new poenntial function highlighted in Insight 1 is well-known (e.g., Ghadimi et al. 2015, equation (7)), combnining those technques and applying them correctly on a new setting is still non-trivial and needs a lot of effort.\n\n##Cons:\n\n1. Momentum seems not useful at all in the analysis. Indeed the existence of momentum seems only to make the analysis more complicated. To achieve the separation against SGD, it seems RMSProp (setting $\\beta_1=0$) is already enough. I note the authors do mention understanding the effect of momentum in future work, but it would be more useful to spell out or give a related discussion in the main paper as it can help the readers to understand.\n\n2. In the abstract, the authors claim that \"Adam can adapt to local smoothness, justifying Adam's adaptivity\". It is not clear what this sentence actually means in the algorithm level, e.g., does it mean the effective lr always maintains a certain relationship to the hessian? It seems the adaptivity of the moment estimation in ADAM holds by definition. This paper is really about the benefit of adaptivity. Same issue exists for the title, that is, instead of provable adaptivity, it should be the provable benefit of adaptivity.\n\n2. It is not clear to me whether Figure 1(b) satisfies the claimed relationship of 'local smoothness = $e \\times$ gradient norm'. It seems to me the slope is larger than 1, e.g., the right-top point is close to (1.0,2.5). I suggest the authors run a linear regression to decide the slope, which would be more convincing.\n\n##Detailed comments:\n\n1. In modern networks like ResNet and Transformer, normalization layers are ubiquitous. But unfortunately, the normalization layers doesn't quite satisfy the $(L_0,L_1)$-smoothness, because if one keeps the parameter before some normalization layer while keeping its direction, the gradient is proportional to the inverse of parameter norm and the hessian is proportional to the inverse of the squared parameter norm, which means that the local smoothness of any neural networks grows provably at the order of squared gradient norm. Thus a related question is, what is the limit of the current approach? Can it be used to deal with an even broader class of loss functions, which allow the smoothness to grow as a higher degree polynomial of gradient norm? \n\n2. typo, in Assumption 1, \"there exists positive constants\" should be there exist.  \n\n\n3. typo, on page 5, \"Assumption 2 is quite general. When $D_1 = 1/n$\". I think it should $D_1=0$.\n\n#References\n\n- Ghadimi, Euhanna, Hamid Reza Feyzmahdavian, and Mikael Johansson. \"Global convergence of the heavy-ball method for convex optimization.\" In 2015 European control conference (ECC), pp. 310-315. IEEE, 2015.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. I really appreciate the effort made by the authors to clearly explains the motivation, setting, and proof ideas. I didn't check the proofs in the appendix due to the limited time fo reviewing, but the sketch in the main paper looks very reasonable.",
            "summary_of_the_review": "This paper studies an open problem that lies at the core of the interest of the ICLR community, that why ADAM can have better faster convergence than SGD. The authors give a theoretical rigorous treatment for convergence of ADAM to approximate the first stationary point under the recent proposed $(L_0,L_1)$-setting, where SGD is not guaranteed to converge, even with $1/\\sqrt{t}$ learning rates. This is a good paper and thus I suggest acceptance.\n\n\n======= \n\nUpdate after rebuttal: After discussion with AC, I realize that the current bound in Theorem 1 also hides factors like $poly(1/(1-\\beta_2))$ in $O(\\cdot)$. This dependency should be mentioned explicitly and discussed in detail because it affects the optimal choice of $\\beta_2$. Therefore I decreased my score to 6.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper388/Reviewer_Ydg4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper388/Reviewer_Ydg4"
        ]
    }
]