[
    {
        "id": "A16ETga1IcQ",
        "original": null,
        "number": 1,
        "cdate": 1666709866268,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666709866268,
        "tmdate": 1666709866268,
        "tddate": null,
        "forum": "rMkd7_6fB7",
        "replyto": "rMkd7_6fB7",
        "invitation": "ICLR.cc/2023/Conference/Paper3694/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a framework to describe the capacity of KRR to generalize different types of functions. An ability to generalize a function f is measured by the so called learnability (a measure that has a version that depends on training set inputs as well as a version that is expectation of the latter value). A key result is Theorem 3.2 that formulates a conservation of learnability. Authors claim that this result is similar in spirit with well-known \u201cno-free-lunch\u201d theorems of Wolpert.",
            "strength_and_weaknesses": "Among experimental results presented, the first one, described in Fig 1, seems to me the most valuable. The fact that even for moderate size widths, neural networks almost follow the same conservation of learnability, seems surprising (or this is true only for small n?). Why does this happen? IMHO, this should be discussed deeper, some additional comments are needed. Does this mean that for any architecture, from NTK one can correctly predict which functions are better learnable? This would be a very strong statement.\n\nSection 6.1 is not self-contained. To understand it one needs to read citations given there. E.g. from the given explanation what is deep bootstrap phenomenon is not clear. The effective training time is not defined and etc.\n\nIn Section 6.2 it is claimed that the result generalizes  Bengio et al. (2006) to more general kernels. It would be interesting if authors discuss also, how different are required training set sizes according to Bengio et al. (2006) and in authors' analysis. How these two approaches are different.\n\nQuantum mechanical analogy is not convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presents an original framework. Certain aspects looks so optimistic (like experiments of Fig.1), that I even start to doubt claims.",
            "summary_of_the_review": "Though some claims are not substantiated, a novelty of the framework is a strong side of the contribution.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3694/Reviewer_F6bZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3694/Reviewer_F6bZ"
        ]
    },
    {
        "id": "ZEvWGiAQbm",
        "original": null,
        "number": 2,
        "cdate": 1666816057204,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666816057204,
        "tmdate": 1666816057204,
        "tddate": null,
        "forum": "rMkd7_6fB7",
        "replyto": "rMkd7_6fB7",
        "invitation": "ICLR.cc/2023/Conference/Paper3694/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents simplified  closed-form estimates for the test risk and other generalization\nmetrics of kernel ridge regression. Compared to prior work, the authors claim their derivations\nare greatly simplified and lead to higher interpretability. Test risk and other objects of interest \nare expressed in a transparent, interpretable way in terms of the conserved quantity evaluated \nin the kernel eigenbasis. Among others, the main contributions include a theoretical explanation\nfor the \u201cdeep bootstrap\u201d of Nakkiran et al. (2020), as well as a generalization of a result regarding \nthe hardness of the classic parity problem.",
            "strength_and_weaknesses": "-) The paper is well-written and clear (apart from parts of the text which are made more complex than necessary using viewpoints borrowed from physics). \n-) The experimental results are interesting and carry the main message with sufficient clarity. \n-) Figure 1. Please explain what 1HL and 4HL mean.\n-) The paper considers some concepts from a physics perspective which might be hard to follow for people without this background.\n-) I do not fully understand why Theorem 3.2 is a \"Conservation Law\". My best understanding is the authors use this phrase because \nthe learnability remains fixed (in expectation) as the dataset evolves? This term is confusing, and rather un-necessary. For example, I can reach the global minimum of a strictly convex problem (assuming there is one) for any starting point. Is this a conservation law?\n-) it would be nice to add more details regarding the training phase with eigenfuctions.",
            "clarity,_quality,_novelty_and_reproducibility": "No issue.",
            "summary_of_the_review": "To the best of my knowledge the theory appears correct and the analysis reasonably novel. I choose \"marginally above threshold\" but I am happy to change my score if the authors provide more clarity.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3694/Reviewer_R9g9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3694/Reviewer_R9g9"
        ]
    },
    {
        "id": "7kVF0ieUBR",
        "original": null,
        "number": 3,
        "cdate": 1666937864047,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666937864047,
        "tmdate": 1666942835476,
        "tddate": null,
        "forum": "rMkd7_6fB7",
        "replyto": "rMkd7_6fB7",
        "invitation": "ICLR.cc/2023/Conference/Paper3694/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This submission computes the generalization error of kernel ridge regression (KRR) which depends on the eigenspectrum of the kernel and the decomposition of the target function onto the kernel eigenbases. The derivation is based on a Gaussian equivalence assumption, and the authors presented various interpretation of the formulae via the notion of learnability, which is the inner product between the KRR estimator and some given test functions. The theoretical predictions are empirically validated on synthetic and real-world datasets. ",
            "strength_and_weaknesses": "## Strength\n\nThe derived generalization error formulae lead to a few interesting findings. \n\n- A heuristic explanation of the deep bootstrap framework using the approximate equivalence between early stopping in gradient descent and ridge regularization. \n- A learning lower bound for kernel methods on the parity problem. \n- An analytic expression of the expected gradient norm of the estimator, which can be seen as a measure of robustness. To my knowledge this computation has not appeared in prior works. \n\n## Weaknesses \n\nMy main concern is that the theoretical contribution in this submission is limited. \n\n1. The generalization error formulae is derived from assuming the kernel features are Gaussian. This reduces the problem to linear regression on Gaussian features, for which the same set of equations has been rigorously proved and extensively studied in many prior works using random matrix theory, see [Hastie et al. 2019] [Wu and Xu 2020] [Richards et al. 2020]. These existing results are not thoroughly discussed in this submission, and I do not see the value of rederiving the same result using a non-rigorous approach (in fact, one could argue that the Stieltjes transform-based derivation is cleaner). \n\n2. The \"no-free-lunch\" implied by the conservation of learnability is not surprising. Given a kernel with certain spectral bias, we should not expect it to perform well on all target functions. Moreover, analogues of \"the recent folklore intuition that, given n samples, KRR and wide NNs can reliably learn at most O(n) orthogonal functions\" have been rigorously shown in [Ghorbani et al. 2019] [Mei et al. 2021]. \n\n3. The applications of the error formulae in Section 6 are also not entirely new. \nThe explanation of \"deep bootstrap\" due to spectral filtering has already appeared in [Ghosh et al. 2021]. And the parity lower bound is a known result from various works such as [Daniely and Malach 2020] [Kamath et al. 2020]. \n\n4. The empirical validation of the generalization error formulae on neural network features has been done in [Loureiro et al. 2021]. Also, unlike the cross-validation estimators, the omniscient risk estimate studied in this submission requires access to \"population\" statistics, which can be computationally demanding in real-world applications. \n\nGhorbani et al. 2019.  Linearized two-layers neural networks in high dimension.  \nHastie et al. 2019. Surprises in high dimensional ridgeless least squares interpolation.  \nDaniely and Malach 2020. Learning parities with neural networks.  \nWu and Xu 2020. On the optimal weighted $\\ell_2$ regularization in overparameterized linear regression.  \nRichards et al. 2020. Asymptotics of ridge (less) regression under general source condition.  \nKamath et al. 2020. Approximate is good enough: probabilistic variants of dimensional and margin complexity.  \nMei et al. 2021. Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration.  \nLoureiro et al. 2021. Learning curves of generic features maps for realistic datasets with a teacher-student model.  \nGhosh et al. 2021. The three stages of learning dynamics in high-dimensional kernel methods. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity & Quality:**\n- The generalization error computation relies on a Gaussianity assumption (and a truncation heuristic) which is not mentioned in the main text. Appendix A only discusses the spectrum universality of kernel matrices, which does not necessarily imply the equivalence in the training/test loss. The authors should look into [Hu and Lu 2020] and references therein.  \nHu and Lu 2020. Universality laws for high-dimensional learning with random features. \n\n-  For MNIST and CIFAR data, can the authors comment on how the number of eigenvalues $M$ is selected, and how the eigencoefficients of the target function are computed from data?\n\n**Novelty:** see weaknesses above. \n\n**Reproducibility:** N/A. ",
            "summary_of_the_review": "In my opinion this submission is below the acceptance bar due to the incremental theoretical contribution. I will consider updating my evaluation if the authors can adequately discuss the prior results and clarify the novelty in their theoretical analysis. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3694/Reviewer_zjAQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3694/Reviewer_zjAQ"
        ]
    },
    {
        "id": "lWf28q7Hi0B",
        "original": null,
        "number": 4,
        "cdate": 1666943792905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666943792905,
        "tmdate": 1670694221600,
        "tddate": null,
        "forum": "rMkd7_6fB7",
        "replyto": "rMkd7_6fB7",
        "invitation": "ICLR.cc/2023/Conference/Paper3694/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a quantity called the \"learnability\" of a kernel, and shows that this satisfies a \"conservation law\". This is applied in four scenarios:\n\ni) to explain \"deep bootstrap\" of Nakkiran et al\nii) to prove a result on the hardness of the parity problem by kernels\niii) to study adversarial robustness\niv) to investigate the relation between kernel ridge regression and quantum mechanics.",
            "strength_and_weaknesses": "## Strengths\nThis paper provides a convenient heuristic to predict the test MSE of a kernel ridge regression (and its bias and variance), as well as the train MSE. The authors use it to power several applications. The most interesting application is perhaps an explanation of the deep bootstrap effect . The authors' analysis extends results by Ghosh et al. 2022 that apply to spherically-distributed data, to any kernel with power law eigenvalue \ndecay.\n\n## Weaknesses\nThere are some weaknesses that the authors should address.\n### Level of formality\nIt is unclear what is proved and what is not. For example, in equations (6) through (14) it is not clear what is proved in the main text. The arguments in appendices H.6 and H.7 are informal, which is fine, but I feel that it should come with a disclaimer in the main text.\n\n### Novelty of the framework\nBe more specific in comparison to Canatar et al. '21 and Jacot et al. '20? It seems similar to the former.\n\n### Novelty of the conservation law\nThe main conservation law, Theorem 3.2, essentially follows from the representer theorem for kernels:\ngiven training data points $(x_i,y_i)\\_{i \\in [n]}$, kernel ridge regression will learn a function of the form $\\hat{f}(x) = \\sum_{i=1}^n \\alpha_i K(x,x_i)$, for some scalars $\\alpha_i$. Therefore, the best one can do to minimize MSE is to learn the projection of the target function $f$ onto this $n$-dimensional space $V$ spanned by the functions $K(x,x_i)$ for $i \\in [n]$.\n\nSo (when $\\delta = 0$) the \"learnability\" of $f$ is $\\mathcal{L}(f) = \\langle f, \\Pi_V f\\rangle = ||\\Pi_V f||^2$. The conservation law states that for any \northonormal functions $f_1,\\ldots,f_m$ we have $\\sum_{i=1}^m ||\\Pi_V f_i||^2 \\leq n$. However, this can be seen by writing $||\\Pi_V f_i||^2 = \\sum_{j=1}^n \\langle v_j, f_i \\rangle^2$, where $v_1,\\ldots,v_n$ is an orthonormal basis of $V$, and switching order of summations. (Note this is essentially the same proof as remarked in Section G.1.)\n\nThe above proof is the basis for the kernel lower bounds technique surveyed in the note by Daniel Hsu https://www.cs.columbia.edu/~djhsu/papers/dimension-argument.pdf, which extends beyond $f_1,\\ldots,f_m$ being orthonormal. It has been the basis for many recent lower bounds for kernels, and is not new.\n\n### Application 2: Hardness of parity problem by rotationally-invariant kernels\nThis result is known (see the Daniel Hsu note). Note that it is true for permutation-invariant kernels (which are a superset of rotationally-invariant kernels). \n\n### Application 3: Adversarial robustness\nPlease specify what the function $f$ is. Is it just $\\mathrm{sgn}(x_1)$, or a smoothed version?\n\nPredicted function smoothness does not seem to match experiment as you take $d$ larger in Figure 4. Why only test for $d \\leq 8$, if the claim is about high-dimensional functions?\n\n### Application 4: Investigation of relation between kernel ridge regression and quantum mechanics\n\nThis section (and Appendix F) has plenty of physics terminology and was difficult for me to understand. The connection to Fermionic particles seems to go through, but I don't understand what the takeaway is? The sigmoidal relationship between $\\mathcal{L}_i$ and $\\ln(\\lambda_i)$ seems  apparent without having to appeal to physics. Simply because by definition $\\mathcal{L}_i = \\frac{\\lambda_i}{\\lambda_i + \\kappa}$.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nI had a difficult time reading this paper, in part because it is unclear what is formally proved and what is not.\n\nQuality:\nThe math I checked seemed correct.\n\nNovelty:\nThe conservation law and the hardness of the parity problem are not novel. Furthermore, the adversarial robustness application could be explored some more, and the relation to Fermionic statistics needed some more motivation. On the other hand, the deep bootstrapping application seemed new.\n\nFinally, there are already similar heuristics in the literature (Jacot et al. 2020, Canatar et al. 2021) for estimating the test MSE of kernel ridge regression. I am not sure why the one presented by this paper is preferable. It seems close to Canatar et al.'21.",
            "summary_of_the_review": "Unfortunately, because of the weaknesses I have written above, I do not recommend accepting the paper at this time.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3694/Reviewer_zi9Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3694/Reviewer_zi9Y"
        ]
    }
]