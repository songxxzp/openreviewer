[
    {
        "id": "fB99lZshEx",
        "original": null,
        "number": 1,
        "cdate": 1666474312975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666474312975,
        "tmdate": 1666702649320,
        "tddate": null,
        "forum": "BLsM6WymMo6",
        "replyto": "BLsM6WymMo6",
        "invitation": "ICLR.cc/2023/Conference/Paper6183/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Similar to refs [A,B], the paper proposes a new version of Hyperbolic Graph Convolutional Network (HGCN). The main difference with refs [A,B] is that, instead of learning a linear map in the tangent space of the manifold, the paper proposes to learn linear operators directly in the extrinsic geometry of the Lorentzian ambient space of the hyperboloid. \nThe learned linear operators use some boost and spatial rotation operations and directly map to the hyperboloid. \n\n[A] Ines Chami, Zhitao Ying, Christopher R\u00e9, and Jure Leskovec. Hyperbolic graph convolutional neural\nnetworks. NeurIPS 2019.\n\n[B] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks, NeurIPS 2019.",
            "strength_and_weaknesses": "- The main strength of the paper is the simplicity of the approach which is well-explained. I believe the proposed approach could be a useful \"trick\" to know in the community that works on hyperbolic representations.  \n\n- The weaknesses of the paper are missing theoretical comparisons with refs [A,B]. In particular:\n\n1) Can both the boost and spatial rotation operations also be expressed by existing approaches (when working in the tangent space)? If the answer is yes, does the proposed approach act as a regularizer by limiting the nature of possible operations?\n\n2) What are the operations that the proposed approach can or cannot perform?\n\n3) Related to the above questions, in the introduction, it is mentioned: \"However, these works [A,B] performed the network operations in the tangent space of the manifold which is a Euclidean local approximation to the manifold at a point since the network operations such\nas feature transformation and feature aggregation are not manifold-preserving. This makes it hard\nto optimize deeper networks because of these back and forth mappings between the manifold and the tangent space, and limits the representation capabilities of the hyperbolic networks caused by distortion specially most of these works used the tangent space at the origin.\" The tangent space is supposed to preserve the metric of the manifold due to the Levi-Civita connection of the manifold. I then do not think that the issue is due to performing operations in the tangent space. What are the reasons of the worse performance of the baselines? Is it because of the approximations of the mappings from the manifold to the tangent space? Or because of operations that are performed or not performed in the learned linear mappings?",
            "clarity,_quality,_novelty_and_reproducibility": "The description of the approach is clear in general. What is missing is a clear discussion about the differences with the baselines.",
            "summary_of_the_review": "The proposed method is interesting and could be useful to the community. What is missing is a clear discussion about the differences with refs [A,B]. What is possible in refs [A,B] that is not possible in the proposed approach and vice versa? What are the reasons of the improved performance of the method? Either a theoretical or empirical justification would be useful to readers.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6183/Reviewer_Ht7h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6183/Reviewer_Ht7h"
        ]
    },
    {
        "id": "o9XPEubpb0",
        "original": null,
        "number": 2,
        "cdate": 1666619643269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619643269,
        "tmdate": 1669686401942,
        "tddate": null,
        "forum": "BLsM6WymMo6",
        "replyto": "BLsM6WymMo6",
        "invitation": "ICLR.cc/2023/Conference/Paper6183/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper argues that methods based on tangent space have many problems, and therefore, avoid adopting this method. This paper proposes an approach to manifold-preserving Lorentz transformation.",
            "strength_and_weaknesses": "## Weakness\n\n1. Unconvincing conclusions.\n\n(1) Indeed, $W\\otimes_H X$ cannot guarantee that the result still lives in the Lorentz model after transformation. However, the problem has been solved by LGCN; that is, we only need to transform the space-like dimension (with the origin as a reference point) or just use the rotation matrix. Any of these methods can map it back to the Lorentz model. Also, the first approach can contain the rotation and boost operations.\n\nTherefore, according to the transformation, the author cannot derive the conclusion\u2014the method based on tangent space is not manifold-preserve. Similarly, we can utilize the same strategy to achieve the aggregation in the tangent space and successfully map it back to the Lorentz model.\n\n(2) The author claimed that \"This makes it hard to optimize deeper networks because of these back-and-forth mappings between the manifold and the tangent space,\" which could be incorrect since the deep network does not necessarily require frequent maps. For example, in image tasks, the network is deep [1][2]. Besides, it is not hard for us to optimize the HGNN. Please run the HGCN code directly using more than two layers.\n\n[1] Capturing implicit hierarchical structure in 3D biomedical images with self-supervised hyperbolic representations\n[2] Hyperbolic Image Segmentation\n\n2. Confusing experimental results.\n\n- Could the author explain why the performance of the disease and Pubmed reported is different from the original work HGCN? Why not take their original results but present the results from LGCN? \n- The usage of dimension in Table 3 seems to cherry-pick. Why is the disease compared in 8 and 4 dimensions while the Cora is compared in 64?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read and follow. But the work is not easy to reproduce since the details of the experiments are not given, such as optimization method, number of layers, patience, and training strategies.",
            "summary_of_the_review": "This paper argues that methods based on tangent space have many problems, and therefore, avoid adopting this method. This paper proposes an approach to manifold-preserving Lorentz transformation. Although the paper is easy to follow, there is something that I am concerned about, including the claim, method, and experiments.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6183/Reviewer_x9HY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6183/Reviewer_x9HY"
        ]
    },
    {
        "id": "H5oGms0y4d",
        "original": null,
        "number": 3,
        "cdate": 1666654464509,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654464509,
        "tmdate": 1669680955857,
        "tddate": null,
        "forum": "BLsM6WymMo6",
        "replyto": "BLsM6WymMo6",
        "invitation": "ICLR.cc/2023/Conference/Paper6183/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper looks at graph hyperbolic neural networks to learn maps from $\\mathbb{L}^n \\to \\mathbb{L}^d$ (where $\\mathbb{L}^d$ is $d-1$ dimensional Lorentz manifold. In particular, given a graph $G$ we learn features for each node using such a map and then use this for node classification and link prediction.\n\nThe novelty in the paper is in the parameterization of the maps from $\\mathbb{L}^n \\to \\mathbb{L}^d$ (where $\\mathbb{L}^d$. In particular, they use the parameterization given by Moretti 2002. They then show that such networks outperform other networks on a variety of standard datasets. \n\nThey also do an ablation study and show that their method learns near isometries. ",
            "strength_and_weaknesses": "**Strengths**\n---\n\n1) The method captures all Lorentz transformation. This is in contrast to Chen et al 2022 for which they show their parameterization captures all rotations and all boosts but not necessarily both together in one layer. However, this distinction may not be too important. \n\n2) The method seems to outperform other methods. But the important are small and may vanish with changing hyperparameters. \n\n**Weaknesses**\n---\n\n1) It is not clear to me how novel the method is. The decomposition into boosts and rotations is known (Moretti 2002). Further for the proofs in the texts, there are no statements, but statements being proved should be known (Moretii 2002, Chen et al 2022). \n\n2) There quite a few missing details including training details, statements of theorems among other things. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n---\n\nI think the paper's clarity could be significantly improved. I have two main issues with clarity. \n\n1) Due to the writing it is not clear what is known and what is new. \n2) The lack of clarity makes it difficult to understand the method in details and some things seem incorrect. \n\nFor the first, section 4.2 has two proofs for which the statements are missing. However, I think both of these proofs are unnecessary as they statements they are proving are known results. \n\nFor the second, due to the missing details there are few things that seem incorrect. I detail them here. \n\n**The first is in Figure 1**. The left figure in seems to show a hyperbolic rotation in which the point P (in blue) is rotated to the point P (in red). However, this is not a rotation in hyperbolic space, it is actually a boost. Specifically, the figure shows 1 dimensional hyperbolic space, in which the only rotations are given by $P = \\begin{bmatrix} 1 & 0 \\\\\\\\ 0 & \\pm 1 \\end{bmatrix}$ (as given by Equation (5) in the paper as well). In higher dimensions, again, the distance (euclidean) from the origin to the point should not change in a Lorentz rotation. Maybe this is what is being represented in the figure, but due to lack of details this is not clear. \n\n**Another detail that is missing is that the paper does not define what a Lorentz boost is.** This missing definition should be added in. Further the discussion on page 4 is not clear to me. Particularly the statement that the boost can be realized as rotations. \n\n**The next issue is in Figure 3** and equation (9) here the paper claims to be different from Chen et al 2022. However, this seems to be the same as Equation (4) (ArXiv v3 of Chen et al 2022). Note the cosmetic difference between the two due to the fact that this paper considers the curvature to $(-1/K)$ whereas Chen et al denote it by $K$. \n\n**The loss function is not clear**. For classification with more than two labels it is not clear what $\\hat{d}$ is? Do we pick a class at random? Is it weighted average of the distance to other classes?\n\n**An important missing detail is the training method** for their neural network. The paper does not mention any training details. The only place such a detail is present is when the paper says it enforces orthogonalization constraint. However, my question is how is this constraint enforced. Is the method projected gradient descent, Riemannian gradient descent, or some form of natural gradient that preserves the manifold.  \n\n**The title is misleading**. The initial embedding step uses the Tangent space to get an initial hyperbolic embedding. If the authors want to be truly free using of the Tangent space, I would recommend using one of the hyperbolic embedding techniques (Nickel and Kiela NeurIPS 2017, Nickel and Kiela ICML 2018, Sala, De Sa, Gu and Re ICML 2018, Sonthalia and Gilbert NeurIPS 2020)\n\n\n**Novelty**\n---\n\nAs mentioned it is unclear to me how new the ideas in the paper are. In particular, the decomposition is known already and without the training details, the new parameterization doesnt seem that novel. Also the differences to Chen et al. are not quite as clear. \n\n**Reproducibility**\n---\n\nI think the paper could improve its reproducibility. In particular, by adding all of the hyper parameters and training details for the different networks. \n\n\n**Questions**\n---\n\nI have a few questions for the authors.\n\n1) It is not clear to me what section 5.5 is trying to say. Specifically, why should our learned network be an isometry. This is further unclear to me since the loss function is the trying to minimize the distance between an embedding and the centroid for its class. This would suggest to be some sort of Neural Collapse would be beneficial. \n\nHowever, the authors claim that being a near isometry is what we want. If we want an isometry then why learn features at all? Couldn't we just use the graph?\n",
            "summary_of_the_review": "In summary, I think the idea of doing hyperbolic graph neural networks without using the tangent space is important since it increases the flexibility, and stability of the method as well as reduces computational costs. \n\nHowever, it is not clear to be (due to the presentation) the novelty of the paper, the correctness of some of the claims, and the reproducibility of the results. I think these issues are fixable, but very much do need to be fixed. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6183/Reviewer_8Du2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6183/Reviewer_8Du2"
        ]
    },
    {
        "id": "YMkvSOm2YY",
        "original": null,
        "number": 4,
        "cdate": 1666665676925,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665676925,
        "tmdate": 1666665676925,
        "tddate": null,
        "forum": "BLsM6WymMo6",
        "replyto": "BLsM6WymMo6",
        "invitation": "ICLR.cc/2023/Conference/Paper6183/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a version of hyperbolic GCN based Lorentz model without resorting to the tangent space. Some of the layer operations exist in prior work. Experiments are conducted on standard small-scale citation networks and tree datasets. ",
            "strength_and_weaknesses": "There are recently many efforts on removing the tangent space of hyperbolic neural networks. This paper used the Lorentz boost and rotation to operate directly on the Lorentz manifold. \n\nStrength. The presentation and logic of the paper are easy to follow. \nThe ablation study is interesting and shows the learnt structure information from the model. \n\nWeakness.\n1. Lack of novelty and contribution. The Lorentz boost and rotation is already well-characterized and studied in [1], particularly the proposed fully hyperbolic linear layer in [1] contains all Lorentz rotation and boost matrices. In fact, the graph convolution operation is essentially a hyperbolic linear layer. One can just take the linear layer from [1] and make a similar tangent space free Lorentz based hyperbolic GCN, which is pretty much done in the paper. \n\n2. Limited experiments and lack of reproducibility. Experiments are performed on 4 standard small scale datasets, which are 4 highly overfitted datasets. For example, the disease and airport, they are so small datasets. From my experience, I can always get a very high accuracy, but with a large variance during multiple runs. No code or supplementary is provided for the model. Plus I can get higher accuracies with some baselines, for example, using HGCN on pubmed node classification, $80.2\\pm 0.3$ with 10 runs. It's really hard to derive meaningful conclusion from the experiments. \n\n------ 2.1 where is the training hyper-parameter? how many layers of the model, any regularization, hidden dimension? How many epochs? I am aware that many hyperbolic NNs report results of running thousand epochs with early stopping, while standard Euclidean GCNs, SGC report accuracy of 100 or 200 epochs. Is the comparison fair? \n\n3. Some claims are vague or inaccurate. For example, \n\n\"most of the existing hyperbolic networks build the network operations on the tangent space of the manifold, which is a Euclidean local approximation. This distorts the learnt features, limits the representation capacity of the network\"\n\nI don't think this holds true, generally the exp/log map is local approximation of the manifold. However, in the hyperbolic space, they are bijection between the tangent space and the hyperbolic space. Usage of exp map at origin is not a problem with respect to representation capacity, as one can use parallel transport to move it along the manifold, which is already introduced and used in many prior works. \n\n------ 3.1 It claimed that \"the full Lorentz transformation ... increase the model expressiveness ... with deeper networks.\" I failed to find evidence in the paper supporting this claim, in particular, personally I haven't seen any successful deep hyperbolic GCN style structure to work well even in existing/prior work. \n\n4. The word and text needs more polish. For example, \n\n\"This makes it hard to optimize deeper networks because of these back and forth mappings between the manifold and the tangent space, and limits the representation capabilities of the hyperbolic networks caused by distortion specially most of these works used the tangent space at the origin. \"\n\n[1] Chen et al. 2022, Fully hyperbolic neural networks. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper/methodology is generally easy to follow and understand. Lack of originality, reproducibility not clear. ",
            "summary_of_the_review": "It's a straightforward usage of the fully hyperbolic NNs paper in GCN applications. Many pieces in the paper are missing and lack of enough novelty, see above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6183/Reviewer_ehJR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6183/Reviewer_ehJR"
        ]
    }
]