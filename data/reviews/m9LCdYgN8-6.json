[
    {
        "id": "WdrQlQMX-N",
        "original": null,
        "number": 1,
        "cdate": 1666649002957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649002957,
        "tmdate": 1666649002957,
        "tddate": null,
        "forum": "m9LCdYgN8-6",
        "replyto": "m9LCdYgN8-6",
        "invitation": "ICLR.cc/2023/Conference/Paper4960/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a framework for DAG learning from data. The method consists of a two-step procedure, where the first one is based on an observation about optimizing the order of nodes over the permutahedron. Then the edges to keep are selected by using a regularization over the set of functions.\nThe method is tested on two real-world datasets with promising results.\n",
            "strength_and_weaknesses": "The strengths of the paper are the clarity, the observation of the optimization over the permutahedron, and the fact that it is actually a framework where one can choose different methods for the sub-tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written, the description of the related work is adequate, and the contributions are clear.\nAs far as I know, the proposed method is novel, producing good results.\n\nThe code is included as supplementary material, which is much appreciated for reproducibility.\n\nThere seems to be a minor typo in equation (3): the r in the simplex.\n",
            "summary_of_the_review": "The authors of the paper propose an interesting two-step framework for DAG learning, where different methods can be used for the sub-tasks.\nIt is well written, and the results are promising.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4960/Reviewer_SU9Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4960/Reviewer_SU9Y"
        ]
    },
    {
        "id": "sMka2zpVwrW",
        "original": null,
        "number": 2,
        "cdate": 1666675542272,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675542272,
        "tmdate": 1670444343017,
        "tddate": null,
        "forum": "m9LCdYgN8-6",
        "replyto": "m9LCdYgN8-6",
        "invitation": "ICLR.cc/2023/Conference/Paper4960/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper develops a continuous optimization method for learning DAGs by optimizing over the polytope of permutation vectors. Two versions of the methods are provided: (1) alternately iterates between node-ordering and edge-optimization, and (2) optimizes them jointly. The resulting method has several advantages over existing methods, such as (1) returning exact DAGs and (2) accommodating non-differentiable black-box estimators. Experiment results on two real-world datasets are provided.",
            "strength_and_weaknesses": "Strength: \n- A new continuous approach to traverse the DAG space is provided. Solving continuous DAG learning task is a topic with significance.\n- The approach is flexible and accommodates non-differentiable black-box estimators.\n\nWeakness:\n- As the paper mentioned, the possibility of using differentiable approach to learn DAGs via ordering has been proposed by (Cundy et al., 2021; Charpentier et al., 2022). Therefore, the proposed method may seem incremental, i.e., optimizing over permutation vectors instead of permutation matrices as in existing works.\n    - I would suggest the authors to provide a detailed discussion of the advantages of their method over (Cundy et al., 2021; Charpentier et al., 2022). Also, can those existing methods be modified to accommodate non-differentiable black-box estimators?\n- The paper argued that existing works (Cundy et al., 2021; Charpentier et al., 2022) required expensive optimization over the permutation matrices. However, the paper currently lacks empirical studies to support it--either comparison of running time or scalability to very large graphs would be more compelling to demonstrate the efficiency/scalability of the proposed method.\n-  Although the paper mentioned a recent study by Reisach et al. (2021) regarding the flaw of synthetic dataset in DAG learning benchmark, I would still suggest the authors to compare their method to the baselines on those synthetic benchmarks to have an idea about how the method performs. This is because for real world datasets, the model assumption (e.g.,. linear or nonlinear additive noise models) may not hold.\n- In 'over-relaxation' part of Section 1, several methods based on penalizing the 'DAG-ness' of the adjacency matrix are actually *guaranteed to return a valid DAG* under some conditions, as shown by the following papers. Therefore the statement is not entirely true and should be corrected.\n    - Remark 2 of Ng et al. (2022). On the Convergence of Continuous Constrained Optimization for Structure Learning.\n    - Lemma 6 of Bello et al. (2022). DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity Characterization.\n\n----------------\nI appreciate the detailed response from the authors, which has addressed my concern. I have increased my score accordingly.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear. The proposed method may be rather incremental compared to existing ones. See above comments for more details.",
            "summary_of_the_review": "- The proposed method may be rather incremental compared to existing ones.\n- Lack of experiment results on synthetic benchmarks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4960/Reviewer_kPDZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4960/Reviewer_kPDZ"
        ]
    },
    {
        "id": "bCU3pCUPkyQ",
        "original": null,
        "number": 3,
        "cdate": 1666681186784,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681186784,
        "tmdate": 1666681186784,
        "tddate": null,
        "forum": "m9LCdYgN8-6",
        "replyto": "m9LCdYgN8-6",
        "invitation": "ICLR.cc/2023/Conference/Paper4960/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors look at the problem of recovering the DAG structure from samples from a DAG-structured distribution. Despite having seen a lot of important works spanning the last several decades, the problem has remained open and is an active area of research. The proposed solution in this paper consists of two steps i) finding a topological ordering of the DAG, ii) discovering edges according to the ordering. The authors manage to do both steps which is end-to-end on the one hand and accomodates any black-box edge discovery algorithm for step ii).\n\nExperiments conducted by the authors reveal that their algorithm are better in terms of both the metrics SHD and SID. In fact, their solutions learn at the Pareto frontier of these two metrics.",
            "strength_and_weaknesses": "Strength: the proposed solution is flexible and more efficient and more accurate than the existing methods. Comparison with the related work has been covered in great detail. The experimental evidence is extensive, consisting of the best algorithms known before and includes both benchmark and synthetic datasets.\n\nWeakness: Ultimately, the paper reads like a conglomeration of existing ideas. I could not clearly find a single new technique that the authors have found.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. The algorithm has a combination of important ideas. The experiments look reproducible to me.",
            "summary_of_the_review": "Given the superior experimental performance of the proposed approach, I tend to accept the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4960/Reviewer_EZMX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4960/Reviewer_EZMX"
        ]
    },
    {
        "id": "sauj5H9suA",
        "original": null,
        "number": 4,
        "cdate": 1667001994766,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667001994766,
        "tmdate": 1667001994766,
        "tddate": null,
        "forum": "m9LCdYgN8-6",
        "replyto": "m9LCdYgN8-6",
        "invitation": "ICLR.cc/2023/Conference/Paper4960/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the problem of learning DAGs from observational data. Different from prior work under the continuous framework, the authors propose an approach to optimize over the polytope of permutations, where edges can be jointly or conditionally optimized given a particular topological ordering. The key to the approach is to leverage recent progress on sparse relaxation methods such as top-k sparsemax and sparseMAP. Some experiments on real data are provided.",
            "strength_and_weaknesses": "Strengths: I generally find the approach interesting and a nice mix of ideas from the sparse relaxation literature to the DAG learning problem. The paper is also generally well-written modulo some unspecified notation. \n\nWeaknesses: The main weakness is the experimental section. Given that this is not a theoretical work, one should at least expect a more comprehensive set of experiments. The work of Reisach et al. (2021) is cited for skipping comprehensive synthetic experiments basically, and both real-world networks experimented on have a small number of nodes. The scaling problem when using OLS has been known for a while for linear regression, and hence the same is expected to happen for linear SEMs, this is why methods such as PC are robust to this problem since PC works by performing CI tests---Reisach et al. (2021). To alleviate the marginal variance problem, one can just use a bit smaller weights for the simulations. It would be great to see how the proposed method behaves against other baselines for ER, SF graphs for different numbers of nodes. Moreover, it is a must to compare the runtimes as well, given that other methods such as NOTEARS do not have a clear computational complexity, it is important to see what the actual performance is in terms of runtime. \n\nFrom the experiments, I would like to conclude: Is the proposed method better? Is it faster? Both? I cannot conclude that from the current set of experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Most of the paper is clear, but a few wordings for me were confusing. For instance, the term over-relaxation in the introduction is somewhat unclear and to me does not reflect what is said afterwards. In fact, I should bring to the authors' attention a recent work [1] that studies some disadvantages of existing acyclicity regularizers more formally, and perhaps consider referencing it for the next revision. In Section 2, it is stated \"the constraint on acyclicity is expressed as a smooth function and then relaxed to allow efficient optimization\", what do the authors mean by \"relaxed\"?\n\nQuality and Novelty: The work in general is of good quality, and although it does not shine in technical contributions, I think the paper nicely leverages work on sparse relaxations for the problem of DAG learning. \n\nReproducibility: Authors have shared code for reproducibility so that's a plus, although I have not tested it myself.\n\n\n[1] Bello et al. (2022) \u201cDAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity Characterization\u201d. NeurIPS.\n",
            "summary_of_the_review": "My main concerns are on about the experiments, it is not clear if the proposed approach works well for linear/nonlinear models, for different dimensions, and for different types of graphs. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4960/Reviewer_bSZG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4960/Reviewer_bSZG"
        ]
    }
]