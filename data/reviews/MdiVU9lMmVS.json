[
    {
        "id": "EcYa3ka5qOY",
        "original": null,
        "number": 1,
        "cdate": 1666612516611,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612516611,
        "tmdate": 1666612516611,
        "tddate": null,
        "forum": "MdiVU9lMmVS",
        "replyto": "MdiVU9lMmVS",
        "invitation": "ICLR.cc/2023/Conference/Paper579/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper develops a multi-agent algorithm suitable for handling a large number of agents by means of the mean-field approximation.",
            "strength_and_weaknesses": "Strength:\n- an interesting and important problem\n- natural approach\n\nWeaknesses\n- poor empirical evolution",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is rather clear, though I could not answer some issues (see below). The novelty is somewhat limited; the paper is a rather straightforward extension of the mean-field method. I have a major concern about reproducibility. The experiments are presented using one seed only (?) and are rather unstable, which raises the question if the conclusions are statistically significant. ",
            "summary_of_the_review": "The paper develops a multi-agent algorithm suitable for handling a large number of agents by means of the mean-field approximation. The major contribution is utilising an attention mechanism to quality the strength of interactions. This is a very natural approach and likely to be beneficial. \n\nMy major concern is about empirical evaluation. The grid world environment, as far as I understood, does not include any form of interaction. Doesn't it factorise into $100$ of completely independent environments? \n\nQuestions and comments:\n - Is the methods for cooperative/mixed/competitive MARL?\n - I do not quite understand the statement of 'Theory 1'. What does it mean 'approximated'?\n - I am not sure what $\\mathcal{N}_j$  are used in the experiments. Can the authors clarify that?\n - The graph on Fig 3 is hard to read. Also, \"Best episode return\" is a somewhat unusual metric.\n - The method seems not very stable. It is not unusual for MARL trainings; however, the paper would benefit from some discussion or more experiments with this regard.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper579/Reviewer_cg8c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper579/Reviewer_cg8c"
        ]
    },
    {
        "id": "QSG_nEkjRNG",
        "original": null,
        "number": 2,
        "cdate": 1666629812618,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629812618,
        "tmdate": 1666629812618,
        "tddate": null,
        "forum": "MdiVU9lMmVS",
        "replyto": "MdiVU9lMmVS",
        "invitation": "ICLR.cc/2023/Conference/Paper579/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies multi-agent problems with a very large number of agents. In the past, such problems have been tackled using a combination of reinforcement learning and mean field approximations. This approach is valid provided the interactions between the players are homogeneous in the sense that every player interacts roughly in the same way with all the other players. Here, the authors consider a problem in which the interactions have some network structure, meaning that each agent may interact more with some agents rather than some others. To address this aspect, they propose to use graph attention networks. They first explain how to rewrite the Q-function using local information, then they design a graph attention mechanism and finally they conduct experiments on two examples, with up to thousands of agents. ",
            "strength_and_weaknesses": "Strength: (1) The beginning of the paper (motivations and part of the literature review) is quite clear. (2) The experiments show that the method can be used for a relatively large number of agents (a few thousands).\n\nWeaknesses: \n(1) References: It seems that there are already several papers on multi-agent problems using graph attention mechanism, such as (to cite just one example but I do not know if this is the most relevant one):\nMulti-Agent Game Abstraction via Graph Attention Neural Network, Liu et al., AAAI\u201920\nFurthermore, there is a growing literature on mean-field reinforcement learning or reinforcement learning for mean field games, whose goal is precisely to tackle games with a very large number of players. Given the similarities in key-words and goals, it could be relevant to mention the existence of this literature and the main similarities or differences. \n\n(2) Although (Yang et al., 2018) is very related to your work, I would recommend clarifying that the notion of solution is quite different. They consider a Nash equilibrium whereas you focus on a joint optimization problem. \n\n(3) \u201cTheory 1\u201d (\u201cTheorem 1\u201d?): In its current form, it is hard to get any insight from this result or to check its correctness. Indeed, any function \u201ccan be approximated\u201d (to some degree) by any other function. Could you please make it more precise? It would be nice to provide a quantitative statement, and to spell out clearly the assumptions (e.g., on the Q function).\n\nTypos and minor questions:\nPage 3 and 4: In the model, what are the assumptions on the state and action space? Are they finite? If not, are they compact?\nPage 4: \u201cpossibility distribution\u201d \u2192 \u201cprobability distribution\u201d, \u201ccollection of possibility distribution\u201d \u2192 \u201ccollection of possibility distributions\u201d\nPage 5: \u201cTheory 1\u201d \u2192 \u201cTheorem 1\u201d\nPage 6: formula (15): Why are these \u201ctime-varying weights\u201d?\nPage 7: \u201cthe rest parts\u201d \u2192 \u201cthe other parts\u201d \nPage 8: Figure 4: Is it correct that the DDPG method yields a higher reward than the GAT-MF method? Could you comment on this please?\nPage 9: Section 6.2: How is computed the \u201c1.2B\u201d in the expression \u201c1.2B agent-environment interactions\u201d?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Even though the beginning of the paper and the numerical test cases are quite clearly presented, I feel that some improvements are needed due to the gap in the literature review (which makes it hard to know to what extent the method is new) and the theoretical result (claimed as one of the main contributions). ",
            "summary_of_the_review": "I would recommend clarifying the comparison with the existing literature (so that we understand better the algorithmic contribution) and the theoretical contribution.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper579/Reviewer_vFJW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper579/Reviewer_vFJW"
        ]
    },
    {
        "id": "dfuj4_4P-1",
        "original": null,
        "number": 3,
        "cdate": 1666694959766,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694959766,
        "tmdate": 1666695133199,
        "tddate": null,
        "forum": "MdiVU9lMmVS",
        "replyto": "MdiVU9lMmVS",
        "invitation": "ICLR.cc/2023/Conference/Paper579/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to change the mean-field approximation of the Q function proposed by Yang et al., 2018 into a function that takes the weighted average of the neighbors\u2019 actions rather than the simple average.  Weights measure the degree of interaction between an agent and its neighbors. These are computed at every timestep using an attention mechanism so that the weights can adapt to the current context. The method is shown to outperform the baselines in a grid world scenario and a more complex environment that simulates the problem of vaccine allocation during COVID-19.\n",
            "strength_and_weaknesses": "### Strengths\n\nThe paper is clearly written and the idea of modeling the strength of interaction between agents with an attention mechanism is interesting. The two environments used are \n\n### Weaknesses\n\nI don\u2019t think it is fair to list the result in Section 4.2 (Weighted MF Approximation) as one of the main contributions of this paper. This is just a minor modification of the result from Yang et al., 2018 that includes the weights and whose proof follows trivially from the original result.  \n\nThe paper is not well-positioned in the literature. There are many missing references to relevant papers. This makes it very hard to properly assess the novelty of this work. A few of these papers are listed below:\n\n * Guestrin et al., 2002. Coordinated Reinforcement Learning.\n * Nair et al., 2005. Networked Distributed POMDPs: A Synthesis of Distributed Constraint Optimization and POMDPs.\n * Peter Sunehag et al., 2018. Value-Decomposition networks for cooperative multi-agent learning based on team reward.\n * Bohmer et al., 2020. Deep Coordination Graphs.\n *Oliehoek et al., 2021. A sufficient Statistic for Influence in Structured Multiagent Environments.\n\nIf I\u2019m not mistaken the critics in MADDPG and MAPPO take as input the global state and the actions of all the other agents. If this is the case, the experiments are missing two baselines that I believe the method should be compared against:\n * A policy and value function that condition only on the pivot agent\u2019s local states such as independent q-learning (Tan, 1993). This is meant to show that the neighbors' information is important and that the weighted mean-field approximation is really needed.\n * A policy and value function that condition on the pivot agent\u2019s local states, and the neighbor agents\u2019 actions and local states. This is meant to show that feeding the weighted mean-field approximation is more effective than just feeding the neighbors\u2019 local states and actions.\n\n (Tan, 1993) MultiAgent Reinforcement Learning Independent vs Cooperative Agents.\n\n**Other questions/suggestions**\n\nIs the GAT learned end-to-end using the reward signal?\n\nAs far as I can see, the two environments are Dec-POMDPs since the agents\u2019 receive only their own local states and the neighbors\u2019 local states. In general, these are not Markovian. Don\u2019t you need to condition the policy and value function on the action-observation history? Also, why does the policy condition on the neighbors\u2019 local states but not their actions?\n\nI didn\u2019t find any information about the experimental setup: number of random seeds, hyperparameter configurations\u2026 Are the results reported in Figures 3 and 5 averaged over different trials?\n\nThe paper states that the method requires the agents to have fixed relative positions. Is this the case in the grid-world task? Aren\u2019t the miners moving independently?\n\n**Minor mistakes/typos**\n\nSpecify how the actions are represented. Are they one-hot encoding vectors? \n\nI believe the agent\u2019s subscript is missing in equations (8) and (9). $Q_j(s,a)$ and $\\pi_j(s)$.\n\nIn Section 5.1 when you say \u201cGaussian distributions centered at the two grids\u201d do you mean centered at the two \u201ccells\u201d that contain the diamond veins?\n\nIn Figure 3. Why do the contours show two peaks at (0,0) and at (9,9)? Aren\u2019t there just two Gaussians centered at the diamond veins?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear. The paper doesn\u2019t seem very novel (please refer to the previous section). Code is provided.\n",
            "summary_of_the_review": "Overall, I think this paper is not ready for publication. My suggestion to the authors is to relax some of their claims (listing the result in Section 4.2 as a contribution is not appropriate in my opinion), try to position this work better in the literature, and add more baselines to the experiments so as to really demonstrate the benefits of the method.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper579/Reviewer_T6mP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper579/Reviewer_T6mP"
        ]
    }
]