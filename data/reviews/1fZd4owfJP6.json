[
    {
        "id": "CgYMf3lsV7",
        "original": null,
        "number": 1,
        "cdate": 1666591038755,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591038755,
        "tmdate": 1666591038755,
        "tddate": null,
        "forum": "1fZd4owfJP6",
        "replyto": "1fZd4owfJP6",
        "invitation": "ICLR.cc/2023/Conference/Paper1611/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper unleashes the great potential of contrastive learning on denoising autoencoding and introduce a pure MIM method, ConMIM, to produce simple intra-image inter-patch contrastive constraints as the sole learning objectives for masked patch prediction. Additionally, the authors further strengthen the denoising mechanism with asymmetric designs, including image perturbations and model progress rates, to improve the network pre-training.",
            "strength_and_weaknesses": "Strength:\n1/ The proposed ConMIM cleverly combines the conventional contrastive learning and the conventional masked modeling, which gets rid of extra tokenizing networks by revitalizing contrastive learning. \n2/ This paper is well-organized and extremely clearly written with some technical contributions.\n3/ Figure 1 and 2 are highly easy to understand.\n2/ Extensive experiments are conducted to reveal the superiority of the proposed method, in terms of the performance on downstream tasks and running time.\n3/ Sufficient ablation studies are conduced to validate the components\u2019 effectiveness (i.e., denoising auto-encoding mechanism, patch level dynamic dictionary, and the asymmetric designs).\n4/ Clear contributions are provided.\n5/ Detailed experiments settings are provided.\n\nWeakness:\n1/ Some aspects of the proposed model seem somewhat to have been empirically defined. Although good experimental results were obtained, it appears that there is no rigorous theoretical background to justify, from a formal perspective, such as the asymmetric image perturbations designs. The validation of the proposed method is mostly from practical evidence.\n2/ The results of the proposed ConMIM with more epochs should be provided in Table 1,2,3. For example, in Table 1, only the results of 300 and 800 epochs are provided, but the results of 1600 epochs are missing.\n3/ The information about some notations are lack. For example, I haven\u2019t found out what $k$ in Eqn. 1 means so far. Although it is default to represent the keys, I think it would be better to explain it. \n4/ Why not alternately augment the corrupted images and full images by strong and weak augment in the pre-training process, instead of augmenting one type of images by a fixed augment (strong or weak augmentation)? This strategy seems more reasonable.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1/ The clarity of this paper is good. This paper is well-organized and the figures are extremely easy to understand.\n2/ The quality of this paper reaches the standard of this conference.\n3/ The novelty of this paper is sufficient. The proposed ConMIM cleverly combines the conventional contrastive learning and the conventional masked modeling, which gets rid of extra tokenizing networks by revitalizing contrastive learning.\n4/ This paper is easy to reproduce, since all of the detailed experiment settings are provided.\n",
            "summary_of_the_review": "This paper proposes a novel pre-training method for masked image modeling, namely, ConMIM, to get rid of extra tokenizing networks by revitalizing contrastive learning. Besides, the denoising mechanism with asymmetric designs, including image perturbations and model progress rates are strengthened to further improve the network pre-training. Also, the entire paper is very well-written and the experiments are quite rich.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1611/Reviewer_1DfA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1611/Reviewer_1DfA"
        ]
    },
    {
        "id": "ASTxCP8kV3",
        "original": null,
        "number": 2,
        "cdate": 1666675678302,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675678302,
        "tmdate": 1666676653088,
        "tddate": null,
        "forum": "1fZd4owfJP6",
        "replyto": "1fZd4owfJP6",
        "invitation": "ICLR.cc/2023/Conference/Paper1611/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to combine contrastive learning with masked image modeling in order to provide strong pre-text tasks. As opposed to conventional contrastive learning and masked image modeling (MIM) approaches, the authors cast masked image modeling as denoising contrastive learning for an effective vision dictionary look-up. It was validated that the proposed method, ConMIM, competes favorably on downstream tasks (e.g., image classification, semantic segmentation).",
            "strength_and_weaknesses": "Strengths:\n- Quantitative results are promising. The authors validated the proposed method on the models of various sizes, clearly showing the superior performance to prior approaches.\n- The method is simple yet effective. It does not leverage extra training stages of image tokenizer.\n\nWeaknesses:\n- The authors miss some important reference in the field of MIM. For instance, \u2018SimMIM: A Simple Framework for Masked Image Modeling (Z.Xie et al., CVPR 2022)\u2019 was not referenced in this paper. Though SimMIM is remarkably simple in that it applies a simple one-layer prediction head decoder to MIM, it records competitive performance compared to ConMIM. Thus, the effectiveness of the proposed method compared to this simple approach seems rather questionable.\n\n- Similarly, there are various attempts to combine contrastive learning and MIM. For example,\n\n(1) IBOT: Image BERT pre-training with online tokenizer (J.Zhou et al)\n\n(2) Siamese image modeling for self-supervised vision representation learning (C. Tao et al)\n\n(3) Contrastive Masked Autoencoders are Stronger Vision Learners (Z.Huang et al)\n\nThe novelty of the proposed method should be clarified with respect to these methods. It was briefly mentioned in the related work section, but it seems that the basic idea itself of the proposed framework (combining the contrastive learning and MIM) is very similar to the above-mentioned methods, and the only difference is what kind of contrastive learning module (or what kind of MIM module) is used in the proposed framework. Namely, it seems that in terms of combining the two approaches, the proposed framework is essentially very similar to the existing methods.\n\n- The asymmetric augmentation used in the proposed method needs more clarification. To the best of my knowledge, applying weak augmentation to the input of the target encoder (slowly progressing ViT in Figure 2) and strong augmentation to the input of the source encoder is known to be suitable for the field of contrastive learning. In contrast, the proposed method contradicts this common wisdom. Do you have any explanation for this?\n\n- Since the proposed method importantly leverages contrastive learning, it seems that the result of combining a variant of contrastive learning approaches (e.g.. DINO, Byol, Simsiam) and MIM should be provided as an ablation study.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This work attempts to combine the two popular approaches, contrastive learning and MIM, for achieving better performance, but it is hard to catch up the key difference with recent approaches that try to use the contrastive learning and MIM.",
            "summary_of_the_review": "The authors proposed a reasonable approach to take the advantage of the two pre-training approaches, but there are some concerns related to novelty, evaluation and method details, as mentioned in \u2018Strength and Weaknesses\u2019. Nevertheless, if these comments are well-addressed, the initial rating can be adjusted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1611/Reviewer_LV22"
        ]
    },
    {
        "id": "miN75lLP3W",
        "original": null,
        "number": 3,
        "cdate": 1666695142021,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695142021,
        "tmdate": 1666695142021,
        "tddate": null,
        "forum": "1fZd4owfJP6",
        "replyto": "1fZd4owfJP6",
        "invitation": "ICLR.cc/2023/Conference/Paper1611/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new masked image modelling (MIM) method, conMIM, to develop better visual representation learning models. The proposed method introduces intra-image inter-patch contrastive learning design to enhance the self-learning network to predict masked patches, which could be quite useful when the training data is limited. This method also develops asymmetric design to promote the training pipeline so that the model is able to learn better. Extensive experiments demonstrate that the proposed backbone can be utilized in many downstream applications well. ",
            "strength_and_weaknesses": "Strength\n1. This paper is clearly written. The motivation introduced in the first paragraph and the rich literature reviews are made in the following section. \n2. This paper has mainly three contributions, including the dynamic dictionary, contrastive learning as well as asymmetric design. \n3. Comprehensive experiments show that the dedicated network based on this model has outperforms the baseline methods consistently on applications including semantic segmentation,  object detection, and instance segmentation. \n\nWeaknesses\n1. Regarding the second point of the asymmetric design, why using the plain backbone for corrupted input? The reason of such a design is not clearly explained in the text. \n2. Based on Table 8, the asymmetric image perturbation has very little effect on the results. \n3. It would be more convincing if the authors would be able to visualize the dynamic dictionary when the training is converged. Make a comparison between this dictionary against other baseline dictionary settings. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has described the strong motivation of creating such a backbone Transformer that can better learn image representations to serve applications better. It has made a sufficient literature review with respect to the dictionary look-up, vision transformer, self-supervised learning etc. The originality of the work is good as there is no other similar work. ",
            "summary_of_the_review": "In summary, this paper has demonstrated three techniques, patch-level dynamic dictionary and denoising contrastive learning and asymmetric design to enhance a backbone network to learn better image representations. The paper is clearly written and the details and explanations of the key contributions are clearly presented. However, it requires some more in-depth analysis of these techniques beyond the quantitative ablation study. It would be better to introduce the intuitive motivation behind the choice of these techniques. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1611/Reviewer_oXdu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1611/Reviewer_oXdu"
        ]
    }
]