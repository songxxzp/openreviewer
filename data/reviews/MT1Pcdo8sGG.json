[
    {
        "id": "dhXGC_TUJXU",
        "original": null,
        "number": 1,
        "cdate": 1666182423282,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666182423282,
        "tmdate": 1666182423282,
        "tddate": null,
        "forum": "MT1Pcdo8sGG",
        "replyto": "MT1Pcdo8sGG",
        "invitation": "ICLR.cc/2023/Conference/Paper6152/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work presented a new dataset that contains final exam questions, and compares the performance of popular transformer models for answering them. The dataset includes questions that arise from a wide range of topics in machine learning including loss functions, RNNs, reinforcement learning, and etc. ",
            "strength_and_weaknesses": "Strength: This is the first study to present a dataset of machine learning final exams\nWeakness 1: A lot of details are missing with regards to how the dataset was curated. An illustration/figure to show the method used in this work would be advantageous.\n Weakness 2: The recommended models do not consider questions that rely on images for their automated solution. ",
            "clarity,_quality,_novelty_and_reproducibility": "\nThis work is of high quality since it presents a novel approach by being the first study to curate a machine learning dataset of final exams. This work is also reproducible since the authors made their data and code publicly available for the machine learning community. ",
            "summary_of_the_review": "This paper presents an appealing and novel approach by generating a new dataset of machine learning final exams. This work also provided a benchmark of models for answering the final exam questions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None. ",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6152/Reviewer_bhJL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6152/Reviewer_bhJL"
        ]
    },
    {
        "id": "i9YagTgC8z8",
        "original": null,
        "number": 2,
        "cdate": 1666671920355,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671920355,
        "tmdate": 1670965564290,
        "tddate": null,
        "forum": "MT1Pcdo8sGG",
        "replyto": "MT1Pcdo8sGG",
        "invitation": "ICLR.cc/2023/Conference/Paper6152/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduce a new university exam dataset spanning questions on 12 machine learning topics covered at MIT's Introduction to Machine Learning course. This joins a thin body of work of exam datasets that have mostly focused on high school and lower levels. The dataset and code are open-sourced and uploaded as part of the Supplemental Material. The exams span multiple quarters of this class. The experiments include benchmarking LMs on this dataset, as well as generating new exam questions. The results show that the generated questions are of equal quality as the human-written ones, have the same average difficulty rating, and get recognized as human-written with the same probability.\n\nUPDATE: After discussion with my fellow reviewers and area chair, I agree that there are valid concerns about the size of the dataset. While we all praise the authors for the originality of the work and consider it valuable to the community, the dataset as presented may be too small for the community to use. We recommend trying to expand the dataset and resubmitting the paper.",
            "strength_and_weaknesses": "Strengths:\n- Much needed dataset for the community and interesting topic\n- Strong GPT- and Codex-based baselines, as well as baselines with Open LMs\n- Extensive experiments provide benchmarking results to start off of\n- Great experiments and promising results with generated questions\n\nWeaknesses:\n- No major weakness\n\nNotes:\n- The references should be in parentheses\n- In Table 5, there is the exact same result between GPT3-ZS and GPT-3 FS in Fall 2021, is that a typo?\n- \"We extended this work to include final exams from Harvard\u2019s and Cornell\u2019s machine learning classes as well. \" Are these questions included or to be included in the future?",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clear, well evaluated, novel and reproducible with the supplemental material.",
            "summary_of_the_review": "This work introduces a novel dataset of university level machine learning exam questions and answers. This is a much needed resource for the community, and the results of the benchmarking is valuable, as well as the ones for exam question generation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6152/Reviewer_FkVg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6152/Reviewer_FkVg"
        ]
    },
    {
        "id": "T6a7-DSncS",
        "original": null,
        "number": 3,
        "cdate": 1666721076739,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666721076739,
        "tmdate": 1666721076739,
        "tddate": null,
        "forum": "MT1Pcdo8sGG",
        "replyto": "MT1Pcdo8sGG",
        "invitation": "ICLR.cc/2023/Conference/Paper6152/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors provide a new dataset and benchmark of questions from machine learning final exams and code for automatically answering these questions and generating new questions. They build baselines by applying zero-shot and few-shot learning to GPT-3 and Codex, adding chain-of-thought prompting for GPT-3. ",
            "strength_and_weaknesses": "Strength:\n1. A quite interesting dataset\n2. Solid baselines\n\nWeakness:\n1. Seems like one more dataset from BIG-bench, https://github.com/google/BIG-bench . Not too much novelty\n2. The paper is not well-written. Not sure why Figure 1 is shown in the paper. It only shows students taking the final exam which is not related to the dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "What's the difference between the proposed dataset and the BIG-bench datasets?",
            "summary_of_the_review": "I think this is an interesting dataset, but it is like one dataset from BIG-bench. I think we don't need every dataset from BIG-bench to be published.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6152/Reviewer_ffgb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6152/Reviewer_ffgb"
        ]
    }
]