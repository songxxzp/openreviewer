[
    {
        "id": "45XVEfIs5ms",
        "original": null,
        "number": 1,
        "cdate": 1666203488696,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666203488696,
        "tmdate": 1666203488696,
        "tddate": null,
        "forum": "ceOdspvoaEA",
        "replyto": "ceOdspvoaEA",
        "invitation": "ICLR.cc/2023/Conference/Paper4281/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a theoretical analysis, hoping to show that one-step RL is equivalent to a certain type of critic regularization. ",
            "strength_and_weaknesses": "Strength:\n\n- The general idea of discovering equivalence between methods, which look like arbitrary choices, is quite interesting (and can be useful as it clarifies false expectations for performance improvement if different variations are indeed equivalent).\n\nWeaknesses:\n\n- Poor presentation: it is not an easy paper to follow, especially definitions and assumptions are often unclear (aside from notational and conceptual mistakes here and there). \n\n- Some of the results are in question, particularly, switching regression to classification has been done through a nonlinear transformation. It is unclear if this transformation still guarantees convergence to the desired values.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: poor\n\nQuality: average\n\nNovelty: average\n\nReproducibility: good -> Authors stated to release the code (beginning of section 5), also some implementation details are provided. ",
            "summary_of_the_review": "- The introduction is hard to follow. Particularly, your definition of regularization is different from what a normal RL reader may expect. Most of the methods cited in the introduction to combat over-estimation of offline RL are not normally known as *regularization* techniques. I would strongly suggest starting with a clear definition of what \u201cyou\u201d mean by regularization before even start citing other papers and compare various method.\n\n- Last paragraph of the introduction is very unclear, likewise the contributions part. Clearly convey the contributions in short and informative statements. \n\n- Second paragraph of related work: Not sure how imitation learning (IL) falls into these categories. IL normally tries to generalize the seen actions at seen states, with no regard to maximizing expected return. That is, IL tries to clone the behavioural policy. In contrast, one step methods still have a policy improvement step, meaning that their resultant policy is likely to be better than the behavioural policy and it does not necessarily clone that. \n\n- Your Figure 1 seems to have not been referred to at all in the text. \n\n- Section 3.1: \u201c\u2026 we assume rewards are positive, adding a positive constant to all rewards \u2026\u201d this is only correct if there is no terminal state, otherwise the values of terminal states should also be shift from (by definition being) zero. If that\u2019s the case (which seems to be from the following part), make it explicit that the MDP is not episodic and there is no termination. \n\n- Section 3.1: your definition of reward is indeed expected reward (over next state). Paying attention to small details help the reader to follow your main points better. \n\n- Section 3.1: your definition of $Q^\\pi$ is undiscounted, yet your reward is strictly positive. The summation simply diverges. \n\n- Page 4, first paragraph: \u201c\u2026 TD targets $y(s, a)$ are not considered learnable (i.e., we would apply a stop-gradient operator)\u201d -> what do you mean? So, you do not try to solve second line of equation (1)?\n\n- Page 4, last line -> this update is SARSA, not Q-learning. \n\n- Paragraph after equation 4: what do you mean by \u201cin tabular settings\u201d? Do you mean the standard SARSA? Same issue in Lemma 4.1. The proof of Lemma 4.1 does not look right since you have a nonlinear transformation which does not commute with the expectation and Jensen\u2019s inequality applies. \n\n- Again, proof of Lemma 4.1: you said the update can be written as $\\frac{Q}{Q+1}\\leftarrow \\frac{y}{1+y}$. This is only the loss function (and not the update) which then *incrementally* changes the value of $Q$. You need to show that under incremental change, the whole algorithm converges. (See for example textbooks on stochastic approximation, e.g., chapter 4 of Neuro-Dynamic Programming, Bertsekas and Tsitsiklis, 1996)\n\n- Page 5, paragraph after equation 5 -> this also rings a bell in my mind. As an alternative to the cross-entropy, one may directly learn the transformed Q values. See this reference: https://arxiv.org/abs/2203.07171 \n\n- Lemma 4.1 and 4.2 -> You need to clarify the assumptions under which these results hold. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4281/Reviewer_TugW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4281/Reviewer_TugW"
        ]
    },
    {
        "id": "Jg80aDfSmUe",
        "original": null,
        "number": 2,
        "cdate": 1666616400534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616400534,
        "tmdate": 1666672486239,
        "tddate": null,
        "forum": "ceOdspvoaEA",
        "replyto": "ceOdspvoaEA",
        "invitation": "ICLR.cc/2023/Conference/Paper4281/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to draw a connection between the 'one-step' offline RL algorithms and the critic regularization-based ones. With some modification in actor or critic objective, the authors prove the same convergence results for the both methods in tabular settings. And the numerical simulations and empirical results on offline RL benchmark show the close connection between the both algorithms.",
            "strength_and_weaknesses": "Strength:\n\n1. this paper tries to reveal the relationship between 'one-step'-based and critic regularization-based offline RL methods, and provide a new perspective on the understanding of various offline RL algorithms.\n\n\nWeaknesses:\n1. Though this paper proves that both the offline methods can have the same convergence results, some preconditions should be satisfied and some key changes seems strange.\n2. The main results, i.e., thm 4.3 says that \"the critic regularization results in the same policy as one-step RL\" , which is doubtful. Actually, this theorem only shows that their objective is equivalent, not necessarily the algorithm. We know that different algorithms will generally results in quite different policy, which is essentially the key motivation behind one-step RL: all the value-based methods are optimizing the Bellman equation in some sense, but multiple step or iterative type algorithms are not neccearily better than the single step ones.  In particular, for critic regularization algorithm such as CQL, it is their optimizing method that defines what the algorithm really is - in this case it is just an iterative type value-based algorithm, and hence may suffer from the over use of the estimated behavior Q function, no matter its objective is equivalent to that of one-step RL or not.  Due to the above reason, CQL may not results in the same policy as one-step RL.\n\n3. Some of the experimental results require more detailed explanation, (e.g. numerical simulation results in Figure.2 make readers confusing). And there is a minor disagreement on the coefficient choice for both theoretical and empiciral results.\n3. It's better to provide the performance of both methods for more sufficient comparison.\n\n4. for eq.(3) , although the range of the normalized Q function and y function lies in (0,1), their integration is not one and hence not probability function. So what's the justification to compare them with cross entropy?\n\n5. About the coefficient $\\lambda$, the theoretical analysis suggests that $\\lambda=1$ can lead to the same convergence for both methods, while the experiments show both methods have the most similar results (Fig.5) if $\\lambda=10$. Maybe more analysis on various coefficients should be carried out.\n\n6. About the lower bounds on Q-values(Figure.6), the original paper proves that the complete CQL objective can lead to the underestimation on V value while not Q value, so I'm not sure it's appropriate to compare Q values.\n",
            "clarity,_quality,_novelty_and_reproducibility": "fair.",
            "summary_of_the_review": "This paper shows that the objectives of one step RL and CQL are equivalent. However, since the main results ignore the difference in optimization methods,  these algorithms actually have big difference. I think that the paper should mention this.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4281/Reviewer_pLRb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4281/Reviewer_pLRb"
        ]
    },
    {
        "id": "MVmV1hvJq2e",
        "original": null,
        "number": 3,
        "cdate": 1666665228670,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665228670,
        "tmdate": 1669677758787,
        "tddate": null,
        "forum": "ceOdspvoaEA",
        "replyto": "ceOdspvoaEA",
        "invitation": "ICLR.cc/2023/Conference/Paper4281/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a theoretical and empirical analysis with the main goal showing the similarity between two main categories of algorithms, one-step methods and critic regularization-based methods. Specifically, the paper shows that, under some conditions, the two types of methods are theoretically equivalent; it also shows that, in relevant practical cases, they also behave similarly in practice.",
            "strength_and_weaknesses": "Strengths:\n- The writing is very clear and the presentation of the content is easy to follow;\n- The theoretical results are compact and clear, and also well explained in the context of the full paper;\n- The experimental analysis is remarkably clear as well (I really like visualization in Figure 4 and Figure 5);\n- The extended analysis for other settings (e.g., goal-conditioned RL) reported in Appendix adds interesting material to the paper.\n\nWeaknesses:\n- The theoretical analysis a bit limited by the \"classifier actor-critic\" setting; it would be nice to explain in more detail why an analysis in the regression case is especially difficult to help the reader understand this choice.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is exceptional.\n\nThe quality of the research is good.\n\nTo the best of my knowledge, this equivalence was never explicitly investigated, and the work thus looks novel.",
            "summary_of_the_review": "In summary, I found this to be a good paper that can be of general interest for people working with offline RL algorithms of various kinds, and I thus recommend acceptance.\n\n-----\n_After rebuttal phase_: after reading the discussion with the area chair and the other reviewers, I now share their concerns on the significance and technical soundness of the results and I have thus lowered my score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4281/Reviewer_3nPe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4281/Reviewer_3nPe"
        ]
    },
    {
        "id": "vsAoxMhuWS_",
        "original": null,
        "number": 4,
        "cdate": 1666671460048,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671460048,
        "tmdate": 1669322301331,
        "tddate": null,
        "forum": "ceOdspvoaEA",
        "replyto": "ceOdspvoaEA",
        "invitation": "ICLR.cc/2023/Conference/Paper4281/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper makes a connection between regularizing the critic and regularizing the actor in the actor-critic framework. Specifically, the authors show that these two are equivalent with a specific choice of the regularizing parameter.  Extensive numerical simulations are provided to justify the theoretical findings. ",
            "strength_and_weaknesses": "This paper is easy to follow and the result seems interesting. \n\n(1) For Theorem 4.3, since in practice there are stochastic errors and function approximation errors, showing that the ideal update equations are equivalent is not entirely satisfactory. It will be interesting to see how these errors propagate in these two types of algorithms.\n\n(2) Since multi-step critic regularization and one-step RL are equivalent, but multi-step critic regularization seems to be less sample efficient to implement, does this suggest that one should use one-step RL in practice?\n\n(3) In both cases, the authors focus on policy iteration type of policy update. What about using policy gradient or natural policy gradient type of policy update?\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. While the authors do not propose new algorithms, the insight from analyzing existing algorithms seems interesting.",
            "summary_of_the_review": "Overall I think this paper provides an interesting viewpoint on the connection between critic regularization and actor regularization, and the numerical simulations are convincing. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4281/Reviewer_hMZj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4281/Reviewer_hMZj"
        ]
    }
]