[
    {
        "id": "H91mx3E4Lj",
        "original": null,
        "number": 1,
        "cdate": 1666529904832,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666529904832,
        "tmdate": 1666529904832,
        "tddate": null,
        "forum": "iNUtsk4h2q1",
        "replyto": "iNUtsk4h2q1",
        "invitation": "ICLR.cc/2023/Conference/Paper3714/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a generative model in terms of image restoration, named restoration-based generative model (RGM). This paper eliminates expensive sampling by performing MAP estimation and incorporating implicit prior information via GAN. Furthermore, a multi-scale training is proposed to alleviate the latent inefficiency of DGM. The ablation study demonstrates the effect of all parts of the MAP objective. A series of experiments on different datasets demonstrate that the RGM achieves state-of-the-art performance when the number of forward steps is limited.",
            "strength_and_weaknesses": "**strength**\n\n-  The authors propose  a new flexible family of generative models, named restoration-based generative models (RGMs). RGMs achieve state-of-the-art performance using a limited number of forward steps.\n- The proposed MAP estimation is efficacy, and a 2D example gives a visual validation.\n- The ablation study results are extensive and demonstrate the effects of all parts of the MAP. \n- The main paper and supplementary file are well prepared. The motivation is clear and reasonable. The paper is carefully organized.\n- The authors extend RGM to general restoration and propose a new model established upon super-resolution (SR).\n- The authors also provide code, which further shows the solidness of the work.\n\n**weaknesses**\n\n- The multi-scale training is introduced to alleviate the latent inefficiency of DGMs, where experiments are needed to prove it.\n- Figure 4 is not mentioned in the article, and some introduction should be added.\n",
            "clarity,_quality,_novelty_and_reproducibility": "quality: This is a new attempt to use the generative model for graphics restoration, which is reasonable and effective. The proposed RGM uses MAP estimation instead of MMSE in DGMs, which greatly improves the efficiency of the model.\n\nclarity: The overall writing and organization are good.\n\nreproducibility: The authors provide code.\n\noriginality: The innovation of this paper has not appeared in other papers.",
            "summary_of_the_review": "This paper proposes a new generative model, named restoration-based generative models (RGMs). RGM uses MAP estimation and incorporates implicit prior information via GAN. Furthermore, RGM is not limited to denoising and can be effectively extended to other IR tasks, like image SR. The overall writing and organization are good. It is a novel and meaningful work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3714/Reviewer_58kp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3714/Reviewer_58kp"
        ]
    },
    {
        "id": "EFPmDJS41M",
        "original": null,
        "number": 2,
        "cdate": 1666578541812,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578541812,
        "tmdate": 1669898866664,
        "tddate": null,
        "forum": "iNUtsk4h2q1",
        "replyto": "iNUtsk4h2q1",
        "invitation": "ICLR.cc/2023/Conference/Paper3714/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This manuscript proposed a variant of generative models called restoration based generative models (RGM). The key idea is based on a new interpretation of denoting generative models (DGMs) from an image restoration (IR)  perspective. By replacing the MMSE denoiser with MAP denoiser and introducing a regularized prior imposed by another generative models such as GAN, RGM apparently reduces the number of sampling steps. RGM has also been applied to inverse problems. ",
            "strength_and_weaknesses": "Strengths:\n1. A new perspective of RGM from IR is provides, which is interesting and opens up possibilities for improvements from denoiser design perspective.\n\n2. Using MAP with a regularized prior imposed by GAN, RGM achieves very good performance on standard datasets, and in particular, with a very few  (tens of) samplings steps.\n\n3. Extended applications in inverse problems are enabled. \n\nWeaknesses:\n\n1. The training process of RGM is much difficult than previous RGM, as shown in Algorithm 1.  In particular, a prior generator such as GAN is needed. In other words, two generative models are actually needed. Suppose that there is no GAN available at hand, then, to train a RGM, we have to first train a GAN, which itself is not an easy task. This kind of double-training is not preferred. \n\n2. Also related to the use of GAN as implicit prior. Intuitively, it is unsurprising that the performance would improve if an additional generative model is imposed as a prior. The authors claim that the use of MMSE loss is not as good as MAP. The authors verified the benefits of MAP in Figure 3 in the toy model.  I am a bit suspicious of this point. How can we confirm that the improvement comes from the MAP, rather than the implicit prior imposed by the additional generative model GAN in RGM? \n\n3. How is the performance of RGM for out-of-distribution datasets for inverse problems? It seems that by imposing the additional GAN prior, the obtained RGM is difficult to generalize to OOD datasets. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The RGM is an interesting idea by imposing an additional generative model such as GAN as an implicit prior. The idea is new and the presented performance is good. However, due to the re-use of an additional generative model, some limitations are expected as described above.   \n\nOther points:\n\n1. Can the authors explicitly show the sampling time of RGM compared with other generative models?\n\n2. Will different parameters of GAN will affect the final performance of RGM? This is important since, if so, then it implies that RGM highly relies on the success of GAN. ",
            "summary_of_the_review": "Overall the idea of RGM is interesting. From the IR perspective, RGM is designed by  imposing an additional generative model such as GAN as an implicit prior. My  main concerns are about the essential need of an additional generative model such as GAN, which might affect the training, stability, flexibility, and efficiency of the proposed RGM as described above. \n\nUpdate after rebuttal:\n\nI raised the score accordingly. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3714/Reviewer_3aDJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3714/Reviewer_3aDJ"
        ]
    },
    {
        "id": "9Uf-tk0x-Fr",
        "original": null,
        "number": 3,
        "cdate": 1666741834155,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666741834155,
        "tmdate": 1668396358390,
        "tddate": null,
        "forum": "iNUtsk4h2q1",
        "replyto": "iNUtsk4h2q1",
        "invitation": "ICLR.cc/2023/Conference/Paper3714/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes three changes to vanilla diffusion models: i) a regularization term in the training objective, ii) a source of randomness in the training objective to account for the fact that there is no single solution to the denoising problem iii) a class of more general corruption processes. The paper demonstrates that the resulting models are faster to sample from and they achieve better or on par with the baselines.\n",
            "strength_and_weaknesses": "Strengths:\n* The ideas presented in the paper are interesting. It is intuitive that better priors, introduced by a different forward model and a regularization term, can lead to better models.\n* Experimental results are quite convincing.\n* The observation that optimizing for super-resolution leads to better results for super-resolution is interesting. There are many recent papers that generalize diffusion to non-gaussian corruptions (e.g. Cold Diffusion, Soft Diffusion, Blurring Diffusion Models, etc) and it would be interesting to evaluate whether these models also perform better on the inverse problem they train for.\n\n\nWeaknesses:\n* The paper is poorly written. See analytical comments below.\n* The classical objective *is* a MAP method. As long as there is no stochasticity in the solution, you are maximizing something -- the only thing that is changing is your prior (imposed by the regularization term).\n* It is interesting that you introduce stochasticity to account for the ill-posedness nature of the inverse problems. However, what prevents the model from dismissing entirely the randomness? It is just an additional input, right? The model can always ignore it. Why is it best to not ignore it for minimizing the objective?\n* I am struggling to understand the added regularization. It seems that it is a trainable Discriminator (similar to the Discriminator in GANs). If that's the case, then you should also feed to the discriminator the real images, right? Eq. (8) doesn't make sense to me -- if the Discriminator doesn't see real images, how is it regularizing? How is the generator not fooling the discriminator? Now, even if we assume that there is a typo and the Discriminator actually sees real images, doesn't that introduce the training instabilities of GANs? Or mode collapse and less diversity? Please clarify these points in the rebuttal.\n* The theoretical underpinnings of the proposed objective are not discussed/explained. When there is no regularization term, it has been shown that the DSM objective learns the score function (and minimizes the KL between the true and the sampling process). Can similar guarantees be achieved for this new objective?\n* How is the sampling rule derived by using this objective? Since there is no guarantee that you are learning the score-function, can you still solve for the reverse SDE and get reasonable samples? Can we compute likelihoods? If so, how? Would be interesting to add likelihood evaluations to the paper.\n* The paper has several inaccurate and/or unsupported claims. It is not true that all diffusion models diffuse in the pixel space, e.g. see Latent Diffusion Models. It is also not true that Diffusion models typically use thousands of steps. The is a huge line of work for accelerating diffusion models, including Progressive Distillation. The state-of-the-art claim for CelebA-HQ-256 is not exactly accurate. From a quick search, it seems that StyleSwin (which is used as a baseline for the inverse problems) achieves FID 3.25, which is 2x better.\n* From the experimental evaluation of the paper, it seems that the main contribution to the improvement in the FID scores is the change of the forward process. RGM-D performs *worse* than the simple VE SDE. There are several recent works that use different distributions for corrupting images, e.g. Cold Diffusion, Soft Diffusion, Blurring Diffusion Models, Generative Modelling With Inverse Heat Dissipation, etc. Since the improvement is coming from the change of corruption, these other works should be discussed more extensively. Ideally, a performance comparison with these works should be made.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper's writing could be greatly improved.\nFirst, the acronym DGM is typically used for Deep Generative Models, not for Denoising-based Generative Models. Please use diffusion models instead.\n\nThere are several typos, spelling and grammatical mistakes. Some statements in the paper are also not true. Below there are some examples:\n\n* In the Introduction, it is not true that in diffusion models the latent and the data always possess the same dimension. Latent Diffusion Models (e.g. Stable Diffusion) diffuse in a latent space (encodings of natural images).\n* Eq. (4) is only true for specific types of samplers. E.g. in DDIM, the sampling process is not Markovian.\n* what do the authors mean by \"immovable diffusion\"?\n* what do the authors mean by \"latent inefficacy\"?\n* a multiscale training that resolve => [...] that resolves\n* denoising. deblurring, super-resolution and inpainting are all different inverse problems (Section 2). Also, in the Introduction, Image Restoration is not an inverse problem. It is a term that describes a family of inverse problems.\n* The acronyms VPSDE, VESDE are not explained (or even cited).\n* In Equation (5), the neural network should also take as input the timestep t.\n* In Section 3.1, the authors are effectively repeating the loss of Eq. (5) while presenting it as something that is changing.\n* It would be preferable to use more neutral/academic writing in some places. E.g. \"set their mind on\" -> consider. Also, \"tremendous amount of solutions\": the amount of solutions is either finite or infinite.\n* \"whereas DGMs use T=1000, 2000 steps\". Not true for many models/samplers. \n* \"a latent as much as dimension of pixel space\": There is a grammatical mistake here. Not sure what the authors are trying to say.\n* the links in the paper are not clickable.\n",
            "summary_of_the_review": "The paper presents some interesting ideas: GAN regularization in the training objective, different forward processes and a stochastic knob in the input of the generative model. However, there are several weaknesses (detailed above) that prevent me from recommending acceptance. The most important issue for me right now is that the main benefits seem to be coming from the change of the forward process and not from the regularization. If that's the case, I think the presentation of the paper needs a lot of restructuring and the baselines for the comparisons should be other methods that generalize diffusion.\n\nUpdate, Nov. 13: The authors addressed some of my concerns in the rebuttal and I am increasing the score from 3 to 5. I believe that there are still some concerns about the novelty of this work, but I am open to more discussion and perhaps a further increase of the score if these issues are addressed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3714/Reviewer_otH9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3714/Reviewer_otH9"
        ]
    },
    {
        "id": "J_ZnzLifkjW",
        "original": null,
        "number": 4,
        "cdate": 1666807212607,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666807212607,
        "tmdate": 1668712744989,
        "tddate": null,
        "forum": "iNUtsk4h2q1",
        "replyto": "iNUtsk4h2q1",
        "invitation": "ICLR.cc/2023/Conference/Paper3714/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to map from Gaussian noise to images by successively \"removing'' a series of degradations. In one proposed variation, these degradations are the addition of Gaussian noise, as in the diffusion model framework, and in another variation they include downsampling. The authors train a conditional GAN to stochastically \"undo\" each degradation.",
            "strength_and_weaknesses": "Strengths:\n- The method is an interesting combination of GANs with diffusion modelling which shines some light on the benefits of each.\n- Impressive empirical results, including state-of-the-art on CelebA-HQ at 256 resolution.\n\nWeaknesses:\n- The output of $G_\\theta(y,z)$ is repeatedly referred to as a \"MAP estimate\". This is not true; it is the stochastic output of a conditional GAN. Calling it a MAP estimate is misleading and should be changed.\n- I cannot find the training times listed anywhere, only the number of training iterations. I imagine the proposed method takes more time per iteration than a diffusion model due to the more complex GAN objective, and this should be reported. Similarly, it's not clear to me if each function evaluation at test-time is slower than for typical diffusion models. A statement on this would be interesting. \n- Progressive distillation ([Salimans & Ho, 2022](https://arxiv.org/abs/2202.00512), [Meng et al., 2022](https://arxiv.org/abs/2210.03142)) would be relevant work to comment on, since it speeds up sampling from diffusion models. [Salimans & Ho, 2022](https://arxiv.org/abs/2202.00512) achieve a FID of 3.0 on CIFAR10 with 4 NFEs and should probably be added to Table 1.\n\nMinor:\n- ~~It is not clear what metric is being reported in Table 4.~~",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Reasonably clear. In addition to the points listed above, the multi-scale RGM could be much better explained in the main paper though (based on the appendix, I believe it adds Gaussian noise as well as downsampling, which is not clear from the main paper).\n\nQuality: see above.\n\nNovelty: Novel AFAIK.\n\nReproducibility: Good. Code is provided.",
            "summary_of_the_review": "This paper presents shows that a combination of diffusion models with the GAN literature can obtain competitive results, which I believe is a good contribution. The weaknesses I listed were generally related to presentation and I am recommending acceptance as long as this is improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3714/Reviewer_VqGo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3714/Reviewer_VqGo"
        ]
    }
]