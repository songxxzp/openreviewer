[
    {
        "id": "Dfal48A9kQ",
        "original": null,
        "number": 1,
        "cdate": 1666551605192,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666551605192,
        "tmdate": 1670516890444,
        "tddate": null,
        "forum": "WpGqKAEwMn",
        "replyto": "WpGqKAEwMn",
        "invitation": "ICLR.cc/2023/Conference/Paper4417/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work discusses the connection between the sampling process of diffusion probabilistic models and a cognitive paradigm known as serial reproduction. By building the correspondence, the work tries to provide new theoretical understandings of DPMs and intuitions of improvement of key features of DPMs.",
            "strength_and_weaknesses": "Strength:\n\n- Theoretical understanding of diffusion probabilistic models is of great research interest recently. \n- The connection between the concept of serial reproduction and DPMs is also interesting.\n\nWeakness:\n- Contribution: It is not very clear what exactly is the contribution of the paper. The paper introduces a connection between the sampling process of DPMs and serial reproduction. Drawing inspiration from this connection, some key features of diffusion models such as noise scheduling can be principally improved. However, the conclusion that \"the noise schedule should be chosen such that it alters the input distribution gradually\" is not new (Dhariwal & Nichol, 2021 https://arxiv.org/abs/2105.05233).\n- Technical soundness:\n    * On page 4, between Eq.(9) and Eq.(10), the paper describes the sampling process of a DPM as $x_t \\rightarrow \\hat{x}_t  \\rightarrow  \\cdots$. Why a noisy version of $x_t$, i.e., $\\hat{x}_t$ is needed in the sampling process? This seems not true for the well-understood DPMs where the sampling process involves only the optimal posterior distribution (as in Eq.(5)).\n    * Latter derivations all depend on the above-mentioned assumption of the sampling process. So it should be crucial to clarify whether the standard sampling process resembles serial reproduction and what Eq.(10) actually means.\n\nMinors:\n- In section 2.2, the statement \"the noise distribution $q_n$ is stationary\" is not always true. For variance exploding diffusion (Song et al. 2020), the noise distribution is not stationary since the variance does not converge.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The notation used in the paper is a little bit difficult to follow. For example, the mathematical form of $T_{q_n}(x_t|x_{t-1},\\beta_t)$ is not explained in details and $\\beta$ is mentioned as diffusion parameters and strength parameter in different place.\n- Quality: I have raised some concerns about technical soundness. Please see my comments above.\n- Novelty: The connection discovered by the paper is novel.\n- Reproducibility is ok.",
            "summary_of_the_review": "In conclusion, I have raised several concerns regarding the contribution and technical soundness of the proposed method, which prevents me from giving this paper a higher rating now. I will consider increasing my rating if the authors address my concerns and clarify my misunderstandings.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4417/Reviewer_g85r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4417/Reviewer_g85r"
        ]
    },
    {
        "id": "OvCzhEvJLh",
        "original": null,
        "number": 2,
        "cdate": 1666602818481,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666602818481,
        "tmdate": 1670475884778,
        "tddate": null,
        "forum": "WpGqKAEwMn",
        "replyto": "WpGqKAEwMn",
        "invitation": "ICLR.cc/2023/Conference/Paper4417/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper builds connections between existing diffusion models such as DDPM with serial reproduction paradigm. One major conclusion is that, current diffusion models can be explained as a natural consequence of this connection correspondence. Simulations on the MNIST dataset shows that the connection can be utilized for richer noise distribution family such as bimodal and fade.",
            "strength_and_weaknesses": "Strong:\n1. The analysis and connection of between diffusion models and serial reproduction paradigm;\n\n2. Simulations on datasets to show the robustness of applying different types of noise distributions.\n\n\nWeak:\n1. Larger scale datasets and richer experiments can help improve the quality of this paper;\n\n2. The connection between diffusion models and serial reproduction paradigm seems to have less novelty in terms of methodology and it is difficult to say that new knowledge is obtained after reading this article.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper provides a new way of understanding why diffusion models work so well. There are several equations can be modified to align with existing DDPM mathematical equations. \nThe clarity and quality are average.\nThe novelty is limited and can be further improved.\nThe reproducibility of this paper is high since it is based on existing open-source code.\n\nDetailed questions and comments:\n\n1. Equation (5), better attach $\\theta$ for $p$ to indicate that it is trainable with parameter set and differs with $q$ in equation (4).\n\n2. Equation (8), $q$ better with a given sample $x_0$, e.g., from $q(x_{t-1}|x_t)$ to $q(x_{t-1}|x_t, x_0)$, to express the posterior distribution in the forward diffusion process.\n\n3. Do you have more results of simulation on larger datasets besides Mnist dataset? Figure 4 shows some results \u2013 they are largely influenced by the sampling methods you use as well, such as Euler-Maruyama, the predictor-corrector and ODE sampling. Currently, the T=50 vs. T=500 is significantly different and based on my own experiments, T=50 could yield quite good results with better sampling methods and without changing anything of the trained model.\n",
            "summary_of_the_review": "Given current strengthes and weaknesses, it is difficult to score a high recommendation to this paper. Some equations can be updated, richer datasets are preferred and more sampling algorithms can be used to perform rich ablation study.\n\n--\n\nI would like to rank this paper higher, after reading their detailed responses.\nMost of my questions were well answered.\nAgain, if any larger scale datasets' results are reported or shared, this paper worth an even higher score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4417/Reviewer_fE3L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4417/Reviewer_fE3L"
        ]
    },
    {
        "id": "ntwmmcZE9N1",
        "original": null,
        "number": 3,
        "cdate": 1666789857396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666789857396,
        "tmdate": 1668972392313,
        "tddate": null,
        "forum": "WpGqKAEwMn",
        "replyto": "WpGqKAEwMn",
        "invitation": "ICLR.cc/2023/Conference/Paper4417/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Inspired by a cognitive science paradigm, known as \"serial reproduction\", the authors show how the diffusion model approximates a Bayesian posterior at every denoising step, with the diffusion kernel as likelihood and the forward marginal as prior. This re-interpretation is enough to explain two properties of diffusion models that have been empirically observed: i) a diffusion model can approximate\nthe true data distribution irrespective of the choice of noise family for a fine enough noise schedule; and ii) the noise parameter has to diminish in order to reduce reconstruction error. The paper supports the theoretical insights with illustrative experiments, first on a setup where the noising and denoising distributions can be computed analytically and different noise schemes; and second, with MNIST images. One final insight provided by these experiments is that the sample accuracy saturates after some number of steps, and based on the theoretical insights, the authors suggest this is due to the forward process having converged to its stationary distribution.",
            "strength_and_weaknesses": "**Strength**\n- The paper establishes a novel and insightful connection with cognitive neuroscience, making it accessible and hopefully inspire other researchers.\n- The insights derived from this connection are relevant, as they explain two empirically observed properties, especially the robustness against the noise family, which made previous formulations that assumed Gaussian noise incomplete.\n\n**Weaknesses**\n- See comments on clarity below.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n- My only negative comment on clarity is regarding the use of the term \"sampling\". I think it is common to distinguish between \"learning/training\" and \"sampling\", the latter referring to the denoising process. However, in the introduction, it is said that \"sampling is then done using a sequential procedure whereby an input signal is iteratively *corrupted by noise and then denoised*\". I found including noising in the sampling unnaturally trying to resemble the explanation on serial reproduction. \n- Apart of the previous comment, the paper is well organised, generally well written and easy to read.\n\n**Quality.**\nFollowing up with my previous comment on \"sampling\", the same idea of adding noise during sampling is used formally in Sec. 3, when describing sampling process as a Markov chain, with terms like, $x_t \\rightarrow  \\hat{x}_t$. What are these terms? Are they just artefacts for the explanation, meaning that $x_t$ is drawn from some noise kernel? Or it is assumed that $x_t$ is actually corrupted with noise before every denoising step? Please be explicit in the text if this is an artefact to avoid confusion.\n\n**Novelty.** \nThe paper establishes a novel and insightful connection.\n\n**Reproducibility.** \nThe paper includes a final section on reproducibility pointing to the code in the supplementary material. While I agree that making the code available is the gold standard for reproducibility, I encourage the authors to describe all the missing details also in the appendix, especially regarding the deep learning experiments for completeness.",
            "summary_of_the_review": "#### Meta-review\nThe authors have addressed my concern on the definition of the denoising process, explaining that it follows the latest research on more effective sampling for the more general cold-diffusion processes, and showing an equivalence with the original DDPM derivation.\n\n---\n#### First review\nThe paper provides a novel and insightful formulation of diffusion models, shedding light on two empirical observations that had not clear explanation with previous formulations. This is an interesting contribution that improves our understanding of these powerful models. \nMy current recommendation is based on the understanding that my comment on quality is just a matter of clarity, and not a misunderstanding or variation of the denoising process.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4417/Reviewer_Q4TQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4417/Reviewer_Q4TQ"
        ]
    },
    {
        "id": "wY_bW9GGUw",
        "original": null,
        "number": 4,
        "cdate": 1667525450186,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667525450186,
        "tmdate": 1667525450186,
        "tddate": null,
        "forum": "WpGqKAEwMn",
        "replyto": "WpGqKAEwMn",
        "invitation": "ICLR.cc/2023/Conference/Paper4417/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper establishes a connection between diffusion models and serial reproduction. The main result is that as long as the distributions of images along the diffusion time remain approximately stationary, then the perfect solution for minimizing the KL between the sampling process and the real process leads to samples from the correct (clean) distribution. This holds independent of the corruption type, which explains to a certain degree the success of recent diffusion models with more general corruptions.",
            "strength_and_weaknesses": "I think this is an interesting paper. The main take-away (which is fairly intuitive) is that the corruption should be smooth -- and that's pretty much all that matters to sample from the correct distribution at the end. The main issue I have is that the mathematical statements yielding this result are not very precise, i.e. they do not consider how error propagates in Eq. 15 and the result only holds when the learning is perfect. \n\nStrengths:\n* the paper is well-written.\n* The simulation experiments are really interesting. They show that for toy-distributions (that we can characterize their density all the way), the corruption type is not really important to reconstruct the proper distribution. I am wondering if that's true for *any* corruption with smooth transitions.\n* The topic of the paper is relevant since there have been many recent attempts to generalize diffusion models.\n\n\nWeaknesses:\n* the mathematical statements in the paper are not precise. While I appreciate the fact that the authors try to build intuition, it would be cool to have an analysis of how the errors propagate in Equation 15. This \"approximately stationary\" statement might completely blow-up the distribution we are sampling from once you integrate over all diffusion steps. How \"approximately\" is needed to get a reasonable bound?\n* the analysis seems to be held only in the case where the solution gives 0 KL. How do learning errors propagate when that's not the case?\n* There is no characterization of the conditions under which \"approximately stationary\" is not too far off. How is this connected to the smoothness of corruption? Are there toy models that satisfy this property? Are there toy models that don't satisfy this property and for them the learning fails?\n* the connection to serial reproduction is interesting, yet kind of indirect. For me, the main point is that independent of the corruption type, you still sample from the correct final distribution as long as the condition of Eq. 15 is satisfied. It is true that in serial reproduction you also sample from the priors, independent of the transition kernel and I do appreciate the connection. However, I think it is an overstatement to say that this connection yields many insights on how diffusion models work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The connection to serial reproduction is novel. The mathematical analysis seems new, but it is not precise.",
            "summary_of_the_review": "I think this is a paper in an interesting direction. However, I am concerned because of the lack of clarity in the mathematical statements.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4417/Reviewer_xQWb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4417/Reviewer_xQWb"
        ]
    }
]