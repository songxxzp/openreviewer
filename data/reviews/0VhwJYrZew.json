[
    {
        "id": "s8kuRRrdfZ",
        "original": null,
        "number": 1,
        "cdate": 1666704221689,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666704221689,
        "tmdate": 1666840085937,
        "tddate": null,
        "forum": "0VhwJYrZew",
        "replyto": "0VhwJYrZew",
        "invitation": "ICLR.cc/2023/Conference/Paper2109/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the streaming property in ST and propose new streaming methods.\nThe two proposed methods: FINE-mask and FINE-wait are proven to be effective and achieve better trade-offs between translation quality and latency.\nFurthermore, these methods can effectively alleviate the mismatch problem between offline training and online inference.",
            "strength_and_weaknesses": "Strength:\nThe paper propose two different kinds of method and good results are obtained.\n\n\nWeaknesses:\nAlthough the investigation may draw a lot of interest, the conclusion is not surprising.\nIn 3.1, the investigation shows that the representations at the end position has a higher gap with the full input case. This is not surprising as   frames near the boundary has no future context comparing with the full input case.\nBy the way, if the end position is actually the end position of the whole sentence, the representations should have no difference with the full input case.\n\nI also think the authors should compare their methods with the idea in \"Low latency end-to-end streaming speech recognition with a scout network\"\n\nThe FINE-wait method proposed by the paper is not new.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clearly written.\n\nQuality: Standard.\n\nNovelty: Marginal. The novelty is mainly contributed by the FINE-mask method. For the FINE-wait method, it is not new and is just slightly modified from existing wait-k method.\n\nReproducibility: The codes are provided in the paper and others can replicate the results.\n\n",
            "summary_of_the_review": "This paper is motived by an observation that representations at the end position are usually very different from the full input case.\nThe authors try to solve this problem by proposing two different methods and good results are obtained.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_JSnA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_JSnA"
        ]
    },
    {
        "id": "XmNKv5-mtd",
        "original": null,
        "number": 2,
        "cdate": 1666768713330,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666768713330,
        "tmdate": 1666768894365,
        "tddate": null,
        "forum": "0VhwJYrZew",
        "replyto": "0VhwJYrZew",
        "invitation": "ICLR.cc/2023/Conference/Paper2109/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the difference is speech representations towards end for streaming input vs when complete utterance input is available. Based on this investigation the work proposes future aware streaming ST which introduces few frames of future speech signals. Extensive experiments verify the improvements compared to other recent works. ",
            "strength_and_weaknesses": "### Strengths\n- Both FINE mask and FINE wait methods are simple modifications for streaming speech translation\n- FINE Mask is a clever strategy to make the output representations similar to offline full context speech and streaming speech.\n- Results compared to MoSST baseline are quite significant in the low latency region.\n- Paper is well written and supports the hypothesis with extensive experiments and analysis.\n\n\n### Questions\n- FINE-wait approach is not entirely novel, and is also mostly known as a latency/performance trade-off, is there anything different that I missed?\n- As I understand the authors reimplement the MOSST baseline. From Fig 5 RealTrans and MU-ST perform better on low latency region. Can FINE mask and FINE wait extended to other existing methods that perform better in the low latency region?\n- Recently Conformer architecture has shown improvements over transformer for speech related tasks. Did the authors consider trying better encoder layers like conformer as well as better pretraining methods for the encoder like w2vBERT? Will that make applying FINE-mask approach difficult?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and detailed. The experiments are thorough and detailed and the ideas are simple and novel. Code and work should be reproducible. ",
            "summary_of_the_review": "Overall the work is novel and interesting, well supported by experiments and analysis. Hence my high rating.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_3NNH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_3NNH"
        ]
    },
    {
        "id": "ylXEtI4wTs",
        "original": null,
        "number": 3,
        "cdate": 1666851188524,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666851188524,
        "tmdate": 1666851188524,
        "tddate": null,
        "forum": "0VhwJYrZew",
        "replyto": "0VhwJYrZew",
        "invitation": "ICLR.cc/2023/Conference/Paper2109/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper authors present their work on analyzing speech representation mismatch between non-streaming and streaming setting, for speech translation. Based on it, authors propose to leverage future-aware inference (FINE) (including FINE-Mask and FINE-Wait) for mitigation, and experimental results show the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "Strength: The proposed methodology is technically sound. Overall, this paper clearly presents the proposed approach, with well designed analysis and experiments presented in both main paper and appendix.\n\nWeakness: The proposed FINE-Mask and FINE-Wait are mostly based on existing technics, with limited novelty. For example, FINE-Wait appears to be a standard practice to incorporate future contexts. Also I think the experimental section may need clarification on several items (see questions in the summary section below).",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written overall, and it is reproducible. It has limited originality.",
            "summary_of_the_review": "This paper clearly presents authors' work to improve streaming speech translation, including mismatch analysis, the proposed FINE-Mask and FINE-Wait approaches, experimental design and ablation studies etc. While there are good thoughts and experiments put in it, I find the proposed approach is mostly based on existing techniques or standard practice. I'd also suggest authors consider addressing the following questions:\n\n1. For the results shown in Figure 5, do authors have comments on model size and computation cost, when comparing the proposed methods vs. baselines?\n2. On page 7, it's said \"The length of future context tokens (m) is 50 and 10 for FINE-Mask and FINE-Wait, respectively.\" How to select the value of m, and why is it different for both?\n3. Do authors think when training is conducted in a streaming manner (e.g. similar as FINE-Mask and FINE-Wait), accuracy could be further improved with streaming inference? If so, is there a way to train speech translation models using streaming inputs, and generalize it for different streaming inference configurations?\n4. In the last section of conclusion, it's said \"In the future, we will experiment with other methods to improve the accuracy of predicting future information.\" This is not informative. It would help to list examples for methods/directions authors think worth exploring, if applicable.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_2wmu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_2wmu"
        ]
    },
    {
        "id": "-SPBq5Q4IK",
        "original": null,
        "number": 4,
        "cdate": 1666906661120,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666906661120,
        "tmdate": 1666906661120,
        "tddate": null,
        "forum": "0VhwJYrZew",
        "replyto": "0VhwJYrZew",
        "invitation": "ICLR.cc/2023/Conference/Paper2109/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new algorithm for streaming speech translation by improving the wait k policy-based approach with a mask trick. The paper first starts with the streaming speech translation issue caused by the end-position speech feature representations with various analyses. This analysis naturally motivates their proposed method of using mask tokens or extra speech tokens to avoid dealing with unstable end-position speech feature representations. The experiment is based on a public speech translation corpus (MustC). It shows the effectiveness of the proposed method based on the latency and translation performance trade-off with several ablation studies. ",
            "strength_and_weaknesses": "Strength\n- Streaming speech translation becomes an essential technique for conversational AI.\n- Clear analysis of the streaming issue in speech translation by focusing on the end position. I like their formulation of the w2v2 output representation for streaming and offline based on two-time indexes $a _{t, t}$ (but it requires a bit more explanation). This makes it easy to understand the feature representation in the end position.\n- FINE-mask/wait are simple but powerful\n- The paper is well written overall.\n\nWeaknesses\n- Although this would be a strength, I think the method is a bit too simple and does not have as much algorithmic/theoretical novelty as a machine learning conference.\n- I want to have more relationships with streaming speech recognition techniques. I know that some methods (e.g., RNN-T) cannot be straightforwardly used for speech translation due to the reordering issues, but there are several attention-based streaming ASR methods (e.g., monotonic attention-based approaches like mocha https://arxiv.org/abs/1712.05382). \n- It is better to have more challenging language pairs with more serious reordering issues, e.g., En-Ja is recently supported in Must-C.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n- The paper is written in general. \n- Since I have more ASR backgrounds (and many other people interested in this work would be the same backgrounds), I think it is better to mention that this paper deals with an attention-based method and briefly describes other streaming speech processing based on RNN-T and CTC in the beginning.\n- The end of the introduction discusses performance improvement, but since it does not clearly mention the baseline, I could understand the significance of this improvement. It is better to briefly explain the baselines for the readers to understand the importance of this method.\n- MoSST seems to be an important base model of paper, but it does not explain it clearly. Also, it is better to explain why the paper adopts MoSST as a base model.\n- Section 3, 3rd paragraph suddenly uses \"a factor 320,\" which is confusing. I expect the paper to assume 16kHz sampling and 320 samples (corresponds to 20 ms frameshift) in W2V2. It is better to clarify it.\n\nQuality\n- This work has a solid analysis to motivate the proposed method. This study's quality is sufficient with the proposed method's apparent effectiveness.\n\nNovelty\n- I'm not entirely convinced of this method in terms of the significant novelty for the machine learning community since the method attaches special tokens or waiting schemes. They are very logical based on their analysis, but the techniques are not significantly novel.\n- If there are more technical novelty, solid theoretical background, etc., for the FINE-Mask/FINE-wait, I would recommend the authors spend more space explaining them in more detail.\n\nReproducibility\n- The database and evaluation metrics are publically available. The reproducibility is high.",
            "summary_of_the_review": "Although the paper is clearly written with its rationale for solving the end-position issue by simple speech token augmentations, I feel that the technical novelty of this paper is not significant due to the above reasons.  \n\nSeveral other suggestions and questions:\n- How about applying this technique to steaming ASR? This will prove the generalization of this work and would attract more machine learning researchers\n- I thought that $t'=t$ in Fig.2 (c) has the highest cosine similarity compared with the other two examples since this streaming condition is the same as the offline case (I think I misunderstood something). Can you explain it?\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_xgfh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_xgfh"
        ]
    },
    {
        "id": "lSpKWi7PAx",
        "original": null,
        "number": 5,
        "cdate": 1667176004783,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667176004783,
        "tmdate": 1667176004783,
        "tddate": null,
        "forum": "0VhwJYrZew",
        "replyto": "0VhwJYrZew",
        "invitation": "ICLR.cc/2023/Conference/Paper2109/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the differences between the training and inference conditions of streaming models. They first present a thorough analysis of the problem by finding correlation between representations during offline and streaming modes and plot that against the model performance in terms of BLEU score. They show that the deterioration happens from the last representations of the input. \n\nIn order to fix this problem they propose a simple strategy during inference to match the conditions that wav2vec 2.0 based models are trained towards. They propose two strategies FINE-Wait and FINE-Mask and also discuss a combination of it called FINE-Hybrid. These techniques essentially create a pseudo longer context using mask tokens (matching the wav2vec 2.0 training criterion) or using actual speech tokens for better contextualization and then dropping it during inference. \n\nOverall, the technical contribution is low but the idea is neat and simple with a good motivational analysis. The drawback is that it's quite constrained towards models trained on wav2vec 2.0 criterion. ",
            "strength_and_weaknesses": "**Strengths**\n1) Strong motivation with supported analysis for the motivation and a simple solution towards this problem. \n2) The paper is well written and clear to understand. \n\n**Weaknesses**\n1) It seems like the approach is limited to wav2vec type models from its motivation. But in my opinion this approach should work for any kind of models, specially the FINE-Wait. A study or discussion on this would have made the paper stronger. \n2) The paper is also missing details/analysis on how does number of fine-tuning steps affects the FINE-Mask strategy. If the model is reliant on the MASK tokens that are used during self-supervised pre-training I would be curious to learn if amount of fine-tuning causes a change in the performance.\n3) Similarly, the approach could have been stronger if the authors also studied how these model training could be modified to match the inference conditions. For example, maybe providing future biased masking during fine-tuning would allow the models to match the inference conditions. ",
            "clarity,_quality,_novelty_and_reproducibility": "1) **Clarity and Quality** : Very Clear and Well Presented \n2) **Novelty** : Low, yet a simple and effective approach\n3) **Reproducibility** : Low, no details on the model was trained\n",
            "summary_of_the_review": "I think the paper is well motivated, simple yet strong, if the authors are able to comment on the weaknesses I pointed out, this paper can be quite insightful. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_uAxq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2109/Reviewer_uAxq"
        ]
    }
]