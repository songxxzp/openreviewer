[
    {
        "id": "Nblc1uYrij",
        "original": null,
        "number": 1,
        "cdate": 1666574303345,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574303345,
        "tmdate": 1666574303345,
        "tddate": null,
        "forum": "-XC_lMynIT",
        "replyto": "-XC_lMynIT",
        "invitation": "ICLR.cc/2023/Conference/Paper2439/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "Authors applied deep learning algorithms to propose a segmentation free algorithm for RNA modification position detection. This study suggested a new tool for analyzing RNA modification position detection with efficient procedures from raw signals.",
            "strength_and_weaknesses": "Strength: While I am not an expert in RNA analysis and detection, the methods used in this study are widely and popularly applied in image and speech processing for recognition and detection task (such as deep feature extraction, CTC sequence mapping and alignment, attention, as well as the multiple instance learning framework), it is a good idea to apply these techniques for the new task. If there is no such kind of research in this specific field (for inference of RND modifications), I believe that it will be a good start to explore these kind of techniques for this new task, and possibly bring a large convenience for advancing the study in this new field.\nWeakness: Several information are missed for clearly capture the idea and application: 1. A detailed network architecture framework may be drawn to show the signal flow including feature extraction and position detection (CNN and LSTM, layers, number of neurons, and possible effects of different selection of their size).\n2. Training data size, usually in image and speech, the training data size are large, e.g., millions of pictures for images, and thousands of hours of speech. I am not sure for RNA data, how large size of the training sequences. And usually, data of RND modification is rare, how authors deal with this kind of lack of data to balance the training data.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was written well and easy to follow, and the algorithms seems not difficult to reproduce (of course further detailed information of algorithms are necessary, e.g., network architectures, optimizers, and detailed training process)\nThe novelty depends on this new task if nobody has investigated deep learning algorithms for in this field (otherwise, the algorithms and techniques have already been widely used in other field). ",
            "summary_of_the_review": "Authors propose to apply deep learning algorithms (as well as MIL framework) for inference RNA modifications which is segment free from raw signal. The study provided a new efficient way for (RNA modification inference) if there are no or few study in this aspect. The idea seemed work well and could provide a new tool for inference of RNA modification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2439/Reviewer_MYMy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2439/Reviewer_MYMy"
        ]
    },
    {
        "id": "f_hrhcmbk8",
        "original": null,
        "number": 2,
        "cdate": 1666597170958,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597170958,
        "tmdate": 1666597170958,
        "tddate": null,
        "forum": "-XC_lMynIT",
        "replyto": "-XC_lMynIT",
        "invitation": "ICLR.cc/2023/Conference/Paper2439/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposed a scheme of segmentation-free inference of RNA modifications.\n\nThe scheme includes:\n1. Basecalling of RNA squiggles: transcribe the current signal into RNA nucleotides (G, A, C, U) using the method of connectionist temporal classification (CTC). The representation of the basecalling, say f(x), will be used for the subsequent basecalling model.\n2. Embedding of z at position j: passing K=10 flanking nucleotides around j-th position using bidirectional LSTM.\n3. Compute the final representation: using scale-dot attention between the embedding obtained in stage 2 and the basecalling representation f(x).\n4. Multiple Instance Learning (MIL): use attentive pooling to aggregate multiple signal chunk representations.\n5. RNA modifications detection model training.\n\nCompared to the conventional methods, this work does not require the segmentation process which accelerate the inference.",
            "strength_and_weaknesses": "Strength:\n\n1. The paper seems to be the first to use segmentation-free methods for RNA modifications.\n2. Well written with enough experimental comparisons.\n\nWeakness:\n1. The authors should mention this is the first one to use segmentation-free methods for RNA modifications somewhere in paper.\n2. Typos:\n    (1). The formula before eq.1, there are two As, should be T for the second A.\n    (2). Section 3.2, s hat x_i, should it be s hat instead?\n3. In Section 3.3, could you please add which table you are describing for each paragraph? It would be easier for readers.\n4. For the results in Figure 3, I am wondering if you can add the results of testing m6Araw with CPU. It is fair to compare other methods using the same configurations. If the methods are used in practice, CPU time would be a more important factor from my perspective.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: clean, well written\n\nQuality: good\n\nNovelty:  I am not confident in the novelty because most of the methods are proposed in other areas. For example, CTC is proposed for speech recognition and attentive pooling is used in many places. The contribution would be the whole scheme by combing these methods to achieve the task.\n\nReproducibility: should be reproduced",
            "summary_of_the_review": "Since the paper emphasizes the advantage of inference speech for the proposed method because of the segmentation-free process, the CPU time results should be added in Figures. This should be a very important factor either as a fair comparison, or for readers' interest.\n\nOverall, this paper uses many machine-learning techniques in other areas for RNA modifications. The segmentation-free scheme is novel. \n\nThis paper is above the acceptance threshold",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2439/Reviewer_rbVq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2439/Reviewer_rbVq"
        ]
    },
    {
        "id": "vwWW3abLNSQ",
        "original": null,
        "number": 3,
        "cdate": 1666626575193,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626575193,
        "tmdate": 1666626575193,
        "tddate": null,
        "forum": "-XC_lMynIT",
        "replyto": "-XC_lMynIT",
        "invitation": "ICLR.cc/2023/Conference/Paper2439/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper works on the problem of RNA sequencing. This problem in the nutshell is to go from the electric signal x to the sequence of the nucleotides z. In particular, the paper considers the sequences that contain some particular RNA modifications. In order to tackle the modification, one needs to adjust the nucleotide sequence by making some changes.\n\nThe paper proposes to use several methods: the CTC loss for the alignment of the input to the output; the attention to perform the modification; and the multiple instance learning to train a model for instance modification.",
            "strength_and_weaknesses": "# Strengths\n\nThe main strength of this paper seems to be the experimental evaluation. While I'm not an expert in the area, I cannot thoroughly review this part. Nevertheless, it seems that the paper conducts a great number experiments with prior and the proposed models on two datasets.\n\nAnother strength is how the paper describes the biology part. While my knowledge here is very low, I was able to follow and understand the problem setup.\n\n# Weaknesses\n\nThe main weakness is the low novelty of the paper from the point of view of the machine learning community. The methods used in the paper are well known and the main novelty seems to be the application to the new task.\n\nThen, it was hard to follow parts of the Section 2. The description of the ML methods used in the paper is very clunky and contains many small mistakes aggravating the issue.",
            "clarity,_quality,_novelty_and_reproducibility": "# Quality\n\nIn general, the paper has good quality. The proposed methods are well motivated and correctly applied. While there are many small mistakes, I believe that the paper is generally correct.\n\nThe experimentation seems to be performed well. The paper gives multiple comparisons to prior works and provides ROC curves for comparison allowing for deeper analysis.\n\nThe quality of citations is low.\n - CTC: Graves et al. paper's contribution was the introduction of CTC and a dynamic programming algorithm for the gradient computation, not the maximum likelihood estimation. This paper also introduced the epsilon, not the cited paper Silvestre-Ryan & Holmes. \n- Attention: Vaswani et al., 2017 did not introduce the attention. Niu et al. does not look like a high quality citation.\n\n# Novelty\n\nThe main novelty of the paper is the application to the novel task. While this application is clever, the novelty for the machine learning community is low.\n\n# Clarity\n\nI found the introduction to be well written and easy to follow. I am not an expert in biology, but I was able to understand the task and the motivation of the paper.\n\nThe Section 2 was much harder to follow. The main reason is that the paper doesn't indicate which methods are existing and which are being proposed. For example, 2.1 describes the standard CTC loss restricted to 5 characters.\n\nAnother reason I found it hard to follow is the notation. Specifying the dimensionality is a good practice, but it makes all the expressions much clunkier. Then, using the domain as the argument of a function as in Eq. 3 is a very heavy abuse of notation. VQ in Eq. 4 should be Q K^T. Perhaps, it would be easier to follow if the paper introduced one symbol notation for function outputs. It could also help to give an overview before diving into the details (\"we apply an LSTM on features xyz followed by the attention between abc and def\").\n\nFig. 3 and 4 are not readable for the people with problems with color perception.\n\n# Reproducibility\n\nThe work should be reproducable.\n\n# Typos\n\n- p.3 \"dynamic programming\"\n- p.3 \"dimensional representations\"\n- p.4 \"that transforms the representation\"\n- p.4 \"abusing notation\" (and this abuse of notation was not used later in the paper, maybe just remove this phrase?)\n- p.4 \\pi(t, T)\n- p.4 possible paths\n- p.4 all these operations\n- p.5 QK^T\n- p.7 several partitions\n- p.9 Several works have\n",
            "summary_of_the_review": "In general I liked the proposed application in this work, but I have to reject it for this conference because the novelty for the machine learning community is low. The secondary reason for rejection is that some crucial parts of the paper are hard to follow.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2439/Reviewer_FDNP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2439/Reviewer_FDNP"
        ]
    },
    {
        "id": "8gQSkBOW4Z",
        "original": null,
        "number": 4,
        "cdate": 1667675588324,
        "mdate": 1667675588324,
        "ddate": null,
        "tcdate": 1667675588324,
        "tmdate": 1667675588324,
        "tddate": null,
        "forum": "-XC_lMynIT",
        "replyto": "-XC_lMynIT",
        "invitation": "ICLR.cc/2023/Conference/Paper2439/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work the authors present m6araw, a method for detecting m6A modifications directly from raw ONT long reads. As the authors nicely explain, current methods require a two step process: One for translating the raw signal from ONT into RNA sequence, another one for detecting the m6A modifications in those sequence segments. Instead, m6araw performs both tasks simultaneously. One component of the model is trained to do sequence detection while the second part uses the hidden representation from the first part but combines it with additional representation to call the modification sites. The authors evaluate their method on two datasets showing that they perform similar to state of the art methods while enjoying a significant boost in mem/time performance.\n",
            "strength_and_weaknesses": "Overall, we found the paper to be much more clear and better written than a previous version we reviewed for NeuIPS. It was much easier to understand how the model is organized, the motivation behind the modeling approach, and the relation to previous work. The authors also added information about comparative time/memory which is crucial to make a claim about significance.\n\nAdmittedly, the proposed method for segmentation free detection of m6A uses sequence to sequence modeling components that by themselves are not novel and commonly applied in other domains (e.g. CTC loss, LSTM, and gap character). Thus, the method itself is not as new as the tailoring of these components to the problem at hand which, as far as we can tell, has not been done. That said, the combination of those components in a sensible way to produce competitive results is not a trivial feat and should be acknowledged, especially with ICLR being focused on learning representations.\n\nGiven the above our overall excitement about the paper is not high yet positive. We believe the specific issues listed below are addressable.\n\n1.\nPlease define the dimensions of the spaces you are dealing with and the relation between them: L, T, D, H. M. What are used sizes of those and why?\n2.\nBasecalling: Authors report results for this task but not other methods\u2019 performance. We understand it\u2019s not the main topic but it would be good to at least have a sense how well the method does on this task.\n3.\nThis question was asked about the previous version as well but was left unanswered: How does the model perform and generalize to unseen datasets that were not included in training? How does the generalization compare with other existing methods? Mateos et al., 2022 follow Yao et al 2021 and make a difference between classification of read signals from kmers context used for training (but read signals not used) aka \u201csensor generalization\u201d and ability to classify signals from k-mers contexts not seen during training i.e. k-mer generalization. They also include assessment of effects in cases where multiple (and different) modifications may be present.\n4.\nWhat are the motifs in Table 2 left column? How are these params decided upon?\n5.\nWhat is \u201cBackground\u201d in Fig 6? How are these results supporting your claims exactly?\n\nMinor comments:\nThe term \u201cdeep features\u201d is used repeatedly but not defined.\nThe paper is peppered with small errors, some of which are listed below. The authors should make a concerted effort to scan and remove those.\n\nP2: The squiggle is deciphered a series of \nP3: \u2026. Is leveraged\u2026.. \nP4: The output the two modules\nP6: The dataset was spit on (definitely my favorite)\n",
            "clarity,_quality,_novelty_and_reproducibility": "See comments above. There is no code so reproducibility can not be assessed and parameters and settings (some are asked about above) can not be figured out either. This will need to be addressed in order to be fully assessed.",
            "summary_of_the_review": "Overall the paper offers a method which uses components from other domains, tailored together in a sensible way to address an important question (detection of RNA modifications) with the main advantage being significant improvement in time/memory compared to existing methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2439/Reviewer_NJ1g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2439/Reviewer_NJ1g"
        ]
    }
]