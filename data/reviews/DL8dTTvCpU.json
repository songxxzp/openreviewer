[
    {
        "id": "hu1R3mLicrg",
        "original": null,
        "number": 1,
        "cdate": 1665768954034,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665768954034,
        "tmdate": 1665995760924,
        "tddate": null,
        "forum": "DL8dTTvCpU",
        "replyto": "DL8dTTvCpU",
        "invitation": "ICLR.cc/2023/Conference/Paper2312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work propose DGT, a transformer architecture for dealing with graphs with a large number of nodes.\nDGT combines an adaptation of the deformable attention scheme to graphs and a new learnable position encoding for nodes.\n\nThe deformable attention mechanisms consists in sorting all the nodes in the graph w.r.t. some criterion viewed from all nodes (e.g., BFS). This is a pre-processing step. Then, for a given query node and each combination of (sorting criterion, head), for each k of the K first nodes, a deformable attention is computed. More precisely, the deformable attention interpolates the values of all the nodes w.r.t. node k in the sorting using the RBF kernel.\n\nOn the other hand, the new Katz position encoding counts all paths between a given node and all the others with decaying weight for longer paths. A subsequent processing by a MLP makes it learnable.\n\nThen, the authors demonstrate the superiority of their approach on several homophilic and heterophilic node-classification datasets in terms of accuracy and FLOPs. Some ablations study are performed to demonstrate the usefulness of their relative multi-criteria search and position encoding separately. Finally, the work study the nodes that are selected by their attention mechanism compared to Graphormer.\n",
            "strength_and_weaknesses": "Strengths:\n- This work address an important problem, namely how to make graph transformers work when the number of nodes is large.\n- The strategy proposed by the authors seems very effective on all datasets both in terms of accuracy and FLOPs.\n- The authors provide ablations to study the effectiveness of their two contributions (DGA and Katz PE).\n\nWeakness:\n- Although the paper is mostly well-written, I had a hard time understanding how DGA works. In particular, the idea of sampling offsets could be made more clear both in the text and in the figure (I do not understand what do the arrows do in the blue boxes in the center) so that the work is more self-contained.\n- The claim that other architectures query irrelevant nodes could be backed more rigorously (see below).\n- I am not sure the comparison are fair or at least clear in terms of model size (see below). \n- It would be even more convincing to demonstrate that DGT is also good at graph classification/regression tasks",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- Are the K values the first from the sorting? If so, it could be interesting to space these values since you already interpolate between neighbors. \n- If I understand correctly, DGA interpolate on all the nodes of a given sorting for K=4 references. Wouldn't it be worth trading the interpolation for increased K?\n- It could be useful to give the size of the models. It seems that the number of parameters in DGT scales linearly with the number of sorting criteria?\n- I am not super convinced by the idea proposed in Figure 2: why querying nodes with similar labels necessarily makes an attention mechanism better? Why would only similar nodes be relevant and not dissimilar nodes? In light of this, I think the claim that other methods aggregate information from irrelevant node exaggerated.\n\nQuality:\n- The work successfully tackles an important problem and both contributions (DGA and Katz PE) could be reused in subsequent works\n- The experiments use several baselines with thorough error bars and both homophilic and heterophilic graphs, with ablations hence are very convincing.\n\nNovelty:\n- I have a slight doubt on the novelty of Katz PE. Indeed, it is common in the graph kernel litterature to enumerate paths. Could you comment on this? See e.g. [1].\n\nReproducibility:\n- The code and hyperparameter grid were provided.\n\n[1] Shortest-path kernels on graphs (Borgwardt and Kriegel, 2005).\n",
            "summary_of_the_review": "This work combines two effective mechanisms to propose a graph transformer scaling to large number of nodes with very good performance in terms of accuracy and FLOPs. It also has some defaults such as lack of clarity for one of the two mechanisms (sampling offset in DGA), claims about novelty of Katz PE and about how classical architectures aggregate \"irrelevant\" nodes that could be better supported. I also think it would be interesting to demonstrate the usefulness of DGT for graph classification. \n\nOverall, I think the pros outweight the cons and I recommend acceptance. I would further raise my score if these concerns are answered.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_KagM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_KagM"
        ]
    },
    {
        "id": "l_OY0qD1fOd",
        "original": null,
        "number": 2,
        "cdate": 1666631970051,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631970051,
        "tmdate": 1666631970051,
        "tddate": null,
        "forum": "DL8dTTvCpU",
        "replyto": "DL8dTTvCpU",
        "invitation": "ICLR.cc/2023/Conference/Paper2312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Graph transformers have drawn growing attention in representation learning on graph-structured data. However, their success is limited to small graphs due to the quadratic complexity of the dot-product attention module in transformers. This paper proposes the Deformable Graph Transformer (DGT) to address this issue, by using a deformable graph attention (DGA) module with a linear complexity in the number of nodes. The proposed DGA, inspired by recent work in computer vision, only attend to a small set of key sampling nodes around a reference node. The set of key sampling nodes is learned by learning their positions in a sorted list pre-defined by each reference node based on some similarity criteria with respect to the reference node, such as personalized Pagerank score (PPR), BFS, or node feature similarity. The paper also proposes a positional encoding to use with the proposed DGT. The authors validate the proposed method on several node classification tasks.",
            "strength_and_weaknesses": "##### Strengths\n1. Development of efficient transformers is a challenging and important task. To my best knowledge, this is the first work on linear transformer specifically developed for graphs.\n2. The proposed method seems to scale well to large datasets with up to 232k nodes.\n##### Weaknesses\n1. (Presentation of the method) The paper does not do a great job of presenting the proposed method and highlighting the technical novelty. Section 3.2 is not very accessible for readers not familiar with previous work on deformable transformers, and is hard to identify the technical contribution from the previous work. It seems that many new concepts introduced here such as sampling offsets or using interpolation to find the representation at fractional location have already been introduced in [1]. I recommend a separate background section including section 3.1 and more preliminaries of deformable transformers to make the paper self-contained.\n2. (Clarity of the method) The main contribution of the work should be to use a NodeSort module to enable offset sampling and thus key node sampling. However, the presentation of this module is very short and there is even no detailed description about each sorting criteria, neither a reference provided. Besides, I am not sure some of them have a proper definition of ordering. For instance, if I understand correctly that BFS denotes the breadth-first search (the paper uses this abbreviation without giving the full name), then its order cannot be unique and could be exponential in the worst case. I expect more a more detailed presentation of all these sorting criteria. In addition to this, I also found some other missing details. For instance, how the offsets are computed is only described in the caption of Figure 1, which largely reduces the readability of the paper.\n3. (Missing baselines) There are a few recent graph transformers leveraging better the structural information about the graph such as [2, 3]. In particular, [3] explore different linear/sparse attention methods from NLP, such as Performer or BigBird, to make graph transformers more scalable. I expect an empirical comparison with these methods to justify the practical impact of DGT.\n4. (Weak results on ogbn-arxiv) A quick check on the leaderboard of [ogbn-arxiv](https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-arxiv) suggests that the SOTA performance is around 76% while the best performing methods in Table 1 of this work only achieves 71%, which is far behind the real SOTA. I am skeptical about the so-called SOTA performance on other datasets and the claim in the abstract that DGT achieves SOTA performance on 7 datasets.\n5. (Novelty of positional encoding) The proposed Katz positional encoding is very similar to the random walk positional encoding introduced in [4, 5], which have already been widely used in many graph transformers. I recommend the authors to check and correct the claims on this contribution.\n6. (Missing hyperparameter studies) Comparison of using different sorting criteria is shown in Table 3. However, the results are incomplete and results for only using PPR or Feat are not given. In addition, a study on the effect of different values of $\\gamma$ would also be useful as its search space seems to be much larger than other hyperparameters.\n\n##### Minor comments\n1. The way of measuring FLOPs could be variable and ambiguous. Could the authors elaborate more on how they measured the FLOPs for different methods? Could the authors also report the runtime per epoch?\n2. In section 3.3, \"path\" should be replaced with \"walk\" that allows repeated nodes.\n\n##### References\n[1] Zhu, Xizhou, et al. \"Deformable DETR: Deformable Transformers for End-to-End Object Detection.\" ICLR 2020.\n\n[2] Chen, Dexiong, et al. \"Structure-aware transformer for graph representation learning.\"\u00a0ICML 2022.\n\n[3] Ramp\u00e1\u0161ek, Ladislav, et al. \"Recipe for a General, Powerful, Scalable Graph Transformer.\"\u00a0NeurIPS 2022.\n\n[4] Dwivedi, Vijay Prakash, et al. \"Graph Neural Networks with Learnable Structural and Positional Representations.\"\u00a0ICLR 2021.\n\n[5] Li, Pan, et al. \"Distance encoding: Design provably more powerful neural networks for graph representation learning.\"\u00a0NeurIPS 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of method is unclear and hard to follow. More details about this can be found in point 1, 2 of __weaknesses__.\nI also have some concerns with the quality of experiments (3, 4 in __weaknesses__), technical novelty (5 in __weaknesses__), and reproducibility (6 in __weaknesses__).\n",
            "summary_of_the_review": "Despite the efforts made by this work to solve a challenging and important for graph transformers, I recommend weak reject because 1) the presentation of method is imprecise and lacking a lot of details; 2) there are important missing baselines and the reported results seem to be far behind SOTA results; 3) the contribution to positional encoding is over-claimed due to incomplete related work; 4) some hyperparameter studies are missing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_wsk3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_wsk3"
        ]
    },
    {
        "id": "PtY_ANExCU",
        "original": null,
        "number": 3,
        "cdate": 1666709512916,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666709512916,
        "tmdate": 1666709512916,
        "tddate": null,
        "forum": "DL8dTTvCpU",
        "replyto": "DL8dTTvCpU",
        "invitation": "ICLR.cc/2023/Conference/Paper2312/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper propose a new graph sampling mechanism for reducing the computational complexity in full attention of graph transformer. The sampling mechanism takes structural and semantic similarity into consideration. Experimental results show the effectiveness of the proposed method. Additionally proposed positioanl encoding enhances the performance of the model. ",
            "strength_and_weaknesses": "Pros:\nThe motivation makes sense. The paper is easy to follow. \n\nCons:\n1. The proposed method seems to be a graph sampling mechism + full attention on the sampled node subset. Therefore the claim of \"sparse attention\" should be clarified.\n2. It's strange to run a Transformer-based graph model on full graph for node representation tasks without any graph sampling methods (e.g., random walk). Therefore, the comparision seems unfair to me from both the performance and the computation cost perspectives. More detailed benchmarking experiments with graph sampling methods will make the paper more solid.\n3. The proposed Katz PE seems to be an important component contributing to the final performance, but the motivation of the Katz PE is less relevant to the main motivation of this paper. Therefore, it makes the solidness of the main contribution of this work (the graph samping mechnism) weak.",
            "clarity,_quality,_novelty_and_reproducibility": "quality, clarity and originality are good.",
            "summary_of_the_review": "See Weaknesses. I'm willing to raise my score if my concerns are well addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_HE3Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_HE3Q"
        ]
    },
    {
        "id": "lO6cCR50VwB",
        "original": null,
        "number": 4,
        "cdate": 1666792052639,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666792052639,
        "tmdate": 1666792301532,
        "tddate": null,
        "forum": "DL8dTTvCpU",
        "replyto": "DL8dTTvCpU",
        "invitation": "ICLR.cc/2023/Conference/Paper2312/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes deformable graph transformer (DGT) to efficiently perform attention on graphs. The deformable graph transformer mainly consists of two components: deformable graph attention and Katz positional encoding. Experiments on eight datasets show better performance.",
            "strength_and_weaknesses": "Strengths \n1. The research problem is a fundamental and crucial one.\n2. The experimental results show superior accuracies and efficiency (FLOPs).\n\nWeaknesses\n1. The novelty and technical contribution are limited.\n2. It is unclear for the deformable graph attention module.\n3. It is unclear why the proposed method has lower computational complexity.\n\nDetailed comments:\n1. What is the motivation to choose personalized pagerank score, bfs, and feature similarity as sorting criteria?\n2. For NodeSort, 1) how to choose the base node, or is every node a base node? \n2)\u201cNodeSort differentially sorts nodes depending on the base node.\u201d Does this mean that the base node affects the ordering,  affects the key nodes for attention, and further affects the model performance?\n3)After getting the sorted node sequence, how to sample the key nodes for each node? And how many key nodes are sampled\uff1fis the number of key nodes a hyper-parameter? 4)What are the Value nodes used in Transformer in this paper? 5)How to fuse node representations generated by attention for different ranking criteria.\n3. Intuitively, the design of deformable graph attention is complicated, and the Katz positional encoding involves the exponentiation of adjacency matrix, so Is the computational complexity really reduced? Where can the reduction in complexity be explained from the proposed method compared to baselines? or just from the sparse implementation? ",
            "clarity,_quality,_novelty_and_reproducibility": "For clarity, some details are missing about deformable graph attention module.\nFor quality, the writing is easy to follow.\nFor novelty, the novelty and technical contribution are limited.\nFor reproducibility, the code is attached.\n\n",
            "summary_of_the_review": "The deformable graph attention module should be presented in a clear way. The reduction of complexity should be explained further.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_PKKU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_PKKU"
        ]
    },
    {
        "id": "PkVBrHdLdf",
        "original": null,
        "number": 5,
        "cdate": 1667222897239,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667222897239,
        "tmdate": 1668793221191,
        "tddate": null,
        "forum": "DL8dTTvCpU",
        "replyto": "DL8dTTvCpU",
        "invitation": "ICLR.cc/2023/Conference/Paper2312/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new transformer-like architecture for processing graphs. The main component is a special sparse attention module that considers only the neighbor nodes based on certain metrics. These metrics can be either structure or feature-based. For sampling from the neighborhood, it uses learned offsets. The authors also propose a novel position encoding method for the nodes in the graph.\n",
            "strength_and_weaknesses": "Strengths:\n - better performance on various datasets\n - the topology-based sorting ideas are interesting and novel to the best of my knowledge\n\n Weaknesses:\n  - some parts (for example, the preprocessing) are not specified in the paper\n  - the paper is difficult to read\n  - some design choices need more justification\n  - very complex\n  - claiming that this method is the best among the transformer-based ones is somewhat unfair. The main difference between the transformer-based methods and the GAT-style GNNs is the unrestricted attention, but here the preprocessing step introduces very similar restrictions. \n\nThe main weakness of the method:\n- From Table 3, picking nodes at random is not much worse than choosing the nodes in the smart way proposed by the paper. This is surprising and questions what deformable attention does and its importance.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: I found the paper extremely hard to read. There are many ambiguities, the notation is unnecessarily complex, and many parts need to be specified more clearly.\n- Novelty: The idea of ordering and subsampling the nodes is novel and interesting.\n- Reproducibility: Because the preprocessing is unspecified, I would probably not be able to reproduce the paper.\n- Quality: The quality of the paper is not enough to recommend acceptance.\n\n",
            "summary_of_the_review": "\nTo recommend acceptance, I would like to see the following:\n- Clarity has to be improved\n- Specification of the preprocessing step in a clear way, including in the complexity calculation\n- I would like to see an analysis of what the KatzPE does\n- Some analysis on why the DGA's performance is very close to random\n\n\nQuestions to clarify:\n- How do the sorting criteria work? What are they exactly? What is their input? Do they have learnable parameters?\n- The paper claims that it is more efficient than the transformer-based methods. However, it does not consider the preprocessing step, which can be very slow (although it has to be done only once). Also,  \"feature similarity\", as far as the name suggests, is just ordinary attention, so it should be the same speed.\n- \"Although restricting the attention scope to local neighbors is a simple remedy to reduce the computational complexity, it leads to a failure in capturing local-range dependency, which is crucial for large-scale or heterophilic graphs.\" -> do you have any reference for this? According to Table 3, even using random nodes is almost as good as the best model. Isn't BFS doing just that?\n- You mention that \"DGT-light\" uses a single sorting criterion. Which one?\n- All of Eq. (4) is position based, but the positions are defined by the most relevant features (by S). The attention matrix only depends on the query node's feature and not the key's (it is just a fixed function for a given key index, k). This can act as a regularizer because the network can hardly choose individual, well-defined nodes to attend to. This can by itself improve performance.\n- What is p? It can be inferred that it's probably an offset vector, but it would be nice if that is written in the paper.\n- It would be nice to visualize p for some learned tasks. It would be informative about how far the model attends.\n- What value of \\gamma is used in eq (5)?\n- \"A major issue with positional encoding on graphs is the absence of absolute positions of nodes, unlike other domains.\" - what does it mean to have absolute positional encodings in a graph? What defines the ordering of the nodes?\n- I don't see why KatzPE makes sense. It would be nice if the authors could add some more explanation. For me, it seems more like some kind of structure descriptor added to the nodes than it has anything to do with positions.\n- A_k in Eq. (6) is of a fixed size. Does this mean that, unlike the transformer-based methods, KatzPE depends on the graph size? (for big graphs, this limitation is not present since there are only N' anchor nodes)\n- It would be worth writing down that the so-called sampling is not actually an active sampling operation but just taking the features from the learned offsets. It can be confusing on the first read.\n- Could you provide more details on how O(NC^2T + NKC^2T + WNKCT) becomes O(NC^2T + WNKCT) in Appendix A? Why does swapping the interpolation help with this?\n- In 4.4, Figure 2, a should be Figure 2a\n- In Appendix C.2, the list of parameters should be a table\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_Xtmm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2312/Reviewer_Xtmm"
        ]
    }
]