[
    {
        "id": "XsIWv9RbKoj",
        "original": null,
        "number": 1,
        "cdate": 1666579266209,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579266209,
        "tmdate": 1669061538136,
        "tddate": null,
        "forum": "oCx90Ezdox_",
        "replyto": "oCx90Ezdox_",
        "invitation": "ICLR.cc/2023/Conference/Paper1722/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the author extended using domain knowledge to improve adversarial attacks from supervised learning to reinforcement learning and makes learned main policy universally resistant to adversarial attacks without prior knowledge of the attack. \n\nThe main contributions consist of 1. It generalizes previous work, which encodes domain knowledge as logical formula and a set of features and adds to the objective to apply constraints on the learned policies close to logical formulae from prediction tasks to reinforcement learning approaches. 2. The new extended approach is first used to tackle adversarial attack problems in reinforcement learning (RL), which is very important to keep RL performing appropriately under noisy input signals. 3. It is the first paper investigating using domain knowledge to defend against adversarial attacks in the form of policies. \n",
            "strength_and_weaknesses": "Strength: \n\n- The author is trying to introduce methods used in prediction tasks into reinforcement learning and solving the problem of the adversarial attack, which is a very important limitation hindering RL implementation in the real world. The idea is pretty concise and focused on an interesting task. \n\n- Using the past policies or sub-tasks policies and domain knowledge to help improve the RL model's robustness can help reduce the implementation cost and explainability of RL models. Also, the proposed framework is very flexible and easily extended to different agents. \nThe experiments support the author's claims and are extensively investigated in different scenarios. And the proposed methods uniformly outperform other baseline models.\n\nWeakness:\n\n\n- Part of this paper\u2019s idea is extended from past work on prediction tasks. The basic idea is using assembly learning and combining the idea from past prediction tasks as constraints on the policies, which slightly weakens the novelty of this paper. \n\n- The baseline model seems not to involve adversarial detection approaches, and in Table 1, the performance of MLP Fusion and KPR even increased after applying Square adversarial attacks. It is strange but not discussed by the author.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: This work aims to solve the robustness of RL models and provides a flexible framework to encode domain knowledge into the RL training procedure. And experiments are extensive and strongly support the author\u2019s claims. In short, the quality of this paper is high. \n\nClarity: The paper is well written and pretty clear in demonstrating the ideas and how the framework details are implemented. The idea is easy to follow, but since the code is not provided, the evaluation is neutral. \n\nOriginality: This paper is extended based on past work in prediction tasks which weakens its novelty. But the extension is nontrivial, and the proposed framework is flexible and easy to apply to different RL agents. So the novelty is neutral. \n",
            "summary_of_the_review": "This paper focuses on a very interesting domain which is the robustness of RL models under adversarial attacks, and the extended framework from the past prediction tasks is decent and easy to apply to various RL agents. Through the experiment results, this proposed method clearly outperforms the other baseline models universally. Thus I consider it a high-quality paper. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1722/Reviewer_XTBx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1722/Reviewer_XTBx"
        ]
    },
    {
        "id": "L7FKo_a_JT",
        "original": null,
        "number": 2,
        "cdate": 1666641868649,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641868649,
        "tmdate": 1666641868649,
        "tddate": null,
        "forum": "oCx90Ezdox_",
        "replyto": "oCx90Ezdox_",
        "invitation": "ICLR.cc/2023/Conference/Paper1722/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the problem of adversarial attacks against deep RL policies. They propose a defense that uses an ensemble policy based on policies for various auxiliary tasks and logical relations between those auxiliary tasks and the main task. A graph neural network is used to combine action distributions from the various auxiliary policies over the graph induced by each logical relation. Then, policies are aggregated across all logical relations via a voting mechanism. In experiments in three Atari games and a simulated robotics environment, the aggregated policy seems to be more robust to various adversarial attacks.",
            "strength_and_weaknesses": "Overall, much of the paper is well-written and the experiments show promising results. However, I have a couple major concerns about the evaluation:\n\n * First, the evaluation seems to be underspecified and thus not reproducible. The authors say they use standard white-box adversarial attacks like FGSM and PGD to attack policies. However, these algorithms cannot actually directly solve the attack optimization problem given in (2). This is because (2) is a function of the return of the policy, and one cannot directly differentiate the return with respect to the adversarial perturbations. In general, optimizing equation (2) would require more RL-like methods to approximate the gradient rather than the autograd methods used for typical whitebox attacks in supervised learning. Since the authors do not specify how they resolve this contradiction, I'm don't think the evaluation is reproducible. There are not further details in the appendix and there is no code, either.\n * Second, I'm also worried about gradient masking [1] in the evaluation. In particular, the aggregation mechanism given in equation (9) is not actually differentiable with respect to the underlying policies! This means that there are no gradients that an attacker can use to optimize the state towards lower reward. I'm not sure how the attacks are expected to work given this, and this makes me distrust the evaluation. While blackbox attacks should avoid this problem, looking at the results it appears that KPR does not show as large an improvement over the main task policy against black-box attacks, indicating that gradient masking may be occurring.\n * Third, it does not seem like the authors ran the entire training procedure using multiple different random seeds for the evaluation. This is a standard practice in RL evaluation and is needed to reduce the vast variance in RL outcomes across random seeds. \n\nGiven these three concerns, I think the evaluation needs to be improved before the proposed method can be reliably judged to perform better than the baselines. I encourage the authors to precisely report how the white-box attacks optimize equation (2), avoid gradient masking, and report results across multiple training runs with different random seeds.\n\nBesides the evaluation concerns, there are some other downsides to the method, like the need for auxiliary tasks and logical relations between them. \n\n[1] Athalye et al. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. ICML 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the writing is generally good, although it took a couple of times reading through Section 3 to fully understand all the pieces. One suggestion that might help improve clarity is to refer to the parts of Figure 1 throughout Section 3 as each piece of the method is introduced such that readers can follow along visually.\n\nAs far as originality, it seems like the method is fairly novel. I'm not so familiar with related adversarial defenses based on domain knowledge, so it's hard for me to judge how similar or different this paper's approach is.\n\nI am most concerned about reproducibility. Besides not providing code, there are also a number of important pieces of information missing from the paper that would be necessary to reproduce the results:\n * The details of how the white-box attacks are applied to the RL domain (see first bullet above in strengths and weaknesses)\n * Parameters of attacks like step size, number of iterations, norm bound, etc.\n * Details of neural network architectures\n * Hyperparameters for RL",
            "summary_of_the_review": "Overall, I think this paper explores a promising direction but needs a fair amount of work to improve the evaluation and reproducibility. Therefore, I don't think it is ready for publication at ICLR. However, if the authors can provide a more convincing evaluation along with all the details and code to reproduce the experiments, I am happy to raise my score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1722/Reviewer_EmJQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1722/Reviewer_EmJQ"
        ]
    },
    {
        "id": "0sze09ljXjy",
        "original": null,
        "number": 3,
        "cdate": 1666671728286,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671728286,
        "tmdate": 1666671728286,
        "tddate": null,
        "forum": "oCx90Ezdox_",
        "replyto": "oCx90Ezdox_",
        "invitation": "ICLR.cc/2023/Conference/Paper1722/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The manuscript proposes an approach for combining domain knowledge with RL; the methodology encodes propositional logical rules via Graph Neural Networks, with multiple pooling and attention mechanisms.",
            "strength_and_weaknesses": "(Strengths)\n\nI like the notion of combining domain knowledge with learning-based methodology.\n\nSections 1-3 are pretty well-written.\n\n(Weaknesses)\n\nSection 3.1: What are these success thresholds and how are they obtained, for an arbitrary collection of tasks and attacks? This seems non-trivial to define and combinatorial in complexity.\n\nSection 3.2: Architecturally, the proposed approach is very straightforward, and I was hard-pressed to find much novelty there.\n\nSection 4: It would be helpful if the manuscript provided the full names, brief descriptions, and some intuition behind why specific attacks were chosen.\n\nSection 4.1: In many practical applications, the subtasks/objectives can be conflicting \u2014 how are these situations reconciled?\n\nSection 4.1, Section 4.2: I am not really convinced by the experiments that the manuscript chose to perform. I would have liked to see empirical results on more standardised or challenging evaluation benchmarks in RL \u2014 especially those that include aspects of safety or robustness, such as OpenAI Safety Gym, Learn-to-Race, Carla Challenge, etc. Discussion of adversarial attacks in these tasks, where subtask objectives are often conflicting (!), would be much more compelling and intuitive.\n\nSection 4.1: It would be helpful to show the cumulative episodic reward curves (with error areas), by repeatedly evaluating on the test environment after a fixed number of training iterations or environment interactions: this would also give intuition on the training dynamics and sample-complexity.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: sections 1-3 of the paper are well-written; manuscript clarity/quality somewhat declines afterwards. See above on issues with experimental design.\n\nThe novelty of the manuscript is limited to its problem formulation, but even this relies on strong assumptions; I found neither the problem definition nor the proposed methodology particularly compelling.\n\nNo major issues with reproducibility, although I would recommend capturing the main hyperparameters in the main content of the manuscript.",
            "summary_of_the_review": "The clarity of the manuscript declines in its experimental design and adversarial attacks illustration.\n\nThe problem formulation and methodology rely on strong assumptions.\n\nMissing discussion of limitations, ethics, and reproducibility statements.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1722/Reviewer_dRJe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1722/Reviewer_dRJe"
        ]
    },
    {
        "id": "v2Fdic-0QKB",
        "original": null,
        "number": 4,
        "cdate": 1666683490870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683490870,
        "tmdate": 1666683490870,
        "tddate": null,
        "forum": "oCx90Ezdox_",
        "replyto": "oCx90Ezdox_",
        "invitation": "ICLR.cc/2023/Conference/Paper1722/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new framework called Knowledge-based Policy Fusion (KPR), which leverages domain knowledge to defend against adversarial attacks in RL. KPR incorporates domain knowledge from auxiliary policies and specified logical relations between tasks, then learns flexible relations from interaction data via graph neural networks. Different from prior defense methods in reinforcement learning, such as adversarial training and robust learning, the main advantage of KPR is that it is both policy and attack agnostic; any type of policy could be utilized, and no access nor information about the attack is required. They demonstrated its efficacy empirically in both Atari games and the complex Robot Food Court environment (RoFoCo). According to the authors, this is the first work to demonstrate that domain knowledge in the form of policies can be used to defend against adversarial attacks.",
            "strength_and_weaknesses": "a)\tStrength:\nThe authors describe in detail the differences between KPR and previous methods:\npolicy ensemble, policy distillation, and adversarial training, and designed a very sufficient experiment to verify the advantages of KPR. To investigate the role domain knowledge plays, authors included a variant of KPR, MLP Fusion, which replaces the GNN with an MLP whose input is the action distributions of the auxiliary policies. This ablation study is convincing and shows the effect of domain knowledge. In the experimental part, the choice of experimental environment is very interesting, the visual effect is great, and the experimental data is relatively sufficient, showing the advantages of KPR in the form of a table\nb)\tWeaknesses:\nThe author makes a strong assumption that the susceptibility of a policy to an attacker is due principally to overfitting on a specific task. If only one specific type of attack is considered, it is not enough to demonstrate the generalization of defense strategies and the use of GNN for a well-structured use of domain knowledge. In terms of theory, there seems to be a lack of more analysis. The author needs to explain why this is necessary. The theoretical analysis in methods such as fuzzy control can have a fuzzy mathematical system to describe how to convert from natural language to numerical output.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The author has a very clear display of the structure of KPR, and the description of the experimental settings and details is also relatively clear.\nQuality: The charts are well made, and the visual effects of the experimental environment constructed are also very good.\nNovelty: KPR is different from previous methods, this is the first work to demonstrate that domain knowledge in the form of policies can be used to defend against adversarial attacks.\nReproducibility: The author did not provide the source code, so I cannot confirm it\n",
            "summary_of_the_review": "This paper proposed a new framework called KPR, which leverages domain knowledge to defend against adversarial attacks in RL. The charts are clear and easy to understand, the descriptions are also very concise and clear, and the experiments are sufficient, but the scenario assumptions are single, and some theoretical explanations are lacking",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1722/Reviewer_Vu3r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1722/Reviewer_Vu3r"
        ]
    }
]