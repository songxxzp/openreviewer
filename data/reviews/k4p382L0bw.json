[
    {
        "id": "4zBX7PWQdg",
        "original": null,
        "number": 1,
        "cdate": 1666444090311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666444090311,
        "tmdate": 1666444090311,
        "tddate": null,
        "forum": "k4p382L0bw",
        "replyto": "k4p382L0bw",
        "invitation": "ICLR.cc/2023/Conference/Paper586/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes two practical regularization techniques for MAE training and the related quantitative results verify their efficacy. One is to exert different weights to the reconstruction loss of different patches based on how difficult it is to reconstruct these masked ones with visible ones. Another is to employ a momentum encoder to regularize the learned intermediate features. Experiments show these two designs are effective and complementary. They can accelerate training and further improve baseline performance, evaluated on two tasks, e.g., classification and segmentation.",
            "strength_and_weaknesses": "Strengths:\n1) It gives a strong motivation for the proposed two designs. Applying spatially variant reconstruction losses to different masked patches based on their distances from the existing ones is a sound technique. The momentum encoder is also a typical method to improve representation learning.\n2) It conducts extensive and solid experiments. Classification and semantic segmentation are conducted along with several compared methods. Ablations are also sufficient.\n\nWeaknesses:\n1) Though dynamic loss is complementary to deep supervision (in Table 3), the sole improvement brought by dynamic loss is limited (only 0.2 compared to MAE) and the major benefits are provided by deep supervision. This experimental result weakens the key role of dynamic loss while the loss is stressed with the most pages. Undoubtedly, dynamic loss can accelerate training convergence, but it is expected to give better improvement if it is the most crucial part of this paper. Besides, such a dynamic loss design is a routine in inpainting literatures [1-4].\n> [1] Pathak, Deepak, et al. Context encoders: Feature learning by inpainting. CVPR. 2016.\n>\n> [2] Yeh, Raymond, et al. Semantic image inpainting with perceptual and contextual losses. arXiv preprint arXiv:1607.07539 2.3. 2016.\n>\n> [3] Yu, Jiahui, et al. Generative image inpainting with contextual attention. CVPR. 2018.\n>\n> [4] Wang, Yi, et al. Image inpainting via generative multi-column convolutional neural networks. NeurIPS. 2018.\n\n2) The analysis of the used momentum encoder is unsatisfying from my perspective. If we treat the masked input as a view of the input, then MAE can be considered as maximizing the similarity between the positive pair in contrastive learning. To this end, the usage of momentum encoder can be discussed in depth.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and easy to follow. I suppose the analysis of deep supervision could be improved for a better understanding about the role of the momentum encoder.\n\nAbout the novelty, as described in Strength And Weaknesses, the dynamic loss and momentum encoder are intensively studied in the inpainting and contrastive learning areas respectively. Their introduction and combination to MAE lead to interesting performance improvement with simple designs, which is inspiring.\n\nThe method description is clear and concise, and the related techniques are widely implemented in different topics. Thus, I believe this work is easily reproducible.",
            "summary_of_the_review": "This paper gives two simple designs, a dynamic loss and deep supervision, to improve the current MAE training and downstream performance. These two designs are widely researched in inpainting and contrastive learning topics, and they perform quite well on MAE, especially on classification and semantic segmentation. The motivations are well-described and well-supported by the experiments. I believe it gives good practices for the development of MAE.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper586/Reviewer_A9rs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper586/Reviewer_A9rs"
        ]
    },
    {
        "id": "LL0C4K-5id",
        "original": null,
        "number": 2,
        "cdate": 1666847570224,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666847570224,
        "tmdate": 1670643557403,
        "tddate": null,
        "forum": "k4p382L0bw",
        "replyto": "k4p382L0bw",
        "invitation": "ICLR.cc/2023/Conference/Paper586/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper designs a deep dynamic supervision mechanism that can be migrated into existing MIM methods.\nIt proposes to dynamically focus on patch reconstructions with different degrees of difficulty at different pretraining phases and depths of the model. \nFurther experiments demonstrate the effectiveness of proposed method.",
            "strength_and_weaknesses": "Strength\n+ The performance is good\n+ Detailed experiments demonstrate the effectiveness of each component \n\nWeakness\n+ The idea seems to be incremental. Add deep supervision is straight-forward and should be expected to work.\n+ It lacks motivation on using both of distillation decoder and regression decoder. It seems minor improvement when combing them. What is the experiment variance? \n+ Using extra tokenizer is not necessary a bad thing. Will proposed method be further improved when use extra tokenizer?\n+ What is the improvement on down-stream fine-tuning when compared to MAE?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read. The experiments are detailed and well conducted.",
            "summary_of_the_review": "Overall, this paper is valid. However, the idea seems to be incremental. This prevents me from further increasing the score. \n\nAfter reading the rebuttal, I still feel that this paper lacks enough novelty and the main components proposed yeild minor improvements.  \n\nHence, I downgrade my rating and would not recommend to accept. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper586/Reviewer_VeHD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper586/Reviewer_VeHD"
        ]
    },
    {
        "id": "tizwWVs80Sq",
        "original": null,
        "number": 3,
        "cdate": 1666941758427,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666941758427,
        "tmdate": 1668490465579,
        "tddate": null,
        "forum": "k4p382L0bw",
        "replyto": "k4p382L0bw",
        "invitation": "ICLR.cc/2023/Conference/Paper586/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This manuscript proposes to introduce deep supervision into masked image modelling. Technically, there are two loss functions: the first one is to reconstruct pixel densities using tokens produced by intermediate transformer encoders; the second one seems to be enforcing the transformer encoder outputs to mimick those of corresponding outputs of an EMA version of the encoder (although I am not very sure about the details due to clarity issues). The primary selling point is to use a transformed (by equation 2) distance map that weights both losses so that patches with different distances to visible ones get different weights. This technique is considered 'dynamic' by the authors, in the tile, as the concentration parameter beta is learned in a layer-wise manner. Evaluations are done with downstreaming ImageNet fine-tuning and linear classification and an ADE20k fine-tuning experiments, reporting marginally improved performance. Codes are not provided or promised.\n\n[Post-rebuttal]\nThe method is conceptually novel and has some value. \nThe significance is left for experts on large-scale pre-training to judge.\nThe major issue is unclear presentation. Although I can guess how to implement it from these vague descriptions, people who are not familiar with topics involved may have some difficulty in understanding this. This is sub-bar for an ICLR paper, in terms of clarity.\nAfter several rounds of communication, the authors correct some factual errors (1-M v.s. M), and typos. They also tune-down some unjustified interpretations that may be misleading for the community. Remaining issues include:\n\n(1) The behavior of the regularization term in Equation.6 lacks clarity. The authors give an intuitive example in the response, which is not included into the paper. If included, this raises the clarity of this part above the bar. But I still recommend an analysis, as mentioned before: 'The impact of this regularization needs more (theoretical and experimental) analysis' in the first round and 'How gradients propagate to beta, through h() is not clear to me and this needs an analysis' in the second round. This is not a difficult thing, which only needs some gradient analysis and I don't understand why authors insist it is not necessary.\n(2) The training loop. After some clarification, I know that the implementation is aligned with my guessing. But the paper still lacks clarity. Including a training loop (like Algorithm 1 in the MoCo paper) is not difficult and I dont' understand why authors insist it is not necessary.\n\nAlthough it is disappointing that the authors still ignore my sincere suggestions on improving the clarity for a larger audience that are not familiar with these techniques, I have raised the recommendation to BA.",
            "strength_and_weaknesses": "Strengths:\n+ I am familiar with and like deep supervision and distance transforms. They are techniques specific to computer vision due to their links to hierarchical representations and spatial affinity. This idea (if better developed) is something that I would like to cite and follow. Introducing deep supervision into masked image modeling is novel to my knowledge. Using distance transforms to weight training is a novel add-on too and makes sense in this masked image modelling setting.\n+ Although the method is unclear in several places, I can somewhat figure out how to implement it as the modules are standard ones.\n+ Although lacking in significance, the experimental results are conclusively better than baseline.\n+ The attempt to reveal the underlying mechanism is appreciated (fig.3 and 4), although I am not fully convinced.\n\nWeaknesses:\n- I think the major issue is clarity:\n- The notations make me confused. In equation 1, we use x_{1-M} to reconstruct x_M. But in equation 2 and 3, it seems that the paradigm changes? For equation 2, I believe we should reconstruct masked-out patches x_M right? Correct me if I am wrong. For equation 3, I think it is possible to distill the features of masked-out patches or remaining patches. But since the equation does not contain decoder I guess the input should be remaining patches x_{1-M} ? Having that said, if the distillation is done with only the encoder, how is the 'distillation decoder' in Fig.1 used?\n- Reading section 3.3 makes me lost. There is no formal description of the EMA procedure or the training loop. And what's worse, the figure, the loss and the notation seem to contradict each other. The sentence 'Deep dynamic supervision is shared with raw pixel regression' brings much more confusion. What is 'shared with'? According to Fig.1 I guess the deep dynamic supervision term refers to the whole training objective?\n- I think the beta regularization is the key to the success. According to my own experience, training this kind of parameters is not easy. The impact of this regularization needs more (theoretical and experimental) analysis. Since beta can be either negative or positive, regularizing its L1-norm is not valid. So I can see the authors use h for regularization, but the behavior is not readily clear to me.\n\n- Although I think the method is novel from several points of view, I have to point out several missing references. In the 'Deep Supervision' related works section, the authors claim that making deep supervision work for modern architecture is not easy. I agree and I would like to point out that papers like [A][B], which are the referred 'Contrastive Deep Supervision' paper's baselines, have studied the problem. \n\n[A] Deeply-supervised knowledge synergy, CVPR 2019\n[B] Be your own teacher: Improve the performance of convolutional neural networks via self distillation, ICCV 2019\n\n- My last major concern is about significance:\n- Personally, I only care about whether these pre-trained models can help my group develop better down-streaming robotic scene understanding algorithms. I feel excited about CLIP, Swin or MDETR, but the performance improvement of this one seems too marginal. I don't think this is a strong enough pre-training paper for ICLR. Maybe other colleagues with hands-on expertise on large-scale pre-training can better assess the significance.\n\nI have other comments:\n- I doubt he central claim of 'locality inductive bias in deeper layers'. I am not convinced why we we should have local bias in late stages. Meanwhile, I cannot understand why the deep dynamic supervision design allows this to happen.\n- 'gradually reconstructed vicinity' I am not convinced. There is no technical modules that allow gradual reconstruction.\n- 'this paper shows another potential use of deep supervision to make the network more diverse' This is too vague and I am not convinced by this 'diverse' claim.\n- The online calculation of distance transforms of random masks brings computation overhead, which should be analyzed.\n- The better convergence claim should be supported by curves instead of two datapoints.\n\nSeveral capical issues:\n- The Experimental results\n- Dynamic Loss\n- The Reconstruction targets",
            "clarity,_quality,_novelty_and_reproducibility": "They have been discussed in the last box.",
            "summary_of_the_review": "The idea is fine, but the current version lacks in clarity. As for significance, I am not fascinated and others with hands-on experience in pre-training may have a better judgement.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper586/Reviewer_x4N6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper586/Reviewer_x4N6"
        ]
    },
    {
        "id": "jr9Fj4f7WLx",
        "original": null,
        "number": 4,
        "cdate": 1667453417786,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667453417786,
        "tmdate": 1667453417786,
        "tddate": null,
        "forum": "k4p382L0bw",
        "replyto": "k4p382L0bw",
        "invitation": "ICLR.cc/2023/Conference/Paper586/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new MIM framework that dynamically focuses on patch reconstructions based on the degree of difficulty (i.e., the nearby visible patches) during pre-training. Besides, this paper proposes to self-distill intermediate features from the momentum encoder. Experiments show that it outperforms previous self-supervised methods on ImageNet for image classification and on ADE20K for semantic segmentation.",
            "strength_and_weaknesses": "Pros:\n\n- Interesting exploration to improving MIM by providing more local priors.\n- State-of-the-art performance among tokenizer-free methods. \n\nCons:\n\n- The motivation of this paper is not convincing. The reconstruction difficulty of patches are not necessarily correlated with distance from visible patches. For example, it may depend on how much information is included in the patch. The background patches will still be easy to reconstruct following regular patterns even if it is far away from visible patches, but the foreground patches (e.g., nose of a dog) are hard to guess and require stronger semantic reasoning.\n\n- The empirical improvement is too marginal. This is no clear advantage of the proposed method compared to SdAE(Chen et al., 2022c) according to the experiments. It has the same accuracy as SdAE for 100 epochs and only outperforms SdAE by 0.3% for 800 epochs.\n\n- This paper is not well-written. The introduction is too redundant and messy. Figure 1 is hard to follow. The caption is not self-contained.",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above in Cons, this paper is not well-written and hard to follow. The basic idea of providing more local priors to MIM is interesting, but the design of the method is not satisfactory.",
            "summary_of_the_review": "Overall, I think this paper is a borderline paper and slightly below the acceptance threshold. I would like the authors to provide more justifications to their motivation to further judge this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper586/Reviewer_aYDP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper586/Reviewer_aYDP"
        ]
    },
    {
        "id": "PHr3FOLew7",
        "original": null,
        "number": 5,
        "cdate": 1667460686554,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667460686554,
        "tmdate": 1670869104381,
        "tddate": null,
        "forum": "k4p382L0bw",
        "replyto": "k4p382L0bw",
        "invitation": "ICLR.cc/2023/Conference/Paper586/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduce a simple mechanism to allow the training of MIM model tacking the reconstruction difficulty into account. It also introduce deep supervision that can help the diversify the feature representation across different layers. Good performance are reported to justfiy its methods. ",
            "strength_and_weaknesses": "Strength:\n- The intuition that reconstructing patches has different levels of difficulty makes a lot of sense. The proposed method is also simple, and the required modifications on top of the MAE framework are quite minimal.\u00a0\n- The experimental results are solid.\u00a0\n- The analysis of locality(eg. Fig 4) is interesting and inspiring.\u00a0\n\nWeakness:\n- The author claims that deep supervision introduces diversification and mentions the independence of beta_i. However, the Fig 5 shows that the beta_i are highly correlated and mostly the same. The difference in beta_i as shown in zoom-in figure in Fig 5 right plot seems too brittle to support the hypothesis that lower layer focus on more simple patches and deeper layers focus on more difficult patches. Also, why is the beta curve for block 12 missing from Fig 5?\n- More discussion of how the initial beta is chosen is needed.  The paper initialize beta to -0.5 and the during the whole training beta varies from -0.6 to -0.425, very close to the initial value. What will happen if you initialize beta differently? \n\n\u00a0\nOther comments\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper proposes a very simple modification to the MIM framework. The intuition behind the approach is clear, and the experimental results are adequate. This paper is well-written in general and it should be not difficult to reproduce the results.\n\n",
            "summary_of_the_review": "The presented approach is simple yet effective as shown in their benchmark results. The analysis is in general good. Thus I am leaning to accept. \n\nPost rebuttal:\nI decided to down-rate the score after the reviewer discussion. Although the core idea of weighting patch reconstruction is inspiring, the improvement from dynamic loss is comparably marginal compared to the deep supervision, which makes the current paper writing less proper as the majority discussion is focused on dynamic loss. Given the current reviewers opinions, I recommend the authors to rephrase the paper writing or provide stronger results to validate dynamic loss and submit to another venue. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper586/Reviewer_pPUj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper586/Reviewer_pPUj"
        ]
    }
]