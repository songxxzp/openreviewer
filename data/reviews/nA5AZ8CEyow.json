[
    {
        "id": "jz87Hap2yS6",
        "original": null,
        "number": 1,
        "cdate": 1665860270759,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665860270759,
        "tmdate": 1665860270759,
        "tddate": null,
        "forum": "nA5AZ8CEyow",
        "replyto": "nA5AZ8CEyow",
        "invitation": "ICLR.cc/2023/Conference/Paper3269/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for creating a concept bottleneck model from any given model. The authors claim that the proposed approach does not sacrifice the performance of the model, and retains the interpretability benefits of concept bottleneck models along with easy model editing.",
            "strength_and_weaknesses": "Strengths:\n\nIt is an extremely well-written paper on an important and timely problem, generating post-hoc concept-based explanations for trained neural networks. It clearly describes the limitations of existing methods and builds upon them in an elegant way, leading to a superior solution. The experiments are thorough and seem convincing, clearly demonstrating the usefulness and flexibility of the proposed method.\n\nWeaknesses:\n\nWhile I am generally supportive of the paper and its claims, I do find the lack of discussion around the causal validity of the generated concepts missing. There have been many studies in the literature highlighting the causal nature of post-hoc model explanations, which the authors do not address. Please see missing references at the next section (Clarity, Quality, Novelty And Reproducibility).\n\n\n\nI understand that this is not the goal of this paper, but addressing the fact that the generated concepts and their estimated effect might not faithfully reflect their true causal effect on the trained model sounds important to me and should be a part of this paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This is a high-quality paper that is clearly written. It proposes a novel approach that extends upon several well-known and widely used methods. The experimentation seems thorough and conclusive, and the methods presented seem flexible and useful in a wide array of problems.\n\nSome minor issues in the paper:\nTypo: \n\nPCBMs more accessible **in** various settings.\n\nMissing references:\n\n1. Explaining Classifiers with Causal Concept Effect (CaCE)\n2. CausaLM: Causal model explanation through counterfactual language models\n3. Amnesic probing: Behavioral explanation with amnesic counterfactuals\n",
            "summary_of_the_review": "It is an interesting paper on an important and timely problem, generating post-hoc concept-based explanations for trained neural networks. It proposes a novel approach that extends upon several well-known and widely used methods. The experimentation seems thorough and conclusive, and the methods presented seem flexible and useful in a wide array of problems. However, the lack of discussion around the causal validity of the generated concepts is missing.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3269/Reviewer_cGSM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3269/Reviewer_cGSM"
        ]
    },
    {
        "id": "RdaMevj0sc",
        "original": null,
        "number": 2,
        "cdate": 1666614166715,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614166715,
        "tmdate": 1668776358526,
        "tddate": null,
        "forum": "nA5AZ8CEyow",
        "replyto": "nA5AZ8CEyow",
        "invitation": "ICLR.cc/2023/Conference/Paper3269/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduce a new class of interpretable neural network classifiers, denoted Post-hoc Concept Based Models (PCBMs).  PCBMs assume a concept vocabulary is given, fit linear representations of the various concepts in the embedding space of the target model using auxiliary positive and negative examples of each concept, then acquire an interpretable classifier on top of these embeddings (or rather the projection of the input on them).  The key advantage of PCBMs over regular CBMs is that the target data set needs not be densely annotated.  The authors also introduce a partially interpretable version of PCBMs (PCBM-h's) that upgrade performance by also making use of possibly uninterpretable features.  Both classes of models are compared on several data sets in terms of accuracy (against a black-box baseline) and amount of supervision required.",
            "strength_and_weaknesses": "PROS:\n\n- PCBMs mix existing techniques into a novel recipe in a sensible manner.\n\n- PCBMs overcome one of the main limitations of CBMs, namely the need for dense annotations.\n\n- The manipulation experiment adds some flavour to the main message.\n\n- The text is very well written and easy to follow.\n\nCONS:\n\n- Empirical results do not entirely match the claims in the text.\n\n- Missing comparison with non-CBM baselines.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is very well written.  There are some grammatical issues here and there, see for instance the abstract.  These can be easily fixed.\n\nQuality: The proposed approach is quite sound.  I have some doubts that CAV vectors can fully capture the nuances of non-linear mappings, but given that they are widely accepted in the community, I do not have major issues with this modelling choice.  PCBM-h's are also sensible.  The authors clearly mention that these are affected by limited interpretability, and mention possible solutions (e.g., adapting ideas from the PIE paper).\n\nThe experiments themselves are quite extensive, however they neglect compakeyring PCBMs against one obvious baseline, namely Concept Whitening (which is however referenced in the paper), which is perhaps the technique that is closest in spirit to CBMs (they are actually quite similar, modulo an extra orthogonality constraint).  Adding this comparisong would strengthen the message substantially.  In my opinion, however, this is not a strict requirement.\n\nMy main issue with the text is the \"Post-hoc concept bottlenecks do not hurt the full model performance\" paragraph in p. 5.  Looking at Table 1 very clearly shows that performance *is* hurt, and only PCBM-h's manage to get close to the accuracy of the black-box model.  Here are the numbers (black-box -> PCBM) for the different data sets:\n\n CIFAR10: 0.888 -> 0.777 (*-11%*)\n\n CIFAR100: 0.701 -> 0.520 (*-18%*)\n\n COCO-Stuff: 0.770 -> 0.741 (3%)\n\n CUB: 0.612 -> 0.588 (2%)\n\n HAM10000: 0.963 -> 0.947 (2%)\n\n ISIC: 0.812 -> 0.736 (*8%*).\n\nIn three data sets out of six, there is a drop in performance > 5%.  It is debatable whether this is significant or not.  Now, I am perfectly fine with there being an (even noticeable) drop in performance, as long as this is reported correctly in the text.  To be clear, all I am explicitly asking  is that that the authors rephrase **the title of the paragraph** to better match what the results show.\n\nNovelty: PCBMs combine ideas from CAVs and concept-based models in a novel way.  I expect this model to be of interest to CBM and XAI specialists.\n\nReproducibility: The code is available and seems well structured.  Pointers to datasets are provided.",
            "summary_of_the_review": "Simple but useful new class of models that overcome major issue with prior work, with a couple of key issues that can be easily fixed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3269/Reviewer_uQCo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3269/Reviewer_uQCo"
        ]
    },
    {
        "id": "JGxD0YzJqG",
        "original": null,
        "number": 3,
        "cdate": 1666669924425,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669924425,
        "tmdate": 1666669924425,
        "tddate": null,
        "forum": "nA5AZ8CEyow",
        "replyto": "nA5AZ8CEyow",
        "invitation": "ICLR.cc/2023/Conference/Paper3269/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present Post-hoc Concept Bottleneck Models (PCBMs) as a method for converting any model into a CBM with its interpretability benefits while retaining the original model performance ( by introducing a residual layer that circumvents the bottleneck and helps with the concept set isn\u2019t rich enough ).   When there isn\u2019t concept supervision for a task, they use CLIP and ConceptNet as a method to learn concepts based on task classes.  They also show that PCBMs can be leveraged to perform global model interventions, specifically to reduce spurious correlations in a contrived dataset.  Through a model-editing user study, they show that editing PCBMs via concept-level feedback can provide significant performance gains without using any data from a target domain or model retraining for tasks in new data distributions.  \n",
            "strength_and_weaknesses": "**Strength:**  \nThe authors demonstrate the utility of their method for constructing CBMs in a post-hoc fashion using Concept net & CLIP if need be along with their model variant which uses a residual layer PCBM-h to get near same accuracy as the original model  ( at the expense of interpretability, but still a novel and simple addition to the architecture that is quite useful.\n\n**Weaknesses:**  \nThe paper had a few areas that could have been clearer (1) how to select negatives when given concept supervision and (2) it took me until after reading section 3 to understand when Visual or Multimodal concepts are used ( maybe that could be added to Figure 1?) You all mention it in your limitations section, but the use of Concept Net will not translate well to domain specific tasks or tasks with classes not in ConceptNet.\n\nFinally on Page 6 before Figure 2, you mention PCBM-h and PCBM provide the same concept interpretations, but this is not exactly the same interpretation since the final prediction in PCBM-h is no longer a linear combination of concept weights, but rather concept weights and residual weights so  although the concept bottleneck learns the  same weights, the overall model behavior is different and the concepts influence in each model differ.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clear and novel, and if the code is provided seems like it should be reproducible ( though i couldn\u2019t check repo as it was redacted and not included as supplemental )",
            "summary_of_the_review": "The authors demonstrate the utility of their method for constructing CBMs in a post-hoc fashion using Concept net & CLIP if need be along with their model variant which uses a residual layer PCBM-h to get near same accuracy as the original model  ( at the expense of intepretability, but still a novel and simple addition to the architecture that is quite useful.\n\nThe paper mostly clearly written, though it took me a bit to understand certain parts such as when ConceptNet and Clip were used ( ie, when labeled concept examples don\u2019t exist ).  The experiments well done and model editing was explained\nwell for the use case of datashift ( which is relatively simplified ), but still has utility.  The human user study was well done and very convincing.  It\u2019d be interesting to explain the differences between the fine-tune oracle and user pruned concept weights in that section to see if fine tuning is actually picking up spurious correlations which are useful for task accuracy or as a way to suggest concepts for a human to consider?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3269/Reviewer_n4tz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3269/Reviewer_n4tz"
        ]
    }
]