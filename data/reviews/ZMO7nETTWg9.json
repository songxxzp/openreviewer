[
    {
        "id": "P0QBbmXPFy7",
        "original": null,
        "number": 1,
        "cdate": 1666490633900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666490633900,
        "tmdate": 1666490633900,
        "tddate": null,
        "forum": "ZMO7nETTWg9",
        "replyto": "ZMO7nETTWg9",
        "invitation": "ICLR.cc/2023/Conference/Paper5740/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studied the convergence of gradient descent algorithm in training the DeepONet in the overparameterized regime. Specifically,  the authors proved a spectral norm bound for the Hessian of DeepONets with smooth activation functions; this bound implies convergence of GD using the RSC property. For ReLU activated DeepONets the authors showed that the corresponding neural tangent kernel at initialization is positive definite which again implies convergence of GD. ",
            "strength_and_weaknesses": "Strengths: The analytical results on the training of operator networks seem to be new and extend the prior analysis for learning finite dimensional functions. The paper is easy to read and follow.\n\nWeaknesses: I personally feel that this paper is merely a trivial extension of the early results of Du et al 2019 and Liu et al 2021 for finite dimensional functions to operator networks. The proof ideas and techniques based on the analysis of kernels and PL-condition are identical to those shown in the early literature. I do not see the essential difficulties in the extension except the technicalities arising from the new network architecture.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read and the main results are clearly stated. However, the results are incremental and not novel enough to be published in ICLR. ",
            "summary_of_the_review": "I am inclined to reject the paper due to the lack of sufficient novelty and significance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_4Wz6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_4Wz6"
        ]
    },
    {
        "id": "W3boKIYuGPU",
        "original": null,
        "number": 2,
        "cdate": 1666593344455,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666593344455,
        "tmdate": 1666593344455,
        "tddate": null,
        "forum": "ZMO7nETTWg9",
        "replyto": "ZMO7nETTWg9",
        "invitation": "ICLR.cc/2023/Conference/Paper5740/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper establishes theoretical guarantees about overparameterized DeepONets. In particular, they show that",
            "strength_and_weaknesses": "Strengths\n-----------\n* The paper's results are presented reasonably cleanly. This is especially nice given the heavy analytic nature of the work and greatly enhances the communication of the results.\n* The paper's results all seem to be correct.\n* The experimental results show that increase network width does improve the results.\n\nWeaknesses\n--------------\n* The paper does not mention any of the more recent operator networks (e.g. fourier neural operators) which tend to show improved performance over DeepONets. This tends to make the analysis feel out of date.\n* The overall results feel rather weak. In particular, it's not that surprising nor insightful that overparameterization would help DeepONets, especially with the wealth of literature on overparameterization.\n* The proof techniques used feel like just reapplication of standard overparameterization tools. In particular, it treats the DeepONet as just a different architectural design (akin to MLPs or Resnets). However, the DeepONet primarily acts on functionals, which should necessitate something different. Is there any operator perspective to the analysis?\n* The experimental results are really not necessary, don't add much, and don't really feel like a worthwhile inclusion. This is true even in comparison with general theory papers (which tend to have a weaker experimental setup), and so I would move them to the appendix and expand upon the theoretical contributions.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n-------\nAs mentioned above, the paper is quite clear despite the analytic nature of the work. This makes it quite easy to read.\n\nQuality\n--------\nOverall, the paper is reasonably well executed. Nothing seems to be wrong in the theoretical justification.\n\nNovelty\n---------\nThe paper does not feel very novel, as the techniques seem to be heavily borrowed from previous analyses and just applied out-of-the-box to DeepONets.\n\nReproducibility\n-----------------\nNo code/architectural design is provided. This technically makes it hard to reproduce the experiments, but this shouldn't really matter as they are toy/sanity checking purposes only.",
            "summary_of_the_review": "Overall, I lean reject. In particular, my concerns are primarily that 1) DeepONets are old and better, more novel architectures have been introduced, 2) the insights given by the paper seem rather weak/intuitive, 3) the proof structure feels like a reapplication of standard techniques from the field to a different architecture.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_BsC5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_BsC5"
        ]
    },
    {
        "id": "7LyIpQnj2M",
        "original": null,
        "number": 3,
        "cdate": 1666714431290,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714431290,
        "tmdate": 1666714431290,
        "tddate": null,
        "forum": "ZMO7nETTWg9",
        "replyto": "ZMO7nETTWg9",
        "invitation": "ICLR.cc/2023/Conference/Paper5740/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the optimization convergence guarantee of GD for training DeepONets with wide layers. Technically, the authors prove the convergence for both smooth activations and ReLU activation and show that a linear convergence rate can be achieved. The authors further present empirical results to back up the theoretical findings.",
            "strength_and_weaknesses": "Strength:\n1. This paper proves the convergence for DeepONets with smooth activations based on the RSC condition.\n2. This paper proves the convergence for DeepONets with ReLU activation based on the NTK analysis.\n3. This paper conducts numerical experiments to justify the theory.\n\nweakness:\n1. The initialization of weights in Assumption 3 seems unreasonable to me. In fact, the authors consider using a small scaling for only the last layer, then the optimization on the last layer weights will dominate the training of the remaining weights. By (5), the neural network model will be very close to the linear model. \n2. It is also not clear to me why the authors need to use two sections to study the convergence of DeepONets with two different types of activations. My understanding is that they can be both performed via standard NTK analysis (in the lazy training regime, there is nearly no difference between NTK and RSC based analysis).\n3. In the ReLU case, it would be better to give a quantitative bound on $\\lambda_{0,g}$  or $c$ in Theorem 5.1.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is a bit incremental given prior works on the NTK based convergence analysis.\n",
            "summary_of_the_review": "Overall, I am not satisfied with the initialization in Assumption 3, the authors may need to make the training of all layers be in the same level rather than using different initialization scaling for different layers. Besides, the contributions of this paper are not strong, especially given a large number of prior works studying the convergence of GD in the NTK regime.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_gCk5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_gCk5"
        ]
    },
    {
        "id": "EKnXe3LmpTa",
        "original": null,
        "number": 4,
        "cdate": 1667257258680,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667257258680,
        "tmdate": 1667257575396,
        "tddate": null,
        "forum": "ZMO7nETTWg9",
        "replyto": "ZMO7nETTWg9",
        "invitation": "ICLR.cc/2023/Conference/Paper5740/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides convergence analyses of gradient descent for learning deep, over-parameterized operator networks. The authors show the guarantees two types of networks: (1) with smooth activations and (2) with ReLU activation using two different analysis techniques.\n",
            "strength_and_weaknesses": "Regarding the strengths, this paper\n+ establishes first guarantees of GD for deep operator;\n+ shows the non-trivial convergence analyses, albeit using standard analysis techniques (restricted convexity and NTK).\n\nHowever, there are some drawbacks: \n- The authors do not explicitly state convergence results in dedicated theorems, so it is difficulty to understand the full picture (e.g., convergence rate, role of learning rate\u2026) In particular, while I understand how the analysis goes, I cannot see how the width (or the over-parameterization) attributes to the overall convergence.\n- The paper presents two different analyses due to the smoothness without discussing limitations or why one can't apply one to the other.\n- The experiments are based on Adam, an adaptive SGD, instead of GD.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some questions and comments about clarity:\n\n1. Why do the learning curves (in Figure 1 and similarly 2 and 3) fluctuate more with larger width?\n\n2. How does m affect the local convexity in the smooth activation case?\n\n3. Any intuition why the NTK is already positive definite at initialization? Does that hold true with u being an identity function? \n\n4. After Definition 2, L is \u03b1-RSC w.r.t. (Q, \u03b8) not (S, \u03b8)? And, in Theorem 4.2, the sum should be gradient of \\bar(G)_t not \\bar(G)_t itself?\n",
            "summary_of_the_review": "The analyses presented in this paper are non-trivial, but I think it needs some more work to be ready for acceptance (e.g., more explicit convergence results, discussions about limitations of each of the analysis techniques....)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_QX4p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_QX4p"
        ]
    },
    {
        "id": "VQphlUruIxo",
        "original": null,
        "number": 5,
        "cdate": 1667277077762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667277077762,
        "tmdate": 1667277077762,
        "tddate": null,
        "forum": "ZMO7nETTWg9",
        "replyto": "ZMO7nETTWg9",
        "invitation": "ICLR.cc/2023/Conference/Paper5740/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper analyzes Deep operator Networks (DeepONets) regarding the optimization convergence guarantees. The author provides optimization analysis on two different networks: smooth activations and ReLU activations. Their analysis shows that the overparameterization on branch and trunk networks allows faster convergence to the global solution. ",
            "strength_and_weaknesses": "Strength\n\n1. The paper is well-written and easy to read.\n2. The author's convergence analysis includes both smooth activations and ReLU activations.\n3. Theoretical results also match the empirical observations.\n\nWeakness\n1. Deng et al. (https://arxiv.org/abs/2102.10621) have studied the convergence analysis on DeepONets. The author should cite this work in the modified draft.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Well written. Please add the Deng et al. to the related work.\nQuality: Good\nNovelty: There exists a previous literature on convergence analysis of DeepONet. However, the analysis technique is different. \nReproducibility: N/A",
            "summary_of_the_review": "Overall, I believe it is a good paper to have the better understanding on DeepONet.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_sgD3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5740/Reviewer_sgD3"
        ]
    }
]