[
    {
        "id": "cibj2IyoAu",
        "original": null,
        "number": 1,
        "cdate": 1666863814782,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666863814782,
        "tmdate": 1666906976933,
        "tddate": null,
        "forum": "Y-J3jGFcnr4",
        "replyto": "Y-J3jGFcnr4",
        "invitation": "ICLR.cc/2023/Conference/Paper1523/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents two novel methods for performing dynamic quantization of the activations and weights used during training. That is the authors suggest two ways to dynamically calculate the number of bits used to compress the activations and weights during training. The first method, Quantum Mantissa, adds an additional term to the loss, so the model is able to learn the required quantization for each layer during training. The authors show that this approach can reduce the footprint significantly without sacrificing validation accuracy while incurring only modest overheads. The second method, BitChop, is a less intrusive method that greedily attempts to decrease the network-wide bitlength while the loss keeps decreasing and increases the bitlength when the loss increases. They show that BitChop also decreases the footprint significantly incurring even smaller overheads. The authors also introduce a lossless delta encoding scheme to encode the exponent bits by taking advantage of the non-uniformity of the exponent values. They call this approach Gecko. By combining the mantissa encoding schemes and Gecko the authors evaluate the entire model, which they call Schrodinger FP, on ResNet18 and MibileNet V3 Small on Imagenet. They show that Schrodinger FP has similar performance to the full model but with a significantly lower footprint that leads to a more performant and energy-efficient training scheme.",
            "strength_and_weaknesses": "# Strengths\n- The paper addresses an interesting problem that seems to suggest a significant opportunity for making training large-scale models more efficient and performant.\n- The results are very compelling.\n- The techniques seem simple to implement.\n- The authors have done a great job providing references and situating their work within the rest of the contemporary research in the community.\n\n# Weaknesses\n- The paper was not very polished and difficult to read.\n- The description for quantization could be improved. I still don't understand how it works. Perhaps an example would help.\n- What does it mean to fix the bitlengths for some training time to fine-tune the network to this state?\n- Error bars and standard deviations are not calculated.\n- I would like to see a fair comparison of Quantum Mantissa used on a single quantization level for the entire network with BitChop.\n- I would like to see more ablations on the experiments tried for the decisions of BitChop.\n- Fig 2c suggests that each network layer should have a different quantization level, but it seems like BtChop only needs a single quantization level for the entire network. The reason for this is not clearly explained.\n- For Gecko, since the distribution is already available why not use an optimal value-based lossless encoding like a Huffman encoding scheme",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper was not very clearly written, there were several sections that were difficult to parse.\n- This is a medium-quality paper with lots of references to contemporary work in the area. The authors do a great job situating their work within the rest of the research in the community. However, the paper still needs a little polishing.\n- To my knowledge this approach is novel.\n- I have not tried to reproduce this work.\n\n# Minor Points\n- Is equation (3) supposed to be $m_i$ instead of $n_i$ and what is the sum over?\n- In Figure 2b the x-axis is clipped.\n- Is Fig 4a the distribution or the CDF?",
            "summary_of_the_review": "While the paper tackles a very interesting problem and has very compelling results, I don't think the paper is ready for publication in its current form. The prose needs some work, and there need to be more experiments as well as standard error measurements before the paper is ready for publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1523/Reviewer_hmeb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1523/Reviewer_hmeb"
        ]
    },
    {
        "id": "hh5Owj66O6",
        "original": null,
        "number": 2,
        "cdate": 1667081370435,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667081370435,
        "tmdate": 1667081370435,
        "tddate": null,
        "forum": "Y-J3jGFcnr4",
        "replyto": "Y-J3jGFcnr4",
        "invitation": "ICLR.cc/2023/Conference/Paper1523/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed reduced-precision quantization methods for efficient deep neural network training. One of the methods mainly proposed in this paper parameterized the number of mantissa bits so that the number of mantissa bits can be automatically adjusted during training (Quantum-Mantissa). The authors also proposed another (more computationally simple yet less effective) mantissa bit decision algorithm based on the difference in the loss (BitChop). Lastly, the authors proposed a lossless exponent bit encoding (Gecko). The authors claim that the proposed method can reduce memory transfer while maintaining original model accuracy after training.",
            "strength_and_weaknesses": "(Strengths)\n- The concept of customizing mantissa and exponent bits would be critical for improving the efficiency of challenging DNN training.\n\n(Weaknesses)\n- Although the proposed Quantum-Mantissa method is based on parameterization of bit-width for the mantissa, there is no in-depth analysis of how well this parameter is optimized. Also, there is no discussion on the impact of these techniques on the training convergence \n\n- The proposed methods are mainly applied to forward-pass computation, and thus the scope of computational savings is limited. Also, the authors presented the energy savings based on their hardware implementation, but it is hard to thoroughly verify the claimed advantages given the limited information provided by the discussion.\n\n- The evaluation is very limited; only covers two CNNs with typical types. So, it is hard to generalize the proposed methods.",
            "clarity,_quality,_novelty_and_reproducibility": "Although combined together, the proposed three quantization techniques seem to be separately proposed with incremental ideas, limiting the novelty of this paper. Also, the hardware benefits are hard to verify given the limited discussion about the proposed hardware architecture and experiment settings.",
            "summary_of_the_review": "The authors proposed a set of bit-reduction methods for floating point format, but without sufficient theoretical analysis and empirical justification. It would be highly appreciated if the authors add more in-depth discussion on the algorithmic success of the proposed methods and if they demonstrate this benefit throughout more variety of applications.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1523/Reviewer_5HSw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1523/Reviewer_5HSw"
        ]
    },
    {
        "id": "vIbUZbXfRO",
        "original": null,
        "number": 3,
        "cdate": 1667463144683,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667463144683,
        "tmdate": 1669005863047,
        "tddate": null,
        "forum": "Y-J3jGFcnr4",
        "replyto": "Y-J3jGFcnr4",
        "invitation": "ICLR.cc/2023/Conference/Paper1523/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper dynamically adjusts the size and format of the floating point activations and weights using gradient-based approaches to improve the training energy efficiency and execution time of ML models. It also implements encoders and decoders that guided by these approaches reduce the off-chip memory accesses, further boosting the energy efficiency.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper is generally well-motivated and the problem chosen is timely, given the adverse environmental impact of training of large-scale ML models.\n\nWeaknesses:\n\n1. The paper is hard to read at times. For example, in the abstract, the authors pose Schrodinger's FP as one of the contributions, but it is only described in the supplementary materials. It is very unclear to me what is the encoding/decoding scheme, how does the bit-length reduction results in the memory footprint reduction, etc.\n2. The paper's contributions look incremental. For example, the stochastic quantization scheme employed for Quantum mantissa seems similar to a wide range of works [1-2]. The loss function to learn the optimal bit-length is not new either [3]. In fact, there are also works that progressively reduce the bit-width of the network during training [4].\n3. The authors did not compare their results with integer (fixed point) quantized neural networks which can even further reduce the memory footprint. I am curious what might be the accuracy drop compared to the proposed quantized FP16 approach.\n\n[1] Dong et al., \"Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization\", BMVC 2017\n\n[2] Gholami et al., \"A Survey of Quantization Methods for Efficient Neural Network Inference\", 2021\n\n[3] Yang et al., \"BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization\", ICLR 2021\n\n[4] Datta et al., \"ACE-SNN: Algorithm-Hardware Co-design of Energy-Efficient & Low-Latency Deep Spiking Neural Networks for 3D Image Recognition\", Frontiers in Neuroscience 2022",
            "clarity,_quality,_novelty_and_reproducibility": "The paper's clarity and quality can be significantly improved as mentioned above. The novelty is also marginal. The authors did not open-source their code yet for reproducibility but that did not affect my rating.",
            "summary_of_the_review": "See weaknesses above. I am giving a score of 3, but I can reconsider my score based on the authors' rebuttal and discussions with the other reviewers.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1523/Reviewer_JoBj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1523/Reviewer_JoBj"
        ]
    },
    {
        "id": "aBOUGP-wLVt",
        "original": null,
        "number": 4,
        "cdate": 1667487958673,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667487958673,
        "tmdate": 1667487958673,
        "tddate": null,
        "forum": "Y-J3jGFcnr4",
        "replyto": "Y-J3jGFcnr4",
        "invitation": "ICLR.cc/2023/Conference/Paper1523/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a combination of three hardware-accelerated techniques (Gecko, Quantum Mantissa, and BitChop) for optimizing the memory footprint of model training through low-precision floating point tensors. The approach is inspired by the observation that in some scenarios, the value distribution of training state, such as weights and activations, could be represented with fewer bits without harming the model performance. In particular, the paper proposes both dynamic and adaptive techniques for lossy and lossless quantization of the mantissa and exponents of training tensors.  The evaluation results show memory savings and negligible convergence impact for some image models. ",
            "strength_and_weaknesses": "Strengths\n1. Scaling model size is the current approach for improving quality, and so the memory footprint of model training is an important problem for training efficiency and costs. \n2. Dynamic and adaptive techniques are likely most effective because of high variability of models, hardware, and training phases. \n3. Hardware-acceleration is probably the most efficient way to realize these optimization techniques. \n\nWeaknesses\n1. I think the evaluation is greatly weakened by only considering image models. In particular, it would have been useful to evaluate the effectiveness on NLP models, which have driven most of the interest in mixed-precision training. \n2. The paper does not discuss the handling of overflows/underflows during training. In my experience, these are important issues for mixed-precision training in practice. \n3. The opportunity for low-precision representation of training tensors because of their value distribution is not a novel observation and is well-studied. I feel that new insights to the community could be in more efficient exploitation approaches (hardware acceleration is certainly a good direction) and demonstrating generality over a range of model architectures (e.g., transformers) and tasks (e.g., NLP, multi-modal). ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to read. ",
            "summary_of_the_review": "Overall, I feel the paper is lacking in novelty and evaluation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1523/Reviewer_pQea"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1523/Reviewer_pQea"
        ]
    }
]