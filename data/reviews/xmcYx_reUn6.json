[
    {
        "id": "IlB_YhYso2k",
        "original": null,
        "number": 1,
        "cdate": 1666626494981,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626494981,
        "tmdate": 1670529472401,
        "tddate": null,
        "forum": "xmcYx_reUn6",
        "replyto": "xmcYx_reUn6",
        "invitation": "ICLR.cc/2023/Conference/Paper985/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "I'm increasing my score from 5 to 6 as the authors have addressed several of my concerns.\nHowever, I am still not convinced about the utility of this architecture or pretrained weights beyond the specific experimental setting that is considered by the authors because the task someone is performing has a big effect on the brain activity. I would expect a paper that claims to deliver the BERT of brain recordings to show this. So I really hope that the authors indeed include a disclaimer about this in their manuscript.\n\n^^^^^^^^^^^ POST-REBUTTAL ^^^^^^^^^^^^^^^^^^\n\nThis work proposes an approach to pretrain a transformer from scratch, akin to BERT (though with fewer layers), to predict masked intracranial recordings of people viewing movies. The pretrained model can generalize to heldout subjects and electrode positions. The claim /hope is that this pretrained model can be used, similarly to pretrained models in NLP, as a multipurpose tool that can be easily fine-tuned for \"downstream\" neuroscience tasks. The representations of intracranial recordings that are constructed by the pretrained BrainBERT lead to higher decoding accuracy in several \"tasks\" (e.g predicting the volume of the stimulus audio, predicting whether the stimulus is speech vs non-speech, etc), than other baseline representation methods (e.g. the raw recordings, a Fourier transform of the raw recordings, a 5 layer feed forward neural network).\n\n\n",
            "strength_and_weaknesses": "Strengths:\n- clearly written for the most part\n- potentially impactful (if major concerns are addressed, see below)\n- timely\n\nWeaknesses:\nMajor:\n1. the 10 subjects used to pretrain the model are very young (ages 4-19, the distribution of ages among the subjects is not clear) and it's not clear how well data from these subjects would generalize to a more general population. The work shows some results from holding out one of these subjects, but it's not clear whether these results are averaged over holding out each of the subjects, or they are for only one randomly selected subject. It would be helpful if the authors can clarify this and show whether there are any differences in results when generalizing to subjects of different ages. The authors should also discuss this possible limitation of their pretrained model.\n2. a truly general-purpose pretrained model would be able to generate brain activity affected by different stimuli and tasks, but there is no validation of this ability of the model in the current work. How do the authors envision this model be used -- with intracranial recordings collected under any stimulus and any task, or only for naturalistic movie stimuli with passive viewing? It's important that this is discussed in the work.\n3. the tested \"tasks\" for which the pretrained embeddings are useful are claimed to vary from low-level to high-level, but to me all of these tasks are indeed low-level because they should not require any processing beyond early auditory cortex. It would be more convincing if a higher-level task, such as word-level decoding, is enabled by the pretrained model\n\nMinor:\n- the name BrainBERT would be more fitting for a model that incorporates multiple types of neural recordings. A name that more accurately reflects the actual recordings used would make it easier to remember and distinguish from any follow-up work that incorporates other types of recordings.\n\nClarifications / Questions:\n1. some of the baseline models are said to have \"randomized weights\" -- does this actually mean that the weights were randomized or that they were randomly initialized? Randomly initialized weights would be more convincing as a baseline, since it's not clear what the weight distribution is and how effective randomizing them would be for reducing the effect of training\n2. how was the stopping point for training determined for all models? \n3. are the intrinsic dimensionality results only possible because of the pretrained model? Can these analyses be done directly with the raw data or the Fourier transform, and if so, would the results be different?",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, and reproducibility of the work are solid. \nThe novelty is also good within the neuroscience community, though the approach heavily builds off of existing approaches for speech self-supervised models.\n\n\n",
            "summary_of_the_review": "In theory, this work should represent an advance for the computational neuroscience community, but it is not clear how general-purpose the representations that this model produces will be to brain recordings from different types of stimuli and different types of experimental tasks. I would change my assessment of the work, if the authors are able to address the three major concerns that I described above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The proposed approach integrates brain recordings from individuals into a pretrained model that will be made publicly available. The authors need to discuss any possible implications for privacy and security.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper985/Reviewer_1MuJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper985/Reviewer_1MuJ"
        ]
    },
    {
        "id": "7xWOmNf3-Y",
        "original": null,
        "number": 2,
        "cdate": 1666656558365,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656558365,
        "tmdate": 1669226547202,
        "tddate": null,
        "forum": "xmcYx_reUn6",
        "replyto": "xmcYx_reUn6",
        "invitation": "ICLR.cc/2023/Conference/Paper985/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "EDIT: I have upgraded my overall score (6 to 8), as well as my correctness score (2 to 4).\n\nThis paper proposes the use of the bidirectional transformer BERT to model intracranial EEG (iEEG) recordings collected from human patients while watching movie stimuli. They transform the iEEG data into spectrograms and use an approach from speech audio modeling to train the model to reconstruct masked parts of the spectrogram (akin to a masked language modeling objective). They call this proposed model BrainBERT. They then show (1) that the representations learned by BrainBERT can be used for classification tasks, such as speech vs. non-speech, (2) that the method generalizes to a held-out subject with different electrode locations, and (3) a variety of controls and ablation studies providing insight into what parts of the proposal are useful. They assert that this model provides useful embeddings for intracranial recordings in a reusable, subject- and electrode-agnostic way. They also assert that the improved performance on neural decoding tasks is an important contribution to neuroscience, where \"many core results in neuroscience hinge on whether a linear decoder can perform a certain task...also critical to building the next generation of brain-machine interfaces.\"",
            "strength_and_weaknesses": "Strengths\n- Method is novel and generalizes to a held-out subject with different electrode positions\n- Authors test a suite of 4 decoding tasks, ranging from low-level (volume/pitch) to high-level feature learning\n- Several analyses are provided to understand the contributions of different pipeline choices -- e.g. STFT vs superlet-generated spectrogram, linear decoders with different inputs, a deep NN decoder, fine-tuning the whole network vs. the classification head only. This provides key information for future experimenters that may wish to tune the pipeline for specific tasks, or create a variant of the current proposal.\n- Authors promise to make the data and trained models available, which should enable their vision of reusability for neuroscientists\n- Method has potential to simplify the development of brain-computer interfaces (BCI)\n\nWeaknesses\n- The generalizability claim is quite bold, and ought to be backed up with more data. The authors only show one held-out subject, but then claim that the method generalizes across *subjects* (plural). I would appreciate a complete leave-one-out analysis, leaving out each subject in turn and quantifying the change in classification accuracy with & without subject N.\n    - This claim is also made when using the top 10 electrodes across all subjects, and the top 10 electrodes for the held-out subject. To support the generalizability claim across electrodes, it would be useful to see where these electrodes are, or at least see a metric of how far apart they are in cortex. Does the model still generalize if the electrodes are in quite different locations?\n- The \"few-shot learning\" claim is also too strong -- the model does require fewer training samples, and may be within the range of other few-shot learning claims in LM papers (which are typically some fraction of the original training set and represent many labeled samples, rather than computer vision papers which are just 2-5 samples per class). However, it definitely does not fall into the range the authors suggest is crucial for many clinical and BCI studies where, in the authors' words, \"subjects may participate in only a few dozen trials\"\n- The intrinsic dimensionality (ID) analysis is of questionable value as it is presented, and not well-supported by the data as far as I can tell.\n    - The authors suggest that there is a clustering of high ID electrodes in motor areas. This does not appear to be true to me -- no more than, say, left lateral PFC, which is where I see a cluster -- perhaps the authors can circle the area they are referring to in Fig. 5.\n    - The authors report two areas with the highest ID in the text, with no interpretation other than it is \"a novel view of functional regions in a task agnostic way.\" First of all, it's not clear what the novelty is -- typically neuroscientists report areas with lower and lower ID, and draw conclusions from this. What is the interpretation for a high ID area, and what is unexpected about these two regions having a high ID? Second, this is not actually task agnostic. All the data used to train their model is from movie viewing, and they may have seen extremely different ID numbers for other tasks (e.g. motion tasks that are not just passive perception). I grant that BrainBERT has the potential to be task-agnostic, but it isn't currently.\n- The authors suggest that \"embeddings of the neural data provide a new means by which to investigate the brain\", but they fall short of illustrating how a neuroscientist would do this. What is a conclusion one could draw from the type of data presented here? One might claim that pitch is processed in lower areas of the language processing stream, e.g. auditory cortex. This claim would need to be supported by showing differences in BrainBERT performance when using electrodes from this area vs. electrodes from another area. Even so, a neuroscientist might ask why one should use BrainBERT embeddings instead of training decoders directly on neural data. I encourage the authors to think about what additional utility / inferential power BrainBERT would provide that other methods cannot. Other authors have claimed that using NN models of the brain provide the ability to perform causal interventions (on the model, instead of the brain), which cannot be done with noninvasive neuroimaging methods like fMRI. However, causal interventions can actually be done with iEEG experiments -- so what else might BrainBERT provide?\n- Some clarity issues -- see below.\n\nFurther questions/suggestions\n- It seems the authors are only using the last layer output of the frozen BrainBERT to train the decoders. Is this correct? If so, I'm curious how different layer outputs would change the performance across tasks. It is possible that tasks requiring higher-level features do better when using late layer embeddings, while tasks requiring lower-level features (e.g. pitch/volume) do better when using early layer embeddings.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- It is difficult to compare performance on the decoding tasks across some of the manipulations.\n    - The averages are reported in the text, but it's not clear what the authors are averaging over since this is not reported in the table. Please include the averages in the tables as well.\n    - Authors also report some variance in performance (e.g. AUC +/- value), but unclear what this represents -- e.g. standard deviation over 3 random seeds? Please clarify.\n\nQuality: The model and analyses appear to be of sound quality, with the exception of the subject generalizability claim. Note I did not review the code in detail.\n\nNovelty/originality: The originality here is mainly in the application of existing techniques to neuroscience data, and then making the connection to neural decoding approaches. This is novel enough to warrant a publication at ICLR. The actual decoding results do not present any additional novelty beyond understanding parts of the model itself.\n\nReproducibility: the authors have included code and plan to publicly release the data. Again, I did not review the code in detail but it appears to be enough to reproduce the analyses once the data is released.",
            "summary_of_the_review": "Overall, I found the proposal and results interesting, and it is certainly a novel application of language modeling approaches that could be quite useful to neuroscience. However, the paper makes many strong claims which are often not supported by the data presented in the paper. I would mainly like the authors to pull back on the strength of some of their claims, and have suggested additional analyses that would bolster some of their current ones. I also encourage the authors to either isolate their claims to BrainBERT's potential use in BCI, or add more content elaborating how a neuroscientist might use the model to better understand the brain. I am rating this a marginal accept because I believe there is some merit to this approach, but the paper needs to be improved. Discussion with the other reviewers and with the authors may change this rating in either direction.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper985/Reviewer_x1NZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper985/Reviewer_x1NZ"
        ]
    },
    {
        "id": "gP30UsZw2G",
        "original": null,
        "number": 3,
        "cdate": 1666684856955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684856955,
        "tmdate": 1666684856955,
        "tddate": null,
        "forum": "xmcYx_reUn6",
        "replyto": "xmcYx_reUn6",
        "invitation": "ICLR.cc/2023/Conference/Paper985/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes BrainBERT , a self-supervised representation learning approach for intracranial recordings inspired by success in NLP and speech recognition. \n\nFirst, BrainBERT is trained on intracranial recordings while the subjects were watching videos without driven by any particular decoding task. Then, pretrained model is either finetuned or used as a feature for linear decoding of different low-level and high-level tasks.\n\nThe experiments show that features learned by BrainBERT\n1. Perform better than linear decoders directly trained on recordings\n2. Requires less data to reach good decoding performance \n3. Generalizes to new subject and electrodes",
            "strength_and_weaknesses": "Strengths:\n1. The paper is clearly written and easy to follow.\n2. The approach is validated on multiple tasks from determining volume level (low-level) to speech onset (high level) , compared with relevant baselines with relevant ablations thus making the evaluation of the proposed approach quite solid.\n3. The results are promising given the brain datasets are generally smaller, this approach shows a new direction of feature learning directly using brain recordings and then applying on decoding tasks.\n\nWeaknesses:\n1. The only weakness of this work I can see is that it is build on existing approach BERT and does not propose something entirely novel. However, I do not believe it is a major weakness as it is applied to a different problem and modified to take into account different modality.\n2. It is not clear to me how 10 best electrodes were selected. I recommend clarifying it and perhaps adding some results with best 50 or best 20 just to give an idea how much impact selection have on the decoding performance. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written very clearly. Almost all the information presented is easy to follow\nQuality: High\nNovelty: The method is not novel as it is based on BERT. However, the application, results are new.\nReproducability: To reproduce this, I hope the authors are planning to release models and data without which it will be difficult.",
            "summary_of_the_review": "This paper presented exciting new results by application BERT type training on intracranial recordings. The approach presented show promise and has potential to be applied on several other type of brain recordings to gain more interesting insights about the brain.\n\nTherefore, I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper985/Reviewer_98Er"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper985/Reviewer_98Er"
        ]
    },
    {
        "id": "V29sdub6vV",
        "original": null,
        "number": 4,
        "cdate": 1666750548909,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666750548909,
        "tmdate": 1672711465178,
        "tddate": null,
        "forum": "xmcYx_reUn6",
        "replyto": "xmcYx_reUn6",
        "invitation": "ICLR.cc/2023/Conference/Paper985/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a transformer-based architecture with a masked modeling task to operate on intra-cranial recordings directly. Some of the important components of this model include a content-aware loss to capture the burst-like nature of neural activity, a superlet representation of the raw neural data and a masking procedure based on removing frequency/time bands. The authors show that using this pretrained architecture leads to better downstream identification performance on a suite of tasks like detecting speech onsets, distinguishing between speech and non-speech sounds etc..that cannot be attributed to the complex architecture of the model alone. They also look at the impact of using frozen vs. trainable encoding layers on downstream accuracy, generalizing to new subjects and accuracy as a function of training examples. Lastly, they apply PCA to the hidden states of each electrode input to compute the electrode\u2019s intrinsic dimensionality.",
            "strength_and_weaknesses": "Strengths: This paper presents a new and interesting computational framework for identification tasks using neural recordings. I think this will be an important research direction going forward and commend the authors for pursuing a novel line of work.\n\nWeaknesses: While the paper was interesting, several of the implementation details were confusing or missing altogether. I highlight a few examples below.\n\n- How does the model handle different electrodes? Is training shared across them? I was also confused by \u201cFirst, the Linear (5s time domain input) network is trained once per electrode, for all held-out electrodes.\u201d on page 6 under baselines.\n    - I am particularly interested in the trade-offs of using fewer electrodes and if the authors found better performance on downstream task based on the location of the electrodes, as would be expected.\n- Is there any possibility for the model to encode information in the population code?\n- What is the context window during training? I presume 5 seconds from details in the introduction of section 3.\n- What do the training curves look like? What is the pretraining performance? How hard was hyper-parameter tuning?\n- Can the authors comment on the fact that the decoding accuracy is above the deep NN baseline only when the model is finetuned?\n- Nit: I would decrease the curvature contrast in Fig. 5B so the electrode values are more visible.\n- I am not sure what to make of the intrinsic dimensionality analysis as the links to working memory and a sensory lobe seem arbitrary. Furthermore, the distribution across cortex doesn\u2019t seem to follow any prior findings on gradients or functional distinctions during speech processing.\n- How many hours of data per subject on average?\n- What does the prediction head look like?\n- Is there any relationship between SNR and subject-transfer abilities (there must be!)",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The technical advancement is in the proposed modeling framework and the diverse experiments the authors conducted to show the usefulness of this approach.\nClarity: Could be improved significantly, especially for model details like the pertaining loss, performance etc..\nNovelty: tied into quality comment above.",
            "summary_of_the_review": "Overall, I think this paper is a promising methodological advancement over linearized encoding models. However, I thought the paper was a bit confusing to read and found some of the empirical results to be weak (effective dimensionality).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper985/Reviewer_TZqs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper985/Reviewer_TZqs"
        ]
    }
]