[
    {
        "id": "X-0XaEiPo4s",
        "original": null,
        "number": 1,
        "cdate": 1666270111947,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666270111947,
        "tmdate": 1666270111947,
        "tddate": null,
        "forum": "iAPs7yMjjyQ",
        "replyto": "iAPs7yMjjyQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3152/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors suggest a data generating process to evaluate the quality of counterfactual explanations with respect to factors of variation and propose two evaluation measures. The goal of the evaluation measures is to evaluate to what extent different explanation methods are (1) successfully changing the predicted label to the target label, and (2) to measure to what extent these methods generate diverse counterfactual explanations.\nFor their first measure the authors propose to use a generative model, where they model, three latent factors (causal, correlational and independent factors). While all three latent factors influence the observed features (X), only the causal latent factors truly influence the target variable (Y). Using this data generating process the authors define a successful counterfactual explanation to be one which relies on the causal as opposed to the correlational or independent features.\nThe second evaluation measure counts how many orthogonal or complementary explanations were output by any of the counterfactual explanation methods. The authors suggest that diversity of counterfactual explanations is best accessed by non-trivially diversifying explanations. To achieve this, the authors suggest using orthogonality between any two explanation vectors as a measure for non-trivial diversity. \nFinally, the authors use their diversity measure to evaluate the performance of 6 different counterfactual explanation methods on a synthetic dataset. \n",
            "strength_and_weaknesses": "Strengths:\n1. The addressed questions are timely and interesting to the XAI community.\n2. The proposed idea for deriving evaluation measures for counterfactual explanations is sensible.\n\nWeaknesses:\n1. The generative model and specifically the dependencies represented in the model (Fig. 1 (a)) are not well-motivated.\n2. The evaluation is weak: (a) the authors use only one (black-and white) synthetic dataset, (b) the different evaluation choices are not well motivated, (c) it is not clear which results generalize to other settings, (d) the evaluated CE methods behave similarly according to the proposed measure (Eq. 7), so either the authors evaluate very similar methods, or the proposed measures are non-discriminative.\n3. The structure and the clarity of the paper could be considerably improved (see points below).",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is based on some interesting and novel ideas. However, the contextualization, motivation, explanation, evaluation, and presentation of the proposed measures are rather weak and raise many questions and remarks:\n\n1. Are the proposed measures discriminative and meaningful for evaluation (given that there is little variance in the results)?\n2. Do the results generalize to other datasets or other similar generative processes? \n3. In general, different explanation techniques use the input space directly or the latent space to identify counterfactual explanations. The authors state that they use the latent space representation for all evaluated methods, which would also mean that the methods under consideration then rely on a similar optimization.  If this is true, then it is quite unusual as some of the counterfactual algorithms first learn a generative model, and then find counterfactuals with respect to the learnt model. This could also be one of the reasons why there is so little variation across the diversity results.\n4. The data generating process from Figure 1(a): Why is there an arrow from Y to Z_corr? Is it intended that X \\independent Y | Z? \n5. The title suggests that this is a general benchmark for counterfactual explanation algorithms; however, the authors only focus on counterfactual explanations for image data, and it would make sense to reflect this in the title. \n6. How does Proposition 1 fit into your benchmarking narrative? Could the authors explain that more clearly?\n7. In section 3.4 you say that \u201cthe goal of counterfactual generation methods is to find all the attributes that make a classifier behave differently from a causal classifier\u201d. Since in practice we never usually know the causal data generating process, I am wondering whether this is really the goal of counterfactual generation methods. How would one evaluate this the achievement of this goal in practice?\n\nAs it can be seen, there are many points that should be clarified or better described and motivated. Also, some real-world examples could help improve the presentation.\n",
            "summary_of_the_review": "Overall, the paper presents some interesting and novel ideas, which unfortunately are not well presented and evaluated. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3152/Reviewer_pgaK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3152/Reviewer_pgaK"
        ]
    },
    {
        "id": "IhHQYzytr3",
        "original": null,
        "number": 2,
        "cdate": 1667405361812,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667405361812,
        "tmdate": 1667405361812,
        "tddate": null,
        "forum": "iAPs7yMjjyQ",
        "replyto": "iAPs7yMjjyQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3152/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims at establishing a comprehensive definition of counterfactuals explanations. After reviewing related work, it establishes the foundations for establishing their process: data generation, counterfactual generation, classifiers, and evaluation. Their approach is compared to a number of baselines on one particular dataset which they can control explicitly, and a number of interesting conclusions are drawn from the experiments. ",
            "strength_and_weaknesses": "Strengths: \n- Highlights some weaknesses of counterfactual evaluation, and proposes interesting solutions to important gaps in the literature. \n- I found the conclusion that different counterfactual explaners in the literature perform similarly to be particularly interesting. I would welcome additional experiments and datasets to confirm this more broadly. \n- The contribution of the paper offering a more comprehensive definition of good counterfactual is very appealing\n\nWeaknesses\n- Several aspects (e.g. FN, FNF, TF, etc) were defined, but I didn't feel the rationale behind why these are useful and relevant properties for counterfactual explanations was sufficiently covered. \n- I would have welcomed some additional discussion about $z_causal$. I understand how you have access to this in the experiments (synbols), but in a more general context, how hard is it to get acces to these traits? Is there room for noisy estimates on these? \n- Similar to the above, I would have welcomed discussion about the dependency on oracles for the conclusions. \n- I wasn't convinced that the results are generally applicable, particularly given the narrow scope of the experiments. The experimental section focuses on one dataset that is rather tightly controlled. This helps us understand the dynamics of the problem very well, but I don't think the discussion allows me to see this as more than experiments on toy contexts. This would not be too problematic, but I feel like the title and conclusions claim a lot based on the experiments. I know that in the appendix some real-world considerations are discussed, but to me these are far too important to leave in the apendix. \n- For me, there are missing links between the results as presented here and the general problem. For me, a section or paragraph illustrating highlighting opportunities for readers to deploy the lessons of this paper to new environments would have been useful. The results are empirical on a well controlled environment. How would this be taken forward? \n\nMisc. & smaller points\n- I found the notation difficult to follow, which hindered me understanding the major contributions. I'm slightly unfamiliar with the literature, so this may be usual convention. \n- The legend on Fig 3 is far too small",
            "clarity,_quality,_novelty_and_reproducibility": "- I didn't find the exposition particularly clear. I think many terms were quite loaded and terminology somewhat confusing. I think it could benefit from a worked clear example before experimentation to communicate the main themes better to the reader. \n- The innovations here seem quite interesting. I think there is value in my understanding of the process described, but i felt that too many of the definitions used were un-justified. This is not to say they are unjustifiable, but I think the authors should have spent some convincing the readers that these are properties that are worth having. \n- The authors linked to the code repository for their empirical results. Although the code doesn't have a 'main' file that will reproduce the results in the paper, the code looks very clear, well documented and extensive. If I didn't miss the main file in the repo (it looks like benchmark covers much of the experiments here), I would encourage the authors to add it whenever possible. \n- The main contributions seem to be the TF, SCE and orthogonal components. These seem quite interesting and useful contributions. Although I ask for a little more clarity and justification in some of these, I think they are quite reasonable and ",
            "summary_of_the_review": "This paper has made some interesting contributions to the literature. I found it difficult to understand whether the conclusions are can be applied broadly to explainability in general, or whether it's particular to the experimental context explored. I think that the main contributions are interesting and useful too, but that some discussion around real-world and practical deployment was lacking. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3152/Reviewer_e2UL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3152/Reviewer_e2UL"
        ]
    },
    {
        "id": "KMNtvP5dWPZ",
        "original": null,
        "number": 3,
        "cdate": 1667407274541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667407274541,
        "tmdate": 1667407274541,
        "tddate": null,
        "forum": "iAPs7yMjjyQ",
        "replyto": "iAPs7yMjjyQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3152/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an evaluation strategy for the counterfactual explanations of images with focus on generative models that operate in the latent space. The paper points out flaws with existing strategies: the diversity criterion can be fooled by making irrelevant changes and the real latent attributes could be misaligned with the learnt latent attributes. The proposed metric works by disentangling the causal vs. non-causal relationships in the latent space. Experiments are reported with several methods.",
            "strength_and_weaknesses": "### Strengths\nLack of reliable evaluation methods for counterfactuals (and explanations in general) is indeed an important problem.\n\n### Weaknesses\nThere are three main weaknesses in the paper: 1) The setting in the paper is overly restrictive and it is not clear how useful the proposed metric is in the real world. 2) Some of the assumptions taken in the paper are not backed up with supportive arguments and seem at odds with what one would expect from explainers. 3) The clarity of writing can be significantly improved. For these reasons, the paper is not ready yet. Please see detailed comments and questions below:\n\n1. First of all, the focus is mainly on the synthetic synbols dataset where the real latent attributes are available and the data generation process can be controlled. On the other hand, learning these two entities constitutes one of the main challenges in learning. While this does not mean that simplified synthetic settings should not be studied, the paper does not provide any indication on how the takeaways from here would generalize to real datasets. \n\n2. Some of the basis assumptions are not clear. In section 3.2, why is it more useful to perturb \"non-causal\" attributes? The model output could be sensitive to causal or non-causal attributes. A faithful explainer should reflect the true behavior of the model. Similarly, the concept of \u201ctrivial\u201d explanation is not very clear. Assuming that the \u201coracle\u201d here refers to a human, there could be perturbations that do not change the human\u2019s prediction, but do change the prediction of the model. For instance, the model might change its prediction based on the brightness of the image (while the human will not). That could be deemed a valid counterfactual explanation since it correctly reflects the behavior of the model. \n\n3. The paper correctly points out issues with existing diversity metrics in the literature. However, I am not sure if using a single \u201cprincipled metric to compare counterfactual explanation methods\u201d is the answer. One could always use different metrics to study different aspects of explanation quality. Or does the paper mean to make the point that a single metric can evaluate all relevant aspects of an explanation?\n\n4. There has already been some work on considering causality when generating counterfactual explanations (https://arxiv.org/pdf/2002.06278.pdf). It would be good to contrast the approach in Sections 3.3 and 3.4 with the existing work.\n\n5. It is fine to focus on generative models for counterfactual explanations. However, note that there exist other methods for generating counterfactuals that do not rely on the latent attributes see for instance (https://arxiv.org/abs/1910.08485) and (https://arxiv.org/pdf/1912.09405.pdf).\n\n6. The exposition could be improved at several places. For instance, what is \"the oracle\". What is $\\hat{h}$ and how is it different from $\\hat{f}$. Since the paper introduces quite a bit of terminology, I would suggest putting all the terms together in a single table so that the reader can easily look it up, and clearly defining the input / outputs of different functions. \n\n7. It would help to differentiate from the work of Zhou et al (https://arxiv.org/abs/2104.14403) which has a very similar goal: adding \u201cground truth\u201c perturbations to the images that the explainers should pick up on.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is difficult to understand and some of the assumptions are not clear (see the detailed review above).\n\nQuality: The setting considered in the paper is very restrictive and the effectiveness of the proposed solutions on more general settings is not very clear.\n\nNovelty: The paper largely ignores existing work on causality when generating counterfactuals. Similarly, there is existing work on generating \"ground truth\" explanations which should be added to the discussion (see main review).\n\nReproducibility: The code will be open sourced on publication.",
            "summary_of_the_review": "The paper does not meet the bar for ICLR. The solution is applicable to only very restrictive settings and the main assumptions are not backed up with arguments. The paper is also quite hard to follow at several places. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3152/Reviewer_WjLU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3152/Reviewer_WjLU"
        ]
    },
    {
        "id": "bp5GtJZeJYw",
        "original": null,
        "number": 4,
        "cdate": 1667434083422,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667434083422,
        "tmdate": 1667434083422,
        "tddate": null,
        "forum": "iAPs7yMjjyQ",
        "replyto": "iAPs7yMjjyQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3152/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a novel way to compare counterfactual explanation methods, which \"explain\" why a classifier made a certain prediction by highlighting nearby points in the domain with a different outcome. Their proposal involves a synthetic dataset in which the latent causal drivers of the outcome are \"known,\" and greedily constructing a set of explanations which are \"orthogonal\" or \"complementary\" to each other. This method is built from the premise that \"The goal for counterfactual generation methods is to find all the attributes that make a classifier behave differently from a causal classifier\" and \"a good explainer should return a ... diverse set of explanations.\" They use their method to benchmark existing methods for generating counterfactuals.",
            "strength_and_weaknesses": "Strengths: The authors provide lots of helpful exposition on why they proposed the method in the way that they did, and are clear about their stance on what counterfactual explanations should be. The insights they draw at the end are interesting; for instance, \"explainers fail to consistently find more than one [attribute to alter to generate interesting counterfactuals].\"\n\nWeaknesses: The premise of the paper posits VERY strong assumptions on the *purpose* of counterfactual explanations: to be diverse, and to illustrate tensions with an optimal causal model. Though I don't deny these are *nice* qualities, I think their introduction could be clarified to assert *why* those properties are desirable (and why they did not consider other qualities). It may seem self explanatory, but really break it down: Why is it useful to know when a model is not acting \"causally\"??\n\nFurther, although their conclusions from applying their benchmark to explanation methods are interesting and believable, they seem to involve a bit of speculation, sometimes providing \"because\" explanations for the results they see that don't seem to have been directly experimentally validated. Sometimes, I felt the authors went off on tangents not totally necessary for the paper; for instance I am not sure why Proposition 1 is important.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is quite clear and novel. I did not attempt to use the code they say they posted.",
            "summary_of_the_review": "I believe this work is interesting and worthy of publication, however I think it could use some polishing, and perhaps a redistribution of emphasis (i.e. providing more results in the body of the paper rather than delegating to the appendix and briefly summarizing them at the end)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3152/Reviewer_jg6E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3152/Reviewer_jg6E"
        ]
    }
]