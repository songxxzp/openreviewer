[
    {
        "id": "7nlGe7a7DIx",
        "original": null,
        "number": 1,
        "cdate": 1666042076887,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666042076887,
        "tmdate": 1666042076887,
        "tddate": null,
        "forum": "jgUqPzuMiJQ",
        "replyto": "jgUqPzuMiJQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4465/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper utilizes the graph neural tangent kernel to theoretically prove the superiority of row normalization in a semi-supervised node classification setting. It is proven that row normalization can preserve the class structure better, and work better at reducing over-smoothing, especially when combined with skip connections,",
            "strength_and_weaknesses": "Strength:\nThis paper provides a theoretical analysis of why row normalization is better than symmetric normalization, which is absent from current research. Thus, it provides insight and potential research direction.\n\nWeakness:\n1. Graph neural tangent kernel is presented by others before.\n2. The observations are discovered previously by others empirically.\n3. The underlying reasons for the observations have been discussed or at least inferred somewhere else.\n4. No further proposal is made based on the proof.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty:\nThe author(s) claims that the paper is the first theoretical proof of this topic. However, after the proof, they fail to provide new knowledge about the GCNs field.\n\nQuality:\nThe proof looks legit and correct.\n\nClarity:\nA few pronouns are not well defined, thus bringing confusion. For example, in the second sentence in the abstract, it is not clear what the authors refer to by \"its\". \n\nReproducibility:\nDatasets and implementation are provided in the supplementary. The empirical part of this paper is not critical though.",
            "summary_of_the_review": "By theoretically proving that row normalization and skip connection work better in semi-supervised node classification tasks, this paper serves as a good suggestion for how to implement GCNs. However, it lacks a further discussion on what we can do differently from the current existing approaches.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_vkDP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_vkDP"
        ]
    },
    {
        "id": "uczsZ57fu6F",
        "original": null,
        "number": 2,
        "cdate": 1666307142905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666307142905,
        "tmdate": 1666307142905,
        "tddate": null,
        "forum": "jgUqPzuMiJQ",
        "replyto": "jgUqPzuMiJQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4465/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "I am not in good position to review",
            "strength_and_weaknesses": "I am not in good position to review",
            "clarity,_quality,_novelty_and_reproducibility": "I am not in good position to review",
            "summary_of_the_review": "I am not in good position to review",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_CyLz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_CyLz"
        ]
    },
    {
        "id": "qK5wDU3sBC7",
        "original": null,
        "number": 3,
        "cdate": 1666586127435,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666586127435,
        "tmdate": 1666586127435,
        "tddate": null,
        "forum": "jgUqPzuMiJQ",
        "replyto": "jgUqPzuMiJQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4465/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper attempted to explain the superiority of row-normalized adjacency matrices as the graph convolution operator for node classification in GNNs from the viewpoint of graph neural tangent kernels (GNTK). This paper derived GNTKs for GNNs with finite and infinite layers (and with an infinite number of units) for various adjacency matrices when the underlying graph is a populated Degree-corrected SBM (DC-SBM), and the input matrix is orthogonal. By comparing the GNTKs, this paper showed that the row-normalized adjacency matrix is more capable of identifying nodes than the other design choices (symmetrized, raw, and column-normalized adjacency matrices). In addition, this paper considered GNTK for GNNs with initial residuals and showed that GNTK could identify nodes even with an infinite number of layers, provably avoiding over-smoothing.",
            "strength_and_weaknesses": "Strengths\n- Few studies have investigated the impact of the design choice of adjacency matrices as a graph convolution operator. \n\nWeakness\n- I have a question about the significance of the research question that this paper addressed (see Novelty section). \n- The theoretical analysis needs to discuss the validity of the assumptions more (see Quality section).\n- The interpretation of the results of the theoretical analysis is questionable\n- It is desirable to clarify the relationship between the theoretical analysis and the numerical experiments more.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThis paper is well-written. I do not have any difficulty understanding the main points of the paper.\n\n\nQuality\n\nI have a question about the problem setting of the theoretical analysis because it looks somewhat artificial. This paper claimed the usefulness of row-normalized adjacency matrices by comparing the behavior of GNTKs (Theorem 2). This result strongly depends on the assumption that the underlying SBM is degree-correlated. I expected the authors to show why it is reasonable to assume the graph-generating process used in this paper.\n\nI could not see why Theorem 2 shows the usefulness of row-normalized adjacency matrices. This paper claimed the superiority of row-normalized adjacency matrices on the ground that \"only $\\tilde{\\Theta}_{\\mathrm{row}}$ exhibits a block structure unaffected by the degree correction $\\pi$, and the gap is determined by $r^2$ and $d$\". However, I could not see why the independence from $\\pi$ implies the superiority of row-normalized adjacency matrices.\n\nThe relationship between theoretical analysis and empirical evaluation was not very clear to me. \nAccording to the paper, we have an empirical fact that GNNs with row-normalized adjacency matrices perform well. Therefore, I expect the theoretical analysis's assumptions are empirically reasonable. However, this paper did not discuss the justification of the assumptions. Also, numerical evaluations considered the situation where the assumption of theoretical analysis does not hold (even approximately). Instead, this paper claimed the practical superiority of row-normalized adjacency matrices without assumptions. Therefore, it seems to me that it implies that the assumptions are superfluous and do not reflect real-world situations.\nTherefore, I could not see what the numerical evaluations wanted to show and how they were related to theoretical analysis.\n\n\nNovelty\n\nAs far as I know, no paper has given a theoretical explanation for the usefulness of row-normalized adjacency matrices in GNNs. So in that sense, this paper is novel.\nOn the other hand, I question whether the problem of this paper is sufficiently significant because, to the best of my knowledge, the superiority of row-normalized adjacency matrices is not well-known. This paper referred Wang et al., 2018, Wang & Leskovec, 2020, and Ragesh et al., 2021 as existing studies of GNNs using row-normalized NNs. However, it is difficult to say that they support the significance of the research question of this paper:\n- Wang et al., 2018 certainly claimed the usefulness of row-normalized adjacency matrices. However, they discussed it in the context of adversarial attacks. The relevance of the node prediction considered in this issue is not apparent.\n- In Wang & Leskovec, 2020, row-normalized adjacency is introduced because of the analogy with label propagation. However, they did not compare it with a symmetrized adjacency matrix was not made.\n- Ragesh et al., 2021 used three types of NNs: raw, row-normalized, and symmetrized matrices. However, it did not state which one was the best.\nTherefore, I have question about the significance of the research question this paper addressed.\n\n\nReproducibility\n\nThe code and datasets for the numerical experiments are provided. It allows readers to see the details of the implementation. In addition, instructions for reproduction are provided. Although I have not run the code, I expect that it is possible to reproduce the same experiments as the authors.",
            "summary_of_the_review": "Although the choice of the adjacency matrix is one of the design choices, the significance of the research question is questionable, considering the line of previous studies. Also, unless I missed it, the reason for making that assumption about the theoretical analysis is not discussed qualitatively or experimentally, and its validity is questionable.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_aSCc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_aSCc"
        ]
    },
    {
        "id": "KapBWJFjte",
        "original": null,
        "number": 4,
        "cdate": 1666661286468,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661286468,
        "tmdate": 1666661286468,
        "tddate": null,
        "forum": "jgUqPzuMiJQ",
        "replyto": "jgUqPzuMiJQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4465/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper defines a graph neural tangent kernel (NTK) in a semi-supervised node classification setting. Under the degree-corrected stochastic block model (SBM), it then uses the graph NTK to explain, theoretically and empirically, why (i) row normalization performs better than symmetric normalization; (ii) with row normalization, performance degrades more slowly with the network depth; and (iii) skip connections improve performance for both types of normalization.",
            "strength_and_weaknesses": "Strengths:\n\n- The problem addressed by the paper---understanding the effect of design choices such row/symmetric normalization, depth, and residual connections---is timely and important.\n- The empirical results in Figure 3 (left) are convincing in showing the effect of skip connections on class separability in node classification problems.\n\nWeaknesses:\n\n- The motivation of the paper is somewhat ill-defined. The motivation for analyzing the effect of depth comes from the observation, in Figure 1, that the performance of GCNs with symmetrically normalized adjacency matrices degrades faster with depth than that of GCNs with row normalized adjacency matrices. However, if the best accuracies in this figure are achieved for 2 layers, and the difference between symmetric and row normalization in the 2-layer case is small, why are we concerned with what happens for a larger (or even infinite) number of layers?\n- The modeling assumptions restrict the applicability of the analysis, as it is likely that the SBM model used to conduct the NTK analysis and the specific task (community detection/separability of communities) are biased towards the row normalized adjacency matrix (also known as the random walk matrix). It is well known that community detection methods based on random walks (e.g., node embeddings such as DeepWalk; spectral methods based on the non-backtracking operator) fare better than methods based on symmetric aggregations. Moreover, not every node classification problem can be modeled as detecting communities, especially when considering heterophilous graphs. This is for instance suggested by [r1] below, where it is noted that the row-normalized adjacency is more limited in the amount of topological information that it carries. \n- Although the result in Theorem 2 agrees with the numerical results, i.e., it explains why the row normalized adjacency does better than the symmetrically normalized adjacency in practice, the result itself gives no intuition as to why that is the case, and an interpretation is not given by the authors.\n- Although the NTK analysis is only accurate in the infinite width limit, the paper uses it to justify behaviors observed in the finite width limit. The justification for doing so is that the infinite width of the GCN does not affect the graph convolutions. However, infinite width certainly affects performance, as it corresponds to an overparameterized regime.\n- The numerical experiments are limited. Specifically, they are restricted to class separability on SBMs and node classification on citation networks. Neither of these problems are adequate choices to analyze the effect of GCN depth, as both are relatively simple, consensus-like problems in which shallow and linear models have been shown to achieve very good performance. Additionally, the classification accuracy, which is the evaluation metric of interest for Cora, is not reported for this dataset.\n\nMinor:\n\n- The **Remark on Assumption 1**, particularly that \"the linearized GCN performance is at par with the non-linear models with much reduced complexity\", is misleading. This may be true in consensus-like problems like Cora and Citeseer, but it is not the case in problems where the relevant information lies in high graph frequencies. See, e.g., Fig. 3c in [r2].\n- It is not \"conventional\" wisdom that GCNs \"exhibit improvement in performance as depth increases\" (third paragraph of the introduction). Overly increasing the depth may lead to overfitting.\n\n[r1] Jeong, Sowon, and Claire Donnat. \"Tuning the Geometry of Graph Neural Networks.\" arXiv preprint arXiv:2207.05887 (2022).\n[r2] Ruiz, Luana, Luiz FO Chamon, and Alejandro Ribeiro. \"Transferability Properties of Graph Neural Networks.\" arXiv preprint arXiv:2112.04629 (2021).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear and well-written, but certain passages are difficult to understand. A non-exhaustive list:\n\n- Assumption 2 and the corresponding remark are unclear; in particular, why does this assumption \"enable(s) the computation of analytic expression for the population NTK instead of the expected NTK?\"\n- Theorem 2 is hard to parse, and the discussion following it is not obvious from the theorem. Either simplify the equations, or conduct a more detailed discussion.",
            "summary_of_the_review": "The problem addressed by the paper---understanding the effect of design choices such row/symmetric normalization, depth, and residual connections---is timely and important. However, the motivation for the proposed approach is ill-defined, the modeling assumptions restrict the applicability of the analysis, and the numerical experiments are limited.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_nvrd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_nvrd"
        ]
    },
    {
        "id": "aaSwOsZ7ZS1",
        "original": null,
        "number": 5,
        "cdate": 1667237258970,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667237258970,
        "tmdate": 1667237258970,
        "tddate": null,
        "forum": "jgUqPzuMiJQ",
        "replyto": "jgUqPzuMiJQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4465/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the effects of different normalizations of the adjacency matrix as the convolution operator in graph neural networks. In particular, they present a theoretical investigation of GNNs in the Neural Tangent Kernel (NTK) limit (very wide limit where training the GNN is equivalent to kernel regression on the NTK \u2013 without the need to optimize hyperparameters). They study the effects of four different convolution operators with different normalizations on SBMs, and show that some normalizations (row norm) preserve the \u2018block\u2019 structure better than others even at (relatively) large depths of 5 or more. They show empirically that row normalization is the most likely to preserve this structure, while other operators diffuse information over the graph as the network gets deeper. The authors also investigate two types of skip connections and show theoretically that they retain the structural information of the SBM even at infinite width.",
            "strength_and_weaknesses": "Strength: Results are clear, novel, and interesting, the math seems correct, and the paper brings a new perspectives on GNNs in the NTK limit.\n\nComments:\nThe results, although compelling, are restricted to one example, i.e., preserving class structure in one model of graphs (DC SBMs). I would be interested in seeing if this holds true more generally for other models of random graphs. For instance if one considered a graph with a few core nodes and some peripheral structure, is row normalization the best choice to preserve such \u2018importance\u2019 structure here as well (where one could look at the difference between core-to-core and core-to-periphery as a metric) \u2013 this should generally hold I think, since diffusion operators capture notions of \u2018importance\u2019? Classes here could be defined as communities analogous to the authors\u2019 suggestion, but the degree distribution within each class is bimodal.\n How about graphs with many triangles, versus graphs that are locally tree-like? Does the analysis extend just as easily to signed graphs (positive and negative weights)? Even if not, a discussion on assumptions of graph structure considered here would be very useful. \nA question of particular interest to me (which may be beyond the scope of this paper), is if there exists a normalization (perhaps one of the four proposed) that best preserves the \u2018topological\u2019 structure of the graph, i.e., holes in graph, in addition to the class structure. \n\nCould one define a metric for \u2018distance\u2019 between graphs. One might define two graphs to be \u2018structurally\u2019 similar if the have equivalent \u2018mixing\u2019 between classes after d layers. A GNN with improved expressive power should be better for defining better distance metrics, I think.\n\nWhat is the conclusion from the section on skip connections? Is one type of skip connection supposed to work better with row norm, and another with sym? This was not clear to me. Panels on second row of Fig 4 (comparison between skip-PC and skip-alpha) look very similar to me. \n",
            "clarity,_quality,_novelty_and_reproducibility": "It is well written, well structured, and is of broad interest to an audience in theoretical machine learning on graphs. Their results are assumptions are clearly stated. The results are novel. \nThey cite other github repos such as GCN code they have adapted code from, but do not provide a link to their own code (as far as I can see). For reproducibility, I would suggest adding a github link to the authors' code in the NTK limit.",
            "summary_of_the_review": "The paper presents an interesting study on the expressive power of GNNs. It presents theoretical results (under certain orthonormality of features conditions), but shows that some of the results hold empirically in a more general case.\nGraph NTKs have been introduced before\u2026the novelty of this work is in the study of different normalizations of the convolution operator. Overall, while I have some questions on the generality of their results to other random graph models (if the results are general, this would strengthen their results), I believe the results are informative and interesting, and would recommend for publication, especially with additional discussion and testing for generality. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_mpKm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4465/Reviewer_mpKm"
        ]
    }
]