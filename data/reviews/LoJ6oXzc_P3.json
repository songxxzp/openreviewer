[
    {
        "id": "DsWUYwzprc",
        "original": null,
        "number": 1,
        "cdate": 1665934202358,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665934202358,
        "tmdate": 1665934202358,
        "tddate": null,
        "forum": "LoJ6oXzc_P3",
        "replyto": "LoJ6oXzc_P3",
        "invitation": "ICLR.cc/2023/Conference/Paper474/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper evaluates a set of known model-theft attacks to vision and NLP transformers. For vision, adaptation of MixMatch [Berthelot et al., 2019] to semi-supervised scenarios is the key novelty. Then the authors adapt the Dataset Inference (DI) [Dziedzic et. al. 2022b] technique for transformer models that get trained on publicly available data. As opposed to having a fully private dataset as in DI, the authors add a private counter-part data source to the publicly available training data; the expectation is that behavior on this data acts as a watermark to detect model theft. The authors show that this idea works for transformers in the vision domain and fails for the transformers in the NLP domain. The success and failure analysis are based on intuitions that are not well tested.\n",
            "strength_and_weaknesses": "### Things I liked\n\n- The paper is well-written making it easy to understand.\n\n- The authors do run a set of experiments that shows that some of the findings hold for NLP model variants like BERT and RoBERTa.\n\n### Things that need clarification / improvement\n\n- There are recent works on stealing transformer-based models [1, 2]. The authors should compare their methods to these works. Given the paper is purely empirical, relevant baselines cannot be ignored.\n\n- Novelty is limited to applying known methods (or its simple adaptations) to a particular settings. On the attack side, they use known model theft attacks (Sha et al., 2022; Dziedzic et al., 2022a) in the context of transformer encoders. For vision scenarios, they adapt MixMatch (Berthelot et al., 2019), which has been shown to work in supervised settings to a semi-supervised setting. On the defense side, they adapt the dataset inference idea (Dziedzic et. al. 2022b) and add watermarking (Uchida et al., 2017; Jia et al., 2021; Adi et al., 2018) to it. It turns out to be effective for the vision domain and doesn't work for the NLP domain. Reading Section 3 & 4 confirms this belief; the authors don't even need to describe the methods (or the modifications they make to it) in detail.\n\n- While the modification of this know defense is shown to work for vision domains, analysis on its security is missing. For example, when clubbed with public data, what are the chances that a classifier classifies as expected on the private data given it is really well trained with the public data? How should one measure the distribution of the public and the private data to ensure that classifiers trained on public data doesn't falsely give the incorrect signal on the private data (and ensure there is no overlap between them given the public training data comes form a huge web corpus)? Can an adversary obtain leak in information about private data via interaction when trying to steal the model? (Given many of these questions don't have promising answers for NLP, which is sort-of obvious from the first-order experimental analysis, further study is necessary for the vision domain to call DSI a defense.)\n\n- The motivation isn't strong and the security threat-model is unclear. For models tried in the paper, all are made available publicly (as per the current trend in vision and NLP research). Hence, showcasing theft in these models is not a strong argument. Second, in the paper the authors know the architecture of the model they are stealing. What if a new architecture comes down the road and there is no information in the public domain about them. How do you know these are transformer-based? In turn, how would you design the architecture of the thief and ensure you can steal it properly?\n\n- The authors make a bunch of statements that need further (experimental or theoretical) support. A few of them are:\n  - \"When stealing with more complex datasets (e.g., CIFAR10 vs SVHN) we can obtain a better generalization of the stolen copy, which is shown by higher accuracy on more downstream tasks. However, we are unable to steal the exact representations from vision transformers. For this, additional knowledge about the victim\u2019s training data or access to a pre-trained encoders are needed. This involves more compute as well.\"\n    - Several assumptions made here. Exact representations can't be stolen without training data or access to pre-trained encoders. Why is more compute needed to obtain victim's training data or pre-trained encoders? Given more compute, how would you steal these?\n  - \"In this work, we rely on their supervised approach leveraging pairs of sentences from natural language inference (NLI) datasets within a contrastive learning framework. It uses the entailment pairs as positives and contradictions as hard negatives.\"\n    - Why? Why is normal BERT/RoBERTa not considered?\n  - [Appendix] \"Our experiments with DI, DSI and cosine similarity for language transformers, yield some additional interesting insights that need to be taken into account when designing future defenses against stealing NLP encoders.\"\n    - I encourage the authors to use these insights to design a defense for NLP transformers. That would definitely make the submission stronger. Some of the statements such as \"We observe that if the training and test sets of an encoder consist of semantically equal or similar sentences, DI is not even enable to mark a victim encoder. This is caused since the encoders are trained to output similar/same embeddings for semantically equal sentences\" are more relevant for the SimCSE bert models maybe? Do these hold for any NLP transformer?  \n\n[1] Lyu, Lingjuan, Xuanli He, Fangzhao Wu, and Lichao Sun. \"Killing two birds with one stone: Stealing model and inferring attribute from bert-based apis.\" arXiv preprint arXiv:2105.10909 (2021).\n\n[2] Rafi, Mujahid Al, Yuan Feng, and Hyeran Jeon. \"Revealing Secrets From Pre-trained Models.\" arXiv preprint arXiv:2207.09539 (2022).\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The novelty is limited. The authors mostly apply known attacks or slightly modify known defenses to new scenarios (see above).\n- The quality of the paper is similar to \"Look, it works.\" There is no strong analysis of why something works vs. fails beyond simple intuitions. There are no sort of guarantees as to when a particular defense works vs not (see above).",
            "summary_of_the_review": "The paper lacks a strong motivation, a clearly defined threat-model, analysis of proposed methods and has limited novelty.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper474/Reviewer_e9r9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper474/Reviewer_e9r9"
        ]
    },
    {
        "id": "9Xo2aYZyXsH",
        "original": null,
        "number": 2,
        "cdate": 1666579747137,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579747137,
        "tmdate": 1666579779143,
        "tddate": null,
        "forum": "LoJ6oXzc_P3",
        "replyto": "LoJ6oXzc_P3",
        "invitation": "ICLR.cc/2023/Conference/Paper474/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method to steal and defend transformer-based SSL encoders in both language and vision domains. The stealing can be completed using the returned representations with 40x fewer queries for the languages-related tasks. And the number of queries can be decreased further for vision encoders by utilizing the semi-supervised learning. And the authors also design the corresponding defense technique, creating a unique encoder signature based on a private data subset.",
            "strength_and_weaknesses": "Strength:\nThe proposed method can successfully steal NLP and vision transformer-based encoder in a real-world API setting. And the proposed DSI can alleviate the problem of stealing transformer-based encoders.\n\n\nWeakness:\n1. For the vision tasks, the authors only proved the success of stealing on the classification models. In the real-world API, the models can be designed for other tasks, like segmentation and detection. In the proposed method is able to steal the models of other tasks?\n\n2. What if the attackers have known the private seed, or some of the private seed? \n\n3. In Table 1, when $D_v$ is SVHN, why the best performance of CIFAR10 is obtained when $D_s$ is CIFAR100?\n\n4. In the Table 3, the best performance should be bold, similar to the results of Table 1.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper proposes a strategy to steal the transformer-based encoders. However, the novelty is blurry and the authors should summary clearly the differences between the proposed stealing strategy and previous methods. ",
            "summary_of_the_review": "This paper's experimental results are sufficient but there are also some results need to be explained. And the authors should demonstrate clearly the novelty of their proposed strategies.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper474/Reviewer_MeVg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper474/Reviewer_MeVg"
        ]
    },
    {
        "id": "GDYT9Y40w8A",
        "original": null,
        "number": 3,
        "cdate": 1666637916951,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637916951,
        "tmdate": 1666637916951,
        "tddate": null,
        "forum": "LoJ6oXzc_P3",
        "replyto": "LoJ6oXzc_P3",
        "invitation": "ICLR.cc/2023/Conference/Paper474/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the model stealing attacks on Transformer-based Encoders and shows that model stealing attacks are very effective for both vision and NLP transformer models In addition, the authors also propose DataSeed Inference, which combines dataset inference and watermarking to defend against model stealing attacks.",
            "strength_and_weaknesses": "Strength: This paper is well motivated as it investigates model stealing attacks on state-of-the-art Transformer-based encoders for vision and language tasks, while most of the previous art has focused on attacking CNNs. The empirical evaluation of this work is also very solid in that it demonstrates the effectiveness of applying existing model stealing attacks on Transformer-based encoders. Furthermore, the proposed DataSeed Inference addresses the problem that Dataset Inference cannot be directly applied to transformer-based encoders, which are likely to be pre-trained from a mixture of private and public data.\n\nWeaknesses: The main weakness of this paper is that the model stealing attack techniques are adopted from previous work. For example, the authors make the observation that Transformer-based encoders are pre-trained with both private and public data. However, this paper does not study the attacks under this threat model. Moreover, Transformer-based encoders are often pre-trained using very large dataset. A recent paper [1] points out that the existing large language models requires even more data to achieve compute-optimal model. As a result, requiring a 40x fewer queries than the victim data points may still be prohibitively expensive. Is it possible to further reduce the number of queries? \n\n[1] Training Compute-Optimal Large Language Models, arxiv'22",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and easy to understand. The quality of the work is good, as the approaches to stealing encoder models, the evaluation setup, and the final defense techniques all seem reasonable to me. Reproducibility should be good as the authors have made use of open source models and code base. The novelty of the paper is limited as it does not propose any new attack method specialized for Transformer-based encoders.",
            "summary_of_the_review": "The paper is well motivated, clearly written, and technically sound. However, due to the limited novelty discussed above, I would rate the paper slightly below the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper474/Reviewer_rUX6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper474/Reviewer_rUX6"
        ]
    },
    {
        "id": "IjeVkYpF5KY",
        "original": null,
        "number": 4,
        "cdate": 1666670207111,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670207111,
        "tmdate": 1666673636226,
        "tddate": null,
        "forum": "LoJ6oXzc_P3",
        "replyto": "LoJ6oXzc_P3",
        "invitation": "ICLR.cc/2023/Conference/Paper474/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents results on stealing a transformer based encoder model (past work has mostly focused on stealing CNN based models). The authors show that both vision and language encoders can easily be stolen with 40x fewer train queries than used to train the model. The authors also present DataSeed Inference (DSI), which is a method based on Dataset Inference, to protect vision transformers against stealing. For DSI, private data is added to the dataset and is trained with augmentations and the probability of private training data and test data is compared for stolen model and independently trained models (for stolen models, the probability of the private training data will be much larger). ",
            "strength_and_weaknesses": "The authors present many experimental results and back up their claim that vision and language encoders can be easily stolen. The experimental results are obtained on multiple datasets and models. Additionally, the authors make use of different training sets to steal the encoder and show that the closer the training set is to the training set used to train the victim encoder, the easier it is to steal the victim encoder. The authors also show that DSI can be used to identify stolen vision transformers. \n\nHere are a few additional questions for the authors. In the case of language encoders, if the architecture is known but the pre-trained checkpoint used to initialize the model is unknown, how does the stealing performance change? With vision encoders, is it important to know what training strategies (crops, data augmentations, etc.) are used in order to steal the encoder? If a lot of the training data is private or pre-processed in an undisclosed manner, can the encoder trained with that data still be stolen? ",
            "clarity,_quality,_novelty_and_reproducibility": "The results are clearly presented and the paper is well written. The results of the paper should be reproducible because the experimental setting is explained in detail. ",
            "summary_of_the_review": "This paper claims to be the first work on stealing transformer based encoders. The empirical evaluation is strong and results with different models/datasets are presented for the stated setting. There is not much novelty in the method; a simple and straightforward method is used to steal the transformer encoders. Although the method is not complicated or novel, this work could as a baseline for future work in this area. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper474/Reviewer_zM28"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper474/Reviewer_zM28"
        ]
    }
]