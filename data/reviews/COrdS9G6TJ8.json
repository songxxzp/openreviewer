[
    {
        "id": "Lad-zhLtsr",
        "original": null,
        "number": 1,
        "cdate": 1666606764317,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606764317,
        "tmdate": 1666606764317,
        "tddate": null,
        "forum": "COrdS9G6TJ8",
        "replyto": "COrdS9G6TJ8",
        "invitation": "ICLR.cc/2023/Conference/Paper331/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new adaptive quantization strategy (and aggregation criteria), AQUILA, by adaptively determining the quantization bits per round per client in lazily aggreagated federated learning. The quantization level $b^*$ is chosen based on the $l_\\infty$ and $l_2$ norm of the difference between the current gradient and the previous quantized gradient. Convergence rates in non-convex and PL condition are provided. Experiments are conducted to show the advantages of the proposed method.",
            "strength_and_weaknesses": "Pros:\n1. The topic of federated learning (FL) is important and interesting to NeurIPS community.\n2. The paper provides both theoretical and empirical results.\n3. The writing and presentation, though can still be improved, is in general clear.\n\nCons:\n1. (i) The motivation is not very strong. While alternating the number of bits $b$ across training rounds might be effective, it is not guaranteed to always bring benefits (reduced communication). Specifically, in formula (10) when choosing $b^*$, it seems that the number of bits largely relies on the largest entry of the vector ($l_\\infty$ norm). What if the vector only has a few 'outliers'? This strategy would use a large $b$ to compromise those outliers. A more practical and possibly better strategy might be sending those large coordinates in full-precision, and quantize others using low bits. This may use much less communication than the proposed strategy. In my understanding, (10) basically says that if the largest magnitude of the vector is big, we use more bits. This does not look very exciting and promising to me.\n\n(ii) From Figure 2 and Figure 3, we see that the communication across training rounds of AQUILA is almost constant (a flat line). Then why not simply using a fixed number of bits? I think a figure with the evolution of the $b^*$ for the clients might help.\n\n(iii) The communication criteria (11) looks more like a byproduct for the convenience of proof (e.g., Lemma A.1). Could you please provide more intuition on it and its difference with (6)? Besides, I have two questions regarding (11). First, how do you know $\\theta^{k+1}$ at time $k$? Second, are both $\\gamma$ and $\\beta$ free hyperparameters? If so, the algorithm becomes much harder to tune which is also a potential drawback.\n\n2. The convergence analysis considers full gradient without noise, which is less practical since people use SGD in most applications. On page 6, the authors wrote 'we could still prove the correctness of Corollary 4.1 in this special condition without any extra assumptions'. Then why is Assumption 4.3 needed? \n\n3. Indeed, Assumption 4.3 seems a little bit strange. In (13), is the $\\gamma$ the same as that in (11)? Also, you can assume a very large $\\gamma$ for this condition to hold. However, I do not see this $\\gamma$ appearing in later theoretical results. So how does this assumption affect the analysis?\n\n4. The experiments uses $M=24$ clients which is rather small for FL. Also, there is no ablation study on the impact of $\\gamma$ and $\\beta$. Thus we do not know how robust this algorithm is to different choices of the hyperparameters. I think the authors should also compare with strategies with fixed-bit quantization like the popular QSGD method. This could help justify the importance of adaptive quantization and strengthen the motivation. Currently, given the above concerns, I think the motivation is not very strong.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see above.",
            "summary_of_the_review": "I feel that the motivation of this paper is not very strong. I have some concerns about the design of the new adaptive quantization strategy. The theoretical analysis needs some seemingly meaningless assumption and does not consider stochastic gradients which is less practical. The experiments lacks ablation study and comparison with important methods. Thus, I think the paper is below the high bar of ICLR in its current version.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper331/Reviewer_NDFY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper331/Reviewer_NDFY"
        ]
    },
    {
        "id": "vcQyPICiDb",
        "original": null,
        "number": 2,
        "cdate": 1666674828843,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674828843,
        "tmdate": 1666674828843,
        "tddate": null,
        "forum": "COrdS9G6TJ8",
        "replyto": "COrdS9G6TJ8",
        "invitation": "ICLR.cc/2023/Conference/Paper331/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a communication-efficient federated learning strategy, AQUILA, that combines lazily-aggregated quantization (LAQ) and adaptive quantization (AdaQuantFL). This strategy balances the communication frequency for each parameter and the quantization level adaptively. Experimental results show that AQUILA outperforms the baselines (including LAQ and AdaQuantFL individually) on CIFAR-10, CIFAR-100, and WikiText datasets. ",
            "strength_and_weaknesses": "The paper is well-written. Overall, I enjoyed reading the paper. Even though the main ideas and the approaches in the proofs are mainly borrowed from existing works, I think the authors did a good job in combining the methods.\n\n- When averaging the local objective functions in (1), why isn't the local function weighted by the number of local samples $n_m$? \n\n- Page 4, first line: quantification -> quantization.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The authors share experimental details and the code to reproduce the results. ",
            "summary_of_the_review": "Please see my comments above. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper331/Reviewer_ogm4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper331/Reviewer_ogm4"
        ]
    },
    {
        "id": "B-QdEpOjGa",
        "original": null,
        "number": 3,
        "cdate": 1667205241135,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667205241135,
        "tmdate": 1667205241135,
        "tddate": null,
        "forum": "COrdS9G6TJ8",
        "replyto": "COrdS9G6TJ8",
        "invitation": "ICLR.cc/2023/Conference/Paper331/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new communication efficient federated learning approach that combines adaptive quantization with lazy aggregation of gradients at each round to reduce the amount of data communicated as compared to prior work while providing comparable reduction in training loss. The approach is validated through theoretical analysis and experiments in both homogenous and heterogeneous federated learning settings.",
            "strength_and_weaknesses": "Strengths:\n\na) The main strength of the paper is that it combines adaptive quantization and lazy aggregation of gradients in a principled manner. Specifically, the quantization level at each round at a given node is clearly connected to the amount of new information (gradient innovation) being provided by the node instead of just depending on the loss value. \n\nb) The principled approach is then backed by analysis and experiments to show that it indeed reduces the amount of data being communicated while providing the same training performance in a variety of settings.\n\nWeaknesses:\n\na) The main weakness according to me is that the explanation of the algorithm is currently not very clear. I have provided some comments/suggestion on improving this in the following section. \n\nb) While the main selling point for AQUILA is the reduction in the amount of data being communicated, the current theoretical results only show that the training converges at the same rate as standard GD. Any analytical result characterizing the expected reduction in amount of data being communicated would significantly strengthen this work. Alternately can you provide some explanation for why such an analysis is difficult?\n\nc) Likewise, from Figure 3 (d-f) it appears that the variance in the amount of data being communicated is also significantly lower in AQUILA which is a point in its favour, especially if the network cannot handle large fluctuations in the amount of communication. Can you provide some analysis/intuition for why that may be the case?",
            "clarity,_quality,_novelty_and_reproducibility": "The overall presentation is clear but the explanation of the algorithm is quite hard to follow as some background information is missing and the explanation is spread over Section 3 of the main paper and Appendices A1, B and C. I have the following suggestion for addressing this issue:\n\na) Consider putting Algorithm 1 in the main paper. It is much easier to follow the explanation in Section 3 after reading Algorithm 1. If space is an issue, Figure 1 can be moved to the Appendix in my opinion.\n\nb) Please formally define gradient innovation, with the relevant notation, instead of just explaining it in one line in the introduction. The term is used several times in Section 3, and I don't think it is a commonly known term.\n\nb) Please provide some background or intuition behind (8) and (9). While these appear to be obtained from the quantizer in LAQ, I believe there should at least be some intuition provided for these expressions for the benefit of readers unfamiliar with LAQ.\n\nc) Please provide some justification for (10) in the main paper by suggesting how it follows from (8) and (9). Currently it seems to appear out of nowhere unless one reads the derivation in Appendix C.\n\nd) There is also no motivation for (11) and I still do not understand why it is a suitable criterion for device groups being skipped. Specifically, it seems to require knowledge of $\\theta^{k+1}$ at all worker nodes in round $k$ but since the nodes do not have access to each other's gradients I do not see how they can calculate the threshold in (11). Please clarify.\n",
            "summary_of_the_review": "The paper proposes a novel principled approach for communication efficient federated learning approach that combines adaptive quantization with lazy aggregation of gradients and is backed by both theoretical analysis and experimental evaluation. However, some points need to be explained more clearly and some background information needs to be provided to make it fully accessible to readers.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper331/Reviewer_qabV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper331/Reviewer_qabV"
        ]
    },
    {
        "id": "BdkBog28KEu",
        "original": null,
        "number": 4,
        "cdate": 1667576713654,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667576713654,
        "tmdate": 1667576713654,
        "tddate": null,
        "forum": "COrdS9G6TJ8",
        "replyto": "COrdS9G6TJ8",
        "invitation": "ICLR.cc/2023/Conference/Paper331/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a framework for federated learning by adjusting the frequency of communication between agents and the server with an adaptive quantization scheme. Specifically, the authors combine two quantization schemes, namely, the adaptive quantization rule (AdaQuantFL) and lazily aggregated quantization (LAQ). ",
            "strength_and_weaknesses": "Weaknesses:\n\n- My main concern is regarding the novelty of the proposed method and also, in improving the existing results. This paper seems to be significantly built on a prior work \"Lazily Aggregated Quantized Gradient Innovation for Communication-Efficient Federated Learning\". It is not clear how the performance of LAQ in the mentioned work is improved in theory. The authors only mention a comparison after Theorem 4.2: \" Theorem 4.2 informs us that AQUILA can achieve a linear convergence rate as in LAG (Chen et al., 2018) and LAQ when certain assumptions are satisfied.\" I wonder what would be the motivation to use the proposed AQUILA method if it achieves the same performance for already existing LAG and LAQ methods.\n\n- The paper considers a (not class) specific quantizer which limits the scope of the theoretical and simulation results. \n\n- Assumption 4.3 states that all devices\u2019 quantization errors are constrained by the total error of the omitted devices. The justification for this assumption is presented as follows: This assumption is easy to verify when M_c\u2260\u2205 this a bounded variable will always be bounded by a part of itself multiplied by a real number. This statement is not accurate and should be: This assumption is easy to verify when M_c\u2260\u2205 this a bounded variable will always be bounded by a non-zero part of itself multiplied by a real number. So, how can we guarantee that the total error of the omitted devices is non-zero?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is marginally novel compared to the existing work on quantized federated learning in the following works:\n\n- Jun Sun, Tianyi Chen, Georgios B Giannakis, Qinmin Yang, and Zaiyue Yang. Lazily aggregated quantized gradient innovation for communication-efficient federated learning. IEEE Transactions on Pattern Analysis & Machine Intelligence, pp. 1\u201315, 2020.\n\n- Tianyi Chen, Georgios B Giannakis, Tao Sun, and Wotao Yin. Lag: Lazily aggregated gradient for communication-efficient distributed learning. In Proceedings of Advances in Neural Information Processing Systems, pp. 1\u201325, 2018.",
            "summary_of_the_review": "The paper falls short to highlight the main novelty compared to the previous works on the lazy aggregation of quantized gradients for federated learning as I detailed above. The theoretical results suggest no improvement compared to existing methods such as LAG and LAQ.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper331/Reviewer_6iqd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper331/Reviewer_6iqd"
        ]
    }
]