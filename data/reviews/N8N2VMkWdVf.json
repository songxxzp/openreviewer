[
    {
        "id": "w3ZS-PiyOh",
        "original": null,
        "number": 1,
        "cdate": 1666151988638,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666151988638,
        "tmdate": 1666151988638,
        "tddate": null,
        "forum": "N8N2VMkWdVf",
        "replyto": "N8N2VMkWdVf",
        "invitation": "ICLR.cc/2023/Conference/Paper862/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposed a Concordance-induced triplet(CIT) loss for Deep Metric Learning tasks. The major hypothesis is that the ordering concordance should be invariant to any monotone transformation of the decision boundary of triplet loss. Therefore, CIT loss should with concordance can help avoid the plague of turning the violation margin. Moreover, the paper also introduces the partial likelihood term on hard triplets to speed up convergence. Multiple benchmark experiments are conducted for the new loss.",
            "strength_and_weaknesses": "Strength\nThe proposed CIT triplet loss avoids tuning the margin hyper-parameter used in the regular triplet loss. \nThe experiments are well-conducted across different types of datasets\nThe paper is well-organized and written.\n\nWeaknesses\nThough multiple datasets results are provided, they are all about retrieval performance. There should be other types of evidence to support the improvement of the new loss functions such as the embedding space structure(TSNE plots), embedding space density, and spectral decay[1] which helps understanding and metrics to show where is gained coming from with the new loss function.\n\nThe author proposes to avoid the tuning of the margin hyper-parameter. But the new loss function introduces another hyper-parameter \\gama, which is dimming the novelty.\n\nIn figure 3, why all the ending points are not with the same epoch size? The training of VIT on In-shop Clothes is not stable. Suggestion: it's better to train the model multiple times and average the curve for these plots.\n\nit's much more interesting to see the gradient difference for the CIT loss and have more discussion on the gradient level and whether the proposed idea fits the explanation of the gradient. In the appendix, it's better to show the gradient in the format of the loss term w.r.t to features x_a, x_p,x_n\n\n\nsome minor issues\nin Section 2.1 the similar hard triplet samples definition should be written in Sap-m<San<Sap. Although it's the same equation, semi-hard mostly describes the negative example. Put San in the relation to Sap is clear.\nmissing the definition of Spn\n\n\n[1] Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bj\u00f6rn Ommer, and Joseph Paul Cohen.\nRevisiting training strategies and generalization performance in deep metric learning. In Proceedings of the\n37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research, pages 8242\u20138252, 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear to read\nThe novelty is incremental due to the introduction of the additional hyper-parameter.",
            "summary_of_the_review": "In sum, SOTA is important for DML research but it's not the only type of metric to evaluate the new loss function. Please check out the Strength And Weaknesses. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper862/Reviewer_nK2W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper862/Reviewer_nK2W"
        ]
    },
    {
        "id": "GUKcOfkLx_E",
        "original": null,
        "number": 2,
        "cdate": 1666590827281,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590827281,
        "tmdate": 1666590827281,
        "tddate": null,
        "forum": "N8N2VMkWdVf",
        "replyto": "N8N2VMkWdVf",
        "invitation": "ICLR.cc/2023/Conference/Paper862/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims at addressing two issues with the triplet loss: 1) needs to set a global violation margin, and 2) slow convergence during training. The paper proposes a \"Concordance-Induced Triplet\" (CIT) loss, which consists of two parts, one can be considered an exponential form of the conventional triplet loss with margin = 0; the other can be considered a variant of the softmax cross-entropy classification loss within each triplet.\n\nExperiments on three tasks (person re-identification, image retrieval, face recognition) show that the proposed CIT loss leads to comparable accuracy and convergence rate with the conventional triplet loss, without having to tune the margin hyperparameter.",
            "strength_and_weaknesses": "### Strengths\n\n1. The paper is quite well motivated at the high-level. It states clearly what issues to be addressed; how they propose to address them; and the experiments validate the proposal.\n\n2. The related works are discussed quite thoroughly.\n\n3. Experiments are conducted on three different tasks, which validate the effectiveness of the proposed method in more applications.\n\n### Weaknesses\n\n1. The paper is written based on the concept of \"concordance\". What is the formal definition of \"concordance\"? Can the conventional triplet loss in Eq(2) be considered a \"concordance-induced triplet\" loss? The paper presents several fancy loss functions, but the foundation does not seem to be that solid.\n\n2. Some detailed tech choices are not well motivated or discussed. For example,\n\n    1. Eq(4), why is it called the exponential lower-bound form? I assume it is because $1 - e^{-x} \\leq x$?\n    2. Eq(4), why do we have to take the exponential lower-bound form? Why cannot we just set the margin = 0 for the conventional triplet loss in Eq(2)?\n    3. Eq(6), what is the \"partial likelihood\" for? Why cannot it be computed as $\\frac{e^{S_{ap}}}{e^{S_{ap}} + e^{S_{an}} + e^{S_{pn}}}$ or other forms inspired by the cross-entropy softmax classification loss?\n    4. The proposed CIT loss formula in Eq(8) reminds me of the classification + triplet + center loss used in [a]. How do they compare?\n\n[a] Bag of Tricks and A Strong Baseline for Deep Person Re-identification, CVPRW 2019\n\n3. There is a typo / inaccurate definition of the set of comparable pairs $E_T$ in section 2.2. Currently it enforces $S_{ap} > S{an}$, which does not seem to be right? Otherwise, the Eq(4) and Eq(5) will become meaningless.\n\n4. The empirical significance of the proposed method is arguably marginal:\n\n    1. In Table 1, the conventional triplet loss seems to be quite robust against the margin choices, and the results are all quite competitive. Then is tuning the margin really a trouble in practice? And how about we just set the margin = 0 as default?\n    2. In Figure 3, $\\gamma < 1.0$ enables the partial likelihood loss Eq (7) and $\\gamma = 1.0$ disables it. However, the figures suggest that the convergence rates are not that different. The difference is slightly more obvious in ViT but still not very convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**. The paper is quite well written despite the typo pointed out in the weakness #3 above.\n\n**Quality**. The proposed method is technically sounding, and the experiments more of less validate the claims. But as elaborated above, it lacks definition on the key concept \"concordance\", lacks the discussions on detailed technical choices, and the significance of the results is less convincing, which dampens the overall quality.\n\n**Novelty**. The proposed method can be considered a novel contribution, although the significance is arguably marginal, given that it is a variant based on the conventional triplet loss.\n\n**Reproducibility**. The paper includes the implementation details. The loss function change also should be quite straightforward. The reproducibility should be good.",
            "summary_of_the_review": "The paper presents some interesting improvements to the triplet loss, but as elaborated above, 1) it lacks some definitions/discussions to make the proposed improvements more theoretically grounded, and 2) the significance of the proposed method is arguably marginal according to the experiments. Thus, I would recommend a reject of the paper in its current version.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper862/Reviewer_mdeV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper862/Reviewer_mdeV"
        ]
    },
    {
        "id": "i0dAvtMohjP",
        "original": null,
        "number": 3,
        "cdate": 1666692530202,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692530202,
        "tmdate": 1666692530202,
        "tddate": null,
        "forum": "N8N2VMkWdVf",
        "replyto": "N8N2VMkWdVf",
        "invitation": "ICLR.cc/2023/Conference/Paper862/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Conventional triplet-based losses require carefully tuning a decision boundary. To circumvent this issue, this paper proposes a novel yet efficient concordance-induced triplet (CIT) loss as an objective function to train deep metric learning model. Furthermore, it introduces a partial likelihood term for CIT loss to impose additional penalties on hard triplet samples, thus enforcing fast convergence. Extensive experiments are conducted on a variety of deep metric learning tasks to demonstrate the advantages of the proposed method.",
            "strength_and_weaknesses": "**Strength**\n1. The proposed CIT loss is new to me, and shows the advantages of elegance and simplicity.\n2. The paper is easy to understand and well written. The motivation of this paper is solid.\n3. Extensive experiments are conducted to demonstrate the superiority of the proposed method.\n\n**Weakness**\n1. In Eq. 4, does it need to tune for the term of \u20181\u2019 in the range of (e$^{-1}$, e) or will there be gains from tuning it?\n2. Figure 1 only shows the property of fast convergence compared with the variant of itself, how about compared with other losses (e.g., circle loss, soft triple loss) ?\n3. Hard mining usually shows benefits to model training, but it seems inconsistent with the results from Tab.3, could the author provide some explanations or discussions?\n4. Results in Tab. 1 and 2 are not always competitive compared with others. From Tab. 1, results of Circle and Angular are fluctuant with different values of margin, but those of Triplet and CT are much more stable. What are the values of margin used in Tab. 2. I am wondering, among these three tasks, if methods like Triplet or CT really need a careful tuning \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, easy to follow and understanding. Codes are not available in the submission. The proposed method is novel to me, showing the property of elegance and simplicity. The overall quality of this is good, except for some weakness in experiments",
            "summary_of_the_review": "This paper proposes a novel concordance-induced triplet (CIT) loss to train deep metric learning model, embracing the advantages of no need for margin tuning and fast convergence. However, there are some limitations as mentioned in the Weakness. I would like to hear the feedback from the authors.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper862/Reviewer_FXz5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper862/Reviewer_FXz5"
        ]
    },
    {
        "id": "-H1wmAz34v",
        "original": null,
        "number": 4,
        "cdate": 1666978407532,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666978407532,
        "tmdate": 1666978407532,
        "tddate": null,
        "forum": "N8N2VMkWdVf",
        "replyto": "N8N2VMkWdVf",
        "invitation": "ICLR.cc/2023/Conference/Paper862/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "The paper tackles triplet loss learning by introducing a novel loss function: CIT. The approach is demonstrated for different applications.",
            "strength_and_weaknesses": "Strength:\n\nThe introduction gives a clear problem statements and a motivation for the proposed approach. In this way, also the related work is caputed in a meaningful way.\n\nThe technical description is simple but seems to be reasonable.\n\nWeaknesses:\n\nThere is only a slight (if any) improvement compared to the used baselines?",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the paper is well written and thus easy to read. As the tackled approach is feasible, it might be possible to re-implement the approach. There are just a few issues that need to be addressed to increase the readability and the clarity.\n\nTo increase the readability, remove the bullet points in the introduction.\n\nCheck the mathematical writing in Secs. 2.1 and 2.2.\n\nCheck the grammar and the terms in Sec.3.1.\n\nThe arrangement of tables and figures is hampering fluently reading the experimental section.\n\nThe plots in Fig.2 are too small to see the relevant details.\n\nThe plot-in-plot plots in Fig.3 are misleading.\n\nThe bibliography needs to be seriously checked for consistency, completeness, and correctness!",
            "summary_of_the_review": "Overall, the approach seems to be feasible and is based on theoretical foundation. The paper is well written and thus easy to read. On the downside, however, the experiments do not show clear benefits to the state-of-the-art.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper862/Reviewer_TC7M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper862/Reviewer_TC7M"
        ]
    }
]