[
    {
        "id": "VAQLwLMw5S",
        "original": null,
        "number": 1,
        "cdate": 1666270318895,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666270318895,
        "tmdate": 1666270318895,
        "tddate": null,
        "forum": "1_jtWjhSSkr",
        "replyto": "1_jtWjhSSkr",
        "invitation": "ICLR.cc/2023/Conference/Paper2865/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This papers studies the generalization properties of $L_q$-stable algorithms. This notion of stability is distribution-dependent, and thus less stringent analogue of uniform stability. The authors first derive a general inequality for the sum of functions of random variables with bounded difference (Theorem 1), and use the result to show generalization bounds improving the best known bounds for $L_q$-stable algorithms, going from a $\\sqrt{N}$ to a $\\log(N)$ factor in one of the terms, where $N$ is the sample size, which is important to avoid vacuous convergence rates in regimes where the uniform stability parameter is $\\approx 1/\\sqrt{N}$. They then use their result to show excess risk bounds for $L_0$-estimators. ",
            "strength_and_weaknesses": "**Strength and Weaknesses**\n\nThe paper's technical and conceptual contributions are solid, clearly stated, and well-explained. The techniques seem sounds and the application to approximate $L_0$-ERM interesting.  \n\n**Questions/Feedback**\n\n- Could you comment on the technical novelty of Theorem 1? I understand this is a generalization of another result, but where the bounded difference is distribution-dependent, so this question is more about the mechanics of the proof. In particular, in Appendix B.1, you state \"*The proof is a generalization of that of Bousquet et al. (2020, Theorem 4) [...] We reproduce the proof here for the sake of completeness.*\". Where and how does the proof differ from that of Bousquet et al. (2020, Theorem 4), and what is the main technical addition to the proof?\n- In the statement of Theorem 1, the difference between the random variables $g_i(S)$ and $g_i(S^{(j)})$ is studied. On p.2, $S^{(j)}$ is defined as S but with the $j$-th sample point replaced by another sample point $Z_j'$. However the sample $S'=Z_1',\\dots,Z_N'$ doesn't appear in the theorem statement. Is it simply that $S'$ is missing from the theorem statement? In that case, are S and S' sampled independently from the same distribution, as stated earlier on p.2?\n- On p.3, it would be a good idea to at least explain in words what $R^*$ and $\\Delta_{\\text{opt}}$ are, instead of just referring to Eqn 8.\n- Typo p.5: \"substantially proves the overhead factor\" -> \"substantially improves the overhead factor\"\n",
            "clarity,_quality,_novelty_and_reproducibility": "The background and significance of the result are both very well-explained in the introduction and throughout the paper. In general, the paper is well written, very clear, and easy to follow (even from someone like me, who works outside of this research area). Theorem 1 seems to be a result of general interest. ",
            "summary_of_the_review": "I believe this is a solid contribution to the study of stable learning algorithms, and that the paper would be of general interest to the learning theory community. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_LaZo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_LaZo"
        ]
    },
    {
        "id": "O3ipLTDUOR",
        "original": null,
        "number": 2,
        "cdate": 1666571880303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666571880303,
        "tmdate": 1666571880303,
        "tddate": null,
        "forum": "1_jtWjhSSkr",
        "replyto": "1_jtWjhSSkr",
        "invitation": "ICLR.cc/2023/Conference/Paper2865/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper derives near-optimal exponential generalization bounds for $L_q$-stable algorithms. Compared to uniformly stability, $L_q$-stability is weaker and distribution dependent. To establish sharper generalization bounds, they first present the moment inequality for sum of random functions that satisfy the $L_q$-norm bounded difference condition, which is an extension of theorem 4 in (Bousquet et al., 2022). With this concentration inequality, they derive generalization bound and excess risk bound for $L_q$-stable algorithms. Moreover, they derive exponential risk bounds for computationally tractable sparsity estimation algorithms.",
            "strength_and_weaknesses": "Strengths:\n1. This paper derives near-optimal exponential generalization bounds for $L_q$-stable algorithms, which match the existing bounds of the uniformly stable algorithms. \n2. Based on the notion of $L_q$-stability, they derive exponential risk bounds for computationally tractable sparsity estimation algorithms under mild assumptions. \n3. To establish exponential generalization bounds for $L_q$-stable algorithms, they first extend the moment inequality in (Bousquet et al., 2022, Theorem 4) from under uniform bounded condition to $L_q$-norm bounded condition. The obtained concentration inequality can be applied to unbounded losses. \n\nWeaknesses:\n1. Theorem 2 is derived under $L_q$-norm boundness condition, which allows for learning with unbounded losses. Theorem 2 and Theorem 3 are both derived by applying the new moment inequality (Theorem 1). However, part (a) of theorem 3 requires the losses to be uniformly bounded. Compared to uniformly stability, $L_q$-stability is weaker since $L_q$-norm allows the losses to be unbounded. I am concerned about the incompatibility between the uniformly bounded assumption and the motivation of analysis of $L_q$-stability algorithm. Part (b) of theorem 3 assumes the loss to be Lipschitz with respect to the first argument. It is necessary to discuss the relationship between this Lipschitz assumption and bounded loss assumption.\n2. Theorem 3 extends the near-optimal exponential risk bounds from uniform stable algorithms to $L_q$-stable algorithms. Theorem 4 presents the theoretical results on the sparse excess risk of the inexact $L_0$-ERM problems. It seems that Theorem 4 is an isolated application of $L_q$-stability, which is not related to previous theorems in the paper. I would recommend including an explanation of the relationship between Theorem 4 and previous theorems.\n3. As pointed out in Remark 10, for misspecified sparsity models, their derived bound in Theorem 4 is slower than the existing results in (Yuan & Li, 2022, Theorem 3). They argue that their result is more broadly applicable. Besides the strong-signal condition, are there other assumptions that limit the application of results in (Yuan & Li, 2022, Theorem 3)? I would recommend including more discussions on differences of assumptions between Theorem 4 and results of (Yuan & Li, 2022, Theorem 3).",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-structured. Their derived bounds are new. The key ingredient of their generalization bounds is a sharper concentration inequality, which is an extension of an existing moment inequality. ",
            "summary_of_the_review": "The paper derives some new exponential generalization bounds for $L_q$-stable algorithm. However, it is confusing for me to make bounded assumptions in the analysis of $L_q$-stable algorithms.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_6gau"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_6gau"
        ]
    },
    {
        "id": "f2C2Yj-Y9ct",
        "original": null,
        "number": 3,
        "cdate": 1666646350020,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646350020,
        "tmdate": 1666646350020,
        "tddate": null,
        "forum": "1_jtWjhSSkr",
        "replyto": "1_jtWjhSSkr",
        "invitation": "ICLR.cc/2023/Conference/Paper2865/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper shows that Lq stable algorithms also generalize well with the \u201ccorrect\u201d rates. Specifically, recent analysis of uniformly stable algorithms has shown that uniform stability implies generalization at a $\\tilde O(\\gamma + 1/\\sqrt{N})$ rate where $\\gamma$ is the uniform stability parameter and $N$ is the sample size, improving prior results of $\\tilde O(\\gamma \\sqrt{N} +  1/\\sqrt{N})$. The current paper shows analogous improvement in results for Lq stable algorithms, where Lq stability indicates that the qth moment of difference of the losses on neighboring datasets is bounded. Since Lq stability is weaker than uniform stability, this applies to a wider variety of algorithms.\n\n",
            "strength_and_weaknesses": "The proposed results appear to both fill a natural hole in the literature and have reasonable applications. The exposition of prior work is well presented as well.",
            "clarity,_quality,_novelty_and_reproducibility": "\nNit: the definition (8) which is claimed to appear \u201cshortly\u201d on page 3 actually only appears on page 6. It would improve the exposition to define terms before usage.\n",
            "summary_of_the_review": "This result seems to fill an important gap in the literature on stability. The contribution appears strong and worthy of acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_Ryqw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_Ryqw"
        ]
    },
    {
        "id": "mMK2cC94fR",
        "original": null,
        "number": 4,
        "cdate": 1666675963078,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675963078,
        "tmdate": 1666675963078,
        "tddate": null,
        "forum": "1_jtWjhSSkr",
        "replyto": "1_jtWjhSSkr",
        "invitation": "ICLR.cc/2023/Conference/Paper2865/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper derives near-optimal generalization bound under the notion of $L_q$-stability, extending previous work on distribution free uniform stability. The authors then apply their results to derive excess risk bounds to inexact $L_0$-ERM.",
            "strength_and_weaknesses": "The paper significantly improves the previous result under the notion of $L_q$ stability, which is a novel and substantial contribution. The paper lacks numerical experiments to support sharpness of their bounds, but this is not a big deal for a theory paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, well-written and easy to follow. To the best of the reviewer's knowledge, the paper is novel.",
            "summary_of_the_review": "The problem the paper studied is well related and the authors made substantial contribution for $L_q$-generalization bounds, significantly improving previous results by removing the overhead $\\sqrt{N}$ factor. I think the paper is a good contribution to the community and would like to recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_dfYo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_dfYo"
        ]
    },
    {
        "id": "TER7ltslQm",
        "original": null,
        "number": 5,
        "cdate": 1667420880494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667420880494,
        "tmdate": 1667420880494,
        "tddate": null,
        "forum": "1_jtWjhSSkr",
        "replyto": "1_jtWjhSSkr",
        "invitation": "ICLR.cc/2023/Conference/Paper2865/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper continues the recent line of work on high-probability generalization and excess risk bounds for stable algorithms. In this regard the paper:\n\n* Extends the nearly-optimal generalization bounds for uniformly stable algorithms to $L_q$-stable algorithms.\n* Similarly, it extends the excess risk bounds for uniformly stable algorithms under the Bernstein condition to $L_q$-stable algorithms under either the Bernstein or quadratic growth conditions.\n* It considers the inexact empirical risk minimization (ERM) with sparsity constraints problem. Under Lipschitzness, (high probability) strong convexity, and bounded domain conditions, it shows it is $L_q$-stable and it proves a new excess risk bound for that problem. The bounds are comparable to others in the literature with different assumptions.",
            "strength_and_weaknesses": "**Strengths**\n\n* The generalization error bounds under $L_q$-stability are near-optimal, improving upon the current bounds, and matching the rate of their uniform-stability counterparts.\n* The excess risk bounds under $L_q$-stability match the rate of the uniform-stability counterparts under the Bernstein condition and add a new form under the quadratic growth condition.\n* The bounds on inexact ERM with sparsity constraints include algorithms like iterative hard thresholding (IHT) and match the rate of current bounds under potentially milder conditions. \n\n**Weaknesses**\n\n* There are some parts in the text and the proofs where the contributions and influence of previous work are not clear. It would be good to clarify these parts.\n\n  * The bound in Proposition 1 is *[Celisse and Guedj 2016, Corollary D1]*, which is a Corollary of *[Boucheron et al 2005, Theorem 2]*. Although the text preceding the proposition mentions the references, the wording is slightly confusing.\n  * The proof of Theorem 1 essentially follows *[Bousquet et al 2020, proof of Theorem 4]* with little variation to adapt it to $L_q$ stability. The same happens with the proof of Theorem 2 which follows essentially the combination of Lemma 7 and Theorem 4 of *[Bousquet et al 2020]*. Also with the proof of Lemma 5 which follows *[Klochkov and Zhivotovskiy 2021, proof of Lemma 3.1]* and the subsequent beginning and part (a) of the proof of Theorem 3 which follows *[Klochkov and Zhivotovskiy 2021, proof of Theorem 1.1]*.\n\n* In the exposition of the results, sometimes the assumptions are unclear, usually due to the notation $\\lesssim$ (which needs to be introduced in the notation section).\n\n  * The standard results from the uniform-stability literature usually require the loss is bounded by some constant, say $L$. This usually appears in (2), (3), (4), and (5), even under the use of $\\lesssim$ to make this assumption explicit. Having these constants would help understand better the improvement of using $L_q$-stability. If they are not there, it would be good to explicitly mention these bounds hold under this assumption prior to their introduction.\n  * Similarly, in (5) the Bernstein condition constant is usually present to make sure this dependence is clear.\n  * The same happens when presenting the contributions in Section 1.2. \n  \n    * The generalization bound should have the constant $M_q$ demanding a finite moment $\\lVert \\ell(A(S);Z) \\rVert_q \\leq M_q$ or at least an explicit mention that this is required. \n\n    * The excess risk bound should either have the Bernstein constant $B$ and a bounded constant, or an explicit mention that these assumptions are required. Moreover, it should include the Lipschitz constant $G$ and the strong-convexity constant $\\mu$  or an explicit mention that these assumptions are required. \n\n* Algorithm 1 seems self-referential. Namely, note that $\\tilde{w}_{S,k} := \\argmin_{w \\in \\mathcal{W}, \\textnormal{supp}(w) \\subseteq \\tilde{J}} R_S(w)$ and $\\tilde{J} = \\textnormal{supp}(\\tilde{w}_{S,k})$. This made understanding the section a little more difficult.\n\n* Remark 10 seems a little unfair. It is comparing a bound for misspecified models *[Yuan and Li 2022, Theorem 3]* with a rate $\\mathcal{O}(1/N)$ with further assumptions with their $\\mathcal{O}(1/\\sqrt{N})$ bound on Theorem 4 with weaker assumptions. However, it does not consider the $\\mathcal{O}(1/\\sqrt{N})$ bound of the same paper *[Yuan and Li 2022, Theorem 1]* which has very similar assumptions (i.e. does not need a bounded parameter space nor a strongly convex population risk as this paper does but requires smoothness of the empirical risk) that can be similarly applicable.\n\n* The paper claims that it uses the developed theory to bound the excess risk of inexact ERM with sparsity constraints. This is not the case, the excess risk bounds are based on $L_q$-stability, yes, but don't use the theorems in Section 2. First, the assumptions are larger in that setting, and second, only some ideas of the proof of Theorem 3 are employed to prove Lemma 6 in the Appendix. \\\nI believe this should be clarified in the Abstract and the Introduction. Section 3 serves as a motivation for the importance of studying $L_q$-stability, but not for the usage of their generic bounds for $L_q$-stable algorithms, since they are not used there.\n\n* Could you please clarify or write explicitly in the text the final step in the proof of Theorem 3, part (a) and part (b)? That is, the step that follows the \"which implies that\" and \"which then implies\".\n\n* The exposition would be clearer if the excess risk bounds and concepts were also introduced in the problem setup instead of that later in Section 2.3. \n\n**References**\n\n*[Boucheron et al 2005]* Moment inequalities for functions of independent random variables. \\\n*[Celisse and Guedj 2016]* Stability revisited: new generalisation bounds for the leave-one-out. \\\n*[Bousquet et al 2020]* Sharper bounds for uniformly stable\nalgorithms. \\\n*[Klochkov and Zhivotovskiy 2021]* Stability and deviation optimal risk bounds with convergence rate o(1/n). \\\n*[Yuan and Li 2022]* Stability and risk bounds of iterative hard thresholding.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* **Clarity**: The paper is clear and well-written.\n\n* **Quality**: The quality of the paper is good. \n\n* **Novelty**: The gross of the proofs to extend the uniform-stability generalization and excess risk bounds to $L_q$ stability is not novel. However, the results are, as well as the results and proofs of Section 3.\n\n* **Reproducibility**: \\\n*Theory*: I reproduced all the proofs except for the questions that I placed the authors in the weaknesses. \\\n*Experiments*: There are no experiments.",
            "summary_of_the_review": "This paper extends and builds upon the current best rate bounds on the generalization and excess risk bounds under the uniform-stability assumption to the milder $L_q$-stability assumption. \n\n* For the generalization bounds, instead of further needing a bounded loss, it needs bounded moments. \n* For the excess risk bounds, it either maintains the requirement of a bounded loss and the Bernstein condition or requires Lipschitness and the quadratic growth condition. These rates under the latter assumptions are novel as far as I know.\n\nThe paper needs to make clearer, though, the influences of the previous literature in their proofs, since some of them follow these papers closely. Similarly, it needs to further clarify some of the assumptions in the introductory text.\n\nThen, the paper finds bounds for the excess risk of inexact ERM with sparsity constraints (which are applicable to IHT). First, they prove these algorithms are $L_q$-stable and then they find specialized bounds that are comparable to those in the literature.\n\nThe paper needs to make clearer the comparison of their bounds and assumptions to those in the literature as well as the reason for Section 3, which does not use directly their bounds for generic $L_q$-stable algorithms. \n\nOverall, I believe the paper is well written and is a good contribution, so I recommend acceptance. Nonetheless, I still believe the comments in the weaknesses part should be addressed. \n\n**Minor comments and nitpicks that did not impact the score of the review**\n\n* Please, introduce the notation $\\lesssim$.\n\n* In the first paragraph of page 2, \"the Efron-Stein inequality\".\n\n* In the paragraph before Definition 1, \"introduce the ~~following~~ concept of uniform stability*.\n\n* In Remark 1, use either \"uniformly bounded\" or \"uniform boundedness\".\n\n* Before (7), please mention explicitly that this holds with probability $1-\\delta$.\n\n* Before the acronym HTP, give the name \"Hard Threshold Pursuit\".\n\n* For completeness, in page 7, after the displayed equation before Definition 3 mentioned that $\\tilde{w}_{S,k}$ is the output of Algorithm 1.\n\n* In Section 3, describe what you mean by support to disambiguate with the support of distributions since you are also working with random objects in this work.\n\n* In the last paragraph in Section 4, either \"a cardinality constraint\" or \"cardinality constraints\".\n\n* In the Conclusion you don't mention anything about your results on inexact ERM with sparsity constraints. Maybe you want to write a little about that.\n\n* Throughout the text you are using $a$ for the constant on the sub-exponential term and $b$ for the constant on the sub-Gaussian term. However, in the first bullet point of Lemma 4, you reversed them.\n\n* Maybe you want to separate Remark 11 into two remarks. \n\n* At the beginning of Theorem 1 you say \"we pad the set with extra...\", please write explicitly which set you mean.\n\n* In the last paragraph of page 15: \"Based on the triangle and Jensen's inequalities we can show that\"\n\n* In (16) I think it should be $4 \\sqrt{2 \\kappa Nq} M_q$ instead of $2 \\sqrt{2 \\kappa Nq} M_q$.\n\n* The first inequality of part (a) of Theorem 3 on page 18 and the last inequality of part (b) on page 18 are not due to H\u00f6lder's inequality since it only holds for exponents $1 < r < s < \\infty$ where the first inequalities are strict. However, it does hold due to Jensen's inequality.\n\n* There is a typo in the first equation in the proof of Lemma 1. In the $\\ell(w^* w_{S|J};Z_j)$ it should be $\\ell(w^*_{S|J};Z_j)$.\n\n* In (23) instead of inequality $\\leq$ it should be $\\lesssim$.\n\n* In the array of equations and inequalities after (29) in the second inequality I believe you forgot a factor of $\\sqrt{2}$ (it should be a 4 instead of $2 \\sqrt{2}$) and it carries over that part.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_Hu4j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2865/Reviewer_Hu4j"
        ]
    }
]