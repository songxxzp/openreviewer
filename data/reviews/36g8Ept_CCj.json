[
    {
        "id": "riMc1cmN32N",
        "original": null,
        "number": 1,
        "cdate": 1666350860980,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666350860980,
        "tmdate": 1666354790653,
        "tddate": null,
        "forum": "36g8Ept_CCj",
        "replyto": "36g8Ept_CCj",
        "invitation": "ICLR.cc/2023/Conference/Paper5833/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "When the observed data comes from a population that consists of several subpopulations, it is useful to model each subpopulation separately. Hence, the paper proposes a method that simultaneously learns a partition of the dataset so that each subset of the data comes from the same distribution. The model parameters and the partitions are learned simultaneouly, which is formulated as an optimization problem. One major contribution of the paper is to design a numerical algorithm. ",
            "strength_and_weaknesses": "**Strengths**\nThe paper is well written and can be easily followed. \n\n**Weakness**\n1. One major concern of the reviewer is that the difference of the proposed method and mixture of linear regression is not highlighted.\n- In Section 2.3, the authors mentioned that mixture of linear regression is a special case of theirs. If so, then why you get different results for these two methods in Table 1. \n- Can the authors please clatify what's the difference between the proposed approach and the general EM algorithm used in fitting mixture of regression models.\n\n2. Numerical algorithm. An MM algorithm is proposed to minimize $G(\\mathbb{S})$. The success of MM algorithm depends heavily on the majorization function, which the reviewer believe shoudl be $m_{\\hat{\\mathbb{S}}}^{G}[\\mathbb{S}]$. However, the reviewer don't think $m_{\\hat{\\mathbb{S}}}^{G}[\\mathbb{S}]$ majorizes $G(\\mathbb{S})$ since the equality does not hold when $\\mathbb{S}=\\hat{\\mathbb{S}}$. Can the author please clarify?\n\n3. Experiment result. \n- Are the differences of different approaches in Table 1 significant?\n- The k-means approaches does not use any information about label $y$ but still performs as good as the proposed method, can the authors give some justificaitons on why this is the case?\n\n\n**Minor issues**\n\n1. The definitions of monotonity and $\\alpha$-submodular need to be double checked. The current version is not correct since the $f$ does not appear anywhere in the definition.\n2. The definition of relizable and non-relizable is not clear.\n3. The number of paritions is the same as the number of classes in the dataset, what happens if the number of partitions is overspecified or under specified?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written in general. The idea is novel though there are some claims need to be better explained or justified. The experiments seem to be reproducible based on the description in the paper.\n\n\n",
            "summary_of_the_review": "One major concern of the reviewer is that the difference of the proposed method and mixture of linear regression.  The paper is marginally below the acceptance threshold before the reviewer's comments are clarified.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5833/Reviewer_jcju"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5833/Reviewer_jcju"
        ]
    },
    {
        "id": "RoE8SzRwRGj",
        "original": null,
        "number": 2,
        "cdate": 1666421667051,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666421667051,
        "tmdate": 1666421667051,
        "tddate": null,
        "forum": "36g8Ept_CCj",
        "replyto": "36g8Ept_CCj",
        "invitation": "ICLR.cc/2023/Conference/Paper5833/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a framework (called PRESTO) for simultaneously partitioning the data space and learning a separate model in each partition. The framework relies on solving an optimization problem with both discrete variables (data partitioning) and continuous parameters \n(model parameters) using a constrained optimization method.  ",
            "strength_and_weaknesses": "Stength\n - An interesting problem formulation is presented.\n - Partitioning is obtained through explicit specification of the partitions without resorting to a clustering model.\n - Theoretical bounds are provided.\n - An approximate solution to the constrained optimization problem is proposed.\n\nWeaknesses\n  - Method presentation (section 3.2) and algorithm description are not clear, thus they are difficult to comprehend.\n  - The method can not directly provide the partition assignments of new examples. Therefore an additional classification \n    model should be trained for the assignment task. \n  - Evaluation is insufficient: PRESTO is not compared to various methods belonging to the Mixture of Experts category.\n  - Time complexity is not reported",
            "clarity,_quality,_novelty_and_reproducibility": "The paper proposes an interesting formulation and includes approximation results. \nHowever, it lacks in terms of clarity, especially in what concerns the 'outline of the PRESTO algorithm' (section 3.2) and the corresponding pseudocode (Algorithm 1).\nIn what concerns reproducibility, the authors provide the code of PRESTO along with details on the used datasets. ",
            "summary_of_the_review": "The paper proposes a framework (called PRESTO) for simultaneously partitioning the data space and learning a separate model in each partition. The framework relies on solving an optimization problem with both discrete variables (data partitioning) and continuous parameters (model parameters) using a constrained optimization method.\n\nThe idea is interesting and supported by theoretical results. \n\nHowever, there are several concerns regarding the manuscript:\n1) I think that PRESTO resembles with the Mixture of Experts (MoE) family of methods. Nevertheless, the paper does not include any reference and comparison with MoE.\n2) The authors compare only with the typical (trivial) case where partitioning is first obtained using clustering and then a separate model is trained on each partition. Even in this case, the results in Table 1 do not indicate any particular accuracy improvement.\n4) The time complexity of the method should be specified and compared to the methods in Table 1. It should be noted that the method must run for various values of K in order for the best number of partitions to be determined through cross-validation.\n5) I do not think that the method could be an efficient alternative for k-means of Bergman clustering as mentioned in section 2.3.\n6) If mixture of SVMs is a special case, why not implementing PRESTO with SVMs as classifiers and compare with the original mixture of SVMs method?\n7) In my opinion PRESTO does not perform mixture modeling. Mixture models are statistical models, PRESTO is a domain decomposition approach.\n8) The intuition behind the a_G hyperparameter should be provided.\n9) Does PRESTO algorithm converge? What is the role of the 'Iterations' parameter in Algorithm 1.  \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5833/Reviewer_f3vu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5833/Reviewer_f3vu"
        ]
    },
    {
        "id": "M4mbpE_KrFG",
        "original": null,
        "number": 3,
        "cdate": 1666883583234,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666883583234,
        "tmdate": 1666883583234,
        "tddate": null,
        "forum": "36g8Ept_CCj",
        "replyto": "36g8Ept_CCj",
        "invitation": "ICLR.cc/2023/Conference/Paper5833/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the following problem: Given a labeled dataset, a parameter $K$, and $K$ model architectures, we want to partition the data into $K$ partitions and also learn all the parameters for each model corresponding to the partitions. To help understand the problem and compare their approach, I will discuss a simple baseline they use in their paper.\n\nGiven a labeled dataset, first run $k$-means++ using only the features (ignoring the labels) to form the $K$ partitions, then independently train a model for each of the partitions (now also using the labels). Note that in this approach, we have completely ignored the labels in the data-partitioning step. What this paper does is to instead also incorporate the labels and simultaneously learn the model parameters and the data-partitions.",
            "strength_and_weaknesses": "Strengths: Interesting paper which is overall well-written and studies a significant problem. Such algorithms can be useful when there is a large dataset, which was potentially generated from heterogeneous data-sources.\n\nWeaknesses: \nAlthough the problem studied by the paper is interesting, the algorithm they provide (PRESTO) is a bit complicated. Some of the simpler baselines (like the k-means++ based approach) give performance which is pretty close to their approach. I am very skeptical if any practitioner would bother using their approach for the marginal improvement in utility compared to the simpler baselines. Also, can the authors comment on the time taken by their algorithm when compared to their baselines?\n\nAlthough the paper provides a approximation bound (Theorem 4), I am skeptical if this is of any use in practice. More concretely, for some of the loss functions and the data-sets which the authors consider in the experiments, what is the value of this approximation factor?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The submission is clearly written and well-organized. I do have one qualm with the paper though. The algorithm discussed in the paper builds heavily on the some prior algorithms (Iyer et al, 2013 a;b, Durga et al. (2021)) but there is no intuition provided in the paper for either the majorization-minimization approach or the modular upper bound. It would be nice if you could add a paragraph or two briefly describing these things (appendix is also fine). I was not familiar with any of these works and it made it hard to follow the algorithm.\n\nQuality: The submission is technically sound. All claims are well-supported with proofs and detailed experiments.\n\nNovelty: I am not an expert in this area so I am not entirely sure about other related work. I have a question here, which I hope the authors can answer. Was the specific problem discussed in this paper studied by any other work? To additionally clarify, I was curious if even this baseline approach with $k$-means++ and learning a model for each partition was discussed in any prior work.\n\nReproducibility: Proofs are provided and also the code for experiments is available.\n\nTypos/minor issues:\n\nPage 2: \u201ckey rational\u201d -> \u201ckey rationale\u201d\n\nDuplicate citations for the same paper: \n\nSoumyabrata Pal, Arya Mazumdar, Rajat Sen, and Avishek Ghosh. On learning mixture of linear regressions in the non-realizable setting. In International Conference on Machine Learning, pp. 17202\u201317220. PMLR, 2022a.\nSoumyabrata Pal, Arya Mazumdar, Rajat Sen, and Avishek Ghosh. \n\nOn learning mixture of linear regressions in the non-realizable setting. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 17202\u201317220. PMLR, 17\u201323 Jul 2022b. URL https://proceedings.mlr.press/ v162/pal22b.html.\n\nPage 4, Section 2.3, K-Means Clustering: \u201cturns out that $\\theta_k = \\mu_k$, and the cluster means and $\\ell$\u2026\u201d -> \u201cturns out that $\\theta_k = \\mu_k$, the cluster means, and $\\ell$\u2026\u201d\n\nSection 2.3 has repeated paragraph titles \u201cMixtures of SVMs and linear regressions\u201d and \u201cMixture of Linear Regression\u201d\n\nDefinition 1 on Page 5: \u201cf is $\\alpha$-submodular submodular\u201d\n\nAppendix A.1 (1) \u201c..., then $T \\subset I$\u201d -> \u201c..., then $S \\in I$\u201d\n                        (2) \u201c If $S \\subset I$ \u2026\u201d -> \u201cIf $S \\in I$\u201d\n\nTheorem 2. $\\underline{\\ell}_{min}$ definition is a bit unclear. The underscript in min should be expanded and there should be a ] after $||\\theta_k||^2$\n\n(3) of Theorem 2: $\\kappa_G(S) \\leq \\kappa_G^*$. Here, should $S$ be $\\mathbb{S}$?\n\nJust before equation (7): \u201coptimal solution $\\mathbb{S}^*$ Thus, the optimization (5) becomes\u201d : Period missing between \u2018$\\mathbb{S}^*$\u2019 and \u2018Thus\u2019\n\nAlgorithm 1 Line 2\n\nMissing period at the end of Theorem 4.\n",
            "summary_of_the_review": "Overall interesting paper. I am currently leaning to accept but would be willing to increase my score after the author response.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5833/Reviewer_dXtq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5833/Reviewer_dXtq"
        ]
    }
]