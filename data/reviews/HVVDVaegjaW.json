[
    {
        "id": "xrKVgsbcgn",
        "original": null,
        "number": 1,
        "cdate": 1666697473160,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697473160,
        "tmdate": 1666777994922,
        "tddate": null,
        "forum": "HVVDVaegjaW",
        "replyto": "HVVDVaegjaW",
        "invitation": "ICLR.cc/2023/Conference/Paper4037/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a theoretical framework for GANs, called MonoFlow, that challenges their usual understanding as an adversarial minimization of a distance/divergence. It interprets GAN training as the generator following a reparameterized gradient flow of the reverse KL, defined through the log density ratio estimated by the discriminator, yielding convergence results. This more accurate framework encapsulates numerous GAN models without gradient penalties and takes into account common GAN practice like non-saturating losses and discarded gradients for the generator. The validity of the developed theory is then empirically assessed on toy data and simple image datasets.",
            "strength_and_weaknesses": "### Strengths\n\nThe proposed framework is **well motivated** in the paper, as it answers several key issues with previous interpretations of GANs highlighted in the introduction. Treating GANs as gradient flows like in MonoFlow **does solve recurring problems in GAN theory and is a relevant approach**. MonoFlow is **well presented** with a sufficient level of details for non-experts of gradient flows to be able to follow the paper. **Experiments illustrating the validity of the theory** are a nice addition supporting its relevance.\n\nTo the best of my knowledge, the key contribution of this paper is **the link between many GAN models and the gradient flow of the reverse KL**. This has **many interesting consequences**:\n - it links conflicting views of GANs and score-based generative models (Section 2);\n - it formalizes why non-saturating losses can make GANs converge, which was a known fact but only understood intuitively;\n - it gives general criteria to design GAN models that converge;\n - it better fits GAN practice than many previous theoretical approaches.\n\nThis makes the contributions **novel and potentially significant** for future studies.\n\n### Weaknesses\n\nUnfortunately, this submission is diminished by several important problems.\n\nFirst of all, I think that there are **issues in the presentation and proofs of theoretical results**. The **formulation of Lemma 3.3 is somewhat convoluted** and should be reformulated. More importantly, the proofs in the appendix of Theorem 3.1, 3.2 and Lemma 3.3 are **too informal** and lack necessary details: I suggest the authors to write complete proofs. Jointly with a **lack of rigorously formulated hypotheses**, this **casts doubt on the presented results**, which is prohibitive. In particular, I found the following problems which the authors should clarify; note that this is not exhaustive and the whole proofs should be rigorously rewritten.\n - Both the main paper and the appendix **lack hypotheses on the optimization space of functions and on manipulated objects**. For example, in Lemma 3.3, in which precise space is $d$ optimized, and what are the hypotheses on $\\psi$ and $\\phi$?\n - Could the authors explain why $\\mathcal{T}$ must be a bijection in Section A.3?\n - I do not understand how the proof of Theorem 3.1 shows that $q_t \\to p$. Indeed, as explained in Section A.1, the only equilibrium point is $q_t = p$, but nothing indicates that the gradient flow tends to this equilibrium.\n\nSecondly, the novel contributions of this paper, highlighted above, are **not clearly contextualized w.r.t. existing literature**. While the related work section mentions previous work linking GANs and gradient flows, other works have tackled this topic as well: e.g., Huang & Zhang (2022) and Franceschi et al. (2022, cited elsewhere in the paper). This should be apparent from the very beginning of the paper, either in the introduction or by moving the related work section right after the introduction: while the presented framework and results are novel, they should be contextualized w.r.t. prior work on GANs and gradient flows. Moreover, the motivation for the framework (points 1, 2, 3 in Section 1) and the explanation behind the differences between adversarial training and VDM (Section 4.2) **share similar arguments as Metz et al. (2017) and Franceschi et al. (2022)** regarding gradient computations: this should be further discussed in the paper as well. Finally, I would suggest the authors to amend the qualification of Monoflow as a \"unified generative modeling framework\", as it is restricted to some GANs, even through covering many models.\n\nThirdly, there are **some clarity issues** that make it difficult to investigate the correctness and significance of the approach, which are listed below.\n - In Section 1, authors state that \"[the discriminator] is a function only depending on samples $x$ and does not include any density information from the generator\u2019s distribution\". This statement, that is repeated throuhout the paper, is quite obscure and should be clarified. This is especially the case in Section 4.2 where it helps understanding the differences between adversarial training and VDM. Moreover, I believe that the correct statement would be that this dependency does exist, but it is ignored when optimizing the generator (as expressed by the stop gradient operator of Section 4.3).\n - The contribution of Section 4.1 w.r.t. results of the previous sections are not immediate and should be explicated.\n - Could the authors clarify the differences between convergence requirements for $r(x, \\theta)$/$r(x, \\theta_{\\mathrm{de}})$ and $r_{\\mathrm{GAN}}(x)$?\n - The empirical illustration of Section 5.1 does not quite match the theory without explanations. If all considered losses are monotonically increasing functions of the log density ratio, how could $q_t \\not\\to p$ like in Theorem 3.1? Note that this is related to the raised doubts on this convergence result above in this review.\n - Less importantly, contrary to the statement in Section 3.2 that the discriminator is optimized by a \"one-step gradient update\", it is also common practice to perform multiple-step optimization of the discriminator between generator updates.\n\nMetz et al. Unrolled Generative Adversarial Networks. ICML 2017.\\\nHuang & Zhang. GANs as Gradient Flows that Converge. arXiv, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nWhile the paper is **mostly clear**, there are **some issues** described above that should be fixed before publication.\n\n### Quality\n\nAs previously discussed in this review, **proofs of theoretical results lack details and are some of these results are questionable**. This issue needs to be addressed for this paper to be publishable.\n\nAdditionally, the paper should be proofread; cf. this list of typos and formatting problems that should be corrected.\n\nTypos:\n - Abstract (p. 1): \"These analysis\" -> These analyses / this analysis.\n - Introduction, (p. 1): \"Brock et al., 201\" -> Brock et al., 2018.\n - Introduction, (p. 2): \"insteand\" -> instead.\n - Section 3.2 (p. 4): \"ascend\" -> ascent.\n - Section 3.2 (p. 5): \"will lost\" -> will lose.\n - Section 3.4 (p. 6): there is a double period at the end of the section.\n - Section 4.1 (p. 6): \"can also alternatively minimizes\" -> can also alternatively minimize.\n - Section 4.2 (p. 7): there is a double space between \"but minimizing it w.r.t.\" and \"$\\theta$\".\n - Table 2 (p. 7): \"means not converges\" -> \"means it does not converge\", \"the convergences\" -> convergence.\n - Section 4.3, Remark (p. 7): the first quote should be reversed.\n - Section 5.1 (p. 8): \"Vainilla\" -> Vanilla.\n - Section 6 (p. 9): \"Diffussion\" -> Diffusion, \"denisty\" -> density\n - References (p. 10, 11 and 12): capital letters are missing for nome proper nouns.\n\nFormatting:\n - Use `\\colon` for function definitions (e.g. $\\mathcal{F}$ on p. 2).\n - The differential operator (e.g. in $\\mathrm{d}x$) should be upright and not italicized.\n - There should be no space before a footnote mark (e.g. with footnote 2 on p. 2).\n - There should be a period after the abbreviation \"Eq\" (e.g. on p.2, in Section 3.1).\n - Tables should be properly formatted with the `booktabs` package and ideally placed on top of pages.\n - Section 6: citations in the flow of the sentence should not be parenthesized.\n - Section B: please add a visual way to differentiate generated and data points besides the color for better readability.\n\n### Novelty\n\nAs explained above, the proposed MonoFlow is **novel** and could be significant for future research on GAN optimization. However, this novelty should be properly contextualized w.r.t. to previous work as this is not the first paper evidencing these issues in previous theoretical frameworks, or linking GANs and gradient flows.\n\n### Reproducibility\n\nNo reproducibility statement is included in the submission. As indicated before in this review, proofs are too informal. There is no code and few details associated to the experimental results. Therefore, **the presented results are not reproducible enough**.",
            "summary_of_the_review": "The proposed framework is well motivated, relevant, interesting and novel. I believe that it could have a significant impact on our understanding of GAN optimization. However, I think that this paper is not ready for publication, because of three main issues: lack of rigor in the theoretical part, improper related work discussion and unclear developments complicating the assessment of the quality of the paper. Therefore, I would recommend to reject this submission and encourage the authors to strengthen it as it could become a great paper for a future top-tier conference such as ICLR.\n\nI am willing to change my evaluation, should any of my concerns or misunderstanding be addressed during the discussion period with the authors or the other reviewers.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4037/Reviewer_4htw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4037/Reviewer_4htw"
        ]
    },
    {
        "id": "L38yXIU9QAp",
        "original": null,
        "number": 2,
        "cdate": 1666801857822,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666801857822,
        "tmdate": 1670470689286,
        "tddate": null,
        "forum": "HVVDVaegjaW",
        "replyto": "HVVDVaegjaW",
        "invitation": "ICLR.cc/2023/Conference/Paper4037/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides new insight to understand GANs based on log density ratios as a gradient flow in the Wasserstein space. Specifically, the bi-level step of adversarial training in GANs is regarded as it first estimates the vector field of the gradient flow (by discriminator), and next the generator is updated to learn to draw particles guided by the vector field. In particular, the particle evolution can be rescaled via an arbitrary monotonically increasing mapping. Such a new insight enables an analysis of what types of generator loss functions can lead to the success of training GAN and identifies the difference between VDM and adversarial training.",
            "strength_and_weaknesses": "Strengths:\n1. An interesting idea with nice theories and consistent practical findings. First, it is very interesting that from the perspective of the gradient flow in this paper, all log density ratio based GAN variants are methodologically equal. These GAN variants all are different methods of estimating the bijection of the log density ratio and then mapping the log density ratio by different monotonically increasing functions. Second, the analysis of generator loss, especially Fig. 2, to reveal the failure of vanilla and MLE loss and the success of NS and logit loss from the perspective of the adopted monotonically increasing mapping, really provides a novel and practical insight.\n2. The simple trick to fix vanilla GAN works well and refreshes me. The trick is also practical support to their theories.\n3. The idea is well presented in this paper.\n\nA potential improvement:\nAn algorithmic inspiration for the generator loss has been proposed. I am wondering if such a framework can provide inspiration for the discriminator loss to compare different estimations of the bijection of the log density ratio, such as JSD divergence and f-divergence. The authors should discuss this.\n\nSome typos:\n1. In the contributions: \u201cinsteand\u201d \u2192 \u201cinstead\u201d\n2. Under Eq. (21), the two $\\tilde{f}(d*)$ are defined differently. \n\nA further thought:\n1. In the related work, the authors somehow unify GANs and diffusion models from particle evolution. I wonder if such a framework can provide some views to discuss the advantages/disadvantages of the vector field of MonoFlow and the vector field by time-dependent neural networks of diffusion models.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The idea is novel and well presented in this paper.\n2. Some typos exist in the paper. The paper needs proofreading.\n\n##Post-rebuttal:\nThe other two reviewers all point out the lacking of clarity in theories, with which I agree.\n",
            "summary_of_the_review": "I think this is a very interesting paper. The perspective of Wasserstein gradient flow indeed brings out a novel and practical insight to the GANs community, especially for the design of the generator loss. I particularly appreciate the analysis of the practical effectiveness of generator loss. However, I am not an expert on the theory of GAN, so it is possible that I did not identify the mistakes in the theoretical results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4037/Reviewer_Wf4w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4037/Reviewer_Wf4w"
        ]
    },
    {
        "id": "HOxVi3h6Tv",
        "original": null,
        "number": 3,
        "cdate": 1667022013603,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667022013603,
        "tmdate": 1669944966886,
        "tddate": null,
        "forum": "HVVDVaegjaW",
        "replyto": "HVVDVaegjaW",
        "invitation": "ICLR.cc/2023/Conference/Paper4037/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new formulation for adversarial training as a special case of wasserstein gradient flow. The discriminator update is explained as approximating the vector field of a gradient flow in the Wasserstein space, and the generator update as moving particles (samples) in this vector field. This perspective unifies several GAN variants, and provides a unified insight as to why these variants work. The paper also provides some toy experiments to verify its predictions about the reasons why some variants might work or not.",
            "strength_and_weaknesses": "**Strengths**\n\nThe proposed theory exposes an interesting connection between Wasserstein gradient flow and divergence GANs, and can better explain why certain choices of final activations (leading to different divergences) work. This new theory is closer to practice in my opinion, and as such can be very valuable in better understanding divergence GANs and how to improve them.\n\n**Weaknesses**\n\nThe paper does not read well in my opinion, some parts can use more clear explanation and better connections to background theory, in particular the convergence of Euclidean ODE to Wasserstein PDE (convergence of the empirical measure to q) deserves more attention.\n\nAnother issue is not discussing some limitations when connecting to practice, in particular, the choice of discriminator itself (and its regularizers such as R1 R2 etc) is very consequential in practice, while the paper does not discuss this choice under the lens of its theory. The new theory assumes the ratio is estimated accurately, but this is of course not true in practice, and as a consequence the updates are not strictly following the vector field of the Wasserstein gradient flow in practice, it is important to discuss to what extent will such inconsistencies affect the theory.\n\nAnother issue is that of the three disconnects you raise about original GAN theory in page 2, I don't agree with 1 and 2. For 1, the divergence's first term derivative is uniformly zero wrt generator so how does not optimizing it matter? For 2, I'm not sure I understand your argument here, can you elaborate on this given Proposition 2 of original Goodfellow et al. theory?\n\nFinally, I think the title and abstract of the paper should mention \u201cdivergence GANs\u201d, rather than GANs, to emphasize that IPM GANs are not covered by the proposed theory.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper can improve in clarity, particularly in laying out the background for readers less exposed to PDEs and continuity equations. The quality and novel are good in my opinion, and the experiments are sufficiently explained to be reproducible.\n\n**Typo fixes**\n\nPage 2: \u201cand instead we should treat GANs\u201d\n\nPage 5: \u201cdensity ratio estimation will lose density information\u201d",
            "summary_of_the_review": "This paper proposes a new theory of divergence GAN that is closer to practice and can better explain the dynamics of their training and what can make/break them. Background must be discussed more substantially, and some missing connections to practice need to be clarified and discussed. Overall, I think this paper can be valuable for both understanding and improving divergence GANs.\n\n### Update post-rebuttal\nI thank the authors for their clarifications and answering my concerns. I consider the paper interesting and valuable, however, the paper is still lacking in clarity and very hard to follow (skips some underlying assumptions and discussions to better place its results wrt existing theory). Given that the paper's main contribution is theoretical, clarity and mathematical rigor is a must, and so I maintain my borderline score of 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4037/Reviewer_wjv3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4037/Reviewer_wjv3"
        ]
    }
]