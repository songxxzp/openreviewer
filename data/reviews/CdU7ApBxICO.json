[
    {
        "id": "HCPA2XnOEuI",
        "original": null,
        "number": 1,
        "cdate": 1666332430984,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666332430984,
        "tmdate": 1666332430984,
        "tddate": null,
        "forum": "CdU7ApBxICO",
        "replyto": "CdU7ApBxICO",
        "invitation": "ICLR.cc/2023/Conference/Paper175/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to leverage the attention weights from a pre-trained transformer for guiding how the nodes and edges are sampled during graph contrastive learning. In contrast to previous work that often performs node or edge dropping for creating different views of a graph, the proposed method claims that the stochastic version by sampling via attention weights provides more meaningful views compared to the uniformly sampled ones. Experiments show improvement on MoleculeNet for graph-level classification tasks.",
            "strength_and_weaknesses": "Strength:\n- Depending on how the graph structures are constructed, different edges certainly contain different levels of information on how the neighboring nodes are related for different tasks. Leveraging additional information for helping identify the importance of the relationship intuitively should help build a more meaningful corrupted view of graphs for better graph contrastive learning.\n- The proposed modification of conventional graph contrastive learning is straightforward and easy to follow.\n- Figure 3 and Figure 4 are helpful in comprehending how the algorithm works.\n- Sensitivity w.r.t hyper-parameters analysis in Figure 6 is appreciated.\n\nWeaknesses\n- It is not clear how the transformer is obtained for providing the attention weights. If the transformer needs to be pre-trained before the graph contrastive self-supervised pre-training, then the whole process would involve 2 large-scale pre-training stages, one for obtaining the transformer, and another one for the actual graph pre-training. If that is the case, the proposed approach requires significantly more computational cost than the compared baseline methods.\n- As mentioned in the limitation section, the current method has been tested on graph-level classification tasks only. However, node classification or edge classification offer more flexibility for practical use cases, for example social network connectivity identification or fraud detection for anti-money laundering. It is important to have signals on those tasks before we can faithfully validate the proposed method works effectively and consistently.\n- The writing is good in the manuscript. However I would appreciate the authors using terms that are simpler and easier to understand. For example, the authors use the term \u201crational finder\u201d but in fact it is simply using the attention weight for getting the importance values. That way, junior readers can have a better understanding of the proposed method quicker.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The proposed method is new but still requires more validation whether the whole pipeline requires much higher computational cost during pre-training.\n- The clarity can be improved, specifically for the term selection as mentioned above.\n",
            "summary_of_the_review": "- The paper can be largely improved by providing node-level and edge-level classification results on more benchmarks. Also would be great to clarify whether the proposed pipeline requires 2 stages of pre-training.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper175/Reviewer_YpSx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper175/Reviewer_YpSx"
        ]
    },
    {
        "id": "tcwiF4ygthp",
        "original": null,
        "number": 2,
        "cdate": 1666536159614,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666536159614,
        "tmdate": 1666536159614,
        "tddate": null,
        "forum": "CdU7ApBxICO",
        "replyto": "CdU7ApBxICO",
        "invitation": "ICLR.cc/2023/Conference/Paper175/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a simple and effective  Self-attentive Rationale guided Graph Contrastive Learning  for graph network learning. The code idea is to learn both node- and edge-wise rationale-aware views. Extensive experimental results show the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "Strength:\n1. The writing is good.  It is easy to understand the main idea.\n2.  The experimental analysis is sufficient and in-depth.\n\nWeakness:\n1. More theoretical analysis about the effectiveness of  SR-GCL should be clarified.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, quality, novelty and reproducibility are good. ",
            "summary_of_the_review": "The proposed solution for graph contrastive learning is simple and clear. The experimental results are sufficient. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper175/Reviewer_KsiL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper175/Reviewer_KsiL"
        ]
    },
    {
        "id": "Y97ZNpwEYRG",
        "original": null,
        "number": 3,
        "cdate": 1666615632433,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666615632433,
        "tmdate": 1666615632433,
        "tddate": null,
        "forum": "CdU7ApBxICO",
        "replyto": "CdU7ApBxICO",
        "invitation": "ICLR.cc/2023/Conference/Paper175/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to mitigate two limitations in the existing graph contrastive learning (GCL) frameworks: i) The lack of view diversity in data augmentation approaches degenerates the effectiveness of self-discriminative contrastive learning. ii) Existing methods utilize separated automated view generator and encoder, which damage the efficiency and efficacy of model training for GCL. To address the limitations, the proposed model employs a self-attention module to generate attention map for view generation and encoding simultaneously, to avoid introducing additional view generator.",
            "strength_and_weaknesses": "Strengths:\nThe proposed model utilizes the attention map in self-attention for automatic data augmentation, which improves the model efficiency and potentially improves the model training to be more stable and less prone to overfitting.\nDiverse types of views are generated using a shared view generator in the proposed model, which not only improves the efficiency but also enhance the data augmentation in GCL.\nThe proposed model is evaluated from different dimensions, including illustrative case study on the explainability of the self-attention module.\n\nWeaknesses:\nThe proposed model does not achieve best performance compared to baselines in some circumstances in Table 1.\nThe experiments are only conducted on biochemistry datasets, which cannot show the generality of the proposed model. More datasets such as academic citation networks, social networks and recommendation graphs could be utilized.\nThe authors claim that combining the view generator and the encoder is beneficial for model efficiency, but no experiments targets this point.\n",
            "clarity,_quality,_novelty_and_reproducibility": "While this work proposes to use attention weights for graph augmentation, the evaluation of combing view generator and the embedding encoder is missing. Furthermore, only one type of dataset is adopted for evaluation. Normally graph contrastive learning methods are evaluated over different types of datasets for more comprehensive experiments.",
            "summary_of_the_review": "Considering many recent efforts on graph contrastive learning, this work proposes to leverage self-attention to infer node correlations for graph augmentation. However, the insufficient experiments may not be able to demonstrate the effectiveness of the new introduced approach.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper175/Reviewer_K54f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper175/Reviewer_K54f"
        ]
    },
    {
        "id": "kQVwRVzJI-",
        "original": null,
        "number": 4,
        "cdate": 1666865410458,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666865410458,
        "tmdate": 1669006742658,
        "tddate": null,
        "forum": "CdU7ApBxICO",
        "replyto": "CdU7ApBxICO",
        "invitation": "ICLR.cc/2023/Conference/Paper175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to use a self-attention mechanism to learn node-level and edge-level importance scores for a graph, and then conduct adaptive sampling considering the node/edge-level scores for GCL. The results have been conducted on 8 molecule datasets.",
            "strength_and_weaknesses": "S1. The paper is easy to follow and the idea is clear.\n\nS2. The paper uses case studies to visualize the attention scores to verify the rationality of the learned scores.\n\nW1. The experiments are only conducted on molecule graphs. How about other types of graphs that are widely used in GCL papers (e.g., social networks, Reddit, etc.)? I have personally tested the code for social network datasets without pre-training, and the results show no obvious improvement from baselines such as RGCL. If my test is true, then the practicality of the method is limited, as it needs a large dataset for pre-training, which may not be easy for the graphs except molecule graphs.\n\nW2. Runtime efficiency is not analyzed and experimentally tested",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the presentation of the method is clear, but the running efficiency is unclear.\n\nNovelty: to my knowledge, using self-attention to learn node/edge importance scores for adaptive GCL augmentation is novel.",
            "summary_of_the_review": "While the method looks good and sensible, whether it can be generalized for other types of graphs (besides molecule) and the running efficiency is not clarified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper175/Reviewer_oC1b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper175/Reviewer_oC1b"
        ]
    },
    {
        "id": "lfg5c6crQNM",
        "original": null,
        "number": 5,
        "cdate": 1667004085737,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667004085737,
        "tmdate": 1667004085737,
        "tddate": null,
        "forum": "CdU7ApBxICO",
        "replyto": "CdU7ApBxICO",
        "invitation": "ICLR.cc/2023/Conference/Paper175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a self-attentive rationalization for rationale-aware agmented pairs, which are used in contrastive learning. The efficiency of self-attentive rationalisation is validated by visualisation on real-world biochemistry datasets, and the performance on downstream tasks shows the good performance of SR-GCL for graph model pre-training.",
            "strength_and_weaknesses": "Strength\n1. A rationale finder finder is proposed with plural views instead of either node- or edge-wise rationales finder independently.\n2. In this paper, based on the transformer framework, they propose self-attentive for dual views to guide the rationale finder.\n\nWeakness:\n1. I do not think of \u201cno single transformation suffices to learn good representations\u2018\u2019. There are many augmentation-free (in raw attributes and the graph) methods working competitively in different benchmarks. I'm curious if augmentation-free is also applicable in specific data.\n2 . In my opinion, the contribution is somewhat limited. It looks like the biggest contribution is a self-attentive augmentation. From a GCL point of view, the contribution seems insufficient. Can this technique be used in the other graphs and tasks? For example node classification and social network.\n3. I'm not sure I understand correctly that sampling makes this model not an end2end optimization. Have you consider using the reparameterization trick in VAE to solve this? or it is unreasonable or useless.",
            "clarity,_quality,_novelty_and_reproducibility": "In my opinion, I cannot follow this paper well. I'm not sure if it's because it's specific tasks on graph learning. Although I am an expert in graph learning, I cannot understand this paper well in motivation and some claims. The originality is ok.",
            "summary_of_the_review": "Based on my comments, I am inclined to give a rejection right now. But I am happy to refer to other reviewers' comments and the author's response before making a decision. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper175/Reviewer_PbQf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper175/Reviewer_PbQf"
        ]
    }
]