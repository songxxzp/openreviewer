[
    {
        "id": "WCtjD2-TJDy",
        "original": null,
        "number": 1,
        "cdate": 1666480238711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666480238711,
        "tmdate": 1668827993122,
        "tddate": null,
        "forum": "UVKwsWsXTt",
        "replyto": "UVKwsWsXTt",
        "invitation": "ICLR.cc/2023/Conference/Paper515/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a progressive distillation strategy for object detection and instance segmentation models, i.e., multiple teacher models transfer knowledge to a student model in a sequential manner. A heuristic method is also given to choose the order of teacher models and its efficacy is demonstrated by experiments. A method for distilling knowledge from transformer-based teachers to convolutional-based students is simply introduced. Many experiments are done to verify the capability of the proposed strategy and ablation studies are provided.",
            "strength_and_weaknesses": "Strengths:\n1. The sequential strategy is inspiring and of interest for knowledge distillation for object detection and segmentation.\n2. The performance of the proposed method is great, as shown in the results.\n3. Comparison between the sequential way and simultaneous way is of interest. \n\nWeakness:\n1. Compared to other studies in the paper, the introduction to the distillation from the transformer-based model to the convolutional-based model is simple. For example, it seems that the transformer-based model is not combined with other models as teacher models. It would be better to provide a possible way or give some discussions in the paper.\n2. How to address a large k, i.e., the maximum number of selected teachers, or how to pick a good k is not given or well-discussed. In practice, it may happen when users have a large pool of teachers.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality:\nIn general, the clarity and quality of this paper are good.\n\nNovelty:\nTo the best knowledge, the paper provides some novelty ideas for this area.\n",
            "summary_of_the_review": "The reviewer generally thinks the paper is written in sound quality as specified by the strengths above. It proposes a sequential manner for knowledge distillation. However, it would be recommended that authors address the weakness mentioned above and also the following comments:\n\n1. In table 7, for length 4, there is no bold sequence marked. Is this a result of algorithm 1? Please add some explanations.\n2. Maybe give a concrete example of $r$ in (1) for two concrete models in a heterogeneous case. It also would be great to see how the transformer-based model is addressed here.\n\n-------------------\nUpdate: I believe the authors well-addressed my comments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper515/Reviewer_zfUg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper515/Reviewer_zfUg"
        ]
    },
    {
        "id": "f7d1YyXKUQ",
        "original": null,
        "number": 2,
        "cdate": 1666703769741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703769741,
        "tmdate": 1670225480091,
        "tddate": null,
        "forum": "UVKwsWsXTt",
        "replyto": "UVKwsWsXTt",
        "invitation": "ICLR.cc/2023/Conference/Paper515/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes the progressive knowledge distillation method for object detection by distilling knowledge from multiple teachers. The main contribution of this work is designing a heuristic algorithm based on the correlation of feature representations to generate the proper sequence of several teachers given a student model. Comprehensive experiments and ablation studies on the COCO dataset evidence the effectiveness of the proposed method.",
            "strength_and_weaknesses": "**Strengths**\n+ Novel idea. The authors propose a heuristic algorithm to produce a near-optimal sequence of multiple teachers, even not the best, still outperforms multiple KD approaches distilled from a single teacher.\n+ Promising results and convincing ablation studies with multiple settings for object detection.\n\n**Weaknesses**\n\nHowever, there are still some concerns to be addressed:\n\n- Even though the student can get better results with the selected order of the teachers by Algorithm 1, sometimes it is not the best, especially with more teachers. Could the author please provide some potential explanation on this? In other world,  how reliable the proposed Algorithm 1 is?\n\n- As stated in Section 4.1, the student is initialized by an off-the-shell student for the proposed KD method. Does the student use the same strategy of initialization for exp 3 and 7 (Directly distilled by Teacher IV) and blue bars in Figure 4? Moreover, as for Figure 4, it is not quite fair to compare the performance of 2x training with 2 1x training. Please clarify it.\n\n- The capacity gap between the student and the best performance teacher may a problem for knowledge distillation. Maybe the intermediate teacher is better to be distilled for the student. Thus, it will be interesting to compare with the distilled results from the intermediate teachers in Tables 3, 4 and 5, and Figure 4.\n\n**Minor comments**\n\nIt is worth discussing a related work [A], which also makes use of several teachers. It is interesting to the research line of KD in object detection.\n\n**Reference**\n\n[A] Guo. et al. Distilling Image Classifiers in Object Detectors. NeurIPS2021\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality: Good. The paper is well-written and easy to follow.  \n\nNovelty: Good. The proposed method improves the KD performance in object detection by using the capacity of different teacher models sequentially.\n\nReproducibility: Good. The supplementary code is provided. ",
            "summary_of_the_review": "The paper is in good shape with extensive experiments, which show promising results. However, there are still some concerns. Please clarify them. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper515/Reviewer_R4rk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper515/Reviewer_R4rk"
        ]
    },
    {
        "id": "khUtoAnCJ8",
        "original": null,
        "number": 3,
        "cdate": 1666953770731,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666953770731,
        "tmdate": 1666953867136,
        "tddate": null,
        "forum": "UVKwsWsXTt",
        "replyto": "UVKwsWsXTt",
        "invitation": "ICLR.cc/2023/Conference/Paper515/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a simple progressive knowledge distillation framework with a sequence of teachers for detectors. ",
            "strength_and_weaknesses": "\nStrength:\n1. The idea of progressively transferring knowledge from a sequence of teachers to a lightweight detector is somewhat novel. \n2. It represents the first effort to distill knowledge from Transformer-based teacher detectors to convolution-based students.\n3. It shows the performance gain comes from better generalization rather than better optimization. \n\nWeaknesses:\n1. Why not compare with other general[4,5,6] / multi-teacher[9,10] / progressive[1-3,7,8] KD methods? \n2. Please clarify the difference between the proposed method and online KD[] algorithms. And it's better to give more experiments and analysis to highlight the strength of your idea. I think that the principle of sequence teacher is similar to online distillation. \n3. The ranking of teachers seems to require empirical design. It's better to give more theoretical analysis. \n4. Another serious concern is that the pre-training of multiple teachers can be cumbersome and time-consuming, making the pipeline complicated and unpractical. \n5. Since the author propose to transfer knowledge between Transformer and CNN, why not adopt Transformer-based student detectors?\n6. There is lack of some important baselines [11-16].\n\n## References\n\n[1] Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher. Mirzadeh et al. arXiv:1902.03393\n\n[2] Snapshot Distillation: Teacher-Student Optimization in One Generation. Yang, Chenglin et al. CVPR 2019\n\n[3] Online Knowledge Distillation by Temporal-Spatial Boosting. Li, Chengcheng et al. WACV 2022\n\n[4] Fitnets: Hints for thin deep nets. Romero, Adriana et al. arXiv:1412.6550\n\n[5] Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Zagoruyko et al. ICLR 2017\n\n[6] Variational Information Distillation for Knowledge Transfer. Ahn, Sungsoo et al. CVPR 2019\n\n[7] Online Knowledge Distillation via Collaborative Learning. Guo, Qiushan et al. CVPR 2020\n\n[8] Knowledge Transfer via Dense Cross-layer Mutual-distillation. ECCV 2020\n\n[9] Learning from Multiple Teacher Networks. You, Shan et al. KDD 2017\n\n[10] Knowledge distillation by on-the-fly native ensemble. Lan, Xu et al. NeurIPS 2018\n\n[11] Learning efficient object detection models with knowledge distillation. Chen, Guobin et al. NeurIPS 2017\n\n[12] Distilling Object Detectors with Fine-grained Feature Imitation. Wang, Tao et al. CVPR 2019\n\n[13] Enabling Incremental Knowledge Transfer for Object Detection at the Edge. arXiv:2004.05746\n\n[14] General Instance Distillation for Object Detection. Dai, Xing et al. CVPR 2021\n\n[15] Distilling Image Classifiers in Object Detectors. Guo, Shuxuan et al. NeurIPS 2021\n\n[16] Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors. ICLR 2021\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good.\nQuality: The organization and architecture of the paper is not very clear, especially the experiment part.\nNovelty: somewhat, but a little incremental. The expression can be improved.\nReproducibility: I think it is easy to follow. But the code in appendix is not complete and not conducted by the common MMDetection framework like most works, e.g. FGD, MGD. \n",
            "summary_of_the_review": "The work is somewhat simple yet effective, my major concern is the novelty and experiments. \nIf the authors can make the experimental part simple and clear, and highlight the competitiveness of the results with more SOTAs, I will consider improve the score. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper515/Reviewer_1tUC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper515/Reviewer_1tUC"
        ]
    },
    {
        "id": "JFlK8QzC1pF",
        "original": null,
        "number": 4,
        "cdate": 1667399844378,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667399844378,
        "tmdate": 1668416341833,
        "tddate": null,
        "forum": "UVKwsWsXTt",
        "replyto": "UVKwsWsXTt",
        "invitation": "ICLR.cc/2023/Conference/Paper515/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a novel knowledge distillation methods which decide the progressive distillation process by the cost of linear regression on validation set, which achieves good performance on multiple student-teacher settings.",
            "strength_and_weaknesses": "Strength: \n1. This paper proposes to choose the policy of student-teacher in knowledge distillation by using the cost of linear regression on the validation set, which is novel in this domain,\n2. Good experimental results have been achieved.\n3. Sufficient ablation studies have been conducted to demonstrate their performance.\n\nWeakness.\n1. Most of the experiments are conducted on RetinaNet and Mask RCNN with different backbones. It will be better if results on more SOTA detection models can be provided, such as Deformable Detr, Yolov4, CenterNet and so on.\n2. This paper applies the naive feature-based knowledge distillation as their knowledge distillation method. It is ok since this paper aims to focus on the order of teachers instead of the specific KD method. But It will be better if better more SOTA KD methods can be utilized to evaluate the effectiveness of the proposed method.",
            "clarity,_quality,_novelty_and_reproducibility": "Good Clarity, Good Quality, Good Novelty, Poor Reproducibility. No codes are provided.",
            "summary_of_the_review": "Please refer to the strength and weaknesses. In summary, I like the core idea of this paper, while some more experiments are still necessary.\n\nUpdate-2022-11-14\nThe response from the authors basically addressed my concern. So I increase my score to 8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper515/Reviewer_XUDr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper515/Reviewer_XUDr"
        ]
    },
    {
        "id": "DhSxs9BlD1",
        "original": null,
        "number": 5,
        "cdate": 1668606276833,
        "mdate": null,
        "ddate": null,
        "tcdate": 1668606276833,
        "tmdate": 1668606447386,
        "tddate": null,
        "forum": "UVKwsWsXTt",
        "replyto": "UVKwsWsXTt",
        "invitation": "ICLR.cc/2023/Conference/Paper515/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper provides a simple yet effective sequential approach to distill knowledge from Transformer-based teacher detectors to convolution-based student detectors. Instead of learning from a single teacher, the proposed framework develops a principled way to automatically design a sequence of teachers appropriate for the student and progressively distill it. Notably, the authors claimed that they were the first to investigate distillation from Transformer-based teacher detectors to a convolution-based student. Extensive experiments and ablation study is given to show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "(1) This paper is well-written and organized.\n\n(2) The idea of learning lightweight object detectors via progressive knowledge distillation and investigating heterogeneous knowledge distillation is interesting.\n\n(3) The experimental results significantly outperform the existing SOTA knowledge distillation methods.\n\n\nWeakness:\n\n(1) The authors failed to prove that the observation \u2018teacher-student capacity gap can be solved by the sequential distillation from multiple teachers arranged into a curriculum\u2019, which is claimed in the Introduction.\n\n(2) Why the heterogeneous knowledge distillation should be performed from the Transformer-based teachers to convolution-based students? Why not from the convolution-based teachers to Transformer-based students?\n\n(3) Is the multiple teachers necessary? How about utilizing a single-teacher network with multiple forward progress? This operation also gives more supervision to the selected student and meanwhile saves the effort of searching for the near-optimal teacher order. Authors need to show the rationality of multiple teachers rather than multiple forward passes. \n\n(4) More SOTA and widely-adopted detection models should be involved, eg., YoloV4.\n\n(5) This paper repeatedly mentioned about \"proper\" either in selecting the teacher network or 'arranging teacher sequence'. Indeed, this reflected that this paper proposed a method that is not adaptable enough but based on \"proper\" (manual) selection and usage to \nachieve good performance. It is actually not easy to be extended by the followers if published.\n\n(6) As mentioned, selecting multiple teachers is actually not very easy in reality, especially for Transformers, which can not be easily trained. But it is more approachable to transfer from CNNs to the transformer as CNNs are already matured.  Therefore, the rationality of this method is less clear.\n\n(7) I do agree that teacher selection is one important aspect, however, currently, the method seems to lack enough novelty and complexity to meet the acceptance bar of ICLR.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper addresses somehow an interesting problem of teacher selection. However, it does don't make enough sense of why selecting multiple transformer models and distill them to learn a CNN student. The selection strategy of teacher models lacks flexibility as it should be based on a \"proper\" selection and measure for successful distillation. From this, we can see that the method itself is not very intuitive and persuasive. What if bad teachers can still be used for learning a good student? References, such as [1,2,3], can be good examples.\n\n[1] Knowledge Distillation: Bad Models Can Be Good Role Models\" \n[2] Student Customized Knowledge Distillation: Bridging the Gap Between Student and Teacher\n[3] Distilling the Undistillable: Learning from a Nasty Teacher",
            "summary_of_the_review": "This paper is somewhat interesting but lacks rationality of clear motivation, technical novelty, and critical ablation studies. Details can be referred to the strength and weakness",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper515/Reviewer_df3Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper515/Reviewer_df3Q"
        ]
    }
]