[
    {
        "id": "7_RcyBbQg8C",
        "original": null,
        "number": 1,
        "cdate": 1666587700307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666587700307,
        "tmdate": 1666587766272,
        "tddate": null,
        "forum": "0bLE93R9d0O",
        "replyto": "0bLE93R9d0O",
        "invitation": "ICLR.cc/2023/Conference/Paper2120/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the problem of systematic generalization in VQA. It proposes a new model that takes advantage of the learning capability of Transformers and the compositional modeling of Module Networks. The proposed model achieves state-of-the-art performance on three different VQA datasets including standard testbeds for VQA systematic generalization. ",
            "strength_and_weaknesses": "\u2022\tStrengths:\n\n1.\tThe proposed method is technically sound and easy to understand. It is reasonable to have a model that makes use of the known advantages of Transformers and module networks.\n\n2.\tIt shows better performance when compared against existing methods.\n\n\u2022\tWeaknesses:\n\n1.\tWhile the proposed model is straightforward and easy to understand, it appears that the model brings very little insight into the systematic generalization task itself. It is well studied that module networks are a potential approach for systematic generalization but it is extremely brittle and ad-hoc which hinders its application in wider settings. Meanwhile, models with better representation learning such as Transformers are beneficial for all machine learning tasks, not just for systematic generalization. The proposed method does not offer any fundamental contributions toward solving systematic generalization.\n\n2.\tExperiments with original transformers do not sound reasonable as the original design of transformer is for translation tasks with having access to a more generous amount of training data. There is no reason to have to use 12 layers for transformers when applying it to a custom problem. The poor performance of transformers in the experiments might simply be due to overfitting.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is technically sound, with sufficient clarity. The details provided are fully explained and can be reproducible. However, it appears to be limited in scientific contribution.",
            "summary_of_the_review": "The paper is well written and easy to follow. However, it is limited in contribution to advance the systematic generalization task.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2120/Reviewer_zXs6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2120/Reviewer_zXs6"
        ]
    },
    {
        "id": "lZlpQFTggog",
        "original": null,
        "number": 2,
        "cdate": 1666614902179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614902179,
        "tmdate": 1666614902179,
        "tddate": null,
        "forum": "0bLE93R9d0O",
        "replyto": "0bLE93R9d0O",
        "invitation": "ICLR.cc/2023/Conference/Paper2120/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on the systematic generalization visual question answering where the test set contains novel combinations of training concepts. The authors propose a new modular network whose modules are transformers.",
            "strength_and_weaknesses": "Strength:\n\nThe paper focuses on the model robustness against VQA bias, which is a common problem in the VQA field.\n\nThe paper proposed a new systematic generalization dataset GQA-SGL based on the GQA. The new dataset can be a new challenge for future work in this field.\n\nWeakness:\n\nThe novelty is limited. The proposed model is a modular network composed of more expressive transformer modules. \n\nThe proposed model also suffers from the weakness of the previous modular network. It needs program supervision and relies on the performance of the question parser. Since all the experiments have a groundtruth program at test time, whether the question parser has the same systematic generalization ability is not verified.\n\nThe improvement on the CLEVR-CoGenT and CLOSURE dataset is not consistent. The proposed model performs worse than the baseline on the CLEVR-CoGenT but improves on the CLOSURE. I suppose the CLEVR-CoGenT and CLOSURE have novel images and questions respectively. However, the TMN can ignore the questions by accessing the groundtruth programs.\nThe authors suggest that the proposed model and baseline share the same visual feature extractor. Thus authors may verify this by training a new extractor from scratch similar to FiLM[1].\n\n[1] Perez E., Strub F., de Vries H., Dumoulin V., Courville, A. FiLM: Visual Reasoning with a General Conditioning Layer. AAAI 2017",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-organized and easy to understand.\n\nThe novelty is not enough. The proposed model only replaces the previous neural module with a transformer.",
            "summary_of_the_review": "The paper proposes a new dataset. But the proposed model is not novel and its effectiveness is not fully verified. Thus I give borderline.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2120/Reviewer_pauq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2120/Reviewer_pauq"
        ]
    },
    {
        "id": "H0PM1r2Jiy4",
        "original": null,
        "number": 3,
        "cdate": 1666632964492,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632964492,
        "tmdate": 1666632964492,
        "tddate": null,
        "forum": "0bLE93R9d0O",
        "replyto": "0bLE93R9d0O",
        "invitation": "ICLR.cc/2023/Conference/Paper2120/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper mainly focuses on combining the strengths of Transformer and NMN by introducing a novel NMN based on compositions of Transformer modules named Transformer Module Network (TMN). The model is evaluated on CLEVR-CoGenT, CLOSURE, and GQA-SGL.",
            "strength_and_weaknesses": "*[Strength]*\n\nThe motivation that combining the strength of NMN and Transformer is contributing.\n\n*[Weakness]*\n1. In the approach, the paper directly substitutes the module in the NMN with the TRM encoder. However, it lacks ablation experiments and discussion about if the TRM structure outperforms all types of module in NMN-based method. (e.g., in n2mn[1] Table 1, the attention based modules like find, relocate, filter modules and the answer based modules like describe, compare and exist modules)\n2. The Tree modular modular is not novel in NMN domain, [1] uses similar manner in Figure 3.\n3. Lacks of SOA TRM-based and NMN-based model performance comparisons. In addition, the SOA method on GQA dataset is 72.1 from [2], and there is [3] with 69.46 on GQA-dev\n\n\n\n\n[1] Hu, Ronghang, et al. \"Learning to reason: End-to-end module networks for visual question answering.\" Pr oceedings of the IEEE international conference on computer vision. 2017.\n\n[2] Nguyen, Binh X., et al. \"Coarse-to-Fine Reasoning for Visual Question Answering.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[3] Kim, Eun-Sol, et al. \"Hypergraph attention networks for multimodal learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper clarified its contributions and approach clearly. \nIn general, the paper can be reproduced and is of fair quality.\nHowever, it lacks novelty since the tree structure and substituting the modular with the TRM encoder are not novel.",
            "summary_of_the_review": "The motivation that combining the strengths of the two structures is good. However, directly substituting the modular with TRM seems not novel to me. Besides, there are no experiments to prove that the TRM encoder outperforms all previous NMN modular in the task, thus, it does not convince me of its superiority by directly substituting all NMN modulars. In addition, it lacks comparisons of the SOA methods. And from my knowledge, the performance is not SOA (the last point in the weakness section). Overall, I'm leaning to reject it.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2120/Reviewer_o4YA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2120/Reviewer_o4YA"
        ]
    },
    {
        "id": "H05O1FWIWAv",
        "original": null,
        "number": 4,
        "cdate": 1666682957925,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682957925,
        "tmdate": 1671894528762,
        "tddate": null,
        "forum": "0bLE93R9d0O",
        "replyto": "0bLE93R9d0O",
        "invitation": "ICLR.cc/2023/Conference/Paper2120/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors reveal that Neural Module Networks (NMNs), i.e., question-specific compositions of modules that tackle a sub-task, achieve better or similar systematic generalization performance than the conventional Transformers, even though NMNs\u2019 modules are CNN-based. \nTo address this shortcoming of Transformers with respect to NMNs, in this paper, the authors investigate whether and how modularity can bring benefits to Transformers.\nNamely, they introduce a Transformer Module Network (TMN), a novel NMN based on compositions of Transformer modules.\n",
            "strength_and_weaknesses": "Strength\n- The observation about the comparison between the neural module networks and the Transformer models is novel and interesting.\n- The authors provide a comprehensive ablation study to make the final model more solid.\n\n\nWeakness\n- It would be better to add qualitative results to see the behavior of the proposed method and the existing methods. Also, it would be better to discuss why the conventional Transformer models perform worse than the neural module networks by analyzing the samples.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is clear and novel. The quality of this paper looks favorable overall.",
            "summary_of_the_review": "The authors provide interesting observations, and the experiments are solid enough to show the effectiveness of the proposed method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2120/Reviewer_W6Kf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2120/Reviewer_W6Kf"
        ]
    }
]