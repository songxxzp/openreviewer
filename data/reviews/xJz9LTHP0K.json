[
    {
        "id": "D1VayUprLuk",
        "original": null,
        "number": 1,
        "cdate": 1666631011566,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631011566,
        "tmdate": 1670365224053,
        "tddate": null,
        "forum": "xJz9LTHP0K",
        "replyto": "xJz9LTHP0K",
        "invitation": "ICLR.cc/2023/Conference/Paper3944/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper first characterize the deviations between the teacher and the student; then provide theoretical perspectives on how distillation induces such deviations. Specifically, they first show that (1) the student under-fits on hard samples and (2) a large deviation in the early stage can be recovered by distillation introduced in the later stage. Then they provide an explanation for (1) by viewing distillation as an eigenspace sparsifier in GD-based linear model and an understanding for (2) by viewing distillation as a gradient denoiser.",
            "strength_and_weaknesses": "Strength:\n\n1. The authors make connections between the theoretical understanding and empirical observations in distillation, and provide an deeper and unified perspective of regularization and denoising.\n2. The empirical observations are extensively conducted on various datasets and models.\n\nWeakness:\n\n1. Explaining under-fitting via eigenspace sparsification is insufficient. The empirical observations are based on non-linear models while the theorem holds for linear models. The connection between the hard samples and lower eigendirections is also unclear.\n2. The theorem for the gradient denoising view is loosely connected with the loss-switching experiments. It might be more concrete to show one-hot update suppresses the similar but non-target logits, and how that produces destructive gradients and hurt generalization.\n3. The proposed idea provides some insights but is not totally novel. It mainly generalizes existing views in wider settings, e.g., GD-optimization case and clean label case.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is clearly written. \n2. The proposed idea provides some insights but is not totally novel. It mainly generalizes existing views in wider settings, e.g., GD-optimization case and clean label case.\n3. The empirical observations are extensive and should be easy to reproduce; the theoretical results seem correct. ",
            "summary_of_the_review": "This paper provides an deeper and unified view of eigenspace sparsification and gradient denoising for distillation. The empirical results are extensive. However, the theoretical results are limited to linear models; Some claims are lack of evidence and are loosely connected with the empirical results (See weakness). ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3944/Reviewer_M824"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3944/Reviewer_M824"
        ]
    },
    {
        "id": "vLU281h1QJ",
        "original": null,
        "number": 2,
        "cdate": 1666646247350,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646247350,
        "tmdate": 1666646247350,
        "tddate": null,
        "forum": "xJz9LTHP0K",
        "replyto": "xJz9LTHP0K",
        "invitation": "ICLR.cc/2023/Conference/Paper3944/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies knowledge distillation by investigating the points where the student deviates from the teacher's predictions. The authors suggest that the success of knowledge distillation is because student networks underfit ``hard'' points. The authors provide empirical evidence by comparing student logits with that of the teacher for a variety of settings. They also show a very interesting phenomenon -- switching losses from standard cross-entropy loss to a distillation based loss recovers the same properties that a model trained with distillation alone. The paper also provides theoretical intuition for this by extending XTK style arguments to show that KD acts as a regularizer. Additionally, the authors also provide a gradient space view suggesting that in the linear setting, distillation loss gradients \"denoises\" the effect of negative examples with similar features. ",
            "strength_and_weaknesses": "**Strengths**\n1. The paper provides an interesting and novel approach to analyze the deviations between teacher and student logits. While others (Mohabi et al., Yuan et al.) have shown regularization effects due to distillation, the experiments here help us understand the form that the regularization takes in terms of the predicted logits.\n2. The results on switching between KD and standard training also provide usable insights into when KD gradients help and may result in better KD scheduling and training algorithms.\n3. The experiments analyse a variety of settings for both KD, and self distillation. \n4. The discussion of the related work is comprehensive and the paper clearly situates itself among existing literature.\n\n**Weaknesses/Questions**\n\n1. It might be useful to include the test and training accuracies (or differentiate between correct/incorrect examples) in all the figures. \n2. Does the underfitting depend on how long the student is trained? I am curious to see if the same holds if the student is trained longer than the teacher.\n3. Fig. 12 actually shows an interesting insight in qualifying which points the two classifiers get wrong. In fact, given that the \"harder\" points tend to be generally misclassified, the argument that the underfitting leads to better generalization may be flawed. If the authors could provide some numerical values for the fraction of underfit points in every case, we might be able to differentiate between the effect of distillation v/s that of optimization error. Thm. B.3 also appears to point to early stopping of the student being an important criterion.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written. Some figures can be improved with better labels and auxiliary information to help the reader. In terms of novelty, the paper presents an interesting instance-level analysis of teacher v/s student logits as well as theoretical arguments to explain the intuition behind these observations. The authors also share the hyperparameters used to reproduce the results, as well as detailed proofs for the theorems.",
            "summary_of_the_review": "The paper explains KD as a regularizer that helps student networks underfit \"harder\" points and therefore make different mistakes from the teacher. The experiments are insightful and reveal previously unstudied behavior in student networks. While there may still be some confounding factors due to early stopping, and certain inconsistencies for language models, the paper still adds value in terms of understanding where distillation may be improved. The authors have conducted in-depth experiments to support their arguments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3944/Reviewer_njgH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3944/Reviewer_njgH"
        ]
    },
    {
        "id": "Cpc-A0VtBcj",
        "original": null,
        "number": 3,
        "cdate": 1666684167955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684167955,
        "tmdate": 1670227715819,
        "tddate": null,
        "forum": "xJz9LTHP0K",
        "replyto": "xJz9LTHP0K",
        "invitation": "ICLR.cc/2023/Conference/Paper3944/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors first observe that in most cases, the student underfits points that the teacher finds hard by experiments across image and language classification with self- and cross-network-distillation settings. Then they analyze how the loss switching affects. Afterward, the student-teacher deviations are formalized in both eigenspace and gradient-space views. They further are used to help understand the knowledge distillation methods from different views. Overall, the contributions of this paper bridge the theory and practice in the field of knowledge distillation. \n",
            "strength_and_weaknesses": "**Strengths**\n+ The paper is well-written and easy to follow.\n+ The observations are drawn from comprehensive experiments with multiple architectures and datasets.\n+ The experiments on and results of loss switching are interesting.\n\n\n**Weaknesses**\n\n- It would be more helpful if the author could conclude some take-home messages from their work and give a clear answer to the question in the title. It will help readers to understand the paper better and inspire further research on the topic of knowledge distillation.\n\n- As shown in Figure 4, it looks like KD from the intermediate steps helps train the student better. Is it possible to get some practical suggestions from these experiments?\n\n- This paper works on the distillation of the predicted logits/probabilities, however, the feature distillation has drawn more attention recently. To some degree, it should also affect the prediction of the logits/probabilities. It would be more interesting to include some analysis of the feature distillation methods.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality: Good. The paper is well-written with clear explanations about each part.\n\nNovelty: Good. The view of the paper is novel while the methods used for analysis are not.\n\nReproducibility: Fair. The supplementary code is not provided.\n",
            "summary_of_the_review": "The paper shows comprehensive experimental and theoretical analysis. It also sheds light on potential research in the future on knowledge distillation. It would be better to improve the manuscript according to the above weaknesses.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3944/Reviewer_EHbp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3944/Reviewer_EHbp"
        ]
    },
    {
        "id": "MM3FLOvmIV",
        "original": null,
        "number": 4,
        "cdate": 1666688085607,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688085607,
        "tmdate": 1670642428046,
        "tddate": null,
        "forum": "xJz9LTHP0K",
        "replyto": "xJz9LTHP0K",
        "invitation": "ICLR.cc/2023/Conference/Paper3944/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis work characterizes the nature of deviations between teacher and student network during knowledge distillation (self as well as cross architectures). \n\nIn the first part, this work studies these properties empirically and notes that the following two observations:\n - Student underfits points which teacher finds hard\n - Initial training phase is not crucial for distillation benefits. One can switch from 0-1 labels to teacher's soft predictions somewhere in the middle or later parts of the training and achieve similar results as training with teacher predictions from the beginning.\n\nIn the second part, it develops following two theoretical viewpoints to explain this observed behavior:\n- Distillation as regularizer in the eigenspace: It analyzes a continuous-flow gradient descent model on linear regression with early-stopping. Thm4.1 says that student relies less on the bottom eigendirections than the teacher under the condition that both models have converged well along the top eigendirections.\n\n\n- Distillation as denoiser of gradients : It constructs a K-class classification problem with each input being K channel and each channel drawn from a multivariate Gaussian with co-variance being a scaled identity matrix. It considers a linear architecture on top of this construction. Thm.4.2 shows that distillation's gradients are more optimal than one-hot gradients and also more optimal than the teacher's weights themselves.\n\n",
            "strength_and_weaknesses": "Strengths:\n- Problem being studied is of importance to the distillation community.\n- Empirical observations and lots of analytical experiments could help understand the distillation mysteries.\n\nWeaknesses:\n- Theoretical analysis seems to be done on very narrow setup, for ex.,\n   - Thm4.2 studies a linear classifier on top of a specially constructed K-class problem. It is unclear how their observations extend beyond this setup. Even in loss switching there are many instances where switching to one-hot loss from KD does not seem to hurt the performance. So their point about One-Hot getting stuck at the optimal solution does not seem to apply here. \n   - Thm4.1 talks about KD enabling the student to learn the \"nice\" directions first. It is not clear why this is something special to the KD. Some recent works have shown that SGD in general learns functions of increasing complexities (see https://arxiv.org/pdf/1905.11604.pdf )\n\n\n- Paper writing makes it very hard to digest the crux of this work. There does not seem to be any coherency in the empirical observations being presented. \n\n\n\n\nQuestions for Authors:\n\n(1) How is the teacher network trained in the self-distillation setup?\n\n(2) In Fig.1 and 2, \n - How do you define hard points? What is the measure of hardness used to classify a data point being easy or hard for a model? In CIFAR-100 Resnet56-56, it seems half the points lie above the Y=X line and half of them lie below this line. \n - What are the accuracies of the teacher and student model?\n - For the train data, do you use data augmentations? If so, do you see similar behavior on the augmented data?\n - In which of these instances, the teacher or the student fit the train data without any error?\n\n(3) I don't understand the conclusions from NoisyCIFAR100 experiments. \n - Teacher is trained with noisy labels\n - Student behaves similar whether the teacher had clean or noisy labels (Fig1a -- first and last plot, Fig2 -- second and last plot)\n - Student just underfits the teacher softened probabilities -- this has been the case so far in all the cases \n - In the noisy case, the teacher had more scores (X values) that are small -- simply due to the fact that the teacher was presented noisy labels in training \n - How does this prove that the \"underfitting is good\"? or that \"smaller student models go beyond a larger teacher\"?\n\n(4) Why is the monotonic logit transformation used $\\log \\frac{u}{1-u}$ ? Do these plot hold for other transformations? Say linear, or other metrics like KL-divergence between teacher and student probabilities?\n\n(5) What is the definition of \"Fidelity\" in Sec.3.1?\n\n(6) Could the authors summarize what is going on the Sec.3.2? \n - The setup tries to switch from One-Hot training to Distillation or the vice-versa in the middle of the training. What was the aim of this experiment? Figure 4 shows that if you switch from One-Hot to Distillation you are better off doing so as early as possible? Is that not just saying that learning from softened probabilities is better than using one-hot labels? Please elaborate if I missed some point.\n\n - Btw, I would exclude Fig4(3) from conclusions purpose simply due to the fact that the accuracies are all close by for almost all distillation experiments ( 0.663 - 0.667 ) which might just be due to the variance in the experimental runs.\n\n(7) Thm.4.1 -- applies to linearized NTK regime for neural networks, which would be applicable for neural networks with width going to infinity. It is unclear how you would apply it to the neural networks setup. Besides, what are nice directions? It is unclear how Thm.4.1 explains anything?  Some recent works have shown that SGD in general learns functions of increasing complexities (see https://arxiv.org/pdf/1905.11604.pdf )\n\n(8) Concrete Example in Thm.4.2\n - Do you have experimental results on this example? Since the theorem 4.2 states that you get better quality updates from distillation does this mean CE is unable to recover the correct classifier? Are there any quantitative / qualitative results for this example?\n\n - Thm4.2 studies a linear classifier on top of a specially constructed K-class problem. It is unclear how their observations extend beyond this setup. Even in loss switching there are many instances where switching to one-hot loss from KD does not seem to hurt the performance. So their point about One-Hot getting stuck at the optimal solution does not seem to apply here.\n\n(9) How are the hyper-parameters tuned? In appendix Table.1 shows that CIFAR dataset uses peak learning rate of 1.0. Is this number a typo? This seems to high for the CIFAR datasets. Besides, even the batch size (1024) seems sub-optimal. Typical setup would include batch size (128-256) and learning rate of 0.1 for SGD+Momentum optimizer.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper needs to be re-written to highlight the important and coherent empirical observations rather than throw all the plots at the reader in the hopes that they would draw their own conclusions. Since this work analyzes empirical behavior, it would greatly improve the quality of the work if it runs through a single illustrative example and show-case various observations one would see in almost all the settings.\n\nSuggestion for improving the paper writing / clarity? \n - Instead of showing all the plots at once (abundance of information)\n - Just pick one setting (say two architecture combinations Res56-56 and Res56-20, one instance of self-distillation and cross-distillation)\n - Go through all the observations at once\n - This would make the paper much more readable and clearly highlight the important observations\n - You can put the remaining plots in the experiments section to show that above results are applicable in other setups\n - Currently, things are just all over the place\n\n\nNit-Pick:\n- Page 3, Eq.1, $\\ell(f(x_n))$ does not incorporate a label $y_n$. It is unclear how this is a loss function? \n- Page 9, Para.1, \" .. teacher's probabilities Menon et al. (2021). ... \"\n",
            "summary_of_the_review": "This work characterizes the nature of deviations between teacher and student network during knowledge distillation (self as well as cross architectures). Although the problem at hand is interesting, its execution from both empirical and theoretical standpoint seems below par. In addition, their seems to be incoherency in the paper writing that obfuscates main observations and their connections with the thoery developed in the paper. \n\n\n----- \n\nI've updated my score post the rebuttal.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3944/Reviewer_xDEX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3944/Reviewer_xDEX"
        ]
    }
]