[
    {
        "id": "tlO3F_ItRkF",
        "original": null,
        "number": 1,
        "cdate": 1666541830466,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541830466,
        "tmdate": 1666541830466,
        "tddate": null,
        "forum": "dGzgbdQbgwm",
        "replyto": "dGzgbdQbgwm",
        "invitation": "ICLR.cc/2023/Conference/Paper1821/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a novel supervised learning model in the scenarios of crowdsourcing based on AUM, which is a confidence indicator for each task. The proposed method generalizes the existing AUM indicator to the crowdsourcing setting as WAUM (the Weighted Area Under the Margin), and the WAUM identifies harmful data and prunes the ambiguous tasks to improve the results of label aggregation. Experimental results show the effectiveness of the proposed method on several simulated and real-world datasets.",
            "strength_and_weaknesses": "This study is intuitive and technically sound. The proposed method is simple but interesting. In addition, the paper is presented clearly and is easy to follow and reproduce.\n\nHowever, there are several weaknesses existing in this paper:\n\n- First, the main idea of this paper is straightforward and not very impressive, and the novelty of the method is limited. The proposed WAUM seems like a simple combination of existing works, i.e. AUM and confusion matrix (which is mentioned in many EM-based label aggregation methods). I suggest the authors discuss the contribution of this paper in more detail and highlight the innovations.\n- Second, the design of the experiments is somewhat weak. 1) The proposed WAUM-based method measures the task difficulty by incorporating feature information and trains a neural network classifier simultaneously, but the comparison methods such as MV, DS, and GLAD are all feature-blind methods, also the comparison methods are classical algorithms but not the latest research works, so the experimental results are less convincing. I suggest that more algorithms (better to use the feature of tasks) should be compared with the proposed methods, e.g., (Raykar et al, 2010), (Rodrigues et al, 2018), and so on.\n- Furthermore, since the main idea of the proposed method is to identify the harmful tasks and prune them to reconstruct a new sub-dataset, could the sub-dataset be used on other existing methods to show the effectiveness of the pruned dataset?\n\nRaykar VC, Yu S, Zhao LH, Valadez GH, Florin C, Bogoni L, Moy L (2010), Learning from crowds. The Journal of Machine Learning Research 11:1297\u20131322.\nFilipe Rodrigues and Francisco C. Pereira. Deep learning from crowds. AAAI\u201918, 2018.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This study is intuitive and technically sound. In addition, the paper is presented clearly and easy to follow and reproduce. The method is simple but interesting and seems effective, but the novelty is limited.",
            "summary_of_the_review": "Overall, the paper has its merits to show a new method introducing a crowdsourcing-specific confidence indicator to prune harmful tasks and improve the performance of the model trained on the pruned dataset. However, it also has  some  weaknesses. Thus, this paper cannot meet the high-quality requirements of ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1821/Reviewer_JrVm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1821/Reviewer_JrVm"
        ]
    },
    {
        "id": "qqEtIWY2PDQ",
        "original": null,
        "number": 2,
        "cdate": 1666688365677,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688365677,
        "tmdate": 1666688365677,
        "tddate": null,
        "forum": "dGzgbdQbgwm",
        "replyto": "dGzgbdQbgwm",
        "invitation": "ICLR.cc/2023/Conference/Paper1821/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work considers the crowdsourcing problems where there exist ambiguous tasks. To address this problem, a new framework that used a similar technique in AUM method to identify ambiguous tasks has been proposed. To show the effectiveness of the proposed method, evaluations on both the synthetic datasets and the CIFAR-10H crowdsourced dataset have been implemented.",
            "strength_and_weaknesses": "This paper is well-organized, the structure is clean and clear, and some nice graphs are provided to help the readers to understand the proposed framework. Also, the authors provided links for almost all the involved datasets and baseline methods, which brings convenience to the readers and shows respect to the original authors at the same time. However, this work can be further improved in the following aspects:\n\n- The baseline methods used are classic methods but not very new. It would be better to use some more recent methods as baseline methods. \n- Since the proposed algorithm used the idea of AUM method, can you compare these two methods in depth?\n- In this paper, the comparison with some vote aggregation algorithms is provided, however, these methods are not new, can you compare the proposed method with more recent aggregation methods [1][2][3]\n[1] https://jmlr.org/beta/papers/v21/19-359.html\n[2] https://arxiv.org/pdf/1909.12325.pdf\n[3]https://proceedings.neurips.cc/paper/2020/file/f86890095c957e9b949d11d15f0d0cd5-Paper.pdf\n- The format of the tables used in this paper should meet the requirement of the conference.\n- It would be better to give some theoretical insights or analyses why the proposed algorithm works well.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the detailed comments above.",
            "summary_of_the_review": "In my opinion, this paper is a borderline paper. The motivation is valid, and the organization is clear. However, the baseline methods used are not very new which is my concern.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1821/Reviewer_jfcJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1821/Reviewer_jfcJ"
        ]
    },
    {
        "id": "2tr1GEKzN6",
        "original": null,
        "number": 3,
        "cdate": 1666850034123,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666850034123,
        "tmdate": 1666850034123,
        "tddate": null,
        "forum": "dGzgbdQbgwm",
        "replyto": "dGzgbdQbgwm",
        "invitation": "ICLR.cc/2023/Conference/Paper1821/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a way to combine crowdsourced multiple labels per task by using a weighting areas under the margin method (WAUM), which uses area under the margin method (AUM) used for detecting mislabeled data in the training dataset with a single hard label per task. It is a weighted version of AUM based on the trust factor. The trust factor can be derived as the inner product between the diagonal of the confusion matrix and the softmax vector. Experiments show how the proposed method is helpful in achieving higher test accuracy and lower ECE scores.",
            "strength_and_weaknesses": "\nStrengths\n\n1. Taking into account task ambiguity is an important research topic.\n2. The proposed method can achieve a higher test accuracy for the simulated synthetic datasets and achieve the same performance as the naive soft method in the CIFAR-10H dataset.\n\nQuestions (and Weaknesses)\n\n1. One of the motivation of the paper is to remove intrinsically ambiguous tasks (which might even fool expert workers) because these tasks may be harmful for the learning step. My understanding is that intrinsically ambiguous tasks are helpful for learning, and I was not sure why these tasks may be harmful. For example, the most intrinsically ambiguous task may have p(y=+1|x) = p(y=-1) = 0.5 in binary classification (assuming y \\in {+1, -1}), where p(y|x) is the underlying true class probability. If we collect many hard labels (n labels) for this same task, close to half of them will be positive hard labels and the rest of the half will be negative hard labels. When we naively regard this as n samples of (x,y) pairs (as well as other tasks), and train a classifier, our (probabilistic) classifier will try to learn hatp(y|x) = 0.5 for this task, and perhaps this will lead to improved ECE. It will do no harm for generalization performance, because test samples close to this x is intrinsically hard. On the other hand, I understand that noisy mislabelled samples (which is not related to the intrinsic difficulty of the task) may be harmful for learning. In Section 3 of the AUM paper by Pleiss et al. (2020), the following sentence \"In general, we assume both easy and hard correctly-labeled samples in D_train improve model generalization, whereas mislabeled examples hurt generalization\" is similar to my understanding. It would be helpful if the paper can provide some discussions about why removing intrinsically ambiguous tasks will be beneficial.\n2. The paper explains that the margin in Eq. 2 is modified from the original version, following Yang & Koyejo (2020). I did not have time to fully go through Yang & Koyejo (2020), but I am predicting that this corresponds to using softmax_y - softmax_2 in Eq. 2 instead of softmax_y - max_{j\\neq y} softmax_j which is used in Pleiss et al. (2020), assuming we ignoring the softmax-logit difference. I wasn't sure if the nice theoretical properties discussed in Yang & Koyejo (2020) directly transfer to AUM as a mislabel identification metric. Intuitively, the original margin definition seems to be easier to understand, e.g., if we consider the right example in Fig.2 of Pleiss et al. (2020), the modified margin will be near zero because Dog logit is the 2nd largest value for most of the epochs. It would be helpful if the paper can further discuss the benefits of the modified margin.\n3. Is it also possible to compare the unmodified and modified margins in the experiments?\n4. The paper explains that CIFAR-10 has very few (intrinsically) ambiguous tasks, because of the careful creation of the dataset. Since the goal of the proposed method is to remove ambiguous tasks, is this a good benchmark dataset to use to test the perfomance of the proposed method?\n\nMinor comments, suggestions\n1. Page 4, typo: \"Appendix A\" should be \"Appendix B\".\n2. What is bold in Table 2, 3, and 4? Are they results of a t-test?\n3. Is it possible to provide a practical suggestion for the number of hard labels per task?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is high. The paper is well organized, but I some important contents are included in the supplementary sections, such as Section B. I did not understand the motivation of the paper (see the previous section on \"Strength And Weaknesses\").\n\nQuality is high. The experiments are based on a single dataset that does not have many ambiguous tasks which does not seem to be a good fit (however, I also understand that there are not many datasets with multiple labels per tasks, which is also discussed in the final section of the paper.) The analysis and visualization with simulated datasets is helpful to gain a better understanding of the proposed method.\n\nNovelty seems to be high. It is based on a very recent paper that was published in Dec. 2020 and I have not heard of a weighted version of the AUM with crowdsourced labels.\n\nThe optional reproducibility statement was not provided in the paper. There are some details of the experiments written throughout the paper. The code was provided as a supplementary file, but I did not read the code.",
            "summary_of_the_review": "The paper works on an interesting problem and provides a novel algorithm that aims to achieve better generalization performance. However I had a hard time understanding the main motivation of the paper along with some other questions/weaknesses. More discussions around those questions would be helpful for the reader of the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Although optional, I recommend adding a ethical statement section at the end.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1821/Reviewer_Zduv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1821/Reviewer_Zduv"
        ]
    },
    {
        "id": "2T94LDNkeF",
        "original": null,
        "number": 4,
        "cdate": 1667046401926,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667046401926,
        "tmdate": 1667046401926,
        "tddate": null,
        "forum": "dGzgbdQbgwm",
        "replyto": "dGzgbdQbgwm",
        "invitation": "ICLR.cc/2023/Conference/Paper1821/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to combine a measure of the difficulty of an example in a supervised classification task (AUM) with the performance of annotators, calculated from their confusion matrix on a crowd-sourcing task. This combined estimator, called WAUM (weighted AUM), allows to take into account both the intrinsect difficulty of an example and the confidence in its label, when the latter comes from a crowd-sourcing campaign. The method is described in detail and compared to four other standard methods in the literature. The experiments are conducted on 2 synthetic datasets and 1 real dataset (CIFAR-10H). The WAUM method achieves slightly better classification results than the other methods on the synthetic data set and similar to the other methods on the real data set. \n\n",
            "strength_and_weaknesses": "* The proposed method is motivated, very clearly described and new as far as I can tell.\n* As far as the performance of the proposed method is concerned, the experiments do not really allow to highlight a major contribution. As indicated by the authors, the CIFAR-10H dataset may already be too clean to show the contribution of the proposed method.\n\nIt seems to me that the results should be confirmed on other real datasets, as indicated by the authors in their conclusion to give more strength to their proposal.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The article is particularly well written, very clear, with references to the main articles in the literature on the subject. The appendices detail the state of the art papers used in the comparisons and give further analysis of the results.",
            "summary_of_the_review": "The proposed metric developed in the paper is well motivated and clearly described. The experimental evaluation is still limited and has not been able to show performances clearly superior to the state of the art. The results described in the article still seem too preliminary.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1821/Reviewer_Q6Gx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1821/Reviewer_Q6Gx"
        ]
    }
]