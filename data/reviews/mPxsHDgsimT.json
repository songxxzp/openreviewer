[
    {
        "id": "XRHBUuMzKfL",
        "original": null,
        "number": 1,
        "cdate": 1666333723823,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666333723823,
        "tmdate": 1666333827731,
        "tddate": null,
        "forum": "mPxsHDgsimT",
        "replyto": "mPxsHDgsimT",
        "invitation": "ICLR.cc/2023/Conference/Paper1009/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes subclass-balancing contrastive learning (SBCL) for long-tailed recognition. It divides head classes into several fine-grained subclasses via adaptive clustering, to enforce equal probability of head/tail classes to be engaged. After clustering, a bi-granularity contrastive loss is proposed. In addition, dynamic temperature and warm-up are also used for SBCL. Experimental results show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n\n(1) The motivation of this work, i.e., dividing head classes to sub-classes to deal with class imbalance, is clear. The basic idea of enforcing both instance- and subclass-balance by leveraging supervised contrastive learning is reasonable. \n\n(2) The paper is well organized and clearly presented. The method seems easy to reproduce as well. \n\n(3) Experiments on LT datasets and downstream tasks are adequate. \n\nWeaknesses:\n\n(1) Though the motivation and proposed method are reasonable, the comparison with similar long-tailed contrastive learning approaches is not complete. In addition to KCL and TSC, there are some other related solutions for long-tailed recognition problem, like PaCo [1] and BCL [2]. What is the advantage of sub-class clustering over these solutions? \n\n(2) There are quite a few hyper-parameters, like temperature, $\\beta$, $\\delta$ and $\\alpha$ in Eq. 5. Though the authors provide some hyper-parameter study on CIFAR-100-LT, I still wonder how the hyper-parameters change across datasets? Do I need to re-adjust these hyper-parameters with lots of efforts for a new dataset to achieve good performance? \n\n(3) In Table 1, it seems these contrastive methods are implemented on different code base. To verify the effectiveness of SBCL over the SCL baseline, it is better to use your own implementation on SCL. \n\n(4) There are quite a few works trying to re-balance the supervised contrastive loss for long-tailed recognition. What if we instead use non-contrastive methods like BYOL? For non-contrastive methods, the subclass-balancing problem naturally disappears. \n\n\n[1] Parametric contrastive learning, ICCV 2021.\n\n[2] Balanced Contrastive Learning for Long-Tailed Visual Recognition, CVPR 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly presented and the somewhat novel. \nThe method seems easy to reproduce. \n\nAdditional concerns:\n(1) What if using adaptive clustering for cross-entropy based long-tailed recognition?",
            "summary_of_the_review": "The motivation is clear. The proposed method is reasonable though some related works have similar ideas.\nThere are some weak aspects in empirical study, as pointed out above.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1009/Reviewer_aZoT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1009/Reviewer_aZoT"
        ]
    },
    {
        "id": "VX7iLAnTdbo",
        "original": null,
        "number": 2,
        "cdate": 1666525307896,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666525307896,
        "tmdate": 1666525307896,
        "tddate": null,
        "forum": "mPxsHDgsimT",
        "replyto": "mPxsHDgsimT",
        "invitation": "ICLR.cc/2023/Conference/Paper1009/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on improving supervised contrastive learning to learn better features over class-imbalanced dataset and to achieve better long-tailed recognition (LTR). The paper argues that, in the literature of LTR, typical class-balancing techniques such a data reweighting, resampling and supervised contrastive learning (SupCon) introduce biases as data instances are imbalanced although classes are balanced in training. Motivated by this, the paper proposes a method called \"subclass-balancing contrastive learning (SBCL)\", which clusters training data in balanced subgroups within (head) classes such that subgroups and tail classes are balanced in size. SBCL samples positive pairs of data within subgroups or tail classes in SupCon. It reports improved LTR performance in experiments.",
            "strength_and_weaknesses": "Strength:\n- Clustering head classes into subgroups for group-level balanced learning is interesting in the context of long-tailed recognition.\n- The study of long-tailed pretraining for downstream tasks is interesting.\n\n\nBelow are weaknesses.\n\nFor LTR, it is misleading to state that \"the major challenge (of LTR) is the discrepancy between the imbalanced training data distribution and the balanced test set.\" In the real world, testing data should also follow the same long-tailed destruction, but the requirement or evaluation metric is \"class balanced\", i.e., it requires recognizing all imbalanced classes equally well.\n\nWhile the paper states that \" it might be wise to break down the head classes into multiple semantically coherent subclasses,\" it should discuss whether the clustering groups correlate to the \"semantically coherent subclasses\". In other words, how to guarantee the clusters are aware of subclasses?\n\nAlgorithm 1 describes a heuristic method about clustering data of a head class into equal-size subgroups. However, would subgroups produced later be noisy (as they will have large distances to their mean features) compared to those generated in early steps? Can authors provide more analyses or visualizations?\n\nTable 2: While SBCL achieves better performance than the compared methods, [R1] performs much better by tuning weight decay in training. [R1] finds that regularizing network weights (towards more balanced norms in network filters) is crucial for LTR, and doing so leads to state-of-the-art. Therefore, it is unclear why SBCL improves LTR (slightly): whether it is due to balanced subgroups of head classes or whether it helps learn balanced weights in networks. Authors should provide more analyses.\n\n[R1] Alshammari, et al., \"Long-Tailed Recognition via Weight Balancing\", CVPR 2022\n\n\nSection 4.3 and Table 3: Can authors list results by \"training from scratch\" (i.e., without pretraining), and \"freezing backbone\" (i.e., not finetuning the whole pretrained network). The two baselines are important as comparisons can clearly show how much imbalanced/balanced pretraining helps the downstream detection task.\n\nTable 5: It is confusing to compare the absolute distances of intra- and inter-class distances. Instead, a ratio between intra- and inter-class distances might better show how well the per-class examples are separated in the feature space. If computing such a ratio, SBCL seems to perform worse than the other methods. Can authors discuss this point?\n\nThe paper does not show how many epochs/iterations needed for convergence by SupCon and SBCL, and for SBCL's warm-up learning. Can authors provide a learning curve, e.g., loss vs. iterations/epochs, accuracy vs. iterations/epochs?\n\n\ntypos:\n- \"a instance\" in Section 3\n- \"a overly-loose/dense cluster\"",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity can be improved. Quality of the paper is okay. Novelty is below average. It is unclear how easy it is to reproduce the results (code is not provided).\n\n",
            "summary_of_the_review": "In the context of long-tailed recognition, the paper proposes a method to cluster head classes into small subgroups such that subgroups and tail classes have roughly equal number of training data, allowing for more balanced learning of feature representations using Supervised Contrastive Learning. However, the paper lacks justifications for some design choices and results, and misses citations and lacks analyses. Therefore, the paper is rated as \"marginally below the acceptance threshold\".",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics issues as I am aware.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1009/Reviewer_S8iv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1009/Reviewer_S8iv"
        ]
    },
    {
        "id": "abiTdSRnVLW",
        "original": null,
        "number": 3,
        "cdate": 1666541625676,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541625676,
        "tmdate": 1668770004769,
        "tddate": null,
        "forum": "mPxsHDgsimT",
        "replyto": "mPxsHDgsimT",
        "invitation": "ICLR.cc/2023/Conference/Paper1009/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new method called subclass balancing contrastive learning for long-tailed problems. The key idea is to divide head classes into subclasses to balance the number of instances in each subclass. The proposed method is evaluated on widely used image classification datasets as well as some additional vision tasks such as object detection and semantic segmentation. ",
            "strength_and_weaknesses": "## Strength\n1. The proposed method is evaluated on wide variety of datasets. Especially the evaluation on other tasks than image classification, i.e., object detection and semantic segmentation, is given.\n1. The effectiveness of the proposed method is shown in the experiment at least to some extent.\n1. The paper generally reads well\n\n## Weakness\n1. The comparison with stronger baselines such as [A1-3] is missing. The reported accuracies in these works are actually higher than the proposed method.\n1. According to Table 4, the performance of the proposed method on CIFAR100-LT becomes 43.7 if the dynamic temperature is not applied. This performance is lower than TSC. I am afraid this suggests that it is the dynamic temperature that makes the paper perform better than other contrastive learning methods. To show the effectiveness of the proposed idea, it is necessary to show the results of other contrastive learning methods such as SCL, KCL, and TSC with the dynamic parameters. \n1. I am afraid the performance of the proposed method is strongly affected by the batch size since the distribution in each mini batch is important in calculating the loss in Eq. (4). It would be nice if the authors could provide the analysis on this point.\n1. Table 5: \u201c(i) the inter-class distances of SBCL is larger than the other methods, which implies that SBCL can push different classes far away from each other, and thus clear the decision boundaries between classes; \u201d It is true that the inter-class distances of SBCL is larger than those of the other methods, but the intra-class distance of SBCL is also larger. Actually, the intra-class distances of other methods are much smaller than those of the proposed method, which means that the other methods have favorable characteristics.\n1. P8 \u201cWe attribute that to feature space dominated by the head class.\u201d Please rephrase and elaborate more so that the claim of this sentence becomes clearer.\n\n[A1] Zhong+, \u201cImproving Calibration for Long-Tailed Recognition\u201d, CVPR 2021    \n[A2] Cui+, \u201cParametric Contrastive Learning\u201d, ICCV 2021  \n[A3] Zhu+, \u201cBalanced Contrastive Learning for Long-Tailed Visual Recognition\u201d, CVPR 2022\n\nMinor points.\n1. P2, (c) \u201c\u2026 and instant segmentation\u201d -> \u201cinstance segmentation\u201d\n1. Eq (2): $\\tilde{V}_i^k$ -> $\\tilde{P}_i^k$?\n1. The second line in Section 3 $|V^+_{i, k}|$ does not appear in equation (2)\n1. P8 \u201csuccessful captured\u201d -> \u201c\u201csuccessfully captured\u201d\u201d\n1. P9 \u201cwell-haved\u201d What does this mean?\n1. P9: \u201cthe table 4\u201d -> \u201cTable 4\u201d\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is mostly clearly written\n- Quality: The quality in terms of presentation is good, but the quality of evaluation is not sufficient as I pointed out above.\n- Novelty: The idea of introducing sub-class contrastive learning into long-tail problems has certain novelty.\n- Reproducibility: The procedure of the proposed method is generally well described, but the initialization of the cluster centers in Algorithm 1 is not explicitly explained. Although I believe that basically important information for reproducibility is given in the paper including the appendix, readers may practically face issues since the code is not provided. \n",
            "summary_of_the_review": "I think the motivation of the paper is sound and the proposed method is somehow reasonable. However, I have doubt on the effectiveness of the proposed method since I do not think the current experiment is not sufficient for supporting it. As such I do not think this paper is ready for publication. I would like to hear the feedback from the authors on the points I listed in the weakness. \n\n### after rebuttal\nI became more positive about the paper through the rebuttal and I increased my score accordingly, but I still have some concerns on the effectiveness of the proposed method.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1009/Reviewer_QXy7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1009/Reviewer_QXy7"
        ]
    },
    {
        "id": "o5_ZNPjEMd5",
        "original": null,
        "number": 4,
        "cdate": 1667024816701,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667024816701,
        "tmdate": 1667024816701,
        "tddate": null,
        "forum": "mPxsHDgsimT",
        "replyto": "mPxsHDgsimT",
        "invitation": "ICLR.cc/2023/Conference/Paper1009/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for contrastive learning on a long-tailed training dataset. The proposed method is called Subclass Balancing Contrastive Learner (SBCL), which uses clustering within the head classes to determine fine-grained subclasses of size similar to the tail clusters. The new subclass labels for the head instances are used w/ tail classes to perform supervised contrastive learning. The paper contains extensive experiments over several vision benchmarks, including classification, object recognition and instance segmentation tasks. In almost all cases, SBCL performs better than several other baselines.\n",
            "strength_and_weaknesses": "**Strengths**\n\nS1: Extensive set of experiments on several vision tasks show SBCL does better than other representation learning methods for learning on long-tail distribution. Many ablations were also included to show several design choices made. \n\nS2: I think the idea of clustering the head class(es) into sub-classes and using the cluster ids as labels is novel.\n\n**Weaknesses**\n\nW1: While there are extensive sets of experiments, the quantitative results (Table 1-5) do not come with standard errors/deviations. The experiments were done w/ 5 random seeds, then I wonder why have the standard errors not been included in the table? Understanding how reliable these improvements seems like an important thing to report.\n\nW2: There is no discussion on the convergence of the model, and the number of training steps/epochs, loss trajectory or learning rate used. Since SBCL uses warm-up and adaptive clustering which may lead to a jump in the loss fn, studying these properties seems highly relevant. It is unclear if the number of training steps/epoch used for the training the feature extractor is the same for all the baselines and the proposed SBCL.\n\nOther weakness/questions:\n- I wonder why the disjoint subset analysis was not shown for the CIFAR-100 LT dataset?\n- The conclusion from the analysis in section 4.4 lacks clarity. What is the conclusion from this analysis? What is the significance of the finding: \u201cthe intra-class distance of SBCL is almost invariant with the decreasing of group split\u201d. Why the increase in the intra-class distance for KCL & TSC is attributed to the head class? The discussion in this section needs some work.\n- Was the negative sampling bias corrected when computing the loss function? While SBCL balances the label distribution (via cluster labels for head instances), it is unclear that this would mitigate the sampling bias in selecting negatives. Typically, models use log(P(x)) correction [1] when using in-batch softmax loss to prevent the head instances to be overly treated as negatives than the tail instances.Having a baselines that uses this logit adjustment would be a definite plus.\n\n[1] Menon et al. (2021), Long-tail learning via logit adjustment, (ICLR 2021).\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Novelty**\nThe paper is clearly written and seems novel.\n\n**Reproducibility**\nI am not sure if the paper has sufficient training details to confirm reproducibility. However, it is mentioned that the training process is same as TSC (Li et al., 2022).",
            "summary_of_the_review": "(Please see the strengths and weaknesses above.) I would appreciate if the authors respond to some of my questions and concerns. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1009/Reviewer_8khH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1009/Reviewer_8khH"
        ]
    }
]