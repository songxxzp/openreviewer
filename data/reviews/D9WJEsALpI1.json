[
    {
        "id": "m50n05MnhDT",
        "original": null,
        "number": 1,
        "cdate": 1665739508685,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665739508685,
        "tmdate": 1665739508685,
        "tddate": null,
        "forum": "D9WJEsALpI1",
        "replyto": "D9WJEsALpI1",
        "invitation": "ICLR.cc/2023/Conference/Paper290/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes to employ neural radiance fields to model the 3D part information of objects. In parallel with the color and density information, the NeRF model also predicts the semantic segmentation probability of each point in the 3D space. This yields a semantic field that enables the model to achieve good 2D and 3D segmentation results.",
            "strength_and_weaknesses": "Strength:\n1. This paper is well written and the ideas are clearly illustrated.\n2. It is nice to investigate NeRF-based model for a discriminative task.\n3. It is good to see that SegNeRF achieves encouraging results on 2D and 3D segmentation tasks. \n\nWeaknesses:   \nThe major concern is the novelty of this work.\n1. For the task of 2D and 3D segmentation, introducing a semantic field is no longer a novel design. For example, SemanticNeRF[1] and PanopticNeRF[2] employed the segmentation field for Scene Labelling and Understanding, FENeRF[3] and sofgan[4] designed the semantic field for 3D face generation and editing. The motivation and method of this paper are highly similar to these previous methods.\n\n2. For the encoder-based frameworks, this paper combines ideas of IBRNet[5] and PointNeRF. \n\n3. I think the novelty is to apply previous ideas to the task of 3D part segmentation, but I don\u2019t think this can reach the bar of ICLR.\n\n[1] In-Place Scene Labelling and Understanding with Implicit Scene Representation.  \n[2] Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation.  \n[3] FENeRF: Face Editing in Neural Radiance Fields.  \n[4] Sofgan: A portrait image generator with dynamic styling   \n[5] IBRNet: Learning Multi-View Image-Based Rendering  \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written, easy to follow and reproduce. But the novelty is rather limited.",
            "summary_of_the_review": "The strength and weakness of this paper are clear. I don\u2019t think this can reach the bar of ICLR because of the novelty of the idea and method.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper290/Reviewer_iUC9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper290/Reviewer_iUC9"
        ]
    },
    {
        "id": "wTH0Zrb3nF",
        "original": null,
        "number": 2,
        "cdate": 1666390588213,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666390588213,
        "tmdate": 1666457507305,
        "tddate": null,
        "forum": "D9WJEsALpI1",
        "replyto": "D9WJEsALpI1",
        "invitation": "ICLR.cc/2023/Conference/Paper290/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an extension of PixelNeRF to extend the neural field with auxiliary dimensions, corresponding to semantic classes of the scene. The authors propose to learn semantic fields using 2D semantic supervision and demonstrate that their approach reaches the performance of several prior approaches enjoying 3D semantic supervision (except one SOTA method).",
            "strength_and_weaknesses": "The paper is well written, the delivery is good, however, there are a few questions for the authors.\n\n- Fig.1 is nice but does not serve the purpose of explaining the method. Color coding segmentation/reconstruction/violet objectives are counter-intuitive. The 6 H/G MLPs are not placed into the context. The chair is presented from 3 different perspectives, yet only two are present on the left side. Without the introduction of the notation, the E symbol reads as taking expectation, and not the feature map explained deep in the Method section. I would recommend a complete re-design of the figure. \n\n- Sec. 2.1. states that voxelized representations have a cubic complexity in dimensions; this is not true in the light of works like TensoRF and other decomposition methods.\n\n- Sec. 3.2. is titled \"Predicting Volume Rendering\" - what does this mean exactly? Perhaps some rephrasing is in order.\n\n- Starting the \"Next, we elaborate\" paragraph in Sec. 3.2 and until the end of that section, the writing is rather negligent with notation. I would recommend checking how a similar concept was introduced in PixelNeRF and using a similar notation, more space to explain this rather critical part of the method and introduce notation before or around the first usage. In the end, I was not quite sure if the method (its part explained in the section) has any important differences from PixelNeRF, apart from the added semantic classes prediction. If not, then using PixelNeRF notation is advised.\n\n- Related to the previous comment, PixelNeRF applies positional encoding to 3D point x, whereas in your paper $\\gamma$ seems to be applied to the projected coordinates. Why was this change introduced, what was the motivation? Have you tried keeping this as in PixelNeRF?\n\n- In Sec. 3.3 you present the function $s(I, r, d)$, meaning that semantic information at each point depends on the viewing direction $d$. How so? Shouldn't it be independent of viewing direction like e.g. optical density $\\sigma$? \n\n- Why is there a zero in Eq. (8)? Is it by any chance there to fix the issue from the previous comment?\n\n- I'm not sure why Sec.3.4 is needed, if the function $s$ functions properly (not requiring Eq. 8 hack).\n\n- Why is there only 81% of PartNet dataset valid for dataset creation purposes?\n\n- View ID 135 must be clarified or moved to the supplmentary\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Mostly clear, writing of high quality except for a couple of places in the main text. Reproducibility must be easy but was not checked, as the method is a rather straightforward extension of PixelNeRF.",
            "summary_of_the_review": "The paper provides an important study of leveraging 2D semantic maps for learning 3D-aware semantic representations for semantic novel view synthesis using NeRF. The method seems like a small extension on top of PixelNeRF, but there are a few points to be clarified (see Weaknesses). The empirical study looks good, but may not present too much insight.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper290/Reviewer_zVwm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper290/Reviewer_zVwm"
        ]
    },
    {
        "id": "RZVCXO5XRFp",
        "original": null,
        "number": 3,
        "cdate": 1666597582532,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597582532,
        "tmdate": 1666597582532,
        "tddate": null,
        "forum": "D9WJEsALpI1",
        "replyto": "D9WJEsALpI1",
        "invitation": "ICLR.cc/2023/Conference/Paper290/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for 3D shape part segmentation. It learns an image-conditioned NeRF together with a semantic field. The method enables novel view (2D image) part segmentation and 3D point cloud part segmentation. The authors evaluate the proposed method on the PartNet dataset and show some real-world demos.",
            "strength_and_weaknesses": "Strengths:\n1. The idea of training an image-conditioned NeRF together with a semantic field is simple and straightforward. \n2. The paper is well-written and easy to follow.\n\nWeaknesses:\n1. The idea of training a semantic field in NeRF is not new. Many prior works (e.g., Semantic NeRF, NeSF, and Semantic SRN) share a similar idea, which limits the novelty of the proposed method.\n2. I think NeSF is very similar to the proposed method and fail to find any major difference between the two methods. However, NeSF is missing in the experimental comparison.\n3. In equation (5), why does the semantic field function s relies on the view direction d? From my understanding, the semantic label of a 3D point should not change with regard to the view direction. \n4. The current method is not tested within each object category. It would also be interesting to see whether the model trained on a shape category can generalize to similar categories as well, since we cannot train separate models for all shape categories in our real-world applications.\n5. Some experimental details are unclear or missing. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: 8/10\nQuality: 7/10\nNovelty: 6/10\nReproducibility: 9/10 if the code released\n\nSome experimental details are not clear:\n1. For the PartNet dataset, do you use all three levels (e.g., from coarse to fine-grained). According to the visual comparison, only coarse-grained level parts are used?\n2. The authors say that they re-run all baseline methods. However, In Table 1, Semantic SRN is only evaluated in two categories. Is there any explanation for that?\n3. \"Once DeepLab v3 is trained, we evaluate it on the validation set generated by PixelNeRF consisting of 25 novel poses, and rendered from a single view of the object instance using a category-specific\nPixelNeRF.\" What does render mean here? It's kind of confusing.\n4. \"Furthermore, Figure 2 also illustrates how PixelLab mislabels fine structures.\" However, PixelLab is not shown in Figure 2.\n5. The training details of 3D point-based methods are missing? Training/test split, input data format (xyz, or xyz + rgb, #points), etc.\n6. The multi-view baseline in section 4.2 is very confusing. How is the DeepLabV3 trained? Where does the input image come from during inference? How is the voting actually performed?\n7. Another alternative multi-view baseline is directly rendering the input point cloud and then using 2D segmentation networks on the rendered 2D images. In this way, we can render an arbitrary number of 2D images instead of being limited to 25 images.\n",
            "summary_of_the_review": "I fail to tell much difference between the proposed method and existing methods (e.g., NeSF). I strongly suggest the authors provide a more thorough discussion and comparison with the existing semantic NeRF works. Otherwise, I would prefer to reject the submission.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper290/Reviewer_miT3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper290/Reviewer_miT3"
        ]
    },
    {
        "id": "tfAJ534AsQ",
        "original": null,
        "number": 4,
        "cdate": 1667366474018,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667366474018,
        "tmdate": 1667366474018,
        "tddate": null,
        "forum": "D9WJEsALpI1",
        "replyto": "D9WJEsALpI1",
        "invitation": "ICLR.cc/2023/Conference/Paper290/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a generalizable neural field representation that combines the traditional neural radiance field with a semantic field, which enables a simultaneous modeling the geometry, appearance and semantic information from a few images. The proposed representation enables 2D novel view segmentation and 3D part segmentation from single and multiple images. They also demonstrate the capability of 3D semantic reconstruction from single in-the-wild image.",
            "strength_and_weaknesses": "** Strengths **\n\n1. The proposed method/representation is quite concise and technically sound. Simply integrating a new semantic field into a NeRF framework can improve the modeling capability. It shows more possibility to extend from geometry and appearence to more information.\n2. They conduct extensive comparison studies to demonstrate the superior of the proposed approach in terms of semantic segmentation.\n3. Code and data will be shared and they would be a nice bonus to the literature.\n4. The paper is generally well written and easy to follow.\n\n** Weaknesses **\n\n1. The main weakness of the submission is the limited contribution it offers. The idea that integrating a semantic field into a traditional NeRF representation is straightforward and not novel enough. It looks more like an incremental work to NeRF and nothing special is proposed to address particular problems.\n2. Although the main focus of the paper is semantic segmentation, we would still like to know how it performs on geometry and appearance quantitatively. If possible, I would also like to see an ablation study to show whether adding a semantic field has any positive or negative effect on the original NeRF's geometry and appearance modeling performance.\n3. In Tab2, why using more views make the final reuslts worse for \u201cEar\u201d, \u201cMicro\u201d and \u201cVase\u201d? Could you provide a more detailed analysis of this observation?\n4. In Fig4, could you show more novel view synthesis results instead of only semantic segmentation? Why the top region of the chair back is recognized as purple?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of the work is clear and solid. The authors will release the code and data for better reproducibility. However, the main concern is that the proposed idea is not novel enough.",
            "summary_of_the_review": "Given the above-mentioned weaknesses (limited contribution, missing analysis, etc), my suggestion is that the paper is currently marginally below the acceptance threshold for a publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper290/Reviewer_ynF6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper290/Reviewer_ynF6"
        ]
    }
]