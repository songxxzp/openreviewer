[
    {
        "id": "f1fd959GpJ",
        "original": null,
        "number": 1,
        "cdate": 1666638416773,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638416773,
        "tmdate": 1666638416773,
        "tddate": null,
        "forum": "iTtGCMDEzS_",
        "replyto": "iTtGCMDEzS_",
        "invitation": "ICLR.cc/2023/Conference/Paper3202/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses the limits of GAN based vocoders to generalize to new/different voices not seen in the training set.\nIt does this by scaling up the generator block of the model an using periodic activations. Specifically the paper call for using the SNAKE activation function, in combination with 2x upsampling and downsampling used to control high-frequency artifacts from SNAKE.\n\n",
            "strength_and_weaknesses": "The main strength of this work is the scaling up, including using the whole Libri dataset, including different recording conditions. This shows up in increased robustness at generation time, as I mention below. The architecture is also well motivated. Also it is well known that scaling up GAN training is not easy due to instability in the process, as outlined by the authors.\nThe increased bandwidth is also a plus for the model.\n\nListening to the samples most of the practical improvement seems to come from samples with music / noise in the background. Also foreign languages seem to be better model. I personally could not hear a major difference from unseen speakers compared to baseline methods, but this is just my personal opinion.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well written. The novelty is mostly in scaling and in adopting some architectural choices that improve the scalability of the model. Paper is clear and well written.",
            "summary_of_the_review": "This is a good paper, with extensive well motivated experiments.\nI would have guessed a bigger improvement from qualitative tests, especially for out of distribution, especially when I listen to the samples.\nSome of the baseline models seem to be completely broken in the samples presented, however the difference in the scores is rather small.\nI look forward for the release of the code and the models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Specifically the ability to generalize to unseen languages, will provide great ethic considerations to this model, because it will allow to address data-limited languages that may not decode correctly with previous models. This is a very positive point of scaling up the model and with this work in particular.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_qkXX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_qkXX"
        ]
    },
    {
        "id": "17aasJr9f_",
        "original": null,
        "number": 2,
        "cdate": 1666672706162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672706162,
        "tmdate": 1666672706162,
        "tddate": null,
        "forum": "iTtGCMDEzS_",
        "replyto": "iTtGCMDEzS_",
        "invitation": "ICLR.cc/2023/Conference/Paper3202/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a neural vocoder based on improved GAN architecture and the use of a bigger training dataset. The system aims at a \"universal\" vocoding function that works on various unseen categories of signals, including different acoustic signatures and languages.  The main claims are the proposed GAN-based approach is faster and more parallelizable than existing autoregressive models while it provides more flexible architectural choices than flow-based models. The main contribution of the paper is the introduction of the \"snake\" activation and the anti-aliasing module, that empirically improved the synthesis quality.",
            "strength_and_weaknesses": "Strengths:\n- The paper presents the proposed methods in an organized fashion, in comparison to the existing work. It is clear what's the new contribution and how they contribute to the performance of the model.\n- The paper presents a rich set of large-scale experimental results on various test scenarios, including OOD cases (on unseen languages) and noise conditions. \n- The paper validates the experimental results using various objective and subjective metrics, including a newly developed SMOS metric that is convincing for this particular use case. \n\nWeaknesses:\n- It is not very clear which part of the model is contributing to better performance. The model is equipped with newly introduced activation functions and the AMP module, which are all making sense in terms of improving objective quality, but not too relevant to the SMOS performance, I believe. Then, the true contribution of the paper in terms of generalization power is just the model's capability of learning from the bigger LibriTTS dataset (involving all the noisy folds). It is a valid contribution point, but I would say then it's just \"beating the state-of-the-art with a bigger computing power\" situation, which is scientifically not significant. For example, I wonder what happens if HiFi-GAN was trained from the same large training set. \n- The MOS and SMOS tests were done via crowdsourcing, but it's not mentioned if the listeners are at least native English speakers for the LibriTTS experiments. Given that MOS on AMT isn't that reliable, the paper could discuss more on their filtering criteria and so on. \n- The snake activation seems to work well for the proposed vocoder, but it's a mere adaptation of existing work. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. Some mathematical details could have been included in the main text rather than being relegated to the appendices. ",
            "summary_of_the_review": "The paper presents a novel GAN-based universal vocoder. While the paper is not entirely clear about which part of the model is the true contributor to its better performance, especially given that it's trained from a much larger dataset, the paper did improve the state-of-the-art. The paper validated the synthesis results thoroughly and the proposed model appears to generalize better to the unseen test environments than the baseline model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_6Yoq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_6Yoq"
        ]
    },
    {
        "id": "3FywGDdCd7",
        "original": null,
        "number": 3,
        "cdate": 1666756389535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666756389535,
        "tmdate": 1666756389535,
        "tddate": null,
        "forum": "iTtGCMDEzS_",
        "replyto": "iTtGCMDEzS_",
        "invitation": "ICLR.cc/2023/Conference/Paper3202/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents BigVGAN, a \u201cuniversal\u201d neural vocoder which achieves impressive results both in- and out-of-domain. The authors detail extensive architectural modifications and hyperparameter tuning necessary to train this model. Thorough comparisons and ablations are conducted to justify the proposed method and demonstrate its efficacy as a universal vocoder.",
            "strength_and_weaknesses": "Neural vocoding is a common component of many audio synthesis systems, and _universal_ vocoding has long been an enticing prospect for audio research. The qualitative performance of BigVGAN both in- and out-of-domain represents a tangible step towards universal vocoding.\n\nOne potential limitation is that BigVGAN outputs 24k audio, which is lower than the standards used for high-fidelity music audio. Another limitation is that GAN training remains fickle---a lot of the architectural modifications and hyperparameter adjustments the authors may be fragile and difficult for subsequent researchers to build on.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nOverall, this paper is well-written and easy to read. The authors provide sufficient detail to understand both their methodological decisions and extensive experimentation. A few low-level comments on clarity:\n\n- (Introduction) \u201cwhere the studio-quality recordings are not available\u201d to which of the many examples in the list does this dangling clause belong?\n- (Introduction) \u201cThis architectural flexibility leads to large model capacity when we scale up the size of the model.\u201d This argument doesn\u2019t make sense to me. One could definitely scale up low-based models arbitrarily (though whether or not they train effectively is another matter)\n- (Section 3.4) \u201c.. by scaling up the generator\u2019s model size to its maximum ..\u201d why does the current configuration constitute a maximum?\n\n**Quality**\n\nThe methodology in this paper is mostly bits and pieces of other papers cobbled together, and parts of these descriptions unfortunately come across as alchemy. However, the modifications are all properly ablated, and the high-level arguments suggested by the authors do make sense intuitively. A few questions about the methodology:\n\n- (Figure 1) In the AMP block, surely the second low-pass filter is applied _before_ downsample1d? Otherwise, if the figure is correct, downsample1d will alias since you haven\u2019t low-passed yet\n- (Section 3.3) The upsampling/downsampling procedure does _reduce_ aliasing risk, but it doesn\u2019t appear to _eliminate_ it. Why not just actually band limit the alpha parameter using a sigmoid or something similar?\n\nThere is a missing citation in the first paragraph to WaveGAN from Donahue et al. 2019, an earlier ICLR paper which was the first non-autoregressive deep generative model capable of producing raw audio.\n\nAnother question I have relates to the homogeneity of the training data. If the goal is to achieve universal vocoding, why not train on a ton of non-speech data as well? E.g., the authors could consider training on AudioSet\n\n**Novelty**: The methodological innovations and the strength of the experimental results (particularly out-of-domain) do constitute substantial novelty in my view.\n\n**Reproducibility**: As is common in deep learning literature, it would be challenging if not impossible to reproduce the results of this paper from the descriptions alone. Fortunately, the authors indicate they will share code and pre-trained models upon publication.\n",
            "summary_of_the_review": "Overall, BigVGAN represents an exciting step towards universal vocoding that will certainly be of interest to many audio researchers, and will likely become a common component of many audio generation systems. I believe it could be an important inclusion in this year\u2019s ICLR conference proceedings.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_iGrd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_iGrd"
        ]
    },
    {
        "id": "bmR21CvLfD",
        "original": null,
        "number": 4,
        "cdate": 1666900364343,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666900364343,
        "tmdate": 1666900364343,
        "tddate": null,
        "forum": "iTtGCMDEzS_",
        "replyto": "iTtGCMDEzS_",
        "invitation": "ICLR.cc/2023/Conference/Paper3202/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents two key ideas: periodic activation function and anti-aliased activation. Both of these choices enable the generation of raw audio from mel-spectrogram with unprecedented quality for unseen speakers and recording conditions. ",
            "strength_and_weaknesses": "Strengths:\n* Addresses some of the key challenges in raw audio synthesis\n* Strong experiments and results\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and easy to understand and follow. The ideas in the paper are novel and is widely applicable for audio generation tasks. Enough details have been provided to enable reproducibility.",
            "summary_of_the_review": "The paper addresses some of the key challenges plaguing GAN-based audio waveform synthesis. Experiments justify the claims.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_F87e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_F87e"
        ]
    },
    {
        "id": "zQRVfpFbp9",
        "original": null,
        "number": 5,
        "cdate": 1667252159608,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667252159608,
        "tmdate": 1667252159608,
        "tddate": null,
        "forum": "iTtGCMDEzS_",
        "replyto": "iTtGCMDEzS_",
        "invitation": "ICLR.cc/2023/Conference/Paper3202/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents BigVGAN, a GAN based vocoder that generalizes well for OOD scenarios without additional finetuning. They seem to achieve state-of-the-art performance for audio synthesis in a variety of novel scenarios like new speakers, unseen recording environments etc. The proposed model is able to reproduce a lot of fine details across sound textures, speech and music that previous state-of-the-art models were not able to replicate. The authors show that introducing periodic activations into the generator provides the desired inductive bias for audio synthesis.  ",
            "strength_and_weaknesses": "Strengths:\n- The model is able to outperform all the previous state-of-the art neural synthesis tools even with a scaled down version\n- The zero shot performance on OOD results is impressive\n- The periodic inductive bias on activations is a very useful addition for more structured types of sounds like speech and music\n- The authors were able to train a very large GAN for audio synthesis without some of the commonly encountered training issues specific to this setting\n\nWeaknesses-\n- Although the authors use a larger frequency range than previous papers, it still falls short of the claimed \u2018full-band\u2019 especially for non-speech sounds where there could be useful information contained in the frequency bands above 12 kHz\n- The SMOS and MOS results are not able to show statistically significant improvement in BigVGAN vs. HiFi-GAN and BigVGAN-base vs. BigVGAN\n- The periodic inductive bias doesn\u2019t make a lot of theoretical sense for sound textures and other physics-based sounds\n- Outside the training set doesn\u2019t necessary mean out of distribution\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written with all the relevant information needed to evaluate the work except the code and model-specific details that are still awaited. The webpage with extensive sound examples is really helpful in evaluating the incremental advantage of components of BigVGAN and its comparison with previous models. The figures are clear and adequately captioned. More information on the model would have been useful but overall, it is a high quality paper that explains its result clearly and answers a lot of the questions a reader might have in the appendix section which is quite elaborate.\n\nFor reproducibility, the authors have promised model details and full code.\n",
            "summary_of_the_review": "Overall, this is a well-written manuscript that presents research that pushes the state-of-the-art in neural audio synthesis using GANs. BigVGAN generalises fairly outside the training set, better than previous models on all major types of structured sounds. The use of a periodic inductive bias through activations is a smart theoretical idea and a choice that seems to work well based on the full model and lesion results. That said, the work has minor limitations including but not limited to lack of statistical analysis for mturk results that seem to be statistically insignificant for some vital comparisons.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_ozMy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3202/Reviewer_ozMy"
        ]
    }
]