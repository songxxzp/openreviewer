[
    {
        "id": "JR2Y3b0cw4q",
        "original": null,
        "number": 1,
        "cdate": 1666166126140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666166126140,
        "tmdate": 1666166126140,
        "tddate": null,
        "forum": "zOHQGKO3WGY",
        "replyto": "zOHQGKO3WGY",
        "invitation": "ICLR.cc/2023/Conference/Paper2477/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors consider the fact that our common benchmark data sets are curated, in the sense of their data points being preselected by the consensus of a set of labellers. Aitchison (2021) already noted this and designed a proper likelihood that takes account of this fact. The authors build upon his work by moving into the semi-supervised setting where they show how to formulate two common approaches (entropy minimization, pseudo-labelling) as lower bounds of this likelihood. The approach is then evaluated in several experiments ranging from synthetic to real-world.",
            "strength_and_weaknesses": "## Strengths\nThe paper is well argued for and shows clear results upon the baseline the authors are considering and demonstrates in varied ways their assumptions\n\n## Weaknesses\n- The strong emphasis on this being _Bayesian SSL_ seems to me to be too strong. While the proposed approach gives us a proper likelihood the method and approach lack any intrinsic \"Bayesianity\". Dropping the Bayes terms the paper could have just as well been formulated as a novel likelihood for a maximum likelihood approach. The authors themselves 'simply' use it in this function in their third experiment. \n- Inconsistent notation and minor errors (see below)\n- A lot of the hyperparameters and experimental details are missing, or partially available but then hidden in the provided code. E.g., Sec 4.1 mentions an architecture for creating the toy data set but lacks any information on whether the trained networks have the same architecture. Sec 4.3 lacks a complete discussion on how the authors arrived at their final nine labels, making replicability and reproducibility impossible. If the preprocessing follows Aitchison (2021) then please properly cite and discuss this fact. ",
            "clarity,_quality,_novelty_and_reproducibility": "- Sec 2.2 uses `n` annotators and switches to `S` for the remainder of the paper\n- (10) contains an error. It should be $\\log(S (1/S)^S) = (1-S)\\log S$, i.e., you are missing a negative sign. Similarly, given that the derivation is about lower terms to a likelihood that is to be maximized (9,10) would read better if they were formulated in terms of an entropy objective that is to be maximized instead of the current negative entropy, giving then $(1-S)L_\\text{entr}$ for both. (I.e., as in (11) which is explicitly formulated as a maximization task)\n\n**Clarity and Quality:** The paper itself is well-written but contains an error in the derivation  \n\n**Novelty:** The approach heavily builds on the prior work by Aitchison (2021) and is a rather minor extension of it.   \n\n**Reproducibility:** In its current form reproducibility is not given for most of the experiments.  \n\n\n\n## Specific questions\n- Q1: Fig 2 speaks of _our generative model_. Can the authors clarify that part, i.e., to which degree does it differ from the example and model of Aitchison (2021) Figure 3?\n- Q2: Fig 3. Can the authors comment on why the test log-likelihood seems to improve with more samplers (A) but the test accuracy decreases? Similarly, the caption lacks an explanation wrt. what the change in B, D consists of which has to be guessed by the reader.\n- Q3: As stated above, the proposed approach could be used in a deterministic setting as well. Can the authors comment on how a maximum likelihood approach would perform in Sec 4.1 and 4.2 and vice versa how a BNN would perform in Sec 4.3 (e.g., Aitchison (2021) relies on an SGLD approach in that experiment)?\n\n## Minor comments\n- The method is restricted to the low-density approach of semi-supervised learning. But this restriction is acknowledged in Sec 5 and properly defended.\n- Sec 2 \n- The experiments as summarized in Fig 3/4 lack standard deviations\n\n## Typos\n- Inconsistency in the equation typography, i.e., some end with `.`,`,` others don't\n- End of Sec 3.1: _psuedo-labeling_",
            "summary_of_the_review": "While the paper contains some interesting formulations and results that are well worth further investigation in its current form it is rather incremental and contains several technical flaws (derivation and esp. wrt reproducibility) that need to be addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2477/Reviewer_fBnW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2477/Reviewer_fBnW"
        ]
    },
    {
        "id": "Tz7l725teH",
        "original": null,
        "number": 2,
        "cdate": 1666721695874,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666721695874,
        "tmdate": 1668829371705,
        "tddate": null,
        "forum": "zOHQGKO3WGY",
        "replyto": "zOHQGKO3WGY",
        "invitation": "ICLR.cc/2023/Conference/Paper2477/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper looks at the topic of semi-supervised learning for classification, through a probabilistic lens. The starting point is the observation that many common SSL datasets (e.g. CIFAR) are assembled via multi-annotator agreement protocol (images are only included if many humans agree on their class label). This leads to a multi-annotator consensus generative model (previously described in Aitchison 2021). Eq. 6 gives this model's likelihood for an image to be a *consensus* image (e.g. images that would be included even in unlabeled set because many annotators agree).\n\nThe new thing in this paper is the realization that this likelihood can be lower bounded by many common SSL objectives, included entropy (Eq 9) or pseudolabel (Eq 11) or FixMatch (Eq 16). This gives a nice interpretation of what these objectives are doing (maximizing the likelihood of a model that is aware that consensus is needed).\n\nExperiments on toy data demonstrate properties of this model, including:\n\n1) Fig 3 suggests that if S =1 (multiple annotators are not needed), unlabeled data does not help under this model, while if S > 1 (several annotators need to agree), unlabeled data can help\n\n2) Tab 1 shows that a Bayesian version of FixMatch, using their view of FixMatch's loss as a likelihood, does better than FixMatch on CIFAR-10 with few labeled images\n\n3) Fig 4 shows that on Galaxy Zoo data, datasets that are *curated* show gains from unlabeled data while *uncurated* sets show losses not gains in classifier performance as more unlabeled data are added\n\n",
            "strength_and_weaknesses": "# Strengths\n\n* Highlights that curation (multi-annotator agreement) is behind many popular SSL benchmarks, which is less widely understood/used than it should be\n* Principled approach to defining likelihoods that correspond to bounds on popular SSL objectives\n* Showing how to use these likelihoods within a Bayesian model seems valuable\n* Analysis in Sec 2.1 and Experiments on toy data are convincing: if data comes from a single-annotator model, hopes for SSL are dim (any unlabeled data is uninformative!)\n\n# Weaknesses\n\n* Why does accuracy get worse as S increases in Fig 3 on toy data?\n* Use of a bound (not exact likelihood) in CIFAR experiments may hide role of augmentation\n* Unclear what prior is used in the Bayesian FixMatch\n* Use of a temperature in the Bayesian FixMatch result needs more explanation\n* Writing needs some work: Lots of abrupt transitions and confusing notation\n\nThese weaknesses are elaborated further below (under Quality/Clarity)",
            "clarity,_quality,_novelty_and_reproducibility": "\n## Novelty\n\nI think there's plenty of contributions here: \n\n1) the observation that most existing SSL benchmarks are tightly curated (e.g. require multi-annotator agreement) is not widely known/mentioned or used within approaches, but this paper makes a clear case that it should be better understood.\n\n2) the interpretation of negative entropy and pseudolabel loss functions as likelihoods appears new to me (despite these ideas being popular for a decade plus) and it nicely offers an explanation of why pseudolabel is more in favor now (its bound is tighter).\n\n3) building a \"Bayesian\" FixMatch that outperforms standard FixMatch at small datasets sizes (Tab 1) is nice to see\n\n\n## Quality\n\n### Why does accuracy get worse as S increases in Fig 3?\n\nFig 3C clearly shows that S=4 annotators (dark purple) reach 70-73% accuracy, while S=1 reaches ~85% accuracy. Can the authors please explain?\n\nAs the number of annotators S increases, I would expect that on *labeled* test data the task of classification becomes easier, as only examples where the true class distribution gave many samples of the same class are included. Requiring more consensus should discard examples near decision boundaries, and thus make learning sharp boundaries easier.\n\nI guess the test dataset itself is changing as S is changing? Is the ratio of pos/neg examples changing? The size of test set changing?\nI think would be useful to clarify this in the figure/caption.\n\nBecause panels B and D are somewhat redundant re-visualizations of the same information in panels A and C, I wonder if the figure would be better off only showing A and C, while also showing some other visualization that makes the nature of the task a bit more clean (e.g. show a scatterplot in 2d of the examples, colored by class label, somehow indicating which examples are \"consensus\" or not under each of the S values)\n\n### Would results improve if used exact likelihood instead of the bound?\n\nBayesian FixMatch uses Eq 16 as the likelihood in CIFAR-10 experiments.\nIn earlier toy experiments, Eq 6 is used as likelihood, and Eq 16 is only a lower bound of Eq 6.\n\nCould we use Eq 6 (the ideal likelihood, not a bound) in CIFAR-10 and get better results than Bayesian FixMatch in Table 1? \nMy guess is the augmentation hiding inside FixMatch is delivering generalization gains that wouldn't be possible with Eq 6, even though it is the \"ideal\" likelihood, but the explanation in this paper for why the Bayesian approach is better does not seem to account for that.\n\n\n### Unclear what prior is used in the Bayesian FixMatch\n\nNaturally, Bayesian methods require a prior and likelihood. I don't see a clear description of the prior used in Sec. 4.2. Can the authors please elaborate?\n\nProbably it is just an independent Gaussian on each weight, but really should be specified in the main paper.\n\n### Use of a temperature in the Bayesian FixMatch result needs more explanation\n\nThe use of the temperature (as in cold posterior paper by Wenzel et al. 2020) to me spoils the interpretation as truly \"Bayesian\"... I'd like to see some analysis of how necessary this is (what is performance with temperature 1.0?)\n\n### Clarify purpose of training on CIFAR-100 images in Table 1?\n\nI'm not sure why the authors report the third line in Table 1, showing FixMatch (a baseline) trained on unlabeled data from a different dataset (CIFAR-100). \n\nNaturally, it is reassuring to know that using different unlabeled data does worse, as we might expect, but it's not clear how this experiment lends credibility to the presented Bayesian approach or the overall interpretation of this paper that a model of \"curation\" is useful to SSL.\n\n\n## Clarity \n\nOverall I found the manuscript's high-level points came through, but it was unfortunately tough to read in places, with awkward/abrupt transitions and frequent notation switching.\n\nI'd recommend that transitions between subsections get revised to help the reader keep a narrative thread of where this is all going. For example, Sec. 2.2. immediately dives into a generative model of data curation without explaining how it fits in, and Sec. 3.1 dives into a proof of a bound without motivating why it is useful.\n\nHere's a few further problematic clarity issues:\n\n* Sec 2 never defines the symbols X or Y (easy enough to guess what they mean)\n* Symbol Y_sup in Equation below Eq 2 is never defined, not clear if different from Y without subscript\n* Sec. 2 uses \"n\" to denote number of annotations, this switches to \"S\" in Sec. 3 without explanation. I'd suggest keeping S, since \"n\" often denotes training data size.\n",
            "summary_of_the_review": "Overall I like the core ideas here: the observation that curation is behind many standard SSL datasets is potentially powerful though underused currently: the methods here suggest that multi-annotator curation can be exploited (if present) via SSL but that datasets with only single annotator curation may not be usefully pursued with SSL methods. \n\n<strike>\nHowever, there's too many writing clarity issues and overall puzzling results to give this a high score currently. I hope discussion period clarifies things, I'd be willing to raise my score given a satisfactory response.\n</strike>\n\n**Update after discussion**: Clarity issues have largely been resolved, and the past improvements suggest remaining ones will also be resolved in good faith. Given the significance of the contributions here, I would vote for and argue for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2477/Reviewer_JPM3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2477/Reviewer_JPM3"
        ]
    },
    {
        "id": "m5fMHP8VBA",
        "original": null,
        "number": 3,
        "cdate": 1666732305484,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666732305484,
        "tmdate": 1666732305484,
        "tddate": null,
        "forum": "zOHQGKO3WGY",
        "replyto": "zOHQGKO3WGY",
        "invitation": "ICLR.cc/2023/Conference/Paper2477/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper aims to formulate the low-density separation term in SSL as a log-likelihood term from a generation model of data curation. Based on which, a Bayesian modeling is possible, and some new insight on why SSL works for curated data set.",
            "strength_and_weaknesses": "Strength: The likelihood for curated/uncurated data set seems interesting, and it successfully establishes a relationship to entropy minimization and  pseudo-labelling. \n\nWeakness: The construction of likelihood is too artificial, aiming to appeal low density separation idea. How to choose S? How to determine a labelled/unlabelled observation is curated or not, beyond common data sets such as CIFAR10? Unlike penalty term which are artificial designed to incorporate prior knowledge, likelihood should reflect data generation rule and tuning-free. \n\nOne experiment shows the improvement when unlabelled curated data is added, and the performance drop with more uncurated data. But this is not a convincing justification of the main of idea of the paper. The theory shows that log likelihood is lower bounded by the entropy or pseudo labelling loss under the generative model of curated data. It does not say what happens when the data is uncurated. To establish empirical justification, one also needs to show that the performance will increase with added uncurated data if likelihood (7) is used.\n\nThe paper claims it is a Bayesian approach, but it never mention anything about prior setting. For all the results displayed, are they based on posterior MAP or posterior mean?\n\nFigure 3, even when there is no unlabelled data, the test accuracy is different w.r.t. the choice of S. S=0 yields the best test accuracy. Please comment on this\n\nWhat is the test likelihood (Figure 3/4)?\n",
            "clarity,_quality,_novelty_and_reproducibility": "I feel that the paper has some reproducibility issue, since not all the detail implementations are given for the experiments. For example, what is S value for the Galaxy zoo experiment; page 7, it says \"We used Eq. (5) as the likelihood for labelled points, Eq. (6) as the\nlikelihood for unlabelled points and Eq. (7) as the likelihood for noconsensus points \",  I have no idea what is the likelihood for no-consensus unlabelled points. ",
            "summary_of_the_review": "Overall, I feel the paper is more like playing with a math trick, but can not deliver more insight. The low-density separation SSL assumes the decision boundary is not close to any data point, thus, even without any math, it implies that all data, including unlabelled data are away from decision boundary, i.e., they must be curated data. A mathematical formulate should provide us with more subtle insight of this matter, and this paper seems fail to do it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2477/Reviewer_3m8s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2477/Reviewer_3m8s"
        ]
    },
    {
        "id": "NCPA-Pufkcv",
        "original": null,
        "number": 4,
        "cdate": 1666924298302,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666924298302,
        "tmdate": 1666924298302,
        "tddate": null,
        "forum": "zOHQGKO3WGY",
        "replyto": "zOHQGKO3WGY",
        "invitation": "ICLR.cc/2023/Conference/Paper2477/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper the authors have formulated Semi Supervised Learning (SSL) objective as a principled log-likelihood estimate in the context of a generative model of data curation. The distribution of data (in the classification context) has multiple or at least two levels of uncertainty: one is the distribution of the classes and the other is the labels themselves. The authors characterize the supervised setting as learning by using curated data labels (the lower bound of the SSL setting) and propose SSL as an extended case with curated plus non-curated and unlabelled data (coins the concept of undefined class while there is a disagreement on labels or no label available).\n\nThe proposed methodology has been tested on some synthetic, CIFAR, and Galaxy Zoo 2 datasets. Statistically significant improvements have been claimed over the FixMatch, a known technique in SSL.\n",
            "strength_and_weaknesses": "Strengths: The main idea of this paper is to derive a Bayesian formulation of the SSL setup by using labeled and unlabeled data. Using curated labeled data they have formulated/defined a lower bound of the solution. They also have proposed two likelihood estimates that are applicable to the curated labeled and unlabeled data and then combine them in a coherent fashion for model prediction.\n\nThe proposed methodology has been tested on CIFAR-10 and Galaxy Zoo datasets. Reported results are found to be better than FixMatch, especially when trained with less number of labeled data points (in the SSL mix).\n\nWeaknesses: This work lacks a proper Bayesian formulation of the SSL problem which requires selection of some suitable priors and a proper estimation of the posteriors. Given that unlabeled data class labels are unknown it adds additional complexity to the modelling. There has been some prior work through the definition and usage of a null category (similar to the definition of the undefined class in this paper) such as [1, 2] which may help improve the Bayesian aspect of the formulation.\n\n[1] https://papers.nips.cc/paper/2004/file/d3fad7d3634dbfb61018813546edbccb-Paper.pdf [2] http://www.bmva.org/bmvc/2011/proceedings/paper3/paper3.pdf)",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the structure of the paper is organized; however, certain parts are difficult to follow. Data curation and principled likelihood estimates are two important concepts of this work. Although these two concepts have dependency, they have been used (the spread) over many different places in the paper which seems to be redundant. The readability of the paper can be improved if such redundancy can be simplified. The experiment section 4.1 is difficult to follow as it is missing some details such as the Gaussian parameters and how those were chosen, etc.\n\nThe paper contains some novelty: estimating the SSL lower bound in the context of data curation and experimenting and explaining the complexity as the uncertainty increases due to the increase of un-curated and unlabelled data.\n\nAs the code has been shared, it is expected the results can be reproduced although some additional details in section 4 could be helpful.\n",
            "summary_of_the_review": "I have gone through the paper more than once; overall, the idea sounds good and the results support some of the claims of this research. If we exclude certain sections, the paper is well written. The experiment section is missing some details and therefore found little difficult to follow.\n\nThe reported results are found to be interesting and provide us some insights on the difficulty of non curated and unlabelled data. I think this work has some value if some of the above mentioned limitations can be resolved/improved.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2477/Reviewer_y8jM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2477/Reviewer_y8jM"
        ]
    }
]