[
    {
        "id": "DR6dwCOMM9",
        "original": null,
        "number": 1,
        "cdate": 1666591113097,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591113097,
        "tmdate": 1670328232128,
        "tddate": null,
        "forum": "ORp91sAbzI",
        "replyto": "ORp91sAbzI",
        "invitation": "ICLR.cc/2023/Conference/Paper3977/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "It has been an convention that uses noisy validation seperated from training set to determine the model performance trained on dataset with label noise. The authors claim that noisy validation is not efficient and thus proposes a new evaluation protocal based on the susceptibility which meaures the difficulty for model to fit on a random labeled set.  Combining \"susceptibility\" with training accuracy can select good models that performs well on clean test set.",
            "strength_and_weaknesses": "**Strengeth:** \n\n- Using random labeled dataset to track memorization is new and interesting. The efficiency of susceptibility is supported by extensive experiments on multiple datasets including synthetic label noise and real-world label noise.\n\n- The proposed metric is supported by theoretical analysis.\n\n**Weakness:**\n\n- It would be better to perform some experiments based on instance-dependent label noise [R1].\n\n- This paper proves that a model with higher acc and lower susceptibility has better performance. But in pratice, it is hard to decide when should I stop training based on susceptibility and training acc. It seems one needs to set some specific thresholds to guide decision. Why noisy validation is not enough to guide decision since [R2] shows that the acc of noisy set is also robust.\n\n\n[R1] Learning with bounded instance-and label-dependent label noise.\n\n[R2] Robustness of accuracy metric and its inspirations in learning with noisy labels. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper is well-motivated and organized. \n- The proposed susceptibility is novel. I have not seen it in other papers.\n- The paper provides the code.",
            "summary_of_the_review": "The proposed metric is intersting. The results make sense but I find it hard to use the proposed metric to guide training in learning with noisy labels.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3977/Reviewer_QKAt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3977/Reviewer_QKAt"
        ]
    },
    {
        "id": "jxkfz9Q3WT9",
        "original": null,
        "number": 2,
        "cdate": 1666593894593,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666593894593,
        "tmdate": 1666593894593,
        "tddate": null,
        "forum": "ORp91sAbzI",
        "replyto": "ORp91sAbzI",
        "invitation": "ICLR.cc/2023/Conference/Paper3977/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new metric, called susceptibility, along with training acc to select good models trained on datasets with label noise. Authors observe that models with high training accuracy and low susceptibility will lead to higher acc on the clean test set. The authors also provide a convergence analysis of their methods.\n\n",
            "strength_and_weaknesses": "Strength: \n\n - Interesting and well-motivated idea. The proposed susceptibility is verified on many datasets including CIFAR, MNIST, and CLothing1m.Ting-ImageNet and Animal-10N.  \n\n - This paper is well written and easy to follow\n\nWeaknesses:\n\n - [1] already shows that the noisy validation is reliable. The authors state that [1] can not lead to comparing two trained models. However, from the experiments in [1], it seems that the models with higher acc on noisy validation also have higher acc on clean test acc. I think if the authors want to prove the efficiency of susceptibility. More experiments should be done to compare with [1]. From my perspective, it is hard to use susceptibility in practice since one also needs to set some threshold to remove models with higher susceptibility.\n\n\n[1] Robustness of accuracy metric and its inspirations in learning with noisy labels. AAAI 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": " - This paper has proposed a novel metric and the code has been attached. But the metric needs to be further verifed to prove its effectiveness in practice.\n",
            "summary_of_the_review": " -I like the motivation. But currently, this paper fails to convince me that their metric is useful in practice. (See weakness). \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3977/Reviewer_ZDzu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3977/Reviewer_ZDzu"
        ]
    },
    {
        "id": "0mZYGbmTNXj",
        "original": null,
        "number": 3,
        "cdate": 1666670761880,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670761880,
        "tmdate": 1669167530674,
        "tddate": null,
        "forum": "ORp91sAbzI",
        "replyto": "ORp91sAbzI",
        "invitation": "ICLR.cc/2023/Conference/Paper3977/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel susceptibility metric to trace the memorization of noisy signals when the model is trained on a dataset with noisy labels. The computation of this metric does not require ground-truth labels. Both empirical evidence and theoretical analysis are presented to support the intuition behind this metric. Moreover, the authors conduct extensive experiments to demonstrate the effectiveness of this metric across different network architectures, datasets and noise types. ",
            "strength_and_weaknesses": "Strength:\n- The proposed susceptibility metric is novel and interesting. \n- Both empirical results and theoretical analysis are included to support the intuition that good models are resistant to memorization.\n- The authors carried out extensive experiments to demonstrate the effectiveness of the proposed susceptibility metric.\n\n\nWeaknesses:\n- The discussion on different noise types is not sufficient. Only one kind of asymmetric noise is considered, and only the average test accuracy of the selected models is provided. \u2028The basic idea of this paper is to connect the easiness of fitting one noisy signal (in the held-out set) and the memorization of the other noisy signal (noisy subset in the training set). Mostly, the paper assumes the two noisy signals are of the same type, i.e., produced by randomly labeling. However, it is likely the type of noisy signal could play a crucial role. If the noise type in the training set is completely different from the one in the held-out set, e.g., the feature-dependent noise, the corresponding susceptibility metric may not be able to reflect the memorization within the training set faithfully.\n- The dependence of the effectiveness of the susceptibility metric on network architecture is not fully explored and discussed. For example,  the susceptibility metric is not that discriminative for the 5-layer CNN as shown in Fig.7 of the paper. Why is it so?\u2028In addition, it would be better to include more recently popular architectures such as the transformer. \n- In section 4, the division of the train ACC - Susceptibility space into four regions is interesting. However, I did not find anywhere in the paper mentioning how to choose the threshold for the division. \n- The discussion on different label noise levels is insufficient. The low label noise level situation deserves more emphasis since the noise levels of real-world datasets are not likely to be as high as 50%.\n- Presentation:\n    - Fig 1 needs better resolution.\n    - Symbol [m] is used without a definition.\n    - At the end of section 2, please indicate that the fully extended analysis is placed in section 5.\n    - Symbol in Algorithm 1 could be confusing. t indicates the index of the epoch of normal training. However, at epoch $t$ the weight \\tilde{W} is indexed with $t+1$. I understand that this refers to the \u201csingle optimization step\u201d on $\\tilde{S}$. Nonetheless, it is weird to have an index $t+1$ in the $t$-th epoch.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the idea is clearly presented.\n\nThe proposed susceptibility metric is novel and of practical importance.",
            "summary_of_the_review": "The finding of the relation between memorization and the easiness of fitting to a randomly-labeled held-out set is interesting from both theoretical and empirical perspectives. The proposed susceptibility metric is simple yet practical. Though the investigations could be extended in more depth, I would like to recommend the publication of this work.\n\n\n------------------20221123----------------\nAs my concerns are all addressed by the helpful discussion with the authors, I decided to increase my rating from 6 to 8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3977/Reviewer_QseH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3977/Reviewer_QseH"
        ]
    },
    {
        "id": "UuZ4yHBow7M",
        "original": null,
        "number": 4,
        "cdate": 1666672066261,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672066261,
        "tmdate": 1666672066261,
        "tddate": null,
        "forum": "ORp91sAbzI",
        "replyto": "ORp91sAbzI",
        "invitation": "ICLR.cc/2023/Conference/Paper3977/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a metrics called \"susceptibility\" that measure the model's resistance to memorization by using randomly labeled data. The authors provide both theoretical and empirical observations that the models which are resistant to memorization will have high test accuracies. Therefore, the extent of memorization to noisy labels can be used for picking models. The authors demonstrate how to pick models based on the values of susceptibility and training ACCs. \n",
            "strength_and_weaknesses": "Strengths: \n\n- The paper is well-motivated and the presentation is clear. \n- The experiments are extensive. \n- Theoretical understanding is also provided.\n\nWeakness & Questions:\n- The method for picking a \"good\" model is slightly ad-hoc. The thresholds to separate regions are obtained by fitting a lot of models and then find the median. Therefore, for different datasets and models we need to repeat this process. More specifically, in Figure 5 the accuracy threshold is 40% and in Figure 15 it is 50%. The difficulties in deciding these values make the practical benefit of the proposed method questionable. \n- What is the optimal value of \"susceptibility\" if we want to achieve good performance? It seems that there would be an optimal range, but is it possible to clearly find such a point that can achieve the best performance?  Ultimately, can the \"susceptibility\" be a predictive indicator for the performance? \n- Can we use \"susceptibility\" to regularize the models' training rather than just observe? ",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear, but massive contents are left in the Appendix. It requires multiple jump back and forward for me to collect details and it might still be possible that some are missed. The originality and novelty is good. ",
            "summary_of_the_review": "This paper introduces an interesting metric, and the authors justify that the metric is useful for selecting models. However, I feel the correlation or the relationship between the metric and the final performance is vague (or at least need to be better illustrated). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3977/Reviewer_E6P4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3977/Reviewer_E6P4"
        ]
    }
]