[
    {
        "id": "h6nMaJq-qU",
        "original": null,
        "number": 1,
        "cdate": 1666592501646,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666592501646,
        "tmdate": 1666592501646,
        "tddate": null,
        "forum": "nMZhFqYsiad",
        "replyto": "nMZhFqYsiad",
        "invitation": "ICLR.cc/2023/Conference/Paper4349/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new method for safe model-based RL that minimizes safety constraint violations during planning.\nThree variants of the method are proposed (S-RS, S-ME, PS-ME) and experiments are performed to show their effectiveness in terms of safety, state space exploration, and rewards earned.",
            "strength_and_weaknesses": "The use of quality diversity methods is an interesting approach to increase the exploration of the agent, which is an important aspect to collect information about the safety conditions in every part of the state space and to collect alternative planning configurations depending on the policy behaviour.\nThe experiments show some of the variants to be competitive with the baseline methods, although not consistently, they however still outperform the model-free alternatives, as we should expect.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe presentation of the method is very short, and many descriptions are only in the appendix, while the paper spends a long time on presenting some of the results. This makes it complicated to follow the presentation and grasp the working of the methods, as well as understand all the results presented, since the ones in the appendix are without or only with a short description.\nI don't think it would be possible to reimplement the method just from the description of the paper.\n\nIt is unclear to me what exactly the behavior space in MAP elites is, what characterizes a policy behavior in this paper?\n\nOf the multiple proposed methods, what is the recommendation of which method to use in which scenario?",
            "summary_of_the_review": "The paper works in an interesting direction and shows some promising results, but the method itself and how it works is unclear. Several methods are proposed, but without a clear winner or recommendation of which method to apply for future work.\n\nAs it is, I do not think the paper is sufficiently ready for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4349/Reviewer_G6tK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4349/Reviewer_G6tK"
        ]
    },
    {
        "id": "kkei2rSf6fv",
        "original": null,
        "number": 2,
        "cdate": 1666611093244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611093244,
        "tmdate": 1666611093244,
        "tddate": null,
        "forum": "nMZhFqYsiad",
        "replyto": "nMZhFqYsiad",
        "invitation": "ICLR.cc/2023/Conference/Paper4349/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a model based safe-RL algorithm that uses custom planners belonging to the family of Quality Diversity methods. Three such custom planners are proposed. The resulting RL approach is tested in three different simulation setups, with randomized safety constraints. ",
            "strength_and_weaknesses": "Strengths:\n\n* The introduced QD-planners are extensively tested in 3 different environments against several competing approaches.\n\nWeaknesses:\n\n* The paper does not introduce a new method per se, as in a new model-based RL algorithm that e.g. learns a model in a particular way, or that implements a new model structure. Rather, three ad-hoc QD-planners are introduced but not analyzed rigorously/theoretically: the heuristics are shown to work well in the chosen experimental scenarios, but what are the failure points? When would they be suboptimal and when could the resulting policies violate safety constraints? These are not discussed theoretically or verbally.\n\n* As the authors themselves mention at the conclusion, \"if the model is wrong, it could easily lead the agent to unsafe states\". Indeed the focus of model-based RL approaches introduced to safety critical environments, need to consider very critically how the learned model could mislead the agent, or how to learn robust models that do not do so. I would suggest the authors to look up the literature on robust control or the more recent uncertainty-aware learning controllers (using GPs, etc.).\n\n* If the contribution of the paper is not a new model-based RL agent but rather the custom planners, then as mentioned above, they would need to be analyzed critically and rigorously, e.g. as is often done in the MPC literature. I would suggest the authors to look up e.g. stochastic/robust/learning MPC formulation.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper suffers from lack of clarity, as to what is the real contribution. I think that methodologically there's no significant contribution of the paper - but the novelty factor perhaps comes from the experiments and how they're set up (e.g. randomized safety constraints). If that is indeed the case, maybe the authors should refocus the paper - title, abstract and conclusions need to be adapted accordingly. \n\nSome minor comments below:\n\n* Using discounted return is problematic for safety-critical applications on real systems without a clear horizon. \n* \"The model is trained and evaluated on the real-system\" is misleading as the experiments are simulation only.\n* page 4: use -> learn?\n* Initial rollouts in Algorithm 1 are unsafe.\n* What distinguishes \"deterministic deep mixture density networks\" as a good choice for safety-critical control applications?\n* How do you fix N and h in your planners? Is it critical?\n* What is the motivation of introducing a Pareto-Safe version of S-ME in Section 4.2.3? Please discuss.\n* TRPO lag and PPO lag stand for I guess TRPO/PPO used with Lagrangian relaxation?\n* What is meant by \"behaviour space\" is not clear to me, do you mean the specific way the QD-approaches evaluate and pick the rolled-out trajectories?",
            "summary_of_the_review": "Overall the paper should be rejected as it is lacking a clear methodological contribution, see comments above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4349/Reviewer_WaLB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4349/Reviewer_WaLB"
        ]
    },
    {
        "id": "zYmR1pqzvd",
        "original": null,
        "number": 3,
        "cdate": 1666626943996,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626943996,
        "tmdate": 1666626943996,
        "tddate": null,
        "forum": "nMZhFqYsiad",
        "replyto": "nMZhFqYsiad",
        "invitation": "ICLR.cc/2023/Conference/Paper4349/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method to obtain safe model-based RL by using a guided safe shooting method (with variations) and applies it to 3 toy tasks.\n\nThe paper a quality-diversity method as an optimization method method with some variations for safe planning. ",
            "strength_and_weaknesses": "Strengths:\n- important subject\n- bringing quality-diversity planners into MBRL\n\nWeaknesses:\n- missing baselines\n- no statistical analysis\n- only toy environments\n- not really clear which improvement the decisive one \n- too small fonts in the graphics\n\nDetails:\nSince you are not dealing with uncertainty while planning, I am wondering what about a standard approach where the costs term is simply substracted from the reward with a big constant factor (referred to as \"big M\" approach in control). This should be a baseline: so ME with this r - M*c as a reward with large M. Then we would see whether the particular optimization improvements that you do really are crucial. How many rounds of ME are you performing, every timestep. Is there any shift initialiation etc?  \n\nI suspect that most of the performance gain of the paper comes from the diversity term in the ME optimizer. However, there was prior work [1] that does all of this already: epistemic uncertainty to collect data needed for model learning, safety awareness including aleatoric uncertainty (which is not so important here, as you have only deterministic and simple environments) and probabistic safety constraints, also in highdimensional control problems.\n\nI cannot find the description of the \"behavior space\" used. \n\n[1] Risk-Averse Zero-Order Trajectory Optimization, CoRL 2021\nhttps://openreview.net/forum?id=WqUl7sNkDre",
            "clarity,_quality,_novelty_and_reproducibility": "The fonts in the graphics are too small. Also the line colors are not easy to tell apart.\n\nWhat means the bolding in Table 1? It seems that some of the numbers would not be significant. You use 3 and 5 seeds and then compute 90% confidence intervals, which is likely not very trustworthy.\n\nCode is provided and details are given, so I think reproducibility is high.\n\nNovelty: The paper is novel for the Map Elite shooting method being applied to MBRL",
            "summary_of_the_review": "The paper looks at an important problem, but in the current state, as the paper is empirical, the evaluation is limited to pretty simple environments. Also a simple baseline, for identifying which part is really important is missing. Apart from that, there is a prior work that was not mentioned that solves the tackled problem.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4349/Reviewer_5gos"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4349/Reviewer_5gos"
        ]
    },
    {
        "id": "mxknkF3P-w",
        "original": null,
        "number": 4,
        "cdate": 1666869390850,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666869390850,
        "tmdate": 1666869390850,
        "tddate": null,
        "forum": "nMZhFqYsiad",
        "replyto": "nMZhFqYsiad",
        "invitation": "ICLR.cc/2023/Conference/Paper4349/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper takes a particular view on safe reinforcement learning in the form of model-predictive control. In particular, the authors present an approach, called \u2018guided safe shooting (GuSS), that combines model-based planning with RL, towards a minimal violation of safety. Technically, a constrained MDP approach is realized, together with three different planning methods. The approach is evaluated by means of three OpenAI environments.",
            "strength_and_weaknesses": "Strengths\n\n- The topic is very relevant and well-fitting for ICLR.\n- The concept of combining model-based control with RL is natural. \n- The evaluation considers state-of-the-art environments.\n\nWeaknesses\n- The novelty of the approach is unclear, and the related work is not discussed appropriately.\n- The paper seems a bit unpolished at parts. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper largely easy to read, and the approach is clearly defined. Taking a model-based approach to safe RL has been discussed at lengths in the literature, and is well-motivated. The results seem solid, and likely they are possible to be reproduced. \n\nI am however strongly concerned about the novelty. A simple google search reveals a number of papers that combine model-predictive control with (safe) reinforcement learning, and none of these papers are cited. Moreover, a paper that is cited [Liu et al ]and declared very related, is not discussed in detail. From the current status, I tend to say that the novelty is absolutely unclear, but I will be happy to read the answers of the authors and re-evaluate my assessment. See below a number of papers that I found very quickly, but I am sure that there are more, especially in the controls community. \n\nSamuel Pfrommer, Tanmay Gautam, Alec Zhou, Somayeh Sojoudi:\nSafe Reinforcement Learning with Chance-constrained Model Predictive Control. L4DC 2022: 291-303\n\nTorsten Koller, Felix Berkenkamp, Matteo Turchetta, Andreas Krause:\nLearning-based Model Predictive Control for Safe Exploration and Reinforcement Learning. CoRR abs/1803.08287 (2018)\n\nAlexandre Didier, Kim Peter Wabersich, Melanie N. Zeilinger:\nAdaptive Model Predictive Safety Certification for Learning-based Control. CDC 2021: 809-815\n\nAs a further remark to the authors, they should consider discussing the area of \u2018shielded RL\u2019, which naturally takes a model-based approach. See (only some of the many) references below.\n\nMohammed Alshiekh, Roderick Bloem, R\u00fcdiger Ehlers, Bettina K\u00f6nighofer, Scott Niekum, Ufuk Topcu:\nSafe Reinforcement Learning via Shielding. AAAI 2018: 2669-2678\n\nNils Jansen, Bettina K\u00f6nighofer, Sebastian Junges, Alex Serban, Roderick Bloem:\nSafe Reinforcement Learning Using Probabilistic Shields. CONCUR 2020: 3:1-3:16\n",
            "summary_of_the_review": "Good idea and approach, but the novelty is in doubt. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4349/Reviewer_uMvM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4349/Reviewer_uMvM"
        ]
    }
]