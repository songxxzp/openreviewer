[
    {
        "id": "KXQVKEU9uq",
        "original": null,
        "number": 1,
        "cdate": 1666643123166,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643123166,
        "tmdate": 1666643123166,
        "tddate": null,
        "forum": "J7CTp-jNyJ",
        "replyto": "J7CTp-jNyJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1873/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The proposed paper aims to introduce a framework for validating neural networks with synthetic data. Specifically, the discussed method proposes to train a generative model trained on the commonly used test data to generate a larger and more diverse synthetic dataset. The generated synthetic data is then used to more carefully validate a downstream task model. The argument of the paper is that validating on synthetic data can help lowering the variance of common real validation datasets and to help conditioning generative models to better understand how these models behave w.r.t. different target settings. The paper suggests that validating with synthetic data leads to more accurate performance estimates compared to using real test data alone. ",
            "strength_and_weaknesses": "Strengths:\n\n- The paper addressees an important and interesting problem. \n- The reasoning behind the proposed method seems technically sound and the experiments toward granular evaluation and distributional shifts seem reasonable. \n- The paper is well-written and easy to follow. \n- Limitations of the method are discussed an an appropriate and fair manner. \n\nWeaknesses: \n\n- The paper only focuses on tabular data for which is rather easy to synthetzise data (compare to text and images). ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well-written, easy to follow, and the topic it addresses is important and interesting. Using synthetic data to validate machine learning models more carefully and w.r.t. distributional shits seems to be an important direction. I much appreciate that the paper provides a candid discussion toward the limitations and the scope of the proposed approach. \n\nThe fact that the method currently only works on tabular data indeed is a strong limitation. Given that there exists severe differences between real and synthetic data in other domains (e.g. for images), it is not clear that the method could easily extend to these areas. The paper also discusses the limitation that generative models for synthetic data may fall short in approximating the real data appropriately. Towards this topic I would appreciate a more in-depth discussion compared to what is currently provided. E.g. I would appreciate pointers to existing work that explores how well generative models are able to approximate tabular data. \n\nGiven that the paper introduces a framework rather than an algorithm, enough information is provided to replicate the proposed approach. ",
            "summary_of_the_review": "Overall, this is an interesting paper that can be accepted to ICLR 2023. The topic this paper address is important and novel, the paper very well written, and the conducted experiments are sound. Furthermore, the paper carefully discusses the scope and limitations of the proposed method. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not have toward ethical considerations. ",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1873/Reviewer_NW99"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1873/Reviewer_NW99"
        ]
    },
    {
        "id": "SRon-BQw74",
        "original": null,
        "number": 2,
        "cdate": 1666657586278,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657586278,
        "tmdate": 1668539127963,
        "tddate": null,
        "forum": "J7CTp-jNyJ",
        "replyto": "J7CTp-jNyJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1873/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an evaluation suite, which generates synthetic test set that helps to better evaluate the performance of supervised learning models for small subgroups (minority groups) and distributional shifts. Augmenting and creating synthetic data enable us to increase the number of data in small subgroups which help to decrease the variance resulting from evaluation metric's performance. Also adding shift to the test distribution make the model more reliable to be tested on different domain shiftings that would happen by testing in different test sets.",
            "strength_and_weaknesses": "The issues behind the evaluation on small test set is written clearly. The thorough experiments and analysis in the paper and appendix make the idea more convincing. \n\nHowever, the actual use-cases of such evaluations specifically for small subgroups is not very clear. Is this type of evaluation on synthetic test set is good if we want to get the performance of the metric on small subgroups separately? Since according to table 1 on majority samples the metric tested on D_test,f  is more reliable. Therefore some explanations on why do we want to evaluate on small minorities is beneficial.\n\nIn the proposed approach to generate synthetic test set, the generator is trained on a small test set. What is the effect of such a small test set that is used as training dataset on the generator's performance, the newly generated synthetic data and subsequently the performance of the evaluation metric. In other words, from low number of test set how much the generative model is able to learn the manifold from D_test,f.\n\nIt is not clear that how much the proposed distributional shiftings in generated synthetic test set are close to the shiftings that could happen during testing.\n\nSimilar to section 5.2.1 incorporating some examples in real-word test cases in 5.2.2 also can be very useful.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation, problem statement and the idea of using synthetic data for test sets looks novel and quite interesting. The positive impact of such evaluation suit on measuring the reliability and robustness of the evaluation metric can be very beneficial to the research community. Overall, paper is written quite neat however some further explanations can transfer the overall idea and purpose of the paper easier. ",
            "summary_of_the_review": "This paper and the idea of generating synthetic data and leveraging them for testing classification model can be promising in identifying  reliable metrics. However, there are some weaknesses that mostly are coming from not clear descriptions which should be addressed.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1873/Reviewer_kuUj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1873/Reviewer_kuUj"
        ]
    },
    {
        "id": "aq-44dASTPH",
        "original": null,
        "number": 3,
        "cdate": 1666714728902,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714728902,
        "tmdate": 1666801468259,
        "tddate": null,
        "forum": "J7CTp-jNyJ",
        "replyto": "J7CTp-jNyJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1873/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a framework for using synthetic data to test predictive models for their performance on sparsely populated subgroups and on robustness under distributional shift. The framework focuses on tabular data. ",
            "strength_and_weaknesses": "Strengths:\n\nLeveraging synthetic data for AI testing is timely and critical for many tasks, which is the focus of this work.\n\nThe paper is clearly written and easy to follow. \n\nThe paper considers two scenarios \u2013 (1) when the original data suffers from sparse population of important subgroups and (2) when there is a distributional shift between training data and test data. \n\n\nWeaknesses:\nThe idea of using synthetic data to evaluate ML models is not entirely new. \"Synthetic Data for Social Good\" arXiv:1710.08874 has proposed  DataSynthesizer, a privacy-preserving synthetic data generator for creating pathological dataset in tabular domain to be used for model testing.   AITEST described in \u201cData Synthesis for Testing Black-Box Machine Learning Models\u201d (arXiv:2111.02161) also considers group fairness testing as studied in this paper and leverages goal-oriented synthetic data generation in tabular domain for model testing. AITEST has used data constraint-based data synthesis, in addition to using CTGAN and TVAE as baselines, whereas the current work mainly uses CTGAN. Syng4me considers distribution drift for robustness testing whereas AITEST studies adversarial robustness. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: OK\nNovelty: low\nReproducibility: low (no code shared)",
            "summary_of_the_review": "While the paper pursues an important direction, the framework, the methods and the results presented do not provide enough novel insights on using synthetic data for testing fidelity, utility, fairness, and privacy of downstream ML models. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1873/Reviewer_DdMu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1873/Reviewer_DdMu"
        ]
    },
    {
        "id": "tZdu-A63_dW",
        "original": null,
        "number": 4,
        "cdate": 1666843998385,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666843998385,
        "tmdate": 1666843998385,
        "tddate": null,
        "forum": "J7CTp-jNyJ",
        "replyto": "J7CTp-jNyJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1873/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose in this paper to use synthetic data for model evaluation and to create an automated suite of synthetic data generators (called SYNG4ME). SYNG4ME, in particular, has two main properties: 1) it provides reliable granular evaluation, and 2) it quantifies model sensitivity to distributional shifts. The paper discusses various SYNG4ME cases, which will provide some insights into using synthetic data for model evaluation.",
            "strength_and_weaknesses": "Strength :\n\n1. The problem studied in this paper, i.e., using synthetic data for model evaluation, is interesting and important.\n\n2. An adequate discussion of SYNG4ME use cases is provided, providing some insights into how to use synthetic data for model evaluation.\n\n3. The entire paper is simple to understand. The authors thoroughly review all related work, summarising its benefits and drawbacks.\n\nWeaknesses:\n\n1. Although the problem studied in this paper is intriguing, it is not a novel one that has received much attention in the machine learning and computer vision communities. Using synthetic data to evaluate models is also not a new topic. On the other hand, the technology employed in the paper is not particularly novel.\n\n2. As stated in the paper, SYNG4ME is only capable of handling tabular data. I believe its application scope is too narrow.",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity and Quality: The paper is clearly written.\n\nNovelty: The problem studied in this paper is interesting, but it has been widely discussed in the machine learning and computer vision community. Meanwhile, using synthetic data for model evaluation also is not a new topic, and the technology used in the paper is not very innovative.\n\nReproducibility: I think this paper is easy to reproduce.\n",
            "summary_of_the_review": "This paper may provide some insights into using synthetic data for model evolution, but I believe it will have little impact on the community (the novelty is limited).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1873/Reviewer_L3gg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1873/Reviewer_L3gg"
        ]
    }
]