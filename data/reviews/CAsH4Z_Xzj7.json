[
    {
        "id": "uniVsvEo5d",
        "original": null,
        "number": 1,
        "cdate": 1666639041541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639041541,
        "tmdate": 1666639041541,
        "tddate": null,
        "forum": "CAsH4Z_Xzj7",
        "replyto": "CAsH4Z_Xzj7",
        "invitation": "ICLR.cc/2023/Conference/Paper5948/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper empirically looks at how changing the neural network architecture can impact continual learning performance. For example, if changing width/depth of the network, or adding BatchNorm layers, or adding skip connections, changes how well the network can learn continually. Many experiments are performed on Split CIFAR-100 and Split ImageNet-1k, which is a large-scale benchmark. Recommendations are made based off empirical results. ",
            "strength_and_weaknesses": "**Strengths**\n\n1. Many experiments are run, including on ImageNet-1k, which is large-scale for continual learning.\n\n2. The paper is well-written: it reads well and the paper structure is good. \n\n3. I am not aware of many other papers that look at how changing the architecture impacts continual learning performance (aside from the papers cited in the Introduction of this paper). \n\n4. I liked the style and content of the 'Limitations' paragraph in the Introduction.\n\n5. I like the result of Figure 1a: ResNet-18 without GAP layer but with fine-tuning outperforms EWC (and ER with 1000 samples) on standard ResNet-18. \n\n\n**Weaknesses (especially first 3 points)**\n\n6. This paper (almost) exclusively uses fine-tuning as the 'continual learning' algorithm to test different architectures. This is a problem because I am not sure if results with fine-tuning translate to results with other algorithms like EWC, ER, or more recent algorithms. As a random representative example: perhaps, using EWC with skip connections improves performance considerably over EWC without skip connections? Other algorithms are considered in Figure 1a and Appendix B1, but I believe such results need to be throughout the entire paper. In Appendix B1, the authors say that 'a good architecture can complement a good algorithm in continual learning': please convince me that this is indeed the case! \n\n7. The authors draw a lot of conclusions from changing an architecture choice and looking at continual learning metrics. For example, with skip connections and pooling layers, they say that there is not much improvement in continual learning. However, I find this misleading as the joint accuracy on this benchmark also does not improve significantly. Previous works (cited in the paper) argue that these architecture changes are important for image classification / joint accuracy in general, but this is clearly not the case with the benchmark used. So I cannot tell if these architecture changes are helpful or not in continual learning: ideally, they would help in joint accuracy before drawing conclusions about continual learning. For MaxPool, the joint accuracy increases by a similar amount as the average accuracy. (I note that the authors do not claim anything specific in the text, but I view these results as not very helpful/useful when it comes to architecture design for continual learning.) \n\n8. Following up from this point, the 'joint accuracy' baseline is missing in many of the tables/results. I want to *always* see if joint accuracy increases with an architecture change, and how this change correlates with the continual learning average accuracy change. Else, any benefits in changing the architecture could just be due to the joint accuracy increasing, which says the importance of this architecture change to the specific datasets, but not to the ability of a network to perform a good stability-plasticity trade-off in general. \n\n9. Section 3.1 is about width and depth of neural networks, which was already studied in Mirzadeh et al. There is no new insight or takeaway in this paper. I agree that this paper's results verify the same conclusions on more benchmarks and architectures, however, this is not too significant by itself. \n\n10. Removing GAP in Table 5 improves results, but also drastically increased number of parameters. This is an interesting result but can the authors dig deeper? For example, CNNx8 has worse average accuracy than CNNx4 (16x) + GAP, despite having twice the pre-classification width. The results do not seem conclusive enough to me that GAP only reduces performance because it reduces the last-layer width. ",
            "clarity,_quality,_novelty_and_reproducibility": "- Very good clarity of writing. \n- Experiments are mostly well-run in what they do (with hyperparameters reported). But some baselines and comparisons are missing that I view as crucial (Joint accuracy baselines for all tables, as well as significantly more results with a continual learning algorithm that is not fine-tuning). \n- I am not aware of papers specifically looking at architecture choices such as BatchNorm and Skip connections for continual learning. Depth and width of network has been considered before at a smaller scale. ",
            "summary_of_the_review": "Unfortunately points 6, 7, and 8 in weaknesses are very important for me and therefore I recommend reject. I am not convinced that these results will hold for continual learning algorithms that are not fine-tuning. I also need to see and compare all results with Joint accuracy. Therefore I do not agree with many of the claims made in the abstract. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5948/Reviewer_b2F4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5948/Reviewer_b2F4"
        ]
    },
    {
        "id": "4RmNjFCs25U",
        "original": null,
        "number": 2,
        "cdate": 1666691964431,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691964431,
        "tmdate": 1666691964431,
        "tddate": null,
        "forum": "CAsH4Z_Xzj7",
        "replyto": "CAsH4Z_Xzj7",
        "invitation": "ICLR.cc/2023/Conference/Paper5948/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the architecture choices in the context of countinual learning (CL). It \n\n1) compares different popular architectures\n2) study component choices for these architectures, such as width/depth, BN, skip, pooling etc.\n3) draw conclusions from the study to make pratical suggestions.\n\nThe experiments are on Rotated MNIST, CIFAR-100 Split and ImageNet-1k Split.\n",
            "strength_and_weaknesses": "Strength:\n\n1. Well-organized goals and supportive experiments. It is a pleasure to read this paper because all of the factors that the author wish to study are well organized, with direct experiments to support the conclusions. Experiments are particularly supportive because 1) the metrics chosen (average forgetting, average accuracy, learning accuracy) are meaningful for CL; 2) direct controlled experiments to study each individual factor.\n\n2. Great pratical suggestions summarized for researchers to be able to use the conclusions in this paper. For example, from the paper, it is clear that \"ResNets and WRNs have better learning abilities, whereas CNNs and ViTs have better retention abilities. Simple CNNs achieve the best trade-off between learning and retention.\"\n\n3. This architecture study can complement algorithm study. From Appendix B.1. It shows briefly that CL algorithms can further improve from the arhictecture that is chosen by this paper.\n\n\n\nWeakness:\nNot exactly weakness, but I am curious of the conclusions with other more recent architectures, and other tasks. The authors do not need to provide these in the rebuttal. If the evaluation pipeline code can be released publicly, it would be even more impactful (other researchers can evaluate those new architectures and contribute). \nAnother thing is that how CL algorithm and architecture impact each other is also an interesting future work, from a pratical view.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the paper is exceptionally good. The authors were able to clearly state the goals and evidence to support claims accurately. The structure of the paper is well done and easy to follow. \n\nNovelty wise, there is no particular new method that is proposed. The main novelty lies on the new analysis/insights that are given by the paper. To the best of my knowledge, such good analysis is new. That is, the empirical novelty of this paper is high. \n\nReproducibility: the paper and appendix gives enough details of the implementation, hyper-parameters. I think it should be not difficult to reproduce the results.",
            "summary_of_the_review": "Overall the paper is well-written. The studies done is novel and very informative. In particular, authors draw conclusions that have good empirical evidence and also point out things that do not have enough evidence to support. The suggestions given are also pratically useful for other researchers. I recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5948/Reviewer_4mDG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5948/Reviewer_4mDG"
        ]
    },
    {
        "id": "ZMJGhIju79",
        "original": null,
        "number": 3,
        "cdate": 1666718610198,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666718610198,
        "tmdate": 1666718610198,
        "tddate": null,
        "forum": "CAsH4Z_Xzj7",
        "replyto": "CAsH4Z_Xzj7",
        "invitation": "ICLR.cc/2023/Conference/Paper5948/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work looks at empirical analysis of the effect of architecture choice and components on continual learning performance and draws a few insights on best practices for architecture selection.",
            "strength_and_weaknesses": "Strengths:\n\n\u2022\tArchitecture choice is a key component of the modeling process and can have a considerable effect on the continual learning performance. This is an important area of research that needs to be considered. \n\n\u2022\tThe experiments within the considered architectures have been done thoroughly with multiple random initializations, and corresponding best hyperparameters. The datasets are also well suited for the analysis did in this work.\n\n\u2022\tIt is interesting to see the effect of various architecture components such as width, depth, batchnorm, skip connections, pooling layers and attention heads.\n\nLimitations:\n\n\u2022\tOnly Task-incremental has been primarily studied and not the class-incremental learning setting which is a harder problem\n\n\u2022\tThe effect of width and depth have already been studied in Mirzadeh et al. 2022, and effect of normalizing schemes has been \nstudied in Pham et al 2022, effect of distribution shift is partially studied in Paul and Chen 2022, with these having similar conclusions as this work.\n\n\u2022\tEffect of skip connections is not fully developed. For example, skip connections can be applied at different spans of layers. It is not clear what is used in this work.\n\n\u2022\tThe explanation of why max pooling helps is speculative, visualizing the activations might be helpful instead of speculating that low-level feature might have been effectively learned and helped generalization.\n\n\u2022\tMajority of the work looks at na\u00efve training in the continual learning setting but the effect of the architecture choice on various classes of continual learning is not fully developed (table 8). I was expecting to see how for example the architecture choices effect the experience-replay methods etc., which will be more useful for future algorithm development. The approaches considered in table 8 are also not the state of the art. If the state-of-the-art approaches are not sensitive to architecture choice then this work will not be useful.\n\n\u2022\tNot clear how the hyperparameter search is done differently for continual learning compared to the single-task learning. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality of the work is good but more work needs to be done in regards to novelty and reproducibility",
            "summary_of_the_review": "Effect of architecture choice on continual learning is an important area of research. A more thorough analysis needs to be done especially connecting the architecture sensitivity to the classes of continual learning approaches to make this work actionable and improve the novelty aspect which currently is less due to similarity with other sensitivity analysis works",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5948/Reviewer_y8Pr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5948/Reviewer_y8Pr"
        ]
    }
]