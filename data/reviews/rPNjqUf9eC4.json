[
    {
        "id": "lxhLqTUnl_6",
        "original": null,
        "number": 1,
        "cdate": 1666594027774,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594027774,
        "tmdate": 1666636167428,
        "tddate": null,
        "forum": "rPNjqUf9eC4",
        "replyto": "rPNjqUf9eC4",
        "invitation": "ICLR.cc/2023/Conference/Paper1098/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces the data lottery ticket hypothesis, which intends to select a subset that matches the performance of the original dataset. This hypothesis, if it works, can reduce training costs and democratize self-supervised pretraining. The authors first suggest that a good data lottery ticket subset should match the original performance in training, generalization, and model consistency. Based on these principles, the authors question the random sampling approach and propose the policies of Empirical Risk Trend and incorporate prior knowledge to generate the winning tickets. The analysis is demonstrated with a suite of self-supervised experiments with various structures (DINO, MAE, MOCOV). \n\n",
            "strength_and_weaknesses": "Strengths: \n1. Data lottery ticket is an important issue and investigating the lottery ticket problem from the view of training data is quite novel. Although data pruning often achieves the same effect as the data lottery ticket, data pruning has a different motivation that aims at removing biases and noises. \n\nWeaknesses:\n1. The definition of the data lottery ticket problem is ambiguous. The author stated in Definition 1 that \"a subset that has the same or similar empirical behaviors and performance trends as the original full dataset when performing different training approaches and hyperparameters on it\". \n-  It lacks an elaboration on \"performance trends\" which can be pre-trained performance, training error, generalization error, downstream performance, etc.\n-  The performance trends involve the selection of models. However, different pools of models may have different performance result trends. The author didn't give a clear definition as to how these training structures and training approaches should be chosen. \n- A formal definition could be given instead of a text description for better clarity. \n\n2. The motivation for choosing a lottery ticket with a consistency principle is ill-justified. For example, the authors stated in 3.2 that \" Some trends are inconsistent, for instance, from subset RS-3 to RS-4, the accuracy increases on DINO ViT-Base/16 while decreases on MAE and MoCo\" and use this to support their claim that uniform sampling lacks consistency. However, the models are tested via linear probing and the increase in model hyperparameters may result in overfitting or instability during fine-tuning and lead to these performance drops. Overall, this inconsistency may be the result of the difficult balancing of model size and subset size. It does not necessarily reflect the trend of the expressiveness of the models.\n\n3. I'm doubtful about the training settings. The authors use full-dataset trained backbones to calculate test accuracies and only test if the performance trends are consistent. In most applications, users are more concerned about the performance achieved by directly tuning their model on the subsets. The paper lacks a discussion on this more practical setting. It seems that section 4.2 discusses this subject but only approximate numbers are provided in the radar char and lack precise results. \n\n4. The class semantic-based methods for choosing lottery tickets are hard to apply beyond labeled image datasets. For example, many pre-trained models are using crawled images without labels. In these cases, the authors would not have the label semantics that is crucial for constructing lottery ticket subsets. \n\n5. The experiment is overall imprecise and missing important settings. \n- Most results are provided as visualization and charts, which are not precise enough. Eg., in Figure 4, the trend of DINO_base and DINO_small is plotted as line chart (performance vs set ID ), yet the scale for set ID is set to 2, making analyzing the trends between RS-3 and RS-4 difficult. Also, in Figure 7, no precise number is given and it is confusing what the scale \"2,4,6,8,10\" means. \n- I'm not convinced by the results that inconsistency is an issue. As shown in Table 2, the Pearson correlation reaches 0.893 using random sampling subsets. After using some simple subset selection procedures, this consistency rises to 0.944. \n- The training accuracy for direct learning on the subsets is missing, as stated in 3. \n- The reproducing details are missing. I can't find the code or the hyperparameters for these experiments. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing of this paper is overall easy to understand but the problem definitions and the experiment settings are confusing. \nNovelty: The data lottery ticket problem is not so different from data pruning though following a different motivation. The methods to choose winning tickets seem novel to me.\nReproducibility: The reproducing details are missing.",
            "summary_of_the_review": "I lean to reject this paper mainly because (1) the consistency principle is not well justified and could be much influenced by subset size and model structures, (2) the algorithm is heavily dependent on class labels and semantics, which may not be available in many datasets, including most NLP tasks where self-training is most heavily used (3) the overall presentation of the paper can be more organized and polished, (4) the reproducing details and the codes for the experiments are missing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_gNgq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_gNgq"
        ]
    },
    {
        "id": "1pqkOj-nNn",
        "original": null,
        "number": 2,
        "cdate": 1666639198232,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639198232,
        "tmdate": 1666639198232,
        "tddate": null,
        "forum": "rPNjqUf9eC4",
        "replyto": "rPNjqUf9eC4",
        "invitation": "ICLR.cc/2023/Conference/Paper1098/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the dataset lottery tickets hypothesis in the context of self-supervised learning. The main hypothesis is that one can identify a subset of the data which has empirical trend similar to the full dataset, i.e. hyperparameters that have better performance on the subset also show better performance on the full dataset.\n\nAuthors propose several methods to find dataset subsets using a SSL pretrain model and potential label information. Authors empirically investigate which approach can identify winning tickets on the ImageNet dataset.\n",
            "strength_and_weaknesses": "Strength:\n- The paper has sound motivation. SSL research involves a non-trivial computational cost, and it is important to reduce this compute burden to stimulate more research in this area.\n\nWeaknesses:\n- It is unclear if the SSL models are retrained on the \u2018winning\u2019 subset. If the models are not retrained, this would be a major limitation as model trained on the subset might not have empirical trends similar to the models trained on the full dataset.\n- Paper clarity could be improved. I found that the paper was hard to follow. For instance, some important details are missing in the ticket policies description (how do you select the number of classes in PD-{Top,Bottom,\u2026), how do you select the merging order and stopping criteria in WNH-ID. Same question for WEC/SEC. The NIR-ID scheme is not explain in the main text. Figures are also hard to understand. For instance, the x-axis in Figure 6 is not explained. It would be nice to provide more details in the caption and explain the main take-away from the figures. \n- If the SSL model are retrained on the smaller subset, what are the computational gain provided by the approach.\n-Authors should compare with dataset pruning approaches as they might already find winning lottery tickets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity of the paper needs to be improved before publication. In addition to the points mentioned before:\n- The introduction discusses the issue of overfitting while referencing to Figure 1. However, Figure 1 only shows training loss. Additionally, the qualitative difference between the winning ticket and random subsets is not clear from the plots.\n- Author should expend the discussion with respect to dataset pruning. Why a dataset pruning method would not allow to find a lottery ticket?\n- How is the number of classes selected in the different methods?  \n- Are the model performances trained on the subset stable for the different classes with respect to the baseline?\n",
            "summary_of_the_review": "The paper investigates the question of dataset lottery tickets for self-supervised learning approach. While the topic is interesting and important, the paper clarity needs to be improved.\nIn particular, it is unclear if the models are re-trained on the dataset subset in the empirical evaluation. \nComparisons with dataset pruning approaches should also be added to better demonstrate the significance of the work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_xj8W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_xj8W"
        ]
    },
    {
        "id": "oJvJ0SRvcBH",
        "original": null,
        "number": 3,
        "cdate": 1666644257197,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644257197,
        "tmdate": 1666644354430,
        "tddate": null,
        "forum": "rPNjqUf9eC4",
        "replyto": "rPNjqUf9eC4",
        "invitation": "ICLR.cc/2023/Conference/Paper1098/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper generalizes Lottery Tickets Hypothesis (LTH) to the subset selection domain, by defining a Dataset Lottery Ticket as a subset that has the same or similar empirical behaviors and performance trends as the original full dataset, which can be identified by some specific approaches (e.g., WordNet Hierarchy). It provides a novel problem that studies the possibility of identifying the subset which can reflect the performance consistency with the full data.",
            "strength_and_weaknesses": "Pros:\n1. The paper is well-written and easy to follow. \n2. This paper provides a novel perspective on sub-dataset selection.\n3. This paper empirically reveals that a randomly selected subset without any prior knowledge is unstable and generally not qualified for reflecting the properties of self-supervised models on the full data. This helps us to use a more reasonable way to screen the important samples of the dataset without misconceptions.\n\nCons:\n1. My major concern is why should we guarantee that the subset categories have consistency in performance similar to that of the full dataset. This is the key contribution/target of this manuscript, and the authors provide an example to explain this issue. However, I am still not sure why we should not select these inconsistent classes in the classification task. For example, can we select some samples which have similar empirical behaviors and performance trends in these classes rather than removing these classes? Dataset Lottery Ticket Hypothesis (DLTH) that recklessly retains all the samples in consistent classes seems to contradict the target of accelerated training.\n2. It seems that only a few establishments (e.g., Google, Meta, etc.) can afford the heavy experiments on large-scale datasets training. This issue is caused by large-scale datasets and the over-parameterized networks that go with them. This paper follows a train-select-retrain process, which is hard to reduce the loss of training resources caused by hyper-parameter adjustments (the full dataset has to be fed to the network for training and testing at the beginning). If it is a misunderstanding, please feel free to correct it.\n3. This manuscript utilizes empirical risk minimization for evaluating model performances. However, in the case of such a dataset with a large network, serious overfitting often occurs after the dataset is screened. It seems more reasonable to use structure risk minimization to overcome the overfitting problem (e.g., through weight decay) and adjustment of the penalty factor should be a consideration.\n4. This manuscript finds the LTH property on datasets, but only verifies this hypothesis on the vision domain, i.e., ImageNet-1K. This is not very convincing although the paper adopts a variety of recent mainstream backbones and baselines. More datasets are appreciated for conducting experiments.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written with reasonable logic. The problem it studies is novel to me. All experiments are repeated multiple times and std-deviations are reported which helps with judging the results. The definition is clear, and the algorithm is easy to follow. Experiments and findings are well presented. Experiments and the conclusions drawn appear to be correct to me \u2013 though, I can't say whether this hypothesis holds for other datasets because the authors didn't use more datasets for evaluation.",
            "summary_of_the_review": "Considering the pros and cons of this paper, I think this paper is good but still has a large space to improve, e.g., more datasets for evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_kkgk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_kkgk"
        ]
    },
    {
        "id": "_T05JvcB6g",
        "original": null,
        "number": 4,
        "cdate": 1666650717532,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650717532,
        "tmdate": 1666650717532,
        "tddate": null,
        "forum": "rPNjqUf9eC4",
        "replyto": "rPNjqUf9eC4",
        "invitation": "ICLR.cc/2023/Conference/Paper1098/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes the Dataset Lottery Ticket Hypothesis(DLTH), a novel problem that studies the possibility of identifying the subset which can reflect the performance consistency with the full data. By Empirical Risk Trend, this work demonstrates the existence of dataset-winning tickets. And extensive experiments are conducted across various self-supervised frameworks, which verify the effectiveness and superiority of the proposed dataset-winning ticket policies.",
            "strength_and_weaknesses": "### Strength:\n\n1. This paper is well-written and easy to follow.\n2. The idea of studying the dataset lottery ticket hypothesis is novel and interesting.\n3. Comprehensive experiments on ImageNet-1K across several self-supervised frameworks are conducted, which is a laudable effort.\n\n### Weakness:\n\nI have one concern about this work: for the randomly sampling strategy, what's the performance of randomly sampling partial data points across all classes rather than selecting partial classes, which seems less biased from the full set distribution and serves as a more competitive baseline?",
            "clarity,_quality,_novelty_and_reproducibility": "None",
            "summary_of_the_review": "None",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_PcqN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_PcqN"
        ]
    },
    {
        "id": "RT0VskS5Ha",
        "original": null,
        "number": 5,
        "cdate": 1666700043757,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666700043757,
        "tmdate": 1666792962377,
        "tddate": null,
        "forum": "rPNjqUf9eC4",
        "replyto": "rPNjqUf9eC4",
        "invitation": "ICLR.cc/2023/Conference/Paper1098/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper inspects the dataset lottery ticket hypothesis, where training on just subsets have similar empirical behaviors and performance trends as training on the full set. So that analysis and hyper-parameter tuning can be conducted efficiently. Various sampling strategy with different models are compared, the proposed ERC strategy achieves the best consistency. ",
            "strength_and_weaknesses": "Strength:\n1. The paper is well motivated, building a performance-consistent sub-training method which can be used as an efficient proxy for hyper-parameter tuning is interesting. \n\n2. The method is simple and easy to implement. \n\nWeakness:\n\n1. Talking about performance consistency, more insightful discussion or theoretical analysis may be required to fully justify the proposed method. Utilizing only 7 models is not enough for this claim. \n2. The name \u201cdataset lottery ticket hypothesis\u201d is a little misleading to me. At the first glance, the \u201cdataset lottery ticket hypothesis\u201d may refer to: there exists a subset that can fully retain the model performance.  This paper, however, concentrates on maintaining the relative performance of different models and a few hyper-parameters.\n\n3. Though the proposed ERC strategy is reported can well retain the \u201cperformance trends\u201d. Picking samples at class level is quite coarse. There are many other fine-grained data selection methods, ranging from active learning to efficient model training. Though not primarily designed for maintaining \u201cperformance trends\u201d consistency, comparing and discussing these methods will strengthen the paper. \n\n4. The rightmost 2 figures of Figure 2 seem to be exactly the same. There are also some typos like subsection 4.1 should refer Figure 6 instead of Figure 7. \n\n\nminor:\n1. It seems that \u201cempirical behaviors and performance trends\u201d refers only to the relative accuracy throughout the paper. I recommend outlining this earlier in the narration. \n\nReferences:\n[1] J T. Ash et.al. Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds. ICLR \u201820 \n\n[2] B. Mirzasoleiman et.al. Coresets for Data-efficient Training ofMachine Learning Models. ICML \u201820\n\n[3] G.Citovsky et.al. Batch Active Learning at Scale. NeurIPS \u201821 \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the proposed method is somewhat novel.",
            "summary_of_the_review": "Interesting paper that tries to build a performance-consistent sub-training method. However, there are some issues mentioned in the weakness part. I would like to raise my score if these questions are properly addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_drZn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1098/Reviewer_drZn"
        ]
    }
]