[
    {
        "id": "tje-89VC61",
        "original": null,
        "number": 1,
        "cdate": 1665741003102,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665741003102,
        "tmdate": 1665744500574,
        "tddate": null,
        "forum": "vtVDI3w_BLL",
        "replyto": "vtVDI3w_BLL",
        "invitation": "ICLR.cc/2023/Conference/Paper1430/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The pretrain-(continued-pretrain)-finetune paradigm has been de-facto in recent NLP state-of-the-arts. However, the construction of auxiliary objectives still need carefully design by experts. This paper proposes to automatically generate a suite of auxiliary objectives. Specifically, they provide theoretical bound for the generalization error in auxiliary learning, and design an efficient algorithm for searching the space of generated objectives to find those most useful to a specified end-task.",
            "strength_and_weaknesses": "**Strength**\n* This is the first work that propose to automatically generate auxilliary objectives\n* The experimental resutls seems promising\n\n\n**Weaknesses**\n* The proposed method presents (potentially heavy) extra computational cost\n* The proposed method only run experiments on continued pretraining setting and the datasets are relatively small.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow and is quite novel.",
            "summary_of_the_review": "To the best of my knowledge, this is the first work that propose to automatically generate auxilliary objectives. Moreover, it provides good results both theoretically and practically. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1430/Reviewer_pGCt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1430/Reviewer_pGCt"
        ]
    },
    {
        "id": "SJPfJDrfXD",
        "original": null,
        "number": 2,
        "cdate": 1666695902917,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695902917,
        "tmdate": 1666695902917,
        "tddate": null,
        "forum": "vtVDI3w_BLL",
        "replyto": "vtVDI3w_BLL",
        "invitation": "ICLR.cc/2023/Conference/Paper1430/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes AANG to automatically generate a suite of auxiliary objectives for end tasks. AANG builds a unified framework/pipeline that consists of several parts: input data, input transformation, model representation, and output. AANG uses algorithmic stability to search for the best combination in each part. The experiments are conducted on the NLP domain, and the results demonstrate the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "This paper proposes AANG to automatically generate a suite of auxiliary objectives for end tasks. AANG builds a unified framework/pipeline that consists of several parts: input data, input transformation, model representation, and output. AANG uses algorithmic stability to search for the best combination in each part. The experiments are conducted on the NLP domain, and the results demonstrate the effectiveness of the proposed method. \n\nEven though many techniques used in this paper are proposed by other works, this paper delicately combines them.\n\nThe paper presents a solid theoretical analysis of the proposed algorithm.\n\nIt would be better to extend and deepen the experiment to non-NLP tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "What if there is more than one end task? ",
            "summary_of_the_review": "I think this paper is novel and soild enough. It can meet the acceptance line of ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1430/Reviewer_rBEM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1430/Reviewer_rBEM"
        ]
    },
    {
        "id": "t6aUJ9bXlR",
        "original": null,
        "number": 3,
        "cdate": 1666940146593,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666940146593,
        "tmdate": 1666940653674,
        "tddate": null,
        "forum": "vtVDI3w_BLL",
        "replyto": "vtVDI3w_BLL",
        "invitation": "ICLR.cc/2023/Conference/Paper1430/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an approach using meta-learning to learn to weight multiple auxiliary losses for any given target task. They first define a space of auxiliary losses that contains popular self-supervised methods like BERT and XLNet as special cases. Then they propose to learn the weighting over a given space of such losses using the META-TARTAN algorithm from previous work. Approach is evaluated on 5 classification tasks where the dynamic weight learning on auxilary losses improves over a multi-task baseline on 4 out 5 datasets by.",
            "strength_and_weaknesses": "Strengths\n- An approach to dynamically learn auxiliary loss weighting that could be useful for finetuning pre-trained models on specific tasks.\n- Empirical results include statistical significance, and interesting analysis of how weights vary during training.\n\nWeaknesses\n- The paper was a bit tough to follow. The setting up of the auxiliary loss space was more detailed than it needed to be. In particular, I do not find this setup to be specially illuminating, or inspiring new losses just by its construction (as one would not trivially expect every possible choice of loss in this space to be useful). It would have been better to provide more details in sections 4/5/6. The paper also assumes knowledge of META-TARTAN paper and heavily relies on it.\n- Differences from the META-TARTAN paper are not clear and are not explicitly mentioned. It should be an important baseline but it seems it is not compared.\n- It is not clear if the multi-task baseline is trained in the same way, i.e. with all the 24 losses in the auxiliary space and updating based on a random subset of 24 losses (which introduces some regularization from overfitting). Improvements over the existing multi-task results appear minor.\n- When scaling on the number of auxiliary objectives, the authors find no improvement. This is disappointing as one would expect that trivially taking the cartesian product of the space of choices will yield a large number of uninteresting losses and the approach is incapable of pruning them out.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall clarity of the paper can be improved by providing more details about the training algorithm and a high-level description.\n\nThe approach seems novel, though it builds and relies heavily on META-TARTAN. The new thing seems to be about the change in dynamic weight updates and it would be good to clarify these differences.\n\nCode is not provided, though there are more experimental details and hyper-parameters in supplementary to help with reproducibility.",
            "summary_of_the_review": "I am not entirely convinced of the utility of this method and it lacks some important baseline comparisons (see weaknesses above).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1430/Reviewer_KWWF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1430/Reviewer_KWWF"
        ]
    },
    {
        "id": "JWinE9G6na",
        "original": null,
        "number": 4,
        "cdate": 1667191788531,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667191788531,
        "tmdate": 1667191788531,
        "tddate": null,
        "forum": "vtVDI3w_BLL",
        "replyto": "vtVDI3w_BLL",
        "invitation": "ICLR.cc/2023/Conference/Paper1430/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studied the problem of how to take advantage of auxiliary loss functions in continuously adapting a pretrained language model. The author proposed to automatically pick auxiliary loss functions from a search space. The loss functions are reweighted during the learning process based on how they can help the downstream task. Author conducted experiments in four datasets to demonstrate the effectiveness of the AANG algorithm. Ablation study also shows that the algorithm for updating the weight for each loss function is effective and can pick auxiliary losses that are mostly related to the downstream problem.\n",
            "strength_and_weaknesses": "Strength: \n1. The paper studied a novel problem: trying to automatically pick auxiliary loss functions from a large search-space.\n2. Experiments show that AANG is able to outperform baselines (Table 2). Figure 4 also shows that AANG is able to identify the important loss functions during the training procedure, and the weighted + META-Train variant outperforms assigning equal weights to each loss function.\n\nWeakness:\n1. The computational complexity of AANG is higher due to the meta-learning phase.\n2. The reviewer feels that the author needs to compare with advanced pretraining + distillation algorithms, like \"[NeurIPS2020] MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\". In the paper, the author only compared with GPT / XLNet / BERT objectives. However, these objectives may not be suitable for continuous pretraining and the author needs to compare with stronger baselines.\n3. The final language model adapted via AANG is applicable to a single specific downstream applications. The author may need to study if the final backbone model adapted via AANG can also generalize to other tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well-written and easy-to-understand.\n\n\nQuality: The paper is technically sound. Certain weakness in evaluation is pointed out in the \"Weakness\" section above.\n\nNovelty: The reviewer feels that the automated auxiliary learning problem is novel. The technical that the author adopted for solving the problem is marginally novel because it largely depends on META-TRAIN\n\nReproducibility: Reproducing such type of AutoML-related work is challenging. Since the author has not released the source code, the reviewer is not sure if all the results are reproducible.",
            "summary_of_the_review": "Voted for weak rejection due to concern in experimental evaluation, and the computational complexity of the AANG algorithm. The problem of automatically search for auxiliary learning tasks that are helpful for adapting the model is novel.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1430/Reviewer_oyPz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1430/Reviewer_oyPz"
        ]
    }
]