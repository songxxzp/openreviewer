[
    {
        "id": "4sMKKO1--P",
        "original": null,
        "number": 1,
        "cdate": 1666643519974,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643519974,
        "tmdate": 1666643519974,
        "tddate": null,
        "forum": "ArPM-xtsFrk",
        "replyto": "ArPM-xtsFrk",
        "invitation": "ICLR.cc/2023/Conference/Paper3241/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a Gated Neural ODE (gnODE) model and empirically investigates the trainability, expressivity, and interoperability of the proposed model.",
            "strength_and_weaknesses": "## Strength\n\n1) This paper proposes a new model with an interesting combination between the gating mechanism and Neural ODEs.\n\n\n## Weaknesses\n\n1) The organization of this paper is a bit messy. The paper investigates three largely independent aspects of Neural ODEs with empirical experiments. It is unclear about the synergy between these three aspects. It is also unclear how these aspects relate to \"understanding how the dynamics in biological and artificial neural networks implement the computations\". \n\n2) Several arguments of the paper are not well-supported. For example, it is uncommon to claim that the model is interpretable just because it can have a relatively lower hidden dimension for achieving the same performance compared to other models, especially when this result is obtained on a single synthetic dataset. \n\n3) The empirical comparison is insufficient. The authors compare the proposed method with very closely relevant baselines such as ODE-LSTM by the reported test MSE in the literature. However, the vanilla Neural ODE and GRU models experimented by this paper already have better test MSE than the reported test MSE of ODE-LSTM in the literature, which casts doubt if the comparison is fair.\n\n4) The paper seems to be a half-baked draft. For example, there is no legend in Figure 3B. The template Author Contributions and Acknowledgements are not removed from the draft. ",
            "clarity,_quality,_novelty_and_reproducibility": "See comments above.",
            "summary_of_the_review": "This is a half-baked draft with significant issues with clarity. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3241/Reviewer_fH84"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3241/Reviewer_fH84"
        ]
    },
    {
        "id": "ESjfeIniAb",
        "original": null,
        "number": 2,
        "cdate": 1666645030746,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645030746,
        "tmdate": 1666645030746,
        "tddate": null,
        "forum": "ArPM-xtsFrk",
        "replyto": "ArPM-xtsFrk",
        "invitation": "ICLR.cc/2023/Conference/Paper3241/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper advances the applicability and interpretability of neural ODE by injecting a regulatory mechanism to adjust the conditional time scale of the vector field. The authors show improvement on a variety of tasks respect to previous methods, and provide insights about the structure of the fixed points by inspecting the maximum eigenvalue of the Jacobian. ",
            "strength_and_weaknesses": "What the authors call the principle of expressivity is a very interesting phenomena of doing more with less. That reveals itself best fitting random trajectories indicates that gnODEs can potentially thread through unstable saddles as shown in Fig. 3\n\nThe simple proposal of using the dissipative -h term to achieve stability for many initial conditions might be perceived as trivial for people working in dynamical systems. And it's surprising that other have not used before.\n\nIt\u2019d be informative to also plot the time scaling of the vector field in each of the tasks. Is gnODE freezing the dynamics when learning the variable amplitude flip-flop?\n\nCan the authors elaborate on the structure of the marginally-stable fixed points for a given autonomous dynamical system realization? The presence of unstable saddle is interesting because those are the main ingredients of heteroclinic orbits which are structurally unstable but can create sequences of quasi-stable states that are input dependent. This type of dynamics is referred in the literature as winnerless-competition principle.\n \nFor a given size N, gNN and non-time dependent input, how many stable and saddle fixed points are detected? GRU and mGRU appear to learn only stable fixed points which is not an interesting dynamics. gnODE, on the other hand, appear to have a combination of saddles and stable FPs, but I\u2019d like to know if the system has a complex structure of fixed points for non-explicit time dependence on the inputs (autonomous dynamical system). Then, tracking the separatrices of the saddles would enrich our understanding of what gnODE can learn.\n\nCan the authors explain what they mean by continuous attractors? There most common attractors one finds in these types of dissipative dynamical systems are limit cycles or even chaotic attractors. Are the authors referring to center manifolds?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Nicely written paper. I am unsure about the interpretability of section with D=2 given that for most applications these nODEs will work in larger dimensions. How can you translate it to higher dimensions is unclear to me?\n\n",
            "summary_of_the_review": "Intriguing paper that I enjoy reading. It shows better performance in a variety of tasks especially when the nODE needs to learn complex random trajectories. Yet, many questions remain in terms of what's the underlying dynamical structure generated in the autonomous regime.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3241/Reviewer_oDgk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3241/Reviewer_oDgk"
        ]
    },
    {
        "id": "JQSotMamyt",
        "original": null,
        "number": 3,
        "cdate": 1666653976513,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653976513,
        "tmdate": 1666653976513,
        "tddate": null,
        "forum": "ArPM-xtsFrk",
        "replyto": "ArPM-xtsFrk",
        "invitation": "ICLR.cc/2023/Conference/Paper3241/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The manuscript aims to build an accurate, explainable digital twin for the biological neural networks. The neural ODEs have a capability to address complex behaviour. The Authors suggest a gated version to ensure also long term memory effects. The gated systems, the authors find are leading to simpler dynamics, all in increase the trainability, expressivity and interpretability. The dynamics of the gated neural ODE is discussed quite lengthly relating to the other already existing alternatives. \n\nExperimental trials are made for N-bit flip flop task, fitting a finite number of samples from an Ornstein-Uhlenbeck (OU) process, and a 2d walker kinematic simulation predictor among others. The experiments shows that the gated version can utilise lower dimensional phase spaces. The paper provides an extensive Appendix where the performance of the gated system is studied from multiple point of views.\n",
            "strength_and_weaknesses": "The paper is comprehensive description of the gated neural ODE and provides  basic intuition of its capabilities. There was one practical example on its capability to fit continuous processes. The strength of the paper is that is describes a novel model that could \"explain\" or \"interpret \"different data sets and provide a model that has the ability to shed light  to the fundaments in the process described by the data, and, for example, show a fixed point structure in the particular data.\n\nThe weak point is that this remains quite an exercise of experimental trainings on selected examples without practical use cases. The question that is hinted at the introduction remains open. Could this be a tool that can reveal some of the algorithms biological neural networks are running. ",
            "clarity,_quality,_novelty_and_reproducibility": "The manuscript is clear and high quality, well written and comprehensive description on the properties of a novel neural ODEs architecture.",
            "summary_of_the_review": "Excellent, experimental review the performance of gated neural ODEs with some mathematical analysis in the appendices.  A good addition to the toolbox for creating models for multi-scale signals.  The capability to the architecture is demonstrated with a wide set of examples. However, I am waiting for the real process with importance that can be \"understood\" and \"interpreted\" by using this architecture in the fit. For example, a discovery of type of biological neural system that can be characterised and interpreted with this novel tool.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3241/Reviewer_XxBD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3241/Reviewer_XxBD"
        ]
    },
    {
        "id": "ShKtGflWeth",
        "original": null,
        "number": 4,
        "cdate": 1666775238428,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666775238428,
        "tmdate": 1666775238428,
        "tddate": null,
        "forum": "ArPM-xtsFrk",
        "replyto": "ArPM-xtsFrk",
        "invitation": "ICLR.cc/2023/Conference/Paper3241/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduce a gated neural ODE, an architecture that extends the neural ODE with a recurrent gating variable. They demonstrate that this architecture can learn continuous attractors, provide somewhat interpretable solutions and perform well for some select benchmarks.",
            "strength_and_weaknesses": "## Strengths:\n\n- The method is a simple extension of neural ODEs that seem to have many useful properties and benefits.\n- The ability to learn continuous attractors in an interpretable way is interesting.\n- Performs well on considered benchmarks.\n- The expressivity measure seems useful and relevant to the community.\n\n## Weaknesses:\n\n- Very limited set of benchmarks are considered.\n- No comparison with architectures such as LSTMs, LMU, S4 etc.\n- The specific motivation of constructing this architecture is not clear -- is it to provide a better performing architecture? If so comparison on lots more benchmarks and architectures are needed.\n- gnODE seems to be very parameter inefficient compared to other methods here.\n\n## Other questions:\n\n- Fig. 1: why does gnODE have much higher variance than other RNN models?",
            "clarity,_quality,_novelty_and_reproducibility": "The  paper is generally clearly written, the methods are described clearly. The motivation for the work could be explained a bit more.\n\nThe work is novel to my knowledge.\n\nBased on the given experimental details, it does look like the work can be reproduced.",
            "summary_of_the_review": "Overall, the gnODE is a very interesting architecture that might have the potential to be an interpretable performant architecture. But the paper only uses a narrow set of benchmarks and compares with very few other relevant architectures, so the generality and scalability of the model proposed here is not clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3241/Reviewer_5GnR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3241/Reviewer_5GnR"
        ]
    }
]