[
    {
        "id": "_eOYvlQpEKv",
        "original": null,
        "number": 1,
        "cdate": 1666541559469,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541559469,
        "tmdate": 1666660363354,
        "tddate": null,
        "forum": "L8qKBr_bht",
        "replyto": "L8qKBr_bht",
        "invitation": "ICLR.cc/2023/Conference/Paper4097/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a technique to improve the efficiency of multi-head attention. \nThe idea is to apply the multi-resolution decomposition with orthogonal bases inspired by wavelets to attention matrics or the V matrices. \nThe Haar Wavelet (box-car like) is considered in this paper, resulting in up/down sampling combined with multi-head attention. \nExperimental results show that the proposed method improves FLOPS and memory uses compared to a naive softmax and the block-diagonal attention matrix approximation. ",
            "strength_and_weaknesses": "(+) Multi-resolution decomposition with orthogonal bases is a well-studied and proven-to-be-effective strategy for approximate signals. A reasonable choice. \n\n(+) The paper reads easily, and straightforwardly. No logical jumps. \n\n(--) Less technical novelty\n\n(--) Less impact in experimental results\n",
            "clarity,_quality,_novelty_and_reproducibility": "The technical discussions are clearly written. Many readers may find the paper easy-to-read \nThanks to the clear writings, reproducing may not be so hard. \n\nI find two major concerns during the review. \n(1) Novelty \nSignal decomposition with orthogonal, multi-resolution bases passes the \"test-of-time\": no doubt in its efficacy. \nIf I correctly understand, the main idea is to simply plug-in the multi-reso decomposition with Haar-like Wavelet on the target matrices (Attention or V). I cannot find any other technical challenges or new ideas to solve the challenges. \nGiven these, it is difficult for me to evaluate the paper novel enough to accept at ICLR. \n\n(2) impact of experiments\nThe experimental results are not strong. \nThe results show that the proposed method is marginally better than the existing Multi-Reso. Approximations. However, the main goal of the paper is to improve the efficiency of Transformer. In that sense, other approaches (not Multi-Reso Approx.) presented in the related work did better jobs. \n\nFor example, the FMMformer [Nguyen+, 2021] claims that the combination of low-rankness and sparsity yields linear-time complexity for self-attention. \nNAFNet [Chen+, 2022] reduces the computational cost (in terms of Multiply-ACcumulation Operations MACs) up to 8.4% of the naive Transformer while achieving a new SotA in denoising. \nPerceiver [Jaegle+, 2021] proposes to replace the pair-wise self-attention to the cross-attention between the low-dimensional Key latent vectors and the high-dimensional input vectors, yielding linear-time complexity with great recognition accuracy in several domains. \n\nCompared to these results, the gains achieved in this paper are not significant. Then it is difficult for me to claim the necessity of the Multi-Res. Approx. approach to improve the efficiency of the Transformer at this moment. \n\nI expect clearer advantages of the multi-res. approx. approch are provided through author-feedback time!\n\n\n[Chen+, 2022] Chen, Chu, Zhang, and Sun, \"Simple Baselines for Image Restoration\", ECCV 2022. \n[Jaegle+, 2021] Jaegle, Gimeno, Brock, Zisserman, Vinyals, and Carreira, \"Perceiver: General Perception with Iterative Attention\", ICML 2021. \n\nMinor comments:\nThe discussion about the redundancy of the Multi-Reso Approximation in Sec 1.2: in general I agree to these claims, but it is better to have some citations to support the claims. ",
            "summary_of_the_review": "The technical idea is clear. But the novelty and the impact of experimental results are not enough for recommend to accept. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4097/Reviewer_sjo4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4097/Reviewer_sjo4"
        ]
    },
    {
        "id": "Roijn7mcEn",
        "original": null,
        "number": 2,
        "cdate": 1666586185932,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666586185932,
        "tmdate": 1666586185932,
        "tddate": null,
        "forum": "L8qKBr_bht",
        "replyto": "L8qKBr_bht",
        "invitation": "ICLR.cc/2023/Conference/Paper4097/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores the idea of using wavelet transform to compress the input data length to achieve the approximation of attention map and value matric in a Transformer. The experimental results show that it outperforms some state-of-the-art methods. And the proposed method performs better computational and memory cost than the vanilla Transformer.",
            "strength_and_weaknesses": "The proposed approach has higher computational efficiency and lower memory overhead than the original Transformer at the cost of a small amount of performance. Although the proposed method is more efficient than the current version of the vanilla Transformer, a comparison with the computational efficiency and memory cost of existing methods is missing.",
            "clarity,_quality,_novelty_and_reproducibility": "The topic is a very interesting and a challenging problem. And the proposed Multiresolution-head Attention is somehow novel. The performance shows the advantage of the proposed method. Overall, the paper is well-organized.",
            "summary_of_the_review": "The paper proposes an improved model of Transformer, Multiresolution-head Attention, to reduce computation and memory overhead. The proposed method is more efficient through additional experiments and complexity analysis than the original Transformer. However, the experimental part lacks a comparison with the algorithmic complexity of existing methods. The overall organization of the paper is clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4097/Reviewer_VX4u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4097/Reviewer_VX4u"
        ]
    },
    {
        "id": "iHE_DyPRq8t",
        "original": null,
        "number": 3,
        "cdate": 1666905126525,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666905126525,
        "tmdate": 1666905126525,
        "tddate": null,
        "forum": "L8qKBr_bht",
        "replyto": "L8qKBr_bht",
        "invitation": "ICLR.cc/2023/Conference/Paper4097/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a multi-resolution based attention mechanism named MrsFormer. Various experiments have been conducted to validate the effectiveness of the proposed method over several baselines",
            "strength_and_weaknesses": "Strengths:\n* The authors focus on an important problem in sequence modeling\n* The technical details are presented well\n\nWeakness\n* The novelty needs to be further justified\n* The draft needs to be better organized\n* The experiments need to be strengthen",
            "clarity,_quality,_novelty_and_reproducibility": "The technical details are sufficient; however the experiments need include more state of the art methods as baselines to be more solid",
            "summary_of_the_review": "OnThe authors needs to better justify the novelty and the motivation of the proposed method. From the introduction, the authors only briefly introduce some technical description without any motivation or rationale of proposing MrsHA. I suggest the authors move the background knowledge parts (e.g., self-attention description) into Sec. 2 then highlight the motivation and justify the novelty.\n\nThe paper also misses many state-of-the-art efficient transformer variants such as Luna, Linformer, Performance, etc.. I suggest the authors include these relevant methods as baselines in the evaluation or better justify the criteria of why selecting the existing baseline methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4097/Reviewer_fFTA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4097/Reviewer_fFTA"
        ]
    }
]