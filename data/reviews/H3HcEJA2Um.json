[
    {
        "id": "qU5BkdYaAE",
        "original": null,
        "number": 1,
        "cdate": 1666273986497,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666273986497,
        "tmdate": 1670154821304,
        "tddate": null,
        "forum": "H3HcEJA2Um",
        "replyto": "H3HcEJA2Um",
        "invitation": "ICLR.cc/2023/Conference/Paper5440/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper makes a systematic study in terms of the critical factors in temporal stereo matching for camera-only 3D object detection and proposes corresponding solutions with a framework, SOLOFusion, to address the problems. It concludes that the limited history usage and low granularity of matching resolution are the two bottlenecks for previous methods, among which the former is more important and can compensate for the latter to some extent. The resulting solution leverages both low-resolution, long-term and high-resolution, short-term information to construct temporal multi-view features effectively and efficiently. It achieves a new SoTA on the nuScenes benchmark. The ablation studies also support the analysis.",
            "strength_and_weaknesses": "Strengths:\n\n- The basic idea is easy to follow and the main motivation is clear.\n- The theoretical and empirical analyses are insightful. They can be important supplements for this line of works in the community.\n- The conclusion is clear and the paper proposes an efficient way to leverage both conclusions.\n- The proposed method achieves strong experiment results, both compared to its baseline and other SoTA methods.\n- Some empirical studies are interesting and important for practical use, such as the analysis about balancing temporal fusion and resolution in Table 7.\n\nWeaknesses:\n\n- The analysis part is a little wordy (although I understand it is fruitful, there is no need to clarify such simple conclusions with so many paragraphs) while the methodology part is so brief, even without a figure showing the details of networks. The re-organization of these contents in the main paper and supplemental materials should be considered.\n- The technical contribution seems incremental (though it may be related to the presentation problems). The framework seems a simple combination of BEVDepth and BEVStereo/DfM while only adjusting some settings for temporal aggregation. The author should clarify this point more clearly, and add at least a specific figure showing the network architecture as well as the distinguished technical design details.\n- As the involved timestep increases, another problem is that the effect of object motion on the stereo matching is more notable. Analysis for this point is missing in this paper. Such problems can severely and intrinsically challenge the conclusion that \"long-term matching\" is better.\n- (Minor) It would be better to have more references at:\n1. In the first paragraph and the conclusion section, the author mentions that \"depth estimation is the main bottleneck of camera-only works\". Although it can be common sense for researchers from this community, it would be better to include some references to support such claims.\n2. In Table 1, it would be better to compare SOLOFusion with a missing reference about the discussion of long-range, \"UniFormer: Unified Multi-view Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View, Arxiv 2022\". Besides, DfM is missing in the MVS part and there is another line of work that is perpendicular to LSS-based, like OFTNet, ImVoxelNet, and MV-FCOS3D++ (which do not predict depth probability for 2D-3D lifting, among them only the last one has temporal modeling).\n- (Minor) Minor typos/grammatical mistakes:\n1. Footnote1, \"because it while it is\"\n2. The caption of Figure 4 seems to be covered by Figure 5.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Overall good, yet insufficient for network details.\n\nQuality: Overall good.\n\nNovelty: The analysis is novel and solid, but the technical contribution seems incremental.\n\nReproducibility: The implementation of the temporal setting is easy to reproduce, but the framework is hard to reproduce without further details.",
            "summary_of_the_review": "This paper formulates the problem of camera-only 3D detection from videos from a temporal multi-view stereo perspective and provides an analysis based on the \"localization potential\" concept. It results in critical conclusions that the usage of long-term histories and high-resolution features for matching are the most important factors in this setting and proposes corresponding solutions. Experiments show the effectiveness of the proposed approaches and demonstrate their importance in real-time practical use. The main problem is focused on the organization of contents in the main paper and supplemental materials, the incremental technical contribution for framework design, and the missing discussion about object motion. I would recommend weak acceptance at the current stage because it is an important supplement for this line of work and believe it can bring new insight to this community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5440/Reviewer_FyEa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5440/Reviewer_FyEa"
        ]
    },
    {
        "id": "sIRxV4wFU_",
        "original": null,
        "number": 2,
        "cdate": 1666611355361,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611355361,
        "tmdate": 1669014872790,
        "tddate": null,
        "forum": "H3HcEJA2Um",
        "replyto": "H3HcEJA2Um",
        "invitation": "ICLR.cc/2023/Conference/Paper5440/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigate how temporal context should be used for vision-based 3D object detection. They treat temporal 3D object detection as a multi-view stereo problem, where history frames can be seen as multiple views. A criterion termed as \"localization potential\" is defined to reflect the level of difficluty for multi-view stereo, upon which they did in-depth analysis and concluded that using long-term, low-resolution temporal context benefits detection.\n\nBased on the analysis, a new 3D object detection method named SOLOFusion is proposed. The core idea is using both long-term and short-term memory. The long-term memory stores low-res features, and is fused by BEV cost volume; the short-term memory stores high-res features from the very last frame, and is fused  by plane-sweep cost volume. Experimental results show the proposed method can serve as a new strong baseline. Despite its simplicity, the results are quite impressive, refreshing previous state-of-the-art by a significant margin.",
            "strength_and_weaknesses": "Strength:\n+ This work borrows a lot of knowledge and tools from multi-view stereo to help us understand and analysis how temporal context should be used for vision-based 3D object detection. Introducing multi-view stereo to 3D object detection is a valuable direction that probably worth much more research, and to my knowledge this work can be seen as one of the few works that open up this direction.\n+ The analysis based on the proposed \"localization potential\" well-explained the motivation of using both long-term and short-term memory, and the conclusions shown in Fig.4-6 are quite interesting and inspiring.\n+ The final method is quite simple, easy to be re-produced.\n+ Experimental results are very promising,  refreshing previous state-of-the-art by a significant margin, showing the importance of using long-temporal context.\n+ Thorough abation study.\n\nWeaknesses:\n- I think the biggest issue is the presentation. I appreciate the dense technical content, which indeed brings difficulty in clarification; But the presentation could definitly be improved. For instance, a reader who has weak background on multi-view stereo may be confusing when discussing the relationship between multi-vew stereo and 3D object detection, since there is little preliminary knowledge introduced in the main text.  Maybe it would be better to elaborate more on background, and defer other stuff to the appendices.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nAccaptable but could be improved. See \"weaknesses\"\n\nQuality:\nGood. I will rank this work as the top 20% in the area of 3D object detection.\n\nNovelty:\nGood. This is one of the few early attempts that successfully employ multi-view stereo for 3D object detection.\n\nReproducibility:\nGood. The proposed method is simple and easy to reproduce.",
            "summary_of_the_review": "In general, this is a good paper that worths acceptance. \nMy biggest concern is the presentation so I give my recommendation as \"6: marginally above the acceptance threshold\". If the presentation could be improved, I will be willing to raise my rating to \"8: accept, good paper\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5440/Reviewer_U6zw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5440/Reviewer_U6zw"
        ]
    },
    {
        "id": "Avk9yXWt6Vd",
        "original": null,
        "number": 3,
        "cdate": 1666667241861,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667241861,
        "tmdate": 1666667241861,
        "tddate": null,
        "forum": "H3HcEJA2Um",
        "replyto": "H3HcEJA2Um",
        "invitation": "ICLR.cc/2023/Conference/Paper5440/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors study vision only 3D object detection for autonomous driving. First, the authors formulate the recent multi-frame multi-view methods as temporal stereo matching. Then localisation potential is proposed and analysed theoretically and empirically to show the necessity of larger time window. To handle the efficiency of long term computation, coarser resolution feature maps are used in the long term and finer resolution in the short term. In the experiments, the proposed method outperforms others in nuScenes leaderboard.",
            "strength_and_weaknesses": "Strengths:\n\n1. New formulation of the exiting multi-view, multi-frame image based 3D detection methods.\n\n2. Use both theoretical and empirical analysis by introducing localisation potential, to unveil the importance of longer time horizon.\n\n3. Impressive performance in nuScenes leaderboard.\n\n4. Writing is clear and easy to follow.\n\nWeakness:\n\nI have one question regarding the experimental analysis. Did the authors analyse the performance improvement for objects with different speed? It is interesting to see the impact of long term fusion on static, slowly moving and fast moving objects. ",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity, Quality, Novelty:\n\nAs mentioned in the strengths, this paper is organised well and has several novel contributions. Overall, this is a high-quality paper. \n\nReproducibility: \n\nFrom Section 5, it seems not very difficult to reproduce the paper. However, we are still look forward to the officially released code.",
            "summary_of_the_review": "\nI would like to accept this paper. If the authors can provide more analysis for the method (for example, the question I have in the weakness), I tend to further increase my rating. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5440/Reviewer_Bu5e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5440/Reviewer_Bu5e"
        ]
    }
]