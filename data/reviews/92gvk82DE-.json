[
    {
        "id": "m8jCFrvduqw",
        "original": null,
        "number": 1,
        "cdate": 1666538184614,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538184614,
        "tmdate": 1669396904696,
        "tddate": null,
        "forum": "92gvk82DE-",
        "replyto": "92gvk82DE-",
        "invitation": "ICLR.cc/2023/Conference/Paper5624/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper propose a novel way of doing prompt engineering in large language models.\nThe model scores a pool of instructions generated using small set of demonstrations by a large language model.\nFor each input output pair (x,y), the goal is to find an instruction z so that f(z,x)=y, where f is a large language model.\nz is then used to prompt a large language model at inference time with and without demonstrations.\nResults on two benchmarks show that the proposed approach produces instructions that are as good as instructions written by humans.\nThe authors also do ablation experiments to understand what is the impact of language model size.\n",
            "strength_and_weaknesses": "The proposed idea is very interesting and this line of works trying to find the best language to query large language models is very trendy these days.\nThe reported results seem to be very good and for practitioners this approaches can be useful to improve performance on specific tasks.\n\nThe paper though is not very well written and there are a few parts that require some clarification.\n\nThe assumptions/intuitions in section 3.1 are not clear.\nFor example why \"would like the model to predict the missing context before the demonstrations\"?\nWhat does it mean more versatile approach?\nI understand that for every set of training pair there will be a possible different instruction, do you consolidate them?\nWhich one do you choose at inference time?\nReal examples in the the Figures and task descriptions would simplify the reading a lot.\n\nAfter the selection of instruction using different metrics, a set of final instruction are returned, how do these extraction are used? prompted all of them in an in-context learning scenario?\n\nIn section 4.1, please explain why Honovich et al. (2022) is the right benchmark for the proposed task, redirecting the reader to the original paper is not good writing, since it is not a well established benchmark.\nIn section 4.1 would be useful to have examples of input/output in the zero-shot and few-shot in context learning scenarios.\nWhy is the model only tested on the dataset released by Honovich et al. (2022), and not on more standard benchmarks like GLUE or big bench?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not very clear, the idea is novel. \nThe authors do not mention to release any code, so I am not sure about reproducibility.",
            "summary_of_the_review": "The idea is novel and interesting.\nThe paper is hard to read although the approach is quite straightforward. I suggest the authors to spend some time to make it more clear.\n\nUPDATE: The authors made the paper more clear and added a set of experiments on more benchmarks that make the paper much stronger.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "Same old concerns related to all large language models about toxic language gender/religious/racial biases etc.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5624/Reviewer_7jaz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5624/Reviewer_7jaz"
        ]
    },
    {
        "id": "OKIbWnZeTPl",
        "original": null,
        "number": 2,
        "cdate": 1666641451204,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641451204,
        "tmdate": 1666641451204,
        "tddate": null,
        "forum": "92gvk82DE-",
        "replyto": "92gvk82DE-",
        "invitation": "ICLR.cc/2023/Conference/Paper5624/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes using LLMs to generate instructions that will be used as prompts for solving tasks with LLMs.\nAuthors use a filtering + selection pipeline to choose top candidates according to various metrics. They compare their method in zero and few shot settings with 1) human-engineered prompts 2) LLM generated outputs without the filtering and section process.\nThey evaluate their approach on several NLP datasets and show it achieves similar results to 1) and improves over 2). \nThey do a comprehensive analysis of their experiments that shed light on important components of their approach. \n",
            "strength_and_weaknesses": "Strength:\n- The paper is well-written and provide a lot of insights that can help other researchers who are working on similar area.\n- The selection + filtering is a new addition to prior work done by [1] which improves the results significantly.\n- Experiments are well-done and multiple ablations are performed to show effectiveness of various components of their approach.\n-The idea is similar to automatic prompt tuning methods such as prefix-tuning, however, the resulting instructions are in natural language which is more interpretable than other approaches which generate embeddings. Also this approach doesn't require fine-tuning which is a plus.\n\nWeakness:\n- I think the tasks studied in this paper could be more challenging. Taking a look at Table1, most tasks require surface-level understanding of language syntax. Translation task is only done for single words. Semantic tasks from GLUE are at 90+% accuracy with simple transformer models. Few tasks here require complex reasoning, or deeper language understanding. I think those tasks require creating more nuanced instructions and seeing how this approach compares against humans would be very interesting. Until then. it's unclear to me how this approach generalizes beyond simple tasks.\n- To build on my previous point, looking at figure 3, results on relatively harder tasks such as sentence similarity and membership show APE is doing worse than humans and vanilla generated prompts. The authors acknowledge this in the paper: \"Additionally, tasks such as Membership and Second Letter are intrinsically challenging for the model, and APE consistently performs worse than humans.\" I think the paper can significantly benefit from further studies on tasks such as summarization, intent classification and slot filling, etc. that are more challenging.\n\n[1] Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. Instruction induction: From few\nexamples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to Strengths.",
            "summary_of_the_review": "Given the Strengths and Weaknesses mentioned above, I think this is a valuable paper and authors have done a lot of work to ensure the accuracy of their claims. However, I'm not quite convinced if this approach can scale to more challenging NLP tasks. I hope authors address this issue in the updated version of the paper. I can revisit my score given the new results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5624/Reviewer_3MTv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5624/Reviewer_3MTv"
        ]
    },
    {
        "id": "1StLb4ZYwDC",
        "original": null,
        "number": 3,
        "cdate": 1666715429237,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666715429237,
        "tmdate": 1666715429237,
        "tddate": null,
        "forum": "92gvk82DE-",
        "replyto": "92gvk82DE-",
        "invitation": "ICLR.cc/2023/Conference/Paper5624/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method to generate prompts that solve textual tasks defined by Input/Output pairs through a Large Language Model.\nThis paradigm was introduced by Hononovic et al. (2022), as well as the 24 tasks used for evaluation. The main element of novelty here is the algorithm based on search and validation of the best prompt among a set of generated candidates, rather than the naive greedy selection used in Hononovic et al. (2022).\n",
            "strength_and_weaknesses": "Strengths\n- The value of a good automatic prompt engineer is so evident that needs no explanation, this paper is definitely of interest to the community.\n- The method is sound, and the discussion and results are valuable.\n- The related works section does a good job in connecting a very new work built on top of LLMs to established research fields such as Program Synthesis.\n- The paper is clear and well written, it is easy to understand the problem, the method, and the experiments.\nWeaknesses\n- The paper does not achieve the expectations set by the title and abstract. I was expecting challenging tasks where humans struggle to find a proper prompt (like leading Dall-e to generate a horse riding an astronaut, which was cited as a limitation of Dall-e until someone found the proper prompt) but the proposed method can automatically find it. Instead, most of the tasks can be solved with human-level accuracy using just the enumeration of a few input/output pairs as prompts (Fig. 4). These pairs would be available in any case since they are needed by the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, the writing is clear as well as the plots. The method and the experiments are sound, I appreciated the multiple runs to estimate the confidence of the accuracies. The exaggerated hype is a smudge in a research work of good quality. \nThe broad topic of prompt engineering is very novel, while the specific idea of an automatic prompt engineer was first explored in the very recent Hononovic et al. (2022). I still consider the novelty of this work satisfactory, given the new method presented and the general novelty of the area of research.\n",
            "summary_of_the_review": "My recommendation is to accept this paper, the research is of interest and well performed. \nStill, I would recommend changing the title to be more descriptive and less pompous. \nRight now it suggests that finding the appropriate prompt to solve a task is no longer a problem since LLMs can do that to the same level as humans.I would say that we are not still there (e.g. https://twitter.com/goodside/status/1581805503897735168).\n\nThis work made me think of https://arxiv.org/abs/2201.10222, the *natural language program synthesis* problem reminded me the *Explanatory Learning* problem, while the APE is similar to the CRN approach there, maybe you could find this paper interesting.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5624/Reviewer_y9rU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5624/Reviewer_y9rU"
        ]
    }
]