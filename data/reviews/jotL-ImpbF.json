[
    {
        "id": "mky32N9B2V",
        "original": null,
        "number": 1,
        "cdate": 1666671849676,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671849676,
        "tmdate": 1666728648735,
        "tddate": null,
        "forum": "jotL-ImpbF",
        "replyto": "jotL-ImpbF",
        "invitation": "ICLR.cc/2023/Conference/Paper2478/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an entity typing technique that uses geometric embeddings, and evaluates it on several data sets, achieving strong performance.",
            "strength_and_weaknesses": "Strengths\n\nThe results in this paper are strong, over a number of data sets, and better handling of hierarchically-organized types in NER is an important task.\n\nWeaknesses\n\nI found the paper to be so unclear that I did not have confidence in the contribution (as discussed below), and to the extent I understood the method I found the novelty to be limited.  The model appears to use box embeddings but with additional size and density parameters which are a fairly straightforward addition.  If the motivation for including these had been described more clearly, it's possible even with limited novelty this could be a sufficient contribution for publication.",
            "clarity,_quality,_novelty_and_reproducibility": "I was unclear on how the methods worked, how the hierarchy was encoded, or why we would expect this approach to outperform box embeddings.\n \nI was unclear about how the hierarchy is actually enforced in the model.  Section 3.4 is titled \u201chierarchical learning loss\u201d but the \u2018hypervolume indicator\u2019 discussed there is never actually defined.  I never understood how the hypercubes defined in equations 1 and 3 were actually used in the model, or whether/how the model might constrain the boxes for hierarchically-organized labels to nest under their parent label boxes.  It could be that the paper is assuming that the reader has knowledge about how the baseline box embeddings are typically used in NER models, which I lacked.\n \nFurther adding to my confusion, after the experiments, section 5.4 Discussion of Hierarchies talks about constraints on the type space that might exist in the model.  For example, it says \u201cSize scale restricts the hRM size and thus keeps subset type inside its root type.\u201d  I was confused, is this an empirical fact that is observed in the trained model, or is this built in to the model?  Further, Figure 6 seems to be illustrating how the hierarchy is reflected in the model, but is very hard to read and understand.  What is an \u201cintersection edge\u201d?  I expected to see here that the person/author box is contained within the person box, but I don\u2019t see the plot including any color as dark as the person/author color in the legend, but it could be just that the line width is lower in the plot than in the legend. [edit: after further reflection I think I understand Fig 6, but it could be explained better]\n \nI think the paper could be greatly improved by presenting box embeddings in technical detail, and then explicitly contrasting that method with the paper\u2019s proposed one (talking about how the size and density parameters are added, and why these are critical).  Then, the experimental analysis should show that the hypothesized source of improvement over box embeddings is in fact leading to the experimental wins.  The paper says in the intro that box embeddings are too simple and can\u2019t learn hierarchical knowledge, but I found these claims to be unsubstantiated.  \n",
            "summary_of_the_review": "While this paper goes after an interesting task, I found the clarity and novelty to be too limited for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2478/Reviewer_6W9o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2478/Reviewer_6W9o"
        ]
    },
    {
        "id": "Uit_ei8XuOg",
        "original": null,
        "number": 2,
        "cdate": 1666698874763,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698874763,
        "tmdate": 1666698874763,
        "tddate": null,
        "forum": "jotL-ImpbF",
        "replyto": "jotL-ImpbF",
        "invitation": "ICLR.cc/2023/Conference/Paper2478/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes hRMM for fine-grained entity typing, which represents mentions and types into the hyper-rectangle mass to capture the relationships of ontology into a geometric mass view. ",
            "strength_and_weaknesses": "\nStrength\n\n- The additional parameter: density and edge scaler make sense for a mass model to represent the hierarchical information in types.\n- The method works well on FIGER\n\nWeakness\n\n- Baselines such as LITE (included in the FIGER datasets, SOTA, 50.6 ma-f1), and MLM-ET (49+ ma-f1) are not included in the UFET benchmarks, authors should clarify why you discard them before claiming that **you are SOTA**.\n- The total model is too close to the Box-embedding for FET (Modeling fine-grained entity types with box embeddings. Onoe et al,. 2021), basically adding a scaler and a vector multiplier for type and mentioned encoding, the contribution is not big. The performance of hRMM is subtle on UFET (0.5 ma-f1 higher than Box embedding with \u201cadaptive threshold\u201d), especially given that the SOTA model is LITE with 50.6 ma-f1.\n- The adaptive threshold part is a simple trick that doesn\u2019t have too much contribution.\n- The density scaler and the size scaler vector is the key difference between the basic box embedding (Onoe et al,. 2021),  however, the authors lack a detailed introduction and analysis of them. Why the density x_d \\in \\mathcal{R}? It should be positive x_d \\in \\mathcal{R^+}?  Is the size scaler the same for all edges in a hyper-rectangle? It should be positive? x_d and x_s are both trainable parameters or fixed during training?\n\nMinors:\n- parenthesis usage: hierarchical hyper-rectangle mass model(hRMM) \u2192 hierarchical hyper-rectangle mass model (hRMM), should leave a space.\n- Figure2 \u2192 Figure 2, Figure4 \u2192 Figure 4",
            "clarity,_quality,_novelty_and_reproducibility": "refer to the weakness part",
            "summary_of_the_review": "This paper proposes a new model for fine-grained entity typing, which represents mentions and types into the hyper-rectangle mass to capture the relationships of ontology into a geometric mass view. The novelty seems limited and the empirical results are relative weak.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2478/Reviewer_Q73W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2478/Reviewer_Q73W"
        ]
    },
    {
        "id": "-DRuHl8FWP",
        "original": null,
        "number": 3,
        "cdate": 1666727661438,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727661438,
        "tmdate": 1666727661438,
        "tddate": null,
        "forum": "jotL-ImpbF",
        "replyto": "jotL-ImpbF",
        "invitation": "ICLR.cc/2023/Conference/Paper2478/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to introduce two additional parameters \u2013 density and size scale in the box embedding for fine-grained entity typing. The experiments on three benchmarks show the proposed approach achieved some marginal improvements over the considered baselines.",
            "strength_and_weaknesses": "Strength:\n\nThe authors introduced density and size scale parameters to improve the representation ability of box embedding.\n\nWeaknesses\n\n1. Weak motivation and novelty: while the authors propose to introduce density and size scale to box embedding, there is no clear explanation why we need these two parameters. The original box embedding has already been able to capture the inter-dependencies among event types.\n\n2. The experiments are not solid. In terms of performance gain, the proposed approach only achieves marginal improvements over the baselines on two of the datasets, and underperforms the previous baseline on OntoNotes. Also, there is no detailed empirical analysis of the effect of the two new parameters.\n\n3. The reported results of LITE in Table 1 are not consistent with the results reported in the original paper. In fact, the results in Table 1 are merged from the two versions of LITE (pre-trained on NLI+UFET + NLI+task-specific training). It seems this is a technical error.\n\n4. Writing needs to be significantly improved. The paper is poorly structured and lacks many details, e.g., the intuition and motivation of the approach, as well as theoretical or empirical analysis.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not clear and written in poor quality. The novelty is limited. The details of the experiments are not clear which makes it difficult to reproduce the main results.",
            "summary_of_the_review": "This paper has poor quality with inadequate empirical or theoretical support. The reported results on some of the baselines may have technical flaws. The writing is poor and unclear. The results may not be easily reproduced.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2478/Reviewer_W1hi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2478/Reviewer_W1hi"
        ]
    },
    {
        "id": "7bZMEMRPzEj",
        "original": null,
        "number": 4,
        "cdate": 1666917405174,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666917405174,
        "tmdate": 1666917405174,
        "tddate": null,
        "forum": "jotL-ImpbF",
        "replyto": "jotL-ImpbF",
        "invitation": "ICLR.cc/2023/Conference/Paper2478/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new method of hierarchical entity typing that represents mentions and types using a hyper-rectangle representation, models the mention-type relation using the intersection of these representations, and classifies per-mention types according to the mass of this intersection. The mass representation differs from related work in box embeddings, with the introduction of size scale and density paramaters. There is also a novel loss formulation to aid learning of hierarchical representations.\n\nThe new approach outperforms previous work on two fine grained entity typing benchmarks (Figer, UFET) and approaches state of the art on OntoNotes. A visualization of the hyper-rectangle representation of the types `/person` and `/person/author` show that the more general type subsumes the more specific one, geometrically.\n",
            "strength_and_weaknesses": "#### Strengths\n- The results are quite positive with respect to recent published work.\n- The claim of building hierarchical representations without complex training procedures, is an interesting one.\n\n#### Weaknesses\n- The current presentation lacks a lot of essential detail and motivation. Neither of the key concepts 'size scale' and 'density' are properly motivated and the full model & losses are not mathematically defined. I have more comments below in the section on clarity & reproducability.\n",
            "clarity,_quality,_novelty_and_reproducibility": "#### Clarity & Reproducibility\nI found this paper quite hard to follow, despite having some familiarity with the tasks and related methods.\n\nA lot of core concepts are only described vaguely in text and the losses used during training are never properly defined. I think that the mass representations detailed in equations (1) and (3) are probably applied to the intersection of mention and type representations (illustrated in Figure 2), but this is never explicitly stated. Instead the mass representation is described as a mention representation only. The hyper-volume loss function seems to be essential but, again, I don't understand the definition in Equation (4). What is $f_i(x)$ here? Section 3.3.2 does not properly describe how the size scale and density parameters are used to enforce the type hierarchy (and it seems to refer back to itself?).\n\n#### Quality & Novelty\nI feel that there might be something interesting and novel here but, given my concerns above, it is hard to properly say.",
            "summary_of_the_review": "This paper presents a method of hierachical entity typing that achieves impressive results, with respect to recent published work. However, the current presentation is unclear and key concepts are not properly motivated or defined. I feel that Section 3 needs to be rewritten to (a) properly motivate the size scale and density parameters, (b) mathematically define the full process of hierarchical entity type classification, (c) fully define the loss used to train the model.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2478/Reviewer_zfGa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2478/Reviewer_zfGa"
        ]
    }
]