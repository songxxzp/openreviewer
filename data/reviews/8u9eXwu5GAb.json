[
    {
        "id": "sIt6bV_1HB",
        "original": null,
        "number": 1,
        "cdate": 1666464282222,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666464282222,
        "tmdate": 1666635396012,
        "tddate": null,
        "forum": "8u9eXwu5GAb",
        "replyto": "8u9eXwu5GAb",
        "invitation": "ICLR.cc/2023/Conference/Paper2648/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on exploration of transfer learning in diffusion models, technically, they propose an Attention-NonLinear (ANL) module to facilitate the conditioning process of CLIP embedding. To avoid the diffusion model overfits into the clip embedding of the images, they also generated an approximate clip embedding from masked images. \n\nThe experimental results show that the adaptation can be achieved by few-training data and steps, and outperforms related baselines. Also, the generated samples can further boost the performance of semi-supervised learning.",
            "strength_and_weaknesses": "Pros:\n\n- The paper is well written and easy to follow.\n\n- The proposed method is well motivated and convincing in real-life scenarios, as the training of diffusion models is really time-consuming.\n\n- The experimental result is solid compared with related baselines.\n\n\nCons:\n\n-\"We investigate transfer learning in recent DPMs, and uncover that previous methods like training from scratch or determining the transferable parts are not effective.\", I am doubtful about this statement in contribution and the failed result in Table 1, maybe running DDPM instead of LDM cannot fail? Failure on LDM is expected, as you cannot update the Encoder-Decoder in LDM in your fine-tuning, any sensitive parameter updating inside will greatly affect the sampling quality of LDM.\n\n- ANL module is not novel, already been proposed in latent diffusion models, etc.\n\n- Contribution 2 is not clear, efficient on what? speed? parameter number? convergence? \n\n- Lacking in-depth analysis of ANL modules, which layer should we insert the ANL module? Should we insert it into all layers by default? Maybe we can find a more efficient inserting pattern?\n\n- Lacking in analysis of overfitting problem, the models will meet the problem of catastrophic forgetting so that it fails to generate samples from old distribution.\n\n- In Figure 2, your model's trainable parameter number is 169M for few images(1k)? This should inevitably lead to an overfitting problem, as mentioned above. \n\n- In sampling, you still need the CLIP embedding, where is the clip-embedding comes from? How masked sampling performs to increase the diversity of the generated samples (the effect of Fig6 can be quantified by Inception Score as function of mask rate.)? As CLIP embedding is a too-fine-grained condition so that the sampling might only mesmerize the training data. \n\n- The exposition of masked sampling is too abrupt, the motivation is missing there.\n\n- FID/IS of generated samples should be compared before and after deploying the masking, I think there will be trade-off issue between FID and IS. \n\n- Details about the \"tuning DPMs for data augmentation\" is missing, how to obtain the condition of CLIP embedding during this process of sampling? How many samples are used in this experiment?\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity: 9/10\n\nQuality: 7/10\n\nNovelty: 6/10\n\nReproducibility: 5/10\n",
            "summary_of_the_review": "This paper is well motivated, while lacking in some in-depth analysis around the proposed contributions. Some details are missing to hinder the reproducibility of the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2648/Reviewer_wtTC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2648/Reviewer_wtTC"
        ]
    },
    {
        "id": "ZUMsQ_kJZP",
        "original": null,
        "number": 2,
        "cdate": 1666600110543,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600110543,
        "tmdate": 1666600110543,
        "tddate": null,
        "forum": "8u9eXwu5GAb",
        "replyto": "8u9eXwu5GAb",
        "invitation": "ICLR.cc/2023/Conference/Paper2648/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This submission presents a method for fine-tuning large generative models on small datasets efficiently. The proposed approach used CLIP to encode images from the small datasets into a semantic latents space that is then used to condition the frozen pre-trained model via a cross-attention module. The approach is evaluated on several standard benchmarks datasets outperforming the baselines where models are trained from scratch or where all the parameters of the pre-trained model are fine-tuned. ",
            "strength_and_weaknesses": "Strengths:\n- The problem of fine-tuning large pre-trained models on small datasets in an efficient way is interesting and deserves exposure in the community.\n- The proposed approach is technically correct and the empirical results support the main claims in the paper.\n\nWeaknesses:\n- There\u2019s one important baseline missing. That is figuring out how important the semantic latent c is for this approach to work. In other words, would this work with latents that do not come from big models like CLIP? I suggest the authors run the following experiments to better understand this phenomenon:\u2028- 1) Use a CLIP visual encoder with random weights and fine tune it on the small dataset.\u2028- 2) Let the latent c of each example be non-parametric (eg. a nn.Embedding in pytorch). In this way, one will let the fine-tuning approach to backprop all the way to these non-parametric latents.  ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clearly written and easy to understand. I believe the proposed approach is novel and technically sound. The details to reproduce the experiments in the submission are provided in the paper and all the datasets are publicly availalble. ",
            "summary_of_the_review": "This paper proposes an approach for fine-tuning large pre-trained generative models on small datasets in an efficient way. I believe this problem is interesting for the community and deserves exposure. The proposed approach is technically correct and the empirical evaluation is sound. However, I believe there\u2019s a couple of major baselines that need to be run in order to better understand the contributions (see weaknesses). \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2648/Reviewer_uRV2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2648/Reviewer_uRV2"
        ]
    },
    {
        "id": "_Ygv-PLEGS4",
        "original": null,
        "number": 3,
        "cdate": 1666681034149,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681034149,
        "tmdate": 1666745539781,
        "tddate": null,
        "forum": "8u9eXwu5GAb",
        "replyto": "8u9eXwu5GAb",
        "invitation": "ICLR.cc/2023/Conference/Paper2648/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces an efficient method to finetune pre-trained LDM to small data. The problem is formulated as an image-to-image generation task. A conditional image is first encoded by the CLIP image encoder and then injected into the LDM by the cross attention mechanism. To avoid overfitting, only the parameters of attention and non-linear layers are finetuned. Experiments show the effectiveness both quantitatively and qualitatively.\n",
            "strength_and_weaknesses": "Strengths\n\n1. It proposes to transfer pretrained LDMs to small data and use limited training resources.\n\n2. The CLIP embeddings are used as the conditions of LDMs and a portion of model parameters (Attention-NonLinear) are finetuned to be efficient.\n\n3. Experiments demonstrate better FID scores than finetuning all the parameters and the GAN related methods.\n\nWeaknesses\n\n1. Some equation types exist. For example, the right of Equation 1 and the left of Equation 3 have one parenthesis missing, respectively. The Gaussian distribution in Equation 1 and 2 have inconsistent forms. Both d_c and d^c appear in the descriptions under Equation 8.\n\n2. Table 2 uses the semi-supervised setting. It generates 10 additional samples for each labeled sample. Can we assume the 10 generated samples have the sample labels as their original sample? Since the generation is conditioned on the original sample, it\u2019s very likely that the generated variants still keep the label. If so, it could be just supervised learning with data augmentations.\n\n3. Section 3.3 mentions using masked images as conditional input. Are they used in training, generation, or both? If used in training, any ablation study of delta? Figure 6 only shows illustrations of masking used in generation.\n\n4. I feel the task is defined clearly. The experiments use the LDM pre-trained on ImageNet. Is the pre-training task a noise-to-image generation task? In the finetuning, the task is an image-to-image generation task due to the image conditioning. This change is not described clearly in the paper.\n\n5. Are there two factors of randomness in generating images, i.e., the random masking of the conditional image and the random Gaussian noises? I guess Figures 3 and 5 use random Gaussian noises but no image masking, and Figure 6 uses fixed Gaussian noises but random conditional image mask, right?\n\n6. CLIP embedding and Attention-NonLinear are two components of the proposed transfer learning method. What if removing the CLIP embedding part, i.e., only finetuning the NonLinear part of the LDM? With CLIP, it is an image-to-image generation formulation since it generates one image conditioned on an image. It\u2019s unclear for me how important the CLIP embedding is in the proposed method. Which one really matters in the low-data regime: the image-to-image generation formulation help (compared to the pure noise-to-image generation) or the CLIP embedding? Maybe also try a different embedder, e.g, a pre-trained vision transformer.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is well-motivated by the fact that training DPMs requires big data and much compute resources. Efficiently finetuning pre-trained LDMs on small data is interesting and novel. \n2. The paper clearly presents the technical components, i.e., CLIP embedding conditioning and Attention-NonLinear finetuning. But it doesn't define the task clearly, a noise-to-image generation or image-to-image translation.\n3. No code is provided and some training configurations are unclear. Not sure about reproducibility.",
            "summary_of_the_review": "This paper proposes an efficient fintuning method to adapt pre-trained LDM to small datasets. The method is well-motivated and the results are promising, but some details and ablations are missing. See the above comments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2648/Reviewer_2H8p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2648/Reviewer_2H8p"
        ]
    },
    {
        "id": "zjjkqeFu5U",
        "original": null,
        "number": 4,
        "cdate": 1666810891711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666810891711,
        "tmdate": 1670819378288,
        "tddate": null,
        "forum": "8u9eXwu5GAb",
        "replyto": "8u9eXwu5GAb",
        "invitation": "ICLR.cc/2023/Conference/Paper2648/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an efficient way of fine-tuning diffusion models on new datasets. Instead of training the entire network or training from scratch, the authors add a attention non-linear block which is the only learnable part which is learned during fine-tuning. The attention non-linear block is a cross attention layer followed by a 2 layer NN. Results are shown on a few datasets where the method shows better results.",
            "strength_and_weaknesses": "Strengths:\n1) The method is simple to implement and shows strong results over good baselines.\n2) It is all the more important given that new latent diffusion models like 'stable diffusion' are publicly available which are trained on hundreds of millions of images and fine-tuning on newer datasets with less data will become even more important. The paper proposes a neat trick to address this problem.\n\nWeaknesses:\nThe cross attention based additional architecture isn't very novel and there have been several attempts around this framework traditional deep learning tasks.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The method is simple and clearly explained. It barely meets the technical novelty bar, but is very relevant to the work going on in the field today. The authors do not mention if they will release the code, so it is not clear how reproducible would be the method, but it does look simple enough for someone to try it out.",
            "summary_of_the_review": "The merits of the paper outweigh the weaknesses, so I am recommending the paper for acceptance. Would have given a 7 if there was an option, but its better than a 6, so I am marking it as an 8.\n\nAfter reading reviews of other reviewers about concerns around novelty, I am updating my rating to 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2648/Reviewer_YaAB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2648/Reviewer_YaAB"
        ]
    }
]