[
    {
        "id": "wu3VoQCnqU0",
        "original": null,
        "number": 1,
        "cdate": 1666393139790,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666393139790,
        "tmdate": 1666433667042,
        "tddate": null,
        "forum": "bjPPypbLre",
        "replyto": "bjPPypbLre",
        "invitation": "ICLR.cc/2023/Conference/Paper6351/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors consider the problem of crafting adversarial attacks that are transferable across different deep neural networks architectures (DNNs). In order to solve this problem the authors propose to craft the attacks on Bayesian neural networks (BNNs) trained with Gaussian posterior approximations. On a set of experiments on Cifar-10 and ImageNet the authors show that their approach outperforms state-of-the-art methods.",
            "strength_and_weaknesses": "Strengths\n\n- The main idea of the paper, which is to use BNNs to obtain transferable adversarial attacks, is clear and intuitive\n\n- Experimental results are convincing and show that the proposed method outperforms competitive algorithms\n\nWeaknesses\n\n- A recent paper [Gubri et al (2022a)] also explored BNNs to obtain transferable attacks. While the methods in this paper seem to obtain better empirical results, the results of this paper feel incremental compared to [Gubri et al (2022a)]\n\n- Section 3 and 4 are often confusing and contain some inaccuracies. In particular:\n\n-- After Eqn 1, p(y | x,w) is the likelihood and not the predictive distribution.\n\n-- In Section 3, experiments are mixed with the explanation of the methods and this creates confusion. Furthermore, in this Section the authors present adversarial attacks for BNNs assuming that the posterior is an isotropic Gaussian. However, for basically all approximate posterior inference methods commonly employed for BNNs, the posterior will in general never be an isotropic Gaussian. In view of this, I would re-organise the Section presenting the method for a general Gaussian posterior.\n\n-- L is Eqn 3 is not defined (I guess it is the loss, but please say it explicitly)\n\n-- In Section 4 the description of the experimental setting takes almost one page. To improve readability I would move some of the details (especially those about the details of the methods the authors compare against) in the appendix. \n\n- Something that is not clear to me is why the authors need to develop a new method to attack BNNs, instead of using methods already available in the literature, see e.g. [1,2,3]. This should be at least discussed and motivated. Also, it is not clear how the authors are attacking BNNs trained with SWAG. Please, clarify\n\n- An important aspect that the authors should keep in mind (and discuss) is that BNNs are known to be more robust to adversarial attacks compared to standard neural networks [4] (depending on the approximate posterior method employed). As a consequence, the results of this paper seems to hint on the fact that while it may be harder to find adversarial attacks for BNNs, these may transfer more easily across different architectures.\n\n- Why in the experiments do you need to use a bigger $\\epsilon$ for ImageNet than for Cifar-10? This is counterintuitive for me  \n\n\n[1] Liu, Xuanqing, et al. \"Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network.\" International Conference on Learning Representations. 2018.\n\n[2] Yuan, Matthew, et al. \"Gradient-Free Adversarial Attacks for Bayesian Neural Networks.\" Third Symposium on Advances in Approximate Bayesian Inference. 2020.\n\n[3] Carbone, Ginevra, et al. \"Robustness of bayesian neural networks to gradient-based attacks.\" Advances in Neural Information Processing Systems 33 (2020): 15602-15613.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of using BNNs to craft transferable adversarial attacks is not completely original and novel. The quality and clarity of the paper could be improved (see Strength and Weaknesses Section). Enough details are given in the paper to reproduce the results.",
            "summary_of_the_review": "The paper presents an interesting empirical idea with encouraging experimental results. The quality of the paper is hindered by the clarity of the writing, some inaccuracies, and missing discussion and justification about the methods used to train and attack a BNN. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6351/Reviewer_o774"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6351/Reviewer_o774"
        ]
    },
    {
        "id": "_6f-yczBhm",
        "original": null,
        "number": 2,
        "cdate": 1666599746810,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599746810,
        "tmdate": 1666601239743,
        "tddate": null,
        "forum": "bjPPypbLre",
        "replyto": "bjPPypbLre",
        "invitation": "ICLR.cc/2023/Conference/Paper6351/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to optimize for diversity in substitute models and advocate attacking a Bayesian model for improving the transferability of adversarial examples.  The author also developed a Bayesian formulation for performing attacks and advocated possible finetuning for improving the Bayesian model.  Extensive experimental results have demonstrated that the proposed method can enhance the transferability of adversarial examples, yielding better attack success rates.",
            "strength_and_weaknesses": "Strengths:\n1.\tUtilizing Bayesian models to increase generative adversarial sample mobility is instructive, and the study of ensemble attacks is interesting. The attack method can be equivalently regarded as performing adversarial attack on a set of infinitely many substitute models.\n2.\tThe baselines are advanced, and extensive experiments demonstrate the effectiveness of the proposed method.\n\nWeaknesses:\n1.\tSome experimental settings are not very reasonable. For example, in Table 4, the authors adopt ResNet-50, Inception v3, MobileNet, and MNASNet altogether as substitute architectures of baselines while adopting ResNet-50 and MobileNet as substitute architectures when combining their method with the ensemble methods. Why not experiment with the same surrogate model? This comparison may be unfair when most of the black box models use the ResNet structure. Moreover, it will be more convincing if the author can add some experiments on advanced defense models, such as adversarial training models, Randomized Smoothing (RS)[1], and Neural Representation Purifier (NRP)[2].\n2.\tThere are some severe spelling mistakes in the article that the author should check carefully, such as \u201cAidmix\u201d should be \u201cAdmix\u201d.\n3.\tThe authors could try to add some analysis on why this method works.\n[1]Cohen J, Rosenfeld E, Kolter Z. Certified adversarial robustness via randomized smoothing[C]//International Conference on Machine Learning. PMLR, 2019: 1310-1320.Naseer M, Khan S, Hayat M, et al.\n[2] A self-supervised approach for adversarial robustness[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 262-271.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing is clear and the paper is easy to follow.\nQuality: The experimental results are impressive.\nNovelty: Somewhat novel.\nReproducibility: Not applicable.\n",
            "summary_of_the_review": "The experimental results are impressive and the idea is interesting.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6351/Reviewer_8LEh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6351/Reviewer_8LEh"
        ]
    },
    {
        "id": "KGOymjDhAZ",
        "original": null,
        "number": 3,
        "cdate": 1667425745698,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667425745698,
        "tmdate": 1667425745698,
        "tddate": null,
        "forum": "bjPPypbLre",
        "replyto": "bjPPypbLre",
        "invitation": "ICLR.cc/2023/Conference/Paper6351/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to attack a Bayesian model for improving the transferability of black-box adversarial attacks. Specifically, the authors employ gradient-based attack algorithms on their constructed Bayesian models and expect the generated adversarial attacks can better fool other unseen models. Extensive experiments show that the proposed method surpassed baseline methods in terms of attack success rate.",
            "strength_and_weaknesses": "Strength\n\n1. In general the paper is well-written.\n2.  The authors conduct extensive experiments to validate the transferability of the proposed method by concerning many different neural architectures and multiple baselines.\n3. The proposed way to employ Bayesian estimation for improving the transferability of attacks looks novel to me.\n\nWeaknesses\n\n1. When both the substitute model and the victim model are adversarially trained (Table 6), the improvement is quite marginal. Therefore, the proposed method may only be employed to improve the transferability of attacks on nonrobust models, which weakens the empirical significance.\n2.  In Eq. 6, \u2206w is sampled from the gaussian prior. Then why do we need p(\u2206w) \u2265 \u03b5? Plus, if p(\u2206w) \u2265 \u03b5 is important, then \u03b5 should be a very important hyper-parameter but there is no experiment showing the sensitivity,\n",
            "clarity,_quality,_novelty_and_reproducibility": "The Clarity is good. The proposed way to employ Bayesian estimation for attack transferability looks novel. The authors claimed that the code will be available.",
            "summary_of_the_review": "Given the strength and weaknesses, I tend to rate the paper as marginally above the acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6351/Reviewer_uSCB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6351/Reviewer_uSCB"
        ]
    }
]