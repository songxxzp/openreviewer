[
    {
        "id": "K9qxGsUKz-",
        "original": null,
        "number": 1,
        "cdate": 1666654320876,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654320876,
        "tmdate": 1666654320876,
        "tddate": null,
        "forum": "rnN4pHyf6jD",
        "replyto": "rnN4pHyf6jD",
        "invitation": "ICLR.cc/2023/Conference/Paper3065/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents new algorithms for learning (two-layer) multi-layer perceptrons with ReLU activations. The approach uses the Burer-Monteiro factorization, which yields a nonconvex optimization problem that (under certain low-rank conditions) does not admit spurious local optima. A collection of other related results are applied to other architectures, and brief computational results are provided.",
            "strength_and_weaknesses": "The paper does a great job summarizing the prior art in this line of work. The contributions are seemingly novel, and the use of the BM factorization in this context is a smart and natural one.\n\nI think the authors could be a bit clearer in stating the scope of their contributions in the abstract and introduction. It seems that the results of the paper are restricted to _two layer_ MLPs, but the abstract and Section 1.1 do not make this explicit (though some preceding text on page 2 does mention that this).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I realize the ship has already sailed here in the literature, but I find it the term \"convex neural network\" without reference to _training_ to be confusing (I read it instead as synonymous to an \"input convex neural network\"). Consider making the reference to the training problem more explicit in the abstract and introduction (as an example, \"a wide variety of ...neural networks ... can be posed as equivalent convex optimization problems\").",
            "summary_of_the_review": "The paper is well-written and self-contained, and is seemingly a natural and worthy contribution to the literature. However, I am not an expert in the subfield studied in the paper, so my degree of confidence in this assessment is relatively low.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_tmAf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_tmAf"
        ]
    },
    {
        "id": "d8rrz2QMuW",
        "original": null,
        "number": 2,
        "cdate": 1666671797705,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671797705,
        "tmdate": 1666671797705,
        "tddate": null,
        "forum": "rnN4pHyf6jD",
        "replyto": "rnN4pHyf6jD",
        "invitation": "ICLR.cc/2023/Conference/Paper3065/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors describe an application of Burer-Monteiro factorization to convex programs on neural networks.  The aim is to produce a non-convex, but tractable, optimization problem that, under certain conditions, do not have spurious local minima.  The approach is applied on a variety of NN architectures and validated with a simple synthetic experiment.",
            "strength_and_weaknesses": "Strengths:  The general approach appears to be quite applicable to a wide variety of NN architectures.\n\nWeaknesses:  There are too many NNs considered at the expense of more detailed experimental evaluation.  The main text just reads like a laundry list of similar results that don't seem to add any depth to the discussion.  In its current form, this work feels more appropriate for journal publication.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:  This looks like a journal paper that was rammed into a conference paper.  There are way too many details that require verification, and it isn't clear why all of the different types of networks needed to be presented.  One well-worked example would have been sufficient for a conference submission (at least in my opinion), e.g., the MLP seems to have sufficient novelty and it is the one used in the experiment.  Or do the technical details vary dramatically between the different types of networks?  Other than breadth, I'm not sure that I see what is gained by including them all.\n\nQuality:  The experimental section is really a bit weak.  Too many theoretical results left little space in the main text for a more detailed experimental evaluation.  In particular, the experiment in the main text is only on synthetic data.\n\nNovelty:  To the best of my knowledge, the application of the Burer-Monteiro factorization approach appears to be novel in this setting.\n\nReproducibility:  While the experimental details have mostly been relegated to the supplementary material, it does seem that there are sufficient details to reproduce the results.",
            "summary_of_the_review": "The paper seems to provide novel approaches and insights, but it is plagued by too much information in too short a space.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_NEnW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_NEnW"
        ]
    },
    {
        "id": "Vdnddx3JN4s",
        "original": null,
        "number": 3,
        "cdate": 1667387864217,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667387864217,
        "tmdate": 1667387864217,
        "tddate": null,
        "forum": "rnN4pHyf6jD",
        "replyto": "rnN4pHyf6jD",
        "invitation": "ICLR.cc/2023/Conference/Paper3065/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper discusses the use of the Burer Monteiro factorization in the setting of convex neural network. The main result of the paper are a number of theoretical guarantees on the convergence of gradient descent (i.e. absence of saddles) for appropriate choices of a variety of parameters such as the size of the hidden layer, number of training examples and rank of the Burer Monteiro factorization. The authors tackle a number of architectures from the simple multilayer fully connected perceptron defined on a variety of activation functions (linear , Relu, gated Relu), convolutional neural networks and self attention networks and derive corresponding convergence guarantees for each architecture. ",
            "strength_and_weaknesses": "The paper is interesting. My main concern is with the readability/organization of the paper. I feel that it would improve readability a lot if you were just removing some of the discussion between your introduction of the formulations p_{LMLP}, p_{GMLP} and p_{RMLP} and their corresponding Burer Monteiro factorizations. I would suggest compressing (if not completely removing) section 1.2 and expand on Lemma 3.1 and Theorem 3.4. providing diagrams (As an example in Corollary 3.8.1. it is hard to picture the network.) for some of the models discussed to clarify the meaning of the variables appearing in each of the optimisation formulations (e.g. 28/29).\n\nAlso after reading the whole paper, instead of introducing the convex programs in section 2.1. and then discuss their BM factorisations in separate later subsections, I would replace section 2.1. with a short general introduction to the whole paper and then introduced the convex formulation along each of their BM factorisations. This will make the reading easier by eliminating the need to go back and forth between 2.1 and pages 6/7/8 . \n\nFrom the space you will spare by eliminating section 1.2, you will also gain space to discuss and improve the numerical experiments. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper contains a number of interesting elements but it has to be rewritten. See my detailed comments below",
            "summary_of_the_review": "\npage 3\n\n- You never introduce F. I assume this corresponds to the loss? l2 loss ?\n\n- Aren\u2019t the D_j matrices redundant? given that the factorization enforced by the quasi nuclear norm requires Ku to be non negative? I.e. this also seems to be suggested by your expression of D_j X Z in the paragraph below equation (4)\n\n- What does e stand for in the bound on P? Is that ln(1) ?\n\npage 4\n\n- When you introduce the convex formulation for the Relu based convolutional neural networks, it is not clear what X_i and in particular h and K represent. Is the matrix X_i reorganizing the prototype x_i as a circulant matrix?\n\n- Again, in the paragraph above Equation (8), you use the notation n. I guess \u2019n\u2019 refers to the input dimension as before. Is c the hidden layer size? you should say it more clearly\n- The condition m>=m^* on the one hand and m^*<= nc on the other seem a little too simple to me. I.e. take m^* = 1 and you get unconditional convexity?\n- When you write \u201cSince P is exponential in the rank of X, for fixed filter size h, P is polynomial in all other problem dimension\u201d do you mean \u201cAlthough P is exponential\u2026\u201d ?  If the D_j\u2019s are the same matrices as those you introduce in (3), why not getting rid of this line and adding a reference to the bound on P which you provide below (3), indicating that in the case of  a convolutional network the dependence on n reduces to a dependence on h?\n- In the paragraph above equation (10). Again, I have some concern with your bounds on m as they seem to suggest that the equivalence holds for any value of m\n\npage 5 \n\n- The sentence \u201cHowever, if the gradient of f is not Lipschitz-continuous, there are no guarantees that gradient descent will find a second-order critical point of (12): one may encounter a stationary point which is a saddle\u201d which comes at the end of the parameter above Equation (13) should come earlier. When you cite the result of Bach according to which all local mins are global mins, you should indicate that convergence to those minimas is not necessarily guaranteed.\n\npage 6\n\n- At the beginning of the page, what do \\mathcal{B}_C and \\mathcal{B}_R stand for here? I guess this is related to the corresponding norms in (16)\n- I don\u2019t see the link between the variational formulation you introduce in (17) and the regularization terms appearing in each of your formulations (19), (20), (21). In particular in (21), what do the sets \\mathcal{B}_C and \\mathcal{B}_R represent? \n- It is not clear to me how formulation (3) can be written equivalently as (21). In particular, if the original formulation (3) is NP hard to solve, how can the Burer Monteiro factorization become tractable? Either you have poorer/lose convergence guarantees, or you must have some conservation of complexity. Some more details would be welcome\n- At the end of the page, in the last paragraph, why would it make sense to take X = I? Isn\u2019t X used to represent the feature matrix ?\n- Equation (22), Either you add the hat on the Z subscript of the gradient, or you remove it from the argument of F\n\npage 7\n- The transition from (22) to (23) in the case of the Relu is not clear. What does \\mathcal{B}_2 mean in this case ?\n- if you look at (21) the K_j matrix multiplies U_j (the left factor of Z) while in (23) it multiplies a vector u, which from the constraint u \\in \\mathcal{B}_2  seems to be the vector appearing in the definition of the D norm in (17).. This is not clear\n- I would remove the paragraph \u201cWe should note that some stationary points are clearly present in any problem, such as (U\u02c6 , V\u02c6 ) = (0, 0), so we cannot conclude that all stationary points are global optima. However, in certain cases, the optimality gap of stationary points (22) is always zero as we show next. \u201d which is unclear\n\npage 8\n- In the statement of Corollary 3.7.1. What is the meaning of Z^{(j)}_{k\u2019} ? What is the meaning of \\mathcal{B}_2 ?\n- In the statement of Lemma 3.8\n- In the statement of Corollary 3.8.1, it would be useful to recall the meaning of d,c,m. d is the number of features. How about c, m ?\n- In the statement of Corollary 3.8.1. Again, I wondering if what you mean is not m^*\\geq min\\left\\{d^2, cd\\right\\}?\n\npage 9\n\n- The plots in figure 2 look very similar. Also from formulation (20) which seems to be the one you use, using d=2, c=3, does not look like a very challenging scenario. In fact you know you are in the regime in which all local minimas are also global. Although I know there can be saddles as well, it might be more interesting to study the evolution of the optimality gap for larger values of n. \n- \u201cWe find that GD applied to the BM factorization finds \u201csubtle\" saddle points: not quite lo- cal minima, but close\u201d. Do you mean that optimality gaps are small but not small enough to ensure a local minimum ?\n- \u201cthere is only a minor relationship between the optimality gap and the size of the BM factorization m \u201d. I would replace \u201csize\u201d by rank (which seems more appropriate to me here). Also when you say \"there is only a minor relationship between the optimality gap and the size of the BM factorization m\u201d, it seems to me that the optimality gap is independent from pretty much all the quantities you consider except for the regularization. You should be able to get the analytic expression of the gap. How does this expression vary as a function of m, n? This should be further discussed. \n- The last paragraph of section 4 is not very clear. In particular, I don\u2019t see why the experiments particularly highlight the need to consider saddles (i.e. in fact if you look at the plots, you see that for m>max(c,d) the gradient iterates converge to the global minimum of the loss. To me, the relevance of saddles would be illustrated if the bound was non zero in that regime where all local mins are also guaranteed to be global mins). \n \nAdditional typos:\n- page 6, first line : \u201cprovides a novel result in the form of a optimality gap bound\u201d \u2014> \u201caN optimality gap bound\u201d \n- same page, next sentence, \u201cTo our knowledge, this is the first result of that generalizes the optimality\u201d \u2014> \u201cthe first result that generalizes\u201d \n- page 7, below equation (24) \u201cwe can be assured\u201d \u2014> \u201cwe can ensure\u201d\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_jgFt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_jgFt"
        ]
    },
    {
        "id": "gCHRkkWqFHB",
        "original": null,
        "number": 4,
        "cdate": 1667582874252,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667582874252,
        "tmdate": 1670862884600,
        "tddate": null,
        "forum": "rnN4pHyf6jD",
        "replyto": "rnN4pHyf6jD",
        "invitation": "ICLR.cc/2023/Conference/Paper3065/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "It has been shown by Ergen and Pilanci (2020) that 2 layer relu neural networks with squared L2 regularization are equivalent to regularized convex problems. The latter convex problems are in extremely high dimension and thus not tractable.\nThe regularization in the equivalent convex problems encourages low rank. hence, the paper proposes to use a Burer-Monteiro factorization to solve them.\n",
            "strength_and_weaknesses": "I see several weaknesses:\n\n- The improvement is marginal. There are such results of tens of types of 2 layer neural networks (see the 9 cited, all in the last two years by Pilanci, Ergen, Sahiner and coauthors). This is yet another paper exploiting the same idea, thus the incrementality of the proposed approach.\n- The focus is on 2 layer neural networks, which contradicts the claims about \"useful practical information\". Can the authors point relevant practical papers in the literature where linear 2 layer networks are used?\n- The experiments are in dimension 2, which tends to indicate that the methods scales very badly with the dimension/rank of the data, as all convex reformulation methods for neural networks. It seems to me the benefits of the method, compared to SGD, are only on toy setting like this one, and the method becomes impractical in real life scenarios.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, but the results are very incremental and with null practical impact.",
            "summary_of_the_review": "Incremental contribution to the field of convex reformulations of neural networks. Experiments are in dimension 2, and models limited to 2 layers.\n\nI would update my score if the rebuttal included an experiment with the number of samples and the number of features in the thousands (without the data matrix being low rank).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_JjJ3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_JjJ3"
        ]
    },
    {
        "id": "nIcbfgdHhZD",
        "original": null,
        "number": 5,
        "cdate": 1667585207481,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667585207481,
        "tmdate": 1670959751776,
        "tddate": null,
        "forum": "rnN4pHyf6jD",
        "replyto": "rnN4pHyf6jD",
        "invitation": "ICLR.cc/2023/Conference/Paper3065/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies the problem of formulating several (non)linear two-layer neural networks (NNs) as equivalent convex optimization problems, where existing results of equivalent convex optimization problems are computationally expensive.\n\nThis work proposes to use Burer-Monteiro (BM) factorization to obtain equivalent and computationally tractable non-convex alternative (rather than convex optimization problems) with no spurious local minima.\n\nSection 3.1 first calculates BM factorization for convex MLP with linear, gated ReLU, and ReLU activations, as shown in Eqs. (19) - (21). Theorem 3.4 characterize the relative optimality gap in and show that under some rank conditions the stationary points of BM factorization  in Eq. (16) are global minimizers of the equivalent convex optimization problem of Eq. (18), meaning that solving stationary points Eqs. (19) - (21) for those NNs could obtain global minimizers of their existing equivalent convex optimization problems.\n\nSection 3.2 first characterizes two-layer convolutional neural networks (CNNs) with arbitrary linear pooling operations as a convex program (Theorem 3.6). Lemma 3.7 gives BM factorization of the convex CNN problem with ReLU activation. Corollary 3.7.1 gives a result for stationary points of Eq. (27) are global minimizers of Eq. (26), under the condition of Eq. (28).\n\nSection 3.3 then studies self-attention networks. Lemma 3.8 shows the BM factorization of the the convex self-attention problem with linear activation (gated ReLU and ReLU activations are also calculated in the appendix), and Corollary 3.8.1 shows that Eq (29) has no spurious local minima as long as the number of heads is large enough.\n\nSection 4 uses experimental results on synthetic dataset, with a gated-ReLU two-layer MLP; as well as (shown in the appendix) on CIFAR-10 and Fashion-MNIST datasets, with  two-layer gated ReLU CNNs. The results show that BM factorization enables layer-wise training of two-layered MLPs and CNNs, achieving comparable results to end-to-end training of networks.",
            "strength_and_weaknesses": "**Strength**:\n\n1. The idea of using BM factorization to improve the computational efficiency of equivalent convex problems of two-layered NNs is interesting, and the results seem novel.\n2. The presentation of results is clear and easy to follow.\n3. Experimental results can verify the proposed BM factorizations.\n\n**Weaknesses**:\n\n1. The neural networks studied in this work are not practical, mainly two-layered MLPs and CNNs. This weakness is from existing equivalent convex optimization problems are for two-layered models.\n2. The results for self-attention networks are interesting. However, it has a similar problem of using linear activation, which makes it questionable how useful those results are in practice.\n3. The characterizations of relative gaps and conditions for stationary points being global minimizers are presented without explanation, making the implications not clear to me, e.g., the gradient norm in Eq. (23), and the trace conditions in Eq. (28). Could the authors provide some intuitions or detailed explanations for the audience to better understand the meaning of those results?\n4. The comments of BM factorization finds \"subtle\" saddle points at the end of Section 4 is interesting. However, it is not clear to me how is it concluded that from the results in Figure 2 that the found solutions are saddle points rather than local minima, and is there a calculation to verify that?",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good, the problems and results are clearly presented, and relate works and techniques are also discussed well enough. The quality and originality are OK, since most results are from using BM factorization on existing equivalent convex optimization problems of NNs, and some results are not known before.",
            "summary_of_the_review": "The idea of using BM factorization to improve the computational efficiency of equivalent convex problems of two-layered NNs is reasonable and novel to me. However, most of the results are for two-layered NNs and restricted models like linear activations in self-attention networks, which makes it questionable how useful those results could be in practical NNs.\n\n\n\n=====UPDATE=====\n\nI would like to thank the authors for the feedback. I increased my score since it addressed my questions well.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_f4LA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3065/Reviewer_f4LA"
        ]
    }
]