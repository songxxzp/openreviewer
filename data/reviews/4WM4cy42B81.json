[
    {
        "id": "tXwCzXcM1h",
        "original": null,
        "number": 1,
        "cdate": 1666268196894,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666268196894,
        "tmdate": 1668733310737,
        "tddate": null,
        "forum": "4WM4cy42B81",
        "replyto": "4WM4cy42B81",
        "invitation": "ICLR.cc/2023/Conference/Paper1068/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel domain adaptation active learning scheme building upon evidential deep learning. Namely, it computes distributional (or epistemic) uncertainty as mutual information and data (or epistemic) uncertainty as the expected entropy.  Then it uses a two stage approach that selects  to label the largest data uncertainty samples from a pool of the largest distribution uncertainty samples. The EDL method is enhanced to minimize data and distribution uncertainty for the newly labeled target samples.  This helps to identify remaining gaps in the labeling of the target samples.  The paper demonstrates via extensive experiments that this new Dirichlet-based Uncertainty Calibration method is able to find a better set of target samples to label than state-of-the-art active learning and domain adaptation active learning methods.",
            "strength_and_weaknesses": "The strength of the paper is that is proposes a novel domain adaptation active learning method that is a significant modification of evidential deep learning in its use of distribution and data uncertainty in its training and its selection of the samples to label.  The experimental validation is done for domain adaptation in image classification and semantic segmentation applications. The experimental results also provides ablation and other studies demonstrating the effectiveness of various components of the proposed DUC method. The weaknesses are fairly minor.  It would be nice if the experimental sections explains that datasets represent different domains to serve as the source and target domains. Perhaps one could quibble that other single-pass Dirichlet-based methods should be studied. Nevertheless, the study is already very extensive.   ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written.  The distinction between data and distributional uncertainty to enhance EDL training for active learning in domain adaptation and for selecting the labeled sets is novel and significant.  The experimentation in terms of baselines and two application areas (image classification and semantic segmentation) is very strong.  The explanation of the approach is detailed enough to reproduce the work. \n\nPerhaps once could quibble that the KL regularization term in EL does not appear to be annealed in (8).  In training EDL, the annealing is important as EDL must learn to discriminate before it can calibrate evidence. Perhaps for the intermediate steps in active learning to select samples to label, this is not critical. \n\nRegarding other Dirichlet-based single pass methods, the  authors argue that Dirichlet Prior Network requires specification of an OOD dataset, whereas, EDL does not.  However, EDL training may not properly capture the epistemic uncertainty.  Not sure this matter, but this reviewer is curious if Dirchlet Posterior Networks (Posterior Network: Uncertainty Estimation without\nOOD Samples via Density-Based Pseudo-Counts, NeurIPS 2020) and/or Generative Evidential Neural Networks (Uncertainty-Aware Deep Classifiers Using Generative Models, AAAI 2020) can provide better results.  Both of these methods do not require one to explicitly provide OOD samples. All of this would be interesting as future work.",
            "summary_of_the_review": "The paper proposes a novel approach that is sensible and validated in extensive experiments spanning two different applications.\n\nI have looked over the revised paper and the authors' response.  I think this is a good paper that should be accepted, and my recommendation reflects this.  I think the other reviewers may not have appreciated the significance of the changes to EDL so that it can be applied for active domain adaptation. I can attest that the contribution of the paper in that regard is significant. I also believe that the authors have significantly enhanced the experiments in response to the other reviewers. The experimental section is comprehensive and demonstrates an advancement relative to the SOTA.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not have any ethical concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1068/Reviewer_Rp4n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1068/Reviewer_Rp4n"
        ]
    },
    {
        "id": "Mwyd4sODdg",
        "original": null,
        "number": 2,
        "cdate": 1666668456814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668456814,
        "tmdate": 1666790155320,
        "tddate": null,
        "forum": "4WM4cy42B81",
        "replyto": "4WM4cy42B81",
        "invitation": "ICLR.cc/2023/Conference/Paper1068/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an uncertainty aware active learning domain adaptation based on imposing a dirichlet prior over model predictions. This allows them to get a distribution over model predictions, which in turn enables them to compute 2 kinds of uncertainty measures for each target datapoint. 1) target distribution uncertainty 2) data complexity uncertainty. \nUsing these two measures of uncertainty the active learning is done in 2 steps, first they select data points with highest amount of target distribution uncertainty and then from those datapoints with highest data uncertainty are chosen for being assigned ground truth labels. \nThe results on 2 classification datasets and 2 segmentation datasets show promising results.  \n",
            "strength_and_weaknesses": "\nStrengths\n- simple yet effective method\n- active domain adaptation is a practical approach for domain adaptation. Probably you can improve results by incorporating ideas from semi-supervised learning to use the unlabeled data in training as well. \n- extensive experiments and calibration analysis\n- ablation studies are good\n\nWeaknesses:\n- there are no error bars on the results of the experiments. Its important to have error bars to know if the improved results are statistically significant .\n- I think experimenting on more datasets would add merit to the paper and give more credibility to the approach. large scale datasets like Vista could be good additional datasets. ",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is not novel by itself , as the authors also mention it has been used for OOD detection. However,  it may to be novel in the context of active learning domain adaptation which makes sense. ",
            "summary_of_the_review": "I think the novelty is limited in the theoretical sense but its been used in an interesting application. That being said, the experiments are sound and do confirm the merits of the approach.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1068/Reviewer_arzy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1068/Reviewer_arzy"
        ]
    },
    {
        "id": "EefJ0uxhk24",
        "original": null,
        "number": 3,
        "cdate": 1666725081453,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725081453,
        "tmdate": 1666725081453,
        "tddate": null,
        "forum": "4WM4cy42B81",
        "replyto": "4WM4cy42B81",
        "invitation": "ICLR.cc/2023/Conference/Paper1068/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a solution to the problem of active domain adaptation, where limited target data is annotated to maximally benefit the model adaptation. The proposed solution called \"Dirichlet-based Uncertainty Calibration (DUC)\" achieves both target representativeness and mitigation of miscalibration. The authors place a Dirichlet prior on the class probabilities which helps interpret it as the distribution over the probability simplex. There is a two-round selection strategy, uncertainty of distribution and uncertainty of data, to select the target samples for annotation. The authors show the superiority of their approach on cross-domain image classification and semantic segmentation tasks.  ",
            "strength_and_weaknesses": "Strengths:\n\nActive domain adaptation is a new sub-track of domain adaptation and the authors have proposed a new DUC approach to solving it. \nThe authors dealing with this use of Dirichlet prior and probability space on simplex mitigate the issues of the point estimate and consider all possible predictions.\nThe authors provide a detailed analysis of their approach with the intuition behind the optimization. \nThe results on miniDomainNet and Office-Home show improvement over the state-of-the-art (SOTA) models. This is true for GTAV and SYNTHIA experiments as well.\nThe ablation study in Table 5 shows that having loss of distribution uncertainty and loss of data uncertainty with the selection process helped in improving other SOTA counterparts.\nAlso, the authors studied the effect of different first-round selection ratios \u03ba% on the Office-Home dataset which remains almost the same except for k=1% , where it was lowest (Fig 4a).\n\nWeaknesses:\nTwo main contributions in this work are derived from previous works:\n\t1: Predictive uncertainty estimation via prior networks (Malinin & Gales, 2018).\n\t2: Evidential deep learning to quantify classification uncertainty (Sensoy et al., 2018).\nIn my opinion, the above two prior work have been put together in this area of active DA.\n\nWhy has the author not reported the result on the VisDA-2017 dataset used in the EADA paper, since EADA is the current SOTA model? Why has the author not included the EADA paper as a baseline for the GTAV dataset in Table 3?\nIt is not reported in the paper where these numbers in Table 1 for EADA on miniDomainet have come from. \nIt is not reported in the paper what is the performance gap between Active DA methods and the \u201cFull Supervised\" on the target domain. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and nicely structured to read. The novelty side of the work is limited. \nFig 1a: the color bar is not clear, even the description of Fig 1b is very short for readers to clearly understand the problem and what needs to be done.\n",
            "summary_of_the_review": "Overall, the paper is well written, and the results show improvement over the baseline models. However, I feel the technical contribution and the novelty are limited.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1068/Reviewer_CH6i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1068/Reviewer_CH6i"
        ]
    }
]