[
    {
        "id": "ORUL16EbWG",
        "original": null,
        "number": 1,
        "cdate": 1666171392328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666171392328,
        "tmdate": 1666171392328,
        "tddate": null,
        "forum": "Fsd-6ax4T1m",
        "replyto": "Fsd-6ax4T1m",
        "invitation": "ICLR.cc/2023/Conference/Paper4928/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper proposes a metric for comparing learned representations, and furthermore doing it so that models can be switched based on data. That sounds useful for a variety of purposes. That said:\n\nThis paper was assigned to a completely wrong reviewer; an indication of some sort of a failure in the ICLR process. I don't have the necessary background to appreciate much of this, and reading the paper very carefully would have taken too long -- and probably wouldn't have been too useful for the review process anyway. I have informed the AC that I'll essentially skip the review by putting in a minimum confidence opinion that should not count towards the outcome.",
            "strength_and_weaknesses": "-",
            "clarity,_quality,_novelty_and_reproducibility": "-",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_4XSa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_4XSa"
        ]
    },
    {
        "id": "RgO4ezdHgWe",
        "original": null,
        "number": 2,
        "cdate": 1666502939567,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666502939567,
        "tmdate": 1669670267997,
        "tddate": null,
        "forum": "Fsd-6ax4T1m",
        "replyto": "Fsd-6ax4T1m",
        "invitation": "ICLR.cc/2023/Conference/Paper4928/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a method to evaluate the representations of deep learning models by combining predictions of readout models. The readout models are selected based on a switching policy according to the Minimum Description Length, which is an evaluation metric for probing models that consider both model accuracy and complexity. The switching policy is determined by a stochastic matrix that defines the probability of replacing the current expert model. With the proposed method, the author evaluates the representation efficiency of popular deep learning with different scales on a set of downstream tasks and investigates the data efficiency.",
            "strength_and_weaknesses": "Strength: \nThe author proposes a new method to evaluate the representations of deep learning models based on a selection of different readout (or expert) models according to the MDL. If the author could adequately approve the advantage of the proposed metric, this could bring a new angle for model evaluation.\n\nWeakness:\n1. Although the authors have claimed that a rigorous method for representation evaluation is lacking for deep learning models, they didn\u2019t show the advantage of their evaluation methods over others, especially in the experiments. Furthermore, for most unsupervised models, the representation quality is evaluated indirectly by multiple downstream tasks, where the model is required to show consistent improvements on multiple tasks to show its advantage. If the author claims that their method could be more accurate in evaluation, they should at least show some empirical comparisons, where the multi-task evaluation metrics fail but the readout switching method succeeds.\n\n2. The proposed switching method is based on Minimum Description Length, which is used as an alternative method for language model evaluation where the probing tasks fail to reflect differences in representations [1]. To adapt this in the vision task (as the models used in the experiment are vision models), the author should show that the vision model suffers from similar problems, or the MDL could provide a better evaluation solution than current solutions.\n\n3. Even if the author's claim is true, the representation could still be compared by a combination of the performance on several probing models without switching. If the author wants to show the advantage of the proposed switching method, there should be an experiment that compares the evaluation difference of using a) the proposed switching method and b) a combination of probing models without switching.\n\n4. The author claims that different probing models could bring differences during the evaluation of the representation of deep learning models. However, the selection of switching policy may also affect the evaluation metric. If so, what is the principle to find the best selection strategy?\n\n5. According to Algorithm 1, the switching happens at each time step when taking in a new pair of data (x_t, y_t). So what will happen to the proposed metric if the order of evaluation data changes?\n\n6. For the experiment in 4.1, the authors apply different supervision methods on the two models (looks like the \u201cSupervised\u201d is the same for both?). Why not just apply the proposed five supervision methods to both models for comparison?\n\n7. Although the author has listed the evaluation scores of different models and shown the rankings, there remains a question of whether the models behave consistently with this conclusion. For example, in Figure 4, the author shows that a deeper network could be worse than a shallow one (like ResNet, and ViT-B16 vs ViT-L16) in \u201cSupervised\u201d. This conclusion is quite questionable as shown in [2][3]\n\nWriting:\n1. For formulas, especially in section 2, it would be easier for the reader to review if the formula could be put in a separate row.\n2. In the last paragraph of Regret Bounds, DNN has been defined at the beginning.\n3. In 4.1, what does it mean by \u201cpre-train on supervised losses\u201d? Is the supervision method the same for ResNet and ViT?\n\nReference:\n\n[1] Voita, Elena, and Ivan Titov. \"Information-theoretic probing with minimum description length.\" arXiv preprint arXiv:2003.12298 (2020).\n\n[2] Gao, Shang-Hua, et al. \"Res2net: A new multi-scale backbone architecture.\" IEEE transactions on pattern analysis and machine intelligence 43.2 (2019): 652-662.\n\n[3] Liu, Ze, et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[4] Khan, Salman, et al. \"Transformers in vision: A survey.\" ACM computing surveys (CSUR) 54.10s (2022): 1-41.",
            "clarity,_quality,_novelty_and_reproducibility": "In this paper, the author proposes a switching policy to select probing models for representation evaluation. It is quite clear for the idea of the paper. I think the major novelty is the authors introducing the switching scheme during the evaluation, not simply applying the MDL. Since the proposed method is time-dependent and the author has re-split the dataset, it would be easier for other researchers to reproduce the results if such details are revealed.",
            "summary_of_the_review": "This paper proposes an evaluation metric for representations of deep learning models that rely on switching within multiple read-out models based on MDL. The author has compared different models, i.e., ResNet and ViT, based on the proposed metric. However, a major concern is that: Is the proposed method could evaluate representations effectively, or at least better than the current methods? If the authors plan to show the stability and reliability of the proposed methods, there should at least be some experiments that show the failure of compared evaluation metrics, such as by a weighted average over the performance of some downstream tasks. Furthermore, if the authors want to propose an effective evaluation metric, it is necessary to show consistency when selecting different switching strategies.\n\n----Post Rebuttal----\n\nThe author has answered most of my questions and the revised version has motivated me to move my rating from 5 -> 6. For me, the author has proposed an effective (or stable) method to evaluate the representation by combining evaluation over different readout models. However, I still have some concerns about the efficiency of the switching policy, which may be left in future works.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_pMpS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_pMpS"
        ]
    },
    {
        "id": "SzpkpdCw0I",
        "original": null,
        "number": 3,
        "cdate": 1666635883724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635883724,
        "tmdate": 1666635883724,
        "tddate": null,
        "forum": "Fsd-6ax4T1m",
        "replyto": "Fsd-6ax4T1m",
        "invitation": "ICLR.cc/2023/Conference/Paper4928/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduce a metric to compare representations in deep learning architectures. This metric is based on thinking of representations as a model selection problem and using the minimum description length principle. The authors also propose the intriguing idea of model switching where different architectures can be used depending on the task demands and characteristics. The metric is applied to traditional architectures for vision encoders across multiple tasks. ",
            "strength_and_weaknesses": "Strengths\n\nThe paper introduces a fresh look at how to compare different architectures and their associated representations of features. \n\nThe proposed metrics claim to take into account the model complexity and data efficiency, which are either not incorporated or only indirectly addressed in other metrics such as when merely comparing task performance.\n\nThe paper presents an extensive empirical demonstration of the use of the proposed metrics across multiple tasks, datasets, and interesting problems such as model scaling.\n\nWeaknesses\n\nThe main results are shown in somewhat unorthodox ways. The rank variable used throughout the figures is a bit hard to interpret. It would be great to expand on the second paragraph of the Experiments section to better explain how this is computed, given that this is critical to interpreting all the figures. Further, it would also be useful to understand how these values relate to other metrics such as accuracy in the datasets, performance for a given number of training examples, and even to show the MDL scores themselves.\n\nWhat is less clear to me about the model switching proposed here is who and when decides when to switch models. What would be particularly useful is a strategy to be able to switch across models for an arbitrary novel task and dataset. It is not clear to me that this is what the authors are showing. Given the results, it is less interesting to decide a posteriori, which models performed best and when. Perhaps the authors are showing predictability in model switching to new tasks/data but this was not clear to me.\n\nI love theorems and theorems can obviously be more valuable than empirical measures. However, it was a bit difficult to follow in this case, what the connection was between Theorem 1 and the empirical results shown in the subsequent figures (also given that some of the model assumptions are not followed in the architectures as the authors clearly spell out). \n",
            "clarity,_quality,_novelty_and_reproducibility": "There are several spelling mistakes. Just to point out a few:\nCan compresses \uf0e0 can compress\nHebbien \uf0e0 Hebbian \nRunning a grammar/spell checker would be useful. \n\nIn general, the paper is very well written. I pointed out above a couple of items that would benefit from further clarification.\n",
            "summary_of_the_review": "This is an interesting and fresh look at the problem of how to characterize and compare different models and their representations. An ideal metric for comparison would take into account not just how the models perform in one or a handful of tasks, but rather a more comprehensive view of the complexity of the architecture, their sample efficiency in training, performance, and generalization. This work is a step in the right direction by introducing new metrics that can satisfy many of these requirements.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_mVad"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_mVad"
        ]
    },
    {
        "id": "05u2gBx9hg",
        "original": null,
        "number": 4,
        "cdate": 1667377622771,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667377622771,
        "tmdate": 1668518204067,
        "tddate": null,
        "forum": "Fsd-6ax4T1m",
        "replyto": "Fsd-6ax4T1m",
        "invitation": "ICLR.cc/2023/Conference/Paper4928/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors of this submission propose to use the Minimum Description Length (MDL) principle to evaluate the quality of the learnt representations in DNNs. In particular, they follow the prequential approach to estimate the description length, which assumes that *good* representations should allow for training prediction models for down-stream tasks in a data-efficient manner.\n\nConcretely, the authors train a set of K models in a prequential manner (with SGD) and use a switching strategy to choose the best model for a given number of training samples. The final code length is then computed as the cumulative prediction loss of this model ensemble. \n\nTo evaluate their proposed method, the authors perform experiments with various DNN architectures (ResNets and ViTs of different sizes) on a wide range of datasets. Further, the authors discuss insights obtained from their experiments, such as regarding the pre-training paradigm used to train the encoders (e.g., supervised vs. self-supervised), the model size, or the transfer learning dataset.",
            "strength_and_weaknesses": "### Strengths \n- Given the importance of self- and unsupervised representation learning and the variety of methods to do so, evaluating the learnt representations fairly and thoroughly is of great interest to the community. With this submission, the authors provide an interesting approach for evaluating the model representations beyond linear probing.\n- The authors evaluate the proposed method on a wide range of experiments, which allows for discussing and analysing differences in the learnt representations with respect to different dimensions of interest (pre-training objective, model size, down-stream dataset.)\n- The idea of using MDL to evaluate the model representations is convincing, as it allows to do so simultaneously with models of different complexity whilst still capturing the representations quality in a single number.\n\n### Weaknesses\nWhile I find the idea of the proposed method intriguing, I have issues with several aspects of the current manuscript. \n\n- **Novelty and contextualisation:** The method is presented with an emphasis on the idea of using MDL for evaluating the learnt representations, which gives the impression that this is in itself novel. However, as the authors also state in the background section, Voita & Titov (2020), as well as Yogatama et al. (2019) have already investigated MDL for evaluating representations. Unfortunately, from the current discussion of the related work, the differences to prior work and the novel aspects and their relevance of the proposed approach do not become entirely clear. More generally, many statements lack appropriate references (e.g., the entire introduction has only 3 references), which will make it difficult for readers unfamiliar with the topic to follow.\n- **Benefits and evaluation of the proposed approach:** While the proposed approach is generally well-motivated, the practical benefits over linear readout models do not become clear. How do the rankings produced by the proposed approach compare to rankings obtained from simple linear readout models? In what sense are the rankings \"better\"? What are tangible benefits? \n- **Discussion of limitations**: One of the key advantages of linear probing or KNN evaluations of the learnt representations is their simplicity; i.e. the result is largely independent of any hyperparameter settings and easy to obtain and compare. The proposed approach combines many different DNN architectures with varying complexity, which all need to be trained and more heavily depend on the optimisation procedure and hyperparamters, which makes the results potentially less stable and requires careful parameter tuning. I would appreciate if the authors could extend the discussion on this. \n- **Organisation and state of the manuscript:** In its current form, the manuscript is difficult to read, mainly due to the following reasons. \n\t- The structure of the manuscript is not easy to follow and the relevance of some sections is unclear. For example, section 3 abruptly switches to regret bounds and introduces Theorem 1, without discussing why this is relevant. In fact, the theorem is introduced and discussed, only to then say that the assumptions of the theorem do not hold in the context of DNNs, i.e. the context of this work. Similarly, an online version for the MDL computation is introduced and discussed, only to then say that this is not used in this work (\"To facilitate experimenting with different switching strategies, we use this 2-stage implementation for all our experiments.\"); this statement conflicts with the description of the experimental details in section 4 (\"We use the online learning framework as described in Section 3\"). I would appreciate if the authors could clarify why these sections are relevant and if the discussion around these sections were improved.\n\t- The mathematical notation could be improved. For example, in the fourth line of the \"Read-out Model Switching\" paragraph, the formula contains a \"p_{\\xi_t}\", which is referred to as \"p_k\" afterwards. Later, a p(k) is used, which refers to p({\\xi_1}) as far as I understand (however, p(k) = 1 iff k=1, which should simplify the L_{BM} equation further), which adds unnecessary notational ambiguity and makes it difficult to follow.\n\t- Finally, as discussed above, I would highly appreciate if the authors could add citations when appropriate (especially in the introduction). \n- **Experimental protocol and ablations**: Some of the experimental details remain unclear. How long are the individual models trained on each time step? Is the prequential method stable w.r.t. the order of the data? Why are some of the datasets reshuffled? This seems to be non-standard and I fail to see why this is required. What happens without reshuffling?\n\nMinor: I would recommend to carefully check the paper for grammatical correctness for the final version.",
            "clarity,_quality,_novelty_and_reproducibility": "For details, please see strengths & weaknesses above.\n\nI think clarity can be improved (see \"Organisation and state of the manuscript\").\n\nThe quality of the content is generally good, but the benefits and novelty of the proposed method are not entirely clear to me (see \"Novelty and contextualisation\", \"Benefits and evaluation of the proposed approach\", and \"Discussion of limitations\"). \n\nWith some additional details, the method should be reproducible. I would urge the authors to additionally release their code.\n",
            "summary_of_the_review": "As discussed above, I think the proposed approach is interesting and could be a valuable addition to the literature on evaluating DNN representations. However, in its current state (see weaknesses), I think the paper is not ready for publication.\n\nEDIT: After the authors' response, some of my concerns were addressed and I therefore raise my score to \"6: marginally above the acceptance threshold\".",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_GYkn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_GYkn"
        ]
    },
    {
        "id": "FcXq04s_nA",
        "original": null,
        "number": 5,
        "cdate": 1667463497533,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667463497533,
        "tmdate": 1670304477048,
        "tddate": null,
        "forum": "Fsd-6ax4T1m",
        "replyto": "Fsd-6ax4T1m",
        "invitation": "ICLR.cc/2023/Conference/Paper4928/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors claim a new metric on the quality of representation, Minimum Description Length (MDL) with model switching. \nWith the representation model and readout model given, the proposed metric is the optimized upper bounds for the regret in switching readout model with respect to MDL loss. \nThe proposed metric can also evaluate data efficiency on online learning.\nFor the experiment, authors use ImageNet and VTAB database with ResNet-50 and ViT/B16 architectures and rank the representation quality with the claimed metric. ",
            "strength_and_weaknesses": "## Strength\n\n* **Novel approach in evaluating representation quality.** Authors provide a new way of assessing representation quality, and their algorithm for obtaining this metric is straightforward.\n\n* **Adopting multiple readout model.** Good representations may not store the crucial information of data in a linear shape. Adopting multiple readout model can alleviate this concern. It was interesting to see data efficiency of each readout models.\n\n## Weakness\n\n* **Justification on using model switching process.** The switching model forces to change the status of $\\eta$ in after some time span, but the case shown in Figure 1, for example, the optimal selection should stick to one model after observing enough amount of data. The switching process is not likely to achieves the optimal sequence of readout model selection. This gap must be discussed. Instead of following a process with discrete status, using an ensemble method may relieve this problem, vaguely speaking.\n\n* **Justification on regret bound.** As acknowledged in the paper, the regret bound (2) holds for exponential family with MLE $\\theta$. Thus, this upper bound crucial for constructing Algorithm 1 may not hold when the readout models include MLP.\n\n* **The claimed metric needs an empirical evidence about the quality of the metric** to assert that the proposed method is feasible. E.g., comparing the MDL rank with quantitative performance metric of the represented data on practical tasks such as classification or domain adaptation.",
            "clarity,_quality,_novelty_and_reproducibility": "I could understand the contents of the paper, but their were some grammatically incorrect sentences that needs to be fixed for the clarity. The claimed work seems original. Algorithms, architectures, and hyper-parameters are well documented for reproducibility.\n\n### Typos\n\nSection 3, first paragraph, line 7: \"a fixed model\"\n\nCaption for Figure 5: \"Data efficiency\"",
            "summary_of_the_review": "Although the authors provides a technical algorithm for obtaining a novel representation quality metric, it lacks theoretical justification and empirical evidence that the metric is feasible to assess quality of representation. \nThus, I reject this submission.\n\n**Post Rebuttal**\n\nThe revised script included discussions and materials that I expected for supporting the argument of the paper. Thus, I have updated my scores.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_Jszx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4928/Reviewer_Jszx"
        ]
    }
]