[
    {
        "id": "SoNNyFNU2Hp",
        "original": null,
        "number": 1,
        "cdate": 1666004387506,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666004387506,
        "tmdate": 1671004522517,
        "tddate": null,
        "forum": "EW00yKKLiX",
        "replyto": "EW00yKKLiX",
        "invitation": "ICLR.cc/2023/Conference/Paper3362/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper investigates the application of the \"top-k trick\" known from SGD, where only the $k$ examples with the highest loss values are used in backward pass, to the sharpness-aware minimization (SAM) method. It shows via extensive experiments that this can reduce the computational cost of SAM to the level of vanilla SGD while maintaining the generalization-boosting effects of SAM.",
            "strength_and_weaknesses": "### Strengths\n\n1) The paper takes a highly relevant technique, SAM, and improves its computational efficiency. In my opinion that is commendable work with a direct positive impact on practitioners.\n\n2) The paper is well-written. It does a good job of positioning itself in the context of existing work and summarizing the work it builds upon. Overall, the claims are scoped and phrased adequately.\n\n3) The experimental comparison is well done. The selected benchmark problems offer a good amount of variety. The most relevant ablation/sensitivity studies (random examples instead of top-$k$, effect of $K_1$ and $K_2$) are included.\n\n\n### Weaknesses\n\n1) The paper is of limited originality. It combines the known optimization method SAM with the known (from vanilla SGD) speed-up method of selecting the examples with the top-k losses. Personally, I am fine with such a rather incremental paper, but it requires excellent execution.\n\n2) My main criticism is that I sorely miss one baseline to compare against in the experiments: top-$K$-SGD (e.g. with $K=K_1+K_2$). The paper evaluates methods with regards to their trade-off between generalization and computational cost. For SAM, it uses the \"top-$k$ trick\", for SGD it does not. In my view, that is an incomplete picture. Assuming that top-$K$-SGD can maintain the generalization performance of SGD for reasonable values of $K$ it would naturally be the fastest method. This would make the claim of providing \"significant generalization boosts over [...] SGD at little to no additional costs\" rather vacuous. (Importantly, I don't think that would reduce the value of the paper. It is still worthwhile investigating the application of the top-k trick to SAM. But I _do_ think that this baseline is needed to give the full picture.)\n\n3) The proposed methods computes the ranking based on the losses computed for the _ascent_ step and then uses that ranking for both the ascent and the descent step. In my view, the most straight-forward way of transferring the top-k trick from vanilla SGD to SAM would be to select the top-k examples _separately_ for the ascent step and the descent step. Since these loss values would be obtained at $w$ and $w+\\varepsilon_\\ast$, respectively, this could lead to slightly different results. Using the ranking from the ascent step might be a totally adequate strategy, but I think the first option should be mentioned/discussed in the paper and it would be nice to see an ablation study on that.\n\n\n### Update after Rebuttal\n\nThank you for your response. I see now that Table 8 in the appendix contains numbers for top-K-SGD on ImageNet. (However, that is a _single_ experiment.) If we compare top-K-SGD with K-SAM such that $K=K_1 + K_2$ for (roughly) equal computational cost, the test performance achieved by the two methods is actually quite close. In any case, I think this is an absolutely crucial baseline to include for all experiments and contrast the results for K-SAM with. Therefore, unfortunately this doesn't resolve my concern outlined in the original review and I will keep my recommendation as is.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nAs stated above, the paper is well-written and adequately positioned in the context of related work. The proposed algorithm is described in full detail.\n\n### Quality\n\nThe experimental comparison is very generally very well done, but I think one very important baseline is missing (see weakness 2).\n\n### Originality\n\nThis is a rather incremental paper, applying the top-k trick, known to work on SGD, to SAM.",
            "summary_of_the_review": "This is a well-written paper providing a small but significant improvement to a relevant technique such as SAM and demonstrates that improvement in extensive experiments. In terms of originality and scientific value, this is certainly a very incremental paper. That, together with the missing baseline of top-$K$-SGD (weakness 2), puts this paper below the bar for a top conference such as ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3362/Reviewer_T2UG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3362/Reviewer_T2UG"
        ]
    },
    {
        "id": "UrZUuidV2X1",
        "original": null,
        "number": 2,
        "cdate": 1666233412955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666233412955,
        "tmdate": 1666233412955,
        "tddate": null,
        "forum": "EW00yKKLiX",
        "replyto": "EW00yKKLiX",
        "invitation": "ICLR.cc/2023/Conference/Paper3362/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to sample two subsets from each batch (choosing examples with the largest loss) to perform the SAM algorithm. The authors justify their method by showing the cosine similarity between the gradients obtained before and after the subsample process. Empirical results on CIFAR-10/100, ImageNet, and BERT finetuning show the effectiveness of the proposes method.",
            "strength_and_weaknesses": "### Strengths\n- Motivation of this paper is clear as SAM usually doubles the computational cost of SGD.\n\n### Weaknesses\n- How do you obtain the K1 and K2 example specifically? Do you just randomly choose from the K1+K2 examples that have the largest loss? Will their order affect the result? The authors should elaborate more on the algorithm.\n- To me, it's surprising to see in Table 1 that just using top-K examples in SGD bring little performance drop. One possible reason is that the   batch size 128 used here is pretty small. Can the authors also provide analysis with a larger batch size.\n- The reported training time shows that K-SAM can achieve speedup compared to SGD, while on ImageNet it's much slower. Can the authors provide metrics like FLOPs, which is not impacted by the hardware.\n- It could be better if the authors can analyze whether we can save more compute in the ascent or the descent steps? Probably you can control the training time but vary the ratio of K1/K2. I assume that the ascent step can be performed with fewer example thus more savings.\n- For the BERT finetuning result, the authors should include the variance as it's know to be pretty high.\n- There are some other efficient SAM approaches that the authors can consider to compare with. For example, only performing SAM every k step[1], etc.\n\nLiu et al. \"Towards Efficient and Scalable Sharpness-Aware Minimization.\" CVPR 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe writing is generally good. But it's not clear to me how to obtain the K1 and K2 example. Additionally, there are too many similar tables in the paper, the authors should consider merging / reorganizing them. \n\n### Novelty\nThe idea is simple and straightforward, and selecting the top-K hardest examples has a long history.\n\n### Reproducibility\nCode is provided so I assume that the reproducibility is good.",
            "summary_of_the_review": "This paper has limited novelty, and I've got some questions for the authors (refer to the weaknesses part). Empirical results is not strong enough to overcome the drawbacks. So I vote for rejection for now.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3362/Reviewer_Y4Jo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3362/Reviewer_Y4Jo"
        ]
    },
    {
        "id": "tQRiXWACmYv",
        "original": null,
        "number": 3,
        "cdate": 1666530628373,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666530628373,
        "tmdate": 1666530975825,
        "tddate": null,
        "forum": "EW00yKKLiX",
        "replyto": "EW00yKKLiX",
        "invitation": "ICLR.cc/2023/Conference/Paper3362/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors study the effectiveness of an iterative stochastic minimization algorithm for SAM (sharpness aware minimization): at each iteration the batch is further reduced in size using a Top-K rule based on the value of the loss. The goal of this choice is to reduce the computational expense, taking the cost closer to vanilla SGD while hopefully retaining the benefits of full-batch SAM. Extensive experiments show that the approach is feasible and it is possible to keep almost the same performance of SAM while reducing the computation time to a similar level to that of SGD.",
            "strength_and_weaknesses": "The main weakness of this work is that it lacks novelty and originality. The idea of choosing the top-k elements of the batch with highest loss is natural and has been explored before. In particular the work of Kawaguchi and Lu 2020 present the idea in its most generic form and the presented algorithm is almost a simple application of the generic method in the context of SAM. The only difference being that because SAM is a two-step process, one has to choose a subset of the batch both for the ascent step and the descent step. I wouldn't be surprised if there were even older references before 2020 exploring the exact same idea of Top-k elements in a batch. Authors should be more transparent about the fact that their algorithm is a special case of that presented in Kawaguchi and Lu 2020, applied to SAM (perhaps with some minor modifications).\n\nAnother weakness is that the authors overstate the computational cost of SAM compared to SGD. It is known that SAM has double the cost of vanilla SGD, but traditionally a constant factor (of 2 in this case) is not necessarily a reason to worry or discard a method as \"too slow\". This is reserved for methods whose complexity increases beyond constants. For example SGD which needs constant time to compute an update, vs GD which has linear complexity in the size of the data. In the third paragraph of the introduction statements like \"the additional cost may make SAM too expensive for widespread adoption\" are simply an exaggeration and should be rewritten.\n\nEven though I started stating the weaknesses, I had a positive impression of the work, mainly due to the extensive experimental evaluation. In my view, the strengths of the paper are twofold:\n1. The problem is relevant and significant to some extent: it is great to halve the time required to achieve the benefits of SAM\n2. The extensive experimental results are convincing and confirm that it is indeed possible to apply Top-K in the context of SAM without a noticeable degradation of the results. The experiments presented cover many different settings like (a) different architectures (b) different and relevant baselines (c) different tasks like supervised learning with images and NLP tasks (d) different hyperparameter settings and (e) different computational infrastructure (single node vs distributed training). Overall I find the authors did a great job in this regard.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clarity: The paper is well written and I found it easy to read and understand except for two items: (a) the second paragraph in the introduction is huge (contains too many unrelated observations) and should be split at least in two to facilitate reading and (b) the notation used in the definition of the batch subsets in section 3 was really confusing and it took me awhile to decipher what the authors mean with the $\\{1. 2\\}$ notation: they actually use it to define the two sets $M_{K_1}$ and $M_{K_2}$ in a single line. I think it would be better to simply use a notation like $M_{K_j} = \\{ \\ldots \\}$, $j=1, 2$.\n\n2. quality: I find the writing and mostly the experimental section of really high quality. As I mentioned above the experiments shine in this work and are extensive and convincing, supporting the main claims.\n\n3. Novelty/Originality: As I mentioned above, this is the main weakness of the paper as it is a straightforward application of simple ideas that have already been presented in a more generic framework in the work of Kawaguchi and Lu 2020 and possibly many works before that.\n\n4. Reproducibility: the authors provide the code to reproduce the experiments. However, I did not test the code.",
            "summary_of_the_review": "The authors present an application of Ordered SGD (Kawaguchi and Lu 2020) in the context of SAM, showing through extensive and convincing experimental evidence that it allows almost a halving of the training time, while retaining almost the same accuracy numbers from the vanilla SAM method. I don't see a strong reason to reject the paper. The lack of technical novelty might be compensated by the experimental evaluation. It might be useful for practitioners in need of halving the cost of performing SAM.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3362/Reviewer_wYVX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3362/Reviewer_wYVX"
        ]
    },
    {
        "id": "znrtlKDF6kL",
        "original": null,
        "number": 4,
        "cdate": 1666789700515,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666789700515,
        "tmdate": 1671020570071,
        "tddate": null,
        "forum": "EW00yKKLiX",
        "replyto": "EW00yKKLiX",
        "invitation": "ICLR.cc/2023/Conference/Paper3362/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work proposes K-SAM as an alternative to SAM to improve algorithmic efficiency. The basic idea is to reduce the number of training samples used to approximate the stochastic gradients in both inner and outer optimization steps based on their loss values. This idea is originally proposed in Ordered-SGD, a biased stochastic optimization method. The authors argue that the proposed methodology can reduce the amount of computations by choosing K well without losing much on the generalization effect by the original SAM.\n",
            "strength_and_weaknesses": "Strength\n* clarity; the main idea of the paper is described straightforward so K-SAM is read easily.\n\nWeakness\n* very limited novelty; this work naively applies the idea of Ordered-SGD to SAM, not more really.\n* overclaiming based on shallow analysis; Figure 1 is clearly showing that the gradient estimate based on less number of samples gets more and more dissimilar to full mini-batch gradient, but the authors drive discussion into a direction where this result unfairly gives grounds for their proposed idea. This is an obvious trade-off between approximation accuracy and computational efficiency. \n* lack of theoretical grounds; the authors borrow some explanations from Ordered-SGD on the convergence analysis, but without any extension on mini-max as in the SAM setting.\n* insufficient empirical evidence; the experiments do not show any significant improvements on both performance or efficiency, except for the expected trade-off.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* This work directly applies the idea of Ordered-SGD or biased stochastic optimization in the context of sharpness aware minimization.\n* There is no concern on reproducibility.\n",
            "summary_of_the_review": "Unfortunately this work does not provide non-trivial addition to algorithmic extension to the original SAM or empirical/theoretical evidence to support the realization of the proposed idea.\n\n(after rebuttal) It is my regret that my concerns are not effectively addressed by the authors. I'm still not convinced that the paper is ready for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3362/Reviewer_gf5z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3362/Reviewer_gf5z"
        ]
    }
]