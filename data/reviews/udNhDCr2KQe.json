[
    {
        "id": "UPaC3q8rKw",
        "original": null,
        "number": 1,
        "cdate": 1666364084789,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666364084789,
        "tmdate": 1666370996529,
        "tddate": null,
        "forum": "udNhDCr2KQe",
        "replyto": "udNhDCr2KQe",
        "invitation": "ICLR.cc/2023/Conference/Paper4243/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In a constraint satisfaction problem (CSP), we are given a set of variables and a set of constraints. Each variable takes values in some discrete domain, and each constraint specifies the allowed combinations of values for some list of variables. The problem is to find an assignment of variables to values that satisfies all constraints. In this setting, the aim of this paper is to handle CSP instances using recurrent transformers. The overall architecture is defined using self-attention blocks, recurrent steps, and self-attention heads. Constraints can be injected using the CL-STE technique. Experiments are performed on visual 9x9 and 16x16 Sudoku tasks, shortest path problems, MNIST Mappings, and Nonogram puzzles.   \n",
            "strength_and_weaknesses": "As we know, constraint satisfaction problems have been a subject of extensive research for almost fifty years in AI. Nowadays, modern constraint solvers are able to efficiently solve a wide variety of constraint satisfaction tasks. So, the key question here is: why do we need another neural (or neuro-symbolic) architecture? \n\nI would say that if the constraint satisfaction task involves some sub-symbolic components (images, etc.) then the use of a neural architecture is indeed relevant. To this very point, the experimental results reported in the paper indicate that recurrent transformers can handle visual Sudoku tasks in an end-to-end manner, which is a significant improvement with respect to existing neural models, such as RRN and SATNet. \n    \nOn the other hand, for most constraint satisfaction tasks, the problem is symbolic in nature: it is defined using a set of discrete variables, a set of discrete domains, and a hypergraph of constraints. So, for those classical CSPs, there is no real need of using recurrent transformers, **unless they are in position to outperform standard solvers.**  From this perspective, the experiments should also include a comparison with standard solvers (e.g. FlatZinc, GeCode, Ace, etc.) on various CSP instances. Note here that the Shortest Path problem is easy (the problem is in P); I would recommend making comparisons on NP-hard tasks, such as, for example, resource allocation, scheduling or planning problems.\n\nBeyond that, it seems that recurrent transformers require a lot of tuning in order to solve CSPs. Notably, the overall architecture involves up to 10 hyper-parameters, including the number of recurrences, the number of layers, the number of allocation heads, etc. Although recurrent transformers can learn to solve CSP instances using only labeled instances, their performance can be significantly improved by injecting logical constraints in the training objective (Sections 3.3 & 4.2). Yet here, we need to carefully design an appropriate loss function for each constraint type, which essentially means that the overall architecture is dedicated to the task at hand. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "As recurrent transformers have already been proposed in the literature, the present architecture is not really novel. I would say that the main originality lies in the use of recurrent transformers for solving constraint satisfaction tasks. Yet, as indicated above, the overall architecture requires a lot of tuning and human expertise on injecting constraints as loss functions, and comparative experiments with standard constraint solvers are missing. \n\nFinally, the paper has some clarity issues. In a constraint satisfaction problem, the overall task is to determine whether the instance is satisfiable, or not. When the CSP instance is satisfiable, a solution (i.e. complete assignment) should be provided. So, based on the architecture described in Section 3, and specifically the output layer $\\mathbf{W}_{\\mathit{out}}$, how do we extract a solution? Furthermore, what is given in output when the CSP instance is unsatisfiable?\n\n",
            "summary_of_the_review": "To sum up, the main message of this paper is that recurrent transformers are amenable to \u201csome\u201d CSP tasks, with good performances in comparison with other neural architectures. Yet, I am not convinced that the paper can be accepted in its current state. Besides clarity issues, one important weakness is the lack of comparisons with standard solvers on various classical CSPs. Another weakness is the need for dedicated knowledge in order to adjust hyper-parameters and to inject constraints in the model. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4243/Reviewer_SH5d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4243/Reviewer_SH5d"
        ]
    },
    {
        "id": "Gp9RsDSQfLv",
        "original": null,
        "number": 2,
        "cdate": 1666654931601,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654931601,
        "tmdate": 1666654931601,
        "tddate": null,
        "forum": "udNhDCr2KQe",
        "replyto": "udNhDCr2KQe",
        "invitation": "ICLR.cc/2023/Conference/Paper4243/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the problem of learning to solve constraint satisfaction problems. This entails finding values of a set of variables that satisfy given constraints. The authors propose using a transformer model, extended with recurrence, to perform this variable assignment.\n\nExisting work has shown that transformers produce subpar performance on constraint reasoning tasks, such as Sudoku. The authors incorporate recurrence into transformers to be able to successfully apply them to CSPs. Additionally, the authors add an explicit constraint loss to all recurrent layers.",
            "strength_and_weaknesses": "I am confused about what is gained by adding recurrence to a transformer model. Large language models prior to transformers consisted of recurrent networks of various architectures that included attentional mechanisms, and can thus be described as recurrent transformers.\n\nIn my opinion, much of the gain found by the authors is likely due to the explicit constraint loss added to the intermediate layers, or due to injecting the logical constraints. In fact, it seems that the constraint loss on attention does a lot of the work of encoding the problem directly into the computation graph of the transformer.",
            "clarity,_quality,_novelty_and_reproducibility": "The work presented is clear and seems like it would be reproducible.",
            "summary_of_the_review": "My impression is that the authors do not fully understand the architecture they have built or why it performs the way it does. It seems to me that they have simply encoded the logical constraints of the domain into the architecture of the network itself. I don't think the recurrence has anything to do with the improved performance of the system.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4243/Reviewer_wMfd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4243/Reviewer_wMfd"
        ]
    },
    {
        "id": "WzND7537RQ",
        "original": null,
        "number": 3,
        "cdate": 1666673345998,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673345998,
        "tmdate": 1666675311148,
        "tddate": null,
        "forum": "udNhDCr2KQe",
        "replyto": "udNhDCr2KQe",
        "invitation": "ICLR.cc/2023/Conference/Paper4243/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a recurrent architecture using Transformers to solve instances of a Constraint Satisfaction Problem with unknown constraints.  Weight sharing between the recurrent transformer layers allows it to create a much deeper network, which is required for solving combinatorial problems like sudoku. It also adapts the formulation of CL-STE (Yang et al 2022) to convert cardinality constraints on Boolean variables to a loss function on the probabilities of those Boolean variables being true. It experiments on symbolic as well as visual sudokus and achieves state-of-the-art results on \u2018ungrounded-visual sudoku\u2019 in which output symbols for only the empty cells are given in the training data.",
            "strength_and_weaknesses": "As the authors have pointed out, the idea of recurrence in Transformers is not novel, but they have been able to successfully apply it to learn the constraints of a CSP. \n\nThe idea of computing loss using the output of every recurrent step has already been used to train RRNs. \n\nAlso, there is a lot of work on using Soft Logic to convert constraints on discrete symbols to equivalent constraints on the symbols\u2019 probabilities computed by a neural network. For example, see Li et al. 2019.  Some of the formulations may give a similar loss term as the proposed constraint loss.\n\nThe task is to solve CSP by implicitly learning the underlying constraints in the network\u2019s weights, but on the other hand, the constraint loss term assumes explicit knowledge of the same unknown constraints.  Though I agree that it is not clear how the knowledge of rules can be used with the ungrounded datasets, but for textual sudokus, if we know the constraints, we can create unlimited training data by using a symbolic solver. Maybe the authors should clarify this in the paper.  Also, the paper points out (almost criticizes) at a couple of places that RRN uses additional knowledge of which variables are allowed to interact, but the proposed constraint loss term uses almost similar knowledge. \n\nHaving said that, the paper does a good job of bringing together all of these ideas and proposing architecture for solving combinatorial problems.\n\nIt archives s-o-t-a performance on difficult ungrounded visual sudoku datasets, beating the baseline by a huge margin. \n\nThe authors have done extensive analysis and ablations to demonstrate the effectiveness and workings of the proposed approach.\n\n\n[Li et al 2019] A Logic-Driven Framework for Consistency of Neural Models. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. All the details of the experiments have been provided in the appendix, and the source code has also been shared, though I haven\u2019t checked the code.\n\nNovelty in terms of new ideas isn\u2019t much, but the paper does a very good job at successfully bringing together different ideas for solving a task. As a result, it achieves s-o-t-a performance in a difficult reasoning task of ungrounded-visual sudoku. As pointed by the authors, the existing s-o-t-a framework for ungrounded visual sudoku requires additional training of InfoGANs, and on the other hand, the proposed architecture is elegant, end-to-end and doesn\u2019t require any external components, such as InfoGANs for clustering.\n",
            "summary_of_the_review": "Overall, I like the paper for its clever use of existing ideas for demonstrating the effectiveness of transformers in learning constraints and solving combinatorial problems.  The experimentation is very thorough and extensive. \nThey achieve state-of-the-art performance on ungrounded visual sudoku and beat the baseline by a huge margin.\n\n\nQuestion for the authors:  \n1.  For 16 x 16 sudokus, are the experiments on textual, or ungrounded visual sudokus? I couldn\u2019t find this detail in the paper.\n2. Table 1, L1R32H4 on RRN-V achieves 74.8% board accuracy. Is this using the constraint loss terms? Why don't we have this number in Table 2? \n3. Did you try to train RRN with the constraint loss terms? I would encourage you to do so and report it in the paper.\n4. \"As RRN was not designed for Visual Sudoku...\" (Pg 7): this claim is not true. Just because RRN authors didn't experiment with visual sudoku, it doesn't imply that RRN wasn't designed for it. As you have demonstrated, RRN can be easily plugged in as a reasoning component in a neural architecture.\n5. Is it possible to extend transformers architecture to learn first-order rules instead of grounded CSPs, like NLM (Dong et al 2019) does? Specifically, can we extend recurrent transformers to train using 9 x 9 sudokus, but test on 16 x 16, like (Nandwani et al 2022) extend RRNs to do so? Given that positional embeddings are critical for training transformers, you may no longer be able to use fixed positional embeddings and may have to try something like ALiBi (Pres et al 2021), or relative positional embeddings (Shaw et al 2018).\n\n[Dong et al 2019] Neural Logic Machines\n[Nandwani et al 2022] Neural Models for Output-Space Invariance in Combinatorial Problems\n[Shaw et al 2018] Self-Attention with Relative Position Representations\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4243/Reviewer_V9ce"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4243/Reviewer_V9ce"
        ]
    },
    {
        "id": "OPCO6wZozr",
        "original": null,
        "number": 4,
        "cdate": 1667169611946,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667169611946,
        "tmdate": 1667169611946,
        "tddate": null,
        "forum": "udNhDCr2KQe",
        "replyto": "udNhDCr2KQe",
        "invitation": "ICLR.cc/2023/Conference/Paper4243/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an architecture for solving CSPs based on a transformer network. The transformer makes multiple passes over the inputs, keeping information across different passes in the form of a recurrent state. The network is also augmented with a constraint loss that injects problem knowledge into the training process. The results are evaluated on visual Sudoku as well as other benchmarks showing substantially improved results. ",
            "strength_and_weaknesses": "Strengths:\n* the presented architecture is fairly generic and applicable to many different types of input and problem structures;\n* the results show substantial improvements over existing algorithms;\n* generally well-written and reasonably evaluated;\n\nWeaknesses:\n* the novelty is not particularly high;",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is generally well-written, well-presented, and sufficiently well evaluated. My main concern here is with respect to novelty. At this point in time, transformers have been applied successfully to every problem known to machine learning and have become almost a universal tool for solving anything. In this light it's not a big surprise that this particular set of applications yields good results. The techniques that handle the input/output encoding are not new either by the author's own admission. On the flip side, based on results alone, I imagine that this paper would be of interest to some fraction of the community since it fixes known problems in the state of the art.",
            "summary_of_the_review": "An application paper where known techniques are combined to yield substantial improvements over SOTA. Low novelty is somewhat offset by results, so there is an argument that this should be published.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4243/Reviewer_KSwb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4243/Reviewer_KSwb"
        ]
    }
]