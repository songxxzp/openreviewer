[
    {
        "id": "y7q90x6KV0",
        "original": null,
        "number": 1,
        "cdate": 1666431030910,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666431030910,
        "tmdate": 1668666424482,
        "tddate": null,
        "forum": "Cs3r5KLdoj",
        "replyto": "Cs3r5KLdoj",
        "invitation": "ICLR.cc/2023/Conference/Paper5203/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work deals with the knowledge distillation between the GNNs and MLPs from the perspectives of effectiveness and the robustness, and proposes a method NOSMOG to learn structure aware and noise robust MLP. For effectiveness, NOSMOG uses position encoding to capture graph structure information and combines it with original node content information for training MLP, along with the representation similarity distillation term. Moreover, adversarial training technique is introduced to address the robustness.\n",
            "strength_and_weaknesses": "strength\n\n(1) this work explores the structure information and noise-robustness in knowledge distillation between GNNs and MLPs, which is a practical issue of this task for real-world applications. Specifically, structure information incorporation is a beneficial component for learning graph-structure data.\n\n(2) the experimental parts are relatively comprehensive for illustrating the effectiveness of the proposed method. And some theoretical analysis, e.g., the information theory aspect, can support the claims of authors to some extent.\n\n\nweakness:\n\n(1) the solutions of this work for structure information capture with position encoding and noise robustness of adversarial training is straightforward, might limiting the novelty of this work.\n\n(2) here are some questions and concerns required to be addressed:\n\na. in the inference stage of the trained MLP, is the node positional information also taken as the input together with the node content features? if so, compared with GLNN, additional information is used, so the performance improvement is a natural result. And whether the deepwalk time is counted in the inference time comparison in figure3? \n\nb. it seems like the positional encoding features are only used in the L_GT and L_SL loss parts? are they also considered in the MLP representations HM for L_RSD loss and L_ADV loss\uff1f if only in L_GT and L_SL, why do not utilize the positional encoding features for the remain components as the structure infomation of graphs is very important? if used for all loss terms, how to understand the performance in \"w/o POS\" in table 3 by removing each of them independently? \n\nc. for the ablation study part, what about the ablation performance for only with the POS features? and what about the performance of NOSMOG without L_adv in figure 2? that would be a baseline result for verifying the performance of L_adv with nosiy features\n\nd. except for feature dimensions, is there any other factors affecting the performance of POS? for example different encoding method? the scale of graphs? Consider the core of transfering knowledge from GNNs to MLPs is addressing the the scalability constraint under the multi-hop data dependency, would such structure information encoding also limit by the scalability?\n\ne. for RSD part, why imposing extra transformation W in the the MLP representations HM? is this part kept in the inference? and according to my understanding, in eq.(3) S_gnn and S_mlp are both calculated with dot product? why it denotes the intra-model representational similarity ? what is the meaning of the intra-model similarity?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and description of this work is relatively clear and understandable, and the novelty, the technique solutions, as well as the experiments denote the good quality of the work. Moreover, the work is not hard to reproduce. \n",
            "summary_of_the_review": "This work explores to the practical issues of structure infomation incorporation and noise-robustness in transfering knowledge from GNNs to MLP, and correpsonding solutions are fair reasonable and the effectiveness can be verified. however, there are still some parts to be further clarified and explained in the 'weakness' part above.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5203/Reviewer_8WF2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5203/Reviewer_8WF2"
        ]
    },
    {
        "id": "emrPEuxhEkp",
        "original": null,
        "number": 2,
        "cdate": 1666632984259,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632984259,
        "tmdate": 1666632984259,
        "tddate": null,
        "forum": "Cs3r5KLdoj",
        "replyto": "Cs3r5KLdoj",
        "invitation": "ICLR.cc/2023/Conference/Paper5203/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to learn MLPs on graphs to address the scalability issue while maintaining effectiveness, robustness, and efficiency. Specifically, the authors identify three significant challenges and propose a novel method NOSMOG. Furthermore, the authors provide theoretical analyses and conduct extensive experiments to demonstrate the superiority of the proposed model.",
            "strength_and_weaknesses": "Strengths:\n1. The problem is well-motivated, interesting, and important. The deployment of GNNs in real-world applications poses a fundamental scalability issue for the machine learning community. This paper addresses this scalability issue by training MLPs from GNNs, resolves three critical and pressing challenges that current methods have, and proposes a novel method to address the problems.\n\n2. The authors present a not-complicated model with a strong capability to address the problems. Three key components are designed to learn an effective, robust, and efficient MLP. The proposed model can fully capture graph structure information in both the feature and representation spaces. In the meantime, the model is insensitive to node feature noises, with superior performance and fast inference speed. The proposed method is relatively novel to me. \n\n3. The performance improvement is significant. The authors conduct extensive experiments and design thoughtful research questions to demonstrate the superiority of NOSMOG. For example, the proposed model can outperform the state-of-the-art by 12% to 23% on two large-scale OGB datasets.\n\n4. The authors analyze the model expressiveness from 1) the information theoretical perspective and 2) the consistency measure of model predictions and graph topology based on Min-Cut. The authors explain well why the proposed model is better. The theoretical analyses are convincing.\n\n5. The results and the claims are reproducible. The paper is well-written.\n\n\nWeaknesses:\n\n1. Descriptions on selecting the hyper-parameters can be introduced. In addition, considering the success of the proposed model relies on various hyper-parameters. Will the model be sensitive to different parameter values?\n\n2. Additional background knowledge can be introduced for utilizing the min-cut problem to measure consistency.\n\n3. Consistency measures for structure-only methods such as DeepWalk can be demonstrated for a comprehensive comparison. In particular, for Table 4, I am curious about the difference between the proposed method (that considers both content and structure) and DeepWalk which only considers the structural information.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and novel, and easily reproducible.\n",
            "summary_of_the_review": "The results and findings in this paper are insightful and would be useful for future research. The paper is also well-written with solid theoretical exposition and strong results. Overall, this is a good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5203/Reviewer_44NK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5203/Reviewer_44NK"
        ]
    },
    {
        "id": "mFpx2Qferm",
        "original": null,
        "number": 3,
        "cdate": 1666685283768,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685283768,
        "tmdate": 1666685657161,
        "tddate": null,
        "forum": "Cs3r5KLdoj",
        "replyto": "Cs3r5KLdoj",
        "invitation": "ICLR.cc/2023/Conference/Paper5203/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work addresses three issues that current MLPs on graph methods have: 1) the misalignment between content feature and label spaces, 2) the strict hard matching to the teacher\u2019s output, and 3) the sensitivity to node feature noises. In particular, this paper presents NOSMOG, a novel method to learn MLPs with remarkable effectiveness, robustness, and efficiency. Extensive experiments and theoretical analyses demonstrate the superiority of NOSMOG by comparing it to GNNs and state-of-the-art methods.",
            "strength_and_weaknesses": "Strengths:\n\n+ The motivation for training student MLPs with teacher GNNs is interesting and important in practice. In addition, the authors identify and address three critical issues, which make the work applicable and significant in real-world cases. Really good motivation, especially since the graph datasets are becoming larger and scalability is becoming a pressing challenge.\n\n+ The idea is compelling and the proposed model is somewhat novel. The authors present three components to address each identified problem correspondingly. The introduced components are simple yet effective. The overall model structure is well-designed. In the meantime, the model facilitates efficient inference, while achieving superior and robust performance.\n\n+ This paper conducts extensive experiments and comprehensive ablation studies to evaluate the proposed model. For example, the authors compare both transductive and inductive settings, noisy input features, and different teacher architectures. The model is tested on multiple datasets and shows very promising results. The proposed model demonstrates state-of-the-art results across different settings and various scales of datasets. The results are remarkable and superior to other methods, which also suggests that NOSMOG can be very useful in practice from both performance and applicability perspectives. In addition, the authors open-source their implementation.\n\n+ The authors provide a theoretical understanding of the model, which is sound. The theoretical statements in this paper look correct to me. The authors develop theoretical insights and build a theoretical basis for the proposed model, which I think is valuable.\n\nWeaknesses and Questions:\n\n- Some technical details are not elaborated for those readers who are less familiar with the respective topics. For example, the section on the consistency measure is quite brief. For better readability, I would suggest the authors provide more information such as how the cut value indicates and measures the consistency between model predictions and graph topology.\n\n- Based on Table 3 in the ablation study, it seems that the POS contributes the most, especially on large graph datasets such as Arxiv and Products. I wonder what the performance looks like if only POS is used, instead of using all the components. Will the model variant have a similar or close performance to NOSMOG when it comes to large graph datasets? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the approach is described clearly. The designed model is novel. The code and data are released for reproducibility.",
            "summary_of_the_review": "The paper address three challenges of learning MLPs on graphs by designing a novel and effective model. The technical statements are sound, the theoretical explanations are rigorous, and the empirical results are impressive. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5203/Reviewer_iP1i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5203/Reviewer_iP1i"
        ]
    },
    {
        "id": "hobVUAF19Mo",
        "original": null,
        "number": 4,
        "cdate": 1666893985739,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666893985739,
        "tmdate": 1668809577389,
        "tddate": null,
        "forum": "Cs3r5KLdoj",
        "replyto": "Cs3r5KLdoj",
        "invitation": "ICLR.cc/2023/Conference/Paper5203/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work extends a recent approach to train MLPs with teacher GNNs to produce models capable of performing well on relational data while preserving low inference latency by omitting belief propagation. To motivate the paper, the authors identify two major limitations to existing MLPs trained with GNNs 1) loss of positional/structural information from the graph as the MLPs only take node content features as input 2) lack of robustness to node content perturbations. \n\nTo address these limitations the authors propose augmentations to the \u201cgraph-less\u201d MLP training pipeline. The three augmentations are 1) concatenating node position embeddings (graph embeddings0 to the node content features for MLP input 2) representational similarity distillation such that the pairwise relationships between hidden representations of training nodes are preserved in the MLP compared to the GNN 3) training on adversarial samples for which the node content features are perturbed but the node labels remain untouched. \n\nThe authors provide extensive experimental results that show the performance of their augmented MLP on transductive and inductive node classification tests. The results show that the augmented graph-less MLP (named NOSMOG) outperforms the teacher GNN in most cases as well as the current SOTA graph-less MLP (GLNN). The ablation study shows that the first augmentation (node embeddings) most improves performance. ",
            "strength_and_weaknesses": "Pros \n* The authors are able to address limitations of current graph-less MLPs in a united system. Further the system (NOSMOG) is introduced clearly and each component is well explained. \n* The experimental results are thorough and show the effectiveness of NOSMOG. The authors conduct evaluations in multiple settings (transductive vs inductive), run an ablation study, and test the robustness of using various teacher GNNs.\n\nCons\n* Concatenating the node position features (embeddings) is a critical component of NOSMOG (contributes the most to an increase in accuracy), but the paper does not specifically discuss the process of re-embedding the graph during the testing phase and the impact of re-embedding the graph. For example, for the transductive setting, is the graph re-embedded at test time or are the embeddings for unlabeled nodes carried over from the training phase? Assuming re-embedding, it is possible that the re-embedded node positions are rotations or translations of the embeddings generated during training. It would be helpful to discuss whether NOSMOG is invariant to these operations on the node position embeddings during the test phase. \n* Since many node position embeddings themselves are generated with GNNs today, it would be helpful to compare the performance of using non-message passing embeddings (i.e. the current DeepWalk embeddings) vs message passing embeddings. If NOSMOG improves with the message passing embeddings it would be helpful to discuss the tradeoff between latency and accuracy. \n* Finally, the explanation of NOSMOG\u2019s effectiveness in section 5.8 is quite abstract and I am not sure the discussion adds to the paper. As is the section largely repeats the justifications for incorporating the node position features in the first place. To improve the section it would be helpful to quantify the term $I(E^{[v]}; y_v)$ under certain modeling assumptions. \n\nMinor comments\n* Should one of the embedding matrices in equation 3 be transposed so that S is a gram matrix? \n* I am assuming the MLP in equation 7 takes concatenated perturbed content features and node position embeddings as input though currently it only takes the content features. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clearly written and the components of NOSMOG are well described.\u2028\nQuality: The experimental results are thorough. To improve the quality, it would be helpful to discuss the relationship between node position embeddings used at test time vs training time and to make section 5.8 more specific. \u2028\nNovelty: Overall the paper provides incremental novelty building on the graph-less MLPs previously introduced. \nReproducibility: The authors provide code and detail the hyperparameters used. ",
            "summary_of_the_review": "The paper presents a unified approach to address the limitations of current graph-less MLPs, a recent approach favored for faster inference latency, and overall extends the capabilities of knowledge distillation from GNNs to MLPs. The three augmentations are clearly explained and the experimental results are very thorough, showing the effectiveness of each component under various testing environments. The main limitation of the paper currently is greater insight into why the augmentations address the motivating limitations of graph-less GNNs and whether these augmentations themselves are robust. Specifically, the node position embeddings are the most important augmentation, as shown in the ablation study, but is unclear whether the MLP is invariant to symmetric operations on the node position embeddings when the graph is re-embedded at test time. Further, the theoretical explanation in section 5.8 would benefit from better quantifying the mutual information associated with the graph structure. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5203/Reviewer_yeLS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5203/Reviewer_yeLS"
        ]
    }
]