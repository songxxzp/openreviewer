[
    {
        "id": "GxzeBpdeJg",
        "original": null,
        "number": 1,
        "cdate": 1666201563057,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666201563057,
        "tmdate": 1668450816665,
        "tddate": null,
        "forum": "IERSU0La-Nt",
        "replyto": "IERSU0La-Nt",
        "invitation": "ICLR.cc/2023/Conference/Paper4318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies privacy in a federated learning setting. It aims to improve upon methods that directly privatize data points or gradient updates using differential privacy by instead separating data points into \"sensitive\" and \"generalizable\" features and then only requiring the client to publish a noisy version of their generalizable features. Each client then constructs their own model based on all shared noisy data and their own raw data. Intuitively, the scale of noise required to privatize the generalizable features may be substantially smaller than that needed to privatize the raw data (as required for local DP), and this enables more accurate final models. The paper concludes with experimental results .",
            "strength_and_weaknesses": "Strengths: The idea of separating useful and sensitive features is conceptually interesting.\n\nWeaknesses: I think the paper has three serious problems.\n\n1. The description of the algorithm is unclear. After reading Section 3.2 several times, I still don't understand how the data distillation process works. Is it performed entirely locally? If so, what is the meaning of the E_{(x,y)} term? As far as I can tell, the paper never makes the actions taken by each client locally explicit. Algorithm 1 abstracts it away as a subroutine in Algorithm 2 in the appendix, and Algorithm 2 abstracts it away as calling an undefined PrivacyDistillation algorithm using Equation 1, though I don't see where the user's actual data point enters into the algorithm. If indeed the distillation process is entirely local, I'm confused about how generalizability is even measured -- how can somebody with one data point reason about which parts of it generalize? Do users have multiple data points? Turning to the privatization step, I do not see which part of the paper actually derives the additive noise scale required to privatize the generalizable features x_g when constructing the globally shared database. The relevant result appears to be Theorem 3.5, which only offers a vague qualitative statement that FedPD's noise standard deviation is \"much less than\" that of conventional federated learning, without a proof. Overall, the paper's clarity is low.\n\n2. The privacy argument seems wrong. As far as I understand it, the paper does not attempt to ensure that the distillation process itself is private. This makes me question the value of adding noise to the distilled generalizable features. The paper is not precise enough for me to make this argument exact, but it seems plausible that the scale of the generalizable feature is itself private information, i.e., certain data points will produce generalizable features with noticeably larger norms than other data points. If indeed noise is scaled to this norm (as I assume it must be -- if not, what is the advantage over privatizing the entire data point?) then the scale of the noise added alone could leak information. As mentioned above, the paper doesn't seem to actually describe how the noise is scaled, so I can't verify this concern, but I think it's a red flag that the paper does not attempt to reason about this possibility.\n\n3. The paper's Theorem 3.4 comes out of nowhere, its significance is not explained (are users applying it to privatize k of their generalizable points?) and it seems to have been plagiarized from previous work (see \"Details of Ethics Concerns\" below).",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper scores poorly on all four of these aspects. Specific to reproducibility, the proofs and pseudocode are vague, and no experiment code is provided.",
            "summary_of_the_review": "The paper's explanation of its algorithm is unclear, no proofs or code are offered for verifying its reasoning, the privacy guarantee of the algorithm it sketches is suspect even apart from these technical details, and one of the results appears to be plagiarized, with an incorrect proof, from previous work. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Theorem 3.4 in this paper appears to be plagiarized from Theorem 3.4 and Theorem 3.5 in the Kairouz+ paper about advanced composition, https://arxiv.org/pdf/1311.0776.pdf. Theorem 3.4 in this paper mixes the two together (homogeneous epsilon, heterogeneous delta) but the wording and notation is nearly identical. This paper, however, does not cite Kairouz+ or give any indication that the result is not original to this paper. The proof of Theorem 3.4 given in this paper is also vague and imprecise enough to be, IMO, \"not even wrong\".\n\nEDIT: The authors have promised to add a citation to this result and clarify that it is not a contribution of this work, so I'm removing the Ethics Review flag. However, I'm going to leave the previous entry here to reflect how the original submission changed.",
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4318/Reviewer_52C4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4318/Reviewer_52C4"
        ]
    },
    {
        "id": "CTVPryekDM",
        "original": null,
        "number": 2,
        "cdate": 1666687373573,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687373573,
        "tmdate": 1670304696615,
        "tddate": null,
        "forum": "IERSU0La-Nt",
        "replyto": "IERSU0La-Nt",
        "invitation": "ICLR.cc/2023/Conference/Paper4318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel method to tackle data heterogeneity in federated learning. Specifically, the authors proposed viewing the data as an addition of public features and private features and generate a set of public features to construct a globally public dataset to aid training. The empirical results demonstrate significant improvement over prior efforts.",
            "strength_and_weaknesses": "Strengths:\n- The method is novel and the empirical results look promising.\n\nWeaknesses:\n- I'm concerned with the privacy guarantee imposed in the work. It doesn't seem that the authors state the clipping bound for Gaussian/Laplacian Mechanism nor does Theorem 3.4 include any information about how the privacy parameter $\\varepsilon$ relates to the noise scale $\\sigma$ and the clipping bound. That makes me concerned about whether the experimental results comes from unbounded $x_g$? Could the authors explain how sensitivity is bounded in the experiments and theory?\n- I'm confused with the separation of $x_s$ and $x_g$ here. Seems that $x_s$ has never been used alone, it seems that the solution is equivalent to generating synthetic data $x_g$ from true data $x$ using a generative model and send the synthetic data to the server. Could the authors explain the motivation of having $x_s+x_g=x$?\n- When communicating the synthetic data to the server, how are the labels communicated? It seems that $p(y|x)$ and $p(y|x_g)$ are the same? If that is the case, how to communicate the label information privately?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is novel conceptually. However, there are missing arguments around the privacy aspect of the proposed method.",
            "summary_of_the_review": "Although the experimental results seem good, critical privacy arguments are missing in the current paper (1. bounding sensitivity, 2. communicating label info). Based on current content, justifications and improvements are needed for this work. \n\n**UPDATES**: After reading the responses and other reviewers' comments, I still do not find the privacy argument convincing. The main concerns still lies in unbounded sensitivity and unmasked label info. Therefore, I decide to maintain my initial evaluation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4318/Reviewer_Lg1w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4318/Reviewer_Lg1w"
        ]
    },
    {
        "id": "r3Yi0isT9qV",
        "original": null,
        "number": 3,
        "cdate": 1666820495943,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666820495943,
        "tmdate": 1666820495943,
        "tddate": null,
        "forum": "IERSU0La-Nt",
        "replyto": "IERSU0La-Nt",
        "invitation": "ICLR.cc/2023/Conference/Paper4318/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an algorithm that distill and privatize the generalizable features to improve FL training.",
            "strength_and_weaknesses": "Strength:\nThe paper proposes an interesting idea to distill the generalizable features from samples to improve training.\n\nWeakness:\n1. The DP analysis is a bit unclear.\n- The paper says that we'll add noise to x_g to form x_p and all the x_p would be sent to the server. That looks like local DP which could usually cause problem to utility. But then the paper says \"considering all clients' data as a whole\", which seems to suggest something very different from local DP. The authors did not provide a rigorous of sensitivity and privacy analysis.\n- It is also unclear whether we're having a per-example privacy or per-user privacy.\n- I didn't see the privacy values in the experiments either.\n2. The paper claims to separate the generalizable and sensitive features, but it looks more like generalizable vs \"non-generalizable\" features / the reminder features to me. I don't think the paper has explained much about why the reminder features contain anything sensitive.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe presentation is in general quite clear except for the privacy analysis and specification in the experiments.\n\nQuality:\nAgain, my main concern is the privacy analysis.\n\nNovelty:\nThe general idea seems quite novel. \nOne minor point: the idea reminds me of https://arxiv.org/pdf/2102.12677.pdf (I might be wrong in understanding either paper though) which decomposes the gradient into a subspace where the majority signal lies in and the orthogonal subspace, and privatized both parts. You might see if there is any connection and anything to borrow there.\n\nSome other comments:\n- In \"optimization view\", the paper mentioned that L in (1) is the cross-entropy loss, but the formalization looks quite general to me that I don't think we need to restrict the loss (or whether the label y should be a scalar or vector).\n- The formulations in optimization view and generalization view only focus on the generalizable feature x_g being an accurate approximation to the original data x, but do not contain privacy-related constraints. (The privacy is guaranteed through additive noise.) So it is a bit unclear to me why we should be calling x - x_g the \"sensitive features\". It looks to me that they are just \"ungeneralizable\" features and do not necessarily contain sensitive information.",
            "summary_of_the_review": "The idea seems interesting, but I'm mostly concerned about the privacy analysis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4318/Reviewer_Yumu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4318/Reviewer_Yumu"
        ]
    },
    {
        "id": "7lkcMjxf3q_",
        "original": null,
        "number": 4,
        "cdate": 1666981059697,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666981059697,
        "tmdate": 1666981059697,
        "tddate": null,
        "forum": "IERSU0La-Nt",
        "replyto": "IERSU0La-Nt",
        "invitation": "ICLR.cc/2023/Conference/Paper4318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tries to tackle the problem of data heterogeneity in federated learning with differential privacy by attempting to divide the features into private and generalizable features. They ask the question of what is necessary to share to learn global models and can the remaining data stay on the client as local models. They provide empirical evaluation of their claims.",
            "strength_and_weaknesses": "Strengths:\n1. The paper proposes an interesting idea to try and improve federated learning training with differential privacy.\nWeaknesses:\n1. Theorem 3.4 is the standard adaptive composition result of differential privacy and it is not clearly stated that this is not one of the papers contributions.\n2. Theorem 3.5 is an informal statement at best, and in no way shows why the necessary noise to be added is lesser to just the generalizable features. Is the L2 sensitivity lowered in some way?\n3. In the experimental evaluations, the high near central training accuracies make it seem like there is some personalization done using the private data on each client $x_s$. Is this the case? If it is the case, the baseline of no local personalization is quite weak. Unfortunately, I couldn't verify this since the code wasn't attached.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is broadly well written, but lacks rigor where it is required.\nThe idea is novel, but the theoretical results (as claimed) are non-existent and empirical improvements are not clear.",
            "summary_of_the_review": "In light of the strengths and weaknesses mentioned above, the weaknesses outweigh the strengths and I recommend the paper for rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4318/Reviewer_Kf6q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4318/Reviewer_Kf6q"
        ]
    }
]