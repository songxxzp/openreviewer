[
    {
        "id": "fsF66LiejrM",
        "original": null,
        "number": 1,
        "cdate": 1666275731834,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666275731834,
        "tmdate": 1666275731834,
        "tddate": null,
        "forum": "cS45VNtZLW",
        "replyto": "cS45VNtZLW",
        "invitation": "ICLR.cc/2023/Conference/Paper2725/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new way of using an ensemble of neural networks to construct a new classifier that achieves better accuracy at a lower number of FLOPS (at inference time). This is achieved by creating so-called \"bridge-classifiers\" that approximate the output of an interpolating classifier between pairs of ensemble-base classifiers. The interpolating classifiers are picked from the low-loss Bezier curve in parameter space between the pair of ensemble members. The approximation classifiers are faster than computing more ensemble members because they are smaller classifiers trained on intermediate features that have already been computed in the ensemble. The paper provides experiments on (small) image datasets (CIFAR and Tiny-ImageNet) and (small) classifiers of the ResNet family. ",
            "strength_and_weaknesses": "**Strengths**\n* The paper proposes a novel idea for approximating gains of ensembles at lower computational cost (at inference time).\n* The presented results are interesting and show that the presented method works (for small data set and classifier sizes).\n* The paper is generally easy to follow. \n\n**Weaknesses**\n* The method is relatively complex in relation to the performance gains that can be achieved.\n* A direct comparison to other methods in terms of the accuracy/computation frontier is missing. It seems that the numbers are reasonable, but e.g. the range of 63% to 67% on Tiny-INet (Table 4) is in the area that can be also reached by a single ResNet-18 according to [Papers with Code](https://paperswithcode.com/sota/image-classification-on-tiny-imagenet-1). Also, there are no direct comparisons to other \"efficient ensemble\" approaches.\n* FLOPS alone are often not a sufficient measure of model efficiency, see e.g. [The Efficiency Misnomer](https://arxiv.org/pdf/2110.12894.pdf)\n\nFurther comments:\n\nIt was not clear to me after reading the paper at which point in the base networks the z_{i} are extracted. I think this should be clarified (or maybe I missed it?)\n\nI did not see an explanation for the term \"Bridge-S\" in the paper. (Maybe I missed it?) I guess it refers to a \"small\" Bridge model, but I did not see what exact configuration \"Bridge\" and \"Bridge-S\" refer to.\n\nI found the discussion of knowledge distillation a bit distracting from the main idea. Especially the experimental setting described in the last paragraph of Sec 5.2 has a form that is not super-convincing in my opinion.\n\nIt seems to me that the most interesting setting among those discussed is the DE-1 + n bridges. In that case it would actually be possible to try to approximate the r=1 responses of the ensemble (i.e. approximate the outputs of the other ensemble members from the intermediate feature). If this worked similarly well, the whole construction of the Bezier interpolation could be avoided. I think this might be an interesting ablation experiment, but maybe not feasible to perform during the rebuttal.\n\nOverall, I found Sec.4 rather short, given that this area of research is currently quite active. For example, the following papers could be considered relevant here. (I'm not requesting that all of these should be cited, just to give a few examples).\n* [Batch Ensembles](https://arxiv.org/pdf/2002.06715.pdf)\n* [Ensembles with Shared Representations](https://arxiv.org/pdf/2103.03934.pdf)\n* [Evaluating Scalable BDL](https://arxiv.org/pdf/1906.01620.pdf)\n\nMinor points or typos not affecting the evaluation:\n* It seems that Eq. (3) and (1) can be combined into a shorter version of the two equations.\n* p.3 line ~5: parameters ... achieves -> achieve\n* p.3 line ~5: add them to [the/an] ensemble\n* p.3 line ~8: \"these strategy provide\" -> this strategy provides\n* p.4 Eq. (9): last \"i\" should be a \"j\"\n* Algorithm 1 looks like a fairly standard training loop, unless I overlooked something. Maybe removing it would enable moving parts of the Appendix into the main paper. \n* p.4 bottom: \"when the bridge network does not step toward the Bezier predictions\" - I did not fully understand what this means.\n* Table 2: The FLOPS are given as a multiple. I assume this is as a multiple of a base network? \n* p.7 ~center: \"achieve decent R^2 score\" -> \"a score\" or \"scores\"",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: High; the paper is overall well-written and easy to understand.\n\nQuality: See strengths and weaknesses above.\n\nNovelty: OK; the main idea is novel and interesting, but it is a rather complex approach for which it is not clear to me whether it would be widely applicable in practice among the large number of approaches that aim to improve the accuracy/efficiency ratio.\n\nReproducibility: High; the authors provide the training code and configurations used.",
            "summary_of_the_review": "The paper contains interesting approaches and results. However, there are also some weaknesses and limitations. I therefore think that the paper in its current form is marginally below the acceptance threshold, but I would be happy to increase my score during the discussion period if some of the concerns can be addressed by the authors.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2725/Reviewer_Kw57"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2725/Reviewer_Kw57"
        ]
    },
    {
        "id": "NQIk5pN8arg",
        "original": null,
        "number": 2,
        "cdate": 1666606619069,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606619069,
        "tmdate": 1666606619069,
        "tddate": null,
        "forum": "cS45VNtZLW",
        "replyto": "cS45VNtZLW",
        "invitation": "ICLR.cc/2023/Conference/Paper2725/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new ensembling framework to reduce inference cost and boost the performance. The proposed method build additional light-weight head as bridges to ensemble different runs. The experiments show the effectiveness of the proposed method on CIFAR-10, CIFAR-100, and tiny-ImageNet.",
            "strength_and_weaknesses": "Strengths:\n(1)\tThe idea of building bridges between different models for ensembling is interesting. It re-uses the features and only focuses on classification heads. Therefore, it would reduce the inference cost compared with directly ensembling different models.\n(2)\tThe experiments on several datasets show the effectiveness of the proposed framework.\n\n\nWeaknesses:\n(1)\tI concern only considering classification would not lead to diverse solutions to ensembling. As described in the paper, the bridges are light-weight MLPs.\n(2)\tThe paper pointed out \u201cthese methods (the existing methods) do not scale well for complex large-scale datasets or require network capacity\u201d. However, this paper also DO NOT scale up on complex large-scale datasets, such as ImageNet-1k. Since the experimental platform is on 8 TPUs, it is expected to conduct on large-scale datasets such as ImageNet-1K, large-scale backbones (or SOTA backbones) such as transformers. The experiments on CIFAR-10, CIFAR-100 or tiny ImageNet are not convincing to me.\n(3)\tMinor comments, Page 1: the modes, three models, two modes? Mode or models?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of the proposed deep ensembling is novel and the clarity and originality of the work are good. However, the experiments are conducted on small-scale datasets, which is not convincing to the community. ",
            "summary_of_the_review": "(1)The idea of the proposed deep ensembling is novel and interesting.\n(2) The experiments on CIFAR-10, CIFAR-100, and tiny-ImageNet shows the improvement, but do not scale up to large-scale datasets and large-scale backbones (or SOTA backbones) .",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2725/Reviewer_tqBD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2725/Reviewer_tqBD"
        ]
    },
    {
        "id": "iIxJpheDIF",
        "original": null,
        "number": 3,
        "cdate": 1666675995132,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675995132,
        "tmdate": 1666675995132,
        "tddate": null,
        "forum": "cS45VNtZLW",
        "replyto": "cS45VNtZLW",
        "invitation": "ICLR.cc/2023/Conference/Paper2725/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes two methods to reduce the inference cost of Deep Ensemble (a collection of ensemble models of the same model structure). B\u00e9zier Curve is fitted on the extracted features by a neural network (bridge neural network), so that \"interpolated\" inference results can be cheaply approximated by the bridge NN.",
            "strength_and_weaknesses": "Strength\nThe work is well motivated and the two proposed bridge networks method are well crafted.\n\nWeakness\n1. there is a key alternative not compared, in Learning Neural Network Subspaces a method is proposed to train models that can be linearly interpolated. Though it is not easy to compare the approaches, one is altering the training process and the other is more about adding an ancillary equipment, the two approaches can be evaluated in the same coordinate system with inference cost and accuracy on two axes.\n1. the method has not been verified on large scale datasets, though it criticized literature with \"these methods do not scale well for complex large-scale datasets\"",
            "clarity,_quality,_novelty_and_reproducibility": "Proper illustrations has been given for the effects of the method. It would be appreciated if heatmap (loss landscape) can be given to facilitated comparisons in this line of work.",
            "summary_of_the_review": "The paper is well motivated and easy to follow, but still lacks solid grounding for its claims. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2725/Reviewer_bBzc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2725/Reviewer_bBzc"
        ]
    },
    {
        "id": "Ig5isFeo4T",
        "original": null,
        "number": 4,
        "cdate": 1666708470490,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666708470490,
        "tmdate": 1666708470490,
        "tddate": null,
        "forum": "cS45VNtZLW",
        "replyto": "cS45VNtZLW",
        "invitation": "ICLR.cc/2023/Conference/Paper2725/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to address an important drawback of Deep Ensemble, the inference cost of executing multiple models. The intuition behind the proposed method is that the outputs in the function subspace can be estimated from the modes without having to forward the actual parameters on the subspace. Based on this intuition, an additional lightweight network is trained as a bridge network to predict the outputs from the connecting subspace. ",
            "strength_and_weaknesses": "### Strength \n\n* Reducing the inference cost of DE is an important topic. This paper is well-motivated. \n* The presentation of this paper is clear and easy to follow. \n* Extensive experiments demonstrate the effectiveness of the proposed two types of bridge networks on TinyImageNet and Cifar. \n\n### Weaknesses\n* In Table 4, why do more bridge models lead to better results? \n* What's the functional difference between type I and II bridge models according to theoretical and empirical results? Is there any conclusion we can reach about how to choose types I and II?\n* Are there any important/sensitive hyper-parameters during the training of bridge models? How to determine the optimal number of bridge models in practice? \n* I am curious about how the proposed methods perform on large-scale datasets like ImageNet. ",
            "clarity,_quality,_novelty_and_reproducibility": "see above",
            "summary_of_the_review": "I have some concerns about \n1) fundamental difference between type I and II bridge models\n2) Why number of bridge models impact \n3) generalization of this method to the large-scale datasets\n\nI am willing to raise my score if the above concerns are addressed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2725/Reviewer_iSwW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2725/Reviewer_iSwW"
        ]
    }
]