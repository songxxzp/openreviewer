[
    {
        "id": "ivGeaYXe6d",
        "original": null,
        "number": 1,
        "cdate": 1666620132809,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620132809,
        "tmdate": 1666620132809,
        "tddate": null,
        "forum": "PRpO-cOCQoX",
        "replyto": "PRpO-cOCQoX",
        "invitation": "ICLR.cc/2023/Conference/Paper3805/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an Interaction Augmented Prototype Decomposition (IPD) model for missing modality learning.",
            "strength_and_weaknesses": "The idea is interesting. However, the motivation is not clearly given. Besides, the writing can be improved.",
            "clarity,_quality,_novelty_and_reproducibility": "Some figures are unclear. The authors should do more experiments on different datasets, including XRMB and RGB-D. Besides, the authors can public their code for reproducibility.",
            "summary_of_the_review": "This paper proposes an Interaction Augmented Prototype Decomposition (IPD) model for missing modality learning. Some key points are easy to follow. However, the clarity needs to be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3805/Reviewer_QUyJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3805/Reviewer_QUyJ"
        ]
    },
    {
        "id": "LLacwd3H-I",
        "original": null,
        "number": 2,
        "cdate": 1666651111028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651111028,
        "tmdate": 1666651111028,
        "tddate": null,
        "forum": "PRpO-cOCQoX",
        "replyto": "PRpO-cOCQoX",
        "invitation": "ICLR.cc/2023/Conference/Paper3805/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper under review considers the problem of multimodal learning in the case where one or more of the modalities might be unavailable during either training or testing. The proposed solution works by decomposing representation space into a collection of orthogonal vectors, a \u201ccommon\u201d component to capture high-level semantic information, and modality-specific components to capture low-level information associated with each modality.",
            "strength_and_weaknesses": "*Strengths:* Dealing with missing modalities during both training and testing is an important practical problem that deserves study. The proposed method outperforms other recent methods on a benchmark introduced by the authors.\n\n*Weaknesses:* I found this paper very difficult to follow. After several close readings, I am still not confident that I understand the technical details of the proposed method. Specifically, \n\n(1) Are the pairs (x, y) the training data? If so, how is the input x formed?\n\n(2) Is Equation (1) the abstract model of data generation? If so, why is that a reasonable model? If not, then where is Equation (1) coming from?\n\n(3) How does the \u201cmaximum suppression\u201d scheme described in Section 3.5 stop Equation (8) from blowing up exponentially?\n\n(4) Is the ensemble learning scheme of Section 3.6 used during training only or also during testing?\n",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity:* In addition to the difficulties understanding the technical details described above, I also found the overall exposition quite difficult to follow. There is no dataflow diagram, model description, or algorithm explicitly describing the proposed method. The authors make frequent reference to \u201cLMF\u201d without providing any hint to the reader how that algorithm works. The writing style varies considerably from paragraph to paragraph.\n\n*Quality:* I think the submission is of low quality. In addition to the lack of clarity, it seems that the authors have assembled several existing ideas (low rank decomposition, orthogonal decompositions, etc.) and shown that they work well together. But they give little explanation as to why they work well. They offer some vague generalities (e.g. \u201cIntuitively, the common component contains more high-level semantic information (i.e., the reasoning of performance), and the modality-specific part contains more low-level detailed information (i.e., the loudness of voice, the movement range\u201d (p4)), but no rigorous empirical or theoretical justification to explain the choices that were made (this includes Proposition 2, which offers little in the way of understanding).\n\n*Novelty:* No single part of the pipeline appears to be novel, but the combination may be. \n\n*Reproducibility:* Several key details are missing that would make it difficult to reproduce the results in this paper. Namely, the precise breakdown of the datasets into training/validation/testing sets, the optimization algorithm used and it\u2019s hyperparameters, and how the data was packaged into (x,y) pairs.\n",
            "summary_of_the_review": "I recommend rejection for this paper. The proposed method may work well, but I think the exposition is too difficult to follow for readers to learn much from this work. I suggest that the authors redraft this work with a focus on clarity of method, clarity of data, and clear understanding of why the proposed method works well.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3805/Reviewer_YKCH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3805/Reviewer_YKCH"
        ]
    },
    {
        "id": "Rae1jfm5Cs",
        "original": null,
        "number": 3,
        "cdate": 1666672145591,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672145591,
        "tmdate": 1666672145591,
        "tddate": null,
        "forum": "PRpO-cOCQoX",
        "replyto": "PRpO-cOCQoX",
        "invitation": "ICLR.cc/2023/Conference/Paper3805/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of robustness in the face of noisy or missing modalities and proposes a method called Interaction Augmented Prototype Decomposition (IPD) for the setting where the number of modalities is arbitrary and there are various incomplete modality conditions happening in both training and inference phases. Their approach jointly learns the common and modality-specific task factors via low-rank decomposition, which seems to promote unseen generalization in empirical experiments. They show strong results on 3 major multimodal datasets.",
            "strength_and_weaknesses": "Strengths:\n1. The paper is well motivated and described. The ideas are clear and experiments are on several large multimodal datasets.\n2. The paper is largely clear with clear figures and exposition.\n\nWeaknesses:\n1. There are insufficient comparisons to work in missing modalities, with 'Jinming Zhao, Ruichen Li, and Qin Jin. Missing modality imagination network for emotion recognition with uncertain missing modalities. In ACL, 2021.' being the only one. The authors cite 'Paul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov, and LouisPhilippe Morency. Learning representations from imperfect time series data via tensor rank regularization. In ACL 2019.' but do not compare to it (they also use low rank decomposition) of multimodal representations, and there are also some simple baselines such as including modality dropout during training that should also be compared to.\n2. There should be more comparisons in both performance and complexity - the proposed method does better but certainly uses more computation so the tradeoff should be analyzed.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is largely clear. Some concerns with comparisons to existing work.",
            "summary_of_the_review": "Some additional comparisons and tradeoff analysis would be good.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3805/Reviewer_pCS1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3805/Reviewer_pCS1"
        ]
    },
    {
        "id": "-tPpZCcoTU",
        "original": null,
        "number": 4,
        "cdate": 1666725455642,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725455642,
        "tmdate": 1669831442917,
        "tddate": null,
        "forum": "PRpO-cOCQoX",
        "replyto": "PRpO-cOCQoX",
        "invitation": "ICLR.cc/2023/Conference/Paper3805/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors are interested in the multimodal learning problem. They want to focus on cases where incomplete data (not data from all modalities) are present during training. To do that, they assume that the fused vector x (output of LMF) can be given by the task classification (or regression) labels times e_l, which is the complete task prototype for all possible modality combinations L. Since (x,y) are given, the paper's main focus is how to learn the vector e_l efficiently. Finally, they test their trained models on unseen modality combinations.",
            "strength_and_weaknesses": "> Strengths\n1.\tThe authors try to develop a method to handle more realistic scenarios where some data modalities are missing.\n2.\tThe authors try to make good use of all available problem information with the regularized losses (disentangled contrastive constraints, ensemble learning). \n3.\tThey reduce the exponential complexity of the problem, which is dependent on the possible modality combinations (parameter l) through low-rank representations and CP decomposition.\n\n> Weaknesses\n1.\tFor the unseen modality combination: We believe that the current experiments do not test the model's performance on unseen modalities. It would make sense if a completely new modality were present or some modality was abnormal or out-of-distribution during testing. The fact that the trained models were exposed to all possible multimodal combinations except the case where all modalities are present seems not to test the unseen case but rather how much the model's performance can be improved when all modalities are available. \n2.\tBased on the previous argument, we would like to ask the authors to compute the performance of their method (and the rest) when all modalities are present to see the impact on the model when we have missing modalities. This will help us assess its performance compared to the other methods. Page 8 \"...methods (i.e. MCTN, MMIN) depends on the existence of all the modalities to obtain the supervised information in the training stage; therefore, when the samples for training are imcomplete, there will be a big drop in performance. \" these experiments could potentially help support this argument. Since the performance of MMIN is very close to IPD, it even, in some cases, outperforms it.\n3.\tEnrichment of related work, most of the referenced publications are from 2017,2018, with only one from 2019 (the most recent one). Also, give more details about previous papers, for example, \" Liang et al. (2019) employs low-rank fusion for each time step of multi-view sequential input. \" and how the proposed method compares to Liang et al. (2019).\n4.\tSome captions of figures are too short. Specifically, Figure 2. Not all diagram variables are explained. Figure 3: similarly, what is k, and what is R?\n5.\tFigures 3: We believe the figures would be more readable if there were a title above the bar graphs \"With, without contrastive constraints.\"\n6.\tFigure 2: Please consider adding more details, like the equation connecting the two decoding parts. How are vectors connected to the architecture, and what are the gray cells? In general, the figure and caption need more work. It is tough to understand what the model is doing and how the different pieces connect to the figure. Even if we read the paper and then look at the figure, it isn't easy to map things from text to pictorial.\n7.\tThroughout the document, there exist claims/sentences that remain unsupported or not fully explained; we would like the authors to go through these sentences and explain what they hold.\na.\tPage 4 \" Furthermore, the decomposition of A\\beta^T does not consider the fine-grained interaction among different modality combinations. \" why is this the case? How does CP decomposition solve this problem? Does it?\nb.\tPages 6: \"We report the metrics of BA (binary accuracy), F1, Corr (Correlation Coefficient), MA (Multi-class accuracy, higher is better), and MAE (Mean-Absolute Error, lower is better). \" \"We report the common metrics of R@K and MdR. \" Can the authors include the definition of these metrics or provide references?\nc.\tPage 7: \"To be realistic, we randomly generate the ratio of 7 pieces (by giving each piece a number from 0 to 1 and employing normalization) \": not clear what normalization is applied and why.\nd.\tPage 8: \" Even MulT utilizes Transformer Vaswani et al. (2017). \" what's the conclusion for this sentence? What are the authors trying to say to the reader?\ne.\tPage 9: \"Note that MMIN, MCTN, and most of the existing methods (i.e. Ji et al. (2022);Lei et al. (2021)) can not be used for 7 modalities, thus, we do not compare IPD with them.\" Please give more details, why not applicable.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper needs to be improved for clarity (unsupported claims, not enough details are given, figure 2 needs to be improved (please see weaknesses).\nThe method seems to be novel, although each submethod seems to be already proposed in other multimodal papers. For example low-rank decomposition and CP decomposition was used in the original LMF paper as well (https://arxiv.org/pdf/2204.13707.pdf).\nWe believe that the results are reproducible.\n",
            "summary_of_the_review": "We believe that it is important that the authors tried to develop a method for incomplete modality information during training. Because it is closer to real scenarios. The proposed method managed to improve the classification/regression performance when compared to other methods. But we believe that the paper can be improved and make more clear its advantages by addressing our comments in the previous sections.\n\n------------------------------------\nPost rebuttal summary:\n\nWe thank the authors for answering all of our questions. \n1) We believe the authors' responses for 2.1, 2.2, and 2.3 should be part of the paper.\n2) For 2.3 and the L1 normalization, the question referred to normalized data. It needs to be clarified from the text on what the L1 is applied.\n3) Figure 2 is more clear now! But it can be further improved; currently, it mainly lists the definitions of each parameter. The authors could reformulate it, so it has a better flow.\n\nAfter reading the other reviews and authors' responses, we kept our recommended score to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3805/Reviewer_SiTt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3805/Reviewer_SiTt"
        ]
    }
]