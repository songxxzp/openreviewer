[
    {
        "id": "LAke4Tsgyo",
        "original": null,
        "number": 1,
        "cdate": 1666541142604,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541142604,
        "tmdate": 1668744358184,
        "tddate": null,
        "forum": "4fZc_79Lrqs",
        "replyto": "4fZc_79Lrqs",
        "invitation": "ICLR.cc/2023/Conference/Paper2102/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Firstly, in this paper, the authors purpose that ACMP can help solve the over-smoothing problem in the GNN network by keeping the Dirichlet energy of GNN. Then, this paper shows that adding the term repulsive force may cause the particles to be pushed away to infinite, the Dirichlet energy becomes unbounded, and how the purposed method helps solve the problem. Finally, the author uses the experimental results to show that ACMP can keep Dirichlet energy in GNN and help improve the performance of GNN methods.\n\n### Update after reading rebuttal\nThe authors address my concerns and I raised my score.",
            "strength_and_weaknesses": "Strength\n1.\tUsing the property of ACMP to help keep the Dirichlet energy and help solve the oversmoothing of GNN is a good attempt.\n2.\tThe experimental results show that ACMP can improve the performance of existing GNN models in the node classification task.\n3.\tIn this paper, the mathematical arguments are detailed without obvious errors.\n\nWeakness\n1.\tAs this ACMP is focused on the oversmoothing problem of GNN. It should work for more tasks. The author only tests it at the node classification task. This cannot show that the purposed method can help solve the oversmoothing of all kinds of GNN task.\n2.\tThe purposed method is not the first method to try to solve this problem. So, there should be experiments to show this method is better than the former methods.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-organized. There are enough experimental details and hyperparameters. Although the source code is not provided, I still think that the purposed method of this paper can be reproduced. The authors propose that Allen-Cahn can reduce the oversmoothing phenomenon by keeping the Dirichlet energy of GNN. Allen-Cahn itself is not the way proposed by the author. So there is some novelty in this paper, but not very significant",
            "summary_of_the_review": "The purposed method can help solve the GNN oversmoothing phenomenon by keeping the Dirichlet energy. The experiment result shows this. However, this paper lacks the experiment to compare the performance of the purposed method and the former method that overcomes GNN oversmoothing. This paper uses the properties of Allen-Cahn. Therefore, the novelty of this paper is not very significant.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2102/Reviewer_boSt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2102/Reviewer_boSt"
        ]
    },
    {
        "id": "nar7h7VcRI",
        "original": null,
        "number": 2,
        "cdate": 1666546713621,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666546713621,
        "tmdate": 1669276021634,
        "tddate": null,
        "forum": "4fZc_79Lrqs",
        "replyto": "4fZc_79Lrqs",
        "invitation": "ICLR.cc/2023/Conference/Paper2102/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper falls in the area of deep learning for graphs. It is presented a new methodology for message passing, inspired from the study of interacting particles, in which the distinctive characteristic is that it has (in addition to conventional attracting forces) a repulsive force term in the message passing equation. This has the effect of improving the neural representations developed for heterophilic datasets and alleviate oversmoothing. Moreover, the proposed method includes an additional term called Allen-Cahn potential, to trying constraining the neural representations. The network architecture is run by a neural ODE solver, and it is demonstrated on several benchmarks.\n\nUpdate after rebuttal:\nI thank the authors for clarifying my doubts regarding the paper, and I am happy to confirm my score.",
            "strength_and_weaknesses": "Strengths:\n* The paper addresses an interesting problem in the area of deep learning for graphs, with an elegant approach\n* The mathematical analysis seems sound and interesting\n\nWeaknesses:\n* Experimental settings are not fully convincing. In particular, from the manuscript it is not clear if hyperparameter tuning with model selection has been performed or not, and how this phase is conceived in order to ensure fairness in the performance assessment wrt literature methods. I could find some details in the supplementary material concerning the hyperparameters values explored for the proposed method (which sounds reasonable): I invite the authors to include this kind of information in the main text of the paper. Moreover, I suggest to clearly comment on the number of trainable parameters when comparing with the literature methods from the literature, in order to ensure that the performance advancements are not simply determined by a higher complexity of the resulting neural network (i.e., please report the number of trainable parameters in the proposed method vs those used in the experiments from the literature methods).\n* Some aspects of the analysis are not fully clear. For instance, in the abstract it is claimed that \"ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers\", while I could not see any analysis in this sense in the paper. Moreover, while figure 2 is sufficiently explanatory and insightful, I could not find specific details on the network used to generate the plots. This, after all hampers the significance of the example, for which I suggest to include full details (at least in the caption).",
            "clarity,_quality,_novelty_and_reproducibility": "Aside from the comments in the previous section, clarity, quality and novelty are good enough. Regarding reproducibility, there is a link to an anonymous repo in the supplementary material pdf (although the code is mostly uncommented).",
            "summary_of_the_review": "The paper presents an interesting methodology. In my view, downsides in the current version of the manuscript are mostly related to the experimental settings, where details about the model selection performed on the proposed model and on the competitors from the literature, do not completely eliminate doubts about the fairness of the reported experimental comparison.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2102/Reviewer_W9W7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2102/Reviewer_W9W7"
        ]
    },
    {
        "id": "ylHRHFIqriV",
        "original": null,
        "number": 3,
        "cdate": 1666664959939,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664959939,
        "tmdate": 1668751070277,
        "tddate": null,
        "forum": "4fZc_79Lrqs",
        "replyto": "4fZc_79Lrqs",
        "invitation": "ICLR.cc/2023/Conference/Paper2102/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel message passing algorithm for graph neural networks. It explores both the attractive and repulsive forces simultaneously. Intuitively, it is introducing a measure to tell when to gather/diverge the neighborhood nodes.\n",
            "strength_and_weaknesses": "## Strengths \n- This paper introduces using an interacting particle system with attractive and repulsive forces and the Allen-Cahn force for message passing.\n- The Sec 3 & 4 are technically sound and clearly explained.\n## Weakness\n- The motivation is missing, and why we want to study Allen-Cahn message passing (ACMP) is not explained.\n  - In Sec 1, the authors mention the that particle system is common in nature and human society, but the difference between ACMP and traditional GNN is not clearly explained.\n  - Namely, a smoother logic is that, what is missing in the current GNN message passing, and how this can motivate ACMP.\n  - The oversmoothing should be briefly introduced in Sec 1. Now it just shows up directly in paragraph 1 and 2, making it hard to follow, e.g., how the repulsive orces can avoid oversmoothing is not clear.\n\nI need to confirm some points in Sec 1 with the authors:\n- For sentence \u201cMost existing message passing neural networks are driven by attractive forces associated with the Dirichlet energy \u2026\u201d Can authors give more detailed explanations?\n- What is the difference between Allen-Cahn energy and Dirichlet energy? So that AC energy can avoid the exploding issue when the GNN gets deeper?\n\nSome questions on Sec 2.\n  - There exist several deep GNN works [1, 2], but not discussed in the paper. They are also related to the oversmoothing. Besides, I\u2019m wondering which category would they fall into the framework introduced by the authors.\n  - Before Eq(2), the authors say \u2018\u2019 (GRANND) \u2026 for some message passings:\u2019\u2019, can authors add citations for these message passing works?\n\n\n\n\n- I have two concerns on the experiments.\n  - The performance on existing datasets are far behind the SOTA, e.g., https://paperswithcode.com/sota/node-classification-on-cora. Can authors also switch to some of the SOTA GNNs in the board?\n  - A minor gap between method and experiments. Since ACMP can mimic the particle dynamics, the authors should conduct experiments on more related datasets, like quantum chemistry dataset. The current experiments are on the datasets like Cora, where the notions like forces and potential are hard to verify.\n\n\n[1] Gallicchio, Claudio, and Alessio Micheli. \"Fast and deep graph neural networks.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 04. 2020.\n\n[2] Yan, Yujun, et al. \"Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks.\" arXiv preprint arXiv:2102.06462 (2021).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The current logic in Sec 1 is not clear. I would suggest the authors changing to sth. like the following:\n1. We summarize the existing GNNs into a unified viewpoint, i.e., they are learning with the Dirichlet energy.\n2. But this energy has xxx drawbacks, e.g., oversmoothing.\n3. We propose using AC energy, with xxx benefits. Then with AC energy, we propse the ACMP.\n\nI would like to raise the score after the authors fix this issue.\n",
            "summary_of_the_review": "I think this paper is technically interesting, and I can understand the core ideas of the authors. I have two major concerns:\n1. The presentation is not clear, especially Sec 1.\n2. The experiments are a little weak. Such particle system can be better verified on some quantum datasets. If the results still hold, then I think this would be a very solid paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2102/Reviewer_A1JY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2102/Reviewer_A1JY"
        ]
    }
]