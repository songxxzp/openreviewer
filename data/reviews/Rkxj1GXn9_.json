[
    {
        "id": "Ik90y4C_e0",
        "original": null,
        "number": 1,
        "cdate": 1666719531573,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666719531573,
        "tmdate": 1666719531573,
        "tddate": null,
        "forum": "Rkxj1GXn9_",
        "replyto": "Rkxj1GXn9_",
        "invitation": "ICLR.cc/2023/Conference/Paper4690/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces equivariant and invariant geometric graph neural networks. The authors provide a theoretical analysis of these architectures by introducing a geometric version of the Weisfeiler-Leman (WL) test. In addition to the graph structure, nodes have now geometric attributes (like position, velocity...) in geometric graphs. Geometric graphs will be said to be geometrically isomorphic, if their underlying graphs are isomorphic and their geometric attributes can be obtained through rotations (or reflections). In this setting, a natural extension of WL is proposed and connected to the expressive power of geometric GNN.",
            "strength_and_weaknesses": "The theoretical analysis extends known results connecting WL to the expressive power of GNN.\n\nI would have liked to get a better motivation for this paper. It is not clear at all to me that this extension to geometric GNN will be of any use. What would be a learning task where these GNNs will produce better results than standard GNNs? It is fine to present theoretical results but I think that a clear connection to machine learning is missing here.\n",
            "clarity,_quality,_novelty_and_reproducibility": "There are several problems that need to be clarified:\n- I do not think that x_{ij} appearing in equations (3) (4) and (6) is defined anywhere. Node coordinates are defined as a matrix in R^{nxd} but here indices i and j are in [n]. I guess x_{ij} is the vector obtained from the coordinates of nodes i and j?\n- The definition of geometric GNN needs to be clarified. In particular the aggregate function \\psi needs to be invariant with respect to the action of the group considered and the permutation group. How do you ensure that?\n- Similarly, the aggregation step in the geometric WL is not well defined. How do you define the final hash in equation (7). Please be more explicit. The authors write: \"by using aG-orbit injective andG-invariant function that we denote by I-HASH.\" but clearly a constant function will be invariant but useless here. So what function do you take exactly?",
            "summary_of_the_review": "This paper is not ready for publication and need some major revisions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4690/Reviewer_J5ce"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4690/Reviewer_J5ce"
        ]
    },
    {
        "id": "C3MBcktZmA6",
        "original": null,
        "number": 2,
        "cdate": 1666735443724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735443724,
        "tmdate": 1670670709343,
        "tddate": null,
        "forum": "Rkxj1GXn9_",
        "replyto": "Rkxj1GXn9_",
        "invitation": "ICLR.cc/2023/Conference/Paper4690/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper considers the problem of isomorphism, in the spirit of the WL test, but for geometric graphs. Such graphs have become prominent in recent times e.g. in applications in chemistry, and have seen notable empirical success. In addition to the usual combinatorial (graph) structure, the geometric nature of such graphs implies that GNNs designed to work on them must respect additional physical symmetries (rotation, reflection, translation). The usual WL test is not sufficient to cover such graphs, and a similar, but more general, geometric analog is needed. This paper fills the gap by proposing such a \"geometric WL test\", which can start to answer questions about the expressive power of geometric GNNs. The proposed procedure is quite straightforward, and the attendant theorems characterize some classes of geometric graphs that can be distinguished by the proposed test. Further, it also manages to quantify questions such as the tradeoffs between G-equivariant GNNs and G-invariant ones. Lastly, the paper also provides an equivalence between a model's ability to discriminate geometric graphs and their universality properties (vis-a-vis to approximate G-invariant functions). \n\n ",
            "strength_and_weaknesses": "- The paper has a very clear motivation, and attacks a problem of interest, given the recent successes of geometric GNNs in various key applications in the physical sciences. \n- The formulation of the geometric WL is straightforward and clean, and also interleaves nicely with existing work using the WL test. \n- The procedure proposed seems to indicate some directions for building provably powerful geometric GNNs, which are also practical. The current formulation is not exactly practical, and is kind of abstract.\n- The connections between universality and expressivity discussed are also quite interesting. \n- Lastly, the paper is quite well written and is a pleasure to read. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper's presentation is top-notch and quite crisp. The motivation and the general formulation is quite clear, partly because of how well-motivated and almost ubiquitous the use of geometric GNNs now is. The results presented, to the best of my knowledge, are original and will serve to inform further development of practical geometric GNNs and the study of their theoretical properties. ",
            "summary_of_the_review": "See above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4690/Reviewer_XpRF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4690/Reviewer_XpRF"
        ]
    },
    {
        "id": "yhblJLOZXHP",
        "original": null,
        "number": 3,
        "cdate": 1667092406637,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667092406637,
        "tmdate": 1670801271537,
        "tddate": null,
        "forum": "Rkxj1GXn9_",
        "replyto": "Rkxj1GXn9_",
        "invitation": "ICLR.cc/2023/Conference/Paper4690/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose a hierarchy (analogous to the WL hierarchy for graphs) $-$ for graphs with geometric features (Typically node features with coordinate information, velocity, etc) which can then be used to characterize the expressive power of equivariant and invariant geometric graph neural network layers to distinguish geometric graphs. The authors then subsequently provide theoretical statements - which link the discriminative power of models on geometric graphs to its universality. ",
            "strength_and_weaknesses": "**Strengths**:\n1. The idea to extrapolate the WL hierarchy to geometric graphs is important given the recent advances and interesting\n2. The background work and the connections to recent works on geometric graphs is very well presented.\n\n**Weakness, corresponding questions and suggestions:**\n1. In my opinion, based on the statements on preserving the relative positions and using appropriate group actions $-$ the paper appears to incorrectly convey that all Lie groups preserve isometry - which is not the case. Also paper in current state cannot directly handle something like quadrapole-quadrapole interactions (e.g. Anderson, et al. 2019 - Comormant: Covariant molecular neural networks). Would suggest the authors rather use group representations rather than presenting them just as group actions (this would avoid any confusion to readers who may assume all lie groups are matrix lie groups for instance)\n2. To me, it appears like the expressive power of IGWL appears to be misrepresented - am I misunderstanding something? For example, in Figure 2, the authors claim IGWL cannot distinguish the two geometric graphs - however the multiset (Eq 9) does include information about vectors - and a powerful and expressive multiset function, can allow for interactions between the vectors in the multiset from different neighbors - which can help distinguishing the graphs.This raises concern about the statements in section 4 - for example - Proposition 4, Theorem 6.\n3. There appears to be a lack of a connection from moving from the statements in Section 4 to the statements in Section 5. For instance, until Section 5, the authors assume countable number of orbits -- which could be violated in Section 5 e.g. Theorem 21 (unless in cases for example where you explicitly show that the function is a proper map or via alternative mechanisms - currently I am unfortunately unable to locate a statement which conveys the same) and hence can't directly see its extension to the geometric graph application\n4. Lack of experiments (please add at least on synthetic geometric graphs) which show the authors claims that equivariant layers are more powerful than invariant layers - all else remaining constant. \n5. Some of the lemmas and theorem statements in the main paper and appendix related to section 5 - are well known results available in most differential geometry books (e.g. John Lee's Smooth Manifolds) - Would suggest the authors clearly indicate this)",
            "clarity,_quality,_novelty_and_reproducibility": "In my opinion, the novelty is limited (however it is not insignificant) - purely because the theorem statements and proofs in Section 4 are very similar to the ones in the k-WL analogue. Please see weakness 3 for statements corresponding to section 5. \n\nNo experiments - so reproducibility is not a cause of concern.",
            "summary_of_the_review": "In current state, the weaknesses associated with the paper out-weigh the strengths. However, I would be happy to increase the scores if the authors provide appropriate clarifications.\n\n\n**Updates:**\n\n27th Nov:\n\nThank you very much for the detailed clarifications to reviewers questions as well as the extensive updates to the paper based on reviewer suggestions. I will increase my score to an 6 and would like to see the paper accepted.\n\n\nAfter reading through the other reviews, I have one follow up question regarding the motivation - and would be happy to increase my score further.\n\nConsider the case where geometric graphs are not rigid - for example molecules typically tend to have multiple conformations - where the relative positions of atoms in the molecule change -- and potentially a different contact graph (say based on 3nm distances) is obtained . In such a case, combining 1WL with equivariant/ invariant layers could be too restrictive -- i.e. different conformers of the same molecule would end up with different representations (while ideally we want them in the same equivalence class). Given, the above scenario, why do we need to study the expressive power of 1WL with Lie group equivariant/ invariant layers?\n\n\n11th Dec:\nAfter discussions with other reviewers, reducing score to 5 (From 6)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4690/Reviewer_Kinq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4690/Reviewer_Kinq"
        ]
    },
    {
        "id": "UMAwDr56gCC",
        "original": null,
        "number": 4,
        "cdate": 1667358581710,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667358581710,
        "tmdate": 1670275259687,
        "tddate": null,
        "forum": "Rkxj1GXn9_",
        "replyto": "Rkxj1GXn9_",
        "invitation": "ICLR.cc/2023/Conference/Paper4690/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work conducts an analysis of expressive power of geometric graph neural networks. Analogous to WL test for generic GNNs, this work proposes Geometric WL (GWL) test and its constraint version IGWL for geometric graphs. It characterizes an expressive-power gap between GWL and IGWL, which is then accordingly extended to a gap between equivariant and invariant GNNs.",
            "strength_and_weaknesses": "Strengths:\n1. Learning on geometric graphs becomes more and more significant these days, and theoretical analysis of geometric GNNs is somehow limited. This work picks a good field.\n2. The presentation is clear and easy to follow. The examples provided in Figures 1, 2 are good to show the gap between equivariant and invariant networks.\n3. The results about numbers of aggregators in Theorem 21-23 are good, as an extension of previous results in PNA.\n\nConcerns:\n1. The major concern is a relative lack of novelty and contribution. Since geometric graphs are generic graphs equipped with additional node coordinates, the definitions of GWL and IGWL are very straightforward (which is not negative). But results in Section 4.1 on characterizing the gap between GWL and IGWL are too easy to obtain, including Prop 2, 3, 4, 7 and Theorem 6. Also, the results in Section 4.2 on the correspondence between GNNs and (I)GWL are too straightforward.\n2. Theorem 21-23 build results on lower bounds of numbers of aggregators for injective functions. It would be great if it discusses whether or not such numbers are linked with the number of layers. In my mind, oversquashing [1]  might be interesting if its discussion can be generalized to geometric graphs. From a broader view, since this work is to generalize theoretical discussion on generic graphs to geometric ones, it would be better to develop a bigger story including richer contents, such as oversmoothing and oversquashing, rather than expressive power only.\n\nReference:\n[1] Understanding over-squashing and bottlenecks on graphs via curvature.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good.\n\nThe quality and novelty are limited as discussed above.",
            "summary_of_the_review": "I would like to recommend rejection. I am willing to raise my score if the above concerns are resolved.\n\n\n---\n\nThe score is raised from 3 to 5.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4690/Reviewer_Vdrv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4690/Reviewer_Vdrv"
        ]
    }
]