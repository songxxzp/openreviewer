[
    {
        "id": "tAHnUwgPUHC",
        "original": null,
        "number": 1,
        "cdate": 1665689982282,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665689982282,
        "tmdate": 1665689982282,
        "tddate": null,
        "forum": "E4-uRvmKkeB",
        "replyto": "E4-uRvmKkeB",
        "invitation": "ICLR.cc/2023/Conference/Paper2025/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes learning a network for action classification by gathering the target data with external datasets in a multi-task scenario. The method consists of a video transformer architecture (ViViT) that produces spatio-temporal features which are then \u201cROI-aligned\u201d and fed into a single classifier. For the tasks with no bounding boxes an average pooling is applied to the full spatio-temporal features. The main study for the paper resides on learning with multiple datasets at a time. \n\nThe method is tested primarily on AVA 2.2 as well as on Kinetics and Moments in Time. A set of ablation studies and comparisons against state of the art works is also provided, showing promising results. \n",
            "strength_and_weaknesses": "The paper is rather weak and does not shed any light neither on multi-task learning nor on the domain of action recognition. In my humble opinion, the paper basically delivers the message that \u201ctraining simultaneously on several datasets at the same time improves performance\u201d, which does not stand strong to be accepted at ICLR. \n\n\nThe paper lacks a proper complexity analysis which is needed to understand where does it stand in terms of the tradeoff pretraining/model size-GFLOPS/mAP. The results on Table 5 are a bit misleading: how would a ViViT-L pre-trained on Kinetics-700 and then fine-tuned with the other datasets perform? Different pre-trainings affect the performance of the models drastically on AVA. Authors are encouraged to follow recent works such as MemVIT [CVPR\u201922], MViTv2 [CVPR\u201922] or WOO, and split the results according to pre-training, architecture details and complexity (FLOPs and Params). Without such analysis the paper sets a nice technical report on how to train with a combination of supporting datasets. \n\nIt is also important to note that one of the main motivations of the proposed approach is that \u201cmany video models are initialised from networks pretrained on ImageNet, then finetuned on Kinetics and then finally finetuned for spatio-temporal detection. In our proposed approach, we simultaneously finetune our model on multiple datasets\u201d. However, pretraining the model on WTS gives +3.3% mAP from 32.8 to 36.1! Isn\u2019t this against the main purpose of the paper? \n\nFinally, I believe the paper should include a clear breakdown of resources and time consumption to train the models between using the three step approach of pretraining on ImageNet, then on Kinetics, then on AVA, w.r.t. using the co-finetuning. What is the converge rate for each of the approaches? This needs to be included.\n\nIn summary, my impression is that the paper, while showing some interesting results, it does not hold a contribution strong enough to be considered for ICLR. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to reproduce. The quality and the novelty, in my opinion, do not meet the standards of a paper expected to appear at ICLR.",
            "summary_of_the_review": "The paper is an interesting report, but does not hold a strong contribution in the domain of action recognition or multi-task learning. The insight seems to be that training with more datasets in a multi-task scenario helps learning a stronger backbone. However, it is shown that without a proper pre-training (in the large-scale dataset of WTS) the method does not achive competitive results. For these reasons I do not feel in the position to recommend this manuscript for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2025/Reviewer_kFF7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2025/Reviewer_kFF7"
        ]
    },
    {
        "id": "Jo3Yg9r0FH",
        "original": null,
        "number": 2,
        "cdate": 1666296707898,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666296707898,
        "tmdate": 1666296707898,
        "tddate": null,
        "forum": "E4-uRvmKkeB",
        "replyto": "E4-uRvmKkeB",
        "invitation": "ICLR.cc/2023/Conference/Paper2025/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper challenges the widely-used \"pretrain-then-finetune\" pipeline, ie, the model is first pre-trained on a large \"upstream\" dataset, and then is finetuned on a smaller \"downstream\" dataset. In contrast, this paper proposes a co-finetuning training strategy. As the name suggests, they try to train a single model on multiple \"upstream\" and \"downstream\" tasks simultaneously. Results on the spatio-temporal action localization tasks show that the model with co-finetuning strategy achieves better performance than the conventional pretrain-then-finetune strategy.",
            "strength_and_weaknesses": "## Strengths\n+ The direction for exploring more effective knowledge transfer is interesting and promising. If a new effective and efficient paradigm can be discovered, it will make a huge contribution to our community.\n\n## Weaknesses\n+ The proposed idea is quite simple and naive. It follows conventional multi-task training, where multiple tasks share the same backbone. Meanwhile, they propose two types of sampling strategies for obtaining sample mini-batches. Thus, there is no technique contributions in this paper.\n+ Since the method looks general enough, it would be more convincing to share more results on different tasks. In another word, it is not clear why this proposed framework is more suitable for spatio-temporal grounding than other tasks?\n+ From Table 4, the results are sensitive to different minibatch sampling strategies. For example, the method with \"Alternating\" samples achieves much worse results than the baseline models (cf. Table 2 and Table 4)",
            "clarity,_quality,_novelty_and_reproducibility": "+ Clarity: The whole paper is well-written, and easy to understand.\n+ Originality: The main idea is essentially the same as the multi-task training.",
            "summary_of_the_review": "The paper focuses an interesting problem, and tries to challenge the prevalent pretrain-then-finetune pipeline. However, the proposed method is a plain multi-task training, which has no new technique contributions. Meanwhile, the proposed method is general, and it would be more convincing by reporting more results on other different tasks.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2025/Reviewer_mBx1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2025/Reviewer_mBx1"
        ]
    },
    {
        "id": "sGR4QLqgoX",
        "original": null,
        "number": 3,
        "cdate": 1666592797331,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666592797331,
        "tmdate": 1666592797331,
        "tddate": null,
        "forum": "E4-uRvmKkeB",
        "replyto": "E4-uRvmKkeB",
        "invitation": "ICLR.cc/2023/Conference/Paper2025/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose \"co-finetuning\" for action localization. Namely, instead of commonly used pre-training and finetuning 2-step procedure, with \"co-finetuning\" the pre-training tasks and downstream tasks are trained simultaneously. The authors show that this performs better than the standard 2-step procedure on datasets including AVA.",
            "strength_and_weaknesses": "Strengths:\n+ The authors conducted the experiments on multiple different datasets (including MiT, K400, AVA, SSv2). This helps understand the generalization of the proposed method on different settings.\n\nWeakness:\n- Regarding the MiT-K400-AVA experiments in Table 2, \"MiT + K400 + AVA\" (26.3) only outperforms \"Pretrain on K400 -> MiT + finetune on AVA\" (25.3) by 1%. I don't find the difference of 1% very significant. Furthermore, while the authors do control that both experiments use the same number of iterations, there're many ways to pick the number of epochs for each dataset in \"K400 -> MiT -> AVA\". It's possible that an \"optimal\" pick will further shrink the performance gain. Thus, from the experiment results alone, I'm still not convinced the effectiveness of co-finetuning. Similar concerns also apply to results in Table 1 and 3.\n- Regarding the results in Table 1, I wonder why the K400 experiment in this paper obtains 74.9 using ViViT-B while the original ViViT paper obtains 79.9. I wonder if this suggest the K400 models are not sufficiently (pre-)trained in this paper. If that's the case, that might contribute to why co-finetuning helps --- it allows the model to train more on K400.\n- Regarding novelty, I still don't find the proposed method very different from multi-task learning. Furmore, there are also prior works, such as \"Simple Multi-dataset Detection\" by Zhou, Koltun, and Krahenbuhl, that also train on multiple datasets. I find the discussions comparing with those papers missing.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good. The authors do provide implementation details and the writing is easy to follow.\n\nI find the novelty limited. Training on multiple datasets simultaneously isn't new (e.g. \"Simple Multi-dataset Detection\" by Zhou, Koltun, and Krahenbuhl does too) and very similar to multi-task learning. \n\nThe experiments are unfortunately not convincing. As discussed above, the K400 performance is quite different from the original paper, and it's not clear how the training recipe of the finetuning baseline (e.g. number of epochs) is picked.",
            "summary_of_the_review": "Overall, my main concerns are regarding the experiments and novelty. I overall find this paper below the acceptance threshold. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2025/Reviewer_L9xn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2025/Reviewer_L9xn"
        ]
    },
    {
        "id": "Bz7bAIdAfwj",
        "original": null,
        "number": 4,
        "cdate": 1666770409756,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666770409756,
        "tmdate": 1666770409756,
        "tddate": null,
        "forum": "E4-uRvmKkeB",
        "replyto": "E4-uRvmKkeB",
        "invitation": "ICLR.cc/2023/Conference/Paper2025/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to leverage multiple upstream datasets to co-finetuning for spatio-temporal action localization. The main contribution of this work is a transfer learning model that involves multiple classification heads for downstream detection tasks.",
            "strength_and_weaknesses": "Strength:\nThe observation that involving multiple upstream tasks in improving spatio-temporal action localization.\n\nWeakness:\n1. The novelty is trivial, the idea of transfer knowledge classifier for object detection is already proposed in previous work[1]. This work extend similar idea to a new task with some improvements like using multiple upstream tasks. The way to use multiple upstream tasks is simply co-finetuning, which is less novel.\n\n2. The experiment is not convincing, Table 1 and 2 only compare with traditional fine-tuning, however, there are plenty of transfering learning approaches that available and you could compare with some of them. A simple package can be found here:https://github.com/thuml/Transfer-Learning-Library\n\n3. Table 5 misses some SoTA methods, which can be found here:https://paperswithcode.com/sota/action-recognition-on-ava-v2-2\nit seems that the reported performance is worse than current SoTA.\n\n\n\n[1]Guo, Shuxuan, Jose M. Alvarez, and Mathieu Salzmann. \"Distilling Image Classifiers in Object Detectors.\" Advances in Neural Information Processing Systems 34 (2021): 1036-1047.",
            "clarity,_quality,_novelty_and_reproducibility": "No code available.",
            "summary_of_the_review": "This paper advocates co-finetuning as transfer learning technique for spatio-temporal action localization, however, if the focus is on \"transfer learning\", the authors should compare with more recent transfer learning approaches instead of simply traditional fine-tuning, if the focus is a simple approach for the downstream task, as I mentioned, the performance is worse than current SoTA.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2025/Reviewer_wkyw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2025/Reviewer_wkyw"
        ]
    }
]