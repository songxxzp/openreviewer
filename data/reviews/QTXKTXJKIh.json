[
    {
        "id": "YxbhDr2av9",
        "original": null,
        "number": 1,
        "cdate": 1666196166869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666196166869,
        "tmdate": 1670833959367,
        "tddate": null,
        "forum": "QTXKTXJKIh",
        "replyto": "QTXKTXJKIh",
        "invitation": "ICLR.cc/2023/Conference/Paper4474/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper study the collaborative MAB, where agents can communicate together to handle the same MAB problem. The authors propose a new notion of regret: the individual regret is the maximum regret over the players. This notion of regret is given in addition to the usual notion of regret. This regret could be related to the notion of fairness, since the closer the individual regret to regret the greater the fairness is. \nIn collaborative MAB the goal is to minimize the regret but also the communication cost. They provide a communication policy that has a communication cost in O(log log T) and that benefits from a near-optimal regret bound for both notions of regret. In some experiments, they show that their algorithm (UCB-TCOM) provides a good compromise between group regret and individual regret, while they obtain a low communication cost, even if it is not the smallest. \n",
            "strength_and_weaknesses": "The proposed communication policy is sound. The analysis and the experiments are well done. \nHowever, the paper presents some shortcomings. \n\n1/The definition of both regrets seems incorrect. Indeed, there is an expectation in the regret, while there is no random variable inside: \\mu(A^i_t) is the mean reward of arm chosen by agent i at time t. Did the authors take the expectation over the choices of players? Why not using the usual definition of pseudo-regret (cumulative difference between the expected reward of the optimal arm and the expected reward of the chosen arm)?\n\n2/ In comparison to DEP2, UCB-TCOM is outperformed in two metrics (group regret and communication time). So the only advantage of UCB-TCOM is that its individual regret is smaller. So in the comparison, individual regret is crucial, and it has to be better motivated. May be by enhancing fairness ?\n\nMinor comments:\nTable 1 presents an error: the communication time is in O(1) for DEP2.\nThe authors could refer some works done in the collaborative exploration setting. Indeed, explore then commit algorithm can also reach a near-optimal regret (Garivier et al 2016), and despite different contexts, the two following papers use similar ideas: doubling trick for communication policy in (Hillel et al 2016), communicating only about bad arms in (Feraud et al 2019).\n\n(Hillel et al 2013) Distributed exploration in multi-armed bandits, NeurIPS 2016. \n\n(Feraud et al 2019) Decentralized exploration in multi-armed bandits, ICML 2019.\n\n(Garivier et al 2016), On explore the commit strategies, NeurIPS 2016.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Despite some weaknesses, the paper is very well written. \nThe idea used in TCOM is well known: the doubling trick. It has already been used for minimizing communication rounds (Hillel et al 2014). However, the present paper introduces some novelty that are sound.\n",
            "summary_of_the_review": "Overall, despite some shortcomings that can be fixed, this paper is technically sound, well written, and presents some novelties.\n_____________________________\nI read the rebuttal and I thank the authors for their answers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4474/Reviewer_VQR1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4474/Reviewer_VQR1"
        ]
    },
    {
        "id": "lspjeZMsJn",
        "original": null,
        "number": 2,
        "cdate": 1666703651555,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703651555,
        "tmdate": 1666703651555,
        "tddate": null,
        "forum": "QTXKTXJKIh",
        "replyto": "QTXKTXJKIh",
        "invitation": "ICLR.cc/2023/Conference/Paper4474/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work focuses on a cooperating learning version of the classical multi-armed bandit problem, where M agents play the same problem instance over T rounds. Agents can communicate their observations through global broadcasts. The authors propose an algorithm that needs O(log log T) communication rounds to achieve logarithmic individual as well as group regret.",
            "strength_and_weaknesses": "The main contribution appears to be showing that a factor of M improvement in group as well as individual regret is possible using only O(log log T) rounds of (broadcast) communications between the M agents. The improvement is relative to the scenario where the agents do not communicate and simply keep playing their own instance, oblivious to the rewards accrued by others. The communication efficiency of O(log log T) is achieved by sharing information about (empirically) sub-optimal arms as opposed to optimal ones, which I find to be a very neat idea.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Typo: In line 292, the lower bound is on group regret (not individual); the superscript 'ind' should be removed I reckon?\n\nIs factor of M the best possible improvement? In particular, how does group as well as individual regret compare with lower bounds for the scenario where an entire vector of M reward realizations is broadcasted per period? This, in my opinion, is a more interesting statistical baseline compared to the no communication one.\n\nCan you black-box the algorithm into communication and learning modules while preserving regret guarantees? This begs the question, is communication efficiency a fundamental property of the algorithm itself? Can you comment on whether Thompson Sampling might work using the same broadcast signals and the same broadcast rate?\n\nIs it possible to trace out a Pareto frontier of regret (individual/group) vs. communication overhead?",
            "summary_of_the_review": "The paper is generally well written and a pleasure to read. Appears to be technically sound, comprehensive in literature survey, and meaningful in contribution as it improves upon extant baselines by a considerable margin. It would be great if the authors could address or at least remark upon the questions raised earlier; that would really cement the contributions well and make it accessible for a more general audience. Overall, I vote for an acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4474/Reviewer_coEu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4474/Reviewer_coEu"
        ]
    },
    {
        "id": "QOcMsBw0k64",
        "original": null,
        "number": 3,
        "cdate": 1667095414087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667095414087,
        "tmdate": 1669416217547,
        "tddate": null,
        "forum": "QTXKTXJKIh",
        "replyto": "QTXKTXJKIh",
        "invitation": "ICLR.cc/2023/Conference/Paper4474/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies distributed multi-armed bandits with the goal of minimizing the largest regret incured among all the agents. This work makes two contributions: First, it suggests a different objective\u2014minimizing individual regret rather than group regret\u2014motivated by real-world applications. Second, it proposes a new algorithm\u2014UCB-TCOM\u2014with almost optimal bound for both individual and group regret. In addition, the new approach only incurs communication cost of $O(\\log \\log T)$ times.",
            "strength_and_weaknesses": "Strength\n* The new objective is strongly supported by practical applications. \n* Numerical simulations with real-world data are offered to verify the proposed algorithm\n* The paper is written well.\n\nWeaknesses\n* It seems to me the key technique to lower the communication complexity from $O(\\log T)$ to $O(\\log \\log T)$ is the doubling technique or observation-buffering broadcast mentioned in the paper, which is a well-known technique in bandit literature. It would be preferable if the author(s) could explain in the study why establishing the intended outcomes was challenging.\n--------------------\n\nAfter rebuttal\n\nThe author(s) have added discussions regarding the challenge raised during the proof of the communication complexity. And I have raised my score.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, in my opinion, and it is organized effectively for ease of reading. Despite the fact that the technique is not new, the proposed goal is intriguing and may have real-world implications.",
            "summary_of_the_review": "This paper is well-motivated by real-world applications. The author(s) have proposed a new algorithm with nearly-optimal individual and group regret. Besides, it incurs only $O(\\log \\log T)$ communication times. However, the techniques used are not brand-new. I struggle to understand how challenging it is to establish the theory in the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not find any concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4474/Reviewer_3zzv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4474/Reviewer_3zzv"
        ]
    }
]