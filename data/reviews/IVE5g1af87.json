[
    {
        "id": "OC9w9zk-Txd",
        "original": null,
        "number": 1,
        "cdate": 1666629636607,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629636607,
        "tmdate": 1666629636607,
        "tddate": null,
        "forum": "IVE5g1af87",
        "replyto": "IVE5g1af87",
        "invitation": "ICLR.cc/2023/Conference/Paper2328/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies the source of the worse performance (i.e. lower adversarial robustness) of importance-aware methods compared to standard adversarial training (and variations like TRADES). It is argued that importance-aware methods yield small logits margins for all points, while naive adversarial training shows large margins for a considerable portion of training points (easy points). This has the effect of reducing robustness on the easy examples. This motivates SOVR, which uses the cross-entropy loss on the easy points and the novel OVR loss on the difficult ones. In the experimental evaluation, SOVR outperforms importance-aware methods, and can be combined with regularization methods like TRADES and AWP.",
            "strength_and_weaknesses": "Strengths\n- The analysis of the logits margins is interesting and suggests an explanation for the worse performance of importance-aware methods compared to standard adversarial training.\n\n- The proposed SOVR is a reasonable solution in light of the analysis of the behavior of the logits margins, and in the experimental evaluation is shown to be effective.\n\nWeaknesses\n- The histograms of the logits margin shown in the paper are mostly for model at the last epoch of training. In Fig. 8 in the appendix one can see that, when using the model selected by early stopping, the left peak for adversarial training is much less pronounced, suggesting that it is mainly due to overfitting. While there's still a difference in the distribution of the margins between naive adversarial training and the importance-aware methods, I think the plots for the models with early stopping should be reported in the main part to give a more complete picture of the phenomenon the paper analyzes.\n\n- While SOVR outperforms importance-aware methods, a comparison to other methods (see e.g. https://robustbench.github.io/) for obtaining robust models is missing. The only direct comparison to TRADES is with WRN on CIFAR-10, and the two methods achieve similar results. SOVR getting similar or better results than existing methods would significantly strengthen the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well written, and the results clearly presented.\n\nQuality, Novelty: the analysis is interesting and the proposed SOVR shows good performance. Both are, to my knowledge, novel.\n\nReproducibility: sufficient details are provided.",
            "summary_of_the_review": "The paper presents an interesting analysis of importance-aware methods and a new training scheme to improve their results. A more thorough comparison to existing methods for robust classifiers would increase its relevance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2328/Reviewer_Fx8Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2328/Reviewer_Fx8Y"
        ]
    },
    {
        "id": "_fTmLFMSxZO",
        "original": null,
        "number": 2,
        "cdate": 1666681622529,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681622529,
        "tmdate": 1666681622529,
        "tddate": null,
        "forum": "IVE5g1af87",
        "replyto": "IVE5g1af87",
        "invitation": "ICLR.cc/2023/Conference/Paper2328/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work reveals that the adversarial examples used in adversarial training can be roughly divided into difficult examples and easy examples according to their logit margins. The proposed SOVR method aims to reduce the adversarial examples which have larger logit margins, then enhance the robustness of DNN models. ",
            "strength_and_weaknesses": "I'm confused about \"On the other hand, logit margins of importance-aware methods concentrate near zero, and thus, importance-aware methods reduce the logit margins of easy samples. This implies that the weighted cross-entropy used in importance-aware methods is not very effective in increasing logit margins. To increase the logit margins of difficult samples, ...\". First, why the observation of \"importance-aware methods reduce the logit margins of easy samples\" can derive \"importance-aware methods is not effective in increasing logit margins\"? Second, why do you want to increase the logit margins of difficult samples? Do the large logit margins indicate the examples are well learned? And the proposed method is to minimize the logit margins of \"easy samples\" if I didn't misunderstand.\n\nAnd some questions about experiments are as follows:\n1. What about TRADES perform in Figure 2? Can the histogram of TRADES also be divided into two peaks?\n2. In Table 1, what about the results of TRADES?\n3. In Table 2, I'm afraid that the results can't convince me that the proposed method can increase the robustness. Compare with AT+AWP and TRADES, the robust accuracy increase while the clean accuracy drops. It is well known that adversarial training methods have a trade-off between clean and robust accuracy. The performance increase of compare with SEAT is marginal.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity needs to be improved.",
            "summary_of_the_review": "Confusing description and lack of convincing empirical results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2328/Reviewer_aV6p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2328/Reviewer_aV6p"
        ]
    },
    {
        "id": "C4sdWX8b8F",
        "original": null,
        "number": 3,
        "cdate": 1666946162359,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666946162359,
        "tmdate": 1670806730465,
        "tddate": null,
        "forum": "IVE5g1af87",
        "replyto": "IVE5g1af87",
        "invitation": "ICLR.cc/2023/Conference/Paper2328/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper first studies the relationship of logit margin loss $\\ell_{LM}$ and adversarial robustness and then proposes using one-versus-the-rest (OVR) loss to improve the logit margin loss.",
            "strength_and_weaknesses": "Strength\n- The paper is well-written.\n- The behavior of the logit margin loss and how OVR loss helps to improve logit margin loss is well-studied.\n\nWeaknesses\n- The idea is incremental because margin-based approaches and logit margin loss are natural and have been well studied before. Additionally, it is also well-known that OVR approach helps to boost the margin.\n- The proposed method only slightly outperforms the baselines.\n- The assumption in Eq. (4) is extremely infeasible and simple compared to real-world uses of the CE and logit margin losses.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Although the paper is well-written and the concepts are deliberately empirically studied, the novelty of this idea is incremental.   The experimental results show that the proposed approach slightly improves the baselines.",
            "summary_of_the_review": "The paper is well-written. The concepts are deliberately empirically studied. However, in my opinion, the idea is incremental.\n\n-----------------------------------------------------------------------------------------------\nThanks the authors for addressing my concerns. I still believe that the idea is incremental and the theory needs to make strong assumptions. Therefore, I am keen on my current score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There are no ethics concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2328/Reviewer_G3JG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2328/Reviewer_G3JG"
        ]
    },
    {
        "id": "iF03KT9sFfe",
        "original": null,
        "number": 4,
        "cdate": 1666986503637,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666986503637,
        "tmdate": 1666986503637,
        "tddate": null,
        "forum": "IVE5g1af87",
        "replyto": "IVE5g1af87",
        "invitation": "ICLR.cc/2023/Conference/Paper2328/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel adversarial training approach, which can increase the logit margin without losing too much clean accuracy. The authors discover that the logit margin of traditional adversarial training has two peaks. Based on this, they designed a switching mechanism and a novel margin loss function (OVR) to improve traditional adversarial training, aiming to push the right peak to the left while maintaining the left peak. Experiments show superiority over baselines.",
            "strength_and_weaknesses": "Pros:\n- The paper is well written and easy to follow: start from some observations, assumptions, theorems, and the design of their approach.\n- The finding about logit margin distribution is interesting. The corresponding analysis is also solid.\n- The experiments demonstrate the effectiveness of the proposed method.\n\nCons:\n- For Eq. 8, why did you choose log(1+e^-z)? The author should explain the rationale behind the choice.\n- In Table 2, if we add SOVR to Trades (TSOVR), the clean accuracy decreases. It is a contradiction to the author\u2019s maintaining clean accuracy claim. AWP results show the same phenomenon, i.e., clean accuracy decreases while applying the proposed method. \n- The authors say: \u201cSOVR achieved better trade-off: SOVR achieved similar robust accuracies to TRADES, while it achieved better clean accuracy\u201d However, the clean acc are 84.2 for TRADES and 83.1 for TSOVR. In addition, I disagree with the \u201cbetter trade-off\u201d for this table.\n- What are the results if the proposed method encounters the logit scaling attack (Hitaj)? In addition, it is better if the author can show some potential attack ideas that can break the proposed defense. \n- Comparing Fig. 2(b) and Fig. 4, the peak shifting is a little weak. Is there any direction to improve this?\n- Observing Fig. 6, it seems that the proposed method did not work well if the number of classes increased. (Cifar-100 is much worse than Cifar-10.) It is better if the authors discuss this. Also, this is a potential limitation of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the overall quality is good. The observation is novel. All the experiment details are in the appendix.\n\n",
            "summary_of_the_review": "The observation in the paper is interesting, but the experiment results are not convincing to support the claims.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2328/Reviewer_HRBR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2328/Reviewer_HRBR"
        ]
    }
]