[
    {
        "id": "IPVF3CHduO",
        "original": null,
        "number": 1,
        "cdate": 1666392326978,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666392326978,
        "tmdate": 1669475337154,
        "tddate": null,
        "forum": "w1w4dGJ4qV",
        "replyto": "w1w4dGJ4qV",
        "invitation": "ICLR.cc/2023/Conference/Paper437/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the benefits of model-based generalization by comparing the performance of DQN on the rollouts of a learned model, and DQN on a replay buffer. Theoretically, this paper proves that learning a transition model can be more efficient (in terms of pruning invalid solutions) than learning a Q-function using Bellman equations. Empirically, this paper proposes three environment characteristics where the model-based methods could be better, and demonstrates them using constructed toy environments. \n\n**Summary of the experiments**\n\nSetup: \n\nExperience replay (ER): Run DQN with a replay buffer of the past trajectories (called $\\mathcal{D}$)\n\nModel-based: use $\\mathcal{D}$ to train a model. And then train a DQN on fake trajectories generated by the learned model starting from a random state from $\\mathcal{D}$ (similar to MBPO).\n\nThe goal is to make the training of DQN exactly the same except for the data distribution (ER vs. model rollouts).\n\nResults: \n\nThis paper compares ER and model-based methods on toy environments in the online RL setting. The number of updates to the Q-network is the same for model-based and ER. In every update, the Q-network is trained using 320 samples from the replay buffer for ER, and 32 model rollouts with length 10 for model-based methods (so the batch size is also the same). This paper shows that for the toy environments model-based is in general better than ER (Figure 3).",
            "strength_and_weaknesses": "Strengths:\n\n- This paper studies the benefit of model-based generalization by comparing model-based rollouts with experience replay. To some extent, this comparison is much fairer because the only difference is the distribution of training data (replay buffer vs model rollouts). This approach is neat and results in some interesting theoretical results (Theorem 1).\n- The illustrative example in Section 2 is very easy-to-follow and already well demonstrates the intuitions of this paper.\n\nWeaknesses:\n\n- This paper only discusses one side of the story. In fact, model-based method outperforms experience replay in all the environments studied in this paper. Together with Theorem 1, is it possible to make the claim that model-based methods are always better than experience replay? In other words, discussions and ablation studies about the necessity of the three characteristics in Section 3 are missing. Hence, the scope of the conclusions of this paper is unclear.\n- It\u2019s unclear to me when and how the characteristics proposed in Section 3 can be applied to benchmarking environments such as OpenAI Gym. For example, if we predict whether model-based methods are better using the intuition in this paper, does the prediction match reality? Since this is mostly an empirical paper, I would expect more discussions on the practical impact of the conclusions.\n- the experiments provide one possible explanation for why MB is better than ER --- learning the transitions can be easier because of the implicit bias of the function class (e.g., the factored structure of the transition) and therefore MB has better sample efficiency. This explanation is somewhat expected and whether it is the main factor in benchmarking experiments such as Mujoco is unclear.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written in general, except for a few ambiguities:\n- The second bullet point in Section 3 states that \u201cThe return should depend sharply on the policy\u201d, and it\u2019s unclear to me what \u201csharply\u201d means here. Does it mean that the return of a policy fluctuates a lot? Then does the sparse reward MDP (e.g., the PanFlute environment) satisfy this statement? \n- How large is the horizon length in all the environments?\n\nSome references are missing in this paper. E.g., [1] also discusses when model-based methods are better than model-free methods. On a high level, both [1] and this paper constructs environments where the dynamics are simple functions but the corresponding Q function is complicated.\n\n[1] Dong, Kefan, et al. \"On the expressivity of neural networks for deep reinforcement learning.\" International Conference on Machine Learning. PMLR, 2020.\n",
            "summary_of_the_review": "On the plus side, the problem setup of this paper is neat and interesting, the results are clean and intuitive. On the minus side, more discussions about the practical relevance and ablation studies are needed since the results are mainly empirical. As a result, I recommend a weak rejection for the current version of this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper437/Reviewer_4bHT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper437/Reviewer_4bHT"
        ]
    },
    {
        "id": "mtAUORtz_FQ",
        "original": null,
        "number": 2,
        "cdate": 1666653191313,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653191313,
        "tmdate": 1666653191313,
        "tddate": null,
        "forum": "w1w4dGJ4qV",
        "replyto": "w1w4dGJ4qV",
        "invitation": "ICLR.cc/2023/Conference/Paper437/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper investigates the question of when and why model generalization is better than value function generalization in reinforcement learning. The paper starts with a theoretical result that gives the intuition on this question: reinforcement learning with a learned model of the transition dynamics can reduce the optimal action value function space than simply solving bellman constraints with experience replay. Then the authors provide some hypothesis/conditions that tends to favor model-based RL, and then justify them by testing on some designed toy experiments.",
            "strength_and_weaknesses": "Strength:\n- An interesting and important question to study, as when and why model-based RL is guaranteed to be better than experience replay can significantly influence us to design practical RL algorithms for real-world applications\n- Well-written and easy to follow, with many concrete examples and interesting discussions of empirical observations\n- Good introduction of the background\n\nWeaknesses/Questions:\n- The first motivating example in the introduction (an agent trying to cross a stream to reach the food) is not very clear and convincing. It is hard to understand the formulation of the state (is it a bandit problem given your state definition?), and what is the data (some state+reward pairs?) in your experience replay, and why model-free learning clearly/definitely fails (it can also generalize via value function)?\n- Perhaps the main rigorous answer is Theorem 1, where I was expecting to see when and why model-based RL is better. However, I found the result is not surprising/contains enough theoretical insights. It is basically saying, if we are given a set of hypothesis class for the transition dynamics, and we can successfully identify one that perfectly matches the data, then using that transition function (probably optimal if data is enough) to solve Bellman optimal equation is better than directly using the data to solve. This might be trivial and in practice, the key question is actually what theoretical guarantee we can achieve when the approximate model makes an error (upper bounded by some constant). If the data D is very limited and the best model consistent with D is still far from the true dynamics, then is model-based RL always better (since it incurs compounding error)? Another essential question to study is how will the model error propagates to solving Bellman optimality equations (the data is always from true dynamics but generated ones can be wrong)?\n- It seems most above important questions are not rigorously answered. The following sections provide nice examples and hypothesis on the conditions. But I feel they should be an empirical evidence to some more general theorems. I was expecting a formal sufficient (maybe not necessary) and hopefully general enough condition for when model generalization + compounding error is better than ER.\nTheoretically this should depend on specific algorithm, data collecting policy and sample size, etc. Otherwise, it may be hard to summarize some general principles that guides the design of algorithm for more complicated and realistic environments.\n- The observation that sometimes learned model is even better than the true model is very interesting. In principle, even using more smooth model can help exploration or optimization, this should eventually lead to bias. Will this be an issue and can we alleviate it using annealing (gradually reduce to true model)? In case of no bias, it seems to be something like a reward shaping or simply a coincidence that two MDPs have same solution. This may deserve more thorough study.\n- \"factored structure\" appears many times but seems fuzzy. Can you give a formal definition?",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good. The paper will benefits from more thorough theoretical analysis (see above section for detailed comments). The empirical results should be reproducible.",
            "summary_of_the_review": "Interesting question, nice examples and discussion of empirical observation. Needs more thorough and rigorous theoretical study on some key questions.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper437/Reviewer_8CUu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper437/Reviewer_8CUu"
        ]
    },
    {
        "id": "vm9WO0rQa-",
        "original": null,
        "number": 3,
        "cdate": 1666662480138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662480138,
        "tmdate": 1666662480138,
        "tddate": null,
        "forum": "w1w4dGJ4qV",
        "replyto": "w1w4dGJ4qV",
        "invitation": "ICLR.cc/2023/Conference/Paper437/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a high-level characterization of the properties of RL tasks in which model-based algorithms can be expected to improve generalization to new observations. Two primary quantities arise from this analysis: first, the state space should exhibit a simple underlying factored structure which the inductive bias of the model is well-suited to learning; second, the distribution of rewarding trajectories that an agent sees should be sufficiently sparse as to make learning from Bellman backups difficult, but still provide non-zero reward density so that there is some signal for a learned model to leverage.",
            "strength_and_weaknesses": "### Strengths\n\n- The paper provides nice intuitive examples of generalization in maze environments that highlight the limitations of model-based and value-based RL algorithms.\n- The paper is well-written and does a good job at summarizing and emphasizing the key take-aways from each section.\n- The experiment setup is clear.\nThe analysis of different learned models was enlightening as it highlighted the importance of the inductive bias of the world model, and because it provided a striking but also simple setting where a learning algorithm benefits more from an imperfect learned model than from the perfect world model. I appreciated that the authors went further to validate their hypothesis that the learned models were benefitting from reward smoothing in these contexts\n- Theorem 1 provides a nice contrast to the findings of Parr et al. in the linear setting. \n\n### Weaknesses\n\n- The paper focuses on a handful of small toy environments. While this is useful to guide the reader\u2019s intuition and to make the experimental findings interpretable, it also limits the generality of the paper\u2019s conclusions.\n- The degree to which the findings in this paper also apply to larger scale models is an open question. While the toy settings considered in the paper do a good job of illustrating some environment qualities can than influence the efficacy of simple model-based algorithms, it\u2019s not clear to me that these are the only parameters that can influence the relative performance of model-learning methods in more challenging tasks. For example, the influence of model learning on exploration, or on the stability of the underlying learning method, could also play a role in an agent\u2019s final performance in these settings. It\u2019s not clear to me whether the environment features and model smoothing properties identified in this paper would be a dominant factor in performance compared to these other factors. \n\n- I suspect that the value-based agents might be worse at generalization than policy-based agents based on prior literature on e.g. the ProcGen suite (Raileanu et al., 2020), and would have liked to see the inclusion of a policy-gradient method in the set of baselines. \n- The presentation of the proof of theorem 1 could be improved: the definition of the transition dynamics for $S_t[0]$ for example, seem to be defining different quantities in the top and bottom line of the bracket, and the hypothesis class $H$ from which the q-functions are selected is not defined. I assume it is the set of all real-valued functions over the state space, but this should be stated explicitly. \n- While Theorem 1 is a nice observation, the statement itself is relatively weak. It does not, for example, provide a means of quantifying the degree to which the hypothesis space can be reduced by model classes with certain properties. Nor does it address settings where the algorithm may need to trade off between a limited set of environment steps and the approximation error of a model. This limits the significance of the theoretical contribution of the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well-written and clear. While model-based RL has been well-studied in the literature, the generalization of learned models to new observations has received less attention. I have not seen the paper\u2019s main theorem elsewhere in the literature.",
            "summary_of_the_review": "Overall, this paper provides a nice set of interpretable examples that highlight the different behaviour of model-free and different model-learning algorithms. One of these examples in particular illustrates how the smoothness bias in DNN function approximators can facilitate learning and generalization in model-based algorithms. The paper suffers two main limitations: first, its main theoretical result is a relatively weak statement, and doesn\u2019t directly offer a concrete set of implications for the effect of model-learning methods on generalization. Second, it is not clear whether the empirical findings concerning the behaviour of model-based and model-free methods will generalize outside of the small environments presented in the paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper437/Reviewer_JiYh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper437/Reviewer_JiYh"
        ]
    },
    {
        "id": "BkIKylM7Fq",
        "original": null,
        "number": 4,
        "cdate": 1667342208347,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667342208347,
        "tmdate": 1667478537286,
        "tddate": null,
        "forum": "w1w4dGJ4qV",
        "replyto": "w1w4dGJ4qV",
        "invitation": "ICLR.cc/2023/Conference/Paper437/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work formalizes the generalization benefits of model-based RL methods in the context of deterministic episodic MDPs who's transitions probability belong to a known hypothesis class. The authors then discuss the intuition gained from their theory in various illustrative environments and take a closer look at the case when transitions can be assumed to be factored.",
            "strength_and_weaknesses": "This work examines an important open question about the benefits of model-based RL. Although these methods are commonly believed to be more sample efficient, formal results on the subject are sparse. This work will likely be of interest to the RL community.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The contributions are well motivated and, to the best of my knowledge, novel.\n\nIt might be worth mentioning [1] in the related work (e.g., as part of paragraph 3, p. 9). Although that work doesn't consider generalization, it does compare the gradient dynamics and convergence rates when using \"end-to-end\" model-based method to when the value function is encoded directly with an equally expressive estimator.\n\n[1] Clement Gehring, Kenji Kawaguchi, Jiaoyang Huang, Leslie Pack Kaelbling. Understanding End-to-End Model-Based Reinforcement Learning Methods as Implicit Parameterization. NeurIPS, 2021",
            "summary_of_the_review": "Assuming the novelty isn't brought in question, this paper seems like a clear accept. Ideas are well motivated and of interest to RL.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper437/Reviewer_5uCS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper437/Reviewer_5uCS"
        ]
    }
]