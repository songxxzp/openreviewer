[
    {
        "id": "2w-NUu86dd-",
        "original": null,
        "number": 1,
        "cdate": 1665589837456,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665589837456,
        "tmdate": 1666767068347,
        "tddate": null,
        "forum": "SNwH0dDGl7_",
        "replyto": "SNwH0dDGl7_",
        "invitation": "ICLR.cc/2023/Conference/Paper3754/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studied the deployment efficient reinforcement learning in linear MDP. The authors proposed new algorithms with improved sample complexity bound than previous papers as well as the near-optimal deployment complexity.",
            "strength_and_weaknesses": "# Strength\n\nThe sample complexity to state of the art, to my knowledge. The algorithm also seems interesting and somewhat novel.\n\n\n# Weakness\n\nI think the main issue is whether it is appropriate to claim the algorithm does not require the knowledge of $\\lambda^*$. It seems to me that the main reason the algorithm does not use the knowledge of $\\lambda^*$ is because the authors assumed that the accuracy level $\\epsilon$ is smaller comparing with $\\lambda^*$. From this perspective, it is kind of misleading to claim this paper get rid of knowledge of $\\lambda^*$. Or maybe the authors should include discussion about how to avoid knowledge of $\\lambda^*$ for arbitrary $\\epsilon$.\n\nMinor issues:\n\n(1) the linear MDP is kind of restrictive, although more general than tabular setting.\n\n(2) when $\\lambda^*$ is extremely small, the algorithm can only success when choosing a extremely small $\\epsilon$.\n\n(3) algorithm seems not computationally efficient",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writing is clear and the proof seems correct. The algorithms and parts of the analysis are novel to me.",
            "summary_of_the_review": "Although some weakness in this paper, I still found the results interesting, and I recommend for the acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3754/Reviewer_i7oS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3754/Reviewer_i7oS"
        ]
    },
    {
        "id": "V7OrpnN3Z2h",
        "original": null,
        "number": 2,
        "cdate": 1666761528664,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666761528664,
        "tmdate": 1666761528664,
        "tddate": null,
        "forum": "SNwH0dDGl7_",
        "replyto": "SNwH0dDGl7_",
        "invitation": "ICLR.cc/2023/Conference/Paper3754/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies reward-free learning for linear MDPs. In particular, the authors focus on reducing the number of deployment (or switching cost). The proposed algorithm achieves a sample complexity of $\\tilde{O}(d^2H^5/\\epsilon^2)$ and deployment complexity $O(H)$, where the order of $d,\\epsilon$ are near-optimal in the sample complexity, and the deployment complexity is tight up to constant factors.",
            "strength_and_weaknesses": "Strengths: As stated above, the theorectical results of this paper are non-trivial. In technique, the major contribution is  utilizing the G-optimal design to reduce the order of $d$. There are also some interesting by-products, e.g., resutls in Section 7.\n\n\nWeakness: My major concern is the computational cost of the proposed algorithm. The authors also admit that the optimization problem (1) is computational intractable. In my opinion it should not be neglected. For the tabular case, recent work [Zhang et. al.] has achieved a computational efficient algorithm to minimize the number of batches (or deployments), where the authors manage to avoid enumeration over the policy space. So it is natural to ask whether efficient algorithm for linear MDPs is possible.\n\nReference: [Zhang et. al.]  Near-Optimal Regret Bounds for Multi-batch Reinforcement Learning",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well presented. There is no numerical experiments.",
            "summary_of_the_review": "Given the considerations above, the paper is on my broaderline. I tend to accept it currently.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3754/Reviewer_idiY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3754/Reviewer_idiY"
        ]
    },
    {
        "id": "GLOAv-hhA8",
        "original": null,
        "number": 3,
        "cdate": 1666763809374,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666763809374,
        "tmdate": 1666763809374,
        "tddate": null,
        "forum": "SNwH0dDGl7_",
        "replyto": "SNwH0dDGl7_",
        "invitation": "ICLR.cc/2023/Conference/Paper3754/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of deployment efficient reinforcement learning with linear function approximation under the reward-free exploration setting. The authors propose a new algorithm that achieves optimal deployment complexity and near-optimal sample complexity at the same time. Their techniques include an exploration-preserving policy discretization and a generalized G-optimal experiment design, which makes incremental contributions to the RL theory community.",
            "strength_and_weaknesses": "Strength\nS1: The proposed algorithm achieves SOTA sample complexity and preserves optimal deployment complexity simultaneously.\nS2: The proposed exploration-preserving policy discretization looks novel.\n\nWeakness\nS1: The proposed algorithm appears to be only making limited improvement compared to existing works in (Huang et al. 2022). Specifically, the paper considered a well-formulated dedicated problem, yet only makes a factor of d improvement on sample complexity, compared to that in (Huang et al. 2022). Besides, the proposed algorithm is computationally inefficient.\nS2: The technical contribution of this paper is limited and below the threshold of ICLR. The basic algorithm framework looks similar to that in (Huang et al. 2022), except upgrading the policy set and estimating the value functions instead of optimism, as pointed out in Appendix G. Besides, the idea of estimating the value functions instead of optimism has already been proposed in (Chen et al. 2021) for reward-free exploration in linear mixture MDPs.\nS3: The overall writing is not very clear such that readers are hard to follow the technical points, while this paper focuses on algorithm techniques. It is strongly recommended that authors could provide more details about G-optimal design in the main body of the paper.\n\n(Chen et al. 2021) Chen, Xiaoyu, Jiachen Hu, Lin F. Yang, and Liwei Wang. \"Near-optimal reward-free exploration for linear mixture mdps with plug-in solver.\" arXiv preprint arXiv:2110.03244 (2021).\n\nComments and Questions\nC1: The writing of the introduction looks similar to that in (Huang et al. 2022) and does not provide much further information.\nC2: The linear MDP definition in (Jin et al., 2020b)\u2019 paper is different from this paper. There $\\mu_h$ is a matrix, not a vector.\nC3: Assumption 2.1 is very strong and limits the generality of the algorithm, as those in (Zanette et al., 2020b; Wagenmaker & Jamieson, 2022). Since under linear MDP, if some direction is hard to encounter, we do not necessarily need to gather much information on this direction. Besides, I do not find analogous assumptions in (Huang et al., 2022).\nC4: It is more common to say different \u201cstages\u201d of $h$, instead of \u201clayer\u201d\nC5: The full-text layout is poor since the authors manually decrease the margin between paragraphs and sections. For example, there is an overlapping between the text and the footnote line.\nC6: In the paragraph below Theorem 7.1, what is the \u201cmild assumption\u201d to recover the best-known result from (Qiao et al. 2022).\nC7: As stated in Section 7.2, why the computational issue of the algorithm is usually not considered as a fundamental barrier? A computationally-inefficient algorithm largely hurts the significance of the proposed algorithm.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper can be improved. The authors should also further highlight the novelty of their results, in particular, compared to that in (Huang et al. 2022). ",
            "summary_of_the_review": "The paper considers a well-formulated theoretical problem and makes incremental improvements to existing algorithms. Although some parts of the algorithm and proof are novel, the proposed algorithm is computationally-inefficient, and the proposed techniques are dedicated to the DE linear MDP. Thus, the overall contributions are marginally below the ICLR threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3754/Reviewer_y46E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3754/Reviewer_y46E"
        ]
    },
    {
        "id": "5R6k5Eqrrfz",
        "original": null,
        "number": 4,
        "cdate": 1667287150565,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667287150565,
        "tmdate": 1667287150565,
        "tddate": null,
        "forum": "SNwH0dDGl7_",
        "replyto": "SNwH0dDGl7_",
        "invitation": "ICLR.cc/2023/Conference/Paper3754/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies deployment-efficient reward-free exploration problems in the linear MDP setting. The authors propose an algorithm that can achieve optimal deployment efficiency.",
            "strength_and_weaknesses": "Strength:\n\nThe theoretical results seem to be sound although I did not check all the details. The writing is clear and the discussion about prior work is sufficient.\n\nWeakness:\n\nI think this paper is a combination of several existing RL theory works. Reward-free exploration problems with linear function approximation have been studied in the literature. Deployment-efficient (similar to low-switching cost RL) problems have also been studied. This paper combines all the ingredients and resolves some additional technical difficulties to obtain the new results. This is good but I feel the novelty is not significant.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is overall well-written. The results appear to be new in the literature, although not novel enough.",
            "summary_of_the_review": "I think this paper is good overall. I did not check the details of the proof so I don't have high confidence in my assessment.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3754/Reviewer_W2e3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3754/Reviewer_W2e3"
        ]
    }
]