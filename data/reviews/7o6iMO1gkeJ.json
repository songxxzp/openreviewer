[
    {
        "id": "vUeZZTQ37G",
        "original": null,
        "number": 1,
        "cdate": 1666150859908,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666150859908,
        "tmdate": 1669590719847,
        "tddate": null,
        "forum": "7o6iMO1gkeJ",
        "replyto": "7o6iMO1gkeJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3984/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes DetectBench - a benchmark for Out-of-Distribution (OoD) Object Detection, as most works mainly focus on OoD image classification. After introducing a new train-test split on existing datasets and newly proposed dataset CtrlShift, the paper benchmarks a wide range of detectors (Faster R-CNN, DETR, etc.) with different backbones, self-implemented OoD image classification algorithms (ERM, RSC, GroupDRO, etc.), and different detectors with same backbone (Faster R-CNN, RetinaNet, DETR) on these datasets. The main findings are that IID object detection algorithms are marginal on OoD object detection, and the authors propose a new synthetic dataset, which can be used to simulate the two types of distribution shifts based on user\u2019s inputs.\n\nObject detection is an important task for many applications, including safety-critical ones such as self-driving cars and health care. Accordingly, improving the ability of object detectors to deal with unseen distribution shifts is highly relevant. This benchmark is thus well-motivated.",
            "strength_and_weaknesses": "Strong points:\n\n1. The paper is well written.\n2. Details of the datasets are provided.\n3. The authors run the benchmark on both two kinds of distribution shifts, proposed by [1].\n4. The authors introduce a new synthetic dataset CtrlShift, of which API can be modified to simulate the two kinds of shifts mentioned above. \n5. This would be a good contribution to the community.\n6. The authors extensively benchmark and run experiments on a wide range of detection algorithms.\n7. Some algorithms from OoD image classification are also adapted to DetectBench.\n\nNevertheless, there are some concerns about the paper that make the findings not convincing. I am happy to raise the score if the authors can address the concerns.\n\nWeak points:\n\n1. The train-test split is still a bit weak. For example, in DomainBed [2], for PACS dataset they divide it into four domains. Using the training-domain splitting strategy, they train the models on three selected domains and test on the other. This is to determine if the distribution shift is actually present, or it is because a model or architecture simply performs better on a specific domain. In this paper, the authors split the Weather in BDD100K into six domains, but only select two domains to train and the rest four domains to test. What would happen if we simply flip the train-test split? Will the ranking still hold?\n2. Missing the potential impact of the small-scale dataset on big models. Due to the dataset size, Sim2real has fewer data compared to BDD100K. This could make DETR models underperform compared to Faster R-CNN, as transformer models are generally data-hungry due to their size [5].\n3. Missing justification for choosing a strong backbone for Faster R-CNN and other detectors, but a regular backbone for DETR. In the paper, the authors choose ResNeXt-101-64x4d for Faster R-CNN and other models but use ResNet50 for DETR and Deformable DETR. Backbone network plays a vital role in the performance, which has been observed in both IID Object Detection [3] and OoD classification [4]. I think it is better to use the same backbone in this table so that we can measure the effect of different detectors.\n4. Missing results in (strong) Diversity Shift datasets. Diversity shift datasets such as Watercolor2k, Comic1k, Clipart1k [7] are missing from the paper. In the classification benchmark, the diversity shift datasets (PACS, VLCS, etc.) are much harder to imporve than correlation shift datasets (NICO, CelebA). From my experience, all OoD classification algorithms achieve very high accuracy by simply using ResNet50 as backbone on Correlation shift datasets, making gains of algorithms on this shift extremely minor.\n5. The findings are limited in novelty and not very convincing. Section 5.1 highlights some findings in the paper, but I do not find two points very \u201csurprising\u201d nor convincing:\n5.1) Finding 1 (The enormous achievements of object detection on IID datasets are marginal on the OOD condition): It is widely known that IID is a strong assumption in real-world use cases, and most models are sensitive to OoD tasks. This is not limited to only image classification, but also object detection, which has already been observed in BDD100K paper [6].\n5.2) Finding 3 (The generalization inconsistency between classification and object detection of domain generalization algorithms happens among different detectors): As pointed out above in Weak points 2 and 3, Sim2real is a relatively small dataset for DETR (8.5k images), which possibly means results in Table 5 are not reflecting an accurate performance of DETR. Based on Table 3, the reviewer thinks Deformable DETR is a more suitable choice, and the conclusion \u201cthe degeneration of domain generalization algorithms has little relevance to the detectors\u201d is probably a bit overclaimed.\n\nMinor fixes:\n1. Figure 4 X-axis is a bit hard to read. The authors may consider using vertical labels.\n2. Finding 3 in Section 5.1, \u201cTable 4.2\u201d should be \u201cTable 5\u201d\n\nReferences\n\n[1] Ye, Nanyang, et al. \u201cOod-bench: Benchmarking and understanding out-of-distribution generalization datasets and algorithms.\u201d In CVPR (2021).\n\n[2] Gulrajani, Ishaan, and Lopez-Paz, David. \u201cIn search of lost domain generalization.\u201d In ICLR (2020).\n\n[3] Zhu, Xizhou, et al. \u201cDeformable DETR: Deformable transformers for end-to-end object detection.\u201d In ICLR (2021).\n\n[4] Angarano, Simone, et al. \u201cBack-to-bones: Rediscovering the role of backbones in domain generalization.\u201d arXiv preprint arXiv:2209.01121 (2022).\n\n[5] Wang, Wen, et al. \u201cTowards Data-Efficient Detection Transformers.\u201d In ECCV (2022).\n\n[6] Yu, Fisher, et al. \u201cBdd100k: A diverse driving video database with scalable annotation tooling.\u201d arXiv preprint arXiv:1805.04687 2.5 (2018): 6.\n\n[7] Inoue, Naoto, et al. \u201cCross-domain weakly-supervised object detection through progressive domain adaptation.\u201d In CVPR (2018).",
            "clarity,_quality,_novelty_and_reproducibility": "See details above, and the authors provide the code but I haven't run it for reproducibility.",
            "summary_of_the_review": "I agree the proposed benchmark is well-motivated and the task is important for the research community. However, with the findings above, I currently give the paper a borderline reject score.\n\nUpdate: My concerns are not entirely addressed after the rebuttal. I will keep the rejection score for this submission, due to unconvincing experimental results (without cross-validation), weak diversity shift benchmark, the limited novelty of the findings, missing related works, and the proposed benchmarks overlapping with previous works.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3984/Reviewer_aJ6J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3984/Reviewer_aJ6J"
        ]
    },
    {
        "id": "Rxnc9LFjrY",
        "original": null,
        "number": 2,
        "cdate": 1666384156436,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666384156436,
        "tmdate": 1666384156436,
        "tddate": null,
        "forum": "7o6iMO1gkeJ",
        "replyto": "7o6iMO1gkeJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3984/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Building on top of BDD100k, Cityscapes and Sim10k, the proposed DetectBench in this work is an object detection benchmark for evaluating the OOD generalization performance of object detectors. The benchmark consists of 4 types of domain shifts, i.e., sim2real, weather, scene and time. The authors compared a number of object detectors on DetechBench, showing their yet poor OOD generalization performance. Using OOD generalization techniques developed for classification tasks, the object detection performance remains unsatisfactory, indicating OOD generalization of object detection is an underexplored task. ",
            "strength_and_weaknesses": "Strength: \nOOD generalization is an important task and is underexplored for object detection algorithms. This paper targets at this important problem. The authors proposed a OOD benchmark for object detection and compared a number of object detectors, empirically showing that existing solutions still struggle at dealing with domain shifts.\n\nWeakness:\n1. DetectBench does not differ much from previous benchmarks in this area. For example, [1] uses the eight shared categories in Cityscapes, Foggy Cityscape and BDD100k, and the Car category on these three datasets and additionally SIM 10k and KITTI. [2] also uses BDD100K, Cityscapes, Sim10K and KITTI as four domains, and train on three of them and test on the last one. The proposed CtrlShift dataset is different to these benchmarks, but is also limited to only two-dimension shift, and is not a very comprehensive new dataset with a large syn2real gap.\n\n2. While considering a good number of object detectors, the authors mainly evaluate their performance on the constructed benchmarks. Beyond the general observation that these methods are not robust under domain shifts, there is no further insightful discussion, e.g., correlation between ID and OOD performance, different generalization behaviors under different shifts, how the model architecture would affect ID and OOD performance. More importantly, there generally miss a good justification that DetectBench is a qualified benchmark that allows cross-compare different methods. Especially, it is already shown by many previous works that current object detectors fail at generalizing to OOD scenes [1] [2] [3]. \n\n3. The compared OOD generalization baselines are not adapted to the task of object detection. All the OOD generalization methods are only used for the classification branch of the object detectors. The localization branch is not considered. It is important that this new benchmark should also investigate these OOD generalization techniques developed for object detection.\n\n[1] Domain-Invariant Disentangled Network for Generalizable Object Detection\n[2] Towards Domain Generalization in Object Detection\n[3] OOD-CV: A Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images",
            "clarity,_quality,_novelty_and_reproducibility": "The paper requires major revision on writing and structuring. Especially, I find it is very hard to have a clear picture of the experiment setup of the work that greatly compromises the quality of the work.\n\nFor instance, in Table 3 and 4, there should include ID performance as the baseline. However, it is either very difficult to find or generally unclear what is the ID training set for each domain shift.\n\nIn Table 5, if the first line is the vanilla version of object detection training, it is unclear why applying OOD techniques would result in performance degradation.\n\nIn 5.2, the experiment and visualization are not clear and sufficient to support the conclusion that correlation shift is easier than diversity shift to tackle.\n\n",
            "summary_of_the_review": "This work targets an important problem, but the usefulness, novelty and effectiveness of the proposed benchmark are not convincing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3984/Reviewer_Sn2d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3984/Reviewer_Sn2d"
        ]
    },
    {
        "id": "WT7kgic_In",
        "original": null,
        "number": 3,
        "cdate": 1666691435648,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691435648,
        "tmdate": 1666691435648,
        "tddate": null,
        "forum": "7o6iMO1gkeJ",
        "replyto": "7o6iMO1gkeJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3984/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is a benchmark for out of distribution generalization in object detection. The authors create a benchmark to evaluate out of distribution generalization algorithms specifically for the task of object detection (object classification and bounding box regression), using multiple well known datasets in specific settings, while also introducing a new synthetic dataset that can be used to study correlation and diversity shift.\n\nThe contributions are:\n- A new benchmark DetectBench to evaluate OOD generalization in object detectors (authors call this OOD-OD).\n- A new dataset CtrlShift to study diversity and attribute shift in OOD-OD.\n- A good evaluation and important insights on the effect of diversity and attribute shift on OOD generalization algorithms in object detectors, signaling the object detectors are more affected by diversity shift than attribute shift. This is an important conclusion that can guide future object detector and OOD-OD method design to improve OOD generalization performance.\n- The evaluation also indicates that standard OOD generalization algorithms (designed for classification), do not transfer successfully to object detection, indicating the need for more specialized OOD generalization algorithms for this task.",
            "strength_and_weaknesses": "\nStrengths\n- The paper is very well written and it is clear, ideas flow clearly, I have no further remarks about writing.\n- An OOD generalization benchmark for object detection makes sense, as there is no dataset/benchmark specifically for this task, and the paper evaluation is of good quality, by combining standard object detectors with OOD generalization algorithms, to see if they work.\n- It is a good idea to have test OOD generalization for object detection, as the authors argue, OOD generalization algorithms are not usually evaluated for more complex tasks like object detection, they usually are only evaluated for classification datasets.\n- The proposed benchmark uses a selection of public datasets, covering multiple kinds of shifts, and additionally a synthetic dataset (CtrlShift) is proposed which has diversity and correlation shifts made in a controlled way, allowing the evaluation of the effect of both shifts combined. \n- The design of the DetectBench benchmark seems sensible, it contains multiple sub-benchmarks where train and test sets contain non-intersecting sets of conditions, like weather, scene types, time, and simulation/reality, which make a simple test set that is out of distribution with respect of the training set.\n- I believe that the evaluation and initial benchmark results presented in the paper are correct. There is a good selection of OOD generalization algorithms to test, and of object detectors (Faster R-CNN, DETR, RetinaNet) for the main results, which makes the overall conclusions robust.\n- Two major conclusions of this benchmark are that improvements to OOD generalization in classification settings do not transfer to improvements to OOD generalization in object detection, as tested on Faster R-CNN like on Table 4, and on sim2real for Faster R-CNN, DETR, and RetinaNet, and measured by mAP in the OOD setting. In some cases the performance actually decreases instead of increase, and that in the OOD setting, diversity shift seems to affect OOD generalization algorithms much more than attribute shift. These two conclusions are good contributions to the state of the art.\n- While the main results (Table 4) are presented for Faster R-CNN on the whole DetectBench benchmark, there are additional results on the sim2real sub-benchmark using Faster R-CNN, DETR, and RetinaNet about OOD generalization algorithms, producing a more robust conclusion that OOD generalization algorithms fail to improve performance in object detectors.\n- The design of CtrlShift, while simple, seems to do the job, as Figure 3 shows, there is correlation shift produced by changing the color of a car model, and for diversity shift there is simulated snow added to the environment, and both shifts can be combined to study their joint effect.\n\nWeaknesses\n- I think the only weakness in this paper is the name of the Benchmark, DetectBench is very generic, sounds like any detection benchmark, it should have OOD and generalization (or similar abbreviations) in the name, to distinguish it from OOD detection benchmarks and standard OOD generalization benchmarks. Please find a good name that includes all of this.\n\nMinor Issues\n- Figure 5 and related heatmaps would be better presented with the axis labeled in the figure itself, instead of relying on the caption. Just as Figure 3, Figure 5 should have axis labels for diversity and correlation shift. Additionally to this, Figure 5 should have the origin at the bottom left instead the top left, which would make the figure easier to interpret.\n- In Table 3, I think it would be good to also provide the original mAP of these detectors, in order to put the benchmarked numbers into context, so the reader can easily see if mAP is lower than expected or not.\n- I believe that in Table 2, simulation and reality are flipped in the sim2real description (last row, third column).\n- Could you provide a bit more information on CtrlShift? I see that there are 2000 images, but how are these distributed according to the two kinds of shifts?",
            "clarity,_quality,_novelty_and_reproducibility": "To me this paper is clear, the writing and presentation of results is good, results and conclusions are clear and supported by the results.\n\nAbout quality, I think this is a high quality paper, it has clear contributions, a clear problem and solution, and major insights that can influence the state of the art in OOD-OD and particular for object detector design. A lot of the performance improvements in object detection are only in in-distribution datasets, and this benchmark would help researchers also evaluate improvements in OOD generalization performance.\n\nI believe this paper to be novel, there is no benchmark for the OOD-OD generalization task, and as the authors mention, OOD generalization algorithms are not usually tested in object detectors and that makes them fail. This is very clear to me.\n",
            "summary_of_the_review": "I believe that this paper should be accepted. There are only minor issues, there is a clear gap in the state of the art that this paper fills, a benchmark/dataset for out of distribution generalization for object detection tasks, and the paper does an excellent initial evaluation. This paper has good insights for future research, mainly that OOD generalization algorithms designed for classification fail to improve performance in object detection, and that object detectors are more sensitive to diversity than correlation shifts.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3984/Reviewer_m6Le"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3984/Reviewer_m6Le"
        ]
    },
    {
        "id": "LtXdEiSH6lO",
        "original": null,
        "number": 4,
        "cdate": 1667591559666,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667591559666,
        "tmdate": 1667591559666,
        "tddate": null,
        "forum": "7o6iMO1gkeJ",
        "replyto": "7o6iMO1gkeJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3984/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper works as a benchmark of out-of-distribution object detection (OOD-OD), which evaluates the detector's generalization ability to a different data (open-world) domain. While many OOD methods have been proposed to amortize the domain gap, DetectBench, consisting of four datasets, shows the current OOD generalization algorithms no longer work on complex object detection tasks. This paper leads to an in-depth discussion on the progress of OOD work, and provides a foundation for OOD-OD research.",
            "strength_and_weaknesses": "[Strength]\n1. This paper works on an interesting and practical problem of OOD generalization of object detection models. The train-test discrepancy exists in many CV problems, and object detection has been well-studied and applied in research and industry. This DetectBench can work as a fundamental benchmark to study the OOD problem in object detection.\n\n2. The benchmark covers multiple conditions in train-test discrepancy: weather, scene, time, and simulation. They can fairly work together to evaluate the OOD progress\n\n3. Extensive experiments of 16 detectors and 12 OOD algorithms are measured on DetectBench. Those results are promising for the next generation of OOD-OD research.\n\n4. The author has several new assumptions from the empirical studies. For example, they claim that improvement in object detection does not always transfer to OOD object detection, and the success in OOD is also not always consistent between classification and object detection, \n\n[Weakness]\n1. The technical contribution is relatively limited, for example, a better OOD-OD method can be proposed/suggested given the benchmark result. \n\n2. Although it is sufficient to have more than ten OOD algorithms to draw the conclusion, but the re-implementation of a different problem may affect the observation. For example, using only 3 possible values for the hyper-parameter in all the methods may not be enough. \n\n3. The synthetic CtrlShift dataset is very fine-grained on a single object class with a single instance per image, therefore, it is lack of generalization. A better motivation is also supposed to explain the importance of the 'car' class in object detection, and why car color and snow level are selected for the only domain shift options. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "clarity/quality:\n- The paper is well-written and easy to follow. The tables are clear to compare different set-ups and results\n\nnovelty:\n- Good. Most of the OOD studies are on classification problems. And this work shows the OOD method is not always working on object detection tasks.\n\nreproducibility:\n- The author provides their code with a short readme file. Each experiment requires 8 V100 GPUs, which should be reproducible in the laboratory. ",
            "summary_of_the_review": "This work is a benchmark of out-of-distribution object detection. Although it has limited technique contribution, it is marginally above the threshold because the problem is novel and less studied.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3984/Reviewer_JACh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3984/Reviewer_JACh"
        ]
    }
]