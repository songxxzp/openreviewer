[
    {
        "id": "Dha0YrnBAYJ",
        "original": null,
        "number": 1,
        "cdate": 1666279659372,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666279659372,
        "tmdate": 1666279659372,
        "tddate": null,
        "forum": "p_jIy5QFB7",
        "replyto": "p_jIy5QFB7",
        "invitation": "ICLR.cc/2023/Conference/Paper4868/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method called Kcal for the (full) calibration of deep neural networks.\nKcal consists in replacing the last layer (usually a softmax) of a neural network with a learned projection layer for dimensionality reduction, followed by a Kernel Density (KDE)-based estimator to produce output probabilities. The learnable parameters introduced by Kcal are the parameters of the dimension-reducing projection layer $\\Pi$, as well the kernel bandwidth $b$. The parameters of $\\Pi$ are learned by (stochastic) gradient descent with an appropriate training strategy: contrary to classical neural networks, the KDE \u201clayer\u201d involves all training points and not only the point which is flowed through the network. The kernel bandwidth $b$ is chosen on a calibration set to minimise the log-loss.\n\nA theoretical analysis is provided in the form of high probability error bound, which shows that as the per-class sample size increases, the estimated probabilities converge to the true probability of the class $Y$ given the input score, i.e, the estimated probabilities are calibrated.\n\nExperiments are provided for 4 vision datasets (CIFAR10, CIFAR100, SVHN, ImageNet) each time with a choice of pre-trained architecture, and 3 health monitoring datasets. Kcal is then compared to 8 calibration baselines in terms of ECE (top-label and class-wise), accuracy and Brier score of the top class, and shown to provide favourable performance across all measures (or on par for ECE).",
            "strength_and_weaknesses": "**Strength**.  \nKcal targets **full calibration**, i.e., a calibration conditioned on the full probability vector rather than just conditioned on the probability of predicted class (confidence calibration). This is an important problem since as stated by the authors, small probabilities should also be reliable in high stakes applications such as healthcare.\n\nKcal provides improvements in terms of Accuracy, Class-wise ECE and Brier score across most datasets.\n\n**Weaknessess**.   \nUsing a KDE-based estimator requires an adapted training scheme, and comes with a presumably high computational cost. What is the training time of Kcal across the different datasets?  \nIt also introduces multiple hyper parameters: the kernel bandwidth, the projection architecture and output size, the batch size used to learn the projection, the number of samples used to build the KDE.  \nTherefore, it comes with both a certain computational and implementation complexity.\n\nNo insights are provided to explain the performance of Kcal compared to the other baselines. I am wondering wether it could be due to the fact that it re-uses the whole train set to fit the projection map, while other methods such as Temperature Scaling only calibrate based on the smaller calibration set.\n\n**Question**.  \nThis is a question regarding the choice of kernel bandwidth $b$. Since it can be folded in the projection map, it is implicitly learned while training the projection map. We see in appendix that across all datasets, the best kernel bandwidth equals 1. it would make sense if it has already been learned with the projection map. Hence the question: it is useful to have a bandwidth selection step on the calibration set?",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper well written, easy to follow, clear and pedagogical.\n\nKcal can be summarised as fine-tuning a DNN with a KDE as last layer instead of a softmax. I have not seen such as attempt earlier and this idea is novel to me.\n\nNote 1: To improve clarity, it could be helpful to have a diagram showing an example of train set with a batch and a background set.\n\nNote 2: There is a mistake in Definition 2 (Confidence Calibration). The argmax should be taken over the whole vector of probabilities, and the conditioning occurs on the maximum entry of $\\hat p$. \n\nNote 3: In the sentence \u201cCalibration is the process of closing the gap between the prediction and the ground truth distribution\u201c, it would be better to use the word \u201creducing\u201d instead of \u201cclosing\u201d as calibration alone, even of a perfectly accurate classifier, is not enough to remove the ground truth probabilities.\n",
            "summary_of_the_review": "I think that the clarity of the paper as well as the favourable empirical results outweight the weaknesses.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4868/Reviewer_2JZP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4868/Reviewer_2JZP"
        ]
    },
    {
        "id": "NNJa1DyLSf",
        "original": null,
        "number": 2,
        "cdate": 1666598834310,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666598834310,
        "tmdate": 1668691238188,
        "tddate": null,
        "forum": "p_jIy5QFB7",
        "replyto": "p_jIy5QFB7",
        "invitation": "ICLR.cc/2023/Conference/Paper4868/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new Kernel-based calibration method called KCal which  learns a metric space on the penultimate-layer\nlatent embedding and generates predictions using kernel density estimates on a calibration set. Theoretical and empirical results shows the effeciveness of KCal. ",
            "strength_and_weaknesses": "Strengths: \n1. A well-established method that builds a kernel-based calibration method. \n2. Theoretical proof of KCal.\n3. Experimental result analysis.\n\nWeakness:\n1. Somewhat bold claims of first theoretical guaratee/kde calibration method: The conformal predictions is well-studied with theoretical guarantee[Shafer 2008, Angelopoulos 2020].  The kernel mean embedding for uncertainty calibration [Kumar 2008, Cui 2020, Bhatt 2022]. \n2. Calibration performance not better in all cases \n3. Computation time not fully explained/demostrated. As said in the paper, computation overhead of KCal is generally larger than ordinary methods and this needs to be empirically analyzed in different scales and acceleration settings. \n\n\nReferences\n1. Shafer G, Vovk V. A Tutorial on Conformal Prediction[J]. Journal of Machine Learning Research, 2008, 9(3).\n2. Angelopoulos A, Bates S, Malik J, et al. Uncertainty sets for image classifiers using conformal prediction[J]. arXiv preprint arXiv:2009.14193, 2020.\n3. Kumar A, Sarawagi S, Jain U. Trainable calibration measures for neural networks from kernel mean embeddings[C]//International Conference on Machine Learning. PMLR, 2018: 2805-2814.\n4. Cui P, Hu W, Zhu J. Calibrated reliable regression using maximum mean discrepancy[J]. Advances in Neural Information Processing Systems, 2020, 33: 17164-17175.\n5. Bhatt D, Mani K, Bansal D, et al. f-Cal: Aleatoric uncertainty quantification for robot perception via calibrated neural regression[C]//2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022: 6533-6539.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity&quality: This paper is clearly written.\nNovelty: kernel is not new to the probability calibration but accroading to the authors, kde is firstly introduced.\nReproducibility: The code to replicate all our experimental results is submitted along with supplementary materials.",
            "summary_of_the_review": "This paper proposes KCal which is claimed as the first kde-based calibration method that brings theoretical analysis. The method itself is interesting and promising but some claims and empirical results needs improving.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4868/Reviewer_W8Nj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4868/Reviewer_W8Nj"
        ]
    },
    {
        "id": "54kNwDvs5W",
        "original": null,
        "number": 3,
        "cdate": 1666649215545,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649215545,
        "tmdate": 1666649215545,
        "tddate": null,
        "forum": "p_jIy5QFB7",
        "replyto": "p_jIy5QFB7",
        "invitation": "ICLR.cc/2023/Conference/Paper4868/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a kernel-based calibration KCal where class proabilities are determined by per-class kernel density estimates in an embedding space. The embedding space is obtained by a multi-layer perceptron applied on top of the penultimate layer of the neural network. Extensive experiments demonstrate that KCal outperforms the methods on most relevant measures and on many datasets. KCal has asymptotic guarantees and finite sample bounds.\n",
            "strength_and_weaknesses": "Strengths:\n* The paper is well-written and clear;\n* The method is novel, theoretically justified and improves the state-of-the-art;\n* The experiments have been carried out with great care in optimizing the hyperparameters for the existing methods;\n\nWeaknesses:\n* I didn't find any major weaknesses.\n\nMinor weaknesses:\n* The list of measures includes ECE, Accuracy, CECE, and Brier score. However, cross-entropy (NLL) could have been considered as well, because all proper losses decompose into refinement and calibration losses, not just the Brier score. NLL has different properties than the Brier score, and pays more attention to the probabilities near the extremes.\n* The number of instances per bin could have been shown for Figure 2 (and for similar figures in the appendix), for example as a separate figure. This would help one to visually assess roughly how high ECE would be.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, original, and of high quality.\n",
            "summary_of_the_review": "The paper is well-written and has very few minor weaknesses. The proposed method is performing better than existing methods for most evaluation measures and datasets. Training of the projection MLP is an extra step that has to be done on the validation data.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4868/Reviewer_VY8v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4868/Reviewer_VY8v"
        ]
    },
    {
        "id": "-R7zYfyN3Y",
        "original": null,
        "number": 4,
        "cdate": 1666930188711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666930188711,
        "tmdate": 1666930188711,
        "tddate": null,
        "forum": "p_jIy5QFB7",
        "replyto": "p_jIy5QFB7",
        "invitation": "ICLR.cc/2023/Conference/Paper4868/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles the question of how to generate \u201cfully calibrated\u201d predictions in multi-class classification problems. It proposes a method, \u201cKCal\u201d, which learns a low-dimensional projection of a DNN\u2019s final-hidden-layer embeddings, and then uses kernel density estimation (KDE) in this projected space to generate the fully-calibrated probability vectors.\n\nTheoretically, this paper shows that as the size of the calibration set increases (where size here is measured in terms of the number of occurrences of the rarest class), the probability vectors generated by KCal get closer to being fully calibrated, and in the limit m -> infinity, they are perfectly calibrated.\n\nEmpirically, this paper compares KCal to a large number of other calibration methods, on a variety of computer vision and health monitoring datasets. It shows that KCal generally maintains or improves the accuracy of the uncalibrated model, while generally attaining better calibration scores relative to the baseline methods.\n",
            "strength_and_weaknesses": "Strengths\n- The method is relatively simple, well-motivated, and easy to understand. Using KDE for calibration seems like a natural idea, given the connection between KDE and the Bayes classifier mentioned at the end of section 3.1 (on that note, it\u2019s surprising to me that KDE has not been proposed elsewhere to generate calibrated predictions, as is claimed at the end of section 2).\n- The method performs well empirically, across a wide range of experiments, and compared to a wide range of baselines.\n- The method is theoretically tractable, and provably leads to calibrated predictions under certain conditions/assumptions.\n\nWeaknesses\n- Although I didn\u2019t study the proofs in detail, I was curious whether there was anything in the proofs that was specific to the proposed learning algorithm. In particular, would the results hold for any feature space? Or is there something specific about the learned low-dimensional feature space that allows these theorems to hold? Are these standard results based on using KDE on a fixed feature space?\n- As far as I can tell, no ablations are performed to better understand what aspects of the algorithm are crucial for attaining good calibrated predictions. For example, what happens if KDE is applied directly on the final-hidden-layer outputs of the DNN?\n- I was also curious whether theorems 3.3 and 3.4 gave tight or loose bounds, given the real values of m, B, C, K, d, and alpha on the real tasks (can all of these be estimated?).\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is quite clear in most places. The experimental results could perhaps be explained in a bit more detail (for example, reviewing the definitions of the various calibration scores).\n- Quality: The work appears to be high-quality throughout.\n- Novelty: The method seems novel. However, I am not very up-to-date with the latest calibration methods, and thus this is hard for me to evaluate.\n- Reproducibility: Code is provided in the supplementary material.\n",
            "summary_of_the_review": "Overall, I believe the proposed KCal method is a novel and promising approach for generating more calibrated predictions for multi-class classification problems, and thus that this paper should be accepted.  However, my review is only medium confidence, given that this is not an area of research I have worked directly in.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4868/Reviewer_LaNX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4868/Reviewer_LaNX"
        ]
    }
]