[
    {
        "id": "n3pDugb2s1a",
        "original": null,
        "number": 1,
        "cdate": 1666471558179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666471558179,
        "tmdate": 1668526076332,
        "tddate": null,
        "forum": "UKr0MwZM6fL",
        "replyto": "UKr0MwZM6fL",
        "invitation": "ICLR.cc/2023/Conference/Paper3890/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extends prior work which represents compactly a large number RL policies via a linear combination of their parameters, by making the linear combination dynamic, being able to grow incrementally as the number of policies increase and more model capacity is needed. The paper provides a heuristic to control this growth, and shows that sublinear parameter increase can be achieved at a modest cost in performance over an oracle strategy.",
            "strength_and_weaknesses": "Strength:\n- The paper is well written, very detailed and thorough in its evaluation.\n- The problem setting is sound and very relevant to lifelong learning.\n- Very good experimental analysis of the transfer / forgetting properties of the approach.\n- The fact that performance on the benchmarks that the method is evaluated on is close to oracle performance is notable, and justifies the empirical decisions made even in the absence of strong theoretical justification.\n\nWeaknesses:\n- The entire premise of the paper rests on the idea that using a subspace representation for policies is a good idea in the first place. While the empirical results of that approach reported in prior works are modestly interesting, they are far from achieving either 1) orders of magnitude in complexity reduction or 2) near zero performance reduction. Being in this 'murky middle' in the first place means this general class of approaches is unlikely to be useful in any practical setting, and raises the question of whether expanding on this research direction is of significant scientific value.\nThat said, since that work has passed the bar of being presented at ICLR 2022, I'll take into account that I might hold a minority view on the value of this line of research.\n- The quality metric W(\\alpha) of the policy is entirely computed on the replay buffer data, which is also used to update the policy parameters, which raises the question: is there a risk of overfitting to past experiences, and not actually measuring effectively the quality of the policy on new experiences? It feels like the potential for overfitting in the absence of regularization or held-out evaluation is severe (?), since the ultimate goal is to be a good anchor point for future policies.\n- CSP-LINEAR is introduced but I don't see results reported for that strategy (?)\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clear. Experimental results very detailed. Good explanation of the limitations.",
            "summary_of_the_review": "Nice paper, but whose usefulness is entirely predicated on the prior work (Gaya & al) being relevant, which I don't entirely agree with but am reluctant to relitigate given prior acceptance. I will rate my review as very low confidence as a result. Scientific contributions on top of that work are incremental as well.\n\n[I had initially rated this paper a '5'. Amending to a '8' after reading other reviewer's perspective, who don't appear to share my overall sentiment, as well as thorough author rebuttals.]",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_9jow"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_9jow"
        ]
    },
    {
        "id": "F80bphTjIh",
        "original": null,
        "number": 2,
        "cdate": 1666514276334,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666514276334,
        "tmdate": 1666514276334,
        "tddate": null,
        "forum": "UKr0MwZM6fL",
        "replyto": "UKr0MwZM6fL",
        "invitation": "ICLR.cc/2023/Conference/Paper3890/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method for continual Reinforcement Learning (learning on a sequence of related tasks) using a continuous subspace of policies. The proposed approach is quite inspired by the work of Gaya et al., 2021 in that solutions to new tasks, where possible, are parametrically represented as a convex combination of previous task parameters. These previous parameters are thought of as anchors in a subspace of policies.\n\nWhen presented with a new task, the proposed algorithm attempts to learn new parameters to solve it. The subspace of policies is either expanded to include this new set or, alternatively, the task solution might be represented as a convex combination of the current subspace depending on whether the gain in performance from expanding the subspace crosses a predefined threshold \\epsilon (which seems to be about 10%). This choice of expansion v/s pruning is implemented by a clever trick using a state-action value function conditioned on convex parameter sets (Q(s, a, \\alpha)). This removes the need for  environment interactions to choose the weights \\alpha that specify how the existing subspace should be combined as parameters for the new task.\n\nResults are presented on locomotion and manipulation domains containing 35 different tasks in all and the proposed method usually performs the best across all domains considered.  A set of ablations on the tradeoff parameter \\epsilon, scalability, learning efficiency and estimation of the alpha-conditioned critic are also presented. \n",
            "strength_and_weaknesses": "Overall I think this is a good paper. The ideas are well presented, the experiments and ablations are reasonably convincing and the ablations are useful and interesting. I particularly liked the critic analysis of Figure 3 which, although only on a single task, does seem to indicate that the critic is learning quite well when conditioned on alpha. The paper also acknowledges inspiration from existing work and the related work section is, as far as I can tell, exhaustive. \n\nTo be honest, when I first read the idea I was surprised Q-learning with the added weights actually worked. The critic here Q(s,a, \\alpha) has to estimate the value of a policy that is parameterised by some weight-vector, one component of which is also changing. However the experimental results seem quite convincing that this works in practice - the performance of CSP is consistently better or similar to the next best benchmarks at a lower memory usage.\n\nAlthough I enjoyed the paper generally there are some aspects in which it could be improved which I\u2019ve listed below. \n\nFirst - I think the paper could do a better job in communicating certain relevant details. For instance the value of \\epsilon typically used for training was only mentioned in the Appendix I think and is quite pertinent when looking at the results. \n\nSimilarly the Q function update actually used to train W(\\alpha) should be mentioned somewhere since it is technically different from the one used in SAC. In fact, in general the explanation in Section 3.3 (`Grow the Subspace`) could be improved to reflect how \\alpha is being used. On first reading, it was not clear to me that \\alpha was being resampled multiple times for the same task. This was made clear by looking at the Algorithm box but it could be made more explicit in the text. \n\nFinally, as a minor point - there is a typo in Appendix B.1 - the text says \u2018two of its continuous control\u2019 but it should be \u2018three\u2019 I think.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well written. All ideas are communicated effectively. The writing is high quality and the ideas are novel. The details in the Appendix should make it clear how to implement this idea andI think the work should be reproducible in theory.",
            "summary_of_the_review": "Overall I think the paper is well written, the ideas are interesting and well tested on a number of domains with interesting ablations and analysis and I would recommend the paper be accepted at this venue.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_hkJZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_hkJZ"
        ]
    },
    {
        "id": "833YyC72PpG",
        "original": null,
        "number": 3,
        "cdate": 1666612723642,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612723642,
        "tmdate": 1666612723642,
        "tddate": null,
        "forum": "UKr0MwZM6fL",
        "replyto": "UKr0MwZM6fL",
        "invitation": "ICLR.cc/2023/Conference/Paper3890/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a method of constructing a space of policies that is beneficial for future tasks. \n",
            "strength_and_weaknesses": "Strengths:\n- novel and appealing method\n- overall quality of the paper\n\nWeaknesses:\n- in some cases, the performance of method is very close to PackNet\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written overall, although I failed to understand some details in some cases. The presented method is new and hopefully brings long-term benefits to CRL.",
            "summary_of_the_review": "The paper presents the method that maintains a growing set of neural networks that span a space of useful policies. Importantly the set is grown adaptively only if a new network would bring significant benefits. In this way, a sublinear number can cover all the tasks.\n\nThe major benefit, in my view, of the method is its flexibility, it can adjust the size depending on the 'capacity demand'. A concern is that the empirical results, although nice, do not clearly indicate that the presented method has clear advantage. \n\nI have a number of questions:\n- Can you explain how the training is stable when $\\alpha$ changes in line 3. It'd imagine that each $\\alpha$ has a much different optimal $\\theta_{j+1}$ and thus some kind of jittering arises.\n- In line 9 of Alg 1 is it $W_\\phi$ which is trained of $Q$?\n- How $Q$ function is handled? Is it reset every task? Do you keep one $Q$ for all $\\alpha$?\n- Do you have any expectations (or experiments) of how PackNet would perform if given $2x$ bigger network for the Brax scenarios? Namely, at the moment, PackNet uses $2x$ parameters, while CSP is $5.3$.\n- It'd be nice to see a histogram of the number of used networks.\n- What is the comparison of the number of parameters used in the CW10 experiments? Say, how many parameters are used for CSP and PackNet.\n- I'd be great to see how the method scales with more tasks. I know it is costly, but at least one longer sequence would be great.\n- As far as I understand, a very similar method could be applied in supervised continual. Could you please comment shortly on what you expect?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_X3Cb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_X3Cb"
        ]
    },
    {
        "id": "-HK0ty_RNUT",
        "original": null,
        "number": 4,
        "cdate": 1666701223138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701223138,
        "tmdate": 1668526420107,
        "tddate": null,
        "forum": "UKr0MwZM6fL",
        "replyto": "UKr0MwZM6fL",
        "invitation": "ICLR.cc/2023/Conference/Paper3890/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes an approach to continual reinforcement learning, where agents need to acquire a variety of skills rather than just solve a single task. The approach they propose is to learn a convex space in policy parameter space (with a mechanism to extend the subspace with a new anchor if necessary to achieve good performance on a task). In this way the number of anchors grows sublinearly with the numbers of tasks. Then, policies within this subspace can be defined by a small weight vector indicating how to combine the anchor points for the task. They test this approach on a locomotion and manipulation continual learning benchmarks where it performs well, particularly considering the resource costs.",
            "strength_and_weaknesses": "Strengths:\n- The paper is well written and explains the method well.\n\n- The empirical results are showing a convincing improvement, particularly over classic approaches like elastic weight consolidation.\n\n- It's a conceptual simple approach that can probably be adapted to other algorithms (i.e. the specifics of how you learn the policy are not crucial to the method).\n\nWeaknesses:\n- The novelty is not clear. In particular, Gaya et al. (2021) [appropriately cited in the paper] have previously demonstrated the idea of using anchor points to define a subspace in parameter space for rapid online adaption. Although that is distinct from continual learning, its not a big conceptual leap.\n\n- The CLEAR baseline is not included because \"storing data from all prior tasks ... is unfeasible due to prohibitive memory costs.\" It would be helpful to include this baseline as a comparison (even though it is \"unfair\") and also to quantify the actual amount of memory necessary for these tasks. In practice, large memories are surprisingly cheap, and one could also consider store to slower access hard drives as well. I suspect store everything and train a large neural network is a surprisingly strong and practical baseline.\n\nMinor:\n  The notation for the replay buffer (Algorithm 1) is quite unusual $\\mathcal{B}uf$ and looks like multiple terms. I would suggest just using $\\mathcal{B}$.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Most of this was discussed in strengths / weaknesses.\n\nClarity:\n- The paper is well written and explains the method well.\n\nQuality: The paper is supports its claims well, with experiments and detailed discussion of the method.\n\nNovelty: Discussed as a weakness above.\n\nReproducibility [updated]. The authors have made the code available so I believe it should be reproducible (but I didn't personally verify).",
            "summary_of_the_review": "This work adapts the idea of using anchor points to define a space space in the parameter space of policies from previously used for online adaptation to the task of continual learning, including a mechanism for adding new anchor points when it results in a large enough improvement. It's well written and supports its claim. However, it is not that novel, and conceptually only a small adaptation from earlier work. Additionally, the code is not currently available. For this reason, I'm ambivalent about acceptance.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_dMX5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_dMX5"
        ]
    },
    {
        "id": "9ByOAxyRjd",
        "original": null,
        "number": 5,
        "cdate": 1668495939638,
        "mdate": 1668495939638,
        "ddate": null,
        "tcdate": 1668495939638,
        "tmdate": 1668495939638,
        "tddate": null,
        "forum": "UKr0MwZM6fL",
        "replyto": "UKr0MwZM6fL",
        "invitation": "ICLR.cc/2023/Conference/Paper3890/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an approach for continual learning using a subspace of policies (collection of \u2018anchor\u2019 parameters). Given a new task, the control policy either uses a linear interpolation of existing anchor parameters, or introduces a new anchor, and then linearly interpolates in the full set. This causes the number of stored parameters to grow sub-linearly with the number of tasks. The authors present results on locomotion environments (half-cheetah, ant, humanoid) with variations, and the continual world benchmark with robot-manipulation environments. \n",
            "strength_and_weaknesses": "\nStrengths - \n\nSignificance of proposed Idea - \nThe problem considered  in the paper, that of continual learning for agents where they need to mitigate forgetting while also avoiding storing all data they see (due to memory constraints), is critical important for lifelong robot learning systems that keep learning different tasks. This paper uses ideas from mode connectivity (using a set of parameters and trying to generalize from the subspace they induce) and applies them to continual learning. This seems a promising direction for further exploration and the community is likely to benefit from it, since this hasn\u2019t been studied before. \n\nThoroughness of evaluation - \nThe authors include multiple continual learning experiments to support their claims, and compare sufficiently to prior work. Across domains it appears that the proposed method gets best (or close to best) performance while also being more memory efficient, thus addressing the critical requirements for continual learning. \n\nWeaknesses - \n\nTasks used for analysis \nThe paper includes quite a bit of analysis done on the halfCheetah environments (Fig 3- smoothness and critic accuracy, diversity and compositionally, Fig 2c - Learning Efficiency). The multiple tasks for this domain however only include parametric variations (eg - changing mass/gravity/friction etc, from Table 7). This is further away from the eventual setting where we would like to deploy continual learning, where the robot needs to learn over semantically different tasks. Related to this I\u2019d be interested how much the subspace approach helps when the tasks are actually different (eg - pickup hammer, close box etc from continual world), but still share some structure/skills like reaching/picking/grasping/placing. While the paper does evaluate on continual world and report that it comes close to the current sota quantitatively, more analysis on this domain could strengthen the claims of the paper. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, and clearly written. Using the mode connectivity idea for continual learning seems novel as described above. The authors also release code so it should be reproducible. \n",
            "summary_of_the_review": "The paper proposes an interesting idea for continual learning which hasn\u2019t previously been studied for RL problems, and provides sufficient evidence/evaluation through their experiments. The claims can be made stronger by using the continual world envs instead of the halfCheetah env for some of the analysis \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_ferp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3890/Reviewer_ferp"
        ]
    }
]