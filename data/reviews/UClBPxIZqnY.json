[
    {
        "id": "0-jcju2SgW",
        "original": null,
        "number": 1,
        "cdate": 1665731758536,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665731758536,
        "tmdate": 1668758928922,
        "tddate": null,
        "forum": "UClBPxIZqnY",
        "replyto": "UClBPxIZqnY",
        "invitation": "ICLR.cc/2023/Conference/Paper1464/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper suggest the use of Generalized Dynamic Time Warping as a way to get differentiable alignments.\nIt then showcases the utility of this method on experiments on which the loss to be optimized depends directly on the obtained path.",
            "strength_and_weaknesses": "* Strengths\n    * Using the path information from DTW alignments in gradient-based optimization is an interesting track of research\n    * Relying on the Generalized DTW formulation to do so makes a lot of sense\n* Weaknesses\n    * The major weakness in this paper is that it misses an important related work [1] that also relies on integrating the path information (from SoftDTW this time) for gradient-based optimization with a DTW-derived loss.\n    \n        1. Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models. Le Guen & Thome, NeurIPS 2019\n\n    * Another weakness of the paper is that the presentation of the experiments is too brief, which makes it hard to get what each competing method does in practice.\n\nIn more details:\n\n* At several places, authors suggest that when using SoftDTW, one cannot differentiate through the path information, which is not true (it is discussed in the original SoftDTW paper and done in [1]). Below are a few occurrences of this claim in the paper:\n    > DTWNet and SoftDTW only allow the DTW discrepancy between two time series to be differentiable wrt its inputs (Legend of Fig 1)\n\n    > This enables for the first time the use of the alignment path itself in a downstream loss (page 2)\n\n    * Comparison to [1] is of prime importance given that the computation for DecDTW is said to be \"between 4-20 times slower than SoftDTW\"\n* The description of the experiments is sometimes hard to follow. As an example, I do not understand Table 1. More specifically, I do not understand what the subparts (left and right) of the Table are, since we have a first subpart dedicated to \"Classic DTW\" in which there is still a DecDTW entry and similarly a second subpart named \"GDTW\" in which there are DTWNet and SoftDTW entries.\nI guess there is something like one can use a different alignment algorithm to train the network and predict a path, but if this is the explanation, things should be made much clearer in the text in my opinion.\n* Here are other remarks related to the experiments:\n    > Interestingly, fine-tuning using DecDTW dramatically reduces independent retrieval performance (p. 9)\n       * This would be worth a discussion\n    * Also, I do not get why \"DTW (Base)\", \"Fine-tune Along GT\" and \"Fine-tune OTAM\" reach the exact same level of performance here. Is there a rational explanation for this fact?\n* Finally, this is a minor remark, but I feel that, when presenting GDTW, authors should not use sentences such as:\n    > This more general description of the DTW problem\n\n    > The dynamic time warping problem can be formulated as follows (page 3)\n\n    since the GDTW problem is not the DTW problem (yet it is an interesting formulation anyway)",
            "clarity,_quality,_novelty_and_reproducibility": "The description of the method is rather clear.\nThe idea to use the Generalized DTW formulation in order to differentiate through the resulting path is novel, but an important related work is missing.\nIn terms of reproducibility, for reasons I listed above, I would have a hard time reimplementing the experiments and I believe their presentation should be re-worked.",
            "summary_of_the_review": "Overall, this paper introduces interesting ideas, but the lack of comparison to an important baseline (SoftDTW with regularization on the path [1]) makes it impossible to assess whether the proposed approach outperforms the said baseline.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1464/Reviewer_yA9i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1464/Reviewer_yA9i"
        ]
    },
    {
        "id": "ZV2iVh50aJi",
        "original": null,
        "number": 2,
        "cdate": 1666594086498,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594086498,
        "tmdate": 1666594086498,
        "tddate": null,
        "forum": "UClBPxIZqnY",
        "replyto": "UClBPxIZqnY",
        "invitation": "ICLR.cc/2023/Conference/Paper1464/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel DTW layer that combines non-linear programing based generalized DTW (Deriso & Boyd) with declarative network (Gould et al.), to not only output the DTW discrepancy value, but also the alignment. More importantly, the output alignment could be compared against ground truth alignment and the difference could be backpropogated to train the network. Experiments on audio-to-score alignment and image query datasets, both show improvement over SOTA methods. Scalability is also discussed in the end.",
            "strength_and_weaknesses": "Pros:\n1. The idea is novel that is very different from existing differentiable DTW methods (e.g. DTWnet or soft-DTW). This is the first attempt of using declarative network in DTW problem.\n2. The experiments are conducted on some interesting and very practical tasks, the audio-to-score and visual query.\n3. The paper is clearly written and easy to follow. Additional details are provided in appendix.\n\nCons:\n1. The motivation of applying declarative network is vague. If just for incorporating alignment path error, simple modifications can be done on DTWNet or SoftDTW to obtain the DTW path as a byproduct, and adding the path into the loss is achievable by defining a proper distance between hypothesis path and gold alignment.\n2. Lack of comparison with other methods that regularize the alignment paths, e.g. Graphical Time Warping (GTW) and following literatures on this method.\n3. The method is not scalable, 2 orders of magnitude slower than other methods. This significantly limit the usage of the method. This is due to the complexity in declarative network.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and novel to the community. Code and data are provided for reproduction.",
            "summary_of_the_review": "Overall, the method is novel but lack some motivation behind it. The applications are interesting to the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1464/Reviewer_Z1je"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1464/Reviewer_Z1je"
        ]
    },
    {
        "id": "WM-ZZZll3-",
        "original": null,
        "number": 3,
        "cdate": 1666677106634,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677106634,
        "tmdate": 1668658170284,
        "tddate": null,
        "forum": "UClBPxIZqnY",
        "replyto": "UClBPxIZqnY",
        "invitation": "ICLR.cc/2023/Conference/Paper1464/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "**Update 11-16-22** I acknowledge the revisions made by the authors. I believe the proposed revisions further improve this already-strong submission. I am holding my score at 8 Accept and am willing to further champion this paper's acceptance if needed.\n\nThis paper proposes an algorithm for improving temporal alignments between related pairs of time series data, applicable in applications like audio-to-score alignment in music information retrieval. Unlike past efforts, the proposed algorithm can leverage ground truth alignment information during training if available, leading to improved performance on several real-world alignment tasks.",
            "strength_and_weaknesses": "A primary strength of this paper is the combination of the theoretical elegance of the proposed method alongside its impressive performance on real-world alignment tasks. Additionally, this paper is well-written and contains precise notation. One weakness is that, while the proposed method is continuous and can be combined w/ neural networks, it does not address the problem of jointly learning to align and _transcribe_, unlike other methods like CTC which can learn alignments without designing an explicit cost function between input and output sequences (e.g., in speech recognition).",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThis paper is refreshingly well-written overall. In spite of this, I found it a bit difficult to follow some important aspects of the core argument. Some examples / suggestions:\n\n- I was searching early on for the connection between the proposed method and well-known algorithms like CTC which also learn alignments between time series pairs. It took me quite a while to understand the high-level distinctions between CTC and the proposed DecDTW (namely, that CTC jointly learns to align and transcribe and does not require a cost function, while DTW learns to align and requires a cost function). It would be helpful for readers to clarify these distinctions in the related work section.\n- I was initially thinking that all of the methods being proposed / compared would output an _alignment_ rather than _features_. This wasn\u2019t made clear to me until Table 6 (and even then it took me quite some time to understand). The source of my confusion was likely Figure 1 where \u201cFeature extractor\u201d is an _input_ to DecDTW; perhaps the authors could mention earlier on that the proposed methods can all act as sort of \u201csecondary feature extractors\u201d, and that these features can be compared using both classic DTW and GDTW (as in Table 6)? Additionally, the meaning of \u201cfeatures\u201d in 3.1 Preliminaries could stand to be clarified.\n- I was caught off guard by the strong performance of GDTW compared to Classic DTW on _all_ features in Table 6 - perhaps the authors could set this expectation (namely, that GDTW >> Classic DTW) earlier on in related work?\n- Section 3.2 could remind readers that we\u2019re talking about GDTW (I had forgotten after spending time scrutinizing the preliminaries. E.g., \u201cGDTW formulates DTW as a constrained continuous optimisation problem\u2026\u201d\n- In Section 4, could the authors clarify at a high level why we are now returning to a discretized formulation after spending all that effort instantiating a continuous formulation?\n\nLow-level:\n- (Figure 3) \u201ccan applied\u201d -> \u201ccan be applied\u201d\n- (Section 4) \u201cIt is easy to see\u201d pet peeve - this is _not_ easy for everyone to see (myself included)\n\n**Quality**\n\nI often find there is an unfortunate tradeoff in ML papers between the elegance / descriptive clarity of the methods and the application of those methods to real-world empirical settings. I was pleasantly surprised to find both in this paper. The experiments look at two very different but realistic alignment settings, and show strong performance in both cases.\n\n**Novelty**\n\nThis paper draws heavily on Deriso & Boyd 2019, but has substantial novelty in its combination w/ Gould et al. 2021 to embed the GDTW framework into neural networks. One novel aspect w.r.t. other work in alignment is the ability to integrate ground-truth alignments during training. The actual utility of this is a bit unclear to me, e.g., in the case of music alignments, the ground-truth alignments are only available here due to unusual circumstances (the Disklavier yields alignments for MAESTRO and synthesis yields alignments for Kernscores). I would have loved to see the authors attempt to tackle the problem of jointly learning to align and transcribe, though I cannot fault them for putting this out of scope. \n\n**Reproducibility**: In contrast to much of deep learning literature, it appears to be possible to reproduce this work from the information present in the paper alone. As an added bonus, the authors include code.\n",
            "summary_of_the_review": "Overall, I think this paper presents a well-motivated method w/ strong empirical results on an important task. The scope of the exploration is somewhat limited to aligning (as opposed to _transcribing_ as in tasks like speech recognition), but this is an important task for which there are certainly useful real-world applications. Overall, I think the ICLR community will benefit from this paper\u2019s inclusion and the ensuing discussion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1464/Reviewer_EEJt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1464/Reviewer_EEJt"
        ]
    },
    {
        "id": "JVoE9Y-cy9",
        "original": null,
        "number": 4,
        "cdate": 1667342724122,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667342724122,
        "tmdate": 1667345234731,
        "tddate": null,
        "forum": "UClBPxIZqnY",
        "replyto": "UClBPxIZqnY",
        "invitation": "ICLR.cc/2023/Conference/Paper1464/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel differentiable dynamic time-warping algorithm. The approach is advantageous over existing variants as it outputs a learnable warping path between the time series representations.\n\nThe authors test their approach on two distinct tasks: audio-score alignment and visual place recognition, and achieve results comparable to or better than the existing state-of-the-art.",
            "strength_and_weaknesses": "Strengths:\n\n- Directly outputs the alignment path\n- Tested on two completely different domains\n- Typically better results on both tasks compared to existing DTW variants\n\nWeaknesses:\n\n- Not compared to anything else but DTW-variants\n- The experimental setup, in particular, the train/validation/test divisions are not explained in detail. See the specific comment below.\n- Nit: The figures aren't colour-blind friendly\n- For camera ready, adding a companion with audio and video examples could be good.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The narrative is straightforward, and the tasks are described in detail, which would benefit readers who aren't from music information retrieval or robotics backgrounds. \n\nThe formulations and results are explained reasonably well. \n\nThe code is relatively easy to read and cross-check with the declarative DTW formulation. The experiments are structured. As a nitpick, the code could be organized better (e.g. by modularizing classes), but it's already more than enough for a research paper.\n",
            "summary_of_the_review": "The authors improve over DTW, a commonly used tool for sequential alignment tasks. Their formulation directly outputs the alignment path, which could be further used to improve the alignment. The paper is well written with minor corrections or clarifications required. \n\nGiven the above, I would like to recommend this paper to be presented at the conference.\n\nBelow I will list some minor/specific comments:\n\n- Problem formulation & methodology: The text reads a bit like Thickstun is the first person to formalise audio-score alignment. I understand the authors refer to the wording in the cited paper. On the other hand, they also cite much earlier work (e.g. Ewert et al.) which has made similar definitions. It might be good to re-word; e.g. stating the pepper adopts the framework described in Thickstun et al.\n- I think the setup needs more elaboration for both experiments. In both experiments, the authors use a subset of the data available, but they do not explain why. Moreover, in the audio-score alignment experiments, it is not explained if the scores could be present in multiple training/validation/test sets, potentially causing leakage.\n- A7: It would also be good to include worse alignment results and discuss why such failures occur.\n- Future work: it would be very interesting to extend the work on \"subsequence alignment with jumps,\" see JumpDTW.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1464/Reviewer_UgzC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1464/Reviewer_UgzC"
        ]
    }
]