[
    {
        "id": "yLF2v_SVhC",
        "original": null,
        "number": 1,
        "cdate": 1666263457919,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666263457919,
        "tmdate": 1666263457919,
        "tddate": null,
        "forum": "MFD2b2cwr5d",
        "replyto": "MFD2b2cwr5d",
        "invitation": "ICLR.cc/2023/Conference/Paper3033/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a regularization method for mitigating artifacts. Their method (SimReg+) is simple: take a model that is known to be artifact-free, and train another model to be similar to that model. The idea is somewhat counter-intuitive, but works reasonably well on some datasets, and is also shown to have fewer internal biases. The authors also propose several variants of the main idea (e.g., decreasing the similarity between the model and some model that is known to be biased), but those do not work as well as the main idea. ",
            "strength_and_weaknesses": "\nStrengths:\n- A simple approach for mitigating artifacts\n- Results for SimReg+ are generally better than most baselines\n- The second-order similarity measure seems interesting\n- The internal bias representation results are interesting\n\nWeaknesses: \n- The proposed approach is somewhat counter-intuitive: why would making a model imitate a weak model (i.e., one trained on less data) result in a stronger model? This is sort of like doing knowledge distillation where the teacher is the smaller model. I see that empirically the idea sort-of works, although sometimes it does much worse (e.g., HANS non-entailment, PAWS) and in some cases the differences are small (Table 2). I wonder how generalizable this approach is.\n- Related to the previous point - for some reason the Guidance baseline is missing from the Deberta experiments (Table 12 doesn't show any baselines except the original model).   \n- Given that SimReg+ works best among all proposed methods, it might make sense to focus on it, and present the rest as ablations\n\nOther questions:\n- It is not clear why the authors chose HANS- as the X axis in Figure 3. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear. The approach is closely tied to existing methods such as distillation (see above), though I haven't seen it applied for this purpose. \n\nTypos and such: \nSection 3.3: the the \n",
            "summary_of_the_review": "The paper presents a simple method that seems to overall work well. However, it is not entirely clear how generalizable it is, given that it is not very intuitive and the results are sometimes mixed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3033/Reviewer_UAbR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3033/Reviewer_UAbR"
        ]
    },
    {
        "id": "0WHsZ3hdQoA",
        "original": null,
        "number": 2,
        "cdate": 1666517108434,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666517108434,
        "tmdate": 1666572483803,
        "tddate": null,
        "forum": "MFD2b2cwr5d",
        "replyto": "MFD2b2cwr5d",
        "invitation": "ICLR.cc/2023/Conference/Paper3033/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes SimReg, a method that achieves the goal of debiasing by increasing(decreasing) the similarity between the final model and an unbiased(biased) model. They explore two different ways to impose the regularization: on model representation or on the gradients. \nExcept for similarity-based regularization, another key component of this paper is the creation of an unbiased dataset, which is critical for regularization's success. \nBy manipulating internal model representations, SimReg differs from previous methods that downweigh the importance of training samples during training. \nUpon extensive evaluation, SimReg is effective in improving models' OOD performance.\n",
            "strength_and_weaknesses": "Strength:\n\n1. The problem is well-motivated and the paper is easy to follow. \n2. The method is simple yet effective.\n3. The experiments are carried over several different datasets that cover different types of bias, and the results are convincing. \n\nWeakness:\n1.  One limitation of SimReg is that exclude highly-confident examples using a biased model (e.g., tinyBERT) might also filter out some \"easy\" or \"unbiased\" examples, because neural models often get over-confident. \n2. Some related works/baselines are missing. SimReg share many similarity with knowledge distillation (eq 3 is basically a variant of the typical KD loss). So some short discussion about KD would improve the paper and KD loss might be be used as another model variant. Although \"intrinsic debiasing\"(manipulating representations) methods are novel in the area of spurious correlation mitigation, they have been widely used in debiasing hate speech. Despite the differences in terms of areas, they might also need to be included in the related works. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is technically solid and clearly presented. ",
            "summary_of_the_review": "The submission proposes a novel method that is effective in mitigating spurious correlation.  But there are some limitations and missing related works that have to be addressed before acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3033/Reviewer_2WEz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3033/Reviewer_2WEz"
        ]
    },
    {
        "id": "qgwLjaHxKSF",
        "original": null,
        "number": 3,
        "cdate": 1666553962132,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666553962132,
        "tmdate": 1666553962132,
        "tddate": null,
        "forum": "MFD2b2cwr5d",
        "replyto": "MFD2b2cwr5d",
        "invitation": "ICLR.cc/2023/Conference/Paper3033/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a method to address the spurious correlations / bias problem in the context of NLU. Starting from the observation in Mendelson & Belinkov (2021) that most de-biasing methods consisting in re-weighting data result in actually more biased representations, they explicitly consider to regularize the representations (or the gradients) of the full model to be more similar to a de-biased \"guidance\" model (trained on small amounts of de-biased data found by either an oracle or by a biased model), in order to train de-biased representations. The method is tested on NLI, Fact checking and paragraph identification.\n",
            "strength_and_weaknesses": "Strengths:\n- *The motivation of the paper is sound*: Recent approaches (such as Deep Feature Reweighting https://arxiv.org/pdf/2204.02937.pdf) show that de-bias models just modify the last output layer, thus re-weighting robust features. It is natural to ask how to make data representations intrinsically less biased.\n- *The paper is well-written*: I found the paper easy to read and self-contained, most of the details are present either in the main text or the appendix.\n\nWeaknesses:\n- *Lack of enough empirical support*:\nThe main hypothesis of this paper is that the guidance model representations are less biased and thus can inform the de-biasing methods into learning representations invariant to the bias features. Right now, I don't feel that this hypothesis is substantiated enough by the experiments. I would first expect convincing evidence supporting the fact that the bias is less extractable from the guidance model. An analysis of bias extractability only comes in Section 6.1, Figure 3, but it is only applied to the full regularized model and not to the \"guidance\" model. I am equally perplexed by Figure 3: the model that *minimizes* the similarity w.r.t the guidance model seem to have representations that are *less* biased than the model that *maximizes* the similarity w.r.t. the guidance model, which seems against the thesis of the paper? I think that most of the paper should be focused on proving that the representations of the final de-biased model are indeed less biased on several datasets: right now this analysis is confined to one dataset (HANS). It would good to include in this study the simple method RWG where the loss is re-weighted by the size of each group.\n\n- *Results are not strong enough / missing baselines*:\nOverall, the results obtained by the proposed de-biased model are not significantly better than other baselines. This would not be such an issue if the paper was focused on training de-biased representations without caring so much for improved performance (although it would leave to the authors to justify why having less biased representations would ultimately be useful). For example, if we compare the results of the paper with https://arxiv.org/pdf/1911.03861.pdf, we see lower performance on HANS and PAWS (unknown bias mitigation setting). It would also be suitable to include results for \"Correct-n-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations\": this method also operates on the representation space, by aligning representations between groups, thus it has an inherent similarity with the proposed approach.\n\n- *Manual hyper-parameter search*:\nI am a bit uncertain what is the rationale by which the authors chose their hyper-parameters. In Section 3.2, the authors mention \"choosing the threshold c_t is performed manually by plotting the confidence of the bias model over the training set\". I am not sure what is the rationale for choosing the threshold c_t, and definitely think that an automatic method to do so should be described and analyzed.\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Questions and concern\n1. An additional concern is that the guidance model is trained on less data and therefore its representations / gradients might not be of good quality, hindering generalization of the base model. This point might be discussed in the paper.\n2. Some of the recent de-biasing baselines would be needed in Table 1 (even RWG would be fine)\n3. The following references are missing:\n- \"Increasing Robustness to Spurious Correlations using Forgettable Examples\", https://arxiv.org/pdf/1911.03861.pdf\n- \"Deep Feature Re-weighting\", https://arxiv.org/pdf/2204.02937.pdf\n- \"Correct-n-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations\", https://arxiv.org/pdf/2203.01517.pdf\n",
            "summary_of_the_review": "This paper focuses on an important problem: how to estimate less biased representations rather than just creating de-biased models by re-weighting features in a biased representation. In its current form, I don't feel confident enough to state that the paper advances on its goals. Moreover, performance is on par - or lower - than standard re-weighting approaches. More empirical evidence to support the validity of the method is needed.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3033/Reviewer_qJJe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3033/Reviewer_qJJe"
        ]
    },
    {
        "id": "lgrTe4MOHkJ",
        "original": null,
        "number": 4,
        "cdate": 1666833846333,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666833846333,
        "tmdate": 1666833846333,
        "tddate": null,
        "forum": "MFD2b2cwr5d",
        "replyto": "MFD2b2cwr5d",
        "invitation": "ICLR.cc/2023/Conference/Paper3033/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method for debiasing internal model components, based on similarity-regularization. This is meant to encourage the internal representations to be either similar to representations of an _unbiased_ model, or dissimilar from representations of a _biased_ model. The authors apply this regularization to either the model representations or its gradients. The paper evaluates the approach on three tasks---natural language inference, fact checking, and paraphrase identification---and demonstrates that it improves performance on one out-of-distribution dataset for each task while incurring little degradation in in-distribution performance.",
            "strength_and_weaknesses": "- The paper is very confusing. Exposition lacks structure, claims are made without providing evidence to back them up, and there are some hand wavy arguments in this paper. Here are several examples of these issues:\n  - \"While improving out of distribution (OOD) performance, it was recently observed that the internal representations of the presumably debiased models are actually more, rather than less biased\": Biased to what? Bias is a statistical term that's in reference to something specific. \n  - The paper relies on having a _biased_ model and an _unbiased_ model, and what those mean doesn't become clear until Section 3.2\n  - It is not clear to me how you train a biased model for the unknown biases. You mention you use TinyBert but that's it.\n  - You make a point in Section 3.2 that excluding samples on which a _biased_ model is correct and confident would result in an unbiased dataset. That makes the assumptions that (i) instances where your biased model is correct and confident are actually instances which contain confounding which is not always true (see [1], but even other than that, they could just be correct and confident for the right reason), and (ii) your biased model captured all instances that contain confounding. There's nothing stated as to why these assumptions might be valid.\n  - \"Choosing the threshold $c_t$ is performed manually by plotting the confidence of the bias model over the training set.\" Yet I can't tell how Figure 3 tells you that $c_t$ has to be 0.8 and not say 0.7 or 0.9 or something.\n  - \"Appendix A.4) shows that regularizing only $D^B$ results in better OOD performance, supporting our intuition.\" Did you use same hyperparameters when just regularizing $D^B$ versus when regularizing all examples or different? How were they chosen?\n  - \"We repeat some of the experiments using DeBERTa to verify our model is not specific to BERT.\" How did you select which experiments would be repeated and which not?\n  - The results tables show means and error terms but its not clear: (i) are these standard error terms or standard deviation?; (ii) what is the source of randomness?; (iii) are differences in performance statistically significant?\n- The paper claims that the proposed method leads to better OOD performance. Better performance on one OOD dataset does not imply better OOD performance. This is like testing the model on one example and claiming 100% accuracy if it correctly classifies that example. But the issue with that is that if the second example is incorrectly classified, it's accuracy would suddenly be 50%. Perhaps this model did better on one OOD dataset, to show that it will likely have better OOD performance overall, you need to evaluate on a battery of OOD datasets. A sound claim would be that it leads to better performance on the specific datasets (which is good).\n- In related work, a whole line of work on debiasing methods is missing. Some relevant papers below:\n  - Victor Veitch, Alexander D'Amour, Steve Yadlowsky, and Jacob Eisenstein. \"Counterfactual Invariance to Spurious Correlations: Why and How to Pass Stress Tests.\" arXiv preprint arXiv:2106.00545 (2021).\n  - Zhao Wang and Aron Culotta. \"Robustness to spurious correlations in text classification via automatically generated counterfactuals.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 16, pp. 14024-14031. 2021.\n  - Divyansh Kaushik, Amrith Setlur, Eduard H. Hovy, and Zachary Chase Lipton. \"Explaining the Efficacy of Counterfactually Augmented Data.\" In International Conference on Learning Representations. 2021.\n  - Zhao Wang and Aron Culotta. \"Identifying Spurious Correlations for Robust Text Classification.\" In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 3431-3440. 2020.\n  - Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. \"Learning The Difference That Makes A Difference With Counterfactually-Augmented Data.\" In International Conference on Learning Representations. 2020.\n\n- Errata:\n  - incorrect quotation usage in Line 5 of introduction.\n  - Third to last line of introduction: \"This is different from previous methods\" ---> \"This is different from some previous methods\" (refer to the citations above that do indeed talk about this)\n  - Section 3.1: \"In the case of decreaseing\" ---> \"In the case of decreasing\"\n  - Section 4.1.1, second paragraph: \"As OOD test sets, we use HANS\" ---> \"As OOD test set, we use HANS\"\n  - Section 4.1.3: Thorne et al. should be \\citep and not \\citet\n  - Section 4.1.4, second last line: \"bias.For\" ---> \"bias. For\"\n  - Section 5.2, second last line: \"support out hypothesis\" ---> \"support our hypothesis\"\n\n[1] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. \"Understanding deep learning (still) requires rethinking generalization.\" Communications of the ACM 64, no. 3 (2021): 107-115.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is confusing, the experimental setup is halfway there and additional evaluations are needed. The novelty appears limited and it is unclear what the results tables are reporting. The authors have shared implementation details in Appendix that should help with reproducibility so that is not much of a concern to me right now.",
            "summary_of_the_review": "The paper is very confusing and lacks structure. Statements are often made without providing evidence to back them up. The paper relies on several assumptions but there is nothing stated as to why these assumptions might be valid. It is not clear what the results table shows (mean and standard deviation? mean and standard error?) but it's not clear what the source of randomness is (mean over what, different samples of training data or different seeds of a model or both?) or if the differences in performance are statistically significant. The paper claims that the proposed method leads to better OOD performance, but better performance on one OOD dataset does not imply better OOD performance overall. In related work, a whole line of work on debiasing methods is missing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3033/Reviewer_73u6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3033/Reviewer_73u6"
        ]
    }
]