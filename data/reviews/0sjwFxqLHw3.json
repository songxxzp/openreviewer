[
    {
        "id": "YOIRp2IKXc",
        "original": null,
        "number": 1,
        "cdate": 1666535853249,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535853249,
        "tmdate": 1666535853249,
        "tddate": null,
        "forum": "0sjwFxqLHw3",
        "replyto": "0sjwFxqLHw3",
        "invitation": "ICLR.cc/2023/Conference/Paper501/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper theoretically proved the existence of spurious local minima in deep convolutional neural networks by construction. Specifically, for a convolutional neural network whose last three layers before the final fully-connected layer are all convolutional layers, the authors constructed a spurious local minimum for square loss or cross-entropy loss under some non-degeneracy assumptions for the input data and model weights. The construction and proof in this paper mainly follow the framework in (He et al., 2020) which deals with fully-connected layers, and the authors solved two main technical challenges (limited receptive field, and parameter sharing) to extend their proof to the convolutional case. Experiments of a 7-conv-layer CNN on CIFAR-10 are also provided to validate the theoretical results.",
            "strength_and_weaknesses": "-Strengths:\n\n1. This paper showed the existence of spurious local minima in deep convolutional neural networks, and this existence is mainly due to the use of convolutional layers. This indicates that convolutional layers can introduce spurious local minima themselves when equipped in a fully-connected neural network.\n\n2. The network structure used for the theoretical proof is realistic and the ReLU activation pattern of the spurious local minima is non-trivial. The model structure can include fully-connected layers, convolutional layers, and max/average-pooling layers, which match the main ingredients of the convolutional neural networks used in practice.\n\n3. This paper introduces new techniques to extend the framework in (He et al., 2020) to convolutional neural networks. Specifically, Lemma 2/Lemma 5 extends the output split to the convolutional case by solving a combinatorial problem, and Lemma 3/Lemma 4 uses a different way (3 consecutive convolutional layers) to perform local perturbation.\n\n-Weaknesses:\n\n1. The new proof techniques used in this paper seem to be specific about the setting in this paper and could not be easily generalized to solve other problems. The framework for constructing local minima mainly follows that of (He et al., 2020), and the additional technical lemmas seem to only work for the specific setting of convolutional layers. This might limit the technical significance of this paper.\n\n2. It appears somewhat unclear what is the main message from the main theorems in this paper. Since the construction of spurious local minima of convolutional layers seems harder than fully-connected ones, does that mean convolutional layers are less likely to introduce spurious local minima? It would be better if the authors could include more implications and discussions of their theoretical results in this paper.\n\n3. The notation in this paper is heavy and somewhat hard to follow, and it might be better if the authors could use more intuitive explanations/figures and fewer formulas to improve the understanding of the readers. Moreover, the organization of the formulas and lemmas can be further improved. For example, The margin around equations (9) and (10) seems too large. Section 3.2 could also be re-arranged to a place closer to Lemma 2 to help with the proof sketch.",
            "clarity,_quality,_novelty_and_reproducibility": "-Clarity: This paper is generally well-structured. However, the proof sketch might be a bit long with too many notations and minor structural problems, so it might need some improvements such as the ones suggested in the \"Weaknesses\" section above.\n\n-Quality and Reproducibility: The theoretical results in this paper appear to be correct, and the experimental details are provided in the appendix, making it possible to reproduce the experimental results.\n\n-Novelty: To the best of my knowledge, the techniques introduced in this paper to extend the previous framework to convolutional layers are novel.",
            "summary_of_the_review": "I tend to lean towards rejecting this paper. Although this paper introduces new techniques to construct spurious local minima for convolutional neural networks, these techniques might not be easily generalized to other settings, and the main message of the results might be somewhat unclear. Besides, the clarity of this paper also needs improvement.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper501/Reviewer_wvat"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper501/Reviewer_wvat"
        ]
    },
    {
        "id": "rnqTOx2svkf",
        "original": null,
        "number": 2,
        "cdate": 1666682499972,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682499972,
        "tmdate": 1666687449876,
        "tddate": null,
        "forum": "0sjwFxqLHw3",
        "replyto": "0sjwFxqLHw3",
        "invitation": "ICLR.cc/2023/Conference/Paper501/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper constructs bad local minima for convolutional neural networks (CNN) with ReLU activation. Specifically, the paper establishes the following results:\n\n1) If a neural network (termed \"subnetwork\") has a local minimum, then a network concatenating the subnetwork and 2 convolutional layers has a local minimum.\n2) There exists another point with a smaller training loss than the above local minimum. Thus, the above local minimum is not a global minimum.\n3) Construct a concrete example of the subnetwork, so as to validate the existence of a bad local minimum.\n\nThis is the first paper that addresses the existence of bad local minima for CNN with ReLU activation.",
            "strength_and_weaknesses": "Studying the loss landscape, especially the existence of bad local minima is important for understanding neural network training. Most existing results are limited to fully-connected networks, hence an attempt to expand our understanding to CNN is valuable.\n\nThe proof of this paper consists of two major steps: 1) the construction of a local minimum, and 2) proof of the constructed local minimum to be spurious. However, in the first step (Lemma 1), the authors seem to intrinsically consider a fully-connected network. Below are detailed comments.\n\n- The authors study the matrix-vector product formulation (2) for CNN. Due to weight sharing, some entries in matrix W and bias b represent the same network weights. That is, during the optimization, some entries in W and b should be kept identical. \n\n- However, in the construction of Lemma 1, entries of $W$ and $b$ seem to be freely designed. If this is the case, it reduces to the fully-connected networks, bypassing the most challenging issue of CNN. Otherwise, the authors should justify how the shared weight constraints are satisfied. For example, the form of W in (2) does not allow assigning nonzero values to all diagonal entries. Then, how is the construction of $W^{L-1}$ in Lemma 1 possible? \n\nLemma 1 (construction of local minimum) serves as a foundation of this paper. If the problem of Lemma 1 can not be addressed properly, the paper has not really proven the existence of spurious local minima for CNN.\n\nOther comments:\n\n- Lemma 1 requires the matrix product $W^L I^{L-1,i} W^{L-1}\\cdots W^1$ to be full-rank. This may not be possible since some of the rows in $W^{L-2}$ (and some of the columns in $W^{L}$) are set to 0. Are there any dimension constraints (on $n_{L-1}, n_{L-2}, n_{L-3}$) missing in Lemma 1?\n\n- In the last sentence of page 3, $i^*_p$ may not be able to keep constant in its neighborhood if two or more features are equal.\n\n- Assumption 1 needs further justification. If neural collapse happens, the inputs to the final layer may be identical for different samples in the same class.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see above.",
            "summary_of_the_review": "Extend an existing technique to construct a spurious local minimum for CNN, which is the first attempt in this field. However, the construction of the local minimum is not sufficiently justified to be a CNN point.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper501/Reviewer_JdM7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper501/Reviewer_JdM7"
        ]
    },
    {
        "id": "9FfdIEQOwU",
        "original": null,
        "number": 3,
        "cdate": 1666717626386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717626386,
        "tmdate": 1666717626386,
        "tddate": null,
        "forum": "0sjwFxqLHw3",
        "replyto": "0sjwFxqLHw3",
        "invitation": "ICLR.cc/2023/Conference/Paper501/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper theoretically investigates spurious local minima (i.e., local minima that are not globally optimal) that arise in the empirical risk of deep convolutional neural networks.\n\n- By embedding a local minimum of a subnetwork to a larger network, the paper constructs a local minimum $\\theta$ of the larger network (Lemma 1). Then, it is shown that a different point $\\theta'$ can be perturbed in a way that the empirical risk at $\\theta'$ is strictly smaller than $\\theta$ (Lemmas 2, 3, and 4). This proves that the local minimum $\\theta$ is spurious (Theorem 1).\n\n- Theorem 1 relies on the existence of a local minimum of a subnetwork. Theorem 2 provides a construction of a local minimum that is \"nontrivial\"---in the sense that different ReLU activation patterns exist for different data points, unlike existing results.\n",
            "strength_and_weaknesses": "- Existence/absence of bad local minima is an important topic in the literature, and it is true that the investigation has mostly focused on fully-connected networks. This paper is one of the first attempts to characterize spurious local minima that arise from convolutional networks, and the authors seem to have overcome several technical barriers that arise in CNNs.\n\n- However, while I got the main idea, reading the paper left me with some questions, mostly related to Theorem 1 and its proof. If all the issues are clarified I would be happy to raise my score.\n\n- I'm confused about the relationship between the number of neurons $n_l = T_l P_{l-1}$ and the number of patches $P_l$. As one can see in Eq (1) and (2), the output $o^l \\in \\mathbb R^{n_l}$ is plugged in as the input for the next layer. Considering that we apply zero-padding and stride is equal to 1, it seems to me that the number of patches at layer $l$ would be the same as $P_l = n_l$. However, it seems that the paper is assuming $P_l = P_{l-1}$ by zero-padding (e.g., see $P_{L-2} = P_{L-3} = P_{L-4}$ right above Lemma 1). To me this looks like a contradiction unless we have $T_l = 1$. Can you clarify?\n\n- In Lemma 1, we make portions of $W^{L-1}$ and $W^{L-2}$ equal to \"identity\" matrices by filling in the diagonal entries with 1. However, I wonder whether this is always possible? For example, if we look at Eq (2), there are certain entries that are *forced to be zero* by the structure of the weight matrix that arises from convolution. Also, it is mentioned that the conclusion of Lemma 1 still holds if the top layers are pooling layers. How? Take average pooling for example. One does not have the freedom to choose $W^{L-1}$ and $W^{L-2}$ arbitrarily; the matrices are just fixed, based on the number of patches and filter length.\n\n- Lemma 1 requires that $\\hat W^L \\hat I^{L-1,i} \\cdots \\hat W^1$ is full-rank for all $i$. While this may look reasonable for networks of general size, for certain cases I think this condition can become a big restriction. For example, consider the case where all $\\hat W^l$ are square matrices. Then necessarily all the diagonal matrices $\\hat I^{l,i}$ must set their diagonal entry to one in order to satisfy the full-rank condition. This means that all ReLUs must be turned on for all datasets, which boils down to a linear model.\n\n- For the local minimum $\\theta$ constructed by Lemma 1, the rest of the proof assumes that the local minimum has $R (\\theta) > 0$. How can you safely assume that? The conclusion that the local minimum $\\theta$ is spurious becomes vacuous if this assumption $R (\\theta) > 0$ actually does not hold for any local minimum. In other words, the theorem does not *disprove* the possibility that all the local minima are globally optimal because it relies on this assumption. Would there be a way to show the existence of $\\theta$ that always satisfies $R (\\theta) > 0$? Maybe by using the construction idea from Theorem 2?\n\n- Why should Eq (10) necessarily hold? If $\\eta$ is chosen based on $h$ with the biggest split threshold, why should the sum in Eq (10) suddenly become zero for all $j \\neq h$?\n\n- In Theorem 2, it is assumed that for any target $y_{i^*}$ there exists $\\tilde y_{i^*}$ such that the loss $l(\\tilde y_{i^*}, y_{i^*}) = 0$. What happens for losses for which such $\\tilde y_{i^*}$ does not exist, e.g., cross-entropy loss? Also, in Theorem 2, is $R (\\theta) > 0$ always satisfied?\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is heavy in notation and in many cases it uses multiple equivalent symbols to denote the same thing. For example, the output of the network for input $x_i$ is denoted by four symbols: $o_i, o(x_i), o^L(x_i), o^{L,i}$. I'm not sure if this improves readability; rather, it is likely to hurt readability.\n\n- It is good to be explicit about the construction, but sometimes too many details can hurt. At first glance, Lemma 3 looked just overwhelming to me. Perhaps it would be better to avoid spelling out the explicit details in the statement?\n\n- Other than the aforementioned issues, the paper is not too difficult to follow.\n\n- As mentioned above, the paper seems to have developed novel techniques to overcome the difficulties that arise in the analysis of CNNs. Unfortunately, I didn't have a chance to check the details of the proofs though.",
            "summary_of_the_review": "This paper develops some novel techniques to show the existence of bad local minima in the empirical risk of CNNs. If fully correct, this looks like a good contribution to the literature. However, some issues (mostly with proof and notation) are raised and I set the recommendation to weak reject for now. If the authors clarify these issues through the rebuttal period, I would be happy to raise my score.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper501/Reviewer_h5wB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper501/Reviewer_h5wB"
        ]
    },
    {
        "id": "XuGBrgGKSX",
        "original": null,
        "number": 4,
        "cdate": 1667237234017,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667237234017,
        "tmdate": 1667237234017,
        "tddate": null,
        "forum": "0sjwFxqLHw3",
        "replyto": "0sjwFxqLHw3",
        "invitation": "ICLR.cc/2023/Conference/Paper501/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThe authors study the loss landscape of multilayer convolutional neural\nnetworks, and in particular consider the construction of spurious minimizers of\nnetworks trained on regression/classification tasks. They consider networks\nwith fully connected, convolutional, max-pooling, and average pooling layers,\nas well as ReLU nonlinearities. Their main results (1) construct spurious\nminimizers in a network assuming that a subnetwork construction is satisfied\nand a certain nondegeneracy assumption holds at the local minimizer obtained\nfrom this subnetwork construction; (2) show in a specific setting (slightly\nnonstandard architecture) that a subnetwork can explicitly be constructed to\ngive a local minimizer. \n",
            "strength_and_weaknesses": "## Strengths\n\nThe authors argue that their approach to constructing local minima is modular,\nin that the task is reduced to studying a local minimum of a subnetwork (which\ncan then be embedded into larger networks). This may be useful for obtaining\ngeneral conclusions. They mention on page 5 that this is new in the context of\nCNNs (standard for feedforward nets).\n\nThe analysis seems highly technical and intricate, involving detailed\nindex-level manipulations of the filters in the network. (This also makes it\nchallenging to verify). The result considers fairly general CNN architectures\nand various loss functions, including regression and classification losses. \n\n## Weaknesses\n\nThe claims of novelty in the introduction (end of first paragraph, end of third\nparagraph) are not correct: the related work section even discusses a work of\nDu et al. on one-layer CNNs that shows spurious minima (and see similar works\nnot cited, such as https://arxiv.org/abs/1909.03172). This is not hard to fix:\nyou can claim the first result on spurious minimizers in deep CNNs / CNNs with\nseveral channels, etc. I would also recommend an additional literature search\nfor possible related references in this context, since the one I mention here\nwas missed.\n\nClaim about argmax at bottom of page 3 and top of page 4: I do not think this\nis correct in general -- consider the case where there are two distinct indices\n$i$ which achieve the argmax corresponding to different filters, and suppose\nthe coordinates $o_p^{l-1}(i)$ are open mappings (is there any reason for them\nnot to be?). Then every neighborhood of the parameters $W, b$ contains points\nat which the argmax is taken uniquely on a distinct coordinate involved in the\ntie; it implies that there is no locally-valid way to define a *constant* matrix\nthat selects the right element of the max-pooling operation. The assumption that the \ncoordinate maps are open is only sufficient; it should be possible to construct\ncounterexamples under more general conditions specific to the definition of the\nfeature maps.\n\nThe result relies on a technical assumption (Assumption 1) that seems to\nsignificantly limit the scope of the theory (or even make it vacuous).\nAssumption 1 is a nondegeneracy assumption on the network features at a local\nminimizer (this is used to prove that the minimizer is spurious). As a result\nof the dependence on this assumption, it does not seem that the theory makes\nany predictions about the existence or non-existence of spurious minimizers in\nany actual network architectures. Based on the way Section 4 and Theorem 2-3\nare written, I understand that even in the authors' construction of a concrete\nlocal minimum, it is still necessary to make assumption 1 in order to conclude\nthat the local minimum is spurious. In the rebuttal, I would appreciate if the\nauthors could enlighten me on whether assumptions like this are standard in\nother works that study spurious minimizers in neural networks (say, in the\nwell-studied feedforward setting). From my perspective, I would find it hard to\nunderstand why the natural extension from the results mentioned in the related\nwork section by Du et al. and the one I mentioned above for concrete spurious\nminimizers non-overlapping-receptive-field and one-layer Convnets would not be\nsomething for single-layer and overlapping receptive field networks, or\nmulti-layer and non-overlapping receptive field networks, rather than the\nextremely general setting the authors consider here (given that there are no\nconcrete conclusions in this setting in the presented work). \n\n## Minor / Questions\n\nBottom of page 1 and elsewhere: \"perturbated\" -> \"perturbed\" (\"perturbate\" does\nnot sound correct)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very technical and written in a nonintuitive way -- for example,\nwhen the authors describe their results and techniques in the introduction of\nthe paper, it reads like a direct translation of some of the proof techniques\ninto natural language rather than as an elucidation of the ideas behind the\nproof (why such a result is true, how one might arrive at it, etc.). This makes\nthe effort required to penetrate the paper for a nonspecialist unduly high.\n\nIn general (following comments in the previous section), the preliminaries\nsection 2 is written in an unclear and disorganized style that is overly\ngeneral and unnecessarily confusing.  For example, the definition of the max\npooling operation (3) is not clearly made with respect to its interaction with\ndistinct filters $T_{\\ell}$ in a layer; the use of a matrix $W^{\\ell}$ for all\noperations does not accurately show the distinct structures of each operation\n(average pool; convolution; fully connected) as functions of the actual weight\nparameters of the network and architecture parameters, and hence makes it seem\nunnecessary to define such intricate notation here (just put it in an appendix\ninstead, if it is not essential to understanding the main body of the paper?);\nthe discussion of feature map padding is done vaguely in an unhelpful way (one\nwants to know here **what restrictions does this place on the various\nparameters of the network**? In (2), the input feature map dimension is 5 and\nthe output dimension is 6. If I pad the input by 1, now the input dimension is\n6 and the output dimension is 8. It does not seem to be possible to pad the\ninput feature map to make the input and output sizes equal!). I would recommend\nthe authors polish the writing and only introduce notation that will be\nnecessary to discuss the main results that are presented in the subsequent\nsections of the main paper.\n\n\nThe technical presentation of the argument reads as though it is poorly\norganized, as well. Lemma 1 is a technical result that seems rather abstruse,\nand it also applies to a very limited setting (only Conv + ReLU layers); the\nlemma has unnatural technical assumptions on nondegeneracy, and after\npresenting it the authors then state that \"the conclusion in Lemma 1 still\nholds\" for more general settings without any proof or connection to a result in\nthe appendices. Why not present this result informally, with the necessary\nlinks to the fully general versions in the appendices made? The second\nparagraph of section 3.1 says \"for ease of presentation\", but this seems to be\noff to me -- the result presented is still very technical, and the\nsimplifications seem to rather have the effect of making it unclear what\nnetworks the authors' result actually applies to, rather than the stated\nintention. The same could be suggested for the following material that leads to\nTheorem 1, although here the technicality may be essential.\n\n",
            "summary_of_the_review": "\nThe strength of the assumptions made in the theory seems to significantly limit\nits scope. As a result, although the work is technically impressive, it does\nnot seem to shed much novel light on when and why spurious minimizers exist in\nconvolutional neural networks trained on various practical tasks. The notation\nand writing could do with additional polishing.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper501/Reviewer_L56V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper501/Reviewer_L56V"
        ]
    }
]