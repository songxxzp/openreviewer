[
    {
        "id": "Y6kX-8Q9frp",
        "original": null,
        "number": 1,
        "cdate": 1666342335633,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666342335633,
        "tmdate": 1666342335633,
        "tddate": null,
        "forum": "gfWNItGOES6",
        "replyto": "gfWNItGOES6",
        "invitation": "ICLR.cc/2023/Conference/Paper1454/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper adds support for boundary conditions to FNO\u2019s, which are neural operators that learn the solution map of unknown PDE systems.\n",
            "strength_and_weaknesses": "\ns: Principled method.\n\ns: The results are convincing. The method achieves perfect behavior at boundaries (as expected), and also shows moderate interim accuracy improvement. \n\nw: This paper is incremental since it does an \u201cFNO + BC\u201d type model. The paper doesn\u2019t motivate or demonstrate why adding BCs is important. I\u2019m not sure if this is that important feature, and the author\u2019s should more clearly argue why it\u2019s important to include boundary conditions in FNOs.\n\nw: There are clarity issues. The algorithms 1-3 are the key contribution of this paper, and yet they are gibberish to me with no explanations. I can\u2019t really follow the main idea of how the kernels are corrected.\n\n\nTechnical comments\n* Since PDEs are not linear in reality, how come it makes sense to still use the kernel form eq 1? I don\u2019t think the \u201cnote\u201d explains this\n* What does L^2 mean in eq 2? a(y) lives in \\A space, so how do we define a product between L^2 and \\A instances?\n* What does s and p denote?\n* In eq 1 G is a function, in eq 3 it's an operator. Is this intentional?\n* G only applies to values at boundary, by definition. In eq 3 we apply G to solutions of any points a \\in A. The G is thus mostly undefined in eq 3.\n* The U_bdy is an intersection of space and its boundary. Isn\u2019t that then trivially just the boundary? Is this a typo?\n* What about the boundary of A, why is that not considered (around eq 3 and fig2)\n* How come in \\D we only have pairs from A and U_bdy. Why can\u2019t we have points from boundary here? Is it intentional that this now restricts A implicitly (or does it)?\n* || is L2 error at boundary, yet most points inside the sum in eq 3 are not at the boundary. I don\u2019t understand. Do you then skip some of the points inside the sum?\n* If we have boundary condition \\alpha=0, doesn\u2019t that lead to division-by-zero in prop1?\n* I would expect a common boundary condition is that we have a time-invariant fixed value at boundary. This would then lead to a Dirichlet kernel that is simply an indicator function. Surely this is not a useful kernel and does nothing. What would we do then?\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity moderate. Quality good. Novelty good. Reproducibility good.\n",
            "summary_of_the_review": "This is a solid paper that succesfully includes proper boundary behavior to FNO models. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1454/Reviewer_2TdD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1454/Reviewer_2TdD"
        ]
    },
    {
        "id": "4ZwYvjIdYH8",
        "original": null,
        "number": 2,
        "cdate": 1666519833771,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666519833771,
        "tmdate": 1666519833771,
        "tddate": null,
        "forum": "gfWNItGOES6",
        "replyto": "gfWNItGOES6",
        "invitation": "ICLR.cc/2023/Conference/Paper1454/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper uses the neural operator framework with kernel to learn solutions to PDE (see e.g. Neural Fourier Operator work, FNO) as a base method. The paper add modifications to the discretised kernel to achieve better predictions on the boundaries on Dirichlet, Neumann, Periodic boundary conditions.",
            "strength_and_weaknesses": "The kernel manipulation process shown in appendix or \"Algorithm\" table in page 5 is simple with low computational complexity. Yielding better numerical results.\n\nWhen the kernel changes (w.r.t. say FNO), the solutions at the internal regions also changes. Authors give a theorem to bound the new solution with respect to the solution without kernel manipulation.\n\nI may have missed it, it seems no bounds for the solutions with respect to the true solution is given.\n\nPaper is evaluated for 1D and 2D systems. Good if evaluation can also be done on 3D systems which is more useful in real world applications.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clearly written. Lots of details in the appendix, which makes reproducing this paper easier.",
            "summary_of_the_review": "Generally a good paper, I have not checked through all equations. \n\nMinor points: \n1. In Eq.(1), would be more complete if the authors define the mapping of u.\n2. Is there a typo when the author define the mapping of kernel right before Eq. (2)?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1454/Reviewer_ESqy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1454/Reviewer_ESqy"
        ]
    },
    {
        "id": "D5i9RXzF4er",
        "original": null,
        "number": 3,
        "cdate": 1667400686949,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667400686949,
        "tmdate": 1669210497853,
        "tddate": null,
        "forum": "gfWNItGOES6",
        "replyto": "gfWNItGOES6",
        "invitation": "ICLR.cc/2023/Conference/Paper1454/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes the additional use of boundary conditions for Neural Operator networks.\nThe boundary conditions are explicitly enforced in the parameterized operator kernel, such that they are in principle fulfilled \"by construction\".\nDirichlet, Neumann, and Periodic boundary conditions are considered.\n\nFor experiments, the proposed method is applied to Fourier Neural Operators (FNO) and compared to state-of-the-art operator learning architectures.\nIn multiple numerical experiments for PDEs in 1D and 2D (+ Time Dimension) with different boundary conditions,\na significant decrease in relative L2 error is demonstrated compared to vanilla FNO and other architectures.",
            "strength_and_weaknesses": "Strengths:\n1. The proposed kernel correction results in a significant performance improvement in settings where the boundary conditions are known. This is exemplified with a wide range of extensive experiments.\n\n2. The kernel corrections are to my understanding generally applicable to Neural Operator architectures, not just FNOs. In the appendix, an experiment with application to the Multi-Wavelet based Neural Operator is showcased.\n\n3. Code is provided, and it seems to be well documented and maintained.\n\n\nWeaknesses:\n1. There is a lack of discussion on how the setting differs from other work on Neural Operators (e.g. FNO), i.e. what assumptions are made and what limitations the proposed kernel refinement has.\nFor example: Is this hard-coded approach for boundary conditions still applicable if we consider settings of the same PDE with differing boundary conditions?\n\n2. There are severe clarity issues. E.g. the algorithms for the kernel corrections are provided as pseudo-code, but there is no proper explanation of the underlying idea. One of the reasons is that a \"kernel module\" $\\mathcal{K}$ is suddenly used, which is hand-wavely mentioned in the paper but never properly defined.\n\nGeneral:\n\nI strongly recommend the authors to restructure the paper.\nInstead of the general and vague introduction of Neural Operators and the kernel correction, I'd suggest concretely introducing one architecture (e.g. FNO), and how the kernel correction can be applied to it.\nHighlighting the generality of the approach can still be done afterwards, but right now the lack of a concrete example is confusing.\nAlthough the extensive experiments are of course valuable, they could partially be moved to the appendix, offering some space for section 3.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- Section 2.2: Neural Operators Architectures are barely introduced and the introduction is generally very high-level, leaving out how the integral operators are actually implemented (e.g. in FNO). \n\n- The notation is at times very confusing, and definitions of important parts are scattered around. \nE.g. $\\mathcal{G}$ is introduced as a function in the BVP setting (1), although for the whole paper $\\mathcal{G}$ is assumed to be a linear operator. \nOr for understanding the meaning of $\\mathcal{U}_{bdy} = \\mathcal{U}\\cup \\delta \\mathcal{U}$ one has to search for the definition of $\\delta \\mathcal{U}$ in Table 1.\n\n- Section 3.2 does not really explain the kernel correction. The explanation for the discretized setting is instead provided in the Appendix, where the correction can be formulated as matrix multiplications. In the main text only the more general algorithm is provided, but without any explanations, leaving the reader confused.  \n\n- Figure 2 is a bit confusing. Shouldn't both arrows originate from the same point in $\\mathcal{A}$, but with different trajectories as only the operator changes? Also, why is here the operator applied to the error( $T(\\mathcal{E}_\\text{bdy}) $  )? \nShouldn't it be $\\mathcal{E}_\\text{bdy}(T)$ so that it is consistent with equation (3)? \n\nQuality:\nThe experimental settings appear to be sound, as well as the motivation for correcting the kernels.\nExperiments however lack repeated runs.\nI did not go through the proofs in the appendix in detail.\n\nNovelty:\nThe paper proposes a general way to hard-code boundary conditions into Neural Operator networks, which has to the best of my knowledge not been done before.\n\n\nReproducibility:\n- Code is provided \n- Experiments appear to be from individual seeds, and lack uncertainty estimates (e.g. standard deviations).",
            "summary_of_the_review": "The paper provides a strong performance improvement to Neural Operator networks in settings where boundary conditions are available.\nThe novelty lies mainly in additionally considering boundary constraints in existing Neural Operator Architectures.\n\nHowever, due to the lack of clarity, I vote against accepting this submission.\nThis is mainly due to the lack of explanation for algorithms 1 to 3 and in general section 3.2.\n\nI'd like to clarify, that the method is a valuable contribution, but would greatly benefit from another iteration.\n\n\n***\n## Edit after Author's Response\nThe authors resolved with their response a few issues, namely:\n- Notation was clarified with additional info provided in the Appendix\n- Appendix C.2 was added, which explains the origin of the previously hard-to-understand Algorithms 1-3\n- The issue of repeated experiments was resolved, by providing information about very little variation in the experiment results.\n- A discussion of the special case for enforcing a boundary value to be zero was added to Appendix B., showing that in principle a solution with any given (but non-zero) precision is still possible.\n\nWith that, most of my major concerns have been resolved and I will raise my score.\nI still have my doubts regarding the structure of the paper and the usefulness of Algorithms 1-3 in the main part, while all of the actual explanations happen in the Appendix.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1454/Reviewer_Z7Rp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1454/Reviewer_Z7Rp"
        ]
    }
]