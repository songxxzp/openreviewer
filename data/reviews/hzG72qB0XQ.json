[
    {
        "id": "Mb61ix2YgM",
        "original": null,
        "number": 1,
        "cdate": 1665975264872,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665975264872,
        "tmdate": 1669963368344,
        "tddate": null,
        "forum": "hzG72qB0XQ",
        "replyto": "hzG72qB0XQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4631/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces Lipschitz constraints into the transformer for higher adversarial robustness. The author proposes the first One-Lipschitz Self-Attention (OLSA) mechanism for Transformer models. The author orthogonalizes all linear operations in the self-attention mechanism and introduces Lipschitz constraints. The proposed method also provides certified guarantees. Experiments demonstrate the effectiveness of this method.",
            "strength_and_weaknesses": "Strength:\n1. This paper proposes the first One-Lipschitz Self-Attention mechanism (OLSA) and proves the Lipschitz bound.\n2. Because the prediction gap is only calculated on a forward pass in the OLSA model, the robustness radius can be certified in much less time.\n\nWeaknesses:\n1. Certified defense is usually divided into bound propagation and randomized smoothing (RS). The paper lacks a comparison with the RS method.\n2. The method is not evaluated on TectFooler, BERT-Attack and TextBugger.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of this paper is clear and has certain theoretical contributions.",
            "summary_of_the_review": "We tend to accept this paper because the method is effective and the presentation is clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4631/Reviewer_XTXc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4631/Reviewer_XTXc"
        ]
    },
    {
        "id": "7uLHGLH5VX3",
        "original": null,
        "number": 2,
        "cdate": 1666576757671,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576757671,
        "tmdate": 1666576757671,
        "tddate": null,
        "forum": "hzG72qB0XQ",
        "replyto": "hzG72qB0XQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4631/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a new multi-head self-attention variant -- OLSA. The proposed method can satisfy the 1-Lipschitz constraint under certain conditions. Thus, the author further proposes a Lipschitz constrained Transformer model. The model achieves a strong certified radius and robust accuracy while maintaining a reasonable vanilla accuracy.",
            "strength_and_weaknesses": "Strength:\n1. The proposed method can achieve a strong certified radius and it is straightforward and easy to implement.\n2. The paper is well-written and easy to follow. The comparison with related works is comprehensive.\n3. Experiments show the good robustness of the proposed model.\n\nWeakness:\n1. The so-called fine-tuning experiment is not a standard fine-tuning setting used by modern works.\n2. The OLSA-based transformer model doesn't have layer norm and dropout. I suspect that's why the author is using a transformer with only 3 layers. In order to make this work has a broader impact, it's would be great the find an adequate substitute for these two modules and train a transformer model with more layers.\n3. The paper didn't explain how the method could be applied to natural language generation tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "Although limited by several constraints, the proposed method seems sufficiently novel to me. The paper is easy to follow and the experiment results are enough to show that the model can achieve better robustness under these constraints.",
            "summary_of_the_review": "Overall the paper proposed an interesting new variant of transformer that can easily satisfy the 1-Lipschitz constraint. The idea of building a robust transformer model could potentially have a big impact on many downstream tasks. However, the current method still has many limitations, especially since it's not compatible with some important components of SOTA models and its scalability is still unclear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4631/Reviewer_noPi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4631/Reviewer_noPi"
        ]
    },
    {
        "id": "keOM6GRn3q8",
        "original": null,
        "number": 3,
        "cdate": 1666632259711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632259711,
        "tmdate": 1668808217171,
        "tddate": null,
        "forum": "hzG72qB0XQ",
        "replyto": "hzG72qB0XQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4631/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes OLSA, an algorithm to train transformers for 1-Lipschitzness in the l2 norm, which is directly linked to robustness to l2 perturbations. The authors show that OLSA attains stronger verified robustness than relevant previous work, at a minimal cost in standard accuracy.",
            "strength_and_weaknesses": "OLTA achieves 1-Lipschitzness via a number of modifications to the standard training process. The authors enforce orthogonality in the self-attention layers (in their additive form), remove dropout and LayerNorm, and employ the GroupSort activation function. While each of the ideas was previously presented in the 1-Lipschitzness literature, this is the first application to training 1-Lipschitz transformers.\nIn spite of the modifications, the authors show that their method achieves larger certified robustness than the relevant previous work from Shi et al. 2020, with a minor cost in terms of standard accuracy. This is definitely a big strength of the work.\n\nNevertheless, I have the following comments/questions on the experimental section:\n- the authors should compare against the bounds from (Bonaert et al. 2021)[https://files.sri.inf.ethz.ch/website/papers/pldi21-transformers.pdf], which were shown to be more effective than those from Shi et al. 2020;\n- the speedup against previous certification methods is repeatedly stated. However, bounding algorithms do not necessarily require an ad-hoc training process, which might be more costly. Could the authors provide information on the training runtimes? What is the overhead of running the Newton's iteration?\n- it would be nice to present an ablation study of the non-standard components of the training process on standard accuracy. In other words, how does the use of GroupSort, the lack of dropout or LayerNorm, and the use of additive attention?\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well-written and easy to follow, except a couple of typos. Some bibliographic entries miss the publication year.\nWhile many of the paper's building blocks already come from the relevant literature (e.g., orthogonality for 1-Lipschitzness), the work is original as it is the first example of 1-Lipschitz transformers (to the best of my knowledge).\nThe authors provide code in the supplementary material for reproducibility (though I have not run it).",
            "summary_of_the_review": "OLTA achieves state-of-the-art deterministic certified accuracy on transformers at a minimal cost in terms of standard accuracy. Nevertheless, the paper could benefit from a more recent baseline (see above), reporting and commenting on training runtimes, and an ablation study.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4631/Reviewer_QwhB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4631/Reviewer_QwhB"
        ]
    },
    {
        "id": "BFVE7uFr3K",
        "original": null,
        "number": 4,
        "cdate": 1666665058180,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665058180,
        "tmdate": 1666665058180,
        "tddate": null,
        "forum": "hzG72qB0XQ",
        "replyto": "hzG72qB0XQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4631/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to use Lipschitz constraints to train certifiable robust transformers. With some modifications to the Transformers models, they bound the Lipschitz constant for each layer, which is related to the robustness of the model. Compared to previous work, their proposed model, named one-Lipschitz self-attention mechanism (OLSA) can achieve tighter bound (therefore larger robustness radius) as well as being more efficient, which is empirically verified on both training-from-scratch and fine-tuning scenarios.",
            "strength_and_weaknesses": "Strengths:\n\n1. OLSA has advantages over its baseline, as it has a tighter Lipschitz bound (compared to Kim et al. (2021)) and requires only the forward pass (compared to Shi et al. (2020)). With those improvements, it achieves a higher robustness radius and gets faster than Shi et al. (2020), which is verified on two datasets in both training-from-scratch and fine-tuning scenarios.\n2. The modification OLSA makes to the Transformer model makes it easier to bound their 1-Lipschitz constant during training, which is well-designed and works well both theoretically and empirically.\n\nWeaknesses:\n\n1. While the changes on Transformers are necessary for the OLSA to achieve robustness, certain features in Transformers must be abandoned. E.g. only additive attention can be used, while most of the pre-trained transformers rely on the dot-product attention mechanism. LayerNorm is removed, which is important when many layers of transformers are stacked. Those changes might be acceptable for 3-layer transformers, but can be dangerous for larger models.\n2. Certain steps to bound the Lipschitz constant have dependencies on the sequence length, such as Theorem 4.1 and 4.2, which means the actual Lipschitz bound would differ for different lengths of the input sequences. \n\nTypo: In the last paragraph before the \"technical contributions\", the paper says OLSA Transformer achieves a radius of 0.071 ... previous work ... 0.368. Should it be 0.368 and 0.071?\n\nThe paper seems to use both n and N symbols to represent the sequence length. E.g. in Theorem 4.1 it uses \"n\" and the remark below 4.1 uses \"N\". Can you make them consistent? As a reference, N in Shi et al. represents the number of layers.\n\nQuestions\n\n1. I have a general question for this line of work: How is it practically useful to model defense? Lipschitz bound guarantees that the model is robust to slight changes in the embedding space, which is not naturally common in attacks. If one or two words are changed/added/deleted, the embeddings will be significantly changed, then how is Lipschitz helpful?\n2. Why do you put the comparison with Kim et al. (2021) in the appendix, and not show their efficiency?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear about its methods, theorems, and corresponding proofs. The paper is well-written, and the statements about the comparison between OLSA and previous work are helpful to distinguish their contributions. However, certain details in the model training might make this work not easy to reproduce. If the authors can release the source codes and training scripts, it would be most helpful.",
            "summary_of_the_review": "The model proposes a new method that guarantees the Lipschitz bound of the Transformer with a smaller robustness radius and efficient certifications. However, certain modifications to the Transformers are necessary under their settings, which could be harmful to its usefulness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4631/Reviewer_UH8A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4631/Reviewer_UH8A"
        ]
    }
]