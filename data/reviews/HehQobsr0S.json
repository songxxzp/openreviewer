[
    {
        "id": "UFUgMCx8tU",
        "original": null,
        "number": 1,
        "cdate": 1666636886190,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636886190,
        "tmdate": 1666636886190,
        "tddate": null,
        "forum": "HehQobsr0S",
        "replyto": "HehQobsr0S",
        "invitation": "ICLR.cc/2023/Conference/Paper3101/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a novel labeling scheme for extractive text summarization. Extractive summarization datasets are most often derived from abstractive datasets using a greedy labeling scheme, providing a single extractive reference per document. Greedy multi-hot labels cause sparsity/under-fitting issues so the authors propose a soft-labeling scheme and show it is equivalent to incorporating multiple hypotheses in a non-autoregressive labeling model. The soft-labeling oracle is implemented using beam search. The authors then show improvements over greedy and beam labeling in several settings, although the model itself is not state-of-the-art.",
            "strength_and_weaknesses": "Overall, this paper is well written and well structured. Although the contribution is fairly simple, it is well theorized. Extensive experiments are also provided.\n\nThis work is likely to have little practical use given the additional complexity of the proposed scheme and its fairly limited experimental improvements. But it does provide interesting insights and may trigger further research.\n\nSuggestions/Comments:\n- can improvements that are statistically significant be added to the result tables? Many improvements seem limited; are they statistically significant?\n- since the method does not yield state-of-the-art results, it would be nice to better explain summary reranking and MatchSum (e.g., how much more complicated/computationally expensive these are).\n- could MatchSum be added as a baseline for the cross-domain experiments?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is clear and well written.\n\nNovelty: the work is novel to the best of my knowledge.\n\nQuality: both theoretical and empirical aspects of the paper are good.\n\nReproducibility: the code and models are open-sourced (I could not check given broken anonymized link).\n\n",
            "summary_of_the_review": "The work is novel to the best of my knowledge but its significance is likely to be limited given:\n- recent improvements to abstractive summarization \n- its complexity over labeling schemes that are simpler to implement\n- it is not state-of-the-art\n\nThat said, the work is overall solid and well theorized.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "If this paper is from Mirella Lapata's group (as I suspect given its citations), there is a potential conflict of interest for me reviewing given the Professor's industry collaborations (although I have never worked with them and do not know them).",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3101/Reviewer_mPYc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3101/Reviewer_mPYc"
        ]
    },
    {
        "id": "ib7NkPFXtn",
        "original": null,
        "number": 2,
        "cdate": 1666641684294,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641684294,
        "tmdate": 1666641684294,
        "tddate": null,
        "forum": "HehQobsr0S",
        "replyto": "HehQobsr0S",
        "invitation": "ICLR.cc/2023/Conference/Paper3101/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel way of extracting silver-standard extractive summaries (Oracle) from gold-standard abstractive summaries which they call \u201cOREO\u201d. Earlier methods mostly relied on the Greedy labelling method, which is sub-optimal and deterministic. The authors propose to instead generate the final oracle summaries by soft labelling so that multiple possible extractive oracles (obtained by beam-search) are considered according to their probability distribution; which leads to the final oracle summary being non-deterministic. \nThe authors have then performed several experiments to show the robustness of their model in different settings - supervised, zero-shot cross-domain, zero-shot cross-lingual, supervised abstractive, etc. The authors have experimented on several datasets, using the BERTSum model (and GSum model for abstractive setting). They show that models trained on OREO perform better than greedy and simple-beam search labels in most cases and generalize better.",
            "strength_and_weaknesses": "Strengths\nThe problem statement is very interesting.\nProposed method is simple to implement and doesn\u2019t require architectural changes for existing models\nExtensive set of experiments on different settings and on multiple datasets performed, showing the method leads to better-performing models\nWeaknesses and suggestions\nOnly the BERTSum model was used for comparison between labelling strategies. Performing similar experiments with some other models would be better, and these models need not be very complex (even some older architectures such as SummaRuNNer [Nallapati et al., 2017] may be used). \nAnother simple labelling strategy that may be a baseline may be to just get the top-K ROUGE sentences from the document individually for every sentence in the original reference summary. [Cheng & Lapata (2016)]\nIn Zero shot multi-lingual setting XLM-base has been used when the given SOTA uses XLM-Large, but then the authors claim the SOTA performs better because of the more complex architecture.\nSome equations are a bit difficult to understand, and it will be good if the terms are defined properly and described in simpler text. \nThe purpose of Section 5.5 is not very clear, and the results do not seem too interesting either. It is also not clear how exactly the ASK metric measures the extent to which a model can attain summary relevant knowledge. This space may be better used for discussing experiments using other models.\nIn many of the results, only a very small improvement in ROUGE scores have been achieved over other labelling strategies (which is kind of expected with ROUGE scores). It is also known that ROUGE scores are probably not a very good way of measuring quality of summaries. Thus it is not very evident if the OREO strategy actually leads to better summaries. It would perhaps be better if some survey analyses were performed with the outputs from models trained on different labelling strategies.\n\nOther Comments:\nThere is some discrepancy with the caption and data in Table 1, it seems the authors talk about sentences 1 & 4 (not 2 & 4) but then the OREO does not provide very high score (relatively) to sentence 1.\nIn line 8 of Algorithm 1, the l\u2019 on the right hand side will probably be x.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - The paper is mostly clear, except that some equations are a bit hard to understand\nQuality - Quality needs improvement in terms of more experiments with different models\nNovelty - Somewhat novel\nReproducibility - The work is reproducible and the codes are available.\n",
            "summary_of_the_review": "While the proposed method is simple to implement and is shown to be robust by several experiments, my main problem is that only BERTSum model was used to evaluate the different labelling strategies. Sections 5.5 and 5.6 are also questionable. Even though the method performs better than others, in most cases the increase in performance is quite small.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3101/Reviewer_perC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3101/Reviewer_perC"
        ]
    },
    {
        "id": "IsnNMEZDAx",
        "original": null,
        "number": 3,
        "cdate": 1666755717808,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666755717808,
        "tmdate": 1666755717808,
        "tddate": null,
        "forum": "HehQobsr0S",
        "replyto": "HehQobsr0S",
        "invitation": "ICLR.cc/2023/Conference/Paper3101/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses two common problems in pseudo-label (or oracle extract) generation for extractive summarization: 1) greedy or beam search does not always lead to a globally optimal solution, and 2) a pseudo-label is a deterministic training target that lacks calibration. \nThe authors then propose to leverage a series of pseudo-summaries generated from beam search to generate sentence-wise soft labels used for extractive summarization training.\n",
            "strength_and_weaknesses": "**Strengths**\n- The identified problem is significant in extractive summarization, and the motivation is clear.\n- The idea of using soft labels for sentence-level labels is novel.\n- Extensive experiments and thorough analyses demonstrate the superiority of the proposed method in many summarization scenarios across domains and languages in supervised and zero-shot settings, with \n- A theoretical proof is provided, showing that maximizing oracle expectation is equivalent to the proposed non-deterministic objective for extractive summarization.\n\n**Weaknesses**\n- The performance improvements look marginal in some settings, and more sophisticated baselines can be considered.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized and presented. Though soft labels are widely used in modern deep-learning models, incorporating them in a sentence labeling process is a novel design. The authors also provide a theoretical explanation of the learning objective involving soft labels. In addition to experimenting with the labeling method in various text summarization scenarios, the analyses on the connection between the upper bound of different oracles and the actual performance (as well as the proposed attainable summary knowledge) are inspirational. The authors release their code for reproducibility.\n\nA minor concern is that performance improvements over baselines or deterministic beam/greedy oracle seem marginal in some cases. It's weird to mark the results of OREO in Table 4 in bold since MatchSum is the most performance model. The authors claim that MatchSum has an extra reranking process, and OREO can potentially serve as a foundation for those more complex methods. I would like to know whether OREO can further enhance performance over MatchSum. Meanwhile,  the idea of using reinforcement learning over evaluation metrics or exploiting supervision signals from multiple target summaries can be widely found in abstract summarization efforts (e.g., [1][2]), and some of them may not be hard to adapt to extractive summarization. Comparing these works can make the evaluation more solid. \n\nI'm also curious why the evaluation metric considers ROUGE-1 and ROUGE-2 but not ROUGE-L. Will the experimental conclusions still hold if using the mean of ROUGE-1, 2, and L?\n\n[1] Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization. EMNLP 2019\n\n[2] BRIO: Bringing Order to Abstractive Summarization. ACL 2022\n",
            "summary_of_the_review": "This paper presents a sentence labeling algorithm creating soft labels for extractive text summarization, with clear motivation, novel technical design, extensive evaluation,  and theoretical analysis. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3101/Reviewer_Sdw2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3101/Reviewer_Sdw2"
        ]
    }
]