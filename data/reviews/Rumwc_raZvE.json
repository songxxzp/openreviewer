[
    {
        "id": "xS7lN_7OiAo",
        "original": null,
        "number": 1,
        "cdate": 1666521666487,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666521666487,
        "tmdate": 1666591908760,
        "tddate": null,
        "forum": "Rumwc_raZvE",
        "replyto": "Rumwc_raZvE",
        "invitation": "ICLR.cc/2023/Conference/Paper1665/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper investigates robust overfitting, an important phenomenon that occurs in the adversarial training. In contrast to previous approach, the paper does not regard the neural network as a black-box model but divide a DNN into a series of layers and investigate the effect of different network layers on robust overfitting. It finds that the latter layer has larger impact on the robust overfitting. To this end, the paper proposes two regularization methods on the latter layers of the networks, namely $RAT_{LR}$ and $RAT_{AWP}$, and shows that two these methods help to mitigate robust overfitting.",
            "strength_and_weaknesses": "Strength\n\n1. The paper studies an important problem of robust overfitting in the adversarial training. It is highly motivated to divide the DNN into several layers instead of regarding it as a black-box model.\n\n2. The experiments are intensive, showing that the proposed methods, RAT, help to mitigate robust overfitting.\n\nWeakness\n\n1. My major concern is about the number of parameters that the paper fix in the experiments of Section 3. Different blocks have different number of parameters. Therefore, fixing different blocks will result in the models with different number of effective parameters so that they will have different capacity. If the model that fixs the latter part has smaller number of parameters than the model that fixs the former part, it cannot prove the latter part has larger impact on the robust overfitting. Instead, the model capacity would be more important. In addition, adding more analyses beyond the architecture of ResNet would help to strengthen the conclusion.\n\n2. As mentioned by the paper, AWP helps to mitigate robust overfitting. What is the performance of applying AWP to all layers? If this one has better performance than $RAT_{AWP}$, then the proposed method seems to have small novelty.\n\n3. In the experiments of Section 4, as AWP is shown to improve the performance. $RAT_{AWP}$ should not only be compared with AT with AWP as well.\n\n4. In the experiment of Section 4, the final accuracy of $RAT_{LR}$ is lower than AT in many cases. Therefore, the empircal application of this method seems very limited.\n\nMinors\n\nHow are the last fully connected layers treated in the experiments? Are they fixed?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to read. Although the code is not provided, I think the algorithm is relatively simple and the experiments are relative standard. Therefore, I think the results may be easy to reproduce.",
            "summary_of_the_review": "Although the findings and directions of paper is novel, I think the main claim of the paper is not well supported. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1665/Reviewer_VqAU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1665/Reviewer_VqAU"
        ]
    },
    {
        "id": "NJxN92vUlM",
        "original": null,
        "number": 2,
        "cdate": 1666540089360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540089360,
        "tmdate": 1666540089360,
        "tddate": null,
        "forum": "Rumwc_raZvE",
        "replyto": "Rumwc_raZvE",
        "invitation": "ICLR.cc/2023/Conference/Paper1665/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the impact of different DNN layers impact the performance of adversarial training (AT). The ablation experiments reveal that the latter (deeper) layers are more influential to the robust generalization gap and the final performance. Two techniques were then proposed to improve the performance of standard AT method via adaptive learning rate or adaptive weight perturbation.",
            "strength_and_weaknesses": "Strengths:\n1. The student of an interesting phenomenon, robust overfitting, in adversarial training.\n2. The key findings are interesting and appear quite new to me.\n3. The two proposed defenses are validated on multiple datasets.\n\nWeaknesses:\n1. Missing important analysis and discussion to existing work [1], where a detailed grid search was applied to WideResNet to find that the deeper layers are also more impactful to the final performance of AT. The conclusion is the same as in this paper. And they find that this is because the deep layers are overparameterized and can be simply reduced to mitigate the robust overfitting. This work did a similar analysis as in [1] but using different techniques, i.e., training or not training, hyperparameter ablation, adaptive lr etc. An in-depth analysis should be conducted to connect this work to [1].\n\n2. Incomplete robustness evaluation with different AT methods. In table 1/2/3, the standard AT is compared with the proposed method. It is thus not clear to me whether other AT methods like TRADES, MART, and AWP are also suffering from robust overfitting, and how the proposed method performs with these AT methods. How the proposed techniques work with data augmentation strategies in [2]. \n\n3. Why latter layers are so special? Any theoretical insights or an in-depth analysis? How can we solve this issue completely? Is it guaranteed to improve the robust accuracy if the latter layers are treated differently, can it improve the current SOTA robustness shown in https://robustbench.github.io/\n\n\n[1] Huang, Hanxun, et al. \"Exploring architectural ingredients of adversarially robust deep neural networks.\" Advances in Neural Information Processing Systems 34 (2021): 5545-5559.\n[2] Rebuffi, Sylvestre-Alvise, et al. \"Fixing data augmentation to improve adversarial robustness.\" arXiv preprint arXiv:2103.01946 (2021).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to read. The proposed methods are somewhat novel. The proposed methods can be easily reproduced.",
            "summary_of_the_review": "An interesting finding for adversarial training, and is confirmed from different perspectives by different techniques. However, the findings need deeper analysis and the robustness of the proposed mitigation methods needs a complete comparison. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1665/Reviewer_8P5a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1665/Reviewer_8P5a"
        ]
    },
    {
        "id": "sOU8nbd7R7",
        "original": null,
        "number": 3,
        "cdate": 1666664487828,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664487828,
        "tmdate": 1666664487828,
        "tddate": null,
        "forum": "Rumwc_raZvE",
        "replyto": "Rumwc_raZvE",
        "invitation": "ICLR.cc/2023/Conference/Paper1665/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this work, the authors study the phenomenon of robust overfitting during adversarial training (AT) of deep neural networks \u2014 specifically about the deviations that arise due to optimization at different layers of these networks. The paper demonstrates that if the deeper layers of the network are frozen or optimized with a lower learning rate during AT, robust overfitting is significantly reduced, albeit the accompaniment of reduction in test robustness as well, in contrast to that seen if the same is performed with the earlier layers. A similar phenomenon is seen with the Adversarial Weight Perturbation (AWP) regularizer applied to different layers, where it is seen that latter layers contribute more to robust overfitting. ",
            "strength_and_weaknesses": "Strengths:\n1) The paper is well-written, and presents a sequence of empirical evaluations in a clear, concise manner to help understand the effect of optimization of different layers on robust overfitting. \n2) The proposed methods RAT-LR and RAT-AWP are fairly simple: RAT-LR applies a fixed learning rate to the deeper layers, while RAT-AWP in a similar vein applies AWP solely to the deeper layers, to impose additional regularization for the latter part of the network alone.\n\n\nWeaknesses:\n1) The contributions and novelty of the techniques presented are fairly limited, given that the two primary methods proposed are to either keep learning rate fixed or apply AWP for latter layers, instead of applying the same to the entire network as a whole. Furthermore, the deeper layers/blocks are known to have many more parameters (especially for ResNet based models), thus it is not entirely surprising that the latter layers contribute more to overfitting. This aspect could have been analyzed in much more detail in the paper, to study of depth vs parameter count on robust overfitting. \n2) The analysis presented is also fairly focussed to the very specific case of step-decay of learning rate as well, when in practice several recent works on robust defenses utilize other learning rate schedules such as cyclic, cosine, annealed cosine etc. Furthermore, the effect of the magnitude of the learning rate drop, the spacing of drop points, effect on deeper layers, only FC layer etc. could have been studied. Since the paper does not present theoretical viewpoints of the problem (which is certainly fine on its own), the extent of empirical study could have been significantly broadened to better understand robust overfitting in slightly more generalized settings.   \n3) This is further reinforced given the fact that AWP based regularization on its own is seen to drastically reduce robust overfitting, and appears to be the best performing method as seen from Figure 3 (a) and 3(c). Thus the contributions of the paper become slightly unclear in this setting, over and above AWP based training alone.\n4) Given the excellent performance of AWP-AT and its effect in mitigating robust overfitting, the baseline AWP method needs to be included in the main empirical evaluations as presented in Tables-1,2, since it is well known that far out-performs the standard AT baseline presented. From the original AWP paper, WideResNet-34-10 models trained on CIFAR-10 with AWP-AT achieves 54.04% and AWP-TRADES achieves 56.17% AutoAttack accuracy, while the proposed RAT-AWP-AT achieves 54.46%. Thus, the improvements in robust performance, or noteworthy contributions over the AWP method  are not clearly seen.\n5) Further, the clean accuracy of models need to be reported alongside the robust accuracies in the same tables, due to the well-known robustness-accuracy tradeoff. Thus, it is difficult to judge if a given method with slightly higher robust accuracy is inherently better, if it is accompanied by a large decrease in clean accuracy.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is presented overall in a clear, concise manner. The contributions and novelty are however fairly limited, given that the primary methods proposed are limited to training with a fixed learning rate or the application of AWP for latter layers alone.\n",
            "summary_of_the_review": "As mentioned in the weaknesses section, the contributions of the paper are fairly limited, and the proposed method thus does not present significant benefits over the prior work AWP, which this paper seeks to incorporate. While the overall idea is well presented, the empirical evaluations could be improved to better understand the phenomenon of robust overfitting. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1665/Reviewer_2pex"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1665/Reviewer_2pex"
        ]
    },
    {
        "id": "X-8s5ul_Yr",
        "original": null,
        "number": 4,
        "cdate": 1666802253307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666802253307,
        "tmdate": 1666802253307,
        "tddate": null,
        "forum": "Rumwc_raZvE",
        "replyto": "Rumwc_raZvE",
        "invitation": "ICLR.cc/2023/Conference/Paper1665/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies the intriguing layerwise properties of robustness overfitting and reveals that adversarial overfitting typically happens in the deep layers of networks. Based on the observations, two regularizations are utilized for the latter layers to tackle the overfitting. Extensive experiments with different datasets, attack methods, and model architectures verify the proposed method.\n",
            "strength_and_weaknesses": "Pros:\n\n1. This paper is well-written and easy to follow.\n\n2. An in-depth analysis is performed to support the hypothesis.\n\n3. Significant performance improvement in many settings.\n\nCons:\n\n1. It would be better if the authors could offer some comparison with SOTA methods.\n\nQuestion:\n\n1. In Section 3.1, for AT-fix-param-[XXX], does fixing mean the parameters are fixed as random initialization? And do you still update the fc layer?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. \n\nQuality: Good.\n\nNovelty: Good. \n\nReproducibility: Fair. (Releasing code can further improve reproducibility)",
            "summary_of_the_review": "The analysis of this paper is rigorous and well-supported. I would lean to accept. If the authors can offer more comparisons with SOTA methods on addressing adversarial overfitting, I would like to increase my score further. Releasing code can also be a bonus.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1665/Reviewer_jxMW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1665/Reviewer_jxMW"
        ]
    }
]