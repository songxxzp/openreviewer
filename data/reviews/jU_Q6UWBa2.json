[
    {
        "id": "xg1ZsfMzqPp",
        "original": null,
        "number": 1,
        "cdate": 1666152918897,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666152918897,
        "tmdate": 1666152918897,
        "tddate": null,
        "forum": "jU_Q6UWBa2",
        "replyto": "jU_Q6UWBa2",
        "invitation": "ICLR.cc/2023/Conference/Paper1219/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a continual learning method by constructing low-coherence subspace projector for each new task. Given a new task, the projector matrix is constructed by minimizing the coherence to the previous task projectors and within the current projector to be optimized, in the oblique manifold. The proposed approach is evaluated for task incremental learning tasks, especially when the number of tasks is large (e.g., 64, 150). The results show improvements over the baseline methods, including, EWC, OWM, GPM, etc. ",
            "strength_and_weaknesses": "Strength:\n\n(1) The proposed approach tackles the challenging continual learning task, and proposed a gradient projection method, based on low-coherence, instead of orthogonality. \n\n(2) The experiments on Permuted MNIST, Rotated MNIST, Split CIFAR100 and Split miniImageNet show the improved results compared with some baseline methods, e.g., EWC, GPM, OWM, etc. \n\nWeakness:\n\n(1)  The proposed method is based on construction of the projection matrix by minimizing the coherence of the projection matrix with respect to the projection matrices of previous task, and also within the current task projection matrix.  Based on the sect. 4.1, it is unclear how to generate the projection matrix for the first task. Moreover, for the projection matrix formulation in eq. (6), it seems that the current O_t is purely determined based on the previous task O_i (i = 1, ..., t-1). It seems that the task data (or their features) are not utilized in the determination of O_t. How to guarantee that that learned O_t is optimal without seeing the task data or their features?\n\n(2) According to the results in Table I, the LcSP achieves significantly lower results than the compared methods, on Split CIFAR100, Split miniImageNet. But LcSP-GN and LcSP-BN are better than the compared methods. How to explain this results? It seems that the learning BN for each task is essential to the good results on these two tasks. Are these compared methods also use the normalization technique as the LcSP-GN and LcSP-BN? Moreover, the comparisons of LcSP, LcSP-GN and LcSP-BN should be also included for other datasets.\n\n(3) The comparisons are insufficient, e.g., the Adam-NSCL is also an typical continual learning method based on orthogonal projection, which should be also compared. The compared methods in Table 1 for different datasets are not consistent, for example, GPM and OWM are not compared for the two versions of MNIST dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "Writing clarity can be improved. For example, how to set the O_1 for the first task? The algorithm 1 should be revised in a more clear way. For example, line 1 in Alg. 1 is not necessary. The settings of \\alpha and \\beta in algorithm 1 should be clear. ",
            "summary_of_the_review": "My major concerns on this work are some unclear details, e.g., the question (1) in weakness, and insufficient comparisons in experiments. Moreover, there are some unclear points in the experiment results, especially on the effect of normalization techniques in the results.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1219/Reviewer_8QtN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1219/Reviewer_8QtN"
        ]
    },
    {
        "id": "puD0BdsvFj",
        "original": null,
        "number": 2,
        "cdate": 1666492709100,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666492709100,
        "tmdate": 1666493352793,
        "tddate": null,
        "forum": "jU_Q6UWBa2",
        "replyto": "jU_Q6UWBa2",
        "invitation": "ICLR.cc/2023/Conference/Paper1219/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to learn new tasks in low-coherence subspaces rather than orthogonal subspaces to mitigate catastrophic forgetting in continual learning. The authors believe that Gradient Orthogonal Projection (GOP) (though helps battle catastrophic forgetting) causes  learning capacity degradation and its learning capacity \"is gradually degraded as the number of tasks increases and eventually becomes unlearnable\". \n\nTo learn new tasks in low-coherence subspaces, the authors propose a unified cost function to seek projectors and develop a gradient descent algorithm (on the Oblique manifold) by jointly minimizing inter-task coherence and intra-task coherence. The original mutual coherence (often used in compressive sensing) of a matrix is the maximum absolute value of the cross-correlations between the matrix's columns. The authors extend the concept of coherence to two matrices to capture the maximum absolute value of the cross-correlations for the columns from two matrices. To deal with catastrophic forgetting heightened by batch normalisation (BN), the authors use two strategies: (1) learning specific BN for each task, or (2) using group normalisation (GN) instead of BN. \n\nThe authors did experiments on Permuted MNIST, Rotated MNIST, Split CIFAR100 and Split miniImageNet. For 20 tasks (a typically setting), the proposed method achieved best accuracies on Permuted MNIST and Rotated MNIST, and 3nd best forgetting on Permuted MNIST and 2rd best forgetting on Rotated MNIST. The proposed method was reported to achieve best accuracies and zero forgetting (surprising to see zero forgetting) on Split CIFAR100 and Split miniImageNet. \n\nTo show other methods would struggle with increasing tasks, the authors did experiments on 64 tasks and 150 tasks where the proposed method shows a clear advantage on accuracies, but not necessarily on forgetting (the proposed method is reported to achieve  best accuracies on both Permuted MNIST and Permuted CIFAR10, and 2nd best forgetting on Permuted MNIST and 3rd best forgetting on Permuted CIFAR10). ",
            "strength_and_weaknesses": "Strength: \n1) learning in low-coherence subspaces can be an alternative to GOP. \n2) the experimental results are impressive.\n\nWeakness:\n1) The authors believe that GOP causes learning capacity degradation and its learning capacity \"is gradually degraded as the number of tasks increases and eventually becomes unlearnable\". This should be more thoroughly analysed and supported by ablation study (curves on 64 tasks and 150 tasks with other methods are not sufficient to serve this purpose). \n2) The proposed method's forgetting performance on different datasets and settings vary widely from 0 forgetting to the state of being beaten by one or two competitors (e.g. see Fig. 1 (b) (d)).\n3) Coherence metric can be restrictive. Babel function which extends cross-correlation between pairs of columns to the cross-correlation from one column to a set of other columns might be a better alternative.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Low-coherence subspaces projection can be a good alternative to GOP. It is novel. \nMore thorough analysis can be provided for the severity of GOP's learning capacity degradation issue. The authors' claim \"their learning capacity is gradually degraded as the number of tasks increases and eventually becomes unlearnable\" needs solid analysis and evidence. Likewise analysis and evidence for the proposed method does not suffer from this should be provided too. \n\nGOP should be used when the network has sufficient learning capacity as Chaudhry et al. (2020) pointed out that \u201cWe assume that the network is sufficiently parameterized, which often is the case with deep networks, so that all the tasks can be learned in independent subspaces.\u201d If learning capacity can not catch up with the demands of increasing tasks, perhaps we can try to increase capacity on the fly? One way is to grow the network's depth adaptively [1*]. \n\n[1*] Online Deep Learning: Learning Deep Neural Networks on the Fly, by Doyen Sahoo, Quang Pham, Jing Lu, Steven C. H. Hoi, IJCAI 2018. \n",
            "summary_of_the_review": "At this stage, I am happy for this paper to be accepted. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethic concerns. ",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1219/Reviewer_em94"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1219/Reviewer_em94"
        ]
    },
    {
        "id": "BB5Ms4bqES",
        "original": null,
        "number": 3,
        "cdate": 1666680979391,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680979391,
        "tmdate": 1670639888480,
        "tddate": null,
        "forum": "jU_Q6UWBa2",
        "replyto": "jU_Q6UWBa2",
        "invitation": "ICLR.cc/2023/Conference/Paper1219/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Regularizing the subspace of the network representation has been used in continual learning to alleviate forgetting. To mitigate learning capacity degradation caused by orthogonal projection/regularization, this paper proposes to learn in low-coherence subspace. It learns task-specific projectors to represent each task in subspace with low coherence to others. To learn in low-coherence subspace, the projectors are learned on the Oblique manifold. The proposed method performs better than the compared methods (especially the orthogonal subspace learning method) on the task-incremental setting (with known task identifiers). ",
            "strength_and_weaknesses": "Strength\n- This paper proposes to learn the projections in the low-coherence subspace, instead of the orthogonal subspace, which lets the model capacity be used better, while reducing the parameter interference, with less capacity degradation. This is well motivated.\n\n- The experiments prove the claim about the benefits of learning in the low-coherence subspace, compared to learning with the orthogonal subspace. \n\n- The paper is generally written clearly, with some points that can be improved, as discussed in the following. \n\nWeakness\n- The work can be seen as an extension of the orthogonal subspace based continual learning paper (Chaudhry et al. 2020). The main difference is the projectors are learned on the Oblique manifold instead of the Riemannian manifold, which enables learning in the low-coherence subspace with some overlaps between the tasks. It brings limited novelty for continual learning. \n- Intuitively, the low-coherence subspace learning can be seen as a \u201crelaxation\u201d of the orthogonal subspace learning. The mechanism of low-coherence learning and the difference with orthogonal learning is not clearly analyzed and discussed. \n-- Specifically, some questions can be asked and discussed. For example, comparing with the orthogonal subspace learning, how the method alleviates the forgetting issue while not totally isolating the subspaces. \n- The proposed method is limited to only the task-incremental setting where the task identifiers are required in both training and testing phases, as shown on page 3 and experiments. It further influences the significance of the work, considering the insignificant technical novelty, although this is with the same setting as (Chaudhry et al. 2020). The authors may further \n- Some detailed questions:\n-- What is the gap between the implementation and the analysis on page 6? \n-- How is the efficiency of the proposed method? How is the running time compared to other methods? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The whole paper is generally written clearly, and the work is with good reproducibility. The details on the task-specific BN can be improved for better reproducibility. The analyses, discussion, and introduction for the settings can be improved. The novelty and contribution are not significant enough. ",
            "summary_of_the_review": "The whole paper is with a clear motivation and representation overall. The novelty and the significance of the contribution can be improved. Although the experiments can prove the method can work well and better than the orthogonal subspace learning method, the experiments are limited to a very specific and even simple setting in CL, which influences the generality and significance. \n\nAfter checking all the reviews and responses, I would like to keep my original score - below the acceptance line. \nThe concept of using low-coherence subspace projection in continual learning is interesting. However, the contribution is relatively limited, considering the relationship with the methods working on orthogonal subspace and the restricted task-incremental setting (i.e., the requirement on task id). The technical details and explanations of the experimental results (in Table 1) are unclear and confusing, as indicated by 8QtN. It is blurry what is the main role of the low-coherence projection on performance improvement, compared with the normalization trick. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1219/Reviewer_FMJV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1219/Reviewer_FMJV"
        ]
    }
]