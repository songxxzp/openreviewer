[
    {
        "id": "FzWw7e7VuR",
        "original": null,
        "number": 1,
        "cdate": 1666607503292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607503292,
        "tmdate": 1669392535120,
        "tddate": null,
        "forum": "JpRExTbl1-",
        "replyto": "JpRExTbl1-",
        "invitation": "ICLR.cc/2023/Conference/Paper3998/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method for gating the updates in message passing graph neural networks. The key idea is to measure the similarity of a node's representation with its neighbors, and to reduce the update magnitude when they become very similar (i.e., oversmoothing happens). The authors present theoretical and experimental evidence of the effectiveness of their approach to tackle the oversmoothing problem in GNNs. ",
            "strength_and_weaknesses": "Strengths:\n* The method achieves strong experimental results on a variety of tasks and datasets.\n* The method is generally well-motivated and clearly explained.\n* The authors present theoretical justification of their method.\n\nWeaknesses (more details in the next section):\n* Some of the experiments do not convincingly evaluate the described properties.\n* There is relatively little ablation analysis of the proposed model.\n* There is a plethora of methods claiming to solve the oversmoothing problem. The authors do not explain why their method should be better than existing ones.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: very good; the presentation is clear and easy to follow.\n\n**Quality**: the model is theoretically justified and obtains strong experimental results. \n\n**Novelty**: while the idea of gating is not new and many methods to solve the oversmoothing problem have been proposed, the model presented by the authors appears sufficiently novel.\n\n**Reproducibility**: Good, as the descriptions are sufficiently clear and the authors provide their implementation.\n\n**Detailed comments**: \nRegarding the first weakness: Table 1 evaluates the model's ability to perform multi-scale node-level regression. However, the datasets studied are heterophilic datasets that are also studied in the detailed experiments in Table 2. So one explanation of the results could be simply that the model performs well on these particular (heterophilic) datasets, and not necessarily the fact that it does well on multi-scale prediction. To properly evaluate this, the authors could compare to a method that performs well on Squirrel/ Chameleon but does not as good on the multi-scale prediction.\n\nRegarding the second weakness: Some more ablation analysis is needed in my view. For instance:\n* Is the second GNN in Figure 1 required? Why not compute the local graph gradients on the representations coming from the GNN F instead of F_hat?\n* How does the gradient gating perform without the multi-rate component? A simpler architecture could simply average the P-norms of the differences per node and thus get a single gating score per node instead of one per node and feature dimension.\n\nThe last point is related to the claim in the paper:\n> We observe that in (1), at each hidden layer, every node and every feature channel gets updated with exactly the same rate. However, it is reasonable to expect that in realistic graph learning tasks one can encounter multiple rates for the flow of information (node updates) on the graph. \n\nFirst, the claim that \"in realistic graph learning tasks one can encounter multiple rates for the flow of information\" is not clear to me and not justified. Clearly showing that having multiple rates per feature channel yields benefits over just one rate per node could bring some empirical justification for this.\n\nAnother concern I have is the large error bars in Table 2. In many cases, the difference between the leading models is much smaller than the error intervals. How can we claim that one method outperforms the other in such a scenario?\n\nIn addition, the authors only compare their method on a single homophilic dataset (genius) according to their own definition of homophily levels of <0.3 are considered heterophilic. Given that most GNNs are made for homophilic datasets, there should be a better comparison in this setting given the claim ghat G^2 improves performance on both homophilic and heterophilic datasets.\n",
            "summary_of_the_review": "Overall, I think the method is interesting and promising enough to warrant publication if my concerns from the detailed comments are adequately addressed. Currently, the evaluation is not rigorous enough to support all of the authors' claims about their method.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_uYz6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_uYz6"
        ]
    },
    {
        "id": "1sfMA3trze",
        "original": null,
        "number": 2,
        "cdate": 1666857272247,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666857272247,
        "tmdate": 1666857272247,
        "tddate": null,
        "forum": "JpRExTbl1-",
        "replyto": "JpRExTbl1-",
        "invitation": "ICLR.cc/2023/Conference/Paper3998/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a general module that alleviates the over-smoothing problem and allows the design of deep GNNs. The theoretical results are useful.",
            "strength_and_weaknesses": "Strength:\n- The paper proposes reasonably theoretical results.\n\nWeakness:\n- The proposed method is not well motivated. The paper shows that \"However, it is reasonable to expect that in realistic graph learning tasks one can encounter multiple rates for the flow of information (node updates) on the graph\". I hope the paper can include any example showing that multi-rate learning is necessary.\n- The proposed idea, shown in Figure 1, reminds me of the GRU layer. I hope the authors can clearly point out the similarity of ideas in the paper, and discuss why the proposed idea is different/novel.\n- The paper does not provide a sufficient description of the methods. There is no description of the number of layers, hidden dim size, number of trainable parameters, etc. of the proposed and baseline methods, making the experimental results hardly convincing.\n- I hope the paper can at least include experiments on some ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively clearly written. \nThe paper is hardly reproducible, given the lack of descriptions of full architecture/hyperparameters for the proposed method and the baselines.",
            "summary_of_the_review": "Overall, the paper proposes a GRU-like module for alleviating the over-smoothing problem for GNN. The proposed idea lacks sufficient motivation. Although the paper provides reasonable theoretical results, the proposed method is not convincingly evaluated. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_f9D8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_f9D8"
        ]
    },
    {
        "id": "cbeivQD51Oq",
        "original": null,
        "number": 3,
        "cdate": 1667118830979,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667118830979,
        "tmdate": 1669639468221,
        "tddate": null,
        "forum": "JpRExTbl1-",
        "replyto": "JpRExTbl1-",
        "invitation": "ICLR.cc/2023/Conference/Paper3998/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a graph gating method for advancing the capacity of overcoming over-smoothing problem of GNN models. Theoretical proof and analysis are provided to demonstrate its superiority on dealing with over-smoothing issue with the help of ordinal differential equation (ODE). Experimental results look impressive on graph heterogeneity problem. The presentation and writing are overall good for catching up with the main idea.",
            "strength_and_weaknesses": "Strength:\n1. A multi-rate update schema implemented as graph gating operation is designed as a wrapper for GNN layers.\n2. Theoretical analysis on the properties of  the proposed method is provided.\n3. Extensive experimental results on classification and regression tasks demonstrate the superiority of the proposed method.\n\nWeakness:\n1. Employing gating method to control the message passing flow is not a new idea. The difference of this work is the employment of graph gradient method. The idea of graph gradient seems to be directly from previous work (Hodge laplacians on graphs, 2015). However, it's lack of critical details about how to get the gradient of node features as the difference of node features of end points in single edge, and how to adapt to the gradient gating module for the graph gating method. \n\n2. From the main content, it's still difficult to conclude a strong connection between over-smoothing problem and the gradient gating method. Before dividing into the theoretical analysis,  the gradient gating can adaptively control the message passing flow with information gathering from the difference of node features between central node and its neighbors. While, it's not easy to draw the conclusion because dynamic changes of the aggregation weight over the neighbors is a common property of many GNN methods. Until the theoretical part, some clues induced from the theoretical upper bound of Dirichlet energy state give an alternative way to help dig more insights about the proposed gating method. Maybe authors should consider to move some conclusions from the theoretical analysis ahead to the main content (Section 2).\n\n3. The experimental analysis mainly focuses on the classification performance. It misses important experiments to show the capacity of overcoming over-smoothing problem. I strongly suggest authors to show the performance of the proposed method along with the increasing number of GNN layers, and the employ the metrics from pioneering work [1] to validate the proposed method. Besides, it should consider more baseline which also applies gating mechanism to the GNN layers, such as but limited to GaGN [2] etc. \n\nReferences:\n1. Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View, AAAI 2020.\n2. GaAN: gated attention networks for learning on large and spatiotemporal graphs, UAI 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the proposed method is easy to follow, but the connection between theoretical part and methodology is the weak point. ",
            "summary_of_the_review": "Overall it's a good work along with both methodology and theoretical analysis. But it still has certain space for further improvement.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_WoCb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_WoCb"
        ]
    },
    {
        "id": "iZ6NSsOFSuB",
        "original": null,
        "number": 4,
        "cdate": 1667191622477,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667191622477,
        "tmdate": 1669560199006,
        "tddate": null,
        "forum": "JpRExTbl1-",
        "replyto": "JpRExTbl1-",
        "invitation": "ICLR.cc/2023/Conference/Paper3998/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel GNN architecture based on a gradient gating module. The module is proved to alleviate oversmoothing through theoretical analysis and simulations, and shows good experimental results on the benchmarks. ",
            "strength_and_weaknesses": "Strength: The proposed operator is backed by theory to alleviate the oversmoothing issue, and demonstrates good results in practice.\n\nWeakness: It introduces another hyperparameter $p$ which requires additional tuning efforts. It is not discussed whether the performances still match when number of trainable parameters or computations are the same. \n\nCould the authors provide some additional results to show:\n\n1. Is there a good $p$ that transfers well across different tasks? Or is there some principle to make tuning $p$ easier?\n\n2. How do the results compare in the parameter or compute match settings?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow, while providing theoretical insights. The method is novel to my knowledge. Code is already provided to prove reproducibility.",
            "summary_of_the_review": "Overall I like the method proposed in the paper and the experimental results shows good improvements, therefore I lean towards accepting the paper. It would be even better if my concerns are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_xxu8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_xxu8"
        ]
    },
    {
        "id": "biGmeMOtXit",
        "original": null,
        "number": 5,
        "cdate": 1667377189891,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667377189891,
        "tmdate": 1669643625766,
        "tddate": null,
        "forum": "JpRExTbl1-",
        "replyto": "JpRExTbl1-",
        "invitation": "ICLR.cc/2023/Conference/Paper3998/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes gradient gating ($G^2$), which assigns different trainable weights for different nodes and representation channels in each layer of GNN. The proposed method can be easily combined with different GNN, e.g., GCN or GAT. Theoretical results prove that $G^2$ can help alleviate the over-smoothing problem in GNN, and empirical results demonstrate that the proposed framework achieves state-of-the-art performance on node classification with different heterophilic graphs, both standard and large-scale. ",
            "strength_and_weaknesses": "Strength:\n- This paper proposes a novel mechanism for GNN to alleviate the over-smoothing problem\n- Empirical results against several existing baseline methods show state-of-the-art performance of the proposed method\n\nWeakness:\n- Some parts on the proposed method may need more explanations:\n  - I am a bit curious on how $G^2$ can reduce back to standard GNN with residual connection.  Equation (5) in section 3 shows its connection with standard GNN when $\\tau^n=0$. And from (2), I suppose setting $\\tau^n=1/2$ will only differ in a constant with (1). Nevertheless, it is a bit confusing why there is a difference in constant. The authors may need to add some explanations on that. \n  - It is a bit unclear why there is an additional hyper-parameter $p$ on computing $\\tau$, considering the expressive power of GNN. The sensitivity analysis in appendix A seems to show that we need a large enough $p$ to ensure good performance. Are there some explanations on that behavior?\n  - Also, since $\\tau \\in [0,1]$, why tanh activation function is used for $\\tau$? I suppose the output of tanh function lies in $[-1,1]$? \n  - Moreover, it is a bit confusing how the proposed method is connected to gradient, as the method name \"gradient gating\" indicates. I suppose it is more appropriate to call it \"channel weighting\" as $\\tau$ is directly multiplied on each channel of node representations. \n- Empirical results:\n  - I think a very relevant paper [1] is missing in current version. The authors may consider adding some discussions or empirical comparisons with it. \n  - Also, why are GloGNN and LINKX missing in Table 2? Are there any special reasons not to compare with them?\n  - It may be also good to show some visualization results on $\\tau^n$, and see how it changes with layer $n$. I wonder if there will be some interesting patterns, e.g., only few channels receive large weights while other channels have near-zero weight. \n- Minor issue: some references seem to be duplicated. Please double-check. \n\n[1] Designing the Topology of Graph Neural Networks: A Novel Feature Fusion Perspective. WWW 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the main idea of this paper is clear from my perspective, though some parts need further clarifications. \n\nQuality: overall I think this paper has a fair quality, while both theoretical justifications and empirical results can be further improved. \n\nNovelty: though I am not really an expert in this field, I think the proposed method is novel and substantially different from existing methods.\n\nReproducibility: the authors have uploaded their code as supplementary material, and hyper-parameters for each method and data set are also provided in appendix. Though I did not really run their code, I think it should not be difficult to reproduce the experimental results. \n",
            "summary_of_the_review": "While this paper proposes a novel method and empirical results show some improvements, more explanations can be added to make the proposed method more clear, and some baseline methods are also missing in comparison. As such, I think current version is still marginally below the acceptance bar. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_8MQJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3998/Reviewer_8MQJ"
        ]
    }
]