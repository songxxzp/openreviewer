[
    {
        "id": "InUbIbK-BWA",
        "original": null,
        "number": 1,
        "cdate": 1666607056651,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607056651,
        "tmdate": 1669158193405,
        "tddate": null,
        "forum": "wUcNUTnOvq",
        "replyto": "wUcNUTnOvq",
        "invitation": "ICLR.cc/2023/Conference/Paper62/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose GraphCG, a method for finding steerable factors in the latent space of already-learned deep generative model (DGM) for graphs. The method learns an energy-based model (EBM) in the latent space of a DGM with the goal of finding direction vectors $d_i$ that correspond to meaningful semantic modification of the latent vectors. The authors evaluate their method on molecular graphs and point clouds. \n",
            "strength_and_weaknesses": "Strengths:\n* Steerable generation is a key challenge in generative modeling.\n* The paper is generally well-written and the presentation is mostly clear.\n* The authors present some interesting experimental results.\n\nWeaknesses:\n* There seems to be nothing graph-specific about the proposed method itself.\n* The authors only evaluate their method on molecular graphs and point clouds, where the latter are not graphs in the narrow sense.\n* Some aspects of the model description are confusing to me (see next section for details).\n* No results about ablation or hyperparameter studies are provided, e.g., comparing the linear and non-linear editing functions. It is not clear which version the authors used in each of their experiments. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: mostly good, with the exception of the model description, which I find vague and confusing in parts. The loss function is not entirely clear to me. First, below Eq. (10) the authors refer to Appendix D for details, but none can be found there. Next, I'm confused about the positive and negative pairs. Equation 10 takes a latent code pair as input and does not distinguish whether the latent code pair is a positive or negative pair. Should negative pairs not be treated differently than positive pairs? Next, the authors say that the noise distribution is to sample uniformly on the input set. However, in Eq. (10) the noisy samples also have $j$ and $\\beta$, which are different than $i$ and $\\alpha$ of the original data point. Is this also a part of the noise distribution? Also, the noise distribution (take a uniform sample from the empirical data) is the same as the construction of **positive** pairs as described for GraphCG-R. How can this be resolved? This part of the paper should be thoroughly revised to improve clarity.\n\n**Quality**: Technically, the method is relatively simple and not specific to graphs at all. It could equivalently be applied to any other data domain, as it only relies on input samples to be encoded as continuous latent vectors. As a consequence, it should also be evaluated as a general-purpose method, e.g., also on some image datasets. Moreover, even the evaluation focusing on graphs is very narrow, with a heavy focus on molecules plus a small evaluation on point clouds, which are not even graphs in the narrow sense. Overall, the set of datasets and backbone models is quite limited.\n\n**Novelty**: I am not aware of all the latest literature in steerable generation for DGMs, so I cannot judge this aspect adequately.\n\n**Reproducibility**: Given the paper and the supplementary material it is impossible to reproduce the results because the authors do not specify for each of their results which version of the editing function they used (Fig. 4) and which hyperparameter settings performed best. There are two variants of linear editing function in Appendix E, and it is not clear which of those was actually used. The authors did not provide an implementation of their method.",
            "summary_of_the_review": "The current manuscript's weaknesses outweigh the merits in my view. The most important drawbacks to me are:\n* The vague and (at least to me) not sufficiently clear model description.\n* Labeling the method as graph-specific while there is (as far as I can see) nothing graph-specific about it.  \n* Insufficient ablation/ hyperparameter studies are provided. The only ablation study is visual in nature and thus not a quantitative comparison of the variants.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper62/Reviewer_VdZt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper62/Reviewer_VdZt"
        ]
    },
    {
        "id": "aUvZIcuoH0",
        "original": null,
        "number": 2,
        "cdate": 1666666411716,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666411716,
        "tmdate": 1666666411716,
        "tddate": null,
        "forum": "wUcNUTnOvq",
        "replyto": "wUcNUTnOvq",
        "invitation": "ICLR.cc/2023/Conference/Paper62/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tries to develop a method called GraphCG for the unsupervised discovery of steerable factors in the latent space of deep graph generative models. The authors observe that the learned representation space of some methods is not perfectly disentangled.  Thus, they propose to learn the semantic-rich directions by maximizing the corresponding mutual information for tackling this problem. The experiments on molecular graphs and point clouds show the effectiveness of the proposed GraphCG.",
            "strength_and_weaknesses": "The strengths of this paper are as follows:\n1. This paper focuses on a fundamental and important research problem which is discovering steerable factors in graphs.\n2. This paper is clearly written and the proposed method is easy to understand to me. Figure 1 clearly presents the training phase of the method, which is also clearly listed in Algorithm 1. \n3. The experimental results especially for Figures 3 and 4 well show the effectiveness of the proposed method.\n\nThe weaknesses of this paper are as follows:\n1. The authors seem to ignore some literature on disentangled graph learning methods. The main difference between disentangled latent factors in disentanglement literature and the steerable factors in the paper is not well discussed. I am not sure whether the disentangled graph learning method is good enough for solving the problem, which raises my concerns about the technical novelty of this paper.\n2. The time complexity as well as the empirical efficiency of the proposed method is not well discussed, since relatively complex techniques are proposed in the model.\n3. Another concern is about the experiment. I think the experiments should consider more baselines (e.g., disentangled graph learning methods and SSL methods as discussed in section 4.4) for more convincing results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of this paper are good, considering its clear expressions and good experiments results, while the novelty and experimental baselines are two of my biggest concerns. It would be good if the authors can address these concerns above during the rebuttal.",
            "summary_of_the_review": "In summary, this paper focuses on an important problem and proposes an effective method to tackle this problem. But I still have some concerns above. Overall, I recommend weak reject for this paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper62/Reviewer_CQ2j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper62/Reviewer_CQ2j"
        ]
    },
    {
        "id": "XfAnAw9xjvS",
        "original": null,
        "number": 3,
        "cdate": 1666673684691,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673684691,
        "tmdate": 1666673684691,
        "tddate": null,
        "forum": "wUcNUTnOvq",
        "replyto": "wUcNUTnOvq",
        "invitation": "ICLR.cc/2023/Conference/Paper62/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": " The paper proposes a method denoted as GraphCG to perform unsupervised discovery of steerable factors in latent space of deep graph generative models. The paper is to answer questions:  (1) is the latent representation space learned from DGMs on graph data disentangled\n(2) Given a pretrained DGM with not perfectly disentangled latent space, is there a flexible framework enabling the graph controllable generation in an unsupervised manner?\n\n",
            "strength_and_weaknesses": "The paper is solving an interesting problem.\n\nWeakness: \n1. Idea: \nThe paper combines ideas like graph encoder/decoder, mutual information, contrastive estimation, EBM. I do not see how these ideas help with better understanding of the latent space \n\nCan the authors give some intuition about \"It starts with the assumption that the latent representations edited\nwith the same semantic direction and step size should possess similar information (corresponding\nto the factors) to certain degree, thus by maximizing the mutual information them, we can learn the\nmost semantic-rich directions.\" and \"we set the editing condition as containing both the semantic directions and step\nsizes, and we assume that maximizing the MI between different conditions can maximize the shared\ninformation within each condition. \" -- why doing so we can learn meaningful latent representation? Is figure 4 can show some intuition?\n\n2: Experiments: \nCan the paper include comparison with other disentanglement methods? e.g. in Table 1\n\n3 evaluation: \nHow do we sure the learned latent representation is really meaningful? The authors give Tanimoto measure for molecular editing. ",
            "clarity,_quality,_novelty_and_reproducibility": "the evaluation can be more clear",
            "summary_of_the_review": "The paper asked two questions in the introduction; however, I do not feel fully persuaded by the solution proposed in the paper. The evaluation of the paper can be more improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper62/Reviewer_wzWp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper62/Reviewer_wzWp"
        ]
    },
    {
        "id": "4ZrxZRpg1u",
        "original": null,
        "number": 4,
        "cdate": 1666786354264,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666786354264,
        "tmdate": 1666786354264,
        "tddate": null,
        "forum": "wUcNUTnOvq",
        "replyto": "wUcNUTnOvq",
        "invitation": "ICLR.cc/2023/Conference/Paper62/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims at finding the steerable factors of a graph. It probes the latent space, trains an edit function and a distance function.\nThe objective function of learning the edit function and the distance function consists of three quantities--- the noise contrastive estimation based objective, sparsity prone regularizer and the diversity encouraging similarity regularizer.\nExperiments have been conducted to support the efficacy of their model.",
            "strength_and_weaknesses": "> Strengths\n\n1. The paper tackles an important problem for controllable graph generation\n2. Apparently simple approach seems to work well\n\n> Weakness\n\n1. Lack of clarity in many places\n2. Lack of contrast from prior work ",
            "clarity,_quality,_novelty_and_reproducibility": "> Clarity\n\nThe paper lacks clarity significantly.  My main concern in understanding the paper are two folds--- the exact mode of operation of $z$ and how the effect of editing $z$ is being realized on graph generation.\nIn my understanding the latent code $z$ is a random variable which is being sampled from some distribution $\\mathcal{N}(\\mu(G),\\Sigma(G))$-- here $\\mu$ and $\\Sigma$ are neural nets. Is it the case, that we are performing edits on z and then computing the expected objective on the edited z? I suspect there will be a lot of variance for this. In principle, are you not trying to shift the distribution?\nWhy then do you not modify the encoder of a VAE directly and regularize with respect to the trained encoder?\n\nEq. (2) is very confusing  to say the least. Is $z$ a deterministic function of the graph data $x$? Then, are the graphs being generated in deterministic manner. What do you mean by $\\bar{x} = g(z)$? How, the graph data is being generated in a deterministic manner.\n\n> Quality\n\nGiven such lack of clarity, I am not in  a position to comment on the quality of the paper.\n\n> Novelty\n\nAlthough, the novelty is limited, I am fine as long as this simple method is sound and works well in practice.\n\n> Reproducibility\n\nI tried to find the code for better understanding of the method, unfortunately, I could not find one.\n",
            "summary_of_the_review": "My main concern is clarity (this can impact quality and technical soundness too) of the paper, as I mentioned above. Also, a not-so-minor point is that in ICLR, citations are given in author-year format rather than the citation-number format, whereas the authors follow latter. Note that, changing the current format to citation-number format would change the content significantly.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper62/Reviewer_F7x5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper62/Reviewer_F7x5"
        ]
    }
]