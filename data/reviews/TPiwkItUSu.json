[
    {
        "id": "Hzbr5kKiSNm",
        "original": null,
        "number": 1,
        "cdate": 1666029280799,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666029280799,
        "tmdate": 1669047385015,
        "tddate": null,
        "forum": "TPiwkItUSu",
        "replyto": "TPiwkItUSu",
        "invitation": "ICLR.cc/2023/Conference/Paper1380/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the training dynamics of gradient descent (GD) on Hilbert Space ($L^2$). A key idea from this paper is to utilize orthogonalities from the Hilbert space to decompose the GD dynamics into a union of (possibly countably infinite) per eigenmode dynamics. In the general and realistic setting, the per eigenmode dynamics is in-trackable as each mode interacts with other modes in a complex and structural manner. To simplify the analysis, the authors make a key assumption, among many others: (`GIA`=Gradient Independent Assumption) gradients of the loss with respect to modes are `mutual` independent (orthogonal) throughout the whole training trajectory. This assumption reduces the original complex system into a 1-dimensional problem in the sense that each eigenmode evolves independently. With an additional Polyak-Lojasiewicz type assumption (called Gradient dominant in the paper), convergence to the global minima can be proved.    \n\nThe GIA does not strictly hold in general (the only known example in the paper is the kernel regression). Nevertheless, the authors can apply the idea of eigendecomposition to symmetry matrix/tensor decomposition (with some additional assumptions, and finer analysis to control the coefficient gradients) and recover/improve convergence results from prior work. \n\nOverall, I think the decomposition of the dynamics of GD into eigenmodes is natural and interesting. However, the GIA is too strong and unnatural, which trivializes the rich and complex dynamics of GD into a union of independent 1D dynamics. ",
            "strength_and_weaknesses": "#Strength\n- The paper proposes a natural framework to study gradient descent dynamics in Hilbert space. \n- The paper recovers existing convergence results for symmetry matrix/ tensor decomposition.  \n\n# Weaknesses\n\n- The GIA is too strong and almost trivializes the dynamics. My understanding is that the rich, structural, and coupled dynamics between eigenmodes are the most interesting part of GD + representation learning. Note that this framework does not even hold for symmetry matrix/tensor decomposition, which is much simpler than neural networks. Note that the authors argue that, empirically, the assumptions (GIA, Gradient dominant) approximately hold. However, I don't think such empirical results suffice to justify the assumptions in Theorem 1. For example $\\sum_{j\\neq i} \\langle \\nabla \\beta(\\theta_j) , \\nabla\\beta(\\theta_i) \\rangle$ can be large even each $\\langle \\nabla \\beta(\\theta_j) , \\nabla\\beta(\\theta_i) \\rangle$ is small.    \n\n- The framework does not yield new results or solve a new problem. Further work needs to be done to justify the usefulness of the framework. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow. The overall presentation looks good to me. ",
            "summary_of_the_review": "The paper has several interesting insights. However, the assumptions on Theorem 1, in particular, the GIA, are too strong and unnatural, which basically remove all interesting structures in GD dynamics. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1380/Reviewer_CW4U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1380/Reviewer_CW4U"
        ]
    },
    {
        "id": "MexVoOSXoUW",
        "original": null,
        "number": 2,
        "cdate": 1666485229692,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666485229692,
        "tmdate": 1669047847329,
        "tddate": null,
        "forum": "TPiwkItUSu",
        "replyto": "TPiwkItUSu",
        "invitation": "ICLR.cc/2023/Conference/Paper1380/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors show how an appropriate basis function decomposition can be used to provide a much simpler convergence analysis for gradient-based algorithms on several representative learning problems, from simple kernel regression to complex DNNs. 1) The authors prove that GD learns the coefficients of an appropriate function basis that forms the true model. 2) The authors improve the convergence of GD on the symmetric matrix factorization and provide an entirely new convergence result for GD on the orthogonal symmetric tensor decomposition. 3) The authors show that different gradient-based algorithms monotonically learn the coefficients of a particular function basis defined as the eigenvectors of the conjugate kernel after training.\n",
            "strength_and_weaknesses": "Strength: \n\n1. The paper is clearly written and easy to read.\n\n2. The analysis seems new to me.\n\nWeakness and questions to the authors (I am not an expert in this area): \n\n1. For the motivating example, what is the behavior of SGD with momentum for training ResNet-18? I am asking as SGD with momentum is the baseline method for training ResNet-18.\n\n\n2. What are the major advantages of the proposed analysis of GD via basis function decomposition over the existing analysis of GD? Indeed, we also analyze the eigendecomposition of the GD dynamics for the classical analysis.\n\n\n3. In practice, we may not be able to get the exact gradient. Can authors comment on how to extend the analysis to stochastic gradient descent and related algorithms?\n\n\n4. One weak point is the analysis relies on Assumption 1, which is much stronger than the assumption used in the classical analysis of gradient descent.\n\n\n5. For neural networks, how to choose the orthogonal basis? Perhaps I missed something, but after reading the paper, I still do not know how to do this.\n\n\n6. In Theorem 1, can authors comment on the choice of the step size \\eta? \n\n\n7. In section 3.1, how can we assume the kernel function are orthonormal for the neural tangent kernel? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is clear and easy to read.\n\nQuality: the quality of this paper is fine, mainly on analyzing gradient descent for a few models using basis function decomposition.\n\nReproducibility: Not available.",
            "summary_of_the_review": "The paper proposes analyzing the convergence of gradient descent via basis function decomposition, which seems interesting. However, I have a few questions on this paper, see details in \"Strength and Weaknesses\"",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1380/Reviewer_iyzY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1380/Reviewer_iyzY"
        ]
    },
    {
        "id": "XyUs6Vh-ox",
        "original": null,
        "number": 3,
        "cdate": 1666842790692,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666842790692,
        "tmdate": 1666842790692,
        "tddate": null,
        "forum": "TPiwkItUSu",
        "replyto": "TPiwkItUSu",
        "invitation": "ICLR.cc/2023/Conference/Paper1380/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the convergence behavior of SGD. Specifically, it says that there is a way to project the learning process onto a set of orthogonal basis functions so that the leading coefficients in the basis functions exhibit 'smoothly' convergence behavior. While not explicitly stated in the paper, I think that implies certain generalization properties of the SGD (still a bit confused see below). The authors then proceed to analyze three specific applications/models and demonstrate the power of their generic framework. \n\nI have some clarification questions: \n\nQ1. If I understand the result correctly, it is a convergence result of the optimization problem and it does not explicitly say anything about generalization errors. But because the coefficients of leading basis functions get stabilized over time, does that also imply some generalization result, and if so, is there an explicit way to characterize that. \nQ2. For application 2 (matrix completion), the result looks too strong (also related to my Q1). Specifically, this is only a convergence result but at the end it also seems to imply all local optimal are the same. Ge, Jin, and Zheng's result made a quite heavy effort in matrix manipulation to prove the no spurious local optimal result. I wonder how this result manages to circumvent the technical barriers there. Also, it seems these results usually require some incoherence assumptions. I cannot find the counterpart in this paper too. ",
            "strength_and_weaknesses": "Strength: the authors find a quite interesting way to construct the orthogonal basis, and under some special circumstances (the applications), the basis can be explicitly written down. The analysis seems substantial. \n\nWeakness: it would be helpful if the authors can clarify my two questions above. ",
            "clarity,_quality,_novelty_and_reproducibility": "the presentation is mostly clear. The paper is a bit analysis heavy so was a bit difficult to parse (maybe quite inevitable for this kind of result). ",
            "summary_of_the_review": "1. it is a strong result and presents a new and interesting way to interpret the SGD.  \n2. In fact, it actually appears to be too strong so I need some clarification. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1380/Reviewer_5bKV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1380/Reviewer_5bKV"
        ]
    },
    {
        "id": "ofYci6ODvxj",
        "original": null,
        "number": 4,
        "cdate": 1666872713269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666872713269,
        "tmdate": 1666872713269,
        "tddate": null,
        "forum": "TPiwkItUSu",
        "replyto": "TPiwkItUSu",
        "invitation": "ICLR.cc/2023/Conference/Paper1380/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the training dynamics of gradient descent based algorithms using a particular orthonormal basis for the function space spanned by the (function) iterates during training. Convergence to the function in the model space that minimizes the expected l2 loss is shown to occur at a novel convergence rate under some assumptions.",
            "strength_and_weaknesses": "I am unable to fully evaluate this current work due to time mismanagement from my side, sorry.\n\nHere are however a few remarks:\n - I would have appreciated (I am not an expert) a more comprehensive comparison to previous convergence rates (theorem 1 and proposition 2 e.g.)\n - in theorem 1, the required learning rate depends in $\\frac{1}{\\sqrt{d}\\log{d}}$. For problems in high dimension, it implies a very small learning rate. Doesn't it force the GD dynamics to the gradient flow regime? In the CNN examples, it is common practice to use a learning rate as large as possible: would you say that your analysis still applies?\n - in section 3.4 \"Surprisingly, A-CK [...] captures the underlining solution of different gradient-based algorithms\" -> can you elaborate on what makes A-CK so special that its SVD can be used in your framework, instead of any other choice of basis. How is it \"surprising\" ?",
            "clarity,_quality,_novelty_and_reproducibility": "The content is clearly explained, with intuition, formal theorems, then discussion of the assumptions. The experiments support the claim.\n\nI am unable to appreciate the novelty and correctness of the convergence rate proofs.",
            "summary_of_the_review": "Sorry for a very lightweight review.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1380/Reviewer_yaF6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1380/Reviewer_yaF6"
        ]
    }
]