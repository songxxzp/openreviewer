[
    {
        "id": "ruYlM9JE4F",
        "original": null,
        "number": 1,
        "cdate": 1666575863178,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575863178,
        "tmdate": 1666575863178,
        "tddate": null,
        "forum": "vep-Hlmn0tc",
        "replyto": "vep-Hlmn0tc",
        "invitation": "ICLR.cc/2023/Conference/Paper5422/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes new stochastic algorithms to solve both nonconvex and convex KL divergence constrained Distributionally Robust Optimization (DRO) problems. The proposed methods are dual-free, which means the per-iteration computational complexity is independent of sample size. By utilizing the recursive variance-reduced technique and stagewise step size scheme, optimal convergence rates are also established in both nonconvex and convex cases. Empirical results demonstrate both the effectiveness of the proposed methods and the privilege of the underlying model.",
            "strength_and_weaknesses": "Strength:\nOverall, the paper is clear and well-organized. This work proposes four efficient dual-free algorithms to solve nonconvex and convex KL divergence constrained DRO problems. Optimal convergence rates are also established in both nonconvex and convex cases. The proposed ASCDRO and BASCDRO methods are the first optimal dual-free algorithms for the KL divergence constrained DRO problem.\n\nWeakness:\n1. It seems that there are both a Kullback-Leibler (KL) divergence constraint and a KL penalty. Is there a particular reason for doing this?\n\n2. The author claims primal-dual algorithms require maintaining and updating an O(n) dimensional vector for updating the dual variable. But I believe some stochastic primal-dual methods can take coordinate steps on the dual side and have per-iteration complexity O(d). Please correct me if I am wrong.\n\n3. In the convex part of Table 1, The complexity of Dual SGM is the same as the proposed method. Could the author discuss the advantage of their method compared to Dual SGM?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality, and Reproducibility are good. Novelty is minor.",
            "summary_of_the_review": "I am confused about why there are both KL penalty and constraint in the problem setting. In addition, the author basically reformulates the minimax problem to a minimization problem by using convex conjugate, which does not require much technical effort.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5422/Reviewer_E9G7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5422/Reviewer_E9G7"
        ]
    },
    {
        "id": "DxtuKI6_nD",
        "original": null,
        "number": 2,
        "cdate": 1666614923377,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614923377,
        "tmdate": 1666614923377,
        "tddate": null,
        "forum": "vep-Hlmn0tc",
        "replyto": "vep-Hlmn0tc",
        "invitation": "ICLR.cc/2023/Conference/Paper5422/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes and analyzes stochastic algorithms that apply to solve a specific distributional robust optimization problem. This problem, Eq. (1), is not a general formulation solved by the existing methods. The uncertainty set of Eq. (1) must be a KL-ball, and the KL constraint in Eq. (1) seems to be too deliberate. With the help of the particular form, the proposed method shows a better complexity independent of the sample size for convex and non-convex settings. ",
            "strength_and_weaknesses": "* **Strength**\n  - This paper is well-written and easy to follow.\n\n* **Weakness**\n  - This method is only applicable to solving problems with a particular form.  \n  - The KL regularization on $\\boldsymbol{p}$ is not a common regular term. Eq. (1) is not a generic objective function but a deliberate one. \n  - Computational complexity of the proposed method is not well portrayed in the experimental studies.  ",
            "clarity,_quality,_novelty_and_reproducibility": "* **Clarity**\n  - This paper is well-written and easy to follow.\n\n* **Quality**\n  -  It fails to meet the requirements of ICLR. \n\n* **Novelty**\n  - The novelty is limited as the objective is too specific. \n\n* **Reproducibility**\n  - The study reported in sufficient detail to allow for its reproducibility.",
            "summary_of_the_review": "This target problem, Eq. (1), is not a general formulation solved by the existing methods. The uncertainty set of Eq. (1) must be a KL-ball. The KL regularization on $\\boldsymbol{p}$ is not a common regular term. Eq. (1) is not a generic objective function but a deliberate one. \nConsequently, the reviewer suggests \"marginally below the acceptance threshold\" as the recommendation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5422/Reviewer_p2MP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5422/Reviewer_p2MP"
        ]
    },
    {
        "id": "IGq1Am-d44",
        "original": null,
        "number": 3,
        "cdate": 1666684087333,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684087333,
        "tmdate": 1666684087333,
        "tddate": null,
        "forum": "vep-Hlmn0tc",
        "replyto": "vep-Hlmn0tc",
        "invitation": "ICLR.cc/2023/Conference/Paper5422/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the problem of constrained DRO, and proposed a dual-free algorithm for solving the robust optimization problem, and hence the per-iteration complexity is independent of the sample size. ",
            "strength_and_weaknesses": "The algorithm for solving the constrained DRO doesn't use dual information and hence enjoys per-iteration complexity O(d). The algorithm also enjoys relatively good iteration complexiy. One drawback in the experiment is that, the per-iteration complexiy is the main improvement from this algorithm, but there is no running time comparison which would be more interesting. Also in Assumption 1 the smoothness assumption on $l$ is pretty strong.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. The results are new to the reviewer though I'm not an expert in this field.",
            "summary_of_the_review": "The paper is well motivated and thoroughly discussed related literatures. The authors have made concrete contributions, especially developed a dual-free algorithm for the constrained DRO, which can be much cheaper to run especially in modern ML high dimensional regime. Although there are a few points that the paper can improve on, e.g. defining $F$ earlier in the paper to cause less confusion, and discuss about the total running time when $d$ is large in the experiment. I think the paper is still a good contribution to ML community and I would like to suggest accepting this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5422/Reviewer_Z1u3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5422/Reviewer_Z1u3"
        ]
    },
    {
        "id": "KO-wlJv-gX",
        "original": null,
        "number": 4,
        "cdate": 1666883834535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666883834535,
        "tmdate": 1666884215840,
        "tddate": null,
        "forum": "vep-Hlmn0tc",
        "replyto": "vep-Hlmn0tc",
        "invitation": "ICLR.cc/2023/Conference/Paper5422/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to develop new stochastic algorithms for solving the popular KL-divergence-constrained distributionally robust optimization (DRO) problem for both non-convex and convex losses. The proposed method, SCDRO or its variants, establishes (near-)optimal oracle complexities for both convex and non-convex losses. Furthermore, the said complexities are independent of per-iterate sample and batch sizes. Empirical studies further validate the effectiveness of SCDRO variants.",
            "strength_and_weaknesses": "Strength\n\nI find this paper interesting and well-written. To me, the methodology behind SCDRO is highly innovative in deriving a primal-only formulation linked with existing compositional optimization works. The developed theory for SCDOR was designed for non-convex loss functions and further extended to convex loss functions, enjoying optimal complexities for finding $\\epsilon$-accurate solutions for both convex ($O(1/\\epsilon^2)$) and non-convex ($\\tilde{O}(1/\\epsilon^3)$) smooth loss functions $\\ell_i(\\mathbf{w})$. Both of these complexities are first in literature in the given settings.\n\n\nWeaknesses\n\nOn pp. 3 ending part, the authors essentially indicate that DRO with KL divergence regularization enjoys shown superior performance for distributional shift. End of pp. 1, \"They are either restricted to problems with no additional constraints on the dual variable p except for the simplex constraint (Qi et al., 2021; Jin et al., 2021), ...\" On pp. 13 middle part the authors indicate that Qi et al. (2021) firstly cast the KL-regularized DRO problems into stochastic compositional optimization. They also indicate the difference with this work that they \"solve KL-constrained DRO problems, which is more challenging than KL-regularized DRO problems.\"\n\nAs indicated in the paper, the hardness of their analyses lies on the non-smoothness of $F(\\mathbf{x})$ and the non-Lipschitzness of outer function in the given unbounded regime, which breaks the assumptions of stochastic compositional optimization theory, hindering the achievement of optimal complexities for both convex and non-convex loss functions. I am wondering if the said challenge bring essential new difficulties for this change? From the perspectives of Lagrangian's multiplier these two formulations are essentially the same. Can the authors bring more details on this comparison?\n\n\nMinor comments:\n\nPage 2, middle part, the authors said $f$ being \"monotone\" which can confuse readers since this work does not involve variational inequalities.\n\nPage 13, middle part, \"(i) we do not need to maintain and update ... (i) we do not need to worry ... \" the second (i) should be (ii).",
            "clarity,_quality,_novelty_and_reproducibility": "I praise the authors for good clarity and quality of this work. The SCDRO framework is innovative but I am concerned on the technical novelty on the relationship with earlier works, including Qi et al. (2021).",
            "summary_of_the_review": "As indicated above, this work enjoys strength of good clarity and quality but suffer from potential weakness of limited technical novelty. My rating is based on this, barring further rebuttals and revisions from the authors.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5422/Reviewer_rwfs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5422/Reviewer_rwfs"
        ]
    }
]