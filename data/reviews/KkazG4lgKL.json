[
    {
        "id": "nDkKy_IlEsV",
        "original": null,
        "number": 1,
        "cdate": 1665653969761,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665653969761,
        "tmdate": 1665653969761,
        "tddate": null,
        "forum": "KkazG4lgKL",
        "replyto": "KkazG4lgKL",
        "invitation": "ICLR.cc/2023/Conference/Paper3969/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a store-then-compare paradigm for out-of-distribution detection. The proposed method is motivated by Modern Hopfield Energy, and a simplified version of the proposed method is also derived.",
            "strength_and_weaknesses": "Strength:\n1. The paper is well-written and easy to follow.\n2. The experiment result is quite significant and impressive, demonstrating the strong performance of the proposed method.\n3. Further analysis of the penultimate layer versus the logits layer is also interesting.\n\nWeaknesses:\n1. The authors mention that \"the energy function of Hopfield Network is well suited as a desirable measure of the discrepancy between the OOD sample and the stored patterns\". Could the authors elaborate more on the reason behind this statement? Specifically, there exists a lot of metrics that can measure the discrepancy between features, such as measuring the distance of mean, the distance of covariance, the earth mover's distance, and so on. Thus, I am curious on is there a specific reason that the authors choose to use the energy function of the Hopfield Network to measure the discrepancy.\n2. To further elaborate on my point 1, I think the idea proposed in this paper is quite similar to the metric learning methods in other areas (e.g., few-shot learning). Could the authors also discuss the relationship between the proposed method and metric learning?\n3. Another small confusion I have is about the title. Does the \"efficient\" here imply that SHE is more efficient than HE or SHE is more efficient than the other methods as well? If the formal case, I suggest the authors delete the word \"efficient\" in the title as it is slightly misleading, if the latter case, I hope that some ablation study w.r.t. efficiency can be given to demonstrate this.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the presentation of this paper is clear, the proposed method is novel, and enough details have been given to reproduce the method.",
            "summary_of_the_review": "Overall, while I have concerns about whether the proposed method is just an alternative to metric learning, I think even if that is the case, the proposed method is still interesting in the field of OOD detection. Thus, I tend to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3969/Reviewer_ANA9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3969/Reviewer_ANA9"
        ]
    },
    {
        "id": "3OARy3JpfU",
        "original": null,
        "number": 2,
        "cdate": 1666450787909,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666450787909,
        "tmdate": 1669083566570,
        "tddate": null,
        "forum": "KkazG4lgKL",
        "replyto": "KkazG4lgKL",
        "invitation": "ICLR.cc/2023/Conference/Paper3969/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies out-of-distribution detection, aiming at making classification models excel at discerning ID and OOD data. It is an important problem for safety-critical applications, and has attracted increasing attention recently. In this paper, the authors state that previous OOD methods estimate the OOD confidence in the logit space with tedious hyperparameters, which are hard to be deployed in reality. To this end, the authors suggest a new store-then-compare paradigm. It is motivated by the modern Hopfield networks for the discrepancy score calculation, which is free from hyper-parameters and is easy in deployment. The authors conduct experiments on nine widely-used OOD datasets, and the authors claim their superiority over the state-of-the-arts.  ",
            "strength_and_weaknesses": "> Strength:\n\nTo the best of my knowledge, this paper is the pioneer in using the modern Hopfield networks in OOD detection. The authors interpret the energy function as the measure of similarity, which can be used to discern ID and OOD data. Further, the authors suggest a simplified realization named SHE, which approximate the original energy function by the Taylor series, which eases the burden in memorizing all embedding features in energy calculation. \n\n> Weakness:\n\n- *The motivation of the proposed method is not well supported*. The authors claim the deficiencies of previous works in using logit outputs. However, the discussion is only valid for their proposed method in using the energy function. To fully demonstrate the deficiencies in using logit outputs, I think the authors could take other advanced works as examples, e.g., [1,2]. To me, these methods can also demonstrate promising results regarding various OOD setting. Further, I think there are also many other works that use the embedding features in OOD detection [3], which may challenge the novelty of the paper. \n\n[1] Yiyou Sun, et al. Out-of-distribution Detection with Deep Nearest Neighbors. ICML'22. \n\n[2] Vikash Sehway, et al. SSD: A Unified Framework for Self-supervised Outlier Detection. ICLR'21. \n\n[3] Kimin Lee, et al. A Simple Unified Framework for Detecting Out-of-distribution Samples and Adversarial Attacks. NeurIPS'18. \n\n- *The novelty of the proposed method is limited*. Both the energy functions [4] and the so-called store-then-compare paradigms [5] have been studied. Therefore, I think it can improve the quality of the paper in discussing the superiority of the proposed method over these representative baselines. \n\n[4] Weitang Liu, et al. Energy-based Out-of-distribution Detection. NeurIPS'20. \n\n[5] Yiyou Sun, et al. Out-of-distribution Detection with Deep Nearest Neighbors. ICML'22. \n\n- *The experiments are not enough*. I do not think the authors conduct extensive experiments that can fully support the effectiveness of the proposed method. Experiments with large semantic space [6] (e.g., ImageNet benchmark) and hard OOD scenario [5] (e.g., CIFAR-10 vs. CIFAR-100) are not considered. Further, comparison with advanced baseline methods (e.g., [1,2,5]) is also missing. \n\n[6] Rui Huang and Yixuan Li. MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space. CVPR'21. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clearly written and is easy to follow. To me, the motivation is a little bit of unclear and the novelty is limited. From my opinion, addressing these issues can be helpful in improving the quality of the paper. Besides, I did not check the reproducibility of the paper. ",
            "summary_of_the_review": "This paper adopts the Hopfield network in OOD detection, which may have some novelties to benefit the community. However, to improve the quality of the paper, it will be great if the authors could address my concerns in the Summary of the Paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3969/Reviewer_bCQs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3969/Reviewer_bCQs"
        ]
    },
    {
        "id": "F5kTPPwx4-",
        "original": null,
        "number": 3,
        "cdate": 1666667374413,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667374413,
        "tmdate": 1666667374413,
        "tddate": null,
        "forum": "KkazG4lgKL",
        "replyto": "KkazG4lgKL",
        "invitation": "ICLR.cc/2023/Conference/Paper3969/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to use the energy function of modern Hopfield networks to detect out-of-distribution datapoints.",
            "strength_and_weaknesses": "The premise of Hopfield networks is to place plausible data points at the local minima of the energy function. The authors propose to use this property for detecting datapoint that are located far away from those local minima, which correspond to in-distirbution training examples. Overall, this is a very natural idea, and the empirical evaluation supports its usefulness. I have a few suggestions that I describe below that may improve the proposed method and the quality of the presentation.  \n\nThroughout the paper the authors focus on one specific model from the modern HN family. This model uses the dot-product similarity function between the memories $S$ and the datapoints $\\xi$.  Although this is a plausible measure, it is unclear why the representations in the penultimate layer of the front-end networks (e.g. ResNet18) would respect this measure. I suspect that a much more natural measure of the similarity would be the Euclidean distance between the memories and the data points, see for example [(Millidge et al., 2022)](https://arxiv.org/abs/2202.04557) for a description of the HN with this similarity measure. A third possible choice would be a cosine similarity between the memories and the new test datapoints. I suspect that among these three networks the one with the Euclidean distance will demonstrate the best performance in terms of out-of-distribution sample detection, the dot-product based network (considered in this work) would be second, and the cos-similarity would perform the worst among these three. I would like to see the results of these ablation studies in the revised manuscript. \n\nThe description of the prior work on Hopfield networks is not entirely accurate. Binary HN were introduced in [(Hopfield PNAS 1982)](https://www.pnas.org/doi/10.1073/pnas.79.8.2554), continuous HN were introduced in [(Hopfield PNAS 1984)](https://www.pnas.org/doi/10.1073/pnas.81.10.3088). Modern HN (both continuous and binary) were introduced in (Krotov & Hopfield 2016). The contribution of (Ramsauer et al., 2020) was to generalize the theory of (Krotov & Hopfield 2016) to the case of softmax activation function, and point out the relationship with transformers. For example, the first sentence of the second paragraph of section 3.2 is misleading - HN have been used for continuous pattern retrieval since 1984, see [(Hopfield PNAS 1984)](https://www.pnas.org/doi/10.1073/pnas.81.10.3088).  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly, the proposal is novel, numerical experiments look convincing. ",
            "summary_of_the_review": "Nice paper, I would like to see the results of the ablation studies that I requested, and revisions regarding the more accurate description of various models of the Hopfield family. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3969/Reviewer_boCN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3969/Reviewer_boCN"
        ]
    },
    {
        "id": "dlAWVfYqnS",
        "original": null,
        "number": 4,
        "cdate": 1667263178269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667263178269,
        "tmdate": 1669881784364,
        "tddate": null,
        "forum": "KkazG4lgKL",
        "replyto": "KkazG4lgKL",
        "invitation": "ICLR.cc/2023/Conference/Paper3969/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel out-of-distribution (OOD) detection algorithm with \u201cstore-and-compare\u201d fashion: store the average pattern in the penultimate layer for each class, and during test, compute the inner product for the test blob with the stored pattern for the predicted class, and this inner product will be thresholded to determine OOD or not. The authors try to motivate the algorithm from Hopfield energy with a series of approximations, and also argue that the penultimate layer is more effective than the logit layer as used in most existing work. Experiments are conducted on vision datasets (two training sets CIFAR10 and CIFAR100, and 9 OOD sets), with comparison with three baselines, and demonstrated superior results most of the time.  ",
            "strength_and_weaknesses": "strengths\n* The algorithm is intuitive and simple with strong empirical results, writing is easy to follow. \n\nweakness\n* Lack of analysis and understanding on when the algorithm should be better than baseline, there are huge tables on many datasets and backbone networks, but not much insights on more granular analysis.  \n* Lack empirical analysis and visualization on the stored patterns\n* no results with ImageNet dataset with 1k+ classes, which seems a critical aspect of the algorithm: number of classes, especially given that the performance seems not as good on CIFAR100 than on CIFAR10. \n\n\n\nMore details below: \n\nI don\u2019t feel the need to motivate the algorithm with Hopfield network. At the end of the day, it seems all it does is to store the average outputs from the penultimate layer for each class, and during test time, we compute the inner product between the test blob and the stored \u201cpattern\u201d for the predicted class. I don\u2019t feel motivating it with Hopfield energy is necessary, especially as there is a series of approximations not fully justified with empirical data demonstrated.  \n\nWhat I would prefer is to just start with the algorithm and briefly mention it could be motivated from Hopfield energy and put section 3.2-3.4 in the appendix, and leave more space for empirical analysis and visualization of the patterns remembered? For example, could we calculate the distances between classes and visualize the patterns in 2-D (or 3-D), and see where the ID and OOD examples land? even more interestingly, how are they evolving during training?  \n\nIt is misleading to say \u201cSimilar performance gain can be observed when CIFAR100 is used as the ID training data\u201d \u2013 if we look at Table 5 in Appendix A, where with ResNet34, ReAct is around 6% better on average. This raises several questions we should study, e.g., why on certain training sets the OOD performance is better,  why on certain backbone networks the OOD performance is worse than baseline, etc.. I believe these are important analyses to be done. Also interesting to see results on ImageNet with even more classes \u2013 Will the algorithm still perform better? or is it only good in the small-num-classes regime?   \n\nThe proposed algorithm is not w/o any hyperparameters: we still need a threshold deciding what are OOD examples? OOD examples can be of any form. How do we determine such a threshold? is the threshold the same for each class or different for different classes? \n\nAbout why the penultimate layer is better than the logit layer, I find the intuitive argument more convincing than the theoretical analysis. First, there are two strong assumptions w/o any justification: (1) z follows zero-mean Gaussian distribution, (2) $y_k^{id}$ follows another distribution $\\cal{I}$ whose expected value is a positive number larger than the expected value of $y_{q\\not=k}^{ood}$. At least, show empirical histograms to justify such assumptions? Second, there seems not much point to compare the inner products of two layers, what we care about is the relative values of the inner product of ID vs OOD examples in the same layer.   \n\ntypos\n* \u201cIntuitive Explanation.Given\u201d, add space \n* section 4.3.4, \u201ccomputation(Hendr\u201d, add space\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good \nQuality: fair \nNovelty: good\nReproducibility: good\n",
            "summary_of_the_review": "This paper proposed a novel OOD algorithm with an intuitive idea that simply compares the test examples with stored average patterns for the predicted class on the 2nd-to-last layer. It attempts to motivate it from Hopfield energy, which I don\u2019t think is necessary; the intuitive argument that the penultimate layer is better than the logit layer is convincing and well supported by experiments, but theoretical analysis seems futile. Experiments are relatively comprehensive in terms of datasets, baselines and average performance, but not results with ImageNet dataset as state-of-the-art method ReAct did, and lack insights on when the algorithm should perform better than baseline. The writing is clear and easy to follow, w/o little typos. Overall I can see this becoming a good paper, but not in current form. \n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3969/Reviewer_sLwk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3969/Reviewer_sLwk"
        ]
    }
]