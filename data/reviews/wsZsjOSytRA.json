[
    {
        "id": "ZeHngXCvI5",
        "original": null,
        "number": 1,
        "cdate": 1666524520561,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666524520561,
        "tmdate": 1666524567541,
        "tddate": null,
        "forum": "wsZsjOSytRA",
        "replyto": "wsZsjOSytRA",
        "invitation": "ICLR.cc/2023/Conference/Paper3594/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper explores the necessity of incorporating transformer elements (self-attention, swin module) into 3D segmentation networks. It demonstrates SOTA performance on three challenge datasets (FLARE, FeTA, AMOS) with comparable model size and computational FLOPS. It is heavily inspired Liu's \"A ConvNet for the 2020s\" CVPR paper, but nonetheless makes a meaningful contribution for practical 3D applications that could be further enhanced by a slightly better ablation study. ",
            "strength_and_weaknesses": "**Strengths**\n+ the paper is well written, easy to follow and addresses a relevant task in 3D/medical image analysis. \n+ the experimental section contains numerous direct comparisons with different versions of Swin/Vision-Transformers and demonstrates both faster convergence and higher accuracy of the proposed 3D UX-Net\n+ the authors also evaluate pre-train/fine-tune scenarios and reach SOTA performance on AMOS22\n+ within the method a new module: \"depthwise convolutional scaling (DCS)\" is introduced that performs slightly better than MLPs\n\nthe work gives (yet another) reason to consider claimed advantages of transformers with much care, since e.g. the increased receptive field of using 7x7x7 convolutions together with reasonable practical design choices alone can outperform those in a simpler Conv-only model\n\n**Weaknesses**\n- while closely following the narrative of ConvNeXt (which I liked) the authors perform there ablation study in the reverse direction: ie. in Tab 3 they start from the proposed model and **remove** individual parts, while ConvNeXt starts from a plain ResNet and **adds** the transformer-inspired changes. This is unfortunate, because for 3D segmentation there is indeed one prominent framework the nnUNet that uses a plain Conv-only model and has to date won nearly every medical segmentation challenge (incl. AMOS22). This is also evident from Tab. 1 where the nnUNet outperforms every single transformer approach and is only beaten by the proposed method (in light of these facts I would also strongly recommend to rephrase/remove the statement \"To our best knowledge, this is the first 3D ConvNet architecture that competes favorably with transformers SOTA in volumetric segmentation tasks.)  A somewhat more elaborate discussion on the differences of the nnUNet (which e.g. also doesn't use BatchNorm) would be of important value. Importantly: the kernel-size ablation should in addition be extended by 3x3x3 and 5x5x5!\n- I am also missing a more detailed discussion about runtimes for training and inference, since the raw FLOPs numbers are not always very favourable for UX-Net and depth-separable convolutions are additionally prone to lower efficiency (when comparing similar FLOP numbers). On the other hand the nnUNet is known to be fairly resource intensive for training, so any method that reduces this burden would be of great impact!\nAs a minor comment (since this was published after the ICLR submission deadline): one could include the nnUNet results for AMOS22 from https://arxiv.org/pdf/2208.10791.pdf in the final version ",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above the paper is clear, of high quality and reproducible. The novelty is high and comes from a mixture of empirical and methodological findings. The impact and the interest of the community in performant alternatives to both nnUNet and SwinUNETR is arguably high (looking at the large number of medical segmentation challenges e.g. at MICCAI). ",
            "summary_of_the_review": "A recommend acceptance, subject to moderate improvements in terms of the comprehensiveness of the ablation study. Both the practical impact and the methodological insights (revisiting larger kernels and depth-separable convolutions in contrast to transformer modules) are relevant.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3594/Reviewer_8E6F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3594/Reviewer_8E6F"
        ]
    },
    {
        "id": "cZF_PdFOqD",
        "original": null,
        "number": 2,
        "cdate": 1666612208627,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612208627,
        "tmdate": 1666612208627,
        "tddate": null,
        "forum": "wsZsjOSytRA",
        "replyto": "wsZsjOSytRA",
        "invitation": "ICLR.cc/2023/Conference/Paper3594/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present an extension to convolutional networks (ConvNets) tailored to simulate the behavior of hierarchical transformers, aiming to provide an alternative to visional transformers (ViT) with a significantly reduced number of trainable parameters. The architectural variations in the proposed model focused on volumetric depth-wise convolutions with large kernel size for feature extraction that enables larger global receptive field. They then provide experimental evidence using 3 datasets that demonstrated an overall improvement in performance of 3D semantic segmentation in medical imaging.",
            "strength_and_weaknesses": "Strength:\nThe manuscript is well written and constructed, the figures and tables are coherent.\nThe provided context in the literature is adequate.\nThe experimental validation is well applied and provides evidence to support the authors conclusions.\n\nWeaknesses:\nWhile the authors provide an extensive literature review to provide the right context to their research, it would be beneficial if they could explicitly outline in paragraph 2.2 the differences of the proposed approach to the previously published models. Particularly, relating to the main inspiration of this paper, e.g. Liu et al. 2022, in the context of novelty should be better clarified.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The study is well communicated with a high standard of clarity and reproducibility. \nThe novelty of the paper may be questioned as many design choices for the proposed model have been examined separately or in part.",
            "summary_of_the_review": "In the current stage of scientific research in medical image segmentations, after the immersion of Visual transformers, this study provides a relevant point of view, aiming to simulate a convolutional neural network alternative with comparable performance and simplified architecture. The study is well applied and communicated in this manuscript and would be a good contribution to the field and ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3594/Reviewer_ERdC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3594/Reviewer_ERdC"
        ]
    },
    {
        "id": "k-JVkPNySvV",
        "original": null,
        "number": 3,
        "cdate": 1666816621656,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666816621656,
        "tmdate": 1666816621656,
        "tddate": null,
        "forum": "wsZsjOSytRA",
        "replyto": "wsZsjOSytRA",
        "invitation": "ICLR.cc/2023/Conference/Paper3594/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Volumetric segmentation of medical images is still an open problem. This work proposes a new U-net type segmentation network that uses large receptive field depth-wise convolution operations to emulate vision transformers (ViTs). Additionally, patch-wise feature propagation and 1x1 convolutions are introduced as means of reducing the compute compared to ViTs. Experiments on multiple volumetric datasets and relevant CNN and ViT baseline models are reported with small to no improvements. ",
            "strength_and_weaknesses": "**Strengths:**\n\n* The authors propose a volumetric segmentation method, that is evaluated on multiple relevant datasets. The choice of different architectural components in this work are explained clearly. \n\n* Ablation studies are reported and model hyperparameters are reported clearly.\n\n* Experiments on multiple datasets and several relevant baselines are conducted. \n\n**Weaknesses:**\n\n* **Motivation for the model**: This work aims to reintroduce CNNs to match the performance of Vision transformers (ViTs) for volumetric medical image segmentation. This is strange to me, as the chronology of how these models were developed is somehow warped. This work claims to revisit CNNs for volumetric segmentation, and improve them so that they can emulate ViTs -- which were already mimicking CNNs. This logic of arguments is circular to me. I found it very hard to subscribe to the primary motivation of this work. Further, and more fundamentally, the claim that ViTs have surpassed CNNs in volumetric segmentation is highly exaggerated (first sentence in abstract). Going by even the results reported in this work this claim does not hold up.\n\n* **Experimental evaluation**: Results in Table 1 which have the main results are reported in single repeats. For the scale of performance improvements that is claimed, this could be just random, as the difference in Dice accuracy is very small (first or second decimal in most cases). Reading them as improvements based on single model runs is highly problematic. These small differences could simply be overcome with different initialisations. Also, the number of parameters between the current model and a CNN baseline like nn-Unet is double. The claims of superior performance under these conditions is not convincing.\n\n* **Additional insights**: Unfortunately, there are no new insights gained from this work. How does the use of the depth-wise convolution with large kernels influence segmentation across different tasks. What is the reason for these improvements? What type of errors that the other ViT or CNN models make are alleviated with this new architecture? If ViTs were capable of modelling longe range interactions and CNNs introduced image-specific inductive biases, how does the new model bridge this gap? How can this be evaluated?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, however as mentioned above is not sufficiently well motivated. The reading of the results is problematic based on single runs and minor improvements. This could have implications on reproducibility. \n",
            "summary_of_the_review": "This work proposes a volumetric segmentation method with a motivation of bridging the performance gap between ViTs and CNNs, by adding depth-wise convolutions with large kernels and couple of other tricks. The premise of the work that ViTs have surpassed CNNs, and CNNs need to emulate ViTs is flawed. Experimental evaluation shows small or no improvements, even with orders of magnitude more compute/parameters compared to other baseline methods. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3594/Reviewer_Ud7h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3594/Reviewer_Ud7h"
        ]
    }
]