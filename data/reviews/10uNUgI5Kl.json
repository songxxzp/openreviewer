[
    {
        "id": "QwJ0Wb2FXP",
        "original": null,
        "number": 1,
        "cdate": 1666303519329,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666303519329,
        "tmdate": 1668317843740,
        "tddate": null,
        "forum": "10uNUgI5Kl",
        "replyto": "10uNUgI5Kl",
        "invitation": "ICLR.cc/2023/Conference/Paper5175/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes using LLMs to understand underlying reward functions given a few language examples, then demonstrate that RL agents trained with this LLM-provided sparse reward function nearly match those trained with true underlying reward functions on some simple tasks.",
            "strength_and_weaknesses": "## Strengths\n\n****************************************Writing and Clarity:****************************************\n\n- The writing is very intuitive and straightforward; the paper is easy to read.\n\n******************************Framing/Method:******************************\n\n- This is a great execution of a simple idea (simple being a good thing here). The authors tested reward design with LLM on a variety of environments and train an RL-agent in-tandem to prove that not only are the labels accurate with the LLM, but these labels allow RL agents to learn well on tasks compared to the true underlying reward functions.\n\n************************Experiments:************************\n\n- The authors have comprehensive experiments demonstrating one of the claims of the paper: that LLM\u2019s can learn from far fewer examples due to all the data they\u2019re trained on.\n\n## Weaknesses\n\n**Clarity:** \n\n- Some details regarding the games seem to be missing, for example what is the fixed Bob agent in DealOrNoDeal?\n\n**Experiments:** \n\n- Missing LLM size comparison: While using GPT-3 helps demonstrates the points the authors aim to make about the ability of LLMs to act as reward functions, GPT-3 `text-davinci-002` and even open source alternatives of the same size are prohibitive to most researchers either due to cost (GPT) or compute limitations (not enough VRAM to run inference). This also presents a problem because the experiments don\u2019t really show that LLMs in general can help design reward functions, instead it shows that a 176B param GPT-3 can help design reward functions. Can the authors experiment with smaller alternatives? For example, at least GPT-3 `curie` (a 1-line code change for the authors), and/or the OPT family of open-sourced models (e.g. 350m, 1.3b, 6.7b - compute-matched with `curie`, 30b).\n\n******************************************Missing related work:******************************************\n\n- Using Natural Language to aid with reward design is a related subfield with some works, here is one for example: [https://www.ijcai.org/proceedings/2019/0331.pdf](https://www.ijcai.org/proceedings/2019/0331.pdf). This is a little bit related to the framing, as the authors should probably add a related works section explaining language as a proxy for rewards.\n\n****************************Minor details:****************************\n\n- With temperature 0 isn\u2019t GPT-3 just producing the max-likelihood token? In that case I don\u2019t believe top-p changes anything about the model\u2019s outputs and therefore is irrelevant (Section 4 2nd paragraph).\n- Unfinished Appendix: A.5 and A.6 are not used and don\u2019t have any text.\n- Typos:\n    - \u201cThey require far few examples\u201d > \u201cfar fewer\u201d\n    - \u201con the agreed upon the agreed upon split\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Very good\nQuality: The paper is above-average to high quality.\nNovelty: I believe this idea is novel. \nReproducibility: Some RL training details are sparse, but LLM prompts are given. Somewhat reproducible.",
            "summary_of_the_review": "I like this paper, and it demonstrates a well-written execution of a simple idea. The main issue that I have with this paper is the lack of experiments on smaller or more accessible LLMs. With that said, I still believe this paper should be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5175/Reviewer_49Yr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5175/Reviewer_49Yr"
        ]
    },
    {
        "id": "iBEpTV2Zojt",
        "original": null,
        "number": 2,
        "cdate": 1666311632907,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666311632907,
        "tmdate": 1669653410690,
        "tddate": null,
        "forum": "10uNUgI5Kl",
        "replyto": "10uNUgI5Kl",
        "invitation": "ICLR.cc/2023/Conference/Paper5175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to use a pre-trained large language model to specify rewards for reinforcement learning. The model takes a description of the task and reward, and optionally a small set of demonstrations and outputs a binary reward for a new queried game state. Based on this reward, an RL agent is trained to complete the desired task and is compared to agents trained with the GT reward as well as a reward generated by training a supervised model on the task demonstrations. \nThrough experiments in different domains, authors show that: 1) The proposed model can represent rewards, and train successful RL agents from even a single example, 2) LLMs can use background knowledge and predict correct rewards from commonly known objectives, even in a zero-shot setting and 3) They can model rewards in long horizon tasks, provided demonstrations and explanations.  \n",
            "strength_and_weaknesses": "\n**Strengths:**\n\n- Clear and well written paper. The goal of the paper is clear and well motivated, backed by prior work evidence, the contributions of the paper are clearly stated, the method is also clearly written, even though a bit more clarity could be added into the particular example in figure 1. The experimental design, results and takeaways are clear as well.\n- The approach for reward specification is flexible, allowing both example-based specification, through a description of the desired task or a combination of the 2.\n- To my knowledge, this is the first approach using LLMs to specify rewards, it is thus a novel formulation and a promising one, as LLMs become a building block of applications in ML.\n- Good experimental design. While the tasks are relatively simple, they represent scenarios for which we care about specifying rewards, and prove the 3 main points that authors are claiming with the proposed metthod. \n- Very interesting to see that the LLM contains concepts such as pareto-optimality. It would be great to see if the LLM can combine multiple concepts at the same time, when possible, or negate concepts. I don't think it is a requirement for this paper but I would suggest adding it as follow up work. \n- As shown in 5.2, the method is relatively robust to different prompts, as long as an explanation for the reward is given, implying that the method should not necessarily require thorough prompt design.\n\n**Weaknesses:**\n\n- Given that one of the motivations in this work is to provide rewards in cases where it hard to engineer them, I think that this paper would strongly benefit from having a case where the reward cannot be formally described, and measure with a human study whether people prefer policies that come from rewards generated from the LLM vs policies that come from people playing successful games according to such reward. \n\n- Experiments:\n    - I am missing more results to understand the role of explanations vs few-shot. In experiment 1, what is the performance if we use a LLM with only one example and no explanation? Does the high performance come solely from explanation or it is also because the model has better capacity for few-short learning. On that line, what is the importance of \\rho1? Does the LLM perform the task worse when given few shot examples and no description of the task?\n\n    - I am missing more analysis and context on some of the results. In 4.2.1 what is the performance of a random baseline? Is there bias towards a given user objective when it is not given, or in the failure cases, or are the best case solutions random?\n\n\n\n- While this is also the case of works like Huang et al. 2022, a limitation of this approach is that it requires states and actions in the environment to be represented as tokens that the LLM has been trained on. This limitation is transparently addressed in the last section though, and since many tasks can actually be specified via language, I still think this work is a valuable contribution. A minor suggestion would be that besides commenting on multi-modal states, authors also mentioned non-token actions. This may be important for continuous control for instance, where we may not want aggressive motion in a robot arm.\n\n- See my comments on clarity.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\n- Generally a very clear and well written paper, as stated in the strength sections. I like how each game is designed to test one of the hypothesis and the summary at the end of each experiment with an explanation of the findings.\n- The example in Figure 1 could be improved. For instance you could replace item0... for books/hats/balls, and specify the total number, or how many go for Alice and How many for Bob.\n- The sentence \"we assume access to the true reward by constructing user reward functions that humans have been shown to have inspired by prior work\", is a bit confusing. Are the rewards based on prior work? While this is further clarified in Sec 4.1 GT User objectives, an example of such reward here would be helpful.\n\n- Clarification Question: In 4.2.1 the authors seem to be describing recall on correct outcomes (identifying correct outcome) but I imagine that they are actually measuring accuracy of outcomes? If so they should modify the text.\n- Comment: What is the role of the description \\rho1, when prompts in \\rho2 are given.\n- Minor:\n    - Moving Figure 2 to page 5 would make the results a bit easier to read.\n    - Because SL actually does not have the explanation, I would modify the title in Figure 2, and add explanation in the Ours bar.\n\n**Novelty**:\n- To my knowledge, this is the first approach using LLMs to specify rewards, it is thus a novel formulation and a promising one, as LLMs become a building block of applications in ML.\n\n**Reproducibility**:\nThe domains and rewards are thoroughly explained in the appendix. More description of the process for selecting the right prompts, and especially the natural language explanations for the reward would be appreciated. It would also be valuable if authors released the dataset used for prompting the language model. As long as `text-davinci-002` remains available the results should be reproducible but given the importance of good explanations as per figure 6, it it important that authors share the dataset of explanations to ensure reproducibility.\n",
            "summary_of_the_review": "As stated in my strengths section, the paper addresses a very important problem with a solution that is simple and relatively general-purpose (safe for some of the cases mentioned in the limitations). The contributions are very clear and the design of the games and experiments show that using language models to specify rewards offers advantages in multiple ways. Moreover, as LLMs become a more pervasive building block in decision-making models, the approach has the potential to be extended and generalized as more research is done on this end (for instance through multi-modal foundation models to address more general states). As such, I think this is a valuable paper for this conference. The reason why I cannot give a strong accept is because I am missing experiments at the core of this paper goal: providing rewards where there is no way to specify them, evaluating them using human ratings.  Moreover, I would really like to clarify some of the details in the experimental results, as stated in the weakness section, experiments. This would help me convince that the proposed approach offers an effective way to specify rewards.\n\n[EDIT]: The authors have addressed all my concerns, clarifying experimental design and metrics, adding thorough ablation studies and a human experiment that proves the effectiveness of the approach under real scenarios with hard to specify rewards. I recommend this paper for acceptance.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5175/Reviewer_7igZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5175/Reviewer_7igZ"
        ]
    },
    {
        "id": "mxp8FFEaIPL",
        "original": null,
        "number": 3,
        "cdate": 1666374240442,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666374240442,
        "tmdate": 1669976419378,
        "tddate": null,
        "forum": "10uNUgI5Kl",
        "replyto": "10uNUgI5Kl",
        "invitation": "ICLR.cc/2023/Conference/Paper5175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper \"Reward Design with Language Models\" proposes to study to effectiveness of pre-trained large language models for designing rewards in RL tasks (tasks implying language). It shows that, somehow unsurprisingly given recent literature, leveraging these models to drive agents with textual prompts is possible in few-shot or zero-shot settings. ",
            "strength_and_weaknesses": "Strengths: \n- Using textual prompts to drive RL agents is an emerging topic with promising outcomes\n\n- Promising results for controling RL agents with language\n\nWeakness: \n    \n- Using pre-trained LLMs in zero-shot settings is clearly not new, many studies already exist on that topic. Not on the specific tasks authors suggest, but the transferability of their embedded knowledge with well specified prompts is well known. Prompt tuning is an emerging field following that observation. \n\n- Shaping rewards with LLMs is not new either, as many works recently proposed to leverage them to split difficult tasks into textual subgoals, or to drive agents by asking questions for instances, in a curriculum RL fashion. These works should at least be mentioned in the related work.   \n\n- Writing is not always very clear. Readers have to go in many various places again and again during reading to get things together. Very difficult to understand without appendix. Many settings not sufficiently formally specified to well analyse results (e.g., the supervised learning of SL, the definition of function g, etc.). \n\n- Environments look quite simple (short trajectories, few actions, deterministic transitions, full observability). It would be helpful to see more classical complex tasks of RL, in environments such as TextWorld, Alfred or BabyAI for instances. ",
            "clarity,_quality,_novelty_and_reproducibility": "My major concern about this paper is its novelty. While it may be quite interesting for the community, I feel that its findings are not sufficient for the level of ICLR.   ",
            "summary_of_the_review": "  - Could authors position their work w.r.t literature on prompt tuning, zero-shot learning with pre-trained LLMs (in particular, the last reference I give below looks very connected to the presented work), and also w.r.t. reward shaping with language? \n   - The global learning setting is not fully clear to me. How is defined or learned the function g that maps the textual outcome of the LLM to an int reward ? It is a language model too ? It is learned from binary true rewards ? If it is manually defined, how do authors manage variability of llm outputs ? (for expected answers such as yes or no, it is possible that the llm outputs something different right ? or do authors restrict its possible outcomes ?) \n   - In fig 8 right, the prompt finishes with \"let think step by step :\". So the LLM is supposed to propose a step by step forecasting of reasoning ?  \n   \n\n\n\n\nMultitask Prompted Training Enables Zero-Shot Task Generalization. ICLR 2022\n\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\n\nElla: Exploration through learned language abstraction. In Advances in Neural Information Processing Systems (NeurIPS), 2021\n\nEAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL. CoRR abs/2206.09674 (2022)\n\nDo as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. (2022)\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5175/Reviewer_UKq1"
        ]
    },
    {
        "id": "lpdR0xaQR3",
        "original": null,
        "number": 4,
        "cdate": 1666779161356,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666779161356,
        "tmdate": 1666779194474,
        "tddate": null,
        "forum": "10uNUgI5Kl",
        "replyto": "10uNUgI5Kl",
        "invitation": "ICLR.cc/2023/Conference/Paper5175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors in this paper study how to utilize large language models (LLM) as a proxy reward function for text based RL such as Ultimatum game, matrix game or the DealOrNoDeal negotiation tasks. The main idea is to consider to take a combined prompt $\\rho$ as the input of LLM, which consists of the task description, user provided example, converted outcome of an episode, and  a question asking if the outcome episode satisfies the user objective, and then use. the LLM's response as signals for reward.  ",
            "strength_and_weaknesses": "**Strength**\n- The idea is simple and intuitive, which is quite easy to understand. \n- The authors conducted detailed ablation study to show the effective of using LLM as Reward \n\n\n**Weakness** \n\n- I believe the approach can only be used for text based games, can not used for other more general tasks such as control tasks. \n- The authors only evaluate three tasks, and two of the are just 1 step tasks, and the left one is multi-step horizon based, but from the description it seems that the horizon is not that long. \n- Also, from the illustration figure, I feel the current approach may only deal with short horizons, since you will need to consider the whole trajectory as the prompt input to the LLM. \n- There is no user study. Since your tasks involves languages, I feel it should be necessary to conduct some user study with human evaluation, thus we can get some sense of what exactly happens here. \n- Also, for RL researchers, people would usually have the impression that  sparse reward would not actually work in practice, and require some more techniques to improve. Can authors explain why sparse reward provided by your approach actually work? What's the difference if we use a sparse intermediate reward function for training? Will that actually work? ",
            "clarity,_quality,_novelty_and_reproducibility": "**Originality**\n\nThe authors provide an interesting approach to solve text based RL tasks. As far as I know, this approach is new. \n\n**Clarity**\n\nI feel the authors should clarify and explore the pros and cons of the approach, for both discussion and concrete experiments. It is not enough to show the limitations at the final conclusion section, since the the limitation of your approach I feel is some very fundamental problems to study and may help others to understand. \n\n**Quality**\n\nOverall the paper is well written and the experiments shows the effectiveness of their approach on the limited domains. The quality can be further improved if the authors could answer my previous questions. ",
            "summary_of_the_review": "Overall I think the paper introduce an interesting approach for utilizing reward function for text based RL tasks, though the current approach is not quite general and can only applied  to short horizon based methods, and there is no user study with human evaluation to understand what is really going on here. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5175/Reviewer_1rSQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5175/Reviewer_1rSQ"
        ]
    }
]