[
    {
        "id": "yQuqZOH2UkU",
        "original": null,
        "number": 1,
        "cdate": 1666674603409,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674603409,
        "tmdate": 1666674603409,
        "tddate": null,
        "forum": "ZMxVNpd76mw",
        "replyto": "ZMxVNpd76mw",
        "invitation": "ICLR.cc/2023/Conference/Paper1935/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work targets the robust reinforcement learning problem with an uncertainty set with respect to $\\chi^2$. It proposes an alternative 'Q-value' $Z$ and arrives at the mean standard deviation RL. Then the algorithms in the discrete case and continuous case are proposed separately and evaluated in different experiments with comparisons to baselines.",
            "strength_and_weaknesses": "-Strength:\n1. The intuition and derivation of the designing algorithm in robust RL are well-explained by showing the relationship between robust RL and mean standard deviation optimization.\n2. The presentation and writing of this work are very clear and easy to follow.\n\n-Weaknesses:\n1. It seems the main contribution is that this work proposes a different 'Q-value' function by using mean standard deviation optimization and minus $\\alpha$ multiplying the standard deviation. However,  the experiments do not show that such designing of 'Q-value' has benefits if I didn't miss something. Such as in Figure 1 and Figure 2, the algorithm with $\\alpha=0$ (vanilla Q-value without extra design) and distributed critics works even better or the same as when $\\alpha>0$ (proposed methods in this work). It seems there does not exist a  great $\\alpha$ for all environments.",
            "clarity,_quality,_novelty_and_reproducibility": "-For clarity, some minors:\n1. \"ovoid\" in the last several lines on Page 5\n2. The notations of $N or $C$, $n$ and $c$ keep rotating is very confusing in the last several lines on Page 5 and Page 6.\n3. The index $i$ in Equation 4 is not mentioned before?\n4. Is there a missing of $Z$ in Equation 6 and the equation above Equation 6?\n5. There are also some confusing notations in Algorithm 2, such as what is $i$?\n\n-The novelty and originality of this work seem limited. The tailored design of the 'Q-value' does not have convincing results in experimental results.",
            "summary_of_the_review": "In summary, although this work is motivated by a clear intuition between the robust RL and the mean standard derivation optimization, the proposed technical 'Q-value' does not have convincing results in evaluated experiments. So I suggest for rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1935/Reviewer_UvPJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1935/Reviewer_UvPJ"
        ]
    },
    {
        "id": "zcxx98RTKG8",
        "original": null,
        "number": 2,
        "cdate": 1666724873544,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724873544,
        "tmdate": 1666724873544,
        "tddate": null,
        "forum": "ZMxVNpd76mw",
        "replyto": "ZMxVNpd76mw",
        "invitation": "ICLR.cc/2023/Conference/Paper1935/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a risk-sensitive criterion based on the expected value and standard deviation of a distributional RL trianing scheme. The authors evaluate their method w.r.t. robustness over repetitions and find more robust behavior",
            "strength_and_weaknesses": "Strengths:\n- the paper is in a relevant field of research\n- the mathematical derivations appear sound\n- relevant work is metioned\n\nWeaknesses:\n\n- Novelty: \n   - using a mean/std approach for risk-sensitive RL has been around for some time and is standard procedure with many publications\n   - the results are very much to be expected, adding a standard deviation makes training more stable at the cost of performance\n\n- Flexibility:   Compared to  Smirnova et al. (2019) the authors state that \"since they use the variance estimate under a Gaussian assumption of distributions while we use a standard deviation penalization without any distribution assumptions\" (Page 1). \n    - However, parameterizing a distribution using its first and second moment corresponds to moment matching (which is minimizing the KL divergence between the target distribution and a Gaussian variational distribution.\n    - E.g. if the distribution Z would be multi-modal, the standard deviation  and thus \\mu - \\sigma estimate would not be meaningful\n\n- Applicability: the fact that the parameter must be carefully chosen, not only its specific value but even its order of magnitude (compare \n( Hopper-v3 and Walker2d-v3 w) makes it hard to apply in practice. Ideally, the author would have proposed an estimation procedure\nto migitate this effect\n\n-  Empirical evaluation: \n    - minor things like y-axis labeling on all figures, and unclear what \"Mean over 20 trajectories\" would mean and no mention of experiment repetitions\n    - The only quality metric studied is performance(?) and spread of performance over trajectories. There would be more metrics to estimate robustness, such as worst-case (or some 10,20 percentile) performance. Variation over experiment repetitions etc.\n  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite clear, although I was not able to check the mathematical derivations\n\nI have doubts about the novelty of the approach, specifically using a mean/std approach as objective function is well established in safe, risk-sensitve, and curiosity based RL and other fields of research (e.g. active learning).\n",
            "summary_of_the_review": "Overall, I cannot recommend acceptance, because I have concerns regarding the novelty and applicability of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1935/Reviewer_anYu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1935/Reviewer_anYu"
        ]
    },
    {
        "id": "NtwVZRPT9av",
        "original": null,
        "number": 3,
        "cdate": 1666845062248,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666845062248,
        "tmdate": 1666845062248,
        "tddate": null,
        "forum": "ZMxVNpd76mw",
        "replyto": "ZMxVNpd76mw",
        "invitation": "ICLR.cc/2023/Conference/Paper1935/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies risk-averse reinforcement learning problems where the robust objective can be formulated as a penalization term on the standard deviation of the stochastic total return. The authors propose two algorithms for the setting with discrete action space and the setting with continuous action space respectively. Experiments are conducted to compare the proposed method with SAC on Mujoco environments.\n",
            "strength_and_weaknesses": "This paper provides a clear derivation for the reformulation of the robust RL objective to a variance-regularized objective, which is very helpful for readers to understand the motivation. The experimental results seem to be promising and show the effect of regularization on robust performance. However, the experimental comparisons are not complete and thus may not justify the proposed algorithm and its advantage in robust RL compared with other baselines.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "There are multiple typos in the current manuscript. The writing needs more polishing. The algorithm described in Section 3.3 is a little bit confusing and hard to read.  \n",
            "summary_of_the_review": "As mentioned in the previous sections, I appreciate the mathematical derivation of the reformulation of the robust RL objective. The main shortcoming that I think the authors could improve is its relationship with related methods, which I elaborate in the next paragraph. \n\nThere is no comparison with other robust RL methods in the literature. The authors might need to complete this comparison both from an algorithmic design perspective and the experiment perspective. In particular, since there are multiple ideas that motivate the development of the algorithms in this paper such as that of Kumar et al. (2022) and Moskovitz et al. (2021), the authors should at least have some discussion about the performance comparison with them. \n\n\u201cWe show that is it possible to improve the Robustness of RL algorithms with variance/standard deviation regularisation\u201d: it is \u2026\n\nCan you explain why the first equality in (1) holds?\n\nIn the sentence \u201c...to obtain better gradients when u is small, the (asymmetric) Huber quantile loss\u201d, what do you mean by better gradients? \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1935/Reviewer_RvL2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1935/Reviewer_RvL2"
        ]
    },
    {
        "id": "p3YhfqgOHFb",
        "original": null,
        "number": 4,
        "cdate": 1666872294090,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666872294090,
        "tmdate": 1666872516139,
        "tddate": null,
        "forum": "ZMxVNpd76mw",
        "replyto": "ZMxVNpd76mw",
        "invitation": "ICLR.cc/2023/Conference/Paper1935/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper aims at achieving robustness in uncertain environments. The proposed approach, a mean-standard deviation formulation, seemingly reduces the desired level of robustness to one parameter tuning. Experiments are conducted on discrete and continuous control domains. ",
            "strength_and_weaknesses": "The idea of relating distributional RL for the sake of robustness is interesting and novel, although the connection is quite natural. However, I think there are two major flaws in this paper, in terms of 1/ novelty; 3/ quality. I detail my concerns below.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "1/ Novelty\n\nIt is not clear to me what novelty this paper brings, may it be from a distributional, a regularization, or a robust RL perspective. What are the theoretical results? Robust MDPs with f-divergence uncertainty sets have been studied in [1]. It seems that this paper is a particular case of that work: [1] considers s-rectangular uncertainty sets as opposed to the (s,a)-rectangular sets of this one. Indeed, Bellman recursions and critic estimates are all considered in terms of Q, not v. By design, this implicitly means that uncertainty sets are independently defined for each state-action pair. \n\nFrom a robust/regularized RL perspective, learning the variance besides the expected reward has been done in [2] already, while [3] has trained the exact same mean-std objective as here. I would have expected [3] to appear as a baseline in the experiments. This yields my second question to the authors: what are the experimental contributions? \n\n\n\n2/ Quality/Clarity\n\nIn my opinion (please correct me if I am wrong), there are some misunderstandings regarding some cited works. I detail my claim below:\n\n- In the 2d paragraph (Sec 1), it says \u00ab\u00a0in general, this problem is NP-hard due to the complex max-min problem, making it challenging to solve in a discrete state action space and to scale to a continuous action space\u00a0\u00bb: Actually, the complexity of computing a robust optimal policy is unrelated to the size of the state-action space, but rather, it is related to the structure of the uncertainty set. See [4] - for the problem to be tractable we basically need some rectangularity assumption, as mentioned above. \n\n- The 3rd \u00a7 of Sec 1 is unclear to me. The authors claim that they focus on more general, continuous state space, but they do not provide any theoretical result. Additionally, although not formally studied in most RL works, most standard properties such as contractive operator, greediness, etc., still hold for continuous action and state spaces, as long as the action space is compact and the state space Polish + measurable (see [5])\n\n- p. 2 - \u00a72: \u00ab\u00a0\u00a0The problem of uncertainty under the distribution of the environment is transformed into a problem with uncertainty over the distribution of the rewards, which makes it tractable\u00bb: In fact, this is arguable. As shown in [6], it depends on the type of regularization being used. For example, policy regularization alone may not account for uncertainty in dynamics, whereas value regularization (as in this paper) does. This point should be made very clear, as I think this represents the main interest in doing distributional RL for robustness purposes. \n\n- In general, related works are being cited and described without being compared to this study (for example in \u00a74 of Sec. 1)\n\n",
            "summary_of_the_review": "Overall, I think this paper holds an interesting intuition by relating distributional RL to robustness, but it should be further investigated and analyzed: formally, why is the std penalty adding robustness to varying P, theoretically? How different is it from the policy regularization perspective? Why not just do TD learning on the variance as in [2]? \n\nReferences\n\n[1] Ho, Chin Pang, Marek Petrik, and Wolfram Wiesemann. \"Robust Phi-Divergence MDPs.\"\u00a0arXiv preprint arXiv:2205.14202\u00a0(2022).\n\n[2] Tamar, Aviv, Dotan Di Castro, and Shie Mannor. \"Learning the variance of the reward-to-go.\"\u00a0The Journal of Machine Learning Research\u00a017.1 (2016): 361-396.\n\n[3] Pan, Xinlei, et al. \"Risk averse robust adversarial reinforcement learning.\"\u00a02019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.\n\n[4] Wiesemann, Wolfram, Daniel Kuhn, and Ber\u00e7 Rustem. \"Robust Markov decision processes.\"\u00a0Mathematics of Operations Research\u00a038.1 (2013): 153-183.\n\n[5] Puterman, Martin L. \"Markov decision processes.\"\u00a0Handbooks in operations research and management science\u00a02 (1990): 331-434.\n\n[6] Derman, Esther, Matthieu Geist, and Shie Mannor. \"Twice regularized MDPs and the equivalence between robustness and regularization.\"\u00a0Advances in Neural Information Processing Systems\u00a034 (2021): 22274-22287.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1935/Reviewer_pRgU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1935/Reviewer_pRgU"
        ]
    }
]