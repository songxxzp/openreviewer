[
    {
        "id": "_YbPJcLh36L",
        "original": null,
        "number": 1,
        "cdate": 1666489374156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666489374156,
        "tmdate": 1668569722995,
        "tddate": null,
        "forum": "Ur_qORZ6-9R",
        "replyto": "Ur_qORZ6-9R",
        "invitation": "ICLR.cc/2023/Conference/Paper2316/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a deep convolution network (DECN) that mimics the operation of evolutionary search techniques, specifically recombination and selection, for black-box optimization problems. The paper first describes the general challenges in creating a convolution operator to mimic evolutionary, as well as related work, and then provides definitions of the operations the authors aim to achieve in DECN design (recombination and selection). Subsequently, the authors describe their method, including how they construct a tensor from a population of solutions for a given black-box optimization problem and arrange it to make it amendable for learnable convolutions. The convolution reasoning module (CRM) serves to produce the offspring for the population, whose components are described in detail by the authors, while the selection module (SM), based on pairwise comparisons of solution fitnesses, serves to select the solutions of the evolving population. Taken together CRM and SM make up an evolution module (EM), which can then be used for end-to-end gradient based optimization of the DECN modules. \n\nThe authors then test DECN on six black-box optimization functions and compare DECN's performance to a set of black-box optimization algorithms with the Table 1 showing outperformance of DECN. Subsequently, authors show results on a protein docking task, provide a study of generalization of DECN module across the six aforementioned optimization functions, as well as GPU runtime study. ",
            "strength_and_weaknesses": "**Strengths**\n\n* The paper proposes a novel method to introduce learnable convolution operations into an evolution inspired setting. The results shown in the paper indicate that the proposed method outperforms some common black-box optimizers on a given set of tasks in addition to providing some generalization benefits.\n* The papers provides a cohesive data structure for applying the method and outlines its different component in much detail.\n\n**Weaknesses**\n\n* The paper studies only limited setting (six standard black box optimization functions) with limited experimental settings. Given that the authors claim scalability and speed of their method as an advantage, it would have been nice to see more challenging optimization settings with greater compute costs. \n* The protein docking experiments is not very well described. How does this setting turn into continuous optimization and what is the search space? It would also be helpful to have a consistent set of baselines across all experiments.\n* The setting of the transferability study is unclear. Is this zero-shot learning or few-shot learning? It's hard to asses how meaningful the results are without those details.\n* The authors dedicate a large part of the paper to describing many details of their methods, which then ends up taking up the majority of space. I think the paper could be strengthened by focusing the method on the important parts and re-using the rest of the paper to perform a more thorough analysis.\n\n**Additional Questions**\n\n* Are you initializing multiple populations at the same time? In Algorithm 1, you say \"initialize a minibatch comprised of K populations\", which appears to imply that there are multiple populations solving the same problem?\n* What is $\\zeta$ in equation 7? I did not find a definition.\n* It would be good to clarify what DE is in your baselines.\n* Did the GPU implementation rely purely on deep learning frameworks? Your general description seems to indicate that. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe clarity in the current draft is often weak, making it difficult to follow important parts of the paper. Furthermore, many of the tables and figures (including Table 2, 3, 4) do not have enough information to understand them on their own forcing the reader to track relevant sections of text which are sometimes not co-located.\n\n**Quality**\n\nOverall, the presented experiments appear sound, but it is difficult to judge their overall quality based the lack of clarity in some sections of the paper.\n\n**Novelty**\n\nOverall, the core part of the methodology (creating the convolution operator and associated data structure for the solution space) appears novel and relevant.\n\n**Reproducibility**\n\nThe detailed descriptions provided in the paper should make it reasonable to reproduce the presented results. While the authors do not mention release of code explicitly in the paper it appears to be included in the supplementary material. Moreover, the current draft does not include a reproducibility statement that could clarify reproducibility questions.",
            "summary_of_the_review": "Overall, I would say that the weaknesses of the paper in its current form outweigh the strengths leading me to vote for rejection. My most pressing concerns include the lack of clarity in the way the paper is currently written (which affects many other parts of my assessment), as well as the quality and relevance of the current experiments (would like to see more challenging optimization problems and the inclusion of more modern baseline algorithms). I would say that the core novelty proposed in the method holds promise and would encourage the authors to continue to refine their research to make the paper stronger. \n\n---\nUpdating my score during discussion phase given the changes and responses provided by the authors.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_g29J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_g29J"
        ]
    },
    {
        "id": "nAql0ZCCnkw",
        "original": null,
        "number": 2,
        "cdate": 1666612959034,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612959034,
        "tmdate": 1666612959034,
        "tddate": null,
        "forum": "Ur_qORZ6-9R",
        "replyto": "Ur_qORZ6-9R",
        "invitation": "ICLR.cc/2023/Conference/Paper2316/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new evolution based search technique inspired by convolutional networks used for black box optimisation.  The paper introduces a novel mechanism for generating and then selecting individuals in a population based on learnable convolutions.  The paper empirically test the new approach on a number of problems and demonstrates superior performance to some baseline optimisers both in terms of accuracy and speed. They further show that the method benefits from transfer between problems.",
            "strength_and_weaknesses": "The strength of the approach is that it proposes a novel technique that are shown to have better performance than other well known methods.\n\nFor me there are a number of weaknesses.  The approach is motivated by a rather unconvincing (at least to me) analogue between recombination operators and convolutions.  Convolutions in deep neural networks have advantages such as locality and translational invariance that do not seem to carry over to optimisation.  The model of recombination operator is rather strange as it looks like a mixing operator that is likely to result in rapid convergence.  This seems to addressed by a diversity mechanism that is described in a single line.  For me the paper needs to make a more convincing case why the convolution being proposed is in anyway useful.  The resulting model seems to me very complicated and ad hoc with no real motivation.  Maybe I am missing something, but for me I need much more convincing that the proposed method is well motivated.\n\nOf course, the results seem impressive and may well justify the method.  However, the test functions used are a very old set of problems that are often solved efficiently by algorithms that are not generally very competitive.  They are not particularly representative of the type of continuous variable optimisation problem that most people are interested in solving.  To make a convincing empirical case for this approach I would like to see this tested on a modern set of test problems.",
            "clarity,_quality,_novelty_and_reproducibility": "I found part of the writing hard to follow.  For example, the introduction to section 4 was very dense and confusing. As I minor comment it would help to correctly use citations (\\citep or \\citet depending on the package you are using).  I felt the inspiration from CNNs to be superficial rather than profound.  At least, I needed a lot more convincing that it was profound.  A lot of the statements about the remarkable performance of EAs needed more justification or just needed to be a more balanced assessment of their value.  The method looks very novel and I assume the results can be reproduced.",
            "summary_of_the_review": "The authors have clearly worked hard to develop a novel method to perform black-box optimisation.  However, the field of EAs are full of such novel methods each claiming to be SOTA based on a rather limited empirical test.  For me, I need to see a convincing argument why the approach should work and this seemed missing.  I also need some strong empirical comparison on competitive problems and I felt that despite some effort in this direction it was still not sufficiently convincing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_j4Dd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_j4Dd"
        ]
    },
    {
        "id": "q_5vYksjofy",
        "original": null,
        "number": 3,
        "cdate": 1666656869258,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656869258,
        "tmdate": 1666656869258,
        "tddate": null,
        "forum": "Ur_qORZ6-9R",
        "replyto": "Ur_qORZ6-9R",
        "invitation": "ICLR.cc/2023/Conference/Paper2316/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a deep evolutionary convolution network (DECN) for continuous black-box optimization. DECN is composed of two modules: convolution-based reasoning module (CRM) and selection module (SM). The paper describes both modules and shows how to integrate them. It also contains a description of the process of training including the design of the loss function. The experiments carried out on unconstrained continuous optimization problems show that DECN surpasses some black-box optimization baselines and obtains good performance when transferred to optimization problems unseen during the training stage. The paper also presents experiments on accelerating DECN using GPU suggesting its good adaptability to GPU acceleration due to the tensor operator. The main article is also followed by the Appendix providing more details, including limitations of the model.",
            "strength_and_weaknesses": "Strengths:\n- Originality: the idea seems to be quite novel.\n- Comprehensive description: many details are provided, and the description of the method and experiments seem to be comprehensive.\n- Quality of results: the experiments carried out on unconstrained continuous optimization problems show that DECN surpasses some black-box optimization baselines and obtains good performance when transferred to optimization problems unseen during the training stage. The paper also presents experiments on accelerating DECN using GPU suggesting its good adaptability to GPU acceleration due to the tensor operator.\n- The authors are honest about the limitations of DECN.\n\nWeaknesses:\n- In some cases, writing could be improved.\n- It doesn't look that the code is available, so the reproducibility is limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality seems to be very good and the idea seems to be novel.\nIt doesn't look that the code is available, so the reproducibility is limited, but the general description of experiments is quite detailed.\nIn general, the clarity is good but some parts could be improved:\n- it's not clear how to interpret the results in brackets in tables 1,2,3\n- In the case of Dragonfly, there were no results for D=100, it might be good to explain why\n- Section 4: it's not sure what is \\phi (and the description at the beginning of this section is quite complicated, so maybe could be improved)\n- p. 9: Appendix.A.6. -> should be Appendix A.5.",
            "summary_of_the_review": "In general, I like the paper and the idea - it seems to be quite original and the quality of the results seems to be very good. The weaknesses of the article that I see are related mostly to clarity and lack of reproducibility. The weaknesses of the method are clearly explained in the appendix. The quality of the results seems to be very good.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_Rucq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_Rucq"
        ]
    },
    {
        "id": "1MwL1jsHua",
        "original": null,
        "number": 4,
        "cdate": 1666716366548,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666716366548,
        "tmdate": 1670694269768,
        "tddate": null,
        "forum": "Ur_qORZ6-9R",
        "replyto": "Ur_qORZ6-9R",
        "invitation": "ICLR.cc/2023/Conference/Paper2316/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a black-box optimization algorithm by incorporating convolutional neural networks into an evolutionary algorithm. Convolutional layers are used for both generating the offspring as well as selecting the offsprings that survive. Experiments are performed on standard black-box functions and protein docking.",
            "strength_and_weaknesses": "Strengths:\n1. Adopting neural networks within evolutionary algorithms for black-box optimization is interesting.\n2. The method seems to be effective, especially for the protein docking problem, task-specific optimization methods have not been compared against.\n\nWeaknesses:\n1. While the idea of modeling the recombination and selection operators through neural networks .\n2. The approach may suffer from the common problems with current learning based methods, e.g., overfitting, lack of domain generalization etc. If the optimization landscapes used for training and deployment are not similar, the approach may lead to sub-optimal solutions. Furthermore, there is no mechanism to identify such scenarios. The paper acknowledge this limitation.\n\nOther Comments:\n1. The topic of the paper, evolutionary black-box optimization, may be more suitable and better appreciated by evolutionary optimization community.",
            "clarity,_quality,_novelty_and_reproducibility": "- Parts of the paper are not clear, especially the notation in Section 4. The notation in other sections are also not very clear. \n- The quality of the paper could be significantly improved. First, the motivation for modeling the operations through convolutional networks is not clear. Parts of the exposition are not clear, both in terms of notation and grammar.\n- I did not check, but code was provided for reproducing the proposed method.",
            "summary_of_the_review": "The paper proposes to incorporate convolutional networks into evolutionary algorithms for solving black-box optimization problems. Both recombination and selection operations are modeled through learned convolutional networks. The motivation for modeling the operators through convolutional layers is not well justified, given that the population members do not necessarily have spatial relations. Experimental evaluation is limited in some respects.\n\nPost Rebuttal Update: \n\nI read the response of the authors, the discussion between reviewers and authors, and the updated paper.\n\nThe original submission certainly had some writing and presentation issues. The author's rebuttal pointed to at least two reviewers who misunderstood the paper. The paper has since been revised significantly. I think the revised form is certainly better, but there are still some claims that are too strong, and I do not agree with them. For example, the claim that evolutionary algorithms have poor generalization.\n\nI have slightly increased my rating, but I still lean toward rejection. I feel the paper has potential but needs further revision.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_9TVi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_9TVi"
        ]
    },
    {
        "id": "iLo5NSVrW7",
        "original": null,
        "number": 5,
        "cdate": 1666811534577,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666811534577,
        "tmdate": 1666811534577,
        "tddate": null,
        "forum": "Ur_qORZ6-9R",
        "replyto": "Ur_qORZ6-9R",
        "invitation": "ICLR.cc/2023/Conference/Paper2316/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes the use of convolutional neural networks to act as a recombination operator in a population-based black-box optimization algorithm (aka evolutionary algorithm). That operator is combining the solutions making the population, ranked according to their fitness, with weights provided by the learned convolutions making the neural network. The recombination is made both between individuals, through some kind of mix-up between all individuals of the population through a depthwise separable convolution. The weights are modulated according to the fitness of the individuals used, and multiple convolutions are averaged to produce the output individuals. The neural network model is trained on some problems to generate a combination operator to use for similar problems. Results are provided by comparing the proposed approach with some evolutionary algorithms, with competitive performances.",
            "strength_and_weaknesses": "Strength\n- The proposal is relatively straightforward and carries interesting ideas for the  learning-to-optimize context.\n- The results show the approach is able to outperform some black-box optimization baselines for the problems tested.\n- The approach appears to be GPU-friendly.\n\nWeaknesses\n- The approach proposed relies on some intuitive idea that convolutions in neural networks make some computations that can be useful to replace combination operators in an evolutionary algorithm. This intuition appears really rough to me, and the overall conceptual proposal is not well principled.\n- The writing clarity of the paper is low, there is a lack of precision and details, several elements remain unclear and reproducibility appears not obvious to me.\n- Problems tackled are a subset of what can be tested for real-valued black-box optimization (although they are the common baselines).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall paper is not always clear, the writing is lacking at times and is not very precise. For example, the DECN model presentation at the beginning of sec. 4 is not very helpful nor clear, and I am not sure what we want to express with figure 1. Likewise for the explanations in sec. 4.1 on the Convolution Reasoning Module, the explanations are long and not always necessary to the point, informative/pedagogical or even relevant. I found it difficult to extract the useful information from the method over the sections. Another example of clarity lacking is figure 7, which is hard to read and not very helpful regarding the proposal.\n\nOther examples of unclear statements are with the beginning of sec. 5.1, which states an important methodological element as \u201cWe test DECN on six standard black-box functions (Appendix Table 6). We train DECNs on the original functions, and then we train six DECNs in this part.\u201d I don\u2019t get it, are the six standard black-box functions directly related to the six DECNs trained in that part? And is the training made on the original functions (which concept is not properly defined) done over all these functions simultaneously, or one at the time? Said otherwise, do we have a DECN trained on all original functions, or six DECNs because we have six original functions (I know from the appendix this is not the case since there are only three original functions, F1, F2 and F3). \n\nThe overall quality of the experiments remains quite below the usual standards. It is written \u00ab The parameters of DE, ES, CMA-ES, and Dragonfly are adjusted to be optimal. \u00bb, which is not specific. Are these parameters adjusted once, or for each problem? I don\u2019t get the transferability experiments (sec. 5.3), I am not even sure what we mean here by transferability. The lack of details over the experiments makes reproducibility difficult to achieve in my opinion.\n\nI am really unsure about the speedup presented by using GPUs over other approaches. First, the execution time with the EA is really high, higher than what I see and know with these optimization approaches. There might be implementation or configuration issues \u2013 which library has been used to run them. Also, in real-life, more of the computation is usually passed on fitness evaluation. where GPUs are often not helpful.\n\nAs for the originality, this is not the first work on learning-to-optimize and the overall motivations are not well developed, not in a convincing way. I am far from being convinced by the approach, it doesn\u2019t sound like the way to go for learning-to-optimize, as the operations are based on fitness values and fitness ranking, that signal appears rather weak for guiding the recombination operator.\n",
            "summary_of_the_review": "The overall proposal is not well motivated, the explanations are not sufficiently precise and clear, the experiments reported appear good, but still do not bring high confidence in them given the lack of clarity and precision of the experimental configuration. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_Tk68"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2316/Reviewer_Tk68"
        ]
    }
]