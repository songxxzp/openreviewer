[
    {
        "id": "JYyqr_6WJXt",
        "original": null,
        "number": 1,
        "cdate": 1666188806026,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666188806026,
        "tmdate": 1666188806026,
        "tddate": null,
        "forum": "rByagyHWlpb",
        "replyto": "rByagyHWlpb",
        "invitation": "ICLR.cc/2023/Conference/Paper4376/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work suggests that the attention matrix, a key module in Transformer but computationally inefficient, can be approximated by low-rank matrix multiplications. Given the arguments, the authors proposed a new efficient Transformer variant using random projections. Experiments, including popularly used LRA benchmarks, are conducted to analyze the empirical performance of the model with comparisons to several baselines.",
            "strength_and_weaknesses": "I believe the paper is far below the paper acceptance threshold and here are several critical points that needs to be essentially addressed.\n\n[Writing Quality]\n\nThr writing is rather poor and there are plenty of mistakes. I list some of them as below but there are much more in the paper.\n\n1. In most places, term \"the\" is not correctly used. \n2. In section 3.1, what is non-destructively?\n3. In section 3.1, what is conditional extropy?\n4. In section 3.2, why you call Johnson\u2013Lindenstrauss lemma as \"JS lemma\"\n5. In section 3.2, \"an d \\times d matrix\"\n\n[Math]\n\nA critical flaw occurs in equation (6). It is ok to assume that QK is a low-rank matrix. However, P = softmax(QK), and softmax is a non-linear operator. There is no guarantee that a low-rank matrix passing through a non-linear operator is still a low-rank matrix. Furthermore, in practice, for the softmax function, P is usually a high-rank function. I recommend the authors have a try. Given this, we cannot get P=WrP'Wc, where all of them are low-rank.\n\nNote that in any case, you can always use JK lemma and random projections. But such an analysis has already been used in Linformer.\n\n[Experiment]\n\nThe author missed important effcient Transformer S4 (\"Efficiently Modeling Long Sequences with Structured State Spaces\"). In all of the LRA tasks, S4 is roughly better than the proposed DBA for 25 points. I have little reason to vote a paper for acceptance given the significant performance gap.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "As I described above, I found the paper is not easy to read given the writing mistakes (clarity), has mathematical flaws in the analysis (quality), leads to poor experimental results compared to SoTA (novelty).",
            "summary_of_the_review": "I believe proposing efficient Transformer is essential but the current quality of the work (writing, analysis, empirical results) are not ready to publish in the venue, and I recommend rejection.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4376/Reviewer_YUik"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4376/Reviewer_YUik"
        ]
    },
    {
        "id": "pd1rB0qFct",
        "original": null,
        "number": 2,
        "cdate": 1666643304729,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643304729,
        "tmdate": 1666643304729,
        "tddate": null,
        "forum": "rByagyHWlpb",
        "replyto": "rByagyHWlpb",
        "invitation": "ICLR.cc/2023/Conference/Paper4376/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposed to learn the projection matrices for reducing the dimension of intermediate matrices in Transformer models and achieves significant speedup while achieving better performance than previous efficient attentions.",
            "strength_and_weaknesses": "Pros:\n- The learnable projection in the paper is novel and squeezes the computation not only in terms of sequence length but also in terms of hidden dimension. It is encouraging to see that simply choosing d_p and d_in to be fixed suffices for achieving good performance.\n- The empirical results are promising compared to other linear attention-based models.\nCons:\n- The presentation in Section 3.1 is a little confusing because the W_r' and W_c' depend on the input QK, while Equation (4) makes readers feel they are given.\n- Table 2 in the paper is a little misleading because the methods are not sorted by any meaningful order (e.g., Avg)\n- (Minor) the performance cannot be comparable with new models based on state-space models. It might be helpful to apply similar ideas as this paper to those models to achieve further gain.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation needs to be improved, and please refer to the cons.",
            "summary_of_the_review": "In summary, the method in this paper is novel and interesting, while some minor modifications are needed to clarify the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4376/Reviewer_PZVd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4376/Reviewer_PZVd"
        ]
    },
    {
        "id": "gIGXj9eRjQ",
        "original": null,
        "number": 3,
        "cdate": 1666664248975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664248975,
        "tmdate": 1669056186527,
        "tddate": null,
        "forum": "rByagyHWlpb",
        "replyto": "rByagyHWlpb",
        "invitation": "ICLR.cc/2023/Conference/Paper4376/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new low-rank method, called DBA, to approximate the attention mechanism used in transformer layers. It has two main differences from previous approaches:\n\n1. DBA maps the input sequences $Q, K \\in \\mathbb{R}^{n \\times d}$ to shorter sequences $Q', K' \\in \\mathbb{R}^{d_p \\times d}$ via a linear transformation that captures information from the input matrix itself, e.g., $Q' = \\phi(WQ^\\top)Q$, where $W \\in \\mathbb{R}^{d_p \\times d}$ are learnable parameters. \n2. DBA also projects queries and key vectors to lower dimensionalities ($d_{in} \\ll d$)\n\nResults show that DBA is faster and consumes less memory than related approaches while achieving strong predictive accuracies on several tasks.\n\n",
            "strength_and_weaknesses": "Things that I liked in this paper:\n\n- The authors explain all the matrix transformation steps in detail, making the linear algebra math easy to follow in section 3.1.\n- The idea of using information about the input sequence before projecting it to a shorter sequence.\n- The results are impressive.\n\nHowever, there are many things that I disliked:\n\n- The overall method is not novel, and most central ideas were proposed in previous approaches (Linformer and Performer).\n- The introduction is very repetitive.\n- Section 3.2 seems unnecessary and confusing. \n- The experimental section is uninformative, especially for sections 4.3 and 4.4.\n- Some math terms are not defined. For example, what is \"Linear\" in Equation 18? Is it needed? What is the actual function used as $\\psi$?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality:\n- The results in Tables 3 and 4 are very close to the ones obtained by related approaches. It is hard to assess the quality of the results without confidence intervals.\n\nClarity:\n- It is unknown how DBA was applied for dynamic length inputs in section 4.3.\n- The writing is casual in some parts. E.g., \"A lot of studies ...\" or \"... by digging into the ...\"\n- Overall, I think the writing can be improved.",
            "summary_of_the_review": "This paper proposes a new efficient alternative to attention in transformers. The proposed method achieves impressive results on several tasks while being time and memory efficient. However, I believe this paper can be improved in terms of clarity and quality, especially regarding the experimental section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4376/Reviewer_9GWA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4376/Reviewer_9GWA"
        ]
    },
    {
        "id": "ws4hvWlX2U",
        "original": null,
        "number": 4,
        "cdate": 1666908036554,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666908036554,
        "tmdate": 1666908036554,
        "tddate": null,
        "forum": "rByagyHWlpb",
        "replyto": "rByagyHWlpb",
        "invitation": "ICLR.cc/2023/Conference/Paper4376/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a low-rank attention mechanism named DBA. Several experiments have been conducted to validate the proposed method's effectiveness.",
            "strength_and_weaknesses": "Strength:\n* The technical details are presented clearly and with details\n* The experiments include state-of-the-art methods as baselines\n* The paper is structured well and easy to follow\n\nWeakness:\n* The novelty needs to be further justified\n* More ablation studies would be ideal to better understand the model",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the technical details are sufficient and the experiment results look promising. It would be better if the authors can further justify the novelty of the proposed method.",
            "summary_of_the_review": "My major concern is about the novelty of the proposed method. It would be great if the authors can make a table to compare the proposed method w/ other low-rank based attention modules on the their differences and why the contribution is not incremental. The authors do mention that the the proposed projection is not pre-determined, it would be better if the authors can better motivate why this pre-determination is not ideal and validate this from experiment results.\n\nThe authors list the hyper-parameters used in Sec. A.2, I wonder how these parameters may affect model performance. It would be better if the authors can conduct more ablation study to analyze the robustness of the proposed approach.\n\nI also suggest the authors to describe how many seeds/repetitions each model has been trained in the experiments and report the accuracy variance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4376/Reviewer_tdpB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4376/Reviewer_tdpB"
        ]
    }
]