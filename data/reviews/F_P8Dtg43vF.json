[
    {
        "id": "0J3FKc29nO0",
        "original": null,
        "number": 1,
        "cdate": 1666604140878,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604140878,
        "tmdate": 1666604140878,
        "tddate": null,
        "forum": "F_P8Dtg43vF",
        "replyto": "F_P8Dtg43vF",
        "invitation": "ICLR.cc/2023/Conference/Paper1953/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a spatial-temporal self-attention model (Ego-STAN) for egocentric-view human pose estimation (HPE) task. The main contribution is a learnable feature map token which aggregates features from past image frames which are then used as input to a spatial-temporal Transformer. Experiments on three public datasets (two egocentric-view and one outside-in-view) are conducted to verify the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:  \n1) A feature map token (FMT) with learnable parameters is designed to aggregate information from multiple image frames to augment the contextual feature embedding of a spatial-temporal Transformer.\n2) A hybrid spatial-temporal Transformer is proposed to process convolutional feature maps of image sequence with spatial-temporal self-attention.\n3) Extensive experiments are conducted to verify the proposed method. Superior performance is achieved with the proposed Ego-STAN compared with related SOTA methods in egocentric HPE. Detailed ablation studies are also conducted to analyze the impact of each component of Ego-STAN.\n\nWeakness:  \n1) Most of the performance improvement seems to come from the spatial-temporal Transformer architecture (+10%), while the improvement by the core component of FMT is limited (+2%). The spatial-temporal Transformer can hardly be seen as the technical contribution of this work.  \n2) Some technical details are missing or not clear. The network (probably a fully connected layer) for estimating the feature map token is not given. Is each unit of FMT accompanied with a unique network or a shared network? The details for the learnable position embedding are not provide either. In Figure 2, there should be a connection from CNN feature maps (F) to feature map token (K).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is almost well written and easy to follow, with minor details missing as described above.  \nImplementation details and code are provided in the supplementary material, which eables reproducibility of the paper.",
            "summary_of_the_review": "My overall rating is positive with several concerns. Please see above comments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1953/Reviewer_cA4Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1953/Reviewer_cA4Q"
        ]
    },
    {
        "id": "nfB6hn-acP",
        "original": null,
        "number": 2,
        "cdate": 1666752923430,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666752923430,
        "tmdate": 1666752923430,
        "tddate": null,
        "forum": "F_P8Dtg43vF",
        "replyto": "F_P8Dtg43vF",
        "invitation": "ICLR.cc/2023/Conference/Paper1953/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents spatio-temporal transformer model for ego-HPE. A spatial concept called feature map tokens is introduced to attend to all other spatial units in the spatio-temporal feature maps. The proposed method achieved superior performance on the xR-EgoPose dataset. ",
            "strength_and_weaknesses": "Strength:\n\n1. The proposed method achieves superior performance on the xR-EgoPose dataset.\n\n2. Comprehensive ablation study is conducted. \n\n\nWeakness:\n\n1. The technical novelty of the proposed method is somewhat incremental. The proposed feature map token is just a simple way to perform global attention from the extracted features of CNN backbone. Some of the tricks used for performance improvement like L1-based 3D loss are incremental. \n\n2. Although there is a graphical illustration of the FMT, it is still not clear about the specific operation. Concrete equations can be provided to make it clear about the actual operations.  \n\n3. There are some details about the overall framework are missing. For example, how many (image) frames are used to reconstruct a 3D pose? Are the previous t frames used to reconstruct the (t+1) 3D pose? These settings should be made clear. Also, in the experiments, are these settings keep the same for all the competing methods?\n\n4. Direct regression from heatmap (or 2D pose) to 3D pose with a simple neural-network based design is not a significant contribution. This is common in the literature of general 3d human pose estimation. \n\n5.  The experimental evaluation is weak. 1) Only one ego-HPE method (i.e., reference [21]) is used for comparison on the Ego-dataset. Reference [21] was published in 2019. It may not represent the state-of-the-art. 2) 3d human pose estimation from image/video is a well-established field with many recent works using vision-transformer for achieving impressive results. It is important to also include the recent state-of-the-art (image-based and video-based) outside-in 3d pose estimation methods for a comprehensive evaluation on the datasets used in the experiments. There are many works also focus on occlusion-robust pose estimation. Otherwise, the result comparison is not convincing, and it is difficult to justify why a specific ego-focused HPE method is needed.  \n\n6. How the proposed method can address the occlusion issue? Some results and visualization should be provided. \n\n7. The computational complexity aspect (e.g., FLOPs) of the method should be discussed and compared with sota methods. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Some details of the proposed method and experimental setting are not clear or missing. \n\nQuality: The technical novelty of the proposed method is somewhat limited. The experimental evaluation should be enhanced significantly. \n\nNovelty: The problem is not new, the novelty of the proposed method is somewhat limited.\n\nReproducibility: Authors provided the code in the supplementary material.  \n",
            "summary_of_the_review": "Overall, the technical novelty of the proposed method is incremental and the experimental evaluation cannot fully justify the effectiveness of the proposed method without a comprehensive comparison to the sota 3d human pose estimation approaches. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1953/Reviewer_kGU6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1953/Reviewer_kGU6"
        ]
    },
    {
        "id": "FvSHW4RtIX",
        "original": null,
        "number": 3,
        "cdate": 1666774857322,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666774857322,
        "tmdate": 1666774857322,
        "tddate": null,
        "forum": "F_P8Dtg43vF",
        "replyto": "F_P8Dtg43vF",
        "invitation": "ICLR.cc/2023/Conference/Paper1953/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a spatio-temporal transformer based model to estimate ego-centric 3D human pose in challenging scenarios, e.g. self-occlusions and strong distortion. The framework consists of feature map token (FMT), heatmap based representations and a direct 2D to 3D pose estimation module. They conducted experiments above two public datasets, and extensive ablation study to verify the effectiveness of each introduced module.",
            "strength_and_weaknesses": "The entire paper is well-written and easy to follow. Their contributions are clear. They introduce simple but effective strategy, e.g. temporal spatial attention to aggregate information of image sequence, into the framework in the challenging egocentric human pose estimation in occlusions and strong distortions. The proposed Ego-STAN outpeforms several representative baselines largely.\n",
            "clarity,_quality,_novelty_and_reproducibility": "It would be good to release code or include a table to list framework structure in the appendix.",
            "summary_of_the_review": "Their proposed framework can solve problems of pose estimation in challenging scenes, but the introduced modules have been already shown in previous works, so that I rate the novelty of this work as marginal above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1953/Reviewer_uHJW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1953/Reviewer_uHJW"
        ]
    }
]